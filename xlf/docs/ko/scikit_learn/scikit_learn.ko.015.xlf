<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="7c13e77bc830b382c041eec0e3fcc1723f375e05" translate="yes" xml:space="preserve">
          <source>Homogeneity metric of a cluster labeling given a ground truth.</source>
          <target state="translated">근거가 주어진 클러스터 라벨링의 동질성 메트릭.</target>
        </trans-unit>
        <trans-unit id="3afc9b67230d985e8782525c7b58b615e013e8f9" translate="yes" xml:space="preserve">
          <source>Homogeneity, completeness and V-measure can be computed at once using &lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt;&lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt;&lt;/a&gt; as follows:</source>
          <target state="translated">균질성, 완전성 및 V 측정은 다음과 같이 &lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt; &lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt; &lt;/a&gt; _ 완전성 _v_measure를 사용하여 한 번에 계산할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0878824f511837fc1a1c8d27240af19053ebdbd4" translate="yes" xml:space="preserve">
          <source>HouseAge median house age in block</source>
          <target state="translated">블록에서 HouseAge 중간 주택 연령</target>
        </trans-unit>
        <trans-unit id="95f00bca96463f976c07bb46f71ce6b37ead0db0" translate="yes" xml:space="preserve">
          <source>How often to evaluate perplexity. Only used in &lt;code&gt;fit&lt;/code&gt; method. set it to 0 or negative number to not evaluate perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be45c283b4c54643c38f84bc65a4bfc525d6d30a" translate="yes" xml:space="preserve">
          <source>How often to evaluate perplexity. Only used in &lt;code&gt;fit&lt;/code&gt; method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.</source>
          <target state="translated">난이도를 평가하는 빈도. &lt;code&gt;fit&lt;/code&gt; 방법으로 만 사용하십시오 . 훈련의 난이도를 전혀 평가하지 않으려면 0 또는 음수로 설정하십시오. 난이도를 평가하면 교육 과정에서 수렴을 확인하는 데 도움이되지만 총 교육 시간도 늘어납니다. 반복 할 때마다 난이도를 평가하면 훈련 시간이 최대 2 배 증가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="82dc6d28bb69b15075cb833b4ea7ee856b1fb8a5" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="060faa287065b4ad6ba6c00f598635b42adc21de" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">분모에서 노멀 라이저를 계산하는 방법. 가능한 옵션은 'min', 'geometric', 'arithmetic'및 'max'입니다. '경고'인 경우 '형상'이 사용됩니다. 버전 0.22에서는 기본값이 '산술'로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="b8efa217d0db9ce56ac60653645beebe151304f9" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;max&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">분모에서 노멀 라이저를 계산하는 방법. 가능한 옵션은 'min', 'geometric', 'arithmetic'및 'max'입니다. '경고'이면 'max'가 사용됩니다. 버전 0.22에서는 기본값이 '산술'로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="cf894bb3f8fceedab10cdd5da9a5edd37e00865d" translate="yes" xml:space="preserve">
          <source>How to construct the affinity matrix.</source>
          <target state="translated">선호도 매트릭스 구성 방법</target>
        </trans-unit>
        <trans-unit id="d34268ba2716d71aaaeee2c35e527ca55a46dd2f" translate="yes" xml:space="preserve">
          <source>However ARI can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO).</source>
          <target state="translated">그러나 ARI는 TODO (클러스터링 모델 선택)에 사용할 수있는 컨센서스 인덱스의 빌딩 블록으로 순수하게 감독되지 않은 설정에서 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0121c2b22395d1a9db3fd77f24d0a9f0e41170e3" translate="yes" xml:space="preserve">
          <source>However MI-based measures can also be useful in purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection.</source>
          <target state="translated">그러나 MI 기반 측정은 순전히 감독되지 않은 설정에서 클러스터링 모델 선택에 사용할 수있는 합의 색인의 빌딩 블록으로 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="117b6230e0e8ab3fcdc1b277507becef8a05f759" translate="yes" xml:space="preserve">
          <source>However care must taken to always make the affinity matrix symmetric so that the eigenvector decomposition works as expected.</source>
          <target state="translated">그러나 고유 벡터 분해가 예상대로 작동하도록 항상 선호도 행렬을 대칭으로 만들도록주의해야합니다.</target>
        </trans-unit>
        <trans-unit id="92447df8a934c98a63d3aa91a9c263efa88d6300" translate="yes" xml:space="preserve">
          <source>However let&amp;rsquo;s keep our high capacity random forest model for now so as to illustrate some pitfalls with feature importance on variables with many unique values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="925f5b77eb2888a89c04118c35bff0f0ace7255e" translate="yes" xml:space="preserve">
          <source>However the RI score does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples).</source>
          <target state="translated">그러나 RI 점수는 임의의 레이블 할당이 0에 가까워지는 값을 보장하지 않습니다 (클러스터 수가 샘플 수와 동일한 크기 인 경우).</target>
        </trans-unit>
        <trans-unit id="11d179b15971b8eaae6270be9fce57c0bd0d2416" translate="yes" xml:space="preserve">
          <source>However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.</source>
          <target state="translated">그러나 사용 가능한 데이터를 세 세트로 분할함으로써 모델 학습에 사용할 수있는 샘플 수를 대폭 줄이며 결과는 (트레인, 유효성 검사) 세트의 특정 무작위 선택에 따라 달라질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="56c680421b5fb07e56baa9a65f13a80fce385b54" translate="yes" xml:space="preserve">
          <source>However, coefficient estimates for Ordinary Least Squares rely on the independence of the model terms. When terms are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed response, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="translated">그러나 정규 최소 제곱의 계수 추정치는 모형 항의 독립성에 의존합니다. 항이 서로 연관되어 있고 설계 행렬 \ (X \)의 열이 대략 선형 의존성을 가질 때, 설계 행렬은 특이에 가까워지고 결과적으로 최소 제곱 추정값은 관측 된 응답의 랜덤 오차에 매우 민감 해집니다. 큰 차이를 만들어냅니다. 이러한 &lt;em&gt;다중 공선 성&lt;/em&gt; 상황은 예를 들어 실험 설계없이 데이터를 수집 할 때 발생할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aca3ba038ade9dc36b01dc839f8a0cfb1c392a3d" translate="yes" xml:space="preserve">
          <source>However, dropping one category breaks the symmetry of the original representation and can therefore induce a bias in downstream models, for instance for penalized linear classification or regression models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf734282463bfc3a9cb343729f546342ec401691" translate="yes" xml:space="preserve">
          <source>However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.</source>
          <target state="translated">그러나 학습 곡선이 문제의 훈련 규모에 비해 가파르면 5 배 또는 10 배 교차 검증이 일반화 오류를 과대 평가할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fc162d85afa0b20b4064f40b16eb0e55ca89c629" translate="yes" xml:space="preserve">
          <source>However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.</source>
          <target state="translated">그러나 추정기가 일부 하이퍼 파라미터 값에 대해 과적 합하는지 또는 과적 합하는지 여부를 확인하기 위해 단일 하이퍼 파라미터가 훈련 스코어 및 유효성 검증 스코어에 미치는 영향을 플롯하는 것이 때때로 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="5c8b24673bb3f660f66f90035a856044be6d6f9e" translate="yes" xml:space="preserve">
          <source>However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. If categorical features are represented as numeric values such as int, the DictVectorizer can be followed by &lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; to complete binary one-hot encoding.</source>
          <target state="translated">그러나이 변환기는 기능 값이 문자열 유형 인 경우 2 진 1- 핫 인코딩 만 수행합니다. 범주 형 기능이 int와 같은 숫자 값으로 표시되는 경우 DictVectorizer 뒤에 &lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt; &lt;/a&gt; 를 사용하여 이진 one-hot 인코딩을 완료 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8b25d9ad009118aef0894664f601ac10786f8b49" translate="yes" xml:space="preserve">
          <source>However, this is not the most precise way of doing this computation, and the distance matrix returned by this function may not be exactly symmetric as required by, e.g., &lt;code&gt;scipy.spatial.distance&lt;/code&gt; functions.</source>
          <target state="translated">그러나 이것이이 계산을 수행하는 가장 정확한 방법은 &lt;code&gt;scipy.spatial.distance&lt;/code&gt; 함수가 반환하는 거리 행렬이 예를 들어 scipy.spatial.distance 함수에서 요구하는대로 정확하게 대칭이 아닐 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="379cfb166aa26713fe1131478e9b37a4224780ad" translate="yes" xml:space="preserve">
          <source>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</source>
          <target state="translated">치앙 린, 샹란 황, 샹 푸유 (2011). 이중 좌표 하강</target>
        </trans-unit>
        <trans-unit id="15d1d4b26d2ba06629e368bf6c8a860d8762ef89" translate="yes" xml:space="preserve">
          <source>Huber (&lt;code&gt;'huber'&lt;/code&gt;): Another robust loss function that combines least squares and least absolute deviation; use &lt;code&gt;alpha&lt;/code&gt; to control the sensitivity with regards to outliers (see &lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt; for more details).</source>
          <target state="translated">Huber ( &lt;code&gt;'huber'&lt;/code&gt; ) : 최소 제곱과 최소 절대 편차를 결합한 또 다른 강력한 손실 함수. 이상치에 대한 민감도를 제어 하려면 &lt;code&gt;alpha&lt;/code&gt; 를 사용 하십시오 (자세한 내용은 &lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt; 참조).</target>
        </trans-unit>
        <trans-unit id="31a935f21354ade96cddb9bceb15816934445a99" translate="yes" xml:space="preserve">
          <source>Huber (&lt;code&gt;'huber'&lt;/code&gt;): Another robust loss function that combines least squares and least absolute deviation; use &lt;code&gt;alpha&lt;/code&gt; to control the sensitivity with regards to outliers (see &lt;a href=&quot;model_evaluation#f2001&quot; id=&quot;id18&quot;&gt;[F2001]&lt;/a&gt; for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ac7569be3812003aa580f2d415abbe085c935f8" translate="yes" xml:space="preserve">
          <source>Huber: less sensitive to outliers than least-squares. It is equivalent to least squares when \(|y_i - f(x_i)| \leq \varepsilon\), and \(L(y_i, f(x_i)) = \varepsilon |y_i - f(x_i)| - \frac{1}{2} \varepsilon^2\) otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d12d436101cbc3212af50bf81000f6d78d4cf01" translate="yes" xml:space="preserve">
          <source>HuberRegressor vs Ridge on dataset with strong outliers</source>
          <target state="translated">특이 치가 강한 데이터 세트의 HuberRegressor vs Ridge</target>
        </trans-unit>
        <trans-unit id="7e58a6e8d89e8504ad31e135de9b485ad40f05f6" translate="yes" xml:space="preserve">
          <source>Hue</source>
          <target state="translated">Hue</target>
        </trans-unit>
        <trans-unit id="c7a8b2b20a9c45f674f17cd8ef7ece305e1c36eb" translate="yes" xml:space="preserve">
          <source>Hue:</source>
          <target state="translated">Hue:</target>
        </trans-unit>
        <trans-unit id="4e99bcdee413a9c98d317e0e8e4199a2bf582f90" translate="yes" xml:space="preserve">
          <source>Hugo Chavez</source>
          <target state="translated">휴고 차베스</target>
        </trans-unit>
        <trans-unit id="27d175ec2bd32b6e89237e92741eaada2632ca6b" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="135e7e12c7ab5ea7496649e72ee134478ecf558e" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter : 알파 매개 변수 이전의 감마 분포에 대한 역 척도 매개 변수 (속도 매개 변수). 기본값은 1.e-6입니다.</target>
        </trans-unit>
        <trans-unit id="b2f7b3253a19f527d0298206233b561d7b41b5a7" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="761054fe5bafcad49a16f057764235860452b8da" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6</source>
          <target state="translated">Hyper-parameter : 람다 매개 변수 이전의 감마 분포에 대한 역 척도 매개 변수 (속도 매개 변수). 기본값은 1입니다 .e-6</target>
        </trans-unit>
        <trans-unit id="6001ea6392d3003569381e7107254e88f75fd600" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter : 람다 매개 변수 이전의 감마 분포에 대한 역 척도 매개 변수 (속도 매개 변수). 기본값은 1.e-6입니다.</target>
        </trans-unit>
        <trans-unit id="dc7fba2810913663c629f4f037b88df90cdd9978" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81e171654bf22a490946ec147c219e96694497ff" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6</source>
          <target state="translated">Hyper-parameter : 알파 매개 변수보다 감마 분포에 대한 모양 매개 변수입니다. 기본값은 1입니다 .e-6</target>
        </trans-unit>
        <trans-unit id="b07af48fd68aeaacb4df041ef30bae006150c237" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter : 알파 매개 변수보다 감마 분포에 대한 모양 매개 변수입니다. 기본값은 1.e-6입니다.</target>
        </trans-unit>
        <trans-unit id="532d93a63b09b5d558170a615d6e76799550e7c3" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1398aea0b1e181e76b6d9d73db4040ccf06ee2f7" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter : 람다 매개 변수 이전의 감마 분포에 대한 모양 매개 변수. 기본값은 1.e-6입니다.</target>
        </trans-unit>
        <trans-unit id="7a5b8a439bb2492412d2944256add4dcdf337928" translate="yes" xml:space="preserve">
          <source>Hyper-parameter optimizers</source>
          <target state="translated">하이퍼 파라미터 옵티 마이저</target>
        </trans-unit>
        <trans-unit id="223bf115da53d3d9cdf837b624135b565596fd92" translate="yes" xml:space="preserve">
          <source>Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;kernel&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt; for Support Vector Classifier, &lt;code&gt;alpha&lt;/code&gt; for Lasso, etc.</source>
          <target state="translated">하이퍼 파라미터는 추정기 내에서 직접 학습되지 않는 파라미터입니다. scikit-learn에서 이들은 추정자 클래스의 생성자에 인수로 전달됩니다. 일반적인 예로는 &lt;code&gt;C&lt;/code&gt; , &lt;code&gt;kernel&lt;/code&gt; 및 지원 벡터 분류기의 &lt;code&gt;gamma&lt;/code&gt; , 올가미의 &lt;code&gt;alpha&lt;/code&gt; 등이 있습니다.</target>
        </trans-unit>
        <trans-unit id="568b05951392672a52de0358537dd29fcafbe544" translate="yes" xml:space="preserve">
          <source>Hyper-parameters of an estimator can be updated after it has been constructed via the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params()&lt;/a&gt; method. Calling &lt;code&gt;fit()&lt;/code&gt; more than once will overwrite what was learned by any previous &lt;code&gt;fit()&lt;/code&gt;:</source>
          <target state="translated">추정기의 하이퍼 파라미터는 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params ()&lt;/a&gt; 메소드 를 통해 생성 된 후에 업데이트 될 수 있습니다 . &lt;code&gt;fit()&lt;/code&gt; 두 번 이상 호출 하면 이전 &lt;code&gt;fit()&lt;/code&gt; 에서 학습 한 내용을 덮어 씁니다 .</target>
        </trans-unit>
        <trans-unit id="3320fca926b13c61acfba24014e8ac870169bd3f" translate="yes" xml:space="preserve">
          <source>Hyper-parameters of an estimator can be updated after it has been constructed via the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-set-params&quot;&gt;set_params()&lt;/a&gt; method. Calling &lt;code&gt;fit()&lt;/code&gt; more than once will overwrite what was learned by any previous &lt;code&gt;fit()&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1db8c072507305b4aa23189287be39423349b8f4" translate="yes" xml:space="preserve">
          <source>Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).</source>
          <target state="translated">역변환을 학습하는 능형 회귀의 하이퍼 파라미터 (fit_inverse_transform = True 일 때).</target>
        </trans-unit>
        <trans-unit id="a15138d06876fc00149292405bf57e4204d00bbe" translate="yes" xml:space="preserve">
          <source>Hyperparameters</source>
          <target state="translated">Hyperparameters</target>
        </trans-unit>
        <trans-unit id="181eca8daf7aaeed93f61701c7eddb643dc6b36a" translate="yes" xml:space="preserve">
          <source>Hyperparameters:</source>
          <target state="translated">Hyperparameters:</target>
        </trans-unit>
        <trans-unit id="8bb86931be2a9d0449c3eec151da751cb88591f1" translate="yes" xml:space="preserve">
          <source>I. Guyon, &amp;ldquo;Design of experiments for the NIPS 2003 variable selection benchmark&amp;rdquo;, 2003.</source>
          <target state="translated">I. Guyon,&amp;ldquo;NIPS 2003 변수 선택 벤치 마크 실험 설계&amp;rdquo;, 2003.</target>
        </trans-unit>
        <trans-unit id="074b587a2df67fa7bdaebb29bf4f1381e49051c3" translate="yes" xml:space="preserve">
          <source>I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Maci&amp;agrave;, B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, &lt;a href=&quot;https://ieeexplore.ieee.org/document/7280767&quot;&gt;Design of the 2015 ChaLearn AutoML Challenge&lt;/a&gt;, IJCNN 2015.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="483f14c4d9fef04833d7508282eb9a08a8019931" translate="yes" xml:space="preserve">
          <source>I.K. Yeo and R.A. Johnson, &amp;ldquo;A new family of power transformations to improve normality or symmetry.&amp;rdquo; Biometrika, 87(4), pp.954-959, (2000).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a238a89365b9d0ce7f5fb26e189eb03cdc08fbe5" translate="yes" xml:space="preserve">
          <source>ICA can also be used as yet another non linear decomposition that finds components with some sparsity:</source>
          <target state="translated">ICA는 또한 희소성이있는 구성 요소를 찾는 또 다른 비선형 분해로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="57933c6e2d57c3e3911db47768687ccc98d16924" translate="yes" xml:space="preserve">
          <source>IDpol</source>
          <target state="translated">IDpol</target>
        </trans-unit>
        <trans-unit id="fcc34dd193c826ae2f0c8b804c532252b4a25480" translate="yes" xml:space="preserve">
          <source>INDUS proportion of non-retail business acres per town</source>
          <target state="translated">도시 당 비 소매 비즈니스 에이커의 INDUS 비율</target>
        </trans-unit>
        <trans-unit id="44a4d7b7db7815be999da6a406f4dadd2c4327c5" translate="yes" xml:space="preserve">
          <source>Identification number of each sample, as ordered in dataset.data.</source>
          <target state="translated">dataset.data에서 주문 된 각 샘플의 식별 번호입니다.</target>
        </trans-unit>
        <trans-unit id="4b5c601d48e24d2806952d270f88677fcd7cfb39" translate="yes" xml:space="preserve">
          <source>Identifying which category an object belongs to.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02d51b4f13558cbcfc807b53522b1ffb156ad7e7" translate="yes" xml:space="preserve">
          <source>Identity: d(x, y) = 0 if and only if x == y</source>
          <target state="translated">동일성 : x == y 인 경우에만 d (x, y) = 0</target>
        </trans-unit>
        <trans-unit id="35bd2069c37f2c6a308bc5401948b247d5bcfc02" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;all&amp;rdquo;, the imputer mask will represent all features.</source>
          <target state="translated">&quot;all&quot;인 경우 imputer 마스크는 모든 기능을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="84934b5d658c0a370c458ee55fa3255bb65884a6" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo; (default), the imputer mask will be of same type as input.</source>
          <target state="translated">&quot;auto&quot;(기본값) 인 경우 imputer 마스크는 입력과 동일한 유형입니다.</target>
        </trans-unit>
        <trans-unit id="7cdc1bc49e801caf1ff212e1000d88bb13d7e93e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">&amp;ldquo;auto&amp;rdquo;인 경우 &lt;code&gt;max_features=n_features&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="164a2722286c1b34bc2df80a90c75397afce3e6b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">&quot;auto&quot;이면 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="911a50d98b398312fa01572b5d7b864da542117b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt;.</source>
          <target state="translated">&quot;auto&quot;이면 &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="dca169b413a6ec050ae0928eb38f43b00b9c08e0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;constant&amp;rdquo;, then replace missing values with fill_value. Can be used with strings or numeric data.</source>
          <target state="translated">&quot;일정한&quot;경우 누락 된 값을 fill_value로 바꾸십시오. 문자열 또는 숫자 데이터와 함께 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fed653e1ff76c14b62a8cb9c0f4474c620b2641e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;log2&amp;rdquo;, then &lt;code&gt;max_features=log2(n_features)&lt;/code&gt;.</source>
          <target state="translated">&quot;log2&quot;이면 &lt;code&gt;max_features=log2(n_features)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="e799052bdd1932be1b28378fc91f87421f6d1065" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along each column. Can only be used with numeric data.</source>
          <target state="translated">&quot;평균&quot;이면 각 열의 평균을 사용하여 결 측값을 바꾸십시오. 숫자 데이터에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8c28cbae695709f5ae6daaba6d2035fa26d4e040" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along the axis.</source>
          <target state="translated">&quot;평균&quot;인 경우 축을 따라 평균을 사용하여 결 측값을 바꾸십시오.</target>
        </trans-unit>
        <trans-unit id="d5353b7f39f25231d62cbbc36fcd604e05d2faa0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along each column. Can only be used with numeric data.</source>
          <target state="translated">&quot;중앙값&quot;인 경우 각 열의 중앙값을 사용하여 누락 된 값을 바꾸십시오. 숫자 데이터에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2c7bf0a70af62c9d1ff80c38810d3732da415b46" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along the axis.</source>
          <target state="translated">&quot;중간 값&quot;인 경우 축을 따라 중간 값을 사용하여 누락 된 값을 바꾸십시오.</target>
        </trans-unit>
        <trans-unit id="b9dfb246debbef95e2bc6e78da2b0aca54b8e768" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;missing-only&amp;rdquo; (default), the imputer mask will only represent features containing missing values during fit time.</source>
          <target state="translated">&quot;missing-only&quot;(기본값) 인 경우, imputer 마스크는 맞춤 시간 동안 결 측값이 포함 된 피처 만 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="fb9cd590a090a11e857ebbc7c5d49f19787d4a57" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along each column. Can be used with strings or numeric data.</source>
          <target state="translated">&quot;most_frequent&quot;인 경우 각 열에서 가장 빈번한 값을 사용하여 누락 된 것을 교체하십시오. 문자열 또는 숫자 데이터와 함께 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a90ab2bff0e7c4a2db0c7d70bcb17fa44e1b8cb3" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along the axis.</source>
          <target state="translated">&quot;most_frequent&quot;인 경우 축을 따라 가장 빈번한 값을 사용하여 누락 된 부분을 교체하십시오.</target>
        </trans-unit>
        <trans-unit id="8007580c79fb9470823eec0b7f7391f7011a44a8" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;prefit&amp;rdquo; is passed, it is assumed that &lt;code&gt;base_estimator&lt;/code&gt; has been fitted already and all data is used for calibration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d25972b438acba3aa495003bff5b880c3dc78f95" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;prefit&amp;rdquo; is passed, it is assumed that base_estimator has been fitted already and all data is used for calibration.</source>
          <target state="translated">&quot;prefit&quot;이 통과되면 base_estimator가 이미 장착 된 것으로 가정하고 모든 데이터를 교정에 사용합니다.</target>
        </trans-unit>
        <trans-unit id="ddc01f2adfc0b5da49bb0bea104c04055f37082b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; (same as &amp;ldquo;auto&amp;rdquo;).</source>
          <target state="translated">&quot;sqrt&quot;이면 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; ( &quot;auto&quot;와 동일)</target>
        </trans-unit>
        <trans-unit id="050de520f25e08567f8dcb7bcdaa6887bd8ca53c" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">&quot;sqrt&quot;이면 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="274dd12ab1d70a7a9d00df8bbe2aa7f35f2ca3c4" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;SAMME.R&amp;rsquo; then use the SAMME.R real boosting algorithm. &lt;code&gt;base_estimator&lt;/code&gt; must support calculation of class probabilities. If &amp;lsquo;SAMME&amp;rsquo; then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.</source>
          <target state="translated">'SAMME.R'인 경우 SAMME.R 실제 부스팅 알고리즘을 사용하십시오. &lt;code&gt;base_estimator&lt;/code&gt; 는 클래스 확률 계산을 지원해야합니다. 'SAMME'인 경우 SAMME 이산 부스팅 알고리즘을 사용하십시오. SAMME.R 알고리즘은 일반적으로 SAMME보다 빠르게 수렴되므로 부스팅 반복 횟수가 줄어 테스트 오류가 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="51b26722f92fe40c28c6f4d36bb516d8181566a9" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;auto&amp;rsquo;, early stopping is enabled if the sample size is larger than 10000. If True, early stopping is enabled, otherwise early stopping is disabled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ae3261c7ce3a365ccb1e46d21ef269f4ad371b0" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;auto&amp;rsquo;, the threshold is determined as in the original paper.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34bbe83eef64f8685433d192f4dd39b726225179" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;auto&amp;rsquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bac8214432dad215f7331c0af16dfd3868b9e19" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;balanced&amp;rsquo;, class weights will be given by &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;. If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform.</source>
          <target state="translated">'balanced'인 경우 클래스 가중치는 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 됩니다. 사전이 제공되면 키는 클래스이고 값은 해당 클래스 가중치입니다. 없음을 지정하면 클래스 가중치가 균일합니다.</target>
        </trans-unit>
        <trans-unit id="2ed37f73bd5305414a6ee47c4802aa22bc780ac0" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;diagram&amp;rsquo;, estimators will be displayed as a diagram in a Jupyter lab or notebook context. If &amp;lsquo;text&amp;rsquo;, estimators will be displayed as text. Default is &amp;lsquo;text&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="456401c87cfc2a15cdd408f2dd8be315dc3e833e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;english&amp;rsquo;, a built-in stop word list for English is used. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">'english'이면 영어 용 내장 중지 단어 목록이 사용됩니다. 'english'에는 몇 가지 알려진 문제가 있으며 대안을 고려해야합니다 ( &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;중지 단어 사용&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="f2019bdbdfa8956b7b54e8a5677954f4996f6374" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;file&amp;rsquo;, the sequence items must have a &amp;lsquo;read&amp;rsquo; method (file-like object) that is called to fetch the bytes in memory.</source>
          <target state="translated">'file'인 경우 시퀀스 항목에는 메모리에서 바이트를 페치하기 위해 호출되는 'read'메소드 (파일과 유사한 오브젝트)가 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="3dd1f3ca74314afe1ddf15184f6ab04977d1a20b" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;filename&amp;rsquo;, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.</source>
          <target state="translated">'filename'인 경우, 맞는 인수로 전달 된 시퀀스는 분석 할 원시 컨텐츠를 페치하기 위해 읽어야하는 파일 이름의 목록 일 것으로 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="6381402e5f026c95872c614d3f284a846d432d3e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;hard&amp;rsquo;, uses predicted class labels for majority rule voting. Else if &amp;lsquo;soft&amp;rsquo;, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.</source>
          <target state="translated">'하드'인 경우 다수의 규칙 투표에 예측 된 클래스 레이블을 사용합니다. 그렇지 않으면 'soft'인 경우 예측 된 확률의 합의 argmax를 기반으로 클래스 레이블을 예측하며, 이는 잘 보정 된 분류기의 앙상블에 권장됩니다.</target>
        </trans-unit>
        <trans-unit id="4a61aad0a278d4a0b23f699c1bb6bc9a25b8f483" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;log2&amp;rsquo;, then &lt;code&gt;max_features=log2(n_features)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="034a602ff18129b75a77961464f62c916fb178cb" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;precomputed&amp;rsquo;, the training input X is expected to be a distance matrix.</source>
          <target state="translated">'사전 계산 된'경우 훈련 입력 X는 거리 매트릭스 일 것으로 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="d286c5a17d9d64b09f62a6cd1cf2dc973056dbb7" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;sqrt&amp;rsquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="efe2693abb0ad6fb0bb32fc7410403c6f8a6131c" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. In that case, &amp;lsquo;n_init&amp;rsquo; is ignored and only a single initialization occurs upon the first call. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">'warm_start'가 True 인 경우 마지막 피팅 솔루션은 다음 fit () 호출에 대한 초기화로 사용됩니다. 이렇게하면 비슷한 문제에서 적합이 여러 번 호출 될 때 수렴 속도가 빨라질 수 있습니다. 이 경우 'n_init'는 무시되고 첫 번째 호출시 단일 초기화 만 발생합니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="65bcde64719b83bdeb345ca5fab269155ae4a753" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. In that case, &amp;lsquo;n_init&amp;rsquo; is ignored and only a single initialization occurs upon the first call. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="245f671779646599b33075b7dcf37bf735b34555" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">'warm_start'가 True 인 경우 마지막 피팅 솔루션은 다음 fit () 호출에 대한 초기화로 사용됩니다. 이렇게하면 비슷한 문제에서 적합이 여러 번 호출 될 때 수렴 속도가 빨라질 수 있습니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="775000d0783a0c4a0cc36e649a7e52e403237e23" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bb29953be4032aa8bb2090970ba2fb09c57dc65" translate="yes" xml:space="preserve">
          <source>If 0, no progress messages will be printed. If 1, progress messages will be printed to stdout. If &amp;gt; 1, progress messages will be printed and the &lt;code&gt;disp&lt;/code&gt; parameter of &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize&quot;&gt;&lt;code&gt;scipy.optimize.minimize&lt;/code&gt;&lt;/a&gt; will be set to &lt;code&gt;verbose - 2&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a2ad0e9541d6795a0339e5a9c03b37f24ea5bc7" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; has not been called before.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e311856e2dbc16a358c30263eb21c62e3f976c09" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt;&lt;code&gt;MinMaxScaler&lt;/code&gt;&lt;/a&gt; is given an explicit &lt;code&gt;feature_range=(min, max)&lt;/code&gt; the full formula is:</source>
          <target state="translated">경우 &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt; &lt;code&gt;MinMaxScaler&lt;/code&gt; 가&lt;/a&gt; 명시 주어진다 &lt;code&gt;feature_range=(min, max)&lt;/code&gt; 전체 공식은 :</target>
        </trans-unit>
        <trans-unit id="6f352ac5787c6e0994736f1793f840aefef2eb74" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.</source>
          <target state="translated">만일 &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; 및 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; , 요구가 설명 될 것을 분산의 양 n_components 의해 특정 비율 이상이되도록 구성 요소의 수를 선택한다.</target>
        </trans-unit>
        <trans-unit id="6154e481a1bfebf053da4021c41ed6b15075ac75" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;Gram&lt;/code&gt; is overwritten.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 인 경우 &lt;code&gt;Gram&lt;/code&gt; 을 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="c8ea59a59509714d84c6c3be2a8959e87ca2c339" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; is overwritten.</source>
          <target state="translated">경우 &lt;code&gt;False&lt;/code&gt; , &lt;code&gt;X&lt;/code&gt; 는 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="ecd953eee019b7cf39fa95c1745e9486ce5fb903" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 인 경우 올바르게 분류 된 샘플 수를 반환하십시오. 그렇지 않으면 올바르게 분류 된 샘플의 일부를 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="8dd52da2b8b6d856acbbc6b969b86fd0a4246941" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of misclassifications. Otherwise, return the fraction of misclassifications.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 인 경우 분류 오류 수를 반환하십시오. 그렇지 않으면 오 분류 부분을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="85fcfc531652a8814592a07e791b2030fbc9598e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the sum of the Jaccard similarity coefficient over the sample set. Otherwise, return the average of Jaccard similarity coefficient.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 인 경우 샘플 세트에 대한 Jaccard 유사성 계수의 합을 반환하십시오. 그렇지 않으면 Jaccard 유사성 계수의 평균을 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="c21045a17e85201e2f77134fc96d9edd698a8ef9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, the &lt;code&gt;cv_results_&lt;/code&gt; attribute will not include training scores.</source>
          <target state="translated">경우 &lt;code&gt;False&lt;/code&gt; 의 &lt;code&gt;cv_results_&lt;/code&gt; 의 속성은 교육 점수를하지 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="363d7b7bc89b4fe7d745ba13b3bf107700064544" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, the &lt;code&gt;cv_results_&lt;/code&gt; attribute will not include training scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f7a2b9af6d7b5ad533a302e51ca948f564886c6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt; the estimator&amp;rsquo;s default scorer is used.</source>
          <target state="translated">경우 &lt;code&gt;None&lt;/code&gt; 추의 기본 득점 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="ab9a136f2eebf764f09dd9b08d3d9d7a3daec6b9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt; the estimator&amp;rsquo;s score method is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8dec298ccc565c5c7f632ae827694845b31aa21" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;estimator&lt;/code&gt; is considered fitted if there exist an attribute that ends with a underscore and does not start with double underscore.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89a2c07a59e8a597581d7c4accbf8ade7624601a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;init_size= 3 * batch_size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0206caad9c301767e28c1eb37b72a3a7f7598e94" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</source>
          <target state="translated">경우 &lt;code&gt;None&lt;/code&gt; , 각 클래스에 대한 점수가 반환됩니다. 그렇지 않으면 데이터에서 수행되는 평균화 유형을 결정합니다.</target>
        </trans-unit>
        <trans-unit id="de8d1676c9bb98aea236fe73b6992134925cbda9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: Note: multiclass ROC AUC currently only handles the &amp;lsquo;macro&amp;rsquo; and &amp;lsquo;weighted&amp;rsquo; averages.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1170dbf5a5618e80add067200212cb5804a58cd" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt; the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">경우 &lt;code&gt;True&lt;/code&gt; 전체 경로에 저장됩니다 &lt;code&gt;coef_path_&lt;/code&gt; 의 속성. 큰 문제 나 많은 대상에 대한 솔루션을 계산하는 경우 &lt;code&gt;fit_path&lt;/code&gt; 를 &lt;code&gt;False&lt;/code&gt; 로 설정 하면 특히 작은 알파에서 속도가 빨라 집니다 .</target>
        </trans-unit>
        <trans-unit id="97edef35821191bb6c4443dade8924b80d003e19" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt; then features with missing values during &lt;code&gt;transform&lt;/code&gt; which did not have any missing values during &lt;code&gt;fit&lt;/code&gt; will be imputed with the initial imputation method only. Set to &lt;code&gt;True&lt;/code&gt; if you have many features with no missing values at both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt; time to save compute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e703de20e5680ee264e2b1b950a8b1ca587cd24f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, X will be copied; else, it may be overwritten.</source>
          <target state="translated">경우 &lt;code&gt;True&lt;/code&gt; , X가 복사됩니다; 그렇지 않으면 덮어 쓸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8e26b0bb501133486ba99496e311f44a49ce472b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, perform metric MDS; otherwise, perform nonmetric MDS.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 인 경우 메트릭 MDS를 수행하십시오. 그렇지 않으면 비 메트릭 MDS를 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="a2b129bca8e38a348fd53d1896c7796acded2f57" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return a sparse feature matrix</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 인 경우 희소 피쳐 행렬을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="887d26ef7077238a636d223d3985225a211c8d82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return the prior class probability and conditional probabilities of features given classes, from which the data was drawn.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 인 경우 데이터가 추출 된 클래스에 대해 지정된 클래스의 확률 및 조건부 확률을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0b7bef40bad08d9d2c4e67da6c9b56ea79751005" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, some instances might not belong to any class.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 인 경우 일부 인스턴스는 어떤 클래스에도 속하지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e223ba26a8622e808f0df02e86007844177282d6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; or &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the penalty applied to the L1 norm. If &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the absolute value of the threshold below which coefficients will be squashed to zero. If &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides &lt;code&gt;n_nonzero_coefs&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; 또는 &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; , &lt;code&gt;alpha&lt;/code&gt; 패널티가 L1 놈인가된다. 경우 &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt; , &lt;code&gt;alpha&lt;/code&gt; 계수가 제로로 압축되는보다 임계 값의 절대 값이다. 경우 &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt; , &lt;code&gt;alpha&lt;/code&gt; 허용 매개 변수 : 재건 오류의 값이 대상. 이 경우 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; 를 대체 합니다 .</target>
        </trans-unit>
        <trans-unit id="c8adb20b3b91095a2c077330ab11f60b186c2818" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;algorithm='lasso_lars'&lt;/code&gt; or &lt;code&gt;algorithm='lasso_cd'&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the penalty applied to the L1 norm. If &lt;code&gt;algorithm='threshold'&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the absolute value of the threshold below which coefficients will be squashed to zero. If &lt;code&gt;algorithm='omp'&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides &lt;code&gt;n_nonzero_coefs&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ef12938b73ad4fc11883a02f290e9f35cac58e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;axes_[i, j]&lt;/code&gt; is the axes on the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;axes_[i]&lt;/code&gt; is the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes in that position.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd2bdf4031503663b9e9168a5bd9a1faa112780b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;contours_[i, j]&lt;/code&gt; is the partial dependence plot on the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;contours_[i]&lt;/code&gt; is the partial dependence plot corresponding to the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes or an axes that does not include a contour plot.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16daa78b71c8fbb0a04353048c9be677f4bd1a67" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;lines_[i, j]&lt;/code&gt; is the partial dependence curve on the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;lines_[i]&lt;/code&gt; is the partial dependence curve corresponding to the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes or an axes that does not include a line plot.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b7c522014bc873fbd61bfb64d47b7833096d1b1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;vlines_[i, j]&lt;/code&gt; is the line collection representing the x axis deciles of the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;vlines_[i]&lt;/code&gt; corresponds to the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes or an axes that does not include a PDP plot. .. versionadded:: 0.23</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03e758847856246528ffcca83ba803cf5d3e9c24" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;vlines_[i, j]&lt;/code&gt; is the line collection representing the y axis deciles of the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;vlines_[i]&lt;/code&gt; corresponds to the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes or an axes that does not include a 2-way plot. .. versionadded:: 0.23</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e5d3c60fbe26a21b3685b7cd50d8157796cb85c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, the &lt;code&gt;bounding_ax_&lt;/code&gt; is the axes where the grid of partial dependence plots are drawn. If &lt;code&gt;ax&lt;/code&gt; is a list of axes or a numpy array of axes, &lt;code&gt;bounding_ax_&lt;/code&gt; is None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a88caf8f6b6232395c9c1524315c6ed672bcf763" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt; and X is encoded as a CSR matrix;</source>
          <target state="translated">&lt;code&gt;axis=0&lt;/code&gt; 이고 X가 CSR 매트릭스로 인코딩 된 경우 ;</target>
        </trans-unit>
        <trans-unit id="160d044408c23f7840586e9cc7d8cab0e43ba9b5" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt;, boolean and integer array-like, integer slice, and scalar integer are supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b74f02ef0c3e3aceaf2040e39764c8bf5d153fee" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt;, then impute along columns.</source>
          <target state="translated">&lt;code&gt;axis=0&lt;/code&gt; 인 경우 열을 따라 대치합니다.</target>
        </trans-unit>
        <trans-unit id="231cba4e2ed9eee1fca5c3a6b02c0c6f66c47550" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt; and X is encoded as a CSC matrix.</source>
          <target state="translated">&lt;code&gt;axis=1&lt;/code&gt; 이고 X가 CSC 행렬로 인코딩 된 경우 .</target>
        </trans-unit>
        <trans-unit id="4405a4c1e894889993d89bb6694cef1a6a8f7db3" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt;, then impute along rows.</source>
          <target state="translated">&lt;code&gt;axis=1&lt;/code&gt; 인 경우 행을 따라 대치합니다.</target>
        </trans-unit>
        <trans-unit id="afe718681bc04a6b175f3e0dbacf56b2e22d3277" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf71a4a70c1ff1b9076f02c43d89e78c4b0ffc27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;backend&lt;/code&gt; is a string it must match a previously registered implementation using the &lt;code&gt;register_parallel_backend&lt;/code&gt; function.</source>
          <target state="translated">경우 &lt;code&gt;backend&lt;/code&gt; 문자열이 그것을 사용하여 이전에 등록 된 구현과 일치해야 &lt;code&gt;register_parallel_backend&lt;/code&gt; 의 기능을.</target>
        </trans-unit>
        <trans-unit id="6456a4494f2ba1f052aff4cf6d35ef66e787bc14" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;base_estimator&lt;/code&gt; is None, then &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; is used for target values of dtype float.</source>
          <target state="translated">경우 &lt;code&gt;base_estimator&lt;/code&gt; 은 다음 없음, 없다 &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; DTYPE 플로트의 목표 값에 사용된다.</target>
        </trans-unit>
        <trans-unit id="898731158b64382ef9aad2307b39d22b5cd2a315" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dense&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the dense binary indicator format. If &lt;code&gt;'sparse'&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the sparse binary indicator format. &lt;code&gt;False&lt;/code&gt; returns a list of lists of labels.</source>
          <target state="translated">&lt;code&gt;dense&lt;/code&gt; 경우 밀도 가 높은 이진 표시기 형식으로 &lt;code&gt;Y&lt;/code&gt; 를 반환 합니다. 만약 &lt;code&gt;'sparse'&lt;/code&gt; 복귀 &lt;code&gt;Y&lt;/code&gt; 스파 스 이진 표시 형식입니다. &lt;code&gt;False&lt;/code&gt; 는 레이블 목록의 목록을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="b1213caecc623f2a5139e82ba5db339edb5f6265" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape (1,) when the given problem is binary. In particular, when &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt;, &lt;code&gt;intercept_&lt;/code&gt; corresponds to outcome 1 (True) and &lt;code&gt;-intercept_&lt;/code&gt; corresponds to outcome 0 (False).</source>
          <target state="translated">경우 &lt;code&gt;fit_intercept&lt;/code&gt; 이 False로 설정되어, 절편은 0으로 설정됩니다. 주어진 문제가 이진 인 경우 &lt;code&gt;intercept_&lt;/code&gt; 의 모양은 (1,)입니다. 특히 &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt; 경우 &lt;code&gt;intercept_&lt;/code&gt; 는 결과 1 (True)에 해당 하고 &lt;code&gt;-intercept_&lt;/code&gt; 는 결과 0 (False)에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="5adb2b902dec2303b1749b6ba9f10123fe5ab03c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape (1,) when the given problem is binary. In particular, when &lt;code&gt;multi_class='multinomial'&lt;/code&gt;, &lt;code&gt;intercept_&lt;/code&gt; corresponds to outcome 1 (True) and &lt;code&gt;-intercept_&lt;/code&gt; corresponds to outcome 0 (False).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4504cae87026fef1f6989cfa20e2e5bc171d37e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape(1,) when the problem is binary.</source>
          <target state="translated">경우 &lt;code&gt;fit_intercept&lt;/code&gt; 이 False로 설정되어, 절편은 0으로 설정됩니다. &lt;code&gt;intercept_&lt;/code&gt; 는 문제가 이진 인 경우 shape (1,)입니다.</target>
        </trans-unit>
        <trans-unit id="646836188c841e4fea39e4e4200d1f27e6191986" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;loss&lt;/code&gt; is a callable, then it should be a function that takes two arrays as inputs, the true and predicted value and returns a 1-D array with the i-th value of the array corresponding to the loss on &lt;code&gt;X[i]&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;loss&lt;/code&gt; 호출 가능한 인, 그것은 입력, 실제 및 예측 된 값과 두 배열 소요의 손실에 대응하는 어레이의 i 번째 값이 1-D 어레이를 리턴하는 함수이어야 &lt;code&gt;X[i]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c42d62680a26d66fc0f439c634f24ee01c670c77" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;memory&lt;/code&gt; is not joblib.Memory-like.</source>
          <target state="translated">&lt;code&gt;memory&lt;/code&gt; 가 작동하지 않는 경우 메모리 와 유사합니다.</target>
        </trans-unit>
        <trans-unit id="e14dd7c153267d74f6b2214ce66bcf041482d792" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_bins&lt;/code&gt; is an array, and there is an ignored feature at index &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;n_bins[i]&lt;/code&gt; will be ignored.</source>
          <target state="translated">경우 &lt;code&gt;n_bins&lt;/code&gt; 가 배열이며, 인덱스에서 무시 기능가 &lt;code&gt;i&lt;/code&gt; , &lt;code&gt;n_bins[i]&lt;/code&gt; 무시된다.</target>
        </trans-unit>
        <trans-unit id="20ab457ec31da79f104a0f7a337ba6bfe94b5438" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the data is reduced from 100,000 samples to a set of 158 clusters. This can be viewed as a preprocessing step before the final (global) clustering step that further reduces these 158 clusters to 100 clusters.</source>
          <target state="translated">경우 &lt;code&gt;n_clusters&lt;/code&gt; 가 없음으로 설정하고, 데이터는 클러스터 (158)의 세트에 10 개 샘플에서 감소된다. 이는 최종 (전역) 클러스터링 단계 이전의 전처리 단계로 볼 수 있으며이 158 개의 클러스터를 100 개의 클러스터로 추가로 줄입니다.</target>
        </trans-unit>
        <trans-unit id="1769c2fe615105013ff722090827f85ac960dff9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components == 'mle'&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, Minka&amp;rsquo;s MLE is used to guess the dimension. Use of &lt;code&gt;n_components == 'mle'&lt;/code&gt; will interpret &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; as &lt;code&gt;svd_solver == 'full'&lt;/code&gt;.</source>
          <target state="translated">만약 &lt;code&gt;n_components == 'mle'&lt;/code&gt; 와 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; , Minka의 MLE는 차원을 추측하는 데 사용됩니다. 의 사용 &lt;code&gt;n_components == 'mle'&lt;/code&gt; 해석합니다 &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; 같은 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="959758e689ea656dd0e72e3bda18b0a45bbef2e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components&lt;/code&gt; is not set then all components are stored and the sum of the ratios is equal to 1.0.</source>
          <target state="translated">경우 &lt;code&gt;n_components&lt;/code&gt; 가 설정되지 않는 모든 성분은 저장 비율의 합은 1.0와 동일한다.</target>
        </trans-unit>
        <trans-unit id="712e62a0190856257c5f6877b0d11f259bd408b5" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components&lt;/code&gt; is strictly smaller than the dimensionality of the inputs passed to &lt;a href=&quot;#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, the identity matrix will be truncated to the first &lt;code&gt;n_components&lt;/code&gt; rows.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4aaa2df3fa37c88b24d06638ef7188d7e8ebe112" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each parameter setting(and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;n_jobs&lt;/code&gt; 가 1보다 높은 값으로 설정된 경우 데이터는 각 매개 변수 설정 ( &lt;code&gt;n_jobs&lt;/code&gt; 시간이 아님) 에 대해 복사됩니다 . 이는 개별 작업에 시간이 거의 걸리지 않는 효율성을 위해 수행되지만 데이터 세트가 크고 사용 가능한 메모리가 충분하지 않으면 오류가 발생할 수 있습니다. 이 경우의 해결 방법은 &lt;code&gt;pre_dispatch&lt;/code&gt; 를 설정하는 것 입니다. 그런 다음 메모리는 &lt;code&gt;pre_dispatch&lt;/code&gt; 만 여러 번 복사됩니다 . &lt;code&gt;pre_dispatch&lt;/code&gt; 의 적절한 값 은 &lt;code&gt;2 * n_jobs&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="53682a81a25d0884d79ca09b064b0fc6e7cabd67" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each point in the grid (and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;n_jobs&lt;/code&gt; 가 하나보다 높은 값으로 설정하고, 데이터는 각 격자 점에 대한 복사 (되지 않고 &lt;code&gt;n_jobs&lt;/code&gt; 회). 이는 개별 작업에 시간이 거의 걸리지 않는 효율성을 위해 수행되지만 데이터 세트가 크고 사용 가능한 메모리가 충분하지 않으면 오류가 발생할 수 있습니다. 이 경우의 해결 방법은 &lt;code&gt;pre_dispatch&lt;/code&gt; 를 설정하는 것 입니다. 그런 다음 메모리는 &lt;code&gt;pre_dispatch&lt;/code&gt; 만 여러 번 복사됩니다 . &lt;code&gt;pre_dispatch&lt;/code&gt; 의 적절한 값 은 &lt;code&gt;2 * n_jobs&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="c1cc035dd2ff12188f95601d6fe5c4679ad6bb78" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_samples == 10000&lt;/code&gt;, storing &lt;code&gt;X&lt;/code&gt; as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = &lt;strong&gt;4GB in RAM&lt;/strong&gt; which is barely manageable on today&amp;rsquo;s computers.</source>
          <target state="translated">경우 &lt;code&gt;n_samples == 10000&lt;/code&gt; , 저장 &lt;code&gt;X&lt;/code&gt; 형 float32의 NumPy와 배열 등은 10000 X 100000 &amp;times; 4 바이트 = 필요 &lt;strong&gt;RAM 4GB를&lt;/strong&gt; 오늘날의 컴퓨터에서 거의 관리입니다.</target>
        </trans-unit>
        <trans-unit id="dd40778e7a383f2053d98b9a07e6219944c6316f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;needs_proba=False&lt;/code&gt; and &lt;code&gt;needs_threshold=False&lt;/code&gt;, the score function is supposed to accept the output of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;. If &lt;code&gt;needs_proba=True&lt;/code&gt;, the score function is supposed to accept the output of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt; (For binary &lt;code&gt;y_true&lt;/code&gt;, the score function is supposed to accept probability of the positive class). If &lt;code&gt;needs_threshold=True&lt;/code&gt;, the score function is supposed to accept the output of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e65ba558868cb56b0c8a76632ea67901b254afe" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the average Jaccard similarity coefficient, else it returns the sum of the Jaccard similarity coefficient over the sample set.</source>
          <target state="translated">경우 &lt;code&gt;normalize == True&lt;/code&gt; , 그렇지 않으면 샘플 세트를 통해 인 Jaccard 유사성 계수의 합계를 반환 평균 인 Jaccard 유사성 계수를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="c606413521700e073d3e669024415faa2300a113" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of correctly classified samples (float), else returns the number of correctly classified samples (int).</source>
          <target state="translated">&lt;code&gt;normalize == True&lt;/code&gt; 인 경우 올바르게 분류 된 샘플의 소수 (float)를 반환하고, 그렇지 않으면 올바르게 분류 된 샘플의 수 (int)를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="cd9dc36a4d7167817c2823908b4ce633913b9765" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int).</source>
          <target state="translated">&lt;code&gt;normalize == True&lt;/code&gt; 인 경우 오 분류의 소수 (float)를 반환하고 그렇지 않으면 오 분류 수 (int)를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="4260d2abf4eeb10994306d99a6424e7eb5ca19c8" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;order='random'&lt;/code&gt;, determines random number generation for the chain order. In addition, it controls the random seed given at each &lt;code&gt;base_estimator&lt;/code&gt; at each chaining iteration. Thus, it is only used when &lt;code&gt;base_estimator&lt;/code&gt; exposes a &lt;code&gt;random_state&lt;/code&gt;. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0b860961bc5b4a4c45189c0fd4de5c2d61f1ab4" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;out=None&lt;/code&gt;, returns a new array containing the mean values, otherwise a reference to the output array is returned.</source>
          <target state="translated">&lt;code&gt;out=None&lt;/code&gt; 인 경우 평균값이 포함 된 새 배열을 반환하고, 그렇지 않으면 출력 배열에 대한 참조가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="a1e72f87e91fc5bb6f8882ece59a46bf9ee089e2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;pos_label is None&lt;/code&gt; and in binary classification, this function returns the average precision, recall and F-measure if &lt;code&gt;average&lt;/code&gt; is one of &lt;code&gt;'micro'&lt;/code&gt;, &lt;code&gt;'macro'&lt;/code&gt;, &lt;code&gt;'weighted'&lt;/code&gt; or &lt;code&gt;'samples'&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;pos_label is None&lt;/code&gt; 이진 분류하는 경우,이 함수는 평균 정밀도, 소환 및 F 측정 값을 반환 &lt;code&gt;average&lt;/code&gt; 의 하나 인 &lt;code&gt;'micro'&lt;/code&gt; , &lt;code&gt;'macro'&lt;/code&gt; , &lt;code&gt;'weighted'&lt;/code&gt; 또는 &lt;code&gt;'samples'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="933b8be58a37dcefc6ca9fd3f4735aba59bace4c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;probability=True&lt;/code&gt;, it corresponds to the parameters learned in Platt scaling to produce probability estimates from decision values. If &lt;code&gt;probability=False&lt;/code&gt;, it&amp;rsquo;s an empty array. Platt scaling uses the logistic function &lt;code&gt;1 / (1 + exp(decision_value * probA_ + probB_))&lt;/code&gt; where &lt;code&gt;probA_&lt;/code&gt; and &lt;code&gt;probB_&lt;/code&gt; are learned from the dataset &lt;a href=&quot;#r20c70293ef72-2&quot; id=&quot;id1&quot;&gt;[2]&lt;/a&gt;. For more information on the multiclass case and training procedure see section 8 of &lt;a href=&quot;#r20c70293ef72-1&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afcc3cf20f075d3a281dbe1f0f610f65f9ef38a7" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;probability=True&lt;/code&gt;, it corresponds to the parameters learned in Platt scaling to produce probability estimates from decision values. If &lt;code&gt;probability=False&lt;/code&gt;, it&amp;rsquo;s an empty array. Platt scaling uses the logistic function &lt;code&gt;1 / (1 + exp(decision_value * probA_ + probB_))&lt;/code&gt; where &lt;code&gt;probA_&lt;/code&gt; and &lt;code&gt;probB_&lt;/code&gt; are learned from the dataset &lt;a href=&quot;#r9709ce4a60d3-2&quot; id=&quot;id1&quot;&gt;[2]&lt;/a&gt;. For more information on the multiclass case and training procedure see section 8 of &lt;a href=&quot;#r9709ce4a60d3-1&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bb4c6eca31ede3ca3e8fe5a9b41ecd9a55b266b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;return_path==True&lt;/code&gt; returns the entire path, else returns only the last point of the path.</source>
          <target state="translated">만약 &lt;code&gt;return_path==True&lt;/code&gt; 경로의 반환 전체 경로, 다른 반환 마지막 지점.</target>
        </trans-unit>
        <trans-unit id="9e477bb8072307702a812f869b8166fe31bf9ca0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;smooth_idf=True&lt;/code&gt; (the default), the constant &amp;ldquo;1&amp;rdquo; is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.</source>
          <target state="translated">경우 &lt;code&gt;smooth_idf=True&lt;/code&gt; (d, t IDF (디폴트), 상수 &quot;1&quot;분자와 방위군의 분모에 추가되는 여분의 문서는 제로 분열을 방지하기 회만 컬렉션에있는 모든 단어를 포함하는 본 것처럼 ) = 로그 [(1 + n) / (1 + df (d, t))] + 1.</target>
        </trans-unit>
        <trans-unit id="7e7ad1036fa8cee418b26fc730608087f9208f2c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;smooth_idf=True&lt;/code&gt; (the default), the constant &amp;ldquo;1&amp;rdquo; is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f772487671a5560a2a2bb3464b54cf0bad5b348" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;svd_solver == 'arpack'&lt;/code&gt;, the number of components must be strictly less than the minimum of n_features and n_samples.</source>
          <target state="translated">경우 &lt;code&gt;svd_solver == 'arpack'&lt;/code&gt; , 구성 요소의 수는 n_features와 N_SAMPLES의 최소보다 엄격 이하 여야합니다.</target>
        </trans-unit>
        <trans-unit id="abdb8ed2b871d03510343cf9ee77736674d298ab" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;validate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; will be checked.</source>
          <target state="translated">경우 &lt;code&gt;validate&lt;/code&gt; 있다 &lt;code&gt;True&lt;/code&gt; , &lt;code&gt;X&lt;/code&gt; 가 확인됩니다.</target>
        </trans-unit>
        <trans-unit id="5cfb594032bd50fcef2738a25df520e95867f411" translate="yes" xml:space="preserve">
          <source>If C is a ground truth class assignment and K the clustering, let us define \(a\) and \(b\) as:</source>
          <target state="translated">C가 기본 진리 클래스 지정이고 K가 클러스터링이면 \ (a \) 및 \ (b \)를 다음과 같이 정의하십시오.</target>
        </trans-unit>
        <trans-unit id="40e72ab25b1921db07187a1c526cc9080a10eaea" translate="yes" xml:space="preserve">
          <source>If False, X will be overwritten. &lt;code&gt;copy=False&lt;/code&gt; can be used to save memory but is unsafe for general use.</source>
          <target state="translated">False이면 X를 덮어 씁니다. &lt;code&gt;copy=False&lt;/code&gt; 를 사용하여 메모리를 절약 할 수 있지만 일반적인 용도로는 안전하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="b5379fd8e8700833a560e6ab84ea58c40e10b6a8" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.</source>
          <target state="translated">False, fit으로 전달 된 데이터를 겹쳐 쓰고 fit (X) .transform (X)을 실행하면 예상 결과가 나오지 않고 대신 fit_transform (X)를 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="3d545281a4ef31d03ffb244079b728a3c5cc8b18" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten. Defaults to True.</source>
          <target state="translated">False이면 적합하게 전달 된 데이터를 덮어 씁니다. 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="4bf616e8d9d604d2525c59d889782525e410270c" translate="yes" xml:space="preserve">
          <source>If False, distances will not be returned</source>
          <target state="translated">False이면 거리가 반환되지 않습니다</target>
        </trans-unit>
        <trans-unit id="b4145c6e6cc098818614b51aca0cae30eca8ed94" translate="yes" xml:space="preserve">
          <source>If False, distances will not be returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8cdf8e9cb326e6212f67c80aca3a4f04326fc4c" translate="yes" xml:space="preserve">
          <source>If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">False 인 경우 소스 사이트에서 데이터를 다운로드하지 않고 로컬로 데이터를 사용할 수없는 경우 IOError를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="707f36c34b2f81eacbaf143a8e62cb9371b1332e" translate="yes" xml:space="preserve">
          <source>If False, raise an IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">False 인 경우 소스 사이트에서 데이터를 다운로드하지 않고 로컬로 데이터를 사용할 수없는 경우 IOError를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="d32001af2bcb0806daa431ab8cf432f700c0bb79" translate="yes" xml:space="preserve">
          <source>If False, the imputer mask will be a numpy array.</source>
          <target state="translated">False이면 imputer 마스크는 numpy 배열입니다.</target>
        </trans-unit>
        <trans-unit id="2823ebb07c9bdb5cae0bfca227f5db48d585ba5a" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and dictionary will not be checked.</source>
          <target state="translated">False이면 입력 배열 X와 사전이 확인되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="bd8e933f9aa74b9b27da566d4ffb96f4e62218cf" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and y will not be checked.</source>
          <target state="translated">False이면 입력 배열 X와 y는 확인되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="85aa52dd8c7d5d29b6bebfd616a7ca3fe91cde14" translate="yes" xml:space="preserve">
          <source>If False, the projected data uses a sparse representation if the input is sparse.</source>
          <target state="translated">False이면 입력이 희소 인 경우 투영 된 데이터가 희소 표현을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="7bfec8f3204bdf713e2d3557ec53ea6f3960ad24" translate="yes" xml:space="preserve">
          <source>If False, there is no input validation.</source>
          <target state="translated">False이면 입력 유효성 검사가 없습니다.</target>
        </trans-unit>
        <trans-unit id="a67320198a0b746d35fcc941198f1221ee73c87b" translate="yes" xml:space="preserve">
          <source>If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.</source>
          <target state="translated">False이면 복사를 피하고 대신 스케일을 조정하십시오. 항상 제대로 작동한다고 보장되는 것은 아닙니다. 예를 들어 데이터가 NumPy 배열 또는 scipy.sparse CSR 행렬이 아닌 경우에도 사본이 반환 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fcd08eda0bca1685e28de89ae046095006b92653" translate="yes" xml:space="preserve">
          <source>If None (default), load all the categories. If not None, list of category names to load (other categories ignored).</source>
          <target state="translated">None (기본값)이면 모든 범주를로드하십시오. None이 아닌 경우,로드 할 카테고리 이름 목록 (다른 카테고리는 무시 됨).</target>
        </trans-unit>
        <trans-unit id="7c0ab0b638c1cad0f1492e60796eecd4f321a731" translate="yes" xml:space="preserve">
          <source>If None (default), then draw &lt;code&gt;X.shape[0]&lt;/code&gt; samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7cbd99fe721a3cd173045eddaf688b83e7620c1" translate="yes" xml:space="preserve">
          <source>If None the estimator&amp;rsquo;s default scorer, if available, is used.</source>
          <target state="translated">None 인 경우, 추정기의 기본 채점자가 사용 가능한 경우 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8c0f950a52950ceff35d04e0ab42f83a05b3adb9" translate="yes" xml:space="preserve">
          <source>If None the estimator&amp;rsquo;s score method is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96ad58db5ee1b903109c173fcab72b0955bb0408" translate="yes" xml:space="preserve">
          <source>If None, defaults to 1.0 / n_features</source>
          <target state="translated">None 인 경우 기본값은 1.0 / n_features입니다.</target>
        </trans-unit>
        <trans-unit id="e5ce9a9046a52014390758ba790166ae01779c1f" translate="yes" xml:space="preserve">
          <source>If None, do not try to decode the content of the files (e.g. for images or other non-text content). If not None, encoding to use to decode text files to Unicode if load_content is True.</source>
          <target state="translated">없음 인 경우 파일의 내용을 해독하지 마십시오 (예 : 이미지 또는 텍스트가 아닌 다른 내용의 경우). None이 아니면 load_content가 True 인 경우 텍스트 파일을 유니 코드로 디코딩하는 데 사용할 인코딩입니다.</target>
        </trans-unit>
        <trans-unit id="3e5e8d666168a7a15a80edb16364256ae0a379e4" translate="yes" xml:space="preserve">
          <source>If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.</source>
          <target state="translated">None이면 중지 단어가 사용되지 않습니다. max_df는 [0.7, 1.0) 범위의 값으로 설정하여 코퍼스 내 문서 용어 빈도를 기반으로 정지 단어를 자동으로 감지하고 필터링 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="02542a43a2f09f5328657402d69f49ce442cb6c2" translate="yes" xml:space="preserve">
          <source>If None, pairwise_distances_chunked returns a generator of vertical chunks of the distance matrix.</source>
          <target state="translated">None이면 pairwise_distances_chunked는 거리 행렬의 수직 청크 생성기를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="eb5d73cb83520641b0e2c815c109159135d569cc" translate="yes" xml:space="preserve">
          <source>If None, the estimator&amp;rsquo;s default scorer (if available) is used.</source>
          <target state="translated">None 인 경우 추정기의 기본 채점자 (사용 가능한 경우)가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="43ea051b06405f0316e7696e80296e11d93edbf6" translate="yes" xml:space="preserve">
          <source>If None, the estimator&amp;rsquo;s score method is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="651c3653c15c90a6722a00465ba57c19c30adb9b" translate="yes" xml:space="preserve">
          <source>If None, the threshold is assumed to be half way between neg_label and pos_label.</source>
          <target state="translated">없음 인 경우 임계 값은 neg_label과 pos_label 사이의 절반으로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="ac652d29bc285e4e46f5aaf2fe5415c63aee1f09" translate="yes" xml:space="preserve">
          <source>If None, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">None이면 &lt;code&gt;max_features=n_features&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="2f9e5ee96434f57529c2481b71d631d9dd0cb5e7" translate="yes" xml:space="preserve">
          <source>If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.</source>
          <target state="translated">True (기본값) 인 경우 제곱 오류 규범은 n_features로 나뉩니다. False이면 제곱 오류 규범의 크기가 조정되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="da82574bb396bf8045c493d20398be74e4e9ef51" translate="yes" xml:space="preserve">
          <source>If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).</source>
          <target state="translated">True (기본값) 인 경우 모든 다항식 거듭 제곱이 0 인 특징 (즉, 1의 열-선형 모델에서 절편 항의 역할을 함) 인 바이어스 열을 포함합니다.</target>
        </trans-unit>
        <trans-unit id="d150b2a4c21e929dfd726f6463d03ca9f005e91a" translate="yes" xml:space="preserve">
          <source>If True (default), transform will raise an error when there are features with missing values in transform that have no missing values in fit This is applicable only when &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt;.</source>
          <target state="translated">True (기본값) 인 경우 누락 된 값이없는 변환에 누락 된 값이있는 피처가있는 경우 변환에서 오류가 발생 &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt; 경우에만 적용 가능합니다 .</target>
        </trans-unit>
        <trans-unit id="9da3cf1a0e0153fc6c646a1aa71f68e34d8d27f5" translate="yes" xml:space="preserve">
          <source>If True (default), transform will raise an error when there are features with missing values in transform that have no missing values in fit. This is applicable only when &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4fe165b3f600e13152308080cc2a736269c867b" translate="yes" xml:space="preserve">
          <source>If True and &lt;a href=&quot;#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; has been called before, the solution of the previous call to &lt;a href=&quot;#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is used as the initial linear transformation (&lt;code&gt;n_components&lt;/code&gt; and &lt;code&gt;init&lt;/code&gt; will be ignored).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d91ad850130f94be79fb668041bf3eecd01a29b0" translate="yes" xml:space="preserve">
          <source>If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to &amp;lsquo;sag&amp;rsquo;. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.</source>
          <target state="translated">True이고 X가 희소 인 경우이 방법은 절편도 반환하며 솔버는 자동으로 'sag'로 변경됩니다. 이것은 희소 데이터로 절편을 맞추기위한 임시 수정일뿐입니다. 밀도가 높은 데이터의 경우 회귀 전에 sklearn.linear_model._preprocess_data를 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="d1eef608e634bc22aa5f2e22e802ae927e355666" translate="yes" xml:space="preserve">
          <source>If True returns MSE value, if False returns RMSE value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a68108e0ea5bf983075f127e077ee80517d6dfdd" translate="yes" xml:space="preserve">
          <source>If True the covariance matrices are computed and stored in the &lt;code&gt;self.covariance_&lt;/code&gt; attribute.</source>
          <target state="translated">True이면 공분산 행렬이 계산되어 &lt;code&gt;self.covariance_&lt;/code&gt; 속성에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="5c5d5facc126a265032a533a4664c8926339ade0" translate="yes" xml:space="preserve">
          <source>If True the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">True이면 전체 경로가 &lt;code&gt;coef_path_&lt;/code&gt; 속성에 저장됩니다 . 큰 문제 나 많은 대상에 대한 솔루션을 계산하는 경우 &lt;code&gt;fit_path&lt;/code&gt; 를 &lt;code&gt;False&lt;/code&gt; 로 설정 하면 특히 작은 알파에서 속도가 빨라 집니다 .</target>
        </trans-unit>
        <trans-unit id="53e960778922c6ba257a9c66f18d0e260655f043" translate="yes" xml:space="preserve">
          <source>If True the function returns the pairwise distance matrix else it returns the componentwise L1 pairwise-distances. Not supported for sparse matrix inputs.</source>
          <target state="translated">True 인 경우 함수는 pairwise distance matrix를 반환하고 그렇지 않으면 componentwise L1 pairwise-distances를 반환합니다. 희소 행렬 입력에는 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2546c89362b151bbba35dab463b810d1f7c0a359" translate="yes" xml:space="preserve">
          <source>If True the order of the dataset is shuffled to avoid having images of the same person grouped.</source>
          <target state="translated">True 인 경우 동일한 사람의 이미지가 그룹화되지 않도록 데이터 세트의 순서가 섞입니다.</target>
        </trans-unit>
        <trans-unit id="618a67ac95fc4ccc3385ae319143bf344e1ffb63" translate="yes" xml:space="preserve">
          <source>If True then raise a warning if conversion is required.</source>
          <target state="translated">True이면 변환이 필요한 경우 경고를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="19362eed638b2dc6d204e12092075aedd87e6e93" translate="yes" xml:space="preserve">
          <source>If True then raise an exception if array is not symmetric.</source>
          <target state="translated">True이면 배열이 대칭이 아닌 경우 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="baf82faf959595f525e5d5a94c6b8526ad942779" translate="yes" xml:space="preserve">
          <source>If True, X will be copied; else, it may be overwritten.</source>
          <target state="translated">True이면 X가 복사됩니다. 그렇지 않으면 덮어 쓸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e25c58f63a5736150718d46a3a4c05d31a44c0e6" translate="yes" xml:space="preserve">
          <source>If True, a &lt;a href=&quot;sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transform will stack onto output of the imputer&amp;rsquo;s transform. This allows a predictive estimator to account for missingness despite imputation. If a feature has no missing values at fit/train time, the feature won&amp;rsquo;t appear on the missing indicator even if there are missing values at transform/test time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df1778fbc0c3701b8b17966bdd64201d1f2550fe" translate="yes" xml:space="preserve">
          <source>If True, a &lt;a href=&quot;sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transform will stack onto the output of the imputer&amp;rsquo;s transform. This allows a predictive estimator to account for missingness despite imputation. If a feature has no missing values at fit/train time, the feature won&amp;rsquo;t appear on the missing indicator even if there are missing values at transform/test time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa91f074d713faca021e35ddd5331805b9848f9e" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, a copy may still be returned if X&amp;rsquo;s dtype is not a floating point type.</source>
          <target state="translated">True이면 X의 사본이 생성됩니다. False 인 경우 X의 dtype이 부동 소수점 유형이 아닌 경우에도 사본이 리턴 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="45f6e4d313f4bf3abc200106ce518a942e07ab23" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, imputation will be done in-place whenever possible.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="054b374d036034be5d7258a6a975038eb8fec935" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if &lt;code&gt;copy=False&lt;/code&gt;:</source>
          <target state="translated">True이면 X의 사본이 생성됩니다. False이면 대치가 가능할 때마다 제자리에서 수행됩니다. 다음과 같은 경우 &lt;code&gt;copy=False&lt;/code&gt; 인 경우에도 항상 새 사본이 작성됩니다 .</target>
        </trans-unit>
        <trans-unit id="f237ade75e04520befb53fc36267d58be41dc5ae" translate="yes" xml:space="preserve">
          <source>If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.</source>
          <target state="translated">True 인 경우 교육 데이터의 영구 사본이 객체에 저장됩니다. 그렇지 않으면 훈련 데이터에 대한 참조 만 저장되므로 데이터를 외부에서 수정하면 예측이 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5b4ca3bdeb594ab34289df8cfc7fa70c9919ad95" translate="yes" xml:space="preserve">
          <source>If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.</source>
          <target state="translated">True 인 경우 0이 아닌 모든 카운트가 1로 설정됩니다. 이는 정수 카운트가 아닌 이진 이벤트를 모델링하는 이산 확률 모델에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="95f1a98443a6aa5501cfb81c289c266bb7baf4d6" translate="yes" xml:space="preserve">
          <source>If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0beec2b2d6b910456e155063f83ec1e59c6c0df1" translate="yes" xml:space="preserve">
          <source>If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs.)</source>
          <target state="translated">True 인 경우 0이 아닌 모든 항의 개수는 1로 설정됩니다. 이는 출력에 0/1 값만있는 것은 아니며 tf-idf의 tf 항은 이진수라는 의미입니다. 0/1 출력을 얻으려면 idf 및 normalization을 False로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="b69242de00e60526a79082b37846a2a724d7b3dd" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling.</source>
          <target state="translated">True이면 스케일링하기 전에 데이터를 중앙에 배치하십시오.</target>
        </trans-unit>
        <trans-unit id="6c9a4d2da449884c97015be12ce056a53dc04757" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">True이면 스케일링하기 전에 데이터를 중앙에 배치하십시오. 희소 행렬을 시도 할 때 작동하지 않으며 예외가 발생합니다. 중복 행렬을 중앙에 배치하면 일반적인 사용 사례에서 메모리에 비해 너무 큰 밀도가 높은 행렬을 작성해야하기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="9d7135e468914be214009091b1fc11f6721afd0a" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This will cause &lt;code&gt;transform&lt;/code&gt; to raise an exception when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">True 인 경우 스케일링하기 전에 데이터를 중앙에 배치하십시오. 희소 행렬을 시도 할 때 &lt;code&gt;transform&lt;/code&gt; 으로 인해 예외가 발생합니다. 중심 행렬은 일반적인 사용 사례에서 너무 커서 메모리에 맞지 않는 밀도가 높은 행렬을 작성하기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="694a00b6963c573c0a68b328feb15483853b44c0" translate="yes" xml:space="preserve">
          <source>If True, compute the log marginal likelihood at each iteration of the optimization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c24f1c3d35266b322135ac3270540ea5ef40a09d" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20839df17b3bca6b55533337f17b7c17f0881d1a" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False</source>
          <target state="translated">True 인 경우 모델의 각 단계에서 목적 함수를 계산하십시오. 기본값은 거짓입니다</target>
        </trans-unit>
        <trans-unit id="afb80999f635ad0619301e31bb4cd6b353188af0" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False.</source>
          <target state="translated">True 인 경우 모델의 각 단계에서 목적 함수를 계산하십시오. 기본값은 거짓입니다.</target>
        </trans-unit>
        <trans-unit id="9df36ef1b24eb49a93a9d932c395b3324554689c" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">True 인 경우 데이터가 계산 전에 중심에 있지 않습니다. 평균이 0과 같지만 정확히 0이 아닌 데이터를 처리하는 데 유용합니다. False이면 데이터가 계산 전에 중심에 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="cdd266c276f30f1ed8fec5e418bed1790d829f9f" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation.</source>
          <target state="translated">True 인 경우 데이터가 계산 전에 중심에 있지 않습니다. 평균이 거의 같지만 정확히 0이 아닌 데이터로 작업 할 때 유용합니다. False (기본값)이면 데이터가 계산 전에 중앙에 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="b4e1adfc1374b7a62eee31a2ac4be08eea958149" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">True 인 경우 데이터가 계산 전에 중심에 있지 않습니다. 평균이 거의 같지만 정확히 0이 아닌 데이터로 작업 할 때 유용합니다. False이면 데이터가 계산 전에 중심에 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="4b7584f328528f9b2a426f648be6c8534ecc99c4" translate="yes" xml:space="preserve">
          <source>If True, data will not be centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data will be centered before computation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="899541250010cf3e75fa34e54556cce0c0296945" translate="yes" xml:space="preserve">
          <source>If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data will be centered before computation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d925d680717ba978e6037a9fa61e005a509ad73c" translate="yes" xml:space="preserve">
          <source>If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data will be centered before computation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2902b07926590508dee697d1e97c541f35cd531" translate="yes" xml:space="preserve">
          <source>If True, ensure that the output of the random projection is a dense numpy array even if the input and random projection matrix are both sparse. In practice, if the number of components is small the number of zero components in the projected data will be very small and it will be more CPU and memory efficient to use a dense representation.</source>
          <target state="translated">True 인 경우 입력 및 랜덤 프로젝션 매트릭스가 모두 희소 한 경우에도 랜덤 프로젝션의 출력이 밀도가 높은 numpy 배열인지 확인하십시오. 실제로, 구성 요소의 수가 적 으면 프로젝션 된 데이터의 제로 구성 요소의 수가 매우 적고 밀도가 높은 표현을 사용하는 것이 CPU 및 메모리 효율성이 더 높습니다.</target>
        </trans-unit>
        <trans-unit id="41679ec27e720c4445a3d18e34d01eeadf44de13" translate="yes" xml:space="preserve">
          <source>If True, explicitely compute the weighted within-class covariance matrix when solver is &amp;lsquo;svd&amp;rsquo;. The matrix is always computed and stored for the other solvers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5a6d396b2b3b1ae647f1eed557078cef4ea31ec" translate="yes" xml:space="preserve">
          <source>If True, for binary &lt;code&gt;y_true&lt;/code&gt;, the score function is supposed to accept a 1D &lt;code&gt;y_pred&lt;/code&gt; (i.e., probability of the positive class or the decision function, shape &lt;code&gt;(n_samples,)&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92550063b64465fcb2c6a46cd5346c9970dc1bb3" translate="yes" xml:space="preserve">
          <source>If True, for binary &lt;code&gt;y_true&lt;/code&gt;, the score function is supposed to accept a 1D &lt;code&gt;y_pred&lt;/code&gt; (i.e., probability of the positive class, shape &lt;code&gt;(n_samples,)&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3946c2202800dbed0f15da39a81b563f2054e41" translate="yes" xml:space="preserve">
          <source>If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed.</source>
          <target state="translated">True 인 경우 개별 트리는 대체로 샘플링 된 훈련 데이터의 임의의 하위 집합에 적합합니다. False이면 교체하지 않은 샘플링이 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="47a8f3ebe1bf3e97122b0404e375e9c09f1cef3f" translate="yes" xml:space="preserve">
          <source>If True, input X is copied and stored by the model in the &lt;code&gt;X_fit_&lt;/code&gt; attribute. If no further changes will be done to X, setting &lt;code&gt;copy_X=False&lt;/code&gt; saves memory by storing a reference.</source>
          <target state="translated">True 인 경우 입력 X는 모델에 의해 &lt;code&gt;X_fit_&lt;/code&gt; 속성으로 복사 및 저장됩니다 . X를 더 이상 변경하지 않으면 &lt;code&gt;copy_X=False&lt;/code&gt; 로 설정 하면 참조를 저장하여 메모리가 절약됩니다.</target>
        </trans-unit>
        <trans-unit id="396eb3a1b9a656b43a14e28fce4ff1f92f4bae42" translate="yes" xml:space="preserve">
          <source>If True, normalizes each document&amp;rsquo;s feature vector to unit norm using &lt;a href=&quot;sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;sklearn.preprocessing.normalize&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f3a3bba5e0b6c545bdd5d5451a2769f0ab6a17d" translate="yes" xml:space="preserve">
          <source>If True, only the parameters that were set to non-default values will be printed when printing an estimator. For example, &lt;code&gt;print(SVC())&lt;/code&gt; while True will only print &amp;lsquo;SVC()&amp;rsquo; while the default behaviour would be to print &amp;lsquo;SVC(C=1.0, cache_size=200, &amp;hellip;)&amp;rsquo; with all the non-changed parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e2f5f725dbeca38c6f078963224eb5885ad4abb" translate="yes" xml:space="preserve">
          <source>If True, only the parameters that were set to non-default values will be printed when printing an estimator. For example, &lt;code&gt;print(SVC())&lt;/code&gt; while True will only print &amp;lsquo;SVC()&amp;rsquo;, but would print &amp;lsquo;SVC(C=1.0, cache_size=200, &amp;hellip;)&amp;rsquo; with all the non-changed parameters when False. Default is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10ab7d9c4ef9751f0861c3cfbcd363da08ee1196" translate="yes" xml:space="preserve">
          <source>If True, return a sparse CSR continency matrix. If &lt;code&gt;eps is not None&lt;/code&gt;, and &lt;code&gt;sparse is True&lt;/code&gt;, will throw ValueError.</source>
          <target state="translated">True이면 희소 CSR 연속성 행렬을 반환합니다. 경우 &lt;code&gt;eps is not None&lt;/code&gt; , 및 &lt;code&gt;sparse is True&lt;/code&gt; , ValueError를 발생합니다.</target>
        </trans-unit>
        <trans-unit id="bbadcb21277fb2fd7500e5e018984adc0515f847" translate="yes" xml:space="preserve">
          <source>If True, return output as dict</source>
          <target state="translated">True이면 출력을 dict로 반환</target>
        </trans-unit>
        <trans-unit id="5d0bfe964a0a56bae92b114342901e57452f609f" translate="yes" xml:space="preserve">
          <source>If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c0f661b8fac7f22363a5a1e55327c6967bb56d8" translate="yes" xml:space="preserve">
          <source>If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. If False, return the average score across folds. Default is True, but will change to False in version 0.21, to correspond to the standard definition of cross-validation.</source>
          <target state="translated">True 인 경우 각 테스트 세트의 샘플 수에 따라 가중치를 부여한 접기 수에 대한 평균 점수를 반환합니다. 이 경우 데이터는 폴드에 걸쳐 동일하게 분포 된 것으로 가정되며 최소화 된 손실은 폴드 전체의 평균 손실이 아니라 샘플 당 총 손실입니다. False이면 폴드에 대한 평균 점수를 반환합니다. 기본값은 True이지만 버전 0.21에서 표준 교차 정의의 정의에 따라 False로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="cd7031ac688b02e25258c5831c7d3ea164486db6" translate="yes" xml:space="preserve">
          <source>If True, return the distance between the clusters.</source>
          <target state="translated">참이면 군집 사이의 거리를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="a6e36ec8e19c4760ce9d4d884ef8627513b82b97" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a &lt;code&gt;Bunch&lt;/code&gt; object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48eeb388f7c432fd1b00c136703cc691939b77aa" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; object.</source>
          <target state="translated">True 인 경우 Bunch 객체 대신 &lt;code&gt;(data, target)&lt;/code&gt; 반환 합니다. &lt;code&gt;data&lt;/code&gt; 및 &lt;code&gt;target&lt;/code&gt; 객체 에 대한 자세한 내용은 아래를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="b8ef976b3bf0a8c5fe0d328bf69ca77595314915" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; objects.</source>
          <target state="translated">True 인 경우 Bunch 객체 대신 &lt;code&gt;(data, target)&lt;/code&gt; 반환 합니다. &lt;code&gt;data&lt;/code&gt; 및 &lt;code&gt;target&lt;/code&gt; 객체 에 대한 자세한 내용은 아래를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="d467c22a2bd71ab649cbad26364f06a31ce58ac1" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data.data, data.target)&lt;/code&gt; instead of a Bunch object.</source>
          <target state="translated">True 인 경우 Bunch 객체 대신 &lt;code&gt;(data.data, data.target)&lt;/code&gt; 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="248b6d0d481e47f352d5f3203242e6800072c658" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;dataset.data&lt;/code&gt; and &lt;code&gt;dataset.target&lt;/code&gt; object.</source>
          <target state="translated">True이면 Bunch 객체 대신 &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; 반환 합니다. &lt;code&gt;dataset.data&lt;/code&gt; 및 &lt;code&gt;dataset.target&lt;/code&gt; 객체 에 대한 자세한 내용은 아래를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="c220af25fde2fda1e9985d78b63166a331905d63" translate="yes" xml:space="preserve">
          <source>If True, scale the data to interquartile range.</source>
          <target state="translated">True 인 경우 데이터를 사 분위수 범위로 조정하십시오.</target>
        </trans-unit>
        <trans-unit id="0f395f8257fe0bdfb8a823c88f3be9843583f8a6" translate="yes" xml:space="preserve">
          <source>If True, scale the data to unit variance (or equivalently, unit standard deviation).</source>
          <target state="translated">True 인 경우 데이터를 단위 분산 (또는 동등하게 단위 표준 편차)으로 조정하십시오.</target>
        </trans-unit>
        <trans-unit id="df95a4486aac1e164d23cfb2a72a9a3a02411ccb" translate="yes" xml:space="preserve">
          <source>If True, the class covariance matrices are explicitely computed and stored in the &lt;code&gt;self.covariance_&lt;/code&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a491ca8d8797fa01293c195b8787d725c30e073a" translate="yes" xml:space="preserve">
          <source>If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope.</source>
          <target state="translated">True이면 클러스터가 하이퍼 큐브의 정점에 배치됩니다. False이면 클러스터는 임의 폴리 토프의 정점에 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="dc426ca785aa82bc3726faf833e38673875ff1ab" translate="yes" xml:space="preserve">
          <source>If True, the coefficients of the underlying linear model are returned.</source>
          <target state="translated">True이면 기본 선형 모델의 계수가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="5724be8fac97575b0a1ba6783afc1c2fbe0fcac8" translate="yes" xml:space="preserve">
          <source>If True, the covariance of the joint predictive distribution at the query points is returned along with the mean</source>
          <target state="translated">True 인 경우 쿼리 지점에서 공동 예측 분포의 공분산이 평균과 함께 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="dbcfae5acdcda56eec77c1115971fac37cdadcbc" translate="yes" xml:space="preserve">
          <source>If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric). The target is a pandas DataFrame or Series depending on the number of target columns. If &lt;code&gt;return_X_y&lt;/code&gt; is True, then (&lt;code&gt;data&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt;) will be pandas DataFrames or Series as described below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e36608088c739a911a649d964516a72573ccb05" translate="yes" xml:space="preserve">
          <source>If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target columns. If &lt;code&gt;return_X_y&lt;/code&gt; is True, then (&lt;code&gt;data&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt;) will be pandas DataFrames or Series as described below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b39e4230ac891358bd0af7c3eedd9e07bde06b43" translate="yes" xml:space="preserve">
          <source>If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target_columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="132ae12ba4f9038c17051f7e128fd887c18e17fe" translate="yes" xml:space="preserve">
          <source>If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target_columns. The Bunch will contain a &lt;code&gt;frame&lt;/code&gt; attribute with the target and the data. If &lt;code&gt;return_X_y&lt;/code&gt; is True, then &lt;code&gt;(data, target)&lt;/code&gt; will be pandas DataFrames or Series as describe above.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab63854b4cbc92de5ee21a5ee4815065271902d4" translate="yes" xml:space="preserve">
          <source>If True, the distances and indices will be sorted before being returned. If False, the results will not be sorted. If return_distance == False, setting sort_results = True will result in an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91db79e9bb9248c604fdbf115bede6d2bc2e0f43" translate="yes" xml:space="preserve">
          <source>If True, the distances and indices will be sorted before being returned. If False, the results will not be sorted. Only used with mode=&amp;rsquo;distance&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d28765388f526f443e8440713d9f120592d23759" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. If True, theta must not be None.</source>
          <target state="translated">True 인 경우, 위치 theta의 커널 하이퍼 파라미터에 대한 로그 한계 가능성의 기울기가 추가로 반환됩니다. True이면 theta는 None이 아니어야합니다.</target>
        </trans-unit>
        <trans-unit id="2ecaf6f4ca3e741011335b25b38b91313c972ea3" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. Note that gradient computation is not supported for non-binary classification. If True, theta must not be None.</source>
          <target state="translated">True 인 경우, 위치 theta의 커널 하이퍼 파라미터에 대한 로그 한계 가능성의 기울기가 추가로 반환됩니다. 비 이진 분류에는 그래디언트 계산이 지원되지 않습니다. True이면 theta는 None이 아니어야합니다.</target>
        </trans-unit>
        <trans-unit id="65ddb79c8d6a76beebeeb976d10455d7eb1fb46d" translate="yes" xml:space="preserve">
          <source>If True, the imputer mask will be a sparse matrix.</source>
          <target state="translated">True 인 경우 imputer 마스크는 희소 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="eb2cac4cae6a6c8ef92320b8c850d0341c9b187b" translate="yes" xml:space="preserve">
          <source>If True, the kernel attribute is copied. If False, the kernel attribute is modified, but may result in a performance improvement.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a134a28236267fd097c12ab6f4f7b85d957aa9ca" translate="yes" xml:space="preserve">
          <source>If True, the method also returns &lt;code&gt;n_iter&lt;/code&gt;, the actual number of iteration performed by the solver.</source>
          <target state="translated">True 인 경우, 메소드는 솔버가 수행 한 실제 반복 횟수 인 &lt;code&gt;n_iter&lt;/code&gt; 도 리턴합니다 .</target>
        </trans-unit>
        <trans-unit id="af50e40087f45610f2fbcc323b88ccd93dcf9324" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learned more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">True이면 회귀 분석 X가 회귀 분석 전에 정규화됩니다. &lt;code&gt;fit_intercept&lt;/code&gt; 가 False로 설정 되면이 매개 변수는 무시됩니다 . 회귀 분석기가 정규화되면 하이퍼 파라미터가 학습 된 수와 샘플 수에 대해 거의 독립적임을 알게됩니다. 표준화 된 데이터에는 동일한 속성이 유효하지 않습니다. 그러나 표준화하려는 경우 &lt;code&gt;normalize=False&lt;/code&gt; 인 견적 도구에 &lt;code&gt;fit&lt;/code&gt; 을 호출하기 전에 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="016e8fefc2c3108a2b7a4351de86dada7ecbd68e" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">True이면 회귀 분석 X가 회귀 분석 전에 정규화됩니다. &lt;code&gt;fit_intercept&lt;/code&gt; 가 False로 설정 되면이 매개 변수는 무시됩니다 . 회귀 분석기가 정규화되면 하이퍼 파라미터가 학습 된 수와 샘플 수에 대해 거의 독립적임을 알게됩니다. 표준화 된 데이터에는 동일한 속성이 유효하지 않습니다. 그러나 표준화하려는 경우 &lt;code&gt;normalize=False&lt;/code&gt; 인 견적 도구에 &lt;code&gt;fit&lt;/code&gt; 을 호출하기 전에 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="c3fdb7e36f654834d666d698b8cb7219451a4481" translate="yes" xml:space="preserve">
          <source>If True, the return value will be an array of integers, rather than a boolean mask.</source>
          <target state="translated">True 인 경우 반환 값은 부울 마스크가 아닌 정수 배열입니다.</target>
        </trans-unit>
        <trans-unit id="d17d4bbcdf2df4ef33c01a7114516c2e4e90d7d8" translate="yes" xml:space="preserve">
          <source>If True, the standard-deviation of the predictive distribution at the query points is returned along with the mean.</source>
          <target state="translated">True 인 경우 쿼리 지점에서 예측 분포의 표준 편차가 평균과 함께 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="3f294130716f356d91654999eee9757bd2ba6c1c" translate="yes" xml:space="preserve">
          <source>If True, the support of robust location and covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">True 인 경우 강력한 위치 및 공분산 추정값에 대한 지원이 계산되고 데이터를 중심으로하지 않고 공분산 추정값이 재 계산됩니다. 평균이 0과 같지만 정확히 0이 아닌 데이터를 처리하는 데 유용합니다. False 인 경우 추가 처리없이 FastMCD 알고리즘을 사용하여 강력한 위치 및 공분산을 직접 계산합니다.</target>
        </trans-unit>
        <trans-unit id="667a36b1a1b94aa0d760aa610f277fcd1918ae0a" translate="yes" xml:space="preserve">
          <source>If True, the support of the robust location and the covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">True 인 경우 강력한 위치 및 공분산 추정값에 대한 지원이 계산되고 데이터를 중심으로하지 않고 공분산 추정값이 재 계산됩니다. 평균이 0과 같지만 정확히 0이 아닌 데이터를 처리하는 데 유용합니다. False이면 추가 처리없이 FastMCD 알고리즘을 사용하여 강력한 위치 및 공분산을 직접 계산합니다.</target>
        </trans-unit>
        <trans-unit id="77487b230b7ba030d96d8037319b1c202fdf89dc" translate="yes" xml:space="preserve">
          <source>If True, the time elapsed while fitting each step will be printed as it is completed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c5c205f98c6f7e0686c6b1dad61d3840345a50a" translate="yes" xml:space="preserve">
          <source>If True, the time elapsed while fitting each transformer will be printed as it is completed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af3eeb64eec0b2d56ce55e625f484e757480a63a" translate="yes" xml:space="preserve">
          <source>If True, the time elapsed while fitting will be printed as it is completed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28346b69fb6f1a64d688b2ef42aabb524b6563d8" translate="yes" xml:space="preserve">
          <source>If True, then X will be converted to a 2-dimensional NumPy array or sparse matrix. If the conversion is not possible an exception is raised.</source>
          <target state="translated">True 인 경우 X는 2 차원 NumPy 배열 또는 희소 행렬로 변환됩니다. 변환이 불가능하면 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="5c0fef9c1e6748fc4d3cb84e62459147a039a4fd" translate="yes" xml:space="preserve">
          <source>If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be &amp;lt; n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless.</source>
          <target state="translated">True 인 경우 고유 값이 0 인 모든 구성 요소가 제거되므로 출력의 구성 요소 수가 &amp;lt;n_components (숫자 불안정성으로 인해 때로는 0)가 될 수 있습니다. n_components가 None이면이 매개 변수는 무시되고 고유 값이 0 인 구성 요소는 상관없이 제거됩니다.</target>
        </trans-unit>
        <trans-unit id="42874c4e7adb064c98a0fb44835161462f985220" translate="yes" xml:space="preserve">
          <source>If True, then compute normalized Laplacian.</source>
          <target state="translated">True이면 정규화 된 라플라시안을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="ba532aca0424d37b34ea4248f58728a030b07d2d" translate="yes" xml:space="preserve">
          <source>If True, then return the centers of each cluster</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7818bd11ce52f2496690f2a19701e1fdc3bace9d" translate="yes" xml:space="preserve">
          <source>If True, transpose the downloaded data array.</source>
          <target state="translated">True이면 다운로드 한 데이터 배열을 바꿉니다.</target>
        </trans-unit>
        <trans-unit id="9b28aadc7e361758f4f56436eeab711f856e9105" translate="yes" xml:space="preserve">
          <source>If True, use a breadth-first search. If False (default) use a depth-first search. Breadth-first is generally faster for compact kernels and/or high tolerances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06e972e2019f64a904722b2934f5b7cf1e54e236" translate="yes" xml:space="preserve">
          <source>If True, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree algorithms can have better scaling for large N.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09f8cdcb36703e0413a45867eb062d48bc42e4a4" translate="yes" xml:space="preserve">
          <source>If True, validation for finiteness will be skipped, saving time, but leading to potential crashes. If False, validation for finiteness will be performed, avoiding error. Global default: False.</source>
          <target state="translated">True이면 유한성에 대한 유효성 검사를 건너 뛰어 시간을 절약 할 수 있지만 충돌이 발생할 수 있습니다. False이면 유한성을 검증하여 오류를 방지합니다. 전역 기본값 : False</target>
        </trans-unit>
        <trans-unit id="f770d204acb3934762188e63b6bd0977cfe619aa" translate="yes" xml:space="preserve">
          <source>If True, will return the parameters for this estimator and contained subobjects that are estimators.</source>
          <target state="translated">True 인 경우이 추정기 및 추정 기인 하위 오브젝트에 대한 매개 변수를 리턴합니다.</target>
        </trans-unit>
        <trans-unit id="edc518974aa9f8ea115d0f36469bc2b1e2cd15a2" translate="yes" xml:space="preserve">
          <source>If True, will return the query_id array for each file.</source>
          <target state="translated">True이면 각 파일에 대해 query_id 배열을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="80fdba1026cc980884a84dfa72ad1747c034afc6" translate="yes" xml:space="preserve">
          <source>If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.</source>
          <target state="translated">X와 y가 C 순서가 아니며 np.float64의 연속 배열이고 X가 scipy.sparse.csr_matrix가 아닌 경우 X 및 / 또는 y가 복사 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a601440183ce5a872479c357e441563196aab652" translate="yes" xml:space="preserve">
          <source>If X is a dense array, then the other methods will not support sparse matrices as input.</source>
          <target state="translated">X가 고밀도 배열 인 경우 다른 방법은 희소 행렬을 입력으로 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="efab9063b47a2afb85758f067069188b21b34f3e" translate="yes" xml:space="preserve">
          <source>If X is encoded as a CSR matrix.</source>
          <target state="translated">X가 CSR 매트릭스로 인코딩 된 경우</target>
        </trans-unit>
        <trans-unit id="f4e2537cdb42d2bfe76e858b065192a8758650ef" translate="yes" xml:space="preserve">
          <source>If X is encoded as a CSR matrix;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da96e9fabf18fc39905756390120e4a7a1e09f97" translate="yes" xml:space="preserve">
          <source>If X is not a C-ordered contiguous array it is copied.</source>
          <target state="translated">X가 C 순서의 연속 배열이 아닌 경우 복사됩니다.</target>
        </trans-unit>
        <trans-unit id="9cc8f34afbd30e04cc65d91f1b233abc1c382996" translate="yes" xml:space="preserve">
          <source>If X is not an array of floating values;</source>
          <target state="translated">X가 부동 값의 배열이 아닌 경우</target>
        </trans-unit>
        <trans-unit id="e7ca7ce1d419c3d60265041304210343f2e8b91d" translate="yes" xml:space="preserve">
          <source>If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational basis: we want to learn loadings L and a set of components C such that &lt;em&gt;X = L C&lt;/em&gt;. Different criteria exist to choose the components</source>
          <target state="translated">X가 다변량 데이터 인 경우 우리가 해결하려고하는 문제는 다른 관측 기준으로 다시 작성하는 것입니다. 하중 L과 &lt;em&gt;X = LC&lt;/em&gt; 와 같은 성분 C 세트를 배우고 싶습니다 . 구성 요소를 선택하기위한 다른 기준</target>
        </trans-unit>
        <trans-unit id="4691b6eeb6f44a63af5f24ac30dc066ab023aa61" translate="yes" xml:space="preserve">
          <source>If X is sparse and &lt;code&gt;missing_values=0&lt;/code&gt;;</source>
          <target state="translated">X가 희소이고 &lt;code&gt;missing_values=0&lt;/code&gt; ;</target>
        </trans-unit>
        <trans-unit id="5bf131136f9283115170d3f032466f07678834a5" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise distance between the arrays from both X and Y.</source>
          <target state="translated">Y가 제공되면 (기본값은 None) 반환 된 행렬은 X와 Y의 배열 사이의 쌍별 거리입니다.</target>
        </trans-unit>
        <trans-unit id="a6e7fe3e345be28d5e67984fa49ad01aeaa444dd" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise kernel between the arrays from both X and Y.</source>
          <target state="translated">Y가 주어지면 (기본값은 None), 반환 된 행렬은 X와 Y의 배열 사이의 쌍 커널입니다.</target>
        </trans-unit>
        <trans-unit id="15e0fd61e3b85d8ebad2fc2135f6d5822723ce41" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}\) is the estimated target output, \(y\) the corresponding (correct) target output, and \(Var\) is &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt;, the square of the standard deviation, then the explained variance is estimated as follow:</source>
          <target state="translated">\ (\ hat {y} \)가 추정 대상 출력이고 \ (y \) 해당 (올바른) 대상 출력이고 \ (Var \)가 표준 편차의 제곱 인 &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt; 인 경우 설명 된 분산은 다음과 같습니다. 다음과 같이 추정됩니다 :</target>
        </trans-unit>
        <trans-unit id="1d8ad99bdd9bde4f8f4c5eb30616979db9d7983c" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value for total \(n\) samples, the estimated R&amp;sup2; is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8fb389b8a60a2b57fc22c9161412c484a73dd18" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the 0-1 loss \(L_{0-1}\) is defined as:</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 표본의 예측값이고 \ (y_i \)가 해당하는 참값이면 0-1 손실 \ (L_ {0-1 } \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="01fda7f04ce93ad5d6b5843c80c53ee91e04866d" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the fraction of correct predictions over \(n_\text{samples}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 샘플의 예측 된 값이고 \ (y_i \)가 해당하는 실제 값인 경우 \ (n_ \ text { 샘플} \)은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="af46aeec0a0c654990b43c84ec26ca8a3817bbd3" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the median absolute error (MedAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 샘플의 예측 된 값이고 \ (y_i \)가 해당하는 참값 인 경우 \ (n_ {\ text {samples}} \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="27ab05c62dcfc0b1ca98228a2c106c2bd25d72f6" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the score R&amp;sup2; estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 표본의 예측 된 값이고 \ (y_i \)가 해당하는 실제 값이면 \ (n _ {\ text { 샘플}} \)은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="2057e5fd21ea6e3999b964ff2858a1023496793a" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the max error is defined as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b2139578cc75e1715d428f9c389fc66abbdc391" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean Tweedie deviance error (D) for power \(p\), estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9be139031431f33624d9549cf24272bbec27cad" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean absolute error (MAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 샘플의 예측 된 값이고 \ (y_i \)가 해당하는 참값이면 \ (평균 절대 오차 (MAE)는 \ ( n _ {\ text {samples}} \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="b7836e2114e345225a74c22cbe0d3b5d52c8f253" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared error (MSE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 표본의 예측 된 값이고 \ (y_i \)가 해당하는 실제 값이면 \ (n)에 대해 추정 된 평균 제곱 오차 (MSE) n _ {\ text {samples}} \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="1cf47fc0a0aaaffd1c0a818b8704218b8ae722c1" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 샘플의 예측 된 값이고 \ (y_i \)가 해당하는 참값 인 경우, 평균 제곱 로그 오류 (MSLE)는 \ (n _ {\ text {samples}} \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="f295fa8851d145ac326bc63b92809e2fbf3d1f72" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_j\) is the predicted value for the \(j\)-th label of a given sample, \(y_j\) is the corresponding true value, and \(n_\text{labels}\) is the number of classes or labels, then the Hamming loss \(L_{Hamming}\) between two samples is defined as:</source>
          <target state="translated">\ (\ hat {y} _j \)가 주어진 샘플의 \ (j \) 번째 레이블에 대해 예측 된 값인 경우, \ ​​(y_j \)는 해당하는 참값이고 \ (n_ \ text {labels} \)는 클래스 또는 레이블의 수이며, 두 샘플 간의 해밍 손실 \ (L_ {Hamming} \)은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="7fa4bf510f83c73a55e8dec038abaacf960351ca" translate="yes" xml:space="preserve">
          <source>If \(c_0 = 0\) the kernel is said to be homogeneous.</source>
          <target state="translated">\ (c_0 = 0 \)이면 커널은 동종이라고합니다.</target>
        </trans-unit>
        <trans-unit id="c7d07701826b4f4c8efa455b46a49990993930f2" translate="yes" xml:space="preserve">
          <source>If \(h_i\) is given, the above equation automatically implies the following probabilistic interpretation:</source>
          <target state="translated">\ (h_i \)가 주어지면 위의 방정식은 다음과 같은 확률 론적 해석을 자동으로 암시합니다.</target>
        </trans-unit>
        <trans-unit id="9d9af8bc9b90a6fbf0d539b126efbd694bdaddf6" translate="yes" xml:space="preserve">
          <source>If \(y_i\) is the true value of the \(i\)-th sample, and \(w_i\) is the corresponding sample weight, then we adjust the sample weight to:</source>
          <target state="translated">\ (y_i \)가 \ (i \) 번째 샘플의 실제 값이고 \ (w_i \)가 해당하는 샘플 무게이면 샘플 무게를 다음과 같이 조정합니다.</target>
        </trans-unit>
        <trans-unit id="4c76ddad0ef7a743f202685a0861880cbc2062e2" translate="yes" xml:space="preserve">
          <source>If \(y_w\) is the predicted decision for true label and \(y_t\) is the maximum of the predicted decisions for all other labels, where predicted decisions are output by decision function, then multiclass hinge loss is defined by:</source>
          <target state="translated">\ (y_w \)가 실제 레이블에 대한 예측 결정이고 \ (y_t \)가 다른 모든 레이블에 대한 예측 결정의 최대 값 인 경우 (예측 결정이 결정 기능에 의해 출력되는 경우) 다중 클래스 힌지 손실은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="b8371056060d53aef38082b274a12c6e92dd981b" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">accept_sparse가 CSR, CSC, COO 또는 BSR 스파 스 행렬을 제공하고 승인하는 경우 accept_large_sparse는 해당 인덱스가 32 비트 dtype으로 저장된 경우에만이를 허용합니다.</target>
        </trans-unit>
        <trans-unit id="ca2f554a4272574081b19f205bd8db66223aa9f8" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse=False will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">accept_sparse가 CSR, CSC, COO 또는 BSR 스파 스 행렬을 제공하고 승인하는 경우 accept_large_sparse = False는 해당 인덱스가 32 비트 dtype으로 저장된 경우에만 허용됩니다.</target>
        </trans-unit>
        <trans-unit id="b0090a224443cfea422c2167734e98ec705e71a5" translate="yes" xml:space="preserve">
          <source>If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.</source>
          <target state="translated">호출 가능 항목이 전달되면 처리되지 않은 원시 입력에서 기능 시퀀스를 추출하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="dcd8eacb988e0fa27afa1a7692943530b07747f6" translate="yes" xml:space="preserve">
          <source>If a callable is passed, it should take arguments X, k and and a random state and return an initialization.</source>
          <target state="translated">호출 가능 항목이 전달되면 인수 X, k 및 임의 상태를 취하여 초기화를 리턴해야합니다.</target>
        </trans-unit>
        <trans-unit id="cf28b181c12fdc4001f26b46f656bc18426882b0" translate="yes" xml:space="preserve">
          <source>If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15b5ddf5d35e7b6d7436896e31247316b726ba30" translate="yes" xml:space="preserve">
          <source>If a float, that value is added to all values in the contingency matrix. This helps to stop NaN propagation. If &lt;code&gt;None&lt;/code&gt;, nothing is adjusted.</source>
          <target state="translated">float이면 해당 값이 우연성 행렬의 모든 값에 추가됩니다. 이것은 NaN 전파를 막는 데 도움이됩니다. 경우 &lt;code&gt;None&lt;/code&gt; , 아무것도 조정하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="aec58020de2b924f9656034ee47c95a7ace302db" translate="yes" xml:space="preserve">
          <source>If a list is passed it&amp;rsquo;s expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">목록이 전달되면 n_targets 같은 배열 중 하나 일 것으로 예상됩니다. 경로를 따라 다양한 계수 값. &lt;code&gt;fit_path&lt;/code&gt; 매개 변수가 &lt;code&gt;False&lt;/code&gt; 인 경우 존재하지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="8e3b6cd9422926a607fefd39c3e9bd3020c06d14" translate="yes" xml:space="preserve">
          <source>If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if &lt;code&gt;analyzer == 'word'&lt;/code&gt;.</source>
          <target state="translated">목록 인 경우 해당 목록에는 중지 단어가 포함 된 것으로 간주되며 모든 결과 토큰에서 제거됩니다. &lt;code&gt;analyzer == 'word'&lt;/code&gt; 경우에만 적용됩니다 .</target>
        </trans-unit>
        <trans-unit id="3a891d1ec4d966c2171dc7ddf73d952ce675676b" translate="yes" xml:space="preserve">
          <source>If a single axis is passed in, it is treated as a bounding axes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58d1b02436a7aa9160e580f582400827e1ad046d" translate="yes" xml:space="preserve">
          <source>If a string, it is passed to _check_stop_list and the appropriate stop list is returned. &amp;lsquo;english&amp;rsquo; is currently the only supported string value. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">문자열 인 경우 _check_stop_list로 전달되고 적절한 중지 목록이 리턴됩니다. 'english'는 현재 유일하게 지원되는 문자열 값입니다. 'english'에는 몇 가지 알려진 문제가 있으며 대안을 고려해야합니다 ( &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;중지 단어 사용&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="564b43bc82acf22a1de3b85bb28ac591ff97b4bf" translate="yes" xml:space="preserve">
          <source>If a string, this may be one of &amp;lsquo;nearest_neighbors&amp;rsquo;, &amp;lsquo;precomputed&amp;rsquo;, &amp;lsquo;rbf&amp;rsquo; or one of the kernels supported by &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt;.</source>
          <target state="translated">문자열 인 경우 'nearest_neighbors', 'precomputed', 'rbf'또는 &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt; 가 지원하는 커널 중 하나 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="675cbfc234c52633edd36cba3388cd72c1b8e2d3" translate="yes" xml:space="preserve">
          <source>If a target is a classification outcome taking on values 0,1,&amp;hellip;,K-1, for node \(m\), representing a region \(R_m\) with \(N_m\) observations, let</source>
          <target state="translated">대상이 \ (N_m \) 관측 값이있는 영역 \ (R_m \)을 나타내는 노드 \ (m \)에 대해 0,1,&amp;hellip;, K-1 값을 사용하는 분류 결과 인 경우</target>
        </trans-unit>
        <trans-unit id="b39b95bbd18e310b00daaab2152e27da5d834f6a" translate="yes" xml:space="preserve">
          <source>If add_indicator=True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="373500a68bbbd934744d157d24ba37240f790a20" translate="yes" xml:space="preserve">
          <source>If affinity is &amp;ldquo;precomputed&amp;rdquo; X : array-like, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples.</source>
          <target state="translated">선호도가 &quot;사전 계산 된&quot;X : 배열과 같은 모양 (n_samples, n_samples) 인 경우 X를 사전 계산 된 인접 그래프로 해석하여 샘플에서 계산합니다.</target>
        </trans-unit>
        <trans-unit id="86d4d97c781f9d1a441e0d9197ea013d3820b489" translate="yes" xml:space="preserve">
          <source>If affinity is &amp;ldquo;precomputed&amp;rdquo; X : {array-like, sparse matrix}, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c306493486d7abaed871db69a1b0f3a0d3e230f4" translate="yes" xml:space="preserve">
          <source>If affinity is the adjacency matrix of a graph, this method can be used to find normalized graph cuts.</source>
          <target state="translated">선호도가 그래프의 인접 행렬 인 경우이 방법을 사용하여 정규화 된 그래프 컷을 찾을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fef3ba1186af33eef8e244a6a4bb530d43ffdf84" translate="yes" xml:space="preserve">
          <source>If all examples are from the same class, it uses a one-class SVM.</source>
          <target state="translated">모든 예제가 동일한 클래스에서 온 경우 단일 클래스 SVM을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="b53805960d76925767243aca9c19243fc1b08d06" translate="yes" xml:space="preserve">
          <source>If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.</source>
          <target state="translated">모든 매개 변수가 목록으로 표시되면 교체하지 않은 샘플링이 수행됩니다. 분포로 적어도 하나의 매개 변수가 제공되면 대체 샘플링이 사용됩니다. 연속 모수에 연속 분포를 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="44ee4928f6faf842a93c2259e4d1b6db16c4073a" translate="yes" xml:space="preserve">
          <source>If all the coordinates are missing or if there are no common present coordinates then NaN is returned for that pair.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f219d1953670922fbbfe88159427df7222c9397" translate="yes" xml:space="preserve">
          <source>If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points \(x_i\), one may use the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm to the mapped data points \(\phi(x_i)\). The advantage of using \(k\) is that the mapping \(\phi\) never has to be calculated explicitly, allowing for arbitrary large features (even infinite).</source>
          <target state="translated">선형 서포트 벡터 머신 또는 PCA와 같은 알고리즘이 데이터 포인트 \ (x_i \)의 스칼라 곱에만 의존하는 경우 적용에 해당하는 \ (k (x_i, x_j) \) 값을 사용할 수 있습니다. 매핑 된 데이터 포인트에 대한 알고리즘 \ (\ phi (x_i) \). \ (k \)를 사용하면 이점은 명시 적으로 계산할 필요가 없으며 임의의 큰 기능 (무한까지도)을 허용한다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="22b4c02685073d2c5c3e900f3a57456658da8b22" translate="yes" xml:space="preserve">
          <source>If an array-like of axes are passed in, the partial dependence</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fea95f95d0577b3d2b8dde1c99a4f5f11240b1d" translate="yes" xml:space="preserve">
          <source>If an exception is triggered, use &lt;code&gt;%debug&lt;/code&gt; to fire-up a post mortem ipdb session.</source>
          <target state="translated">예외가 발생하면 &lt;code&gt;%debug&lt;/code&gt; 를 사용 하여 사후 iptem 세션을 시작하십시오.</target>
        </trans-unit>
        <trans-unit id="5c4a826b768bf0ea44b0de0cf9a279c59410da1d" translate="yes" xml:space="preserve">
          <source>If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details.</source>
          <target state="translated">정수가 제공되면 사용되는 알파 격자의 점 수를 수정합니다. 목록이 제공되면 사용할 그리드를 제공합니다. 자세한 내용은 docstring 클래스의 메모를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="9e0d496b83e5a4c02138b5662acc0d0357377648" translate="yes" xml:space="preserve">
          <source>If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details. Range is (0, inf] when floats given.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f54c86c80b29ba93fdb4405121f2a742f42412e" translate="yes" xml:space="preserve">
          <source>If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.</source>
          <target state="translated">ndarray가 전달되면 모양 (n_clusters, n_features)이어야하고 초기 중심을 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="ce1d1dc15735f50591b21078e51cada592338767" translate="yes" xml:space="preserve">
          <source>If arpack :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57a89ce6b3eb175b37e97fdc8660396b5ef203e5" translate="yes" xml:space="preserve">
          <source>If auto :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bf20b6ab9e24e0313be1f29e5bd87350c62fc72" translate="yes" xml:space="preserve">
          <source>If bandwidth is not given, it is determined using a heuristic based on the median of all pairwise distances. This will take quadratic time in the number of samples. The sklearn.cluster.estimate_bandwidth function can be used to do this more efficiently.</source>
          <target state="translated">대역폭이 제공되지 않으면 모든 쌍별 거리의 중앙값을 기반으로 휴리스틱을 사용하여 결정됩니다. 샘플 수에 이차 시간이 걸립니다. sklearn.cluster.estimate_bandwidth 함수를 사용하여보다 효율적으로 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="307d133eac653119e68c3f800803dcc4776573b9" translate="yes" xml:space="preserve">
          <source>If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If &amp;lsquo;auto&amp;rsquo;, it is assigned to False for dense &lt;code&gt;X&lt;/code&gt; and to True for sparse &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">부울 인 경우 모든 기능을 이산 또는 연속으로 고려할지 여부를 결정합니다. 배열 인 경우 모양이있는 부울 마스크 (n_features)이거나 개별 기능의 인덱스가있는 배열이어야합니다. 'auto'인 경우 밀도가 높은 &lt;code&gt;X&lt;/code&gt; 의 경우 False , 희소 한 &lt;code&gt;X&lt;/code&gt; 의 경우 True로 지정됩니다 .</target>
        </trans-unit>
        <trans-unit id="14d26c2cb6ccf4f84a440b5eee94360749d30490" translate="yes" xml:space="preserve">
          <source>If boolean, whether or not to fit the isotonic regression with y increasing or decreasing.</source>
          <target state="translated">부울 인 경우, 증가 또는 감소와 함께 등장 성 회귀에 적합할지 여부.</target>
        </trans-unit>
        <trans-unit id="34ddc69f78e9ae21f32c4048eb992eb5ea37253a" translate="yes" xml:space="preserve">
          <source>If bootstrap is True, the number of samples to draw from X to train each base estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7380002883f78b7443a4cf144369e2cdf9c7dd5" translate="yes" xml:space="preserve">
          <source>If bytes or files are given to analyze, this encoding is used to decode.</source>
          <target state="translated">바이트 또는 파일을 분석하기 위해 제공 한 경우이 인코딩을 사용하여 디코딩합니다.</target>
        </trans-unit>
        <trans-unit id="b1cd46fc8b5b18d3d8ec258c92a5832fe49ecb69" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the AMI is null:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당이 완전히 완료되지 않으므로 AMI가 null입니다.</target>
        </trans-unit>
        <trans-unit id="e6f2dbc2c288fdff952bc5d6d0612fad044f50c2" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the NMI is null:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당이 완전히 완료되지 않으므로 NMI가 널입니다.</target>
        </trans-unit>
        <trans-unit id="60b93fba5d2befe30dad173ef2989a32caf2707d" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the ARI is very low:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당이 완전히 불완전하므로 ARI가 매우 낮습니다.</target>
        </trans-unit>
        <trans-unit id="e02bb35b2a969fdf2ff25b865a0b3ba938fb92a9" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당이 완전히 완료되지 않으므로 V-Measure가 null입니다.</target>
        </trans-unit>
        <trans-unit id="d0974a75f074fb11fd0a08f495fdb803227dd0c6" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally random, hence the FMI is null:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당은 완전히 임의이므로 FMI는 null입니다.</target>
        </trans-unit>
        <trans-unit id="3a4c44f6cadbe5141305e79bc3f8068062652c4d" translate="yes" xml:space="preserve">
          <source>If classes members are split across different clusters, the assignment cannot be complete:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 분할 된 경우 할당을 완료 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="a2787535a9b0772f98dac284faca0d2184e54d74" translate="yes" xml:space="preserve">
          <source>If coefficients vary significantly when changing the input dataset their robustness is not guaranteed, and they should probably be interpreted with caution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52c8ac55b5c80763168c6804bd2b4b595db63ef9" translate="yes" xml:space="preserve">
          <source>If computed_score is True, value of the log marginal likelihood (to be maximized) at each iteration of the optimization. The array starts with the value of the log marginal likelihood obtained for the initial values of alpha and lambda and ends with the value obtained for the estimated alpha and lambda.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="baf96abe9df5cd386826eafcd47454c9dcc36819" translate="yes" xml:space="preserve">
          <source>If copy is False, the affinity matrix is modified inplace by the algorithm, for memory efficiency</source>
          <target state="translated">copy가 False 인 경우 메모리 효율성을 위해 선호도 매트릭스가 알고리즘에 의해 수정됩니다.</target>
        </trans-unit>
        <trans-unit id="0070f2c19699b732c372ba0e363293b507463ed1" translate="yes" xml:space="preserve">
          <source>If decision_function_shape=&amp;rsquo;ovo&amp;rsquo;, the function values are proportional to the distance of the samples X to the separating hyperplane. If the exact distances are required, divide the function values by the norm of the weight vector (&lt;code&gt;coef_&lt;/code&gt;). See also &lt;a href=&quot;https://stats.stackexchange.com/questions/14876/interpreting-distance-from-hyperplane-in-svm&quot;&gt;this question&lt;/a&gt; for further details. If decision_function_shape=&amp;rsquo;ovr&amp;rsquo;, the decision function is a monotonic transformation of ovo decision function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="671e4e16873255449d2ba54f06975c272f73c34d" translate="yes" xml:space="preserve">
          <source>If density = &amp;lsquo;auto&amp;rsquo;, the value is set to the minimum density as recommended by Ping Li et al.: 1 / sqrt(n_features).</source>
          <target state="translated">밀도 = '자동'인 경우 Ping Li 등의 권장에 따라 값이 최소 밀도로 설정됩니다. 1 / sqrt (n_features).</target>
        </trans-unit>
        <trans-unit id="391517cb3cfce3ac9c6c32b7ceac049807282afc" translate="yes" xml:space="preserve">
          <source>If documents are pre-tokenized by an external package, then store them in files (or strings) with the tokens separated by whitespace and pass &lt;code&gt;analyzer=str.split&lt;/code&gt;</source>
          <target state="translated">문서가 외부 패키지에 의해 사전 토큰 화 된 경우 토큰을 공백으로 구분하여 파일 (또는 문자열)에 저장하고 &lt;code&gt;analyzer=str.split&lt;/code&gt; 을 전달 하십시오.</target>
        </trans-unit>
        <trans-unit id="80416b24b5f24b22b80d50d90aa942e77a2dadfc" translate="yes" xml:space="preserve">
          <source>If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix reveals the biclusters on the diagonal. Here is an example of this structure where biclusters have higher average values than the other rows and columns:</source>
          <target state="translated">각 행과 각 열이 정확히 하나의 bicluster에 속하는 경우 데이터 행렬의 행과 열을 다시 정렬하면 대각선에 biclusters가 나타납니다. 여기에 구조화 기호가 다른 행과 열보다 높은 평균값을 갖는이 구조의 예가 있습니다.</target>
        </trans-unit>
        <trans-unit id="78ba0badd104b3ed95bd2061747613a1b6f84ce9" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of groups to include in the test split (rounded up). If int, represents the absolute number of test groups. If None, the value is set to the complement of the train size. The default will change in version 0.21. It will remain 0.2 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1db9c10662b6d49b6b84ca8b7b7ce2a9580afa10" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default (the parameter is unspecified), the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 테스트 스플릿에 포함 할 데이터 집합의 비율을 나타냅니다. int이면 테스트 샘플의 절대 수를 나타냅니다. None 인 경우, 값은 열차 크기의 보수로 설정됩니다. 기본적으로 (매개 변수는 지정되지 않음) 값은 0.1로 설정됩니다. 기본값은 버전 0.21에서 변경됩니다. &lt;code&gt;train_size&lt;/code&gt; 가 지정되지 않은 경우에만 0.1로 유지되며 , 그렇지 않으면 지정된 &lt;code&gt;train_size&lt;/code&gt; 를 보완합니다 .</target>
        </trans-unit>
        <trans-unit id="73ce93bb920d84bef49715afdf02f771938f8206" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 테스트 스플릿에 포함 할 데이터 집합의 비율을 나타냅니다. int이면 테스트 샘플의 절대 수를 나타냅니다. None 인 경우, 값은 열차 크기의 보수로 설정됩니다. 기본적으로이 값은 0.1로 설정되어 있습니다. 기본값은 버전 0.21에서 변경됩니다. &lt;code&gt;train_size&lt;/code&gt; 가 지정되지 않은 경우에만 0.1로 유지되며 , 그렇지 않으면 지정된 &lt;code&gt;train_size&lt;/code&gt; 를 보완합니다 .</target>
        </trans-unit>
        <trans-unit id="aa74ab3ce21513c4e7c83e9b91dad63582c835b8" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.2. The default will change in version 0.21. It will remain 0.2 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 테스트 스플릿에 포함 할 데이터 집합의 비율을 나타냅니다. int이면 테스트 샘플의 절대 수를 나타냅니다. None 인 경우, 값은 열차 크기의 보수로 설정됩니다. 기본적으로이 값은 0.2로 설정되어 있습니다. 기본값은 버전 0.21에서 변경됩니다. &lt;code&gt;train_size&lt;/code&gt; 가 지정되지 않은 경우에만 0.2로 유지되며 , 그렇지 않으면 지정된 &lt;code&gt;train_size&lt;/code&gt; 를 보완합니다 .</target>
        </trans-unit>
        <trans-unit id="dfe0bc4ba0825c6b9e54b3177711e714d6e28a2b" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 테스트 스플릿에 포함 할 데이터 집합의 비율을 나타냅니다. int이면 테스트 샘플의 절대 수를 나타냅니다. None 인 경우, 값은 열차 크기의 보수로 설정됩니다. 기본적으로이 값은 0.25로 설정되어 있습니다. 기본값은 버전 0.21에서 변경됩니다. &lt;code&gt;train_size&lt;/code&gt; 가 지정되지 않은 경우에만 0.25로 유지되며 , 그렇지 않으면 지정된 &lt;code&gt;train_size&lt;/code&gt; 를 보완합니다 .</target>
        </trans-unit>
        <trans-unit id="09a40f5654bce0b9dd81c58e4db01bd35e373391" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If &lt;code&gt;train_size&lt;/code&gt; is also None, it will be set to 0.1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c86f8c8782626d870a975f8a368ba48b84e1e80" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If &lt;code&gt;train_size&lt;/code&gt; is also None, it will be set to 0.25.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3b7da8f21403a8e0fce55a369b8d82b4da5bfd1" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 열차 분할에 포함 할 데이터 세트의 비율을 나타냅니다. int이면 열차 샘플의 절대 수를 나타냅니다. None이면 값이 테스트 크기의 보수로 자동 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="62b47f7a89d7c976d2813299381cdc4f4f5b3cad" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the groups to include in the train split. If int, represents the absolute number of train groups. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">플로트 인 경우 0.0과 1.0 사이 여야하며 열차 분할에 포함 할 그룹의 비율을 나타냅니다. int이면 열차 그룹의 절대 수를 나타냅니다. None이면 값이 테스트 크기의 보수로 자동 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="561d4c125db4b741d015190537a6b19d8f8e55c1" translate="yes" xml:space="preserve">
          <source>If float, the contamination should be in the range [0, 0.5].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0ffbda1c59db43809cf9245f33efa45a65a5c05" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;max_features&lt;/code&gt; is a fraction and &lt;code&gt;int(max_features * n_features)&lt;/code&gt; features are considered at each split.</source>
          <target state="translated">float이면 &lt;code&gt;max_features&lt;/code&gt; 는 분수이며 &lt;code&gt;int(max_features * n_features)&lt;/code&gt; 기능은 각 분할에서 고려됩니다.</target>
        </trans-unit>
        <trans-unit id="47d0c1ebc9dc44a7018a0ce88d0d45201068f385" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; are the minimum number of samples for each node.</source>
          <target state="translated">float 인 경우 &lt;code&gt;min_samples_leaf&lt;/code&gt; 는 분수이고 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; 은 각 노드의 최소 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="c217707834c84f95b745c6fd735e46ef1d5cb29d" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; is the minimum number of samples for each node.</source>
          <target state="translated">float이면 &lt;code&gt;min_samples_leaf&lt;/code&gt; 는 분수이고 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; 은 각 노드의 최소 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="f5813e9c1656f619ed6234eb86815e1c76ec9071" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; are the minimum number of samples for each split.</source>
          <target state="translated">float이면 &lt;code&gt;min_samples_split&lt;/code&gt; 은 분수이고 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; 은 각 분할에 대한 최소 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="c32957ea85344bbb6b41aa874b4a5e7763ff6b76" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; is the minimum number of samples for each split.</source>
          <target state="translated">float이면 &lt;code&gt;min_samples_split&lt;/code&gt; 은 분수이고 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; 은 각 분할에 대한 최소 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="81fe24c94c96599e85080c0cc195542bdb1ce722" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; features.</source>
          <target state="translated">float이면 &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; 기능을 그립니다 .</target>
        </trans-unit>
        <trans-unit id="a2b1676fcae8577852e20614ce418906e2f79102" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; samples.</source>
          <target state="translated">float이면 &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; 샘플을 그립니다 .</target>
        </trans-unit>
        <trans-unit id="271d97216c612b23c04f108b3da94d4db7dcfcec" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; samples. Thus, &lt;code&gt;max_samples&lt;/code&gt; should be in the interval &lt;code&gt;(0, 1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cf469ccb5c1129883a42e4f4a3183401e643c51" translate="yes" xml:space="preserve">
          <source>If full :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f234188a9694365b66e83538b40de1fa4059a074" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration.</source>
          <target state="translated">1보다 크거나 같은 경우 &lt;code&gt;step&lt;/code&gt; 는 각 반복에서 제거 할 (정수) 기능 수에 해당합니다. (0.0, 1.0) 이내 인 경우 &lt;code&gt;step&lt;/code&gt; 는 각 반복에서 제거 할 기능의 백분율 (반올림)에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="d3aeb6a39c457b69bdde12a943780461d08c388d" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration. Note that the last iteration may remove fewer than &lt;code&gt;step&lt;/code&gt; features in order to reach &lt;code&gt;min_features_to_select&lt;/code&gt;.</source>
          <target state="translated">1보다 크거나 같은 경우 &lt;code&gt;step&lt;/code&gt; 는 각 반복에서 제거 할 (정수) 기능 수에 해당합니다. (0.0, 1.0) 이내 인 경우 &lt;code&gt;step&lt;/code&gt; 는 각 반복에서 제거 할 기능의 백분율 (반올림)에 해당합니다. &lt;code&gt;min_features_to_select&lt;/code&gt; 에 도달하기 위해 마지막 반복 에서 &lt;code&gt;step&lt;/code&gt; 기능 보다 적은 수가 제거 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1351e34960b08f78869e356e9d9129a529a17168" translate="yes" xml:space="preserve">
          <source>If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">QDA 모델에서 공분산 행렬이 대각선이라고 가정하면 입력이 각 클래스에서 조건부로 독립적이라고 가정하고 결과 분류기는 가우스 Naive Bayes 분류기 &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt; &lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt; 와 같습니다&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="47884bf7f490577d7025ceb970813cb997acfc82" translate="yes" xml:space="preserve">
          <source>If init=&amp;rsquo;custom&amp;rsquo;, it is used as initial guess for the solution.</source>
          <target state="translated">init = 'custom'이면 솔루션의 초기 추측으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="25a1f9fc4e56498f0b6e355b29cc8aeb5e3daeb3" translate="yes" xml:space="preserve">
          <source>If init=&amp;rsquo;custom&amp;rsquo;, it is used as initial guess for the solution. If update_H=False, it is used as a constant, to solve for W only.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b9c788e52ff7adb618d2b8cf3dd396e088800c8" translate="yes" xml:space="preserve">
          <source>If int, it is the total number of points equally divided among clusters. If array-like, each element of the sequence indicates the number of samples per cluster.</source>
          <target state="translated">int이면 클러스터간에 똑같이 나누어 진 총 포인트 수입니다. 배열과 같은 경우 시퀀스의 각 요소는 클러스터 당 샘플 수를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="424c096f1eaf5892378c66d2235ced4e356f327c" translate="yes" xml:space="preserve">
          <source>If int, it is the total number of points generated. For odd numbers, the inner circle will have one point more than the outer circle. If two-element tuple, number of points in outer circle and inner circle.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2403d0bd3d2ac8c8a9756c4cde9cd9202cbe9926" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="676c2454734bf9216c5797d643595f0b850b4e7a" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Note that different initializations might result in different local minima of the cost function.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . 초기화가 다를 경우 비용 함수의 로컬 최소값이 다를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a1933900181bd8e24f0237ebecc7a06b2ef8b486" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Only used when &lt;code&gt;svd_method&lt;/code&gt; equals &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;svd_method&lt;/code&gt; 가 'randomized'와 같은 경우에만 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="4914440c8828885c0b1efea7679daefd5f1e4eaf" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;eigen_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;eigen_solver&lt;/code&gt; == 'arpack'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="90657492e3c46d11fb3a1c799a4569e4854211ed" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; == True.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;shuffle&lt;/code&gt; == True 일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="7e39c8ed74a38762200c178da772671eaa6b3f52" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; is True.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;shuffle&lt;/code&gt; 이 참일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="636d95a8f3edcd08bb7a122de05f8944c1a330ee" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;solver&lt;/code&gt; == 'arpack'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="d8bb54edf7a318cb4f3734b3f7721b6c840db8b1" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;svd_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo; or &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;svd_solver&lt;/code&gt; == 'arpack'또는 'randomized'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="49fc99a0f8ec79ab5cf6633c8b91ad397447b0ae" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Note that this is used by subsampling and smoothing noise.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. None이면 난수 생성기는 np.random에서 사용하는 RandomState 인스턴스입니다. 이는 서브 샘플링 및 평활 노이즈에서 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="20ecb73824a9c787c46458c94b754eb343f23997" translate="yes" xml:space="preserve">
          <source>If int, the total number of points generated. If two-element tuple, number of points in each of two moons.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8faba5e55a8f0e899120109354364cac8c2354b" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;max_features&lt;/code&gt; features at each split.</source>
          <target state="translated">int이면 각 분할에서 &lt;code&gt;max_features&lt;/code&gt; 기능 을 고려 하십시오 .</target>
        </trans-unit>
        <trans-unit id="79438cfe8b8de1684467307814da6af61cdfe6ba" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_leaf&lt;/code&gt; as the minimum number.</source>
          <target state="translated">int이면 &lt;code&gt;min_samples_leaf&lt;/code&gt; 를 최소 숫자로 고려 하십시오 .</target>
        </trans-unit>
        <trans-unit id="69e04ca78560d3ef445be4d724f5c0cc8198187a" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_split&lt;/code&gt; as the minimum number.</source>
          <target state="translated">int이면 &lt;code&gt;min_samples_split&lt;/code&gt; 을 최소값으로 고려 하십시오 .</target>
        </trans-unit>
        <trans-unit id="a8d276c242fbe315ce14903af35e7ebf9a0c3619" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_features&lt;/code&gt; features.</source>
          <target state="translated">int이면 &lt;code&gt;max_features&lt;/code&gt; 기능을 그립니다 .</target>
        </trans-unit>
        <trans-unit id="0771ca4ef29dd427aac0ffda56943aa541e3af54" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_samples&lt;/code&gt; samples.</source>
          <target state="translated">int이면 &lt;code&gt;max_samples&lt;/code&gt; 샘플을 그립니다 .</target>
        </trans-unit>
        <trans-unit id="4430c154e22158c0d6435f75a2d640312ee73ab2" translate="yes" xml:space="preserve">
          <source>If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, \(u_1\) and \(v_1\). are discarded. From now on, the &amp;ldquo;first&amp;rdquo; singular vectors refers to \(u_2 \dots u_{p+1}\) and \(v_2 \dots v_{p+1}\) except in the case of log normalization.</source>
          <target state="translated">로그 정규화가 사용 된 경우 모든 특이 벡터가 의미가 있습니다. 그러나 독립적 인 정규화 또는 비 스트로크 화가 사용 된 경우 첫 번째 특이 벡터 \ (u_1 \) 및 \ (v_1 \). 폐기됩니다. 이제부터 &quot;첫 번째&quot;특이 벡터는 로그 정규화를 제외하고 \ (u_2 \ dots u_ {p + 1} \) 및 \ (v_2 \ dots v_ {p + 1} \)을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="f38434d38fce86523bd80aac7625c65019a5f868" translate="yes" xml:space="preserve">
          <source>If max_samples is larger than the number of samples provided, all samples will be used for all trees (no sampling).</source>
          <target state="translated">max_samples가 제공된 샘플 수보다 크면 모든 샘플이 모든 트리에 사용됩니다 (샘플링 없음).</target>
        </trans-unit>
        <trans-unit id="0512919782ceb898f5e82137605d37f1918f7fe8" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;auto&amp;rdquo;, the ratio of n_samples / n_population is used to determine which algorithm to use: If ratio is between 0 and 0.01, tracking selection is used. If ratio is between 0.01 and 0.99, numpy.random.permutation is used. If ratio is greater than 0.99, reservoir sampling is used. The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">method == &quot;auto&quot;인 경우 n_samples / n_population의 비율을 사용하여 사용할 알고리즘을 결정합니다. 비율이 0과 0.01 사이 인 경우 추적 선택이 사용됩니다. 비율이 0.01과 0.99 사이이면 numpy.random.permutation이 사용됩니다. 비율이 0.99보다 큰 경우 저수지 샘플링이 사용됩니다. 선택한 정수의 순서는 정의되어 있지 않습니다. 무작위 순서가 필요한 경우 선택한 하위 집합을 섞어 야합니다.</target>
        </trans-unit>
        <trans-unit id="aa3ae5990e2e00fef99c00cc07049cdbc9d8e931" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;pool&amp;rdquo;, a pool based algorithm is particularly fast, even faster than the tracking selection method. Hovewer, a vector containing the entire population has to be initialized. If n_samples ~ n_population, the reservoir sampling method is faster.</source>
          <target state="translated">method == &quot;pool&quot;인 경우 풀 기반 알고리즘이 추적 선택 방법보다 훨씬 빠릅니다. Hovewer, 전체 모집단을 포함하는 벡터를 초기화해야합니다. n_samples ~ n_population이면 저수지 샘플링 방법이 더 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="6916dcd6d6c8f00e1ac0ef865ab4c75ff38ebedf" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;reservoir_sampling&amp;rdquo;, a reservoir sampling algorithm is used which is suitable for high memory constraint or when O(&lt;code&gt;n_samples&lt;/code&gt;) ~ O(&lt;code&gt;n_population&lt;/code&gt;). The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">method ==&amp;ldquo;reservoir_sampling&amp;rdquo;인 경우 높은 메모리 제약 조건이나 O ( &lt;code&gt;n_samples&lt;/code&gt; ) ~ O ( &lt;code&gt;n_population&lt;/code&gt; )에 적합한 저장소 샘플링 알고리즘이 사용됩니다 . 선택한 정수의 순서는 정의되어 있지 않습니다. 무작위 순서가 필요한 경우 선택한 하위 집합을 섞어 야합니다.</target>
        </trans-unit>
        <trans-unit id="0dea8a6c91cef90e0430014d895bb3954c8fb19c" translate="yes" xml:space="preserve">
          <source>If method ==&amp;rdquo;tracking_selection&amp;rdquo;, a set based implementation is used which is suitable for &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt;.</source>
          <target state="translated">method ==&amp;rdquo;tracking_selection&amp;rdquo;이면 &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt; 에 적합한 세트 기반 구현이 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="b83a3c2dac1d5c17a6e580230ebdf136c940fa15" translate="yes" xml:space="preserve">
          <source>If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79ae0bba548597b9902c41c70fbd6bf9602e8534" translate="yes" xml:space="preserve">
          <source>If metric is &amp;lsquo;precomputed&amp;rsquo;, Y is ignored and X is returned.</source>
          <target state="translated">메트릭이 '사전 계산 된'경우 Y는 무시되고 X가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="ca8cb47e72e166fd730a116349e050254e6876d5" translate="yes" xml:space="preserve">
          <source>If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy&amp;rsquo;s metrics, but is less efficient than passing the metric name as a string.</source>
          <target state="translated">메트릭이 호출 가능한 함수 인 경우 각 인스턴스 쌍 (행)에서 호출되고 결과 값이 기록됩니다. 콜 러블은 두 개의 배열을 입력으로 받아서 이들 사이의 거리를 나타내는 하나의 값을 반환해야합니다. 이는 Scipy의 메트릭에 적용되지만 메트릭 이름을 문자열로 전달하는 것보다 덜 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="574d42008b369aefc553aab20dba12b6d233789b" translate="yes" xml:space="preserve">
          <source>If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy&amp;rsquo;s metrics, but is less efficient than passing the metric name as a string. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27305db22802f1bb3d3e2d60db076f6f0d275369" translate="yes" xml:space="preserve">
          <source>If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen.</source>
          <target state="translated">미니 배치 k- 평균을 사용하는 경우 최상의 초기화가 선택되고 알고리즘이 한 번 실행됩니다. 그렇지 않으면 각 초기화 및 선택한 최상의 솔루션에 대해 알고리즘이 실행됩니다.</target>
        </trans-unit>
        <trans-unit id="16e6684b95c26e373af21b6c0d2ea50b705c0505" translate="yes" xml:space="preserve">
          <source>If multioutput is &amp;lsquo;raw_values&amp;rsquo;, then mean absolute error is returned for each output separately. If multioutput is &amp;lsquo;uniform_average&amp;rsquo; or an ndarray of weights, then the weighted average of all output errors is returned.</source>
          <target state="translated">다중 출력이 'raw_values'인 경우 각 출력에 대해 개별적으로 절대 오류가 반환됩니다. 다중 출력이 'uniform_average'또는 가중치의 ndarray이면 모든 출력 오류의 가중 평균이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="5304821b08fca43ab85cd7593997416e8f601261" translate="yes" xml:space="preserve">
          <source>If neighbors_algorithm=&amp;rsquo;precomputed&amp;rsquo;, X is assumed to be a distance matrix or a sparse graph of shape (n_queries, n_samples_fit).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62bc17472717b60146346d5aed7f5c0278bbe8ae" translate="yes" xml:space="preserve">
          <source>If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="caf3d42023b133b9efdfbb493fc66df503092e71" translate="yes" xml:space="preserve">
          <source>If no scoring is specified and the estimator has no score function, we can either return None or raise an exception.</source>
          <target state="translated">스코어링이 지정되지 않고 추정기에 스코어 함수가없는 경우 None을 리턴하거나 예외를 발생시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5519c1c6825bf59bfd06c08f68bb64b64ee1a0ae" translate="yes" xml:space="preserve">
          <source>If no valid consensus set could be found. This occurs if &lt;code&gt;is_data_valid&lt;/code&gt; and &lt;code&gt;is_model_valid&lt;/code&gt; return False for all &lt;code&gt;max_trials&lt;/code&gt; randomly chosen sub-samples.</source>
          <target state="translated">유효한 합의 세트를 찾을 수없는 경우 이 경우 발생 &lt;code&gt;is_data_valid&lt;/code&gt; 및 &lt;code&gt;is_model_valid&lt;/code&gt; 모든 반환 거짓 &lt;code&gt;max_trials&lt;/code&gt; 무작위로 하위 샘플을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="e48a960f323664c14eed43108cfafa1159796090" translate="yes" xml:space="preserve">
          <source>If normalize is &lt;code&gt;True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.</source>
          <target state="translated">normalize가 &lt;code&gt;True&lt;/code&gt; 인 경우 오 분류의 소수 (float)를 반환하고 그렇지 않으면 오 분류 수 (int)를 반환합니다. 최상의 성능은 0입니다.</target>
        </trans-unit>
        <trans-unit id="66ebd47239b72bc82239183dacc1b3e58bfa41bb" translate="yes" xml:space="preserve">
          <source>If not &lt;code&gt;None&lt;/code&gt;, the standardized partial AUC &lt;a href=&quot;#r4bb7c4558997-2&quot; id=&quot;id1&quot;&gt;[2]&lt;/a&gt; over the range [0, max_fpr] is returned. For the multiclass case, &lt;code&gt;max_fpr&lt;/code&gt;, should be either equal to &lt;code&gt;None&lt;/code&gt; or &lt;code&gt;1.0&lt;/code&gt; as AUC ROC partial computation currently is not supported for multiclass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d93355ca0c397a92c0eb63483bbae0b531d00cf1" translate="yes" xml:space="preserve">
          <source>If not &lt;code&gt;None&lt;/code&gt;, the standardized partial AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt; over the range [0, max_fpr] is returned.</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; 이 아닌 경우 [0, max_fpr] 범위 에서 표준화 된 부분 AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3&lt;/a&gt; ]이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="954d968337062d6fae676f5915fb0dc48db9ccef" translate="yes" xml:space="preserve">
          <source>If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</source>
          <target state="translated">None이 아니라면, 말뭉치의 용어 빈도로 정렬 된 최고 max_features 만 고려하는 어휘를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="6b6dff5f6d294c2bdbfe5ee6b0ee56319193880c" translate="yes" xml:space="preserve">
          <source>If not None, data is split in a stratified fashion, using this as the class labels.</source>
          <target state="translated">None이 아닌 경우 데이터를 클래스 레이블로 사용하여 계층화 된 방식으로 데이터가 분할됩니다.</target>
        </trans-unit>
        <trans-unit id="d9fe4271c08ca870db7143f08e0938aa49f2d1d0" translate="yes" xml:space="preserve">
          <source>If not None, set the highest value of the fit to y_max.</source>
          <target state="translated">None이 아니라면 가장 큰 적합치를 y_max로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="3c138b5d1ed12eddb3226ed7535814059b7a615c" translate="yes" xml:space="preserve">
          <source>If not None, set the lowest value of the fit to y_min.</source>
          <target state="translated">None이 아니라면 가장 작은 적합치를 y_min으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="ebcf44116da09ed76a723aed5cadbe6d4ed2530d" translate="yes" xml:space="preserve">
          <source>If not None, this argument is passed as &lt;code&gt;sample_weight&lt;/code&gt; keyword argument to the &lt;code&gt;score&lt;/code&gt; method of the final estimator.</source>
          <target state="translated">None이 아닌 경우,이 인수는 &lt;code&gt;sample_weight&lt;/code&gt; 키워드 인수로 최종 추정기 의 &lt;code&gt;score&lt;/code&gt; 메소드에 전달됩니다 .</target>
        </trans-unit>
        <trans-unit id="3798f9f768af1129609b1d811ed41c3721cfae7d" translate="yes" xml:space="preserve">
          <source>If not None, this function is called after every iteration of the optimizer, taking as arguments the current solution (flattened transformation matrix) and the number of iterations. This might be useful in case one wants to examine or store the transformation found after each iteration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0f7d0b7263b16cf926e8314af8096b9ae6c9066" translate="yes" xml:space="preserve">
          <source>If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below).</source>
          <target state="translated">지정하지 않으면 대역폭은 sklearn.cluster.estimate_bandwidth; 확장성에 대한 힌트는 해당 기능의 설명서를 참조하십시오 (아래 참고 사항 참조).</target>
        </trans-unit>
        <trans-unit id="e77fe01e6cae9364473d1714c8219315178ecad2" translate="yes" xml:space="preserve">
          <source>If not provided, labels will be inferred from y_true. If &lt;code&gt;labels&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; has shape (n_samples,) the labels are assumed to be binary and are inferred from &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3fc57ade66d3b29b2e5dacfcee394aac7f4ec951" translate="yes" xml:space="preserve">
          <source>If not provided, labels will be inferred from y_true. If &lt;code&gt;labels&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; has shape (n_samples,) the labels are assumed to be binary and are inferred from &lt;code&gt;y_true&lt;/code&gt;. .. versionadded:: 0.18</source>
          <target state="translated">제공하지 않으면 레이블은 y_true에서 유추됩니다. 경우 &lt;code&gt;labels&lt;/code&gt; 없는 &lt;code&gt;None&lt;/code&gt; 과 &lt;code&gt;y_pred&lt;/code&gt; 형상 (N_SAMPLES은)을 가지는 라벨 진 것으로 가정으로부터 추론 &lt;code&gt;y_true&lt;/code&gt; . .. 버전 추가 :: 0.18</target>
        </trans-unit>
        <trans-unit id="d3a1f4e96f04c6f8dfd4835d50de53587903b2e7" translate="yes" xml:space="preserve">
          <source>If one-of-K coding is applied to categorical features, this will include the constructed feature names but not the original ones.</source>
          <target state="translated">K-one 코딩이 범주 형 피처에 적용되는 경우 생성 된 피처 이름은 포함되지만 원래 피처 이름은 포함되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="c3b8faf61102e14148418b48bf3dbb3389d54ef3" translate="yes" xml:space="preserve">
          <source>If only the diagonal of the auto-covariance is being used, the method &lt;code&gt;diag()&lt;/code&gt; of a kernel can be called, which is more computationally efficient than the equivalent call to &lt;code&gt;__call__&lt;/code&gt;: &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt;</source>
          <target state="translated">자동 공분산의 대각선 만 사용 하는 경우 커널의 &lt;code&gt;diag()&lt;/code&gt; 메서드를 호출 할 수 있으며 &lt;code&gt;__call__&lt;/code&gt; 에 대한 동등한 호출보다 계산 효율이 높습니다. . &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="db7f35e5fc1dd73c10c86bdccb4a2449d5a89ec7" translate="yes" xml:space="preserve">
          <source>If order is &amp;lsquo;random&amp;rsquo; a random ordering will be used.</source>
          <target state="translated">주문이 '무작위'인 경우 무작위 순서가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="f42275492b00fc14b5861ea85e0f4944992d0324" translate="yes" xml:space="preserve">
          <source>If passed, include the name of the estimator in warning messages.</source>
          <target state="translated">전달 된 경우 경고 메시지에 추정기 이름을 포함 시키십시오.</target>
        </trans-unit>
        <trans-unit id="908cd551a5eab6201799122746b2ad3d99f4a3d2" translate="yes" xml:space="preserve">
          <source>If positive, restrict regression coefficients to be positive</source>
          <target state="translated">양수이면 회귀 계수를 양수로 제한하십시오.</target>
        </trans-unit>
        <trans-unit id="66637d66644751acb1ce04342cdfce0be0ef5495" translate="yes" xml:space="preserve">
          <source>If provided, this parameter will override the choice of copy_X made at instance creation. If &lt;code&gt;True&lt;/code&gt;, X will be copied; else, it may be overwritten.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71021bee801a334b18f6587cd74d122911551ceb" translate="yes" xml:space="preserve">
          <source>If query_id is set to True, this will return instead [X1, y1, q1,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54d5c9aca2dfc54fbbbdb98327375a6f374bdf8f" translate="yes" xml:space="preserve">
          <source>If randomized :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5484f943f94af044829e2453a87ea1beff675d6" translate="yes" xml:space="preserve">
          <source>If return_costs is True, the objective function and dual gap at each iteration are returned.</source>
          <target state="translated">return_costs가 True이면 각 반복에서 목적 함수와 이중 간격이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="3a07f641c209d2556442bcd652915ffb4ab857db" translate="yes" xml:space="preserve">
          <source>If safe is false, clone will fall back to a deep copy on objects that are not estimators.</source>
          <target state="translated">safe가 false이면 clone은 추정값이 아닌 객체의 깊은 복사본으로 폴백됩니다.</target>
        </trans-unit>
        <trans-unit id="179d83839b7c246b21dd4fad6260ec3c338cc783" translate="yes" xml:space="preserve">
          <source>If seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError.</source>
          <target state="translated">seed가 None이면 np.random에서 사용하는 RandomState 싱글 톤을 반환합니다. seed가 int 인 경우 seed로 seed 된 새 RandomState 인스턴스를 리턴하십시오. seed가 이미 RandomState 인스턴스 인 경우이를 반환하십시오. 그렇지 않으면 ValueError를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="7108bbb3c9ecad70c2ad038e49ece7ce906a1c8f" translate="yes" xml:space="preserve">
          <source>If seq[i] is an int or a tuple with one int value, a one-way PDP is created; if seq[i] is a tuple of two ints, a two-way PDP is created. If feature_names is specified and seq[i] is an int, seq[i] must be &amp;lt; len(feature_names). If seq[i] is a string, feature_names must be specified, and seq[i] must be in feature_names.</source>
          <target state="translated">seq [i]가 int이거나 하나의 int 값을 가진 튜플 인 경우 단방향 PDP가 작성됩니다. seq [i]가 두 정수의 튜플이면 양방향 PDP가 생성됩니다. feature_names가 지정되고 seq [i]가 int 인 경우 seq [i]는 &amp;lt;len (feature_names)이어야합니다. seq [i]가 문자열이면 feature_names를 지정하고 seq [i]는 feature_names에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="4c1ce6df0b81e7680bbf5092a9535a5fa0cb37a8" translate="yes" xml:space="preserve">
          <source>If set to &amp;ldquo;warn&amp;rdquo;, this acts as 0, but warnings are also raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b01ea458e6ed79ec076f21dd79564eecc3f7a882" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4</source>
          <target state="translated">'random'으로 설정하면 랜덤 계수가 기본적으로 기능을 순차적으로 반복하지 않고 모든 반복마다 업데이트됩니다. 이것은 ( '무작위'로 설정) 특히 tol이 1e-4보다 높을 때 수렴 속도가 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="7a4374896942a67a58d05d59133607fc7483d7a2" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4.</source>
          <target state="translated">'random'으로 설정하면 랜덤 계수가 기본적으로 기능을 순차적으로 반복하지 않고 모든 반복마다 업데이트됩니다. 이것은 ( '무작위'로 설정) 특히 tol이 1e-4보다 높을 때 수렴 속도가 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="b3732982402454957d1d44e2700684220ba9b532" translate="yes" xml:space="preserve">
          <source>If set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to &lt;code&gt;fit&lt;/code&gt; as initialization for &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d15f7a009cd3b6e81a757534913f0edc7a2b7947" translate="yes" xml:space="preserve">
          <source>If set to True, forces coefficients to be positive. (Only allowed when &lt;code&gt;y.ndim == 1&lt;/code&gt;).</source>
          <target state="translated">True로 설정하면 계수가 양수가됩니다. (만 &lt;code&gt;y.ndim == 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6bd31584b0a279bb6a357ab195ba9534cb5cad4e" translate="yes" xml:space="preserve">
          <source>If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged.</source>
          <target state="translated">True로 설정하면 점수는 모든 접기에서 평균화되며 최고 점수에 해당하는 계수와 C가 취해지고이 매개 변수를 사용하여 최종 개조가 수행됩니다. 그렇지 않으면 폴드에서 최고 점수에 해당하는 계수, 절편 및 C가 평균화됩니다.</target>
        </trans-unit>
        <trans-unit id="f3d43f9f7c9e3af1ca6eddc0268b8ee91bb07ee3" translate="yes" xml:space="preserve">
          <source>If set, scikit-learn will attempt to limit the size of temporary arrays to this number of MiB (per job when parallelised), often saving both computation time and memory on expensive operations that can be performed in chunks. Global default: 1024.</source>
          <target state="translated">설정되면 scikit-learn은 임시 배열의 크기를이 MiB 수 (병렬화 작업 당)로 제한하려고 시도하여 종종 청크 단위로 수행 할 수있는 값 비싼 작업에서 계산 시간과 메모리를 절약합니다. 전역 기본값 : 1024</target>
        </trans-unit>
        <trans-unit id="cd9d66e1ab8be1fe689482ddb0cbca43b44a3950" translate="yes" xml:space="preserve">
          <source>If strictly positive, stop reading any new line of data once the position in the file has reached the (offset + length) bytes threshold.</source>
          <target state="translated">엄격하게 양수이면 파일의 위치가 (오프셋 + 길이) 바이트 임계 값에 도달하면 새로운 데이터 행을 읽지 마십시오.</target>
        </trans-unit>
        <trans-unit id="af99c20b0f1015ebcecc8bfb6a50ca848ab08d15" translate="yes" xml:space="preserve">
          <source>If string, specifies the path that will contain the data. If file-like, data will be written to f. f should be opened in binary mode.</source>
          <target state="translated">문자열 인 경우 데이터를 포함 할 경로를 지정합니다. 파일과 같은 경우 데이터가 f에 기록됩니다. f는 이진 모드로 열어야합니다.</target>
        </trans-unit>
        <trans-unit id="d25cbbfb18995acebbdc8e788e3994e16b21b8ae" translate="yes" xml:space="preserve">
          <source>If sum_over_features is False shape is (n_samples_X * n_samples_Y, n_features) and D contains the componentwise L1 pairwise-distances (ie. absolute difference), else shape is (n_samples_X, n_samples_Y) and D contains the pairwise L1 distances.</source>
          <target state="translated">sum_over_features가 False 인 모양이 (n_samples_X * n_samples_Y, n_features)이고 D가 성분 별 L1 쌍별 거리 (즉, 절대 차이)를 포함하는 경우 모양은 (n_samples_X, n_samples_Y)이고 D는 쌍별 L1 거리를 포함합니다.</target>
        </trans-unit>
        <trans-unit id="1979731cc29c616c5ac5593ab888192599b2d46b" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;loss&lt;/code&gt; does not support probabilities.</source>
          <target state="translated">&lt;code&gt;loss&lt;/code&gt; 이 확률을 지원하지 않는 경우 .</target>
        </trans-unit>
        <trans-unit id="41f96f9118cd39448d94e68aca8aa6d327b368ef" translate="yes" xml:space="preserve">
          <source>If the algorithm is &amp;ldquo;deflation&amp;rdquo;, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge.</source>
          <target state="translated">알고리즘이 &quot;deflation&quot;인 경우 n_iter는 모든 구성 요소에서 실행되는 최대 반복 횟수입니다. 그렇지 않으면 그것들은 수렴하기 위해 반복되는 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="b72ab6a8a780a6da86f798b7381c54dc2236b80c" translate="yes" xml:space="preserve">
          <source>If the algorithm stops before fully converging (because of &lt;code&gt;tol&lt;/code&gt; of &lt;code&gt;max_iter&lt;/code&gt;), &lt;code&gt;labels_&lt;/code&gt; and &lt;code&gt;means_&lt;/code&gt; will not be consistent, i.e. the &lt;code&gt;means_&lt;/code&gt; will not be the means of the points in each cluster. Also, the estimator will reassign &lt;code&gt;labels_&lt;/code&gt; after the last iteration to make &lt;code&gt;labels_&lt;/code&gt; consistent with &lt;code&gt;predict&lt;/code&gt; on the training set.</source>
          <target state="translated">알고리즘은 이전에 완전히 (때문에의 수렴을 중지하는 경우 &lt;code&gt;tol&lt;/code&gt; 의 &lt;code&gt;max_iter&lt;/code&gt; ), &lt;code&gt;labels_&lt;/code&gt; 및 &lt;code&gt;means_&lt;/code&gt; 가 일치하지 않을 것, 즉 &lt;code&gt;means_&lt;/code&gt; 는 각 클러스터에있는 점의 수단이 될 수 없습니다. 또한 추정기는 마지막 반복 이후에 &lt;code&gt;labels_&lt;/code&gt; 을 재 할당 하여 &lt;code&gt;labels_&lt;/code&gt; 훈련 세트에 대한 &lt;code&gt;predict&lt;/code&gt; 과 일관성있게 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="12008b7aa6dad452411115d8236f3a855fd0fea9" translate="yes" xml:space="preserve">
          <source>If the algorithm stops before fully converging (because of &lt;code&gt;tol&lt;/code&gt; or &lt;code&gt;max_iter&lt;/code&gt;), &lt;code&gt;labels_&lt;/code&gt; and &lt;code&gt;cluster_centers_&lt;/code&gt; will not be consistent, i.e. the &lt;code&gt;cluster_centers_&lt;/code&gt; will not be the means of the points in each cluster. Also, the estimator will reassign &lt;code&gt;labels_&lt;/code&gt; after the last iteration to make &lt;code&gt;labels_&lt;/code&gt; consistent with &lt;code&gt;predict&lt;/code&gt; on the training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4043c787702cc081c41f25b031d69ea98cf35c42" translate="yes" xml:space="preserve">
          <source>If the array is not symmetric, then a symmetrized version is returned. Optionally, a warning or exception is raised if the matrix is not symmetric.</source>
          <target state="translated">배열이 대칭이 아닌 경우 대칭 버전이 반환됩니다. 선택적으로, 행렬이 대칭이 아닌 경우 경고 또는 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="d4238935d45ce51eea0be6c47146a609e05c26ed" translate="yes" xml:space="preserve">
          <source>If the attributes are not found.</source>
          <target state="translated">속성을 찾을 수없는 경우</target>
        </trans-unit>
        <trans-unit id="10f86bc0a8ef8d94dd88200305e21d6ac290743f" translate="yes" xml:space="preserve">
          <source>If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions).</source>
          <target state="translated">분류 기가 두 클래스에서 모두 동일하게 잘 수행되면이 용어는 일반적인 정확도 (즉, 정확한 예측 수를 총 예측 수로 나눈 값)로 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="9d0651dbf433477af9dfe8c9482b03c0b28a7aea" translate="yes" xml:space="preserve">
          <source>If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross- validation result. However, the opposite may be true if the samples are not independently and identically distributed. For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples.</source>
          <target state="translated">데이터 순서가 임의적이지 않은 경우 (예 : 동일한 클래스 레이블을 가진 샘플이 연속적 임) 의미있는 교차 검증 결과를 얻으려면 먼저 순서를 섞어 야합니다. 그러나 샘플이 독립적으로 동일하게 분포되어 있지 않은 경우에는 그 반대 일 수 있습니다. 예를 들어, 샘플이 뉴스 기사에 해당하고 출판 시점에 따라 주문 된 경우 데이터를 섞으면 과적 합 모델과 검증 점수가 비정상적으로 증가 할 수 있습니다. 인공적으로 유사한 샘플에서 테스트됩니다 (닫기 훈련 시간).</target>
        </trans-unit>
        <trans-unit id="4fdc5debb409dcec7a673c288152f0ef6e4738ef" translate="yes" xml:space="preserve">
          <source>If the default value is passed, then &lt;code&gt;keepdims&lt;/code&gt; will not be passed through to the &lt;code&gt;mean&lt;/code&gt; method of sub-classes of &lt;code&gt;ndarray&lt;/code&gt;, however any non-default value will be. If the sub-class&amp;rsquo; method does not implement &lt;code&gt;keepdims&lt;/code&gt; any exceptions will be raised.</source>
          <target state="translated">기본 값이 전달되면, &lt;code&gt;keepdims&lt;/code&gt; 는 받는 사람을 통해 전달되지 않습니다 &lt;code&gt;mean&lt;/code&gt; 의 하위 클래스의 방법 &lt;code&gt;ndarray&lt;/code&gt; 그러나이 아닌 디폴트 값은 될 것이다. 하위 클래스의 메소드가 &lt;code&gt;keepdims&lt;/code&gt; 를 구현하지 않는 경우 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="322da3aec4cc8f30cb7592a5692203180007e581" translate="yes" xml:space="preserve">
          <source>If the degree is 2 or 3, the method described in &amp;ldquo;Leveraging Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices Using K-Simplex Numbers&amp;rdquo; by Andrew Nystrom and John Hughes is used, which is much faster than the method used on CSC input. For this reason, a CSC input will be converted to CSR, and the output will be converted back to CSC prior to being returned, hence the preference of CSR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca5777d1057fb92ff835301c12f93dc71bd51069" translate="yes" xml:space="preserve">
          <source>If the difference between the current prediction and the correct label is below this threshold, the model is not updated.</source>
          <target state="translated">현재 예측과 올바른 레이블의 차이가이 임계 값보다 낮 으면 모델이 업데이트되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="54f187a0c12dbeb2b22455f8308653334a568512" translate="yes" xml:space="preserve">
          <source>If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes.</source>
          <target state="translated">추정기가 증분 학습을 지원하는 경우 다른 학습 세트 크기에 맞게 속도를 높이는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="28446974a089033b0f005a14dd2e5e7cde0d019d" translate="yes" xml:space="preserve">
          <source>If the file does not exist yet, it is downloaded from mldata.org .</source>
          <target state="translated">파일이 아직 없으면 mldata.org에서 다운로드됩니다.</target>
        </trans-unit>
        <trans-unit id="3377386ec971b5f97505ad0b0efacd641307a6b2" translate="yes" xml:space="preserve">
          <source>If the folder does not already exist, it is automatically created.</source>
          <target state="translated">폴더가 없으면 자동으로 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="bcf86cd76452a354a39a384d6cc008f0521fadc0" translate="yes" xml:space="preserve">
          <source>If the gradient norm is below this threshold, the optimization will be stopped.</source>
          <target state="translated">그래디언트 표준이이 임계 값보다 낮 으면 최적화가 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="aaea0ac91de1101ebb5583d72a39edadc546a9ed" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (&lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt;&lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt;&lt;/a&gt;) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:</source>
          <target state="translated">기본 진리 레이블을 모르는 경우 모델 자체를 사용하여 평가를 수행해야합니다. Silhouette Coefficient ( &lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt; &lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt; &lt;/a&gt; )는 이러한 평가의 예이며, Silhouette Coefficient 점수가 높을수록 클러스터가 더 잘 정의 된 모델과 관련됩니다. Silhouette Coefficient는 각 샘플에 대해 정의되며 두 개의 점수로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="f86f2c18ff62eced8e4c69ce5c4a60d3487f70e2" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Calinski-Harabasz index (&lt;a href=&quot;generated/sklearn.metrics.calinski_harabasz_score#sklearn.metrics.calinski_harabasz_score&quot;&gt;&lt;code&gt;sklearn.metrics.calinski_harabasz_score&lt;/code&gt;&lt;/a&gt;) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabasz score relates to a model with better defined clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb3f5944370bdcf362ff4bffb46e8bf1ded41ef1" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Calinski-Harabaz index (&lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt;&lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt;&lt;/a&gt;) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabaz score relates to a model with better defined clusters.</source>
          <target state="translated">기본 진리 레이블을 알 수없는 경우 분산 비율 기준이라고도 하는 Calinski-Harabaz 지수 ( &lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt; &lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt; &lt;/a&gt; )를 사용하여 모델을 평가할 수 있습니다. 여기서 Calinski-Harabaz 점수가 높은 모델은 더 잘 정의 된 클러스터.</target>
        </trans-unit>
        <trans-unit id="a4a519d35f95c18e319df7e8878b98f013f9bd44" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Davies-Bouldin index (&lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt;&lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt;&lt;/a&gt;) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters.</source>
          <target state="translated">기본 진리 레이블을 알 수없는 경우 Davies-Bouldin 지수 ( &lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt; &lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt; &lt;/a&gt; )를 사용하여 모형을 평가할 수 있습니다. 여기서 Davies-Bouldin 지수가 낮을수록 군집간에 더 잘 분리 된 모형과 관련됩니다.</target>
        </trans-unit>
        <trans-unit id="7ce6861d9ec6948a6bc8aef858e97abae7ed0654" translate="yes" xml:space="preserve">
          <source>If the input is a sparse matrix, only the non-zero values are subject to update by the Binarizer class.</source>
          <target state="translated">입력이 희소 행렬 인 경우 Binarizer 클래스는 0이 아닌 값만 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="c10a8d9d8c40f9f50dafe36b727f5b47e7075f19" translate="yes" xml:space="preserve">
          <source>If the input matrix X is very sparse, it is recommended to convert to sparse &lt;code&gt;csc_matrix&lt;/code&gt; before calling fit and sparse &lt;code&gt;csr_matrix&lt;/code&gt; before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.</source>
          <target state="translated">입력 행렬 X가 매우 드문 경우 fit 및 sparse &lt;code&gt;csr_matrix&lt;/code&gt; 를 호출하기 전에 스파 스 &lt;code&gt;csc_matrix&lt;/code&gt; 로 변환하는 것이 좋습니다. 예측 호출하기 전에. 피처가 대부분의 샘플에서 값이 0 인 경우 밀도가 높은 매트릭스에 비해 희소 매트릭스 입력에 대해 훈련 시간이 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="661cb29a3f5fe68fda8ea5f8c53273efb7753ce1" translate="yes" xml:space="preserve">
          <source>If the labels are encoded with +1 and -1, \(y\): is the true value, and \(w\) is the predicted decisions as output by &lt;code&gt;decision_function&lt;/code&gt;, then the hinge loss is defined as:</source>
          <target state="translated">레이블이 +1 및 -1로 인코딩 된 경우 \ (y \) :는 true 값이고 \ (w \)는 decision_function에 의해 출력 된 예측 &lt;code&gt;decision_function&lt;/code&gt; 후 힌지 손실은 다음과 같이 정의된다 :</target>
        </trans-unit>
        <trans-unit id="30e7605353fedb22ff0c25c7418abbab28137e70" translate="yes" xml:space="preserve">
          <source>If the loss on a sample is greater than the &lt;code&gt;residual_threshold&lt;/code&gt;, then this sample is classified as an outlier.</source>
          <target state="translated">샘플의 손실이보다 큰 경우 &lt;code&gt;residual_threshold&lt;/code&gt; ,이 샘플은 이상치 (outlier)로 분류된다.</target>
        </trans-unit>
        <trans-unit id="fd47c1f065810b1b45f0d8d994aa0a4ff29505d4" translate="yes" xml:space="preserve">
          <source>If the metric constructor parameter is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be the distance matrix between the data to be predicted and &lt;code&gt;self.centroids_&lt;/code&gt;.</source>
          <target state="translated">메트릭 생성자 매개 변수가 &quot;사전 계산 된&quot;경우 X는 예측할 데이터와 &lt;code&gt;self.centroids_&lt;/code&gt; 사이의 거리 행렬로 간주됩니다 .</target>
        </trans-unit>
        <trans-unit id="c8476d320358236ab03a9b2e5775d6207658d4f7" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row.</source>
          <target state="translated">메트릭이 '사전 계산 된'경우 X는 제곱 거리 행렬이어야합니다. 그렇지 않으면 행당 샘플이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="ed2846275337b6c05f70ce353bc177c3fd2fc1b0" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row. If the method is &amp;lsquo;exact&amp;rsquo;, X may be a sparse matrix of type &amp;lsquo;csr&amp;rsquo;, &amp;lsquo;csc&amp;rsquo; or &amp;lsquo;coo&amp;rsquo;.</source>
          <target state="translated">메트릭이 '사전 계산 된'경우 X는 제곱 거리 행렬이어야합니다. 그렇지 않으면 행당 샘플이 포함됩니다. 방법이 '정확한'이면 X는 'csr', 'csc'또는 'coo'유형의 희소 행렬 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3a27e115b0c223dd4eebc69d8c1ee49334684c4e" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row. If the method is &amp;lsquo;exact&amp;rsquo;, X may be a sparse matrix of type &amp;lsquo;csr&amp;rsquo;, &amp;lsquo;csc&amp;rsquo; or &amp;lsquo;coo&amp;rsquo;. If the method is &amp;lsquo;barnes_hut&amp;rsquo; and the metric is &amp;lsquo;precomputed&amp;rsquo;, X may be a precomputed sparse graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be2b4ccc2ee21bcc622b72ad8a09e29905f86d22" translate="yes" xml:space="preserve">
          <source>If the number of features is \(p\), you now require \(n \sim 1/d^p\) points. Let&amp;rsquo;s say that we require 10 points in one dimension: now \(10^p\) points are required in \(p\) dimensions to pave the \([0, 1]\) space. As \(p\) becomes large, the number of training points required for a good estimator grows exponentially.</source>
          <target state="translated">기능의 수가 \ (p \)이면 이제 \ (n \ sim 1 / d ^ p \) 포인트가 필요합니다. 한 차원에 10 개의 점이 필요하다고 가정 해 봅시다. 이제 \ ([0, 1] \) 공간을 넓히려면 \ (p \) 차원에 \ (10 ​​^ p \) 점이 필요합니다. \ (p \)가 커짐에 따라, 좋은 견적에 필요한 훈련 포인트의 수가 기하 급수적으로 증가합니다.</target>
        </trans-unit>
        <trans-unit id="31f6fadae8fdb5e25318685f0dd36c90a3234b90" translate="yes" xml:space="preserve">
          <source>If the number of features is much greater than the number of samples, avoid over-fitting in choosing &lt;a href=&quot;#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt; and regularization term is crucial.</source>
          <target state="translated">피처 수가 샘플 수보다 훨씬 많은 경우 &lt;a href=&quot;#svm-kernels&quot;&gt;커널 기능&lt;/a&gt; 을 선택할 때 과적 합을 피하고 정규화 용어가 중요합니다.</target>
        </trans-unit>
        <trans-unit id="421f2017551080d266c05ad587f0ba1ecff6bff8" translate="yes" xml:space="preserve">
          <source>If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans.</source>
          <target state="translated">데이터 인스턴스 수를 줄이거 나 전처리 단계 또는 기타 방법으로 많은 수의 하위 클러스터를 원할 경우 Birch가 MiniBatchKMeans보다 유용합니다.</target>
        </trans-unit>
        <trans-unit id="0169ea68b458a3b315ac7acca32e943f8bdb1bed" translate="yes" xml:space="preserve">
          <source>If the option chosen is &amp;lsquo;ovr&amp;rsquo;, then a binary problem is fit for each label. For &amp;lsquo;multinomial&amp;rsquo; the loss minimised is the multinomial loss fit across the entire probability distribution, &lt;em&gt;even when the data is binary&lt;/em&gt;. &amp;lsquo;multinomial&amp;rsquo; is unavailable when solver=&amp;rsquo;liblinear&amp;rsquo;. &amp;lsquo;auto&amp;rsquo; selects &amp;lsquo;ovr&amp;rsquo; if the data is binary, or if solver=&amp;rsquo;liblinear&amp;rsquo;, and otherwise selects &amp;lsquo;multinomial&amp;rsquo;.</source>
          <target state="translated">선택한 옵션이 'ovr'이면 이진 문제가 각 레이블에 적합합니다. '다항식'의 경우 최소화 된 손실 &lt;em&gt;은 데이터가 이진 인 경우에도&lt;/em&gt; 전체 확률 분포에 맞는 다항식 손실 입니다. solver = 'liblinear'인 경우 '다항식'을 사용할 수 없습니다. 'auto'는 데이터가 이진이거나 'solver ='liblinear '인 경우'ovr '을 선택하고 그렇지 않으면'다항식 '을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="e56253ed1e137739144617c8f91af1433e314b57" translate="yes" xml:space="preserve">
          <source>If the output of the different transformers contains sparse matrices, these will be stacked as a sparse matrix if the overall density is lower than this value. Use &lt;code&gt;sparse_threshold=0&lt;/code&gt; to always return dense. When the transformed output consists of all dense data, the stacked result will be dense, and this keyword will be ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ff404c7c7b751e78382edbf95c1647019d00b38" translate="yes" xml:space="preserve">
          <source>If the parameter&amp;rsquo;s type does not match the desired type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74c8a6dddf7307f96841c8e0079e93341eb05526" translate="yes" xml:space="preserve">
          <source>If the parameter&amp;rsquo;s value violates the given bounds.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb7bde2a134f67333f646adb3cb422fdb3b0a619" translate="yes" xml:space="preserve">
          <source>If the prediction task is to classify the observations in a set of finite labels, in other words to &amp;ldquo;name&amp;rdquo; the objects observed, the task is said to be a &lt;strong&gt;classification&lt;/strong&gt; task. On the other hand, if the goal is to predict a continuous target variable, it is said to be a &lt;strong&gt;regression&lt;/strong&gt; task.</source>
          <target state="translated">예측 작업이 관측치를 유한 레이블 집합으로 분류하는 것, 즉 관찰 된 대상을 &quot;이름 지정&quot;하는 경우 작업은 &lt;strong&gt;분류&lt;/strong&gt; 작업이라고합니다. 반면에 목표가 연속적인 목표 변수를 예측하는 것이라면 &lt;strong&gt;회귀&lt;/strong&gt; 작업이라고합니다.</target>
        </trans-unit>
        <trans-unit id="c305135e1b17987e86652a7910deafacf0f5dc9d" translate="yes" xml:space="preserve">
          <source>If the pyamg package is installed, it is used: this greatly speeds up computation.</source>
          <target state="translated">pyamg 패키지가 설치되면 사용됩니다 : 계산 속도가 크게 향상됩니다.</target>
        </trans-unit>
        <trans-unit id="04f07265ff7d7b70145be5aec52784e1b24d6f09" translate="yes" xml:space="preserve">
          <source>If the radius of the subcluster obtained by merging the new sample and the nearest subcluster is greater than the square of the threshold and if the number of subclusters is greater than the branching factor, then a space is temporarily allocated to this new sample. The two farthest subclusters are taken and the subclusters are divided into two groups on the basis of the distance between these subclusters.</source>
          <target state="translated">새 샘플과 가장 가까운 서브 클러스터를 병합하여 얻은 서브 클러스터의 반지름이 임계 값의 제곱보다 크고 서브 클러스터 수가 분기 계수보다 크면이 새 샘플에 공간이 임시로 할당됩니다. 가장 먼 2 개의 서브 클러스터가 취해지고 서브 클러스터는 이들 서브 클러스터 사이의 거리에 기초하여 2 개의 그룹으로 분할된다.</target>
        </trans-unit>
        <trans-unit id="41eef1b8a501219131b2619b097a82294e78c28a" translate="yes" xml:space="preserve">
          <source>If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt;, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.</source>
          <target state="translated">샘플에 가중치가 부여되면 &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt; 와 같은 가중치 기반 사전 정리 기준을 사용하여 트리 구조를 최적화하기가 쉬워 리프 노드에 전체 샘플 가중치 합계의 적어도 일부가 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="0c1baaebbfab363e25ace0c6b6ee91539fab116b" translate="yes" xml:space="preserve">
          <source>If the selected solver is &amp;lsquo;L-BFGS&amp;rsquo;, training does not support online nor mini-batch learning.</source>
          <target state="translated">선택한 솔버가 'L-BFGS'인 경우 교육은 온라인 또는 미니 배치 학습을 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="6b79b273c5ccf0e85d810ff5a4ad56e9102a13be" translate="yes" xml:space="preserve">
          <source>If the target is a continuous value, then for node \(m\), representing a region \(R_m\) with \(N_m\) observations, common criteria to minimise as for determining locations for future splits are Mean Squared Error, which minimizes the L2 error using mean values at terminal nodes, and Mean Absolute Error, which minimizes the L1 error using median values at terminal nodes.</source>
          <target state="translated">대상이 연속 값인 경우 \ (N_m \) 관측 값으로 영역 \ (R_m \)을 나타내는 노드 \ (m \)의 경우 향후 분할 위치를 결정할 때 최소화하는 일반적인 기준은 평균 제곱 오차입니다. 터미널 노드에서 평균값을 사용하여 L2 오류를 최소화하고 터미널 절대 값에서 중간 값을 사용하여 L1 오류를 최소화하는 평균 절대 오류.</target>
        </trans-unit>
        <trans-unit id="43d2fac91a90af78192c1e9cb5f608e633304262" translate="yes" xml:space="preserve">
          <source>If the target values \(y\) are counts (non-negative integer valued) or relative frequencies (non-negative), you might use a Poisson deviance with log-link.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25b233fcf730d262746a1d2c5c4f20ca63e89053" translate="yes" xml:space="preserve">
          <source>If the target values are positive valued and skewed, you might try a Gamma deviance with log-link.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73b808b1b5912c4df231c6d6d8790651299cc5e0" translate="yes" xml:space="preserve">
          <source>If the target values seem to be heavier tailed than a Gamma distribution, you might try an Inverse Gaussian deviance (or even higher variance powers of the Tweedie family).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd360b0d17d9758c91116a373249c96c3c493365" translate="yes" xml:space="preserve">
          <source>If the text is in a mish-mash of encodings that is simply too hard to sort out (which is the case for the 20 Newsgroups dataset), you can fall back on a simple single-byte encoding such as &lt;code&gt;latin-1&lt;/code&gt;. Some text may display incorrectly, but at least the same sequence of bytes will always represent the same feature.</source>
          <target state="translated">텍스트가 정렬하기 너무 어려운 인코딩의 혼란에 빠지면 (20 뉴스 그룹 데이터 세트의 경우) &lt;code&gt;latin-1&lt;/code&gt; 과 같은 간단한 단일 바이트 인코딩으로 대체 할 수 있습니다 . 일부 텍스트는 잘못 표시 될 수 있지만 최소한 동일한 바이트 시퀀스는 항상 동일한 기능을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="21286b430a50cd4c4f50df67c996d4b5b475416f" translate="yes" xml:space="preserve">
          <source>If the text you are loading is not actually encoded with UTF-8, however, you will get a &lt;code&gt;UnicodeDecodeError&lt;/code&gt;. The vectorizers can be told to be silent about decoding errors by setting the &lt;code&gt;decode_error&lt;/code&gt; parameter to either &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; or &lt;code&gt;&quot;replace&quot;&lt;/code&gt;. See the documentation for the Python function &lt;code&gt;bytes.decode&lt;/code&gt; for more details (type &lt;code&gt;help(bytes.decode)&lt;/code&gt; at the Python prompt).</source>
          <target state="translated">그러나로드하는 텍스트가 실제로 UTF-8로 인코딩되지 않은 경우 &lt;code&gt;UnicodeDecodeError&lt;/code&gt; 가 발생 합니다. 벡터 &lt;code&gt;decode_error&lt;/code&gt; 는 decode_error 매개 변수를 &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; 또는 &lt;code&gt;&quot;replace&quot;&lt;/code&gt; 로 설정하여 디코딩 오류에 대해 침묵하도록 지시 할 수 있습니다 . 자세한 내용은 Python 함수 &lt;code&gt;bytes.decode&lt;/code&gt; 설명서 를 참조하십시오 ( Python 프롬프트에서 &lt;code&gt;help(bytes.decode)&lt;/code&gt; 를 입력하십시오 ).</target>
        </trans-unit>
        <trans-unit id="13a9f813159d9e6258a164870cb1dc6a301dddd5" translate="yes" xml:space="preserve">
          <source>If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary the parameter \(\gamma\) of an SVM on the digits dataset.</source>
          <target state="translated">훈련 점수와 유효성 검사 점수가 모두 낮 으면 추정기가 적합하지 않습니다. 훈련 점수가 높고 유효성 검사 점수가 낮 으면 견적자가 과적 합하고 그렇지 않으면 매우 잘 작동합니다. 낮은 교육 점수와 높은 검증 점수는 일반적으로 불가능합니다. 세 가지 경우 모두 아래 그림에서 찾을 수 있습니다. 여기서 숫자 데이터 세트에서 SVM의 매개 변수 \ (\ gamma \)를 변경합니다.</target>
        </trans-unit>
        <trans-unit id="80e789647361ff21671194a00300fe312dc530d2" translate="yes" xml:space="preserve">
          <source>If the transformed output consists of a mix of sparse and dense data, it will be stacked as a sparse matrix if the density is lower than this value. Use &lt;code&gt;sparse_threshold=0&lt;/code&gt; to always return dense. When the transformed output consists of all sparse or all dense data, the stacked result will be sparse or dense, respectively, and this keyword will be ignored.</source>
          <target state="translated">변환 된 출력이 희소 데이터와 밀도가 높은 데이터로 구성된 경우 밀도가이 값보다 낮 으면 희소 행렬로 누적됩니다. 항상 밀도를 리턴 하려면 &lt;code&gt;sparse_threshold=0&lt;/code&gt; 을 사용하십시오 . 변환 된 출력이 모든 희소 데이터 또는 모든 조밀 한 데이터로 구성된 경우 누적 결과는 각각 희소 또는 조밀하며이 키워드는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="efca83041c066057e65d83989c4190b74b69dba6" translate="yes" xml:space="preserve">
          <source>If the underlying graph has nodes with much more connections than the average node, the algorithm will miss some of these connections.</source>
          <target state="translated">기본 그래프에 평균 노드보다 훨씬 많은 연결이있는 노드가있는 경우 알고리즘에서 이러한 연결 중 일부가 누락됩니다.</target>
        </trans-unit>
        <trans-unit id="27f470c8c1d74aa01f92e63860f4d2b9149dd0bb" translate="yes" xml:space="preserve">
          <source>If there are few data points per dimension, noise in the observations induces high variance:</source>
          <target state="translated">차원 당 데이터 포인트가 거의없는 경우 관측치의 노이즈가 높은 분산을 유발합니다.</target>
        </trans-unit>
        <trans-unit id="fbaec65eed1139944f6f906201326eaef7bc4d08" translate="yes" xml:space="preserve">
          <source>If there are more than two classes, \(f(x)\) itself would be a vector of size (n_classes,). Instead of passing through logistic function, it passes through the softmax function, which is written as,</source>
          <target state="translated">클래스가 두 개 이상인 경우 \ (f (x) \) 자체는 크기 (n_classes)의 벡터가됩니다. 로지스틱 함수를 통과하는 대신 softmax 함수를 통과합니다.</target>
        </trans-unit>
        <trans-unit id="e2c9b002eac60ce02e4a3cafb47196266855c43e" translate="yes" xml:space="preserve">
          <source>If there are more than two labels, &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; uses a multiclass variant due to Crammer &amp;amp; Singer. &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;Here&lt;/a&gt; is the paper describing it.</source>
          <target state="translated">레이블이 두 개 이상인 경우 &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; 은 Crammer &amp;amp; Singer로 인해 멀티 클래스 변형을 사용합니다. &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;여기&lt;/a&gt; 에 그것을 설명하는 논문이 있습니다.</target>
        </trans-unit>
        <trans-unit id="362b6e0ad023f937918b9ce5c294c8243f2c8ea1" translate="yes" xml:space="preserve">
          <source>If there is a possibility that the training data might have missing categorical features, it can often be better to specify &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; instead of setting the &lt;code&gt;categories&lt;/code&gt; manually as above. When &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is specified and unknown categories are encountered during transform, no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros (&lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is only supported for one-hot encoding):</source>
          <target state="translated">훈련 데이터에 범주 형 기능이 없을 수있는 경우 , 위와 같이 수동으로 &lt;code&gt;categories&lt;/code&gt; 를 설정하는 대신 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 를 지정하는 것이 좋습니다 . 때 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 지정되고 알 수없는 범주를 변환하는 동안 발생하는, 오류가 발생되지 않습니다하지만이 기능의 결과로 하나의 뜨거운 인코딩 된 열 (모두 제로가 될 것입니다 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 하나의 뜨거운 인코딩 지원 ) :</target>
        </trans-unit>
        <trans-unit id="76c3aee0f8dcd7757eeddd2a91b271bc2108fb17" translate="yes" xml:space="preserve">
          <source>If there is more than one such value, only the first is returned. The bin-count for the modal bins is also returned.</source>
          <target state="translated">그러한 값이 두 개 이상인 경우 첫 번째 값만 반환됩니다. 모달 빈의 빈 개수도 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="e4599c6e53b844db2376ed9e56ed7b49e63c6ec3" translate="yes" xml:space="preserve">
          <source>If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.</source>
          <target state="translated">이것이 int의 튜플 인 경우 이전과 같이 단일 축 또는 모든 축 대신 여러 축에 대해 평균이 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="015e500928e7c3f86f2c9b5121c746246fcd7a9f" translate="yes" xml:space="preserve">
          <source>If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.</source>
          <target state="translated">이것이 True로 설정되면 축소 된 축은 결과적으로 크기가 1 인 치수로 남습니다. 이 옵션을 사용하면 결과가 입력 배열에 대해 올바르게 브로드 캐스트됩니다.</target>
        </trans-unit>
        <trans-unit id="1d47783a4de427039b4db643f305b3dd195e7ba2" translate="yes" xml:space="preserve">
          <source>If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two. If there is no room, then this node is again split into two and the process is continued recursively, till it reaches the root.</source>
          <target state="translated">이 분할 노드에 상위 하위 클러스터가 있고 새 하위 클러스터를위한 공간이 있으면 상위가 두 개로 분할됩니다. 공간이 없으면이 노드는 다시 두 개로 분할되고 프로세스는 루트에 도달 할 때까지 재귀 적으로 계속됩니다.</target>
        </trans-unit>
        <trans-unit id="012f5a7e85e6e4a424dae20b5f0c103c4fa516ee" translate="yes" xml:space="preserve">
          <source>If true (default), use a breadth-first approach to the problem. Otherwise use a depth-first approach.</source>
          <target state="translated">true 인 경우 (기본값) 문제에 폭 우선 접근 방식을 사용하십시오. 그렇지 않으면 깊이 우선 접근 방식을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="5f57fd46a52f081b17c9c11ebb2ef30582e94549" translate="yes" xml:space="preserve">
          <source>If true the classification weights will be exported on each leaf. The classification weights are the number of samples each class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cec7c6586dbaf8eb8cda6633d2caddbd361cde5b" translate="yes" xml:space="preserve">
          <source>If true, &lt;code&gt;decision_function_shape='ovr'&lt;/code&gt;, and number of classes &amp;gt; 2, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt; will break ties according to the confidence values of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;; otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8002efc50268110232cc0ffbdad97c50440caadd" translate="yes" xml:space="preserve">
          <source>If true, X and y will be centered.</source>
          <target state="translated">참이면 X와 y가 중앙에 위치합니다.</target>
        </trans-unit>
        <trans-unit id="de73b79cb6d725781d21d3fce59be150e3f40a4c" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. Ignored if seeds argument is not None.</source>
          <target state="translated">true 인 경우 초기 커널 위치는 모든 포인트의 위치가 아니라 이산 된 포인트 버전의 위치입니다. 이 옵션을 True로 설정하면 더 적은 시드가 초기화되므로 알고리즘 속도가 빨라집니다. 씨앗 인수가 없음이 아닌 경우 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="8e21ac3d3be6bbf299e61e387ddc28e4aa0e4b76" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. The default value is False. Ignored if seeds argument is not None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="382a329c7d03e3dc0838d8f846116bb309725e41" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False Ignored if seeds argument is not None.</source>
          <target state="translated">true 인 경우 초기 커널 위치는 모든 포인트의 위치가 아니라 이산 된 포인트 버전의 위치입니다. 이 옵션을 True로 설정하면 더 적은 시드가 초기화되므로 알고리즘 속도가 빨라집니다. 기본값 : False seed 인수가 None이 아닌 경우 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="5f8f1503f4ad4447aabcfddb9ab2a5dcd7bfde79" translate="yes" xml:space="preserve">
          <source>If true, only interaction features are produced: features that are products of at most &lt;code&gt;degree&lt;/code&gt;&lt;em&gt;distinct&lt;/em&gt; input features (so not &lt;code&gt;x[1] ** 2&lt;/code&gt;, &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt;, etc.).</source>
          <target state="translated">참이면 만의 상호 작용 기능을 생산하는 대부분의 제품에있는 기능 &lt;code&gt;degree&lt;/code&gt; &lt;em&gt;구별&lt;/em&gt; 입력 기능 (그렇게하지 &lt;code&gt;x[1] ** 2&lt;/code&gt; , &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt; 등).</target>
        </trans-unit>
        <trans-unit id="2d6ce18ffe19be253728c95b7f8882b85215b418" translate="yes" xml:space="preserve">
          <source>If true, randomize the order of coordinates in the CD solver.</source>
          <target state="translated">참이면 CD 솔버에서 좌표 순서를 랜덤 화하십시오.</target>
        </trans-unit>
        <trans-unit id="c9d7c7ecbdd08be425848806cc6f9d68c29d7323" translate="yes" xml:space="preserve">
          <source>If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses.</source>
          <target state="translated">참이면 샘플 당 평균 손실을 반환합니다. 그렇지 않으면 샘플 당 손실의 합을 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="6d32e9cabd3e10e0279ad9dd2b7c71514057bf52" translate="yes" xml:space="preserve">
          <source>If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1.</source>
          <target state="translated">참이면, 커널 내에없는 고아까지도 모든 지점이 클러스터됩니다. 고아는 가장 가까운 커널에 할당됩니다. false 인 경우 고아에게 클러스터 레이블 -1이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="87535a59e28d16a49b32c83a6882f2aefd67503d" translate="yes" xml:space="preserve">
          <source>If true, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree algorithms can have better scaling for large N.</source>
          <target state="translated">참이면 듀얼 트리 알고리즘을 사용하십시오. 그렇지 않으면 단일 트리 알고리즘을 사용하십시오. 이중 트리 알고리즘은 큰 N에 대해 더 나은 확장 성을 가질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1f1b5d26ff8a3e05067cc01bd3cc7a0f425c7062" translate="yes" xml:space="preserve">
          <source>If two features are almost equally correlated with the target, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a62f22814f97612f53d5c62d8ea50a484acea3fc" translate="yes" xml:space="preserve">
          <source>If two variables are almost equally correlated with the response, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.</source>
          <target state="translated">두 변수가 반응과 거의 동일하게 상관되는 경우 계수는 거의 같은 속도로 증가해야합니다. 따라서 알고리즘은 직감이 예상하는대로 작동하며 더욱 안정적입니다.</target>
        </trans-unit>
        <trans-unit id="c6d813716240a58cb1e341ee096ed78ff4d17824" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are plotted at each iteration.</source>
          <target state="translated">verbose가 True이면 목적 함수와 이중 간격이 각 반복에 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="c845cf733c967b921d034159fbd6e6d551e8f5a1" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are printed at each iteration.</source>
          <target state="translated">verbose가 True이면 각 반복마다 목적 함수와 이중 간격이 인쇄됩니다.</target>
        </trans-unit>
        <trans-unit id="d04bb65f95446e17ac813ad52d517cc52bee8bef" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and duality gap are printed at each iteration.</source>
          <target state="translated">verbose가 True이면 각 반복마다 목적 함수와 이중성 간격이 인쇄됩니다.</target>
        </trans-unit>
        <trans-unit id="c297e2c2dea63aea70529d05594e08d33ba95930" translate="yes" xml:space="preserve">
          <source>If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">웜 스타트가 활성화 된 경우, 이후 모드의 Laplace 근사에서 마지막 Newton 반복 솔루션은 다음 _posterior_mode () 호출에 대한 초기화로 사용됩니다. 하이퍼 파라미터 최적화와 비슷한 문제에서 _posterior_mode를 여러 번 호출하면 수렴 속도가 빨라질 수 있습니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="250e3ab5ccbd8ff9f61320148d394ace048df253" translate="yes" xml:space="preserve">
          <source>If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0beda1307d8fba8bf455ad043421d0e1b5743334" translate="yes" xml:space="preserve">
          <source>If we consider the loss function to be the individual error per sample, then the data-fit term, or the sum of the error for each sample, will increase as we add more samples. The penalization term, however, will not increase.</source>
          <target state="translated">손실 함수를 표본 당 개별 오차로 간주하면 표본을 더 추가할수록 데이터 적합 항 또는 각 표본에 대한 오차의 합이 증가합니다. 그러나 벌칙은 증가하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="232b0b83965c86185ba5cb4047fca7ca1eb2e5e8" translate="yes" xml:space="preserve">
          <source>If we define &lt;code&gt;s = 1 / density&lt;/code&gt;, the elements of the random matrix are drawn from</source>
          <target state="translated">&lt;code&gt;s = 1 / density&lt;/code&gt; 정의 하면 랜덤 행렬의 요소는</target>
        </trans-unit>
        <trans-unit id="de1968292a81bf57ac7d922cf400e8863c649aea" translate="yes" xml:space="preserve">
          <source>If we increase &lt;code&gt;power&lt;/code&gt; to 1,:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1427e0ae27c6ada10257117ddf3e602836e812e6" translate="yes" xml:space="preserve">
          <source>If we note &lt;code&gt;s = 1 / density&lt;/code&gt; the components of the random matrix are drawn from:</source>
          <target state="translated">&lt;code&gt;s = 1 / density&lt;/code&gt; 주목 하면 랜덤 매트릭스의 구성 요소는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f9cbc4d2964857d1865d4f91b696f12d3cce3cff" translate="yes" xml:space="preserve">
          <source>If we note \(n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})\) and \(n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})\), the time complexity of the randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is \(O(n_{\max}^2 \cdot n_{\mathrm{components}})\) instead of \(O(n_{\max}^2 \cdot n_{\min})\) for the exact method implemented in &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">\ (n _ {\ max} = \ max (n _ {\ mathrm {samples}}, n _ {\ mathrm {features}}) \) 및 \ (n _ {\ min} = \ min (n _ {\ mathrm {samples}}, n _ {\ mathrm {features}}) \), 무작위 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 의 시간 복잡도 는 \ (O (n _ {\ max} ^ 2 \ cdot n _ {\ mathrm {components}}) \)입니다. 구현 정확한 방법 \ (O (N _ {\ 최대} ^ 2 \ cdot N _ {\ 분}) \)의 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="483854d9b52bcc6a7814b6c99e197398a7767c0e" translate="yes" xml:space="preserve">
          <source>If we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number of samples is small, we need to shrink a lot. As a result, the Ledoit-Wolf precision is fairly close to the ground truth precision, that is not far from being diagonal, but the off-diagonal structure is lost.</source>
          <target state="translated">Ledoit-Wolf Estimator와 마찬가지로 l2 수축을 사용하는 경우 샘플 수가 적으므로 많이 축소해야합니다. 결과적으로, Ledoit-Wolf 정밀도는지면 진실 정밀도에 상당히 가깝습니다. 즉, 대각선이 아닌 멀지 않은 대각선 구조입니다.</target>
        </trans-unit>
        <trans-unit id="c048d2d806d4fc664d0d49e2240da1f9038d7165" translate="yes" xml:space="preserve">
          <source>If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:</source>
          <target state="translated">평면 대신 데이터에 포물면을 맞추려면 모형을 다음과 같이 2 차 다항식으로 결합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9cd2bdd13889db844fd297c5019d226d5f2214ec" translate="yes" xml:space="preserve">
          <source>If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries are the same) we would obtain &lt;code&gt;PPCA&lt;/code&gt;.</source>
          <target state="translated">가우스 잡음이 등방성 (모든 대각선 항목이 동일 함)이라고 가정하여 모델을 더 제한하면 &lt;code&gt;PPCA&lt;/code&gt; 를 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="722d8ecf13f293f3aeb1f206f30063d8afe3b1aa" translate="yes" xml:space="preserve">
          <source>If whiten is false, the data is already considered to be whitened, and no whitening is performed.</source>
          <target state="translated">미백이 거짓 인 경우, 데이터는 이미 미백 된 것으로 간주되며 미백이 수행되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="7d7146f3cf5f6ee3a12daad9561636b3070c19dc" translate="yes" xml:space="preserve">
          <source>If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.</source>
          <target state="translated">미백이 활성화 된 경우 inverse_transform은 반전 미백을 포함한 정확한 역 연산을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d6701cdf426d6a22381b3dd206332eab0defeb4b" translate="yes" xml:space="preserve">
          <source>If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant &lt;code&gt;c&lt;/code&gt; such that the average L2 norm of the training data equals one.</source>
          <target state="translated">PCA를 사용하여 추출한 피처에 SGD를 적용하는 경우 훈련 데이터의 평균 L2 규범이 1이되도록 일정한 &lt;code&gt;c&lt;/code&gt; 로 피처 값을 스케일링하는 것이 현명하다는 것을 알았습니다 .</target>
        </trans-unit>
        <trans-unit id="88e7b5f8ea1b769958767cd4c4986cb0d84b69d4" translate="yes" xml:space="preserve">
          <source>If you are having trouble decoding text, here are some things to try:</source>
          <target state="translated">텍스트를 디코딩하는 데 문제가있는 경우 다음을 시도해보십시오.</target>
        </trans-unit>
        <trans-unit id="79c8dbf5a9dd8e0bcb21dbc0b3d21d4a002af1f5" translate="yes" xml:space="preserve">
          <source>If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to:</source>
          <target state="translated">L1 및 L2 페널티를 개별적으로 제어하려는 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="fda5569b1e927ca58d1243554ef8331577e43162" translate="yes" xml:space="preserve">
          <source>If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.</source>
          <target state="translated">사전 사전을 제공하지 않고 특정 유형의 기능 선택을 수행하는 분석기를 사용하지 않으면 기능 수는 데이터를 분석하여 찾은 어휘 크기와 같습니다.</target>
        </trans-unit>
        <trans-unit id="44906a85511569286aafb87826155b3c2905ddf9" translate="yes" xml:space="preserve">
          <source>If you don&amp;rsquo;t have labels, try using &lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;Clustering&lt;/a&gt; on your problem.</source>
          <target state="translated">레이블이 없으면 문제에 대해 &lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;클러스터링&lt;/a&gt; 을 사용해보십시오 .</target>
        </trans-unit>
        <trans-unit id="951c73020178956033a4088912e01e49fd5d4ad8" translate="yes" xml:space="preserve">
          <source>If you encounter a bug with &lt;code&gt;scikit-learn&lt;/code&gt; or something that needs clarification in the docstring or the online documentation, please feel free to ask on the &lt;a href=&quot;http://scikit-learn.org/stable/support.html&quot;&gt;Mailing List&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e3c9a76f4703e92f09f761bb01632ba0994c7fc" translate="yes" xml:space="preserve">
          <source>If you experience hanging subprocesses with &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; or &lt;code&gt;n_jobs=-1&lt;/code&gt;, make sure you have a single-threaded BLAS library, or set &lt;code&gt;n_jobs=1&lt;/code&gt;, or upgrade to Python 3.4 which has a new version of &lt;code&gt;multiprocessing&lt;/code&gt; that should be immune to this problem.</source>
          <target state="translated">&lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; 또는 &lt;code&gt;n_jobs=-1&lt;/code&gt; 로 하위 프로세스가 중단 되는 경우 단일 스레드 BLAS 라이브러리가 있는지 확인하거나 &lt;code&gt;n_jobs=1&lt;/code&gt; 을 설정 하거나 새로운 버전의 &lt;code&gt;multiprocessing&lt;/code&gt; 이있는 Python 3.4로 업그레이드 하십시오. 문제.</target>
        </trans-unit>
        <trans-unit id="01bbb2c6066f4ee0c3cd8eec64a157d2ed58c8df" translate="yes" xml:space="preserve">
          <source>If you have a kernel matrix of a kernel \(K\) that computes a dot product in a feature space defined by function \(\phi\), a &lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt;&lt;code&gt;KernelCenterer&lt;/code&gt;&lt;/a&gt; can transform the kernel matrix so that it contains inner products in the feature space defined by \(\phi\) followed by removal of the mean in that space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4d3f940390acd5b7c567c108aba27222ddceb7d" translate="yes" xml:space="preserve">
          <source>If you have a kernel matrix of a kernel \(K\) that computes a dot product in a feature space defined by function \(phi\), a &lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt;&lt;code&gt;KernelCenterer&lt;/code&gt;&lt;/a&gt; can transform the kernel matrix so that it contains inner products in the feature space defined by \(phi\) followed by removal of the mean in that space.</source>
          <target state="translated">함수 \ (phi \)에 의해 정의 된 기능 공간에서 내적을 계산하는 커널 \ (K \)의 커널 행렬이있는 경우 &lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt; &lt;code&gt;KernelCenterer&lt;/code&gt; &lt;/a&gt; 는 정의 된 기능 공간에 내부 제품이 포함되도록 커널 행렬을 변환 할 수 있습니다. \ (phi \) 다음에 그 공간의 평균을 제거합니다.</target>
        </trans-unit>
        <trans-unit id="2615ef2dcc8005e7f8f1fe1a8b864f8c5c8b40ba" translate="yes" xml:space="preserve">
          <source>If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel:</source>
          <target state="translated">거리 행렬과 같이 친 화성 행렬 (예 : 0이 동일한 요소를 의미하고 높은 값이 매우 다른 요소를 의미 함)을 갖는 경우 가우시안 (RBF)을 적용하여 알고리즘에 적합한 유사성 행렬로 변환 할 수 있습니다. 열) 커널 :</target>
        </trans-unit>
        <trans-unit id="fe35ae96a69c356c4c790a684b706493b567f769" translate="yes" xml:space="preserve">
          <source>If you have multiple labels per document, e.g categories, have a look at the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;Multiclass and multilabel section&lt;/a&gt;.</source>
          <target state="translated">문서마다 여러 레이블 (예 : 카테고리)이있는 경우 &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;다중 클래스 및 다중 레이블 섹션을&lt;/a&gt; 살펴보십시오 .</target>
        </trans-unit>
        <trans-unit id="4c5f38a361bcbafd12cc29508483a444dfcf9728" translate="yes" xml:space="preserve">
          <source>If you have several classes to predict, an option often used is to fit one-versus-all classifiers and then use a voting heuristic for the final decision.</source>
          <target state="translated">예측할 클래스가 여러 개인 경우, 일반적으로 사용되는 옵션은 모든 분류기에 적합하고 최종 결정에 투표 휴리스틱을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="f97615967054f5695c197161c38be5160cb380e9" translate="yes" xml:space="preserve">
          <source>If you need the raw values of the partial dependence function rather than the plots you can use the &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt;&lt;code&gt;partial_dependence&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">플롯이 아닌 부분 의존 함수의 원시 값이 필요한 경우 &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt; &lt;code&gt;partial_dependence&lt;/code&gt; &lt;/a&gt; 함수를 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="89ce73b51d6a3d39879546007326b93ba776d729" translate="yes" xml:space="preserve">
          <source>If you need the raw values of the partial dependence function rather than the plots, you can use the &lt;a href=&quot;generated/sklearn.inspection.partial_dependence#sklearn.inspection.partial_dependence&quot;&gt;&lt;code&gt;sklearn.inspection.partial_dependence&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57ce2ca8cd47ab25ffa05da2880829913ab0ea8d" translate="yes" xml:space="preserve">
          <source>If you really want to use &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you can instantiate the estimator with the &lt;code&gt;novelty&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt; before fitting the estimator. In this case, &lt;code&gt;fit_predict&lt;/code&gt; is not available.</source>
          <target state="translated">참신 탐지 (예 : 레이블 예측 또는 보이지 않는 새로운 데이터의 비정상 점수 계산)에 &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; 을 실제로 사용 하려면 추정기를 피팅하기 전에 &lt;code&gt;novelty&lt;/code&gt; 매개 변수를 &lt;code&gt;True&lt;/code&gt; 로 설정 하여 추정기를 인스턴스화 할 수 있습니다 . 이 경우 &lt;code&gt;fit_predict&lt;/code&gt; 를 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="d5eafa493c5ec5c70bb915830d883a4547e5cefd" translate="yes" xml:space="preserve">
          <source>If you set load_content=True, you should also specify the encoding of the text using the &amp;lsquo;encoding&amp;rsquo; parameter. For many modern text files, &amp;lsquo;utf-8&amp;rsquo; will be the correct encoding. If you leave encoding equal to None, then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in &lt;a href=&quot;../classes#module-sklearn.feature_extraction.text&quot;&gt;&lt;code&gt;text&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f2c392b35ce2be9956cf3b6740e21a9cb0e7eb8" translate="yes" xml:space="preserve">
          <source>If you set load_content=True, you should also specify the encoding of the text using the &amp;lsquo;encoding&amp;rsquo; parameter. For many modern text files, &amp;lsquo;utf-8&amp;rsquo; will be the correct encoding. If you leave encoding equal to None, then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;.</source>
          <target state="translated">load_content = True를 설정하면 'encoding'매개 변수를 사용하여 텍스트의 인코딩도 지정해야합니다. 많은 최신 텍스트 파일의 경우 'utf-8'이 올바른 인코딩입니다. 인코딩을 없음으로두면 내용이 유니 코드 대신 바이트로 만들어지며 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 에서 대부분의 기능을 사용할 수 없습니다 .</target>
        </trans-unit>
        <trans-unit id="bc2de8e7176de3a7ec5f2ba66eebe7612bd92fb1" translate="yes" xml:space="preserve">
          <source>If you specify &lt;code&gt;max_depth=h&lt;/code&gt; then complete binary trees of depth &lt;code&gt;h&lt;/code&gt; will be grown. Such trees will have (at most) &lt;code&gt;2**h&lt;/code&gt; leaf nodes and &lt;code&gt;2**h - 1&lt;/code&gt; split nodes.</source>
          <target state="translated">&lt;code&gt;max_depth=h&lt;/code&gt; 를 지정하면 깊이 &lt;code&gt;h&lt;/code&gt; 의 완전한 이진 트리 가 커집니다. 이러한 나무는 (최대) &lt;code&gt;2**h&lt;/code&gt; 리프 노드와 &lt;code&gt;2**h - 1&lt;/code&gt; 분할 노드 를 갖습니다 .</target>
        </trans-unit>
        <trans-unit id="b5e591b33a0bd24a7e9f3db1117e493d465fb600" translate="yes" xml:space="preserve">
          <source>If you use sparse data (i.e. data represented as sparse matrices), &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt; will deal with the data without making it dense.</source>
          <target state="translated">당신이 스파 스 데이터 (희소 행렬로 표현 즉, 데이터)를 사용하는 경우 &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; 을&lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; 는&lt;/a&gt; 밀도를하지 않고 데이터를 처리합니다.</target>
        </trans-unit>
        <trans-unit id="a1ac2d367786359c2f555cec450b280865094e6b" translate="yes" xml:space="preserve">
          <source>If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using &lt;code&gt;warm_start=True&lt;/code&gt; and &lt;code&gt;max_iter=1&lt;/code&gt; and iterating yourself can be helpful:</source>
          <target state="translated">SGD에서 중지 기준 또는 학습 속도에 대한 제어를 강화하거나 &lt;code&gt;warm_start=True&lt;/code&gt; 및 &lt;code&gt;max_iter=1&lt;/code&gt; 을 사용하여 추가 모니터링을 수행 하고 자신을 반복하는 것이 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5572f4380789e92c52811040d71f90082bdb6315" translate="yes" xml:space="preserve">
          <source>If you want to know more about these issues and explore other possible serialization methods, please refer to this &lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;talk by Alex Gaynor&lt;/a&gt;.</source>
          <target state="translated">이러한 문제에 대해 더 알고 다른 가능한 직렬화 방법을 탐색하려면 &lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;Alex Gaynor&lt;/a&gt; 의이 대화 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="f3aeb01a6e584fc1ad67fbb7aa6de9be209b24eb" translate="yes" xml:space="preserve">
          <source>If you want to know more about these issues and explore other possible serialization methods, please refer to this &lt;a href=&quot;https://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;talk by Alex Gaynor&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73e4bcb37920038eaa4bbe1483d8a08f411face1" translate="yes" xml:space="preserve">
          <source>If you want to model a relative frequency, i.e. counts per exposure (time, volume, &amp;hellip;) you can do so by using a Poisson distribution and passing \(y=\frac{\mathrm{counts}}{\mathrm{exposure}}\) as target values together with \(\mathrm{exposure}\) as sample weights. For a concrete example see e.g. &lt;a href=&quot;../auto_examples/linear_model/plot_tweedie_regression_insurance_claims#sphx-glr-auto-examples-linear-model-plot-tweedie-regression-insurance-claims-py&quot;&gt;Tweedie regression on insurance claims&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9d28c176517636a408517ab464ebf5a8ed78a10" translate="yes" xml:space="preserve">
          <source>If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed.</source>
          <target state="translated">속성에 고유 한 스케일 (예 : 단어 빈도 또는 표시기 기능)이있는 경우 스케일링이 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="1c54fdc8274e03b04e776296dd28d5ff9fd81085" translate="yes" xml:space="preserve">
          <source>If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt;&lt;code&gt;robust_scale&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt;&lt;code&gt;RobustScaler&lt;/code&gt;&lt;/a&gt; as drop-in replacements instead. They use more robust estimates for the center and range of your data.</source>
          <target state="translated">데이터에 특이 치가 많이 포함 된 경우 데이터의 평균 및 분산을 사용하여 조정하면 제대로 작동하지 않을 수 있습니다. 이러한 경우, 대신 &lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt; &lt;code&gt;RobustScaler&lt;/code&gt; &lt;/a&gt; 교체로 &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt; &lt;code&gt;robust_scale&lt;/code&gt; &lt;/a&gt; 및 RobustScaler 를 사용할 수 있습니다 . 데이터의 중심과 범위에 대해보다 강력한 추정치를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="a10bfba29a759d89e81956a541262290109cf294" translate="yes" xml:space="preserve">
          <source>If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the &lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;Unsupervised learning&lt;/a&gt; methods implement a &lt;code&gt;transform&lt;/code&gt; method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.</source>
          <target state="translated">기능 수가 많으면 감독 단계 전에 감독되지 않은 단계로 기능을 줄이는 것이 유용 할 수 있습니다. 많은 &lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;비지도 학습&lt;/a&gt; 방법 은 차원을 줄이는 데 사용할 수 있는 &lt;code&gt;transform&lt;/code&gt; 방법을 구현 합니다. 아래에서는 많이 사용되는이 패턴의 두 가지 특정 예에 대해 설명합니다.</target>
        </trans-unit>
        <trans-unit id="7fd7c263c0ce2fcb7f46b0e79b01f4b5e3878456" translate="yes" xml:space="preserve">
          <source>If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the &lt;a href=&quot;https://scikit-learn.org/0.23/unsupervised_learning.html#unsupervised-learning&quot;&gt;Unsupervised learning&lt;/a&gt; methods implement a &lt;code&gt;transform&lt;/code&gt; method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="933c257247173f2464c3cf8d4de4af2d678e5a7c" translate="yes" xml:space="preserve">
          <source>If your number of observations is not large compared to the number of edges in your underlying graph, you will not recover it.</source>
          <target state="translated">관측치 수가 기본 그래프의 모서리 수와 비교하여 크지 않으면이를 복구하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4f34c772816074a373c0d5e918e2e9ac136448b7" translate="yes" xml:space="preserve">
          <source>Ignore the offset first bytes by seeking forward, then discarding the following bytes up until the next new line character.</source>
          <target state="translated">앞으로 탐색하여 오프셋 첫 바이트를 무시하고 다음 줄 바꾸기 문자까지 다음 바이트를 버립니다.</target>
        </trans-unit>
        <trans-unit id="78fee1435d74666b84850cd5e82c18229351da5d" translate="yes" xml:space="preserve">
          <source>Ignored</source>
          <target state="translated">Ignored</target>
        </trans-unit>
        <trans-unit id="9b02e8c10d5a363337d6fcee177ec3e9cae9f1ce" translate="yes" xml:space="preserve">
          <source>Ignored in binary classification or classical regression settings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce03a847a845250be1b2b174971c463802e32813" translate="yes" xml:space="preserve">
          <source>Ignored variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e65bb4eca2d3c71529c96890a4b735eb7dafeac" translate="yes" xml:space="preserve">
          <source>Ignored.</source>
          <target state="translated">Ignored.</target>
        </trans-unit>
        <trans-unit id="d0c592be2a6267cc2802c86a5116a8ba2d4b6ff9" translate="yes" xml:space="preserve">
          <source>Ignored. This parameter exists only for compatibility with &lt;a href=&quot;sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e417badfc4d52f79664b451110854e41b4a0daf" translate="yes" xml:space="preserve">
          <source>Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline.</source>
          <target state="translated">무시되었습니다. 이 매개 변수는 sklearn.pipeline.Pipeline과의 호환성을 위해서만 존재합니다.</target>
        </trans-unit>
        <trans-unit id="2d34b7c897f7b41a0f0625575a2c9cc21b1078a7" translate="yes" xml:space="preserve">
          <source>Illustration of &lt;code&gt;Pipeline&lt;/code&gt; and &lt;code&gt;GridSearchCV&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 및 &lt;code&gt;GridSearchCV&lt;/code&gt; 의 그림</target>
        </trans-unit>
        <trans-unit id="643998f34944846c305de7d49de1c3e80f814d2d" translate="yes" xml:space="preserve">
          <source>Illustration of Gaussian process classification (GPC) on the XOR dataset</source>
          <target state="translated">XOR 데이터 세트의 가우스 프로세스 분류 (GPC) 그림</target>
        </trans-unit>
        <trans-unit id="c2cd661f8089fd4df71dfb566ea137083aa22024" translate="yes" xml:space="preserve">
          <source>Illustration of how the performance of an estimator on unseen data (test data) is not the same as the performance on training data. As the regularization increases the performance on train decreases while the performance on test is optimal within a range of values of the regularization parameter. The example with an Elastic-Net regression model and the performance is measured using the explained variance a.k.a. R^2.</source>
          <target state="translated">보이지 않는 데이터 (테스트 데이터)에 대한 추정기의 성능이 훈련 데이터의 성능과 어떻게 다른지 설명합니다. 정규화가 증가함에 따라 열차의 성능이 저하되는 반면, 테스트의 성능은 정규화 매개 변수의 값 범위 내에서 최적입니다. Elastic-Net 회귀 모형과 성능의 예는 설명 된 분산 (R ^ 2)을 사용하여 측정됩니다.</target>
        </trans-unit>
        <trans-unit id="5790a5aaa3a6c4543a820b9b12ce6d261eeb0581" translate="yes" xml:space="preserve">
          <source>Illustration of prior and posterior Gaussian process for different kernels</source>
          <target state="translated">다른 커널에 대한 이전 및 이후 가우스 프로세스의 그림</target>
        </trans-unit>
        <trans-unit id="71618836e7c136eb1b3d1aef884b22f68af959aa" translate="yes" xml:space="preserve">
          <source>Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27c062ea4e410688effe23ac51313a8ab9a70f1c" translate="yes" xml:space="preserve">
          <source>Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">그라디언트 부스팅에 대한 다양한 정규화 전략의 효과를 보여줍니다. 이 예는 Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 에서 발췌 한 것이다 .</target>
        </trans-unit>
        <trans-unit id="5beb1d257bbb65ddb7ec568445a1c3acdcb42d37" translate="yes" xml:space="preserve">
          <source>Image denoising using dictionary learning</source>
          <target state="translated">사전 학습을 사용한 이미지 노이즈 제거</target>
        </trans-unit>
        <trans-unit id="5ab7decf36c80b04aff06a11c0e8ef068c85a1b9" translate="yes" xml:space="preserve">
          <source>Image histogram</source>
          <target state="translated">이미지 히스토그램</target>
        </trans-unit>
        <trans-unit id="2c151a57b190c9b9e70046810542a00e0344b5af" translate="yes" xml:space="preserve">
          <source>Image representing the confusion matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c328038b14054033bab1147ef5d1ad234b3373d" translate="yes" xml:space="preserve">
          <source>Imagine you have three subjects, each with an associated number from 1 to 3:</source>
          <target state="translated">각각 1에서 3까지의 관련 번호를 가진 세 가지 주제가 있다고 가정하십시오.</target>
        </trans-unit>
        <trans-unit id="8781d615fd77be9578225c40ac67b9471394cced" translate="yes" xml:space="preserve">
          <source>Implementation</source>
          <target state="translated">Implementation</target>
        </trans-unit>
        <trans-unit id="8d522809f4125f5930c1f4f77ec91f8735a003d8" translate="yes" xml:space="preserve">
          <source>Implementation based on &lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt;</source>
          <target state="translated">를 기반으로 구현 &lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1f57545a08e425cccaf152a77959fb187cad06cb" translate="yes" xml:space="preserve">
          <source>Implementation based on &lt;em&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/em&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6a1ab1256c83ccf4dbd316f43007b044bc691a2" translate="yes" xml:space="preserve">
          <source>Implementation detail: taking sample weights into account amounts to multiplying the gradients (and the hessians) by the sample weights. Note that the binning stage (specifically the quantiles computation) does not take the weights into account.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6ac8df85fe47d2d00b4a78e1facdef4fbcae73b" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. Furthermore SVC multi-class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest. It is possible to implement one vs the rest with SVC by using the &lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; wrapper. Finally SVC can fit dense data without memory copy if the input is C-contiguous. Sparse data will still incur memory copy though.</source>
          <target state="translated">libsvm을 사용한 Support Vector Machine 분류기의 구현 : 커널은 비선형 일 수 있지만 LinearSVC처럼 SMO 알고리즘은 많은 수의 샘플로 확장되지 않습니다. 또한 SVC 멀티 클래스 모드는 일대일 방식을 사용하여 구현되는 반면 LinearSVC는 일대일 방식을 사용합니다. &lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt; 랩퍼 를 사용하여 SVC를 사용하여 나머지 하나를 구현할 수 있습니다 . 마지막으로 입력이 C 연속 인 경우 SVC는 메모리 복사없이 밀도가 높은 데이터를 맞출 수 있습니다. 스파 스 데이터는 여전히 메모리 복사를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="78b58091d5da65fb36aa747d9975841d97302dec" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using the same library as this class (liblinear).</source>
          <target state="translated">이 클래스와 동일한 라이브러리를 사용하여 Support Vector Machine 분류기 구현 (liblinear).</target>
        </trans-unit>
        <trans-unit id="0faf8832b17d93a1b230f7a6ca4feb36d15cc4ac" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does.</source>
          <target state="translated">libsvm을 사용한 Support Vector Machine 회귀 구현 : 커널은 비선형 일 수 있지만 LinearSVC처럼 SMO 알고리즘은 많은 수의 샘플로 확장되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="adae10003f16f5885f71700e866f2cc76e2c6af9" translate="yes" xml:space="preserve">
          <source>Implements feature hashing, aka the hashing trick.</source>
          <target state="translated">해싱 기능 (일명 해시 트릭)을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="d98e09b894119d4a2d55bf3e2f04052ec103359a" translate="yes" xml:space="preserve">
          <source>Implements resampling with replacement. If False, this will implement (sliced) random permutations.</source>
          <target state="translated">교체로 리샘플링을 구현합니다. False이면 무작위 순열을 구현합니다 (슬라이스).</target>
        </trans-unit>
        <trans-unit id="9f9d0b6a3b9dbc770ff8e17c3a6979d6ebb5425d" translate="yes" xml:space="preserve">
          <source>Implements the Birch clustering algorithm.</source>
          <target state="translated">버치 클러스터링 알고리즘을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="82028db75262dc1a82a0dc4cf2e6f254032ff9f7" translate="yes" xml:space="preserve">
          <source>Implements the incremental PCA model from: &lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; See &lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; 에서 증분 PCA 모델을 구현합니다 . 2008 년 5 월 참조 &lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eddd9fc5411550a8b67e601c8cf138c977f02e42" translate="yes" xml:space="preserve">
          <source>Implements the incremental PCA model from: &lt;em&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/em&gt; See &lt;a href=&quot;https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06be3bf25c44efb35fdd12e8816c051b94a6e5d6" translate="yes" xml:space="preserve">
          <source>Implements the probabilistic PCA model from: &lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt;Tipping, M. E., and Bishop, C. M. (1999). &amp;ldquo;Probabilistic principal component analysis&amp;rdquo;. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. via the score and score_samples methods. See &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt; Tipping, ME 및 Bishop, CM (1999) 에서 확률 적 PCA 모델을 구현합니다 . &amp;ldquo;확률 적 주성분 분석&amp;rdquo;. 왕립 통계 학회지 : 시리즈 B (통계 방법론), 61 (3), 611-622. score 및 score_samples 메소드를 통해 참조 &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf를&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5153c1832189653f3d836c4da5e485c3d1b56671" translate="yes" xml:space="preserve">
          <source>Implements the probabilistic PCA model from: Tipping, M. E., and Bishop, C. M. (1999). &amp;ldquo;Probabilistic principal component analysis&amp;rdquo;. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. via the score and score_samples methods. See &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="496d8573358dc0bbf8fad0d466b95c37d153e5fd" translate="yes" xml:space="preserve">
          <source>Importance of Feature Scaling</source>
          <target state="translated">기능 스케일링의 중요성</target>
        </trans-unit>
        <trans-unit id="dee0fbd7a096536203f3e083c7a95f20ef772057" translate="yes" xml:space="preserve">
          <source>Important members are fit, predict.</source>
          <target state="translated">중요한 멤버는 적합합니다.</target>
        </trans-unit>
        <trans-unit id="34aace9b4c119f775f92304ec637d19c91f28ab6" translate="yes" xml:space="preserve">
          <source>Importantly, this tabular dataset has very different dynamic ranges for its features. Neural networks tend to be very sensitive to features with varying scales and forgetting to preprocess the numeric feature would lead to a very poor model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7f6e12fca9c93deac0f3e9ce5406887ec818c03" translate="yes" xml:space="preserve">
          <source>Improvements to the histogram-based Gradient Boosting estimators</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eed74af8ff28262ccadec0b3ca567aeb04ce5592" translate="yes" xml:space="preserve">
          <source>Imputation for completing missing values using k-Nearest Neighbors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0004bf233145469d6159f141af0ae0b05f3c5e9a" translate="yes" xml:space="preserve">
          <source>Imputation transformer for completing missing values.</source>
          <target state="translated">결 측값을 완료하기위한 대치 변압기.</target>
        </trans-unit>
        <trans-unit id="8154b566118976ff2097cfffb2c92470797b0a69" translate="yes" xml:space="preserve">
          <source>Impute all missing values in X.</source>
          <target state="translated">X에서 모든 결 측값을 대치합니다.</target>
        </trans-unit>
        <trans-unit id="ad8e498cb05e98f21bd0f42774d74dde4c2bb7ea" translate="yes" xml:space="preserve">
          <source>Impute missing values with mean</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4715d3875af4e3819958eff35e6816030a9c89e" translate="yes" xml:space="preserve">
          <source>Impute the missing data and score</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="689f1aca2f66f40afe86b2a1257542980cceee50" translate="yes" xml:space="preserve">
          <source>Imputer used to initialize the missing values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74ee388ccf36d172dba88b767c11eeb31f63d971" translate="yes" xml:space="preserve">
          <source>Imputes all missing values in X.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="510c592fb9a4fd828788fc0bdd902c165ca78889" translate="yes" xml:space="preserve">
          <source>Imputing missing values before building an estimator</source>
          <target state="translated">추정기를 작성하기 전에 결 측값 대치</target>
        </trans-unit>
        <trans-unit id="87c53ba7fd85032a63ff707cca98951b5852e72d" translate="yes" xml:space="preserve">
          <source>Imputing missing values with variants of IterativeImputer</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5d148df74ab3f703a9d283fda0c99f4936ff674" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt;, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in &lt;code&gt;ElasticNet&lt;/code&gt;, we control the combination of L1 and L2 with the &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) parameter, and the intensity of the regularization with the &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) parameter. Then the priors terms are:</source>
          <target state="translated">에서는 &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; , L1 및 L2는 전과 모델 규칙 화하기 위해 손실 함수에 첨가 될 수있다. L2 이전은 Frobenius 표준을 사용하고 L1 이전은 요소 별 L1 표준을 사용합니다. 마찬가지로 &lt;code&gt;ElasticNet&lt;/code&gt; , 우리는와 L1과 L2의 조합 제어 &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ RHO \)) 파라미터와 함께 정규화 강도 &lt;code&gt;alpha&lt;/code&gt; (\ (\ 알파 \)) 파라미터. 그런 다음 이전 조건은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="eee03375c59654f18a55a7961e4f26a36fbc2cee" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt;&lt;code&gt;OutputCodeClassifier&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;code_size&lt;/code&gt; attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.</source>
          <target state="translated">에서는 &lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt; &lt;code&gt;OutputCodeClassifier&lt;/code&gt; &lt;/a&gt; 상기 &lt;code&gt;code_size&lt;/code&gt; 의 특성은 사용되는 분류의 수를 제어하도록 사용자에게 허용한다. 총 클래스 수의 백분율입니다.</target>
        </trans-unit>
        <trans-unit id="92869ea1268ab85728d32cc145b2fc2a3cf98201" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, if data for classification are unbalanced (e.g. many positive and few negative), set &lt;code&gt;class_weight='balanced'&lt;/code&gt; and/or try different penalty parameters &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">에서는 &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; 분류 데이터 (예를 들어, 많은 양의 거의 제외) 세트 불평형 경우 &lt;code&gt;class_weight='balanced'&lt;/code&gt; 및 / 또는 다른 페널티 파라미터를 시도 &lt;code&gt;C&lt;/code&gt; 를 .</target>
        </trans-unit>
        <trans-unit id="3c0402c73702f19dd4de5bea870fc6f39e9426a9" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, if the data is unbalanced (e.g. many positive and few negative), set &lt;code&gt;class_weight='balanced'&lt;/code&gt; and/or try different penalty parameters &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4a2dd93e9c8bc18123ea577d336109c88e8c2c3" translate="yes" xml:space="preserve">
          <source>In &lt;strong&gt;averaging methods&lt;/strong&gt;, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</source>
          <target state="translated">에서 &lt;strong&gt;방법을 평균&lt;/strong&gt; , 구동 원리는 평균 독립적으로 그들의 예측을 여러 추정량을 구축하는 것입니다. 평균적으로, 결합 추정량은 분산이 줄어들 기 때문에 일반적으로 단일 기본 추정량보다 낫습니다.</target>
        </trans-unit>
        <trans-unit id="ca51c131377adedf53770a869ad18245bd6bd115" translate="yes" xml:space="preserve">
          <source>In a binary classification context, imposing a monotonic constraint means that the feature is supposed to have a positive / negative effect on the probability to belong to the positive class. Monotonic constraints are not supported for multiclass context.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="baeac0931c0e4b4385579000935f2bb52ceb9f07" translate="yes" xml:space="preserve">
          <source>In a binary classification task, the terms &amp;lsquo;&amp;rsquo;positive&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;negative&amp;rsquo;&amp;rsquo; refer to the classifier&amp;rsquo;s prediction, and the terms &amp;lsquo;&amp;rsquo;true&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;false&amp;rsquo;&amp;rsquo; refer to whether that prediction corresponds to the external judgment (sometimes known as the &amp;lsquo;&amp;rsquo;observation&amp;rsquo;&amp;lsquo;). Given these definitions, we can formulate the following table:</source>
          <target state="translated">이진 분류 작업에서``긍정적 ''및``부정적 ''이라는 용어는 분류 자의 예측을 나타내며``true ''및``거짓 ''은 해당 예측이 외부 판단에 해당하는지 여부를 나타냅니다 ( 때때로``관찰 ''이라고도 함). 이러한 정의가 주어지면 다음 표를 공식화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ccc920586f4d3de666e5e909b4599881349f812c" translate="yes" xml:space="preserve">
          <source>In a binary classification task, the terms &amp;lsquo;&amp;rsquo;positive&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;negative&amp;rsquo;&amp;rsquo; refer to the classifier&amp;rsquo;s prediction, and the terms &amp;lsquo;&amp;rsquo;true&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;false&amp;rsquo;&amp;rsquo; refer to whether that prediction corresponds to the external judgment (sometimes known as the &amp;lsquo;&amp;rsquo;observation&amp;rsquo;&amp;rsquo;). Given these definitions, we can formulate the following table:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="995a1ae8b5be72e5d8cbdf391052b665e77e9964" translate="yes" xml:space="preserve">
          <source>In a first step, the hierarchical clustering is performed without connectivity constraints on the structure and is solely based on distance, whereas in a second step the clustering is restricted to the k-Nearest Neighbors graph: it&amp;rsquo;s a hierarchical clustering with structure prior.</source>
          <target state="translated">첫 번째 단계에서 계층 적 군집화는 구조에 대한 연결 제약 조건없이 수행되며 거리 만 기준으로하는 반면 두 번째 단계에서는 클러스터링이 k- 최근 접 이웃 그래프로 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="0336cb4d8e7c9adb72abdea9417802db49cccd1f" translate="yes" xml:space="preserve">
          <source>In a large text corpus, some words will be very present (e.g. &amp;ldquo;the&amp;rdquo;, &amp;ldquo;a&amp;rdquo;, &amp;ldquo;is&amp;rdquo; in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.</source>
          <target state="translated">큰 텍스트 모음에서 일부 단어가 매우 많이 존재하므로 (예 : 영어로 &quot;the&quot;, &quot;a&quot;, &quot;is&quot;) 문서의 실제 내용에 대한 의미있는 정보가 거의 없습니다. 직접 카운트 데이터를 분류 자에게 직접 공급한다면 매우 빈번한 용어는 더 희귀하지만 더 흥미로운 용어의 빈도를 가리게됩니다.</target>
        </trans-unit>
        <trans-unit id="0caaa107286ab067ca8115855386dd04703bccc4" translate="yes" xml:space="preserve">
          <source>In a multiclass setting, specifies the class for which the PDPs should be computed. Note that for binary classification, the positive class (index 1) is always used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52eb2197195ece26c6612081f2107f907b3e4099" translate="yes" xml:space="preserve">
          <source>In a multioutput setting, specifies the task for which the PDPs should be computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd0ed349168abd35a1fce87e6ae9e8ccc5ff58f4" translate="yes" xml:space="preserve">
          <source>In a nutshell, the following table summarizes the solvers characteristics:</source>
          <target state="translated">간단히 말해서, 다음 표는 솔버 특성을 요약 한 것입니다.</target>
        </trans-unit>
        <trans-unit id="b4ec4bcaff3e86d4d99c938f5623ab4b737e65c7" translate="yes" xml:space="preserve">
          <source>In a real world setting, the &lt;code&gt;n_features&lt;/code&gt; parameter can be left to its default value of &lt;code&gt;2 ** 20&lt;/code&gt; (roughly one million possible features). If memory or downstream models size is an issue selecting a lower value such as &lt;code&gt;2 **
18&lt;/code&gt; might help without introducing too many additional collisions on typical text classification tasks.</source>
          <target state="translated">실제 환경에서 &lt;code&gt;n_features&lt;/code&gt; 매개 변수는 기본값 &lt;code&gt;2 ** 20&lt;/code&gt; (약 100 만 개의 가능한 기능)으로 유지 될 수 있습니다. 메모리 또는 다운 스트림 모델 크기가 &lt;code&gt;2 ** 18&lt;/code&gt; 과 같은 낮은 값을 선택하는 데 문제가있는 경우 일반적인 텍스트 분류 작업에 너무 많은 추가 충돌이 발생하지 않으면 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c75e295f24e05d06cacc1a2f9bb570a61bdd802e" translate="yes" xml:space="preserve">
          <source>In a similar manner, the boston housing data set is used to show the impact of transforming the targets before learning a model. In this example, the targets to be predicted corresponds to the weighted distances to the five Boston employment centers.</source>
          <target state="translated">비슷한 방식으로, 보스턴 하우징 데이터 세트는 모델을 학습하기 전에 대상을 변환하는 데 따른 영향을 나타내는 데 사용됩니다. 이 예에서, 예측 될 목표는 5 개의 보스턴 고용 센터까지의 가중 거리에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="c210295417ef2f29cc593be46bbc5efed5892a5f" translate="yes" xml:space="preserve">
          <source>In addition of using an imputing method, we can also keep an indication of the missing information using &lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt;&lt;/a&gt; which might carry some information.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt; &lt;/a&gt; 방법을 사용 하는 것 외에도 일부 정보를 전달할 수있는 sklearn.impute.MissingIndicator 를 사용하여 누락 된 정보를 표시 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="67d21513f58775c1e37b37d0e7fc556492bd761a" translate="yes" xml:space="preserve">
          <source>In addition to imputing the missing values, the imputers have an &lt;code&gt;add_indicator&lt;/code&gt; parameter that marks the values that were missing, which might carry some information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45e3263783ee36dc90c5821ae7ffc8d210750151" translate="yes" xml:space="preserve">
          <source>In addition to its current contents, this module will eventually be home to refurbished versions of Pipeline and FeatureUnion.</source>
          <target state="translated">이 모듈은 현재 내용 외에도 파이프 라인 및 FeatureUnion의 리퍼브 버전이 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="3dda0db479e61a3dc2539c918fe85f072a3cc4a4" translate="yes" xml:space="preserve">
          <source>In addition to standard scikit-learn estimator API, GaussianProcessRegressor:</source>
          <target state="translated">표준 scikit-learn 추정기 API 외에도 GaussianProcessRegressor는 다음을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="6c259b1081efe473add72a6c01ee26dbc96ee486" translate="yes" xml:space="preserve">
          <source>In addition to the mean of the predictive distribution, also its standard deviation can be returned.</source>
          <target state="translated">예측 분포의 평균 외에도 표준 편차가 반환 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f5f01da0b407208bd57121f71ed7408cbbce3fe6" translate="yes" xml:space="preserve">
          <source>In addition, as there is no useful information in the intensity of the image, or its gradient, we choose to perform the spectral clustering on a graph that is only weakly informed by the gradient. This is close to performing a Voronoi partition of the graph.</source>
          <target state="translated">또한 이미지의 강도 나 그레디언트에 유용한 정보가 없기 때문에 그래디언트에 약한 정보 만있는 그래프에서 스펙트럼 클러스터링을 수행하도록 선택합니다. 이것은 그래프의 보로 노이 분할 수행에 가깝습니다.</target>
        </trans-unit>
        <trans-unit id="8be2789c3e2f5a1d410c34b62e20c28c8adc9fef" translate="yes" xml:space="preserve">
          <source>In addition, if the &lt;code&gt;dask&lt;/code&gt; and &lt;code&gt;distributed&lt;/code&gt; Python packages are installed, it is possible to use the &amp;lsquo;dask&amp;rsquo; backend for better scheduling of nested parallel calls without over-subscription and potentially distribute parallel calls over a networked cluster of several hosts.</source>
          <target state="translated">또한 &lt;code&gt;dask&lt;/code&gt; 및 &lt;code&gt;distributed&lt;/code&gt; Python 패키지가 설치되어 있으면 초과 가입없이 중첩 된 병렬 호출을 더 잘 예약하고 '호스트'백엔드를 사용하여 여러 호스트의 네트워크 클러스터를 통해 병렬 호출을 분산시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1068dbee0dd3c16e2fc93e44b0ce455f5b052f8b" translate="yes" xml:space="preserve">
          <source>In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.</source>
          <target state="translated">또한, scikit-learn에는 제어 된 크기와 복잡성의 인공 데이터 세트를 구축하는 데 사용할 수있는 다양한 랜덤 샘플 생성기가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="fc65b58b68e776527046619280b83dca96b85eef" translate="yes" xml:space="preserve">
          <source>In addition, some of the numpy routines that are used internally by scikit-learn may also be parallelized if numpy is installed with specific numerical libraries such as MKL, OpenBLAS, or BLIS.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a9dc36feb47a20be69368bf3755ac289f3cd578" translate="yes" xml:space="preserve">
          <source>In addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the &lt;a href=&quot;#loading-other-datasets&quot;&gt;Loading other datasets&lt;/a&gt; section.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67ed28f1d0cd9fef0560dd0bbfe2b568680ea5a6" translate="yes" xml:space="preserve">
          <source>In addition, there are also miscellanous tools to load datasets of other formats or from other locations, described in the &lt;a href=&quot;#loading-other-datasets&quot;&gt;Loading other datasets&lt;/a&gt; section.</source>
          <target state="translated">또한 &lt;a href=&quot;#loading-other-datasets&quot;&gt;다른 데이터 세트로드&lt;/a&gt; 섹션에 설명 된 다른 형식 또는 다른 위치의 데이터 세트를로드하는 기타 도구도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5f2d75a19b27e52127d76b03616ce749746e4308" translate="yes" xml:space="preserve">
          <source>In addition, we show two different ways to dispatch the columns to the particular pre-processor: by column names and by column data types.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="846c6c3d11b49bd5243a8b72c066c7aae06dcfe3" translate="yes" xml:space="preserve">
          <source>In addition, we use the mask of the objects to restrict the graph to the outline of the objects. In this example, we are interested in separating the objects one from the other, and not from the background.</source>
          <target state="translated">또한 객체의 마스크를 사용하여 그래프를 객체의 윤곽으로 제한합니다. 이 예에서는 배경이 아닌 객체를 서로 분리하는 데 관심이 있습니다.</target>
        </trans-unit>
        <trans-unit id="b490744f01019d4237b3a8568465e031a5ae6e1f" translate="yes" xml:space="preserve">
          <source>In all these strategies, the &lt;code&gt;predict&lt;/code&gt; method completely ignores the input data.</source>
          <target state="translated">이러한 모든 전략에서 &lt;code&gt;predict&lt;/code&gt; 방법은 입력 데이터를 완전히 무시합니다.</target>
        </trans-unit>
        <trans-unit id="450c8a41f7c5da3bb2b7da9a15306b194b36c681" translate="yes" xml:space="preserve">
          <source>In an &lt;strong&gt;unsupervised setting&lt;/strong&gt; it can be used to group similar documents together by applying clustering algorithms such as &lt;a href=&quot;clustering#k-means&quot;&gt;K-means&lt;/a&gt;:</source>
          <target state="translated">&lt;strong&gt;감독되지 않은 설정&lt;/strong&gt; 에서는 &lt;a href=&quot;clustering#k-means&quot;&gt;K-means&lt;/a&gt; 와 같은 클러스터링 알고리즘을 적용하여 유사한 문서를 그룹화하는 데 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e5af8b183011d6bf7238d58f71b94384a5a96f84" translate="yes" xml:space="preserve">
          <source>In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process.</source>
          <target state="translated">어쨌든 모델 복잡성을 줄이면 위에서 언급 한대로 정확도가 떨어질 수 있다는 경고가 표시됩니다. 예를 들어 비선형 적으로 분리 가능한 문제는 빠른 선형 모델로 처리 할 수 ​​있지만 그 과정에서 예측력이 저하 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8c17ce8abfedb506f7ed46ebde27121f581c8bb7" translate="yes" xml:space="preserve">
          <source>In applications where a high false positive rate is not tolerable the parameter &lt;code&gt;max_fpr&lt;/code&gt; of &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; can be used to summarize the ROC curve up to the given limit.</source>
          <target state="translated">높은 오 탐지율이 허용되지 않는 응용 분야에서는 &lt;code&gt;max_fpr&lt;/code&gt; 의 &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt; 매개 변수를 사용하여 ROC 곡선을 주어진 한도까지 요약 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bbcc07c440f8193b3e7ecf64eaaca0e386adcbad" translate="yes" xml:space="preserve">
          <source>In bin edges for feature &lt;code&gt;i&lt;/code&gt;, the first and last values are used only for &lt;code&gt;inverse_transform&lt;/code&gt;. During transform, bin edges are extended to:</source>
          <target state="translated">기능 &lt;code&gt;i&lt;/code&gt; 의 &lt;code&gt;inverse_transform&lt;/code&gt; 가장자리 에서 첫 번째 값과 마지막 값은 inverse_transform 에만 사용됩니다 . 변환하는 동안 빈 가장자리는 다음과 같이 확장됩니다.</target>
        </trans-unit>
        <trans-unit id="9d083c0b55e1a00133d4b27dd85f5ea26aca3922" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, the Jaccard similarity coefficient score is equal to the classification accuracy.</source>
          <target state="translated">이진 및 멀티 클래스 분류에서 Jaccard 유사성 계수 점수는 분류 정확도와 같습니다.</target>
        </trans-unit>
        <trans-unit id="834fbb10f3b1dd0752ef9b7f9aa530ac5202b2db" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equal to the &lt;code&gt;jaccard_score&lt;/code&gt; function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0b30958f6975dadd3d7eaf24f5447254d56c629" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equal to the &lt;code&gt;jaccard_similarity_score&lt;/code&gt; function.</source>
          <target state="translated">이진 및 멀티 클래스 분류에서이 함수는 &lt;code&gt;jaccard_similarity_score&lt;/code&gt; 함수와 같습니다.</target>
        </trans-unit>
        <trans-unit id="e731c9654f26a8d7255165d2c0d5f78e47c3bd9a" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equivalent to the &lt;code&gt;accuracy_score&lt;/code&gt;. It differs in the multilabel classification problem.</source>
          <target state="translated">이진 및 멀티 클래스 분류에서이 함수는 &lt;code&gt;accuracy_score&lt;/code&gt; _ 점수와 같습니다 . 다중 레이블 분류 문제가 다릅니다.</target>
        </trans-unit>
        <trans-unit id="c4cb57bb3e2c0bb1485829473f6ca6362cee5b90" translate="yes" xml:space="preserve">
          <source>In binary class case, assuming labels in y_true are encoded with +1 and -1, when a prediction mistake is made, &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; is always negative (since the signs disagree), implying &lt;code&gt;1 - margin&lt;/code&gt; is always greater than 1. The cumulated hinge loss is therefore an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">이진 클래스의 경우, y_true의 레이블이 +1 및 -1로 인코딩되었다고 가정하면 예측 실수가 발생하면 &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; 은 항상 음수이며 (부호가 일치하지 않기 때문에) &lt;code&gt;1 - margin&lt;/code&gt; 은 항상 1보다 큽니다. 따라서 누적 힌지 손실은 분류기에 의해 발생한 실수의 상한입니다.</target>
        </trans-unit>
        <trans-unit id="f2c8a5d61695d64c32adbdec8051b4ecc7f404cb" translate="yes" xml:space="preserve">
          <source>In binary classification settings</source>
          <target state="translated">이진 분류 설정에서</target>
        </trans-unit>
        <trans-unit id="0e8524872beef3003e748e1d9b4f90c7ce280313" translate="yes" xml:space="preserve">
          <source>In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve &lt;code&gt;n_iter_no_change&lt;/code&gt; times in a row. The improvement is evaluated with a tolerance &lt;code&gt;tol&lt;/code&gt;, and the algorithm stops in any case after a maximum number of iteration &lt;code&gt;max_iter&lt;/code&gt;.</source>
          <target state="translated">두 경우 모두, 기준은 에포크 (epoch)에 의해 한 번 평가되며 기준이 행에서 &lt;code&gt;n_iter_no_change&lt;/code&gt; 시간을 개선하지 않으면 알고리즘이 중지됩니다 . 개선은 공차 &lt;code&gt;tol&lt;/code&gt; 을 사용하여 평가되며 , 최대 반복 횟수 &lt;code&gt;max_iter&lt;/code&gt; 후에 알고리즘이 중지됩니다 .</target>
        </trans-unit>
        <trans-unit id="e7dad232dab7fc7fe4bb0ec13d7d0a07fef6ce88" translate="yes" xml:space="preserve">
          <source>In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve &lt;code&gt;n_iter_no_change&lt;/code&gt; times in a row. The improvement is evaluated with absolute tolerance &lt;code&gt;tol&lt;/code&gt;, and the algorithm stops in any case after a maximum number of iteration &lt;code&gt;max_iter&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f03bffae8069d1108a85252dc34a4485434865b8" translate="yes" xml:space="preserve">
          <source>In both cases, the kernel&amp;rsquo;s parameters are estimated using the maximum likelihood principle.</source>
          <target state="translated">두 경우 모두 커널의 매개 변수는 최대 우도 원칙을 사용하여 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="dc8004c8d5b437fc6c479e2e5882eb635fa97592" translate="yes" xml:space="preserve">
          <source>In both examples below, the main result is that the empirical covariance estimate, as a non-robust one, is highly influenced by the heterogeneous structure of the observations. Although the robust covariance estimate is able to focus on the main mode of the data distribution, it sticks to the assumption that the data should be Gaussian distributed, yielding some biased estimation of the data structure, but yet accurate to some extent. The One-Class SVM does not assume any parametric form of the data distribution and can therefore model the complex shape of the data much better.</source>
          <target state="translated">아래의 두 가지 예에서, 주된 결과는 비 강건한 것으로 경험적 공분산 추정치가 이질적인 관측 구조에 의해 크게 영향을 받는다는 것입니다. 강력한 공분산 추정은 데이터 분포의 주 모드에 초점을 맞출 수 있지만, 데이터가 가우스 분포되어야한다는 가정을 고수하여 데이터 구조에 대한 편향된 추정치를 산출하지만 어느 정도는 정확합니다. One-Class SVM은 파라 메트릭 형태의 데이터 분포를 가정하지 않으므로 복잡한 데이터 형태를 훨씬 더 잘 모델링 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7f2b051010be4e679b8556fa182a1724fa1e49b9" translate="yes" xml:space="preserve">
          <source>In case the file contains a pairwise preference constraint (known as &amp;ldquo;qid&amp;rdquo; in the svmlight format) these are ignored unless the query_id parameter is set to True. These pairwise preference constraints can be used to constraint the combination of samples when using pairwise loss functions (as is the case in some learning to rank problems) so that only pairs with the same query_id value are considered.</source>
          <target state="translated">파일에 쌍별 환경 설정 제한 조건 (svmlight 형식에서 &quot;qid&quot;라고 함)이 포함 된 경우 query_id 매개 변수가 True로 설정되어 있지 않으면 무시됩니다. 이러한 pairwise preference constraints는 pairwise loss 함수를 사용할 때 (일부 학습에서 문제 순위를 매기는 경우와 같이) 동일한 query_id 값을 가진 쌍만 고려되도록 샘플 조합을 제한하는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b2d3bbc7ab1d1edcd850a10f6fb01a251c14a28b" translate="yes" xml:space="preserve">
          <source>In case unknown categories are encountered (all zero&amp;rsquo;s in the one-hot encoding), &lt;code&gt;None&lt;/code&gt; is used to represent this category.</source>
          <target state="translated">알 수없는 범주가 발견되면 (One-hot 인코딩에서 모두 0) 이 범주를 나타내는 데 &lt;code&gt;None&lt;/code&gt; 이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="d452896d1312cac0434176b68ca8ca9a2bb1195e" translate="yes" xml:space="preserve">
          <source>In case unknown categories are encountered (all zeros in the one-hot encoding), &lt;code&gt;None&lt;/code&gt; is used to represent this category.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca499264726caa99e06bab43fc3d4644ea84df01" translate="yes" xml:space="preserve">
          <source>In cases where not all of a pairwise distance matrix needs to be stored at once, this is used to calculate pairwise distances in &lt;code&gt;working_memory&lt;/code&gt;-sized chunks. If &lt;code&gt;reduce_func&lt;/code&gt; is given, it is run on each chunk and its return values are concatenated into lists, arrays or sparse matrices.</source>
          <target state="translated">페어 단위 거리 행렬을 모두 한 번에 저장할 필요가없는 경우 &lt;code&gt;working_memory&lt;/code&gt; 크기의 청크 에서 페어 단위 거리를 계산하는 데 사용됩니다 . 경우 &lt;code&gt;reduce_func&lt;/code&gt; 이 주어집니다, 각 덩어리에서 실행하고 반환 값은 목록, 배열 또는 희소 행렬로 연결됩니다.</target>
        </trans-unit>
        <trans-unit id="261de18f8066fcaced5cb3f145cb26c170301e09" translate="yes" xml:space="preserve">
          <source>In cases where the data is not uniformly sampled, radius-based neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt;&lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt;&lt;/a&gt; can be a better choice. The user specifies a fixed radius \(r\), such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;.</source>
          <target state="translated">데이터가 균일하게 샘플링되지 않은 경우 &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt; &lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt; 의&lt;/a&gt; 반경 기반 이웃 분류 가 더 나은 선택이 될 수 있습니다. 사용자는 고정 반경 \ (r \)을 지정하여 스파 서 이웃의 포인트가 분류에 가장 가까운 이웃을 사용하도록합니다. 고차원 매개 변수 공간의 경우이 방법은 소위 &quot;차원의 저주&quot;로 인해 덜 효과적입니다.</target>
        </trans-unit>
        <trans-unit id="46149a533d1136e96a72fc2595f06ccb02814862" translate="yes" xml:space="preserve">
          <source>In certain cases Theil-Sen performs better than &lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt; which is also a robust method. This is illustrated in the second example below where outliers with respect to the x-axis perturb RANSAC. Tuning the &lt;code&gt;residual_threshold&lt;/code&gt; parameter of RANSAC remedies this but in general a priori knowledge about the data and the nature of the outliers is needed. Due to the computational complexity of Theil-Sen it is recommended to use it only for small problems in terms of number of samples and features. For larger problems the &lt;code&gt;max_subpopulation&lt;/code&gt; parameter restricts the magnitude of all possible combinations of p subsample points to a randomly chosen subset and therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger problems with the drawback of losing some of its mathematical properties since it then works on a random subset.</source>
          <target state="translated">어떤 경우에는 Theil-Sen이 &lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt; 보다 성능이 좋으며 이는 강력한 방법이기도합니다. 이것은 x 축 섭동 RANSAC에 대한 특이 치가있는 아래 두 번째 예에 설명되어 있습니다. 튜닝 &lt;code&gt;residual_threshold&lt;/code&gt; 의 RANSAC 요법이의하지만 데이터 및 필요한 이상치의 성격에 대한 일반적인 사전 지식이 매개 변수를. Theil-Sen의 계산상의 복잡성으로 인해 샘플 및 기능 수의 측면에서 작은 문제에 대해서만 사용하는 것이 좋습니다. 더 큰 문제의 경우 &lt;code&gt;max_subpopulation&lt;/code&gt; 파라미터는 p 개의 서브 샘플 포인트의 모든 가능한 조합의 크기를 무작위로 선택된 서브 세트로 제한하므로 런타임도 제한합니다. 따라서 Theil-Sen은 임의의 하위 집합에서 작동하기 때문에 일부 수학적 속성을 잃는 단점이있는 더 큰 문제에 적용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="08403787ed9849b402f6d04f68a0bae46063dfaf" translate="yes" xml:space="preserve">
          <source>In contrast to &lt;a href=&quot;#id13&quot;&gt;Bayesian Ridge Regression&lt;/a&gt;, each coordinate of \(w_{i}\) has its own standard deviation \(\lambda_i\). The prior over all \(\lambda_i\) is chosen to be the same gamma distribution given by hyperparameters \(\lambda_1\) and \(\lambda_2\).</source>
          <target state="translated">&lt;a href=&quot;#id13&quot;&gt;Bayesian Ridge Regression&lt;/a&gt; 과 달리 \ (w_ {i} \)의 각 좌표에는 자체 표준 편차 \ (\ lambda_i \)가 있습니다. 이전의 모든 \ (\ lambda_i \)는 하이퍼 파라미터 \ (\ lambda_1 \) 및 \ (\ lambda_2 \)에 의해 제공된 동일한 감마 분포로 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="f00603c6aeddee02bf1dc7fdebe82c8fea70d634" translate="yes" xml:space="preserve">
          <source>In contrast to &lt;a href=&quot;#id9&quot;&gt;Bayesian Ridge Regression&lt;/a&gt;, each coordinate of \(w_{i}\) has its own standard deviation \(\lambda_i\). The prior over all \(\lambda_i\) is chosen to be the same gamma distribution given by hyperparameters \(\lambda_1\) and \(\lambda_2\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="741dc2ca1b0b96b753a4293cdc66da483cb961b9" translate="yes" xml:space="preserve">
          <source>In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.</source>
          <target state="translated">GridSearchCV와 달리 모든 매개 변수 값이 시도되는 것이 아니라 지정된 분포에서 고정 된 수의 매개 변수 설정이 샘플링됩니다. 시도 된 매개 변수 설정의 수는 n_iter에 의해 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="f7007cbebb915951a7329a621ec59e7bfd3c1528" translate="yes" xml:space="preserve">
          <source>In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.</source>
          <target state="translated">다수 투표 (하드 투표)와 달리 소프트 투표는 클래스 레이블을 예측 된 확률의 합의 argmax로 반환합니다.</target>
        </trans-unit>
        <trans-unit id="a5222e41535c7e60d0bed8020d5a39a4cdb9c58d" translate="yes" xml:space="preserve">
          <source>In contrast to the original publication &lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]&lt;/a&gt;, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.</source>
          <target state="translated">최초의 출판 &lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]&lt;/a&gt; 과는 달리, scikit-learn 구현은 각 분류자가 단일 클래스에 투표하는 대신 확률 론적 예측을 평균화하여 분류기를 결합합니다.</target>
        </trans-unit>
        <trans-unit id="092465bd0b61837459fb29bf14c2dda6ed20e949" translate="yes" xml:space="preserve">
          <source>In contrast to the regression setting, the posterior of the latent function \(f\) is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">회귀 설정과 달리 가우시안 가능성은 이산 클래스 레이블에 부적합하기 때문에 GP의 경우에도 잠재 함수 \ (f \)의 후부가 가우시안이 아닙니다. 오히려, 로지스틱 링크 함수 (logit)에 대응하는 비 가우시안 가능성이 사용된다. GaussianProcessClassifier는 Laplace 근사법에 따라 가우시안이 아닌 후부를 근사합니다. 자세한 내용은 &lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]&lt;/a&gt; 3 장에서 확인할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8da23c4fea95dfb9e1a4723525c1617ef732103e" translate="yes" xml:space="preserve">
          <source>In contrast, for small amounts of data, the training score of the SVM is much greater than the validation score. Adding more training samples will most likely increase generalization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d7f12a42ea8277b24625f1aff8d53cb363a14e0" translate="yes" xml:space="preserve">
          <source>In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to \(\frac{1}{\text{n\_classes}}\).</source>
          <target state="translated">반대로, 분류자가 불균형 테스트 세트를 이용하기 때문에 기존 정확도가 우연 일 경우에만 균형 정확도가 \ (\ frac {1} {\ text {n \ _classes}} \로 떨어집니다. ).</target>
        </trans-unit>
        <trans-unit id="8343cdcc0bd2973a4149cb63a31822f5be571a78" translate="yes" xml:space="preserve">
          <source>In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to \(\frac{1}{n\_classes}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89611c1358b346353d5469c5670ea64fb02fcdd7" translate="yes" xml:space="preserve">
          <source>In descending order of quality, when trained (outside of this example) on all 4 features using 30 estimators and scored using 10 fold cross validation, we see:</source>
          <target state="translated">내림차순으로, 30 개의 추정자를 사용하여 4 가지 기능 모두에 대해 훈련하고 (이 예제 외부) 10 배 교차 검증을 사용하여 점수를 매길 때,</target>
        </trans-unit>
        <trans-unit id="0732cca6c2251b860da4c331fa5748d479b14945" translate="yes" xml:space="preserve">
          <source>In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).</source>
          <target state="translated">앙상블 알고리즘에서 배깅 방법은 일련의 알고리즘 클래스를 구성하여 원래 트레이닝 세트의 임의의 서브 세트에 블랙 박스 추정기의 여러 인스턴스를 구축 한 다음 개별 예측을 집계하여 최종 예측을 형성합니다. 이 방법들은 임의의 구성 절차에 랜덤 화를 도입 한 후 그로부터 앙상블을 만들어서베이스 추정기 (예를 들어, 결정 트리)의 분산을 줄이는 방법으로 사용됩니다. 많은 경우 배깅 방법은 기본 알고리즘을 적용 할 필요없이 단일 모델과 관련하여 개선 할 수있는 매우 간단한 방법입니다. 과적 합을 줄이는 방법을 제공하므로 배깅 방법은 일반적으로 약한 모델 (예 :얕은 의사 결정 나무).</target>
        </trans-unit>
        <trans-unit id="5305d1e9b70806a8391e61e804a0df6abd8f6cc5" translate="yes" xml:space="preserve">
          <source>In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the &lt;code&gt;average&lt;/code&gt; parameter.</source>
          <target state="translated">이진 메트릭을 멀티 클래스 또는 멀티 라벨 문제로 확장 할 때 데이터는 각 클래스마다 하나씩 이진 문제 모음으로 처리됩니다. 클래스 집합에 걸쳐 이진 메트릭 계산을 평균화하는 여러 가지 방법이 있으며, 각 방법은 일부 시나리오에서 유용 할 수 있습니다. 가능한 경우 &lt;code&gt;average&lt;/code&gt; 매개 변수를 사용하여 이들 중에서 선택해야합니다 .</target>
        </trans-unit>
        <trans-unit id="e87cfc9dff0fe670bd40ebf7e26edaa15ca842ad" translate="yes" xml:space="preserve">
          <source>In extremely randomized trees (see &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt;&lt;code&gt;ExtraTreesRegressor&lt;/code&gt;&lt;/a&gt; classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias:</source>
          <target state="translated">매우 무작위 화 된 트리 ( &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt; &lt;code&gt;ExtraTreesRegressor&lt;/code&gt; &lt;/a&gt; 클래스 참조 )에서 분할은 계산 방식에서 한 단계 더 나아갑니다. 임의 포리스트에서와 같이 후보 기능의 임의의 하위 집합이 사용되지만 가장 차별화 된 임계 값을 찾는 대신 각 후보 기능에 대해 임계 값이 무작위로 생성되며 이러한 무작위로 생성 된 임계 값 중 최고가 분할 규칙으로 선택됩니다. 이것은 일반적으로 바이어스의 약간 더 큰 증가를 희생시키면서 모델의 분산을 조금 더 줄일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5c1305e3ce4cbb99adc8d313e42a43efab81ea5c" translate="yes" xml:space="preserve">
          <source>In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:</source>
          <target state="translated">실제로이 데이터 세트에는 하나의 버전 만 있습니다. 반면 홍채 데이터 세트에는 여러 버전이 있습니다.</target>
        </trans-unit>
        <trans-unit id="63493dde535d33b43819cf48666bb2a9620c2476" translate="yes" xml:space="preserve">
          <source>In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.</source>
          <target state="translated">프랑스어로되어 있지만 참조 : Tenenhaus, M. (1998). La regression PLS : 이론과 실용. 파리 : 에디션 테크닉.</target>
        </trans-unit>
        <trans-unit id="6e95c3ada3b2525ed5f608da19594b4a42ad3dc4" translate="yes" xml:space="preserve">
          <source>In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:</source>
          <target state="translated">일반적으로 대량 예측 (많은 인스턴스가 동시에)을 수행하는 것은 여러 가지 이유 (분기 예측 성, CPU 캐시, 선형 대수 라이브러리 최적화 등)로 인해 더 효율적입니다. 여기서는 추정기 선택과 무관하게 벌크 모드가 항상 더 빠르며 일부 기능의 경우 1-2 배 정도 큰 기능이 거의없는 설정에서 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="73d5a0649f1537ceaa4a43b2819de8ab34f4f95d" translate="yes" xml:space="preserve">
          <source>In general, &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; is a technique used for analyzing similarity or dissimilarity data. It attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5f14cdf8cb9c0df1b6ffce69bd866cdeffd9355" translate="yes" xml:space="preserve">
          <source>In general, a learning problem considers a set of n &lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;samples&lt;/a&gt; of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;multivariate&lt;/a&gt; data), it is said to have several attributes or &lt;strong&gt;features&lt;/strong&gt;.</source>
          <target state="translated">일반적으로 학습 문제는 n 개의 데이터 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;샘플&lt;/a&gt; 집합을 고려한 다음 알 수없는 데이터의 속성을 예측하려고합니다. 각 표본이 단일 숫자 이상이고 다차원 항목 (일명 &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;다변량&lt;/a&gt; 데이터) 인 경우 여러 특성 또는 &lt;strong&gt;특징&lt;/strong&gt; 이 있다고 &lt;strong&gt;합니다&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="9cf7334c38597a2189c7af702ab9abdbe9f10093" translate="yes" xml:space="preserve">
          <source>In general, is a technique used for analyzing similarity or dissimilarity data. &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.</source>
          <target state="translated">일반적으로 유사성 또는 비 유사성 데이터를 분석하는 데 사용되는 기술입니다. &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt; 는 형상 공간에서 거리로 유사성 또는 비 유사성 데이터를 모델링하려고 시도합니다. 데이터는 객체 간의 유사성, 분자의 상호 작용 빈도 또는 국가 간 거래 지수 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="71aab6786f00490669e72ac36911ce2d2486dab4" translate="yes" xml:space="preserve">
          <source>In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding \(p\)-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.</source>
          <target state="translated">일반적으로, 초기 관측 분포의 윤곽을 한정하는 거칠고 가까운 프론티어를 배우려고합니다. 그런 다음 추가 관측치가 경계로 구분 된 하위 공간 내에 있으면 초기 관측치와 동일한 모집단에서 온 것으로 간주됩니다. 그렇지 않은 경우, 그들이 국경을 벗어난 경우, 우리는 평가에 대한 확신을 가지고 비정상적이라고 말할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c9bca25ec918e4e036ec8a37ec502896ec56d542" translate="yes" xml:space="preserve">
          <source>In general, learning algorithms benefit from standardization of the data set. If some outliers are present in the set, robust scalers or transformers are more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in &lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;Compare the effect of different scalers on data with outliers&lt;/a&gt;.</source>
          <target state="translated">일반적으로 학습 알고리즘은 데이터 세트의 표준화를 통해 이점을 얻습니다. 세트에 일부 이상 치가있는 경우 강력한 스케일러 또는 변압기가 더 적합합니다. 한계 이상 값을 포함하는 데이터 세트에서 다양한 스케일러, 변환기 및 노멀 라이저의 동작은 데이터에 대한 &lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;다른 스케일러의 효과와 특이 값 비교에서&lt;/a&gt; 강조 표시됩니다 .</target>
        </trans-unit>
        <trans-unit id="baeb2b7a43c2bc0dd04675c021d6ed663a58bf2d" translate="yes" xml:space="preserve">
          <source>In general, the run time cost to construct a balanced binary tree is \(O(n_{samples}n_{features}\log(n_{samples}))\) and query time \(O(\log(n_{samples}))\). Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through \(O(n_{features})\) to find the feature that offers the largest reduction in entropy. This has a cost of \(O(n_{features}n_{samples}\log(n_{samples}))\) at each node, leading to a total cost over the entire trees (by summing the cost at each node) of \(O(n_{features}n_{samples}^{2}\log(n_{samples}))\).</source>
          <target state="translated">일반적으로 균형 이진 트리를 구성하는 런타임 비용은 \ (O (n_ {samples} n_ {features} \ log (n_ {samples})) \)이고 쿼리 시간은 \ (O (\ log (n_ {samples) })) \). 트리 구성 알고리즘은 균형 트리를 생성하려고 시도하지만 항상 균형을 유지하는 것은 아닙니다. 하위 트리가 대략 균형을 유지한다고 가정하면 각 노드의 비용은 \ (O (n_ {features}) \)를 통해 검색하여 엔트로피를 가장 많이 줄인 기능을 찾습니다. 각 노드에서 \ (O (n_ {features} n_ {samples} \ log (n_ {samples})) \)의 비용이 발생하므로 전체 트리에 대한 총 비용이 발생합니다 (각 노드의 비용을 합산). \ (O (n_ {기능} n_ {samples} ^ {2} \ log (n_ {samples})) \)</target>
        </trans-unit>
        <trans-unit id="144a3925f6b19404e9d474c272482fb04a69a6ff" translate="yes" xml:space="preserve">
          <source>In general, when fitting a curve with a polynomial by Bayesian ridge regression, the selection of initial values of the regularization parameters (alpha, lambda) may be important. This is because the regularization parameters are determined by an iterative procedure that depends on initial values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dba314b8268f3eec306fec03d7cfe13e8e090ace" translate="yes" xml:space="preserve">
          <source>In general, when the problem isn&amp;rsquo;t linearly separable, the support vectors are the samples &lt;em&gt;within&lt;/em&gt; the margin boundaries.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="635895acc09f2d99381585bc2d144c9a66a85f3a" translate="yes" xml:space="preserve">
          <source>In gradient descent, the gradient \(\nabla Loss_{W}\) of the loss with respect to the weights is computed and deducted from \(W\). More formally, this is expressed as,</source>
          <target state="translated">경사 하강에서는 가중치에 대한 손실의 경사 \ (\ nabla Loss_ {W} \)가 계산되어 \ (W \)에서 차감됩니다. 보다 공식적으로 이것은 다음과 같이 표현됩니다.</target>
        </trans-unit>
        <trans-unit id="2c51a2af5a19ac0ce7e4fb04fd6d887c03b6fecb" translate="yes" xml:space="preserve">
          <source>In high-dimensional spaces, linear classifiers often achieve excellent accuracy. For sparse binary data, BernoulliNB is particularly well-suited. The bottom row compares the decision boundary obtained by BernoulliNB in the transformed space with an ExtraTreesClassifier forests learned on the original data.</source>
          <target state="translated">고차원 공간에서 선형 분류기는 종종 우수한 정확도를 달성합니다. 희소 이진 데이터의 경우 BernoulliNB가 특히 적합합니다. 맨 아래 행은 변환 된 공간에서 BernoulliNB가 획득 한 의사 결정 경계를 원래 데이터에서 학습 한 ExtraTreesClassifier 포리스트와 비교합니다.</target>
        </trans-unit>
        <trans-unit id="26c26ee3d75b66c7f22fed706da52f459434240f" translate="yes" xml:space="preserve">
          <source>In linear models, the target value is modeled as a linear combination of the features (see the &lt;a href=&quot;../../modules/linear_model#linear-model&quot;&gt;Linear Models&lt;/a&gt; User Guide section for a description of a set of linear models available in scikit-learn). Coefficients in multiple linear models represent the relationship between the given feature, \(X_i\) and the target, \(y\), assuming that all the other features remain constant (&lt;a href=&quot;https://en.wikipedia.org/wiki/Conditional_dependence&quot;&gt;conditional dependence&lt;/a&gt;). This is different from plotting \(X_i\) versus \(y\) and fitting a linear relationship: in that case all possible values of the other features are taken into account in the estimation (marginal dependence).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2367cf553e95efae790eac559ef2be19cd28f503" translate="yes" xml:space="preserve">
          <source>In machine-learning practice, Ridge Regression is more often used with non-negligible regularization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b577c96674cf299faa19ce0d11e2224d3c2c813" translate="yes" xml:space="preserve">
          <source>In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.</source>
          <target state="translated">다수결에서 특정 샘플에 대한 예측 등급 레이블은 각 개별 분류 기준에 의해 예측 된 등급 레이블의 다수 (모드)를 나타내는 등급 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="589394183aec0e7af2afe4b456559f6baedc9992" translate="yes" xml:space="preserve">
          <source>In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application.</source>
          <target state="translated">따라서 대부분의 경우 기능 추출 코드를 신중하게 시간 지정하고 프로파일 링하는 것이 좋습니다. 전체 지연 시간이 애플리케이션에 비해 너무 느린 경우 최적화를 시작하기에 좋은 장소 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aeae04273a5ed1fc88f796de718e3c2190c04f0d" translate="yes" xml:space="preserve">
          <source>In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness.</source>
          <target state="translated">많은 모델링 시나리오에서 데이터 세트의 기능의 정규성이 바람직합니다. 전력 변환은 분산을 안정화하고 왜도를 최소화하기 위해 모든 분포의 데이터를 가우시안 분포에 가깝게 매핑하는 것을 목표로하는 파라 메트릭 모노 토닉 변환 패밀리입니다.</target>
        </trans-unit>
        <trans-unit id="c82f65d47c3f4e11ad468a4165bdc787c51720a5" translate="yes" xml:space="preserve">
          <source>In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use &lt;code&gt;FeatureUnion&lt;/code&gt; to combine features obtained by PCA and univariate selection.</source>
          <target state="translated">많은 실제 사례에서 데이터 세트에서 기능을 추출하는 방법에는 여러 가지가 있습니다. 좋은 성능을 얻기 위해 여러 가지 방법을 결합하는 것이 종종 유리합니다. 이 예는 &lt;code&gt;FeatureUnion&lt;/code&gt; 을 사용 하여 PCA와 일 변량 선택에서 얻은 기능을 결합 하는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="9c0b7f3861d3fe001968b978c49f3447d1233fa3" translate="yes" xml:space="preserve">
          <source>In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.</source>
          <target state="translated">수학에서 Johnson-Lindenstrauss의 정리는 고차원에서 저 차원 유클리드 공간으로 점의 왜곡이 적은 임베딩에 관한 결과입니다. 정리는 고차원 공간의 작은 점 집합이 점들 사이의 거리가 거의 보존되는 방식으로 훨씬 낮은 차원의 공간에 포함될 수 있다고 말합니다. 임베딩에 사용 된 맵은 적어도 Lipschitz이며 직교 투영으로도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="35a3805825da50966c5f8cb649b1d2ea852b8f59" translate="yes" xml:space="preserve">
          <source>In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of \(v\) and \(h\) given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes.</source>
          <target state="translated">로그 우도를 최대화 할 때 양의 기울기는 모형이 관측 된 훈련 데이터와 호환되는 숨겨진 상태를 선호하게합니다. RBM의 이분 구조로 인해 효율적으로 계산할 수 있습니다. 그러나 음의 구배는 다루기 어렵습니다. 이 모델의 목표는 모델이 선호하는 조인트 상태의 에너지를 낮추어 데이터에 충실하게 유지하는 것입니다. 체인이 혼합 될 때까지 다른 주어진 \ (v \) 및 \ (h \) 각각을 반복적으로 샘플링하여 블록 Gibbs 샘플링을 사용하여 Markov 체인 Monte Carlo에 의해 근사화 될 수 있습니다. 이러한 방식으로 생성 된 샘플을 판타지 입자라고도합니다. 이는 비효율적이며 Markov 체인의 혼합 여부를 결정하기가 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="54db7da5f1b2e2f16e8f4dc3a375dac661b78213" translate="yes" xml:space="preserve">
          <source>In multi-label classification, the &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function is extended by averaging over the labels as &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;.</source>
          <target state="translated">다중 레이블 분류에서 &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt; 함수는 &lt;a href=&quot;#average&quot;&gt;위와 같이&lt;/a&gt; 레이블을 평균화하여 확장됩니다 .</target>
        </trans-unit>
        <trans-unit id="d9be5dcb267dcb84c278d12d7b1a881ada760886" translate="yes" xml:space="preserve">
          <source>In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.</source>
          <target state="translated">다중 레이블 분류에서 이는 각 레이블 집합을 올바르게 예측해야하는 각 샘플에 대해 필요하기 때문에 가혹한 메트릭 인 하위 집합 정확도입니다.</target>
        </trans-unit>
        <trans-unit id="9ff5420b9cd3095ee44bf9941c38c72dce6d517a" translate="yes" xml:space="preserve">
          <source>In multi-label settings</source>
          <target state="translated">다중 라벨 설정에서</target>
        </trans-unit>
        <trans-unit id="cf7a69d811fd496380ea6a3966d13bf17ca83f43" translate="yes" xml:space="preserve">
          <source>In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the &lt;code&gt;average&lt;/code&gt; argument to the &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; (multilabel only), &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt;&lt;code&gt;f1_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt;&lt;code&gt;precision_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt;&lt;code&gt;recall_score&lt;/code&gt;&lt;/a&gt; functions, as described &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;. Note that if all labels are included, &amp;ldquo;micro&amp;rdquo;-averaging in a multiclass setting will produce precision, recall and \(F\) that are all identical to accuracy. Also note that &amp;ldquo;weighted&amp;rdquo; averaging may produce an F-score that is not between precision and recall.</source>
          <target state="translated">멀티 클래스 및 멀티 라벨 분류 작업에서 정밀도, 리콜 및 F 측정의 개념을 각 라벨에 독립적으로 적용 할 수 있습니다. 설명 된 것처럼 &lt;code&gt;average&lt;/code&gt; 인수에 의해 &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt; (멀티 라벨 에만 해당), &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt; &lt;code&gt;f1_score&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt; &lt;code&gt;precision_score&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt; &lt;code&gt;recall_score&lt;/code&gt; &lt;/a&gt; 함수 에 대한 평균 인수로 지정된 레이블간에 결과를 결합하는 몇 가지 방법이 있습니다.&lt;a href=&quot;#average&quot;&gt; 위에서 것처럼&lt;/a&gt;. 모든 레이블이 포함 된 경우 멀티 클래스 설정에서 &quot;마이크로&quot;평균은 정확도와 동일한 정밀도, 리콜 및 \ (F \)를 생성합니다. 또한 &quot;가중&quot;평균화는 정밀도와 리콜 사이에 있지 않은 F- 점수를 생성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="afc91520f5287da47360dcd6fd00b4fb446bcf96" translate="yes" xml:space="preserve">
          <source>In multiclass case, the function expects that either all the labels are included in y_true or an optional labels argument is provided which contains all the labels. The multilabel margin is calculated according to Crammer-Singer&amp;rsquo;s method. As in the binary case, the cumulated hinge loss is an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">멀티 클래스의 경우, 함수는 모든 레이블이 y_true에 포함되거나 모든 레이블을 포함하는 선택적 레이블 인수가 제공 될 것으로 예상합니다. 다중 레이블 마진은 Crammer-Singer의 방법에 따라 계산됩니다. 이진 경우와 마찬가지로 누적 힌지 손실은 분류자가 실수 수의 상한입니다.</target>
        </trans-unit>
        <trans-unit id="a7ec36140af641cfb5e4e5e11dec536798cfb2f8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss correspond to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is equivalent to the subset &lt;code&gt;zero_one_loss&lt;/code&gt; function.</source>
          <target state="translated">멀티 클래스 분류에서 해밍 손실은 &lt;code&gt;y_true&lt;/code&gt; 와 &lt;code&gt;y_pred&lt;/code&gt; 사이의 해밍 거리에 해당하며 이는 하위 집합 &lt;code&gt;zero_one_loss&lt;/code&gt; 함수 와 같습니다 .</target>
        </trans-unit>
        <trans-unit id="a514b0b14d02249930d02d183e261b474a100dbd" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss corresponds to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is equivalent to the subset &lt;code&gt;zero_one_loss&lt;/code&gt; function, when &lt;code&gt;normalize&lt;/code&gt; parameter is set to True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff1916ae5265c4d87d1472e5cc3e0c2594a22de8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss corresponds to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is similar to the &lt;a href=&quot;#zero-one-loss&quot;&gt;Zero one loss&lt;/a&gt; function. However, while zero-one loss penalizes prediction sets that do not strictly match true sets, the Hamming loss penalizes individual labels. Thus the Hamming loss, upper bounded by the zero-one loss, is always between zero and one, inclusive; and predicting a proper subset or superset of the true labels will give a Hamming loss between zero and one, exclusive.</source>
          <target state="translated">멀티 클래스 분류에서 해밍 손실은 &lt;code&gt;y_true&lt;/code&gt; 와 &lt;code&gt;y_pred&lt;/code&gt; 사이의 해밍 거리에 해당 하며 &lt;a href=&quot;#zero-one-loss&quot;&gt;Zero one loss&lt;/a&gt; 함수 와 유사 합니다. 그러나 0 대 1 손실은 실제 세트와 정확히 일치하지 않는 예측 세트에 불이익을 주지만 해밍 손실은 개별 레이블에 불이익을줍니다. 따라서 0-1 손실로 상한 인 해밍 손실은 항상 0과 1 사이입니다. 진정한 라벨의 적절한 서브셋 또는 수퍼 셋을 예측하면 0에서 1 사이의 해밍 손실이 발생합니다.</target>
        </trans-unit>
        <trans-unit id="cf7ce831a18d046dad4e38dc2cae92648b792778" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt;&lt;code&gt;zero_one_loss&lt;/code&gt;&lt;/a&gt; scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets. To get the count of such subsets instead, set &lt;code&gt;normalize&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">다중 레이블 분류에서 &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt; &lt;code&gt;zero_one_loss&lt;/code&gt; &lt;/a&gt; 는 레이블이 예측과 정확히 일치하는 경우 하위 집합을 1로, 오류가있는 경우 0으로 점수를 매 깁니다 . 기본적으로이 함수는 불완전하게 예측 된 부분 집합의 백분율을 반환합니다. 대신 이러한 하위 집합의 수를 얻으려면 &lt;code&gt;normalize&lt;/code&gt; 를 &lt;code&gt;False&lt;/code&gt; 로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="2cdc777c3fd9aacea19e984339f1423c55608098" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes the individual labels.</source>
          <target state="translated">다중 레이블 분류에서 해밍 손실은 부분 집합 0 대 손실과 다릅니다. 일대일 손실은 주어진 샘플에 대한 전체 레이블 세트가 실제 레이블 세트와 완전히 일치하는 경우 올바르지 않은 것으로 간주합니다. 해밍 손실은 개별 레이블에 불이익을 준다는 점에서 더 관대합니다.</target>
        </trans-unit>
        <trans-unit id="602aeb7c2d89b27ea6d03c59146d4b4fecde4c31" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does not entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes only the individual labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00e9bece59054d08c4ac787e06eeb4fc8070bdab" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.</source>
          <target state="translated">다중 레이블 분류에서 함수는 부분 집합 정확도를 반환합니다. 표본에 대한 전체 예측 레이블 세트가 실제 레이블 세트와 완전히 일치하면 부분 집합 정확도는 1.0입니다. 그렇지 않으면 0.0입니다.</target>
        </trans-unit>
        <trans-unit id="7cd1b88a6c55666089bdc7543f7e259d70d5898d" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the zero_one_loss function corresponds to the subset zero-one loss: for each sample, the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one.</source>
          <target state="translated">다중 레이블 분류에서 zero_one_loss 함수는 하위 집합의 0 대 1 손실에 해당합니다. 각 샘플에 대해 전체 레이블 세트를 정확하게 예측해야합니다. 그렇지 않으면 해당 샘플의 손실이 1과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c56a96e702a01557c0cb1c7c6c5d254cdaebcc8b" translate="yes" xml:space="preserve">
          <source>In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must &lt;em&gt;exactly&lt;/em&gt; match the corresponding set of labels in y_true.</source>
          <target state="translated">다중 레이블 분류에서이 함수는 부분 집합 정확도를 계산합니다. 샘플에 대해 예측 된 레이블 세트는 y_true의 해당 레이블 세트와 &lt;em&gt;정확히&lt;/em&gt; 일치 해야합니다 .</target>
        </trans-unit>
        <trans-unit id="00440d1e0316ae49b10a616cf581f0acff1a935a" translate="yes" xml:space="preserve">
          <source>In multilabel confusion matrix \(MCM\), the count of true negatives is \(MCM_{:,0,0}\), false negatives is \(MCM_{:,1,0}\), true positives is \(MCM_{:,1,1}\) and false positives is \(MCM_{:,0,1}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3fad4287dcc0210ad8169708b233947ca706f077" translate="yes" xml:space="preserve">
          <source>In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels.</source>
          <target state="translated">멀티 라벨 학습에서, 각 샘플은 그와 관련된 다수의 그라운드 진실 라벨을 가질 수 있습니다. 목표는 사실 점수에 높은 점수와 더 나은 순위를 부여하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="9d6449537c42279d12e406059563c338784d06f3" translate="yes" xml:space="preserve">
          <source>In multilabel learning, the joint set of binary classification tasks is expressed with label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values: the one, i.e. the non zero elements, corresponds to the subset of labels. An array such as &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample.</source>
          <target state="translated">다중 레이블 학습에서 이진 분류 작업의 공동 세트는 레이블 이진 표시기 배열로 표현됩니다. 각 샘플은 이진 값을 갖는 2 차원 형상 배열 (n_samples, n_classes)의 한 행입니다. 즉, 0이 아닌 요소는 라벨의 부분 집합 &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; 와 같은 배열 은 첫 번째 샘플의 레이블 0, 두 번째 샘플의 레이블 1 및 2를 나타냅니다. , 세 번째 샘플에는 라벨이 없습니다.</target>
        </trans-unit>
        <trans-unit id="65f6ef4e3d2b7a1359958abf87c802c2de77e1d9" translate="yes" xml:space="preserve">
          <source>In normal usage, the Calinski-Harabasz index is applied to the results of a cluster analysis:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c2c0f769c8a98dc6df3f2e7afe566ac80c0f339" translate="yes" xml:space="preserve">
          <source>In normal usage, the Calinski-Harabaz index is applied to the results of a cluster analysis.</source>
          <target state="translated">일반적으로 Calinski-Harabaz 지수는 군집 분석 결과에 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="5f0c7d20ec265094d1673fd625fd38165b384452" translate="yes" xml:space="preserve">
          <source>In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows:</source>
          <target state="translated">일반적인 사용법에서 Davies-Bouldin 지수는 다음과 같이 군집 분석 결과에 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="0488e7351783ef8ef785f4bdea49af8c75724adf" translate="yes" xml:space="preserve">
          <source>In normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis.</source>
          <target state="translated">일반적으로 Silhouette Coefficient는 군집 분석 결과에 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="af7916eabb756a4304309b1e18ceea097a7a5071" translate="yes" xml:space="preserve">
          <source>In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs should thus be taken into account. Many such models will thus be casted as &amp;ldquo;Structured output&amp;rdquo; problems which are currently outside of the scope of scikit-learn.</source>
          <target state="translated">자연어 이해의 광범위한 과제를 해결하기 위해 문장과 단락의 현지 구조를 고려해야합니다. 따라서 이러한 많은 모델은 현재 scikit-learn의 범위를 벗어난 &quot;구조적 출력&quot;문제로 분류됩니다.</target>
        </trans-unit>
        <trans-unit id="819693d214fc959100941f9c2bf3cb570fc069ec" translate="yes" xml:space="preserve">
          <source>In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:</source>
          <target state="translated">이 문제를 해결하기 위해 scikit-learn은 다음과 같이 텍스트 내용에서 숫자 기능을 추출하는 가장 일반적인 방법에 대한 유틸리티를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="5bdd52099ccc039c40b609f18b326c63aea62fae" translate="yes" xml:space="preserve">
          <source>In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the &lt;code&gt;scipy.sparse&lt;/code&gt; package.</source>
          <target state="translated">이러한 행렬을 메모리에 저장하고 대수 연산 행렬 / 벡터의 속도를 높이기 위해 구현에서는 일반적으로 &lt;code&gt;scipy.sparse&lt;/code&gt; 패키지 에서 사용 가능한 구현과 같은 희소 표현을 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="af0531a207de85560d0c6e1dcc4e5a478aa65d8d" translate="yes" xml:space="preserve">
          <source>In order to build histograms, the input data &lt;code&gt;X&lt;/code&gt; needs to be binned into integer-valued bins. This binning procedure does require sorting the feature values, but it only happens once at the very beginning of the boosting process (not at each node, like in &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0bf98f40bc311f4824763dea8c552bc0812d861" translate="yes" xml:space="preserve">
          <source>In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; as demonstrated in the following example that extract &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt; vectors of unigram tokens from a subset of 20news:</source>
          <target state="translated">텍스트 데이터로 예측 또는 군집 모델을 제공하려면 먼저 텍스트를 통계 분석에 적합한 숫자 값의 벡터로 변환해야합니다. 이는 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 의 서브 세트에서 유니 그램 토큰의 &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt; 벡터 를 추출하는 다음 예에서 설명 된 sklearn.feature_extraction.text 유틸리티를 사용하여 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c3614fb1e15f18200960459d2e1c203458a6eae2" translate="yes" xml:space="preserve">
          <source>In order to fit linear models with those predictors it is therefore necessary to perform standard feature transformations as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a439a73e36b65ee0a94b3f1d9d89e3ac154697cf" translate="yes" xml:space="preserve">
          <source>In order to get faster execution times for this first example we will work on a partial dataset with only 4 categories out of the 20 available in the dataset:</source>
          <target state="translated">이 첫 번째 예제의 실행 시간을 단축하기 위해 데이터 세트에서 사용 가능한 20 개 중 4 개 범주 만 사용하는 부분 데이터 세트에 대해 작업합니다.</target>
        </trans-unit>
        <trans-unit id="da7edac191ef2f2a6bab6d167570c5dc3d626b83" translate="yes" xml:space="preserve">
          <source>In order to learn good latent representations from a small dataset, we artificially generate more labeled data by perturbing the training data with linear shifts of 1 pixel in each direction.</source>
          <target state="translated">작은 데이터 세트에서 좋은 잠재 표현을 배우기 위해 각 방향으로 1 픽셀의 선형 이동으로 학습 데이터를 교란시켜 더 많은 레이블이 지정된 데이터를 인위적으로 생성합니다.</target>
        </trans-unit>
        <trans-unit id="6983d2c6ff1cbf277ea5d9522b128070bfd0a615" translate="yes" xml:space="preserve">
          <source>In order to make the vectorizer =&amp;gt; transformer =&amp;gt; classifier easier to work with, &lt;code&gt;scikit-learn&lt;/code&gt; provides a &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; class that behaves like a compound classifier:</source>
          <target state="translated">벡터 &lt;code&gt;scikit-learn&lt;/code&gt; =&amp;gt; 변환기 =&amp;gt; 분류기를 사용하기 쉽게 만들기 위해 scikit-learn 은 복합 분류기처럼 동작 하는 &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 클래스를 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="5257e11193f291f6f81d5d2347e3cbb71ec9f310" translate="yes" xml:space="preserve">
          <source>In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.</source>
          <target state="translated">텍스트 문서에 대한 기계 학습을 수행하려면 먼저 텍스트 내용을 숫자 피처 벡터로 바꿔야합니다.</target>
        </trans-unit>
        <trans-unit id="7b973d24b18f4331d1cc68b945953f9c40c766fe" translate="yes" xml:space="preserve">
          <source>In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support &lt;code&gt;predict_proba&lt;/code&gt; method):</source>
          <target state="translated">예측 된 클래스 확률을 기반으로 클래스 레이블을 예측하려면 (VotingClassifier의 스키 킷 학습 추정기가 &lt;code&gt;predict_proba&lt;/code&gt; 메소드 를 지원해야합니다 .)</target>
        </trans-unit>
        <trans-unit id="a7ffbb7849ad7a74935991324e062c6b6722378d" translate="yes" xml:space="preserve">
          <source>In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf&amp;ndash;idf transform.</source>
          <target state="translated">카운트 기능을 분류자가 사용하기에 적합한 부동 소수점 값으로 다시 가중하기 위해 tf-idf 변환을 사용하는 것이 매우 일반적입니다.</target>
        </trans-unit>
        <trans-unit id="4707665df8a323c1a68b209bc6166b3798e4ea75" translate="yes" xml:space="preserve">
          <source>In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:</source>
          <target state="translated">이후 버전의 scikit-learn을 사용하여 유사한 모델을 재 구축하려면 추가 된 메타 데이터를 절인 모델과 함께 저장해야합니다.</target>
        </trans-unit>
        <trans-unit id="168239ecf279021917cbfef805f1d7d711ae1c44" translate="yes" xml:space="preserve">
          <source>In order to test if a classification score is significative a technique in repeating the classification procedure after randomizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is greater than the classification score obtained in the first place.</source>
          <target state="translated">분류 점수가 유의한지 테스트하기 위해, 라벨을 무작위 화, 순열 한 후 분류 절차를 반복하는 기술. 그런 다음 p- 값은 획득 한 점수가 처음에 얻은 분류 점수보다 큰 런의 백분율로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="fdc8e1656ba1332f0933f9f656403151b15252d2" translate="yes" xml:space="preserve">
          <source>In other words, return an input X_original whose transform would be X.</source>
          <target state="translated">즉, 변환이 X 인 입력 X_original을 리턴하십시오.</target>
        </trans-unit>
        <trans-unit id="f84fbaf022a2c87e2f72b92c7b8059751d7f8963" translate="yes" xml:space="preserve">
          <source>In other words, we &lt;em&gt;decomposed&lt;/em&gt; matrix \(\mathbf{X}\).</source>
          <target state="translated">즉, 행렬 \ (\ mathbf {X} \)를 &lt;em&gt;분해했습니다&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="573ad5780d66d8749d635925a4f90732aa002652" translate="yes" xml:space="preserve">
          <source>In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:</source>
          <target state="translated">특히 Rosenberg와 Hirschberg (2007)는 모든 클러스터 할당에 대해 다음 두 가지 바람직한 목표를 정의합니다.</target>
        </trans-unit>
        <trans-unit id="dafd8fff090495231531a6dce6a0d9bf23cd3c87" translate="yes" xml:space="preserve">
          <source>In particular in a &lt;strong&gt;supervised setting&lt;/strong&gt; it can be successfully combined with fast and scalable linear models to train &lt;strong&gt;document classifiers&lt;/strong&gt;, for instance:</source>
          <target state="translated">특히 &lt;strong&gt;감독 설정&lt;/strong&gt; 에서 &lt;strong&gt;문서 분류기&lt;/strong&gt; 를 훈련시키기 위해 빠르고 확장 가능한 선형 모델과 성공적으로 결합 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b70db829e86f8b0b87ee4b4ea9165e2800cb135e" translate="yes" xml:space="preserve">
          <source>In particular the interrogative form &amp;ldquo;Is this&amp;rdquo; is only present in the last document:</source>
          <target state="translated">특히&amp;ldquo;이것입니까?&amp;rdquo;라는 의문 양식은 마지막 문서에만 있습니다.</target>
        </trans-unit>
        <trans-unit id="48a72aaef5f57348c3c02ddfbd84f34663c56133" translate="yes" xml:space="preserve">
          <source>In particular we name:</source>
          <target state="translated">특히 우리는</target>
        </trans-unit>
        <trans-unit id="32bae48b70c29501503578b88b61dfab45b0637c" translate="yes" xml:space="preserve">
          <source>In particular, \(\nu = 3/2\):</source>
          <target state="translated">특히 \ (\ nu = 3/2 \) :</target>
        </trans-unit>
        <trans-unit id="204dd46cfb26952328568f02a630bc8ec2809e56" translate="yes" xml:space="preserve">
          <source>In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in &lt;a href=&quot;../classes#module-sklearn.feature_extraction.text&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;&lt;/a&gt;. In that context, it is known as latent semantic analysis (LSA).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff34c019cd4067dd0b8a1a6d1536db31ff351b58" translate="yes" xml:space="preserve">
          <source>In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).</source>
          <target state="translated">특히 잘린 SVD는 sklearn.feature_extraction.text의 벡터 라이저가 반환하는 count / tf-idf 행렬에 대해 작동합니다. 이러한 맥락에서,이를 잠재적 의미 분석 (LSA)이라고합니다.</target>
        </trans-unit>
        <trans-unit id="74d856933005d819af2a4b7d2ce24317b5186453" translate="yes" xml:space="preserve">
          <source>In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plan.</source>
          <target state="translated">실제로 Spectral Clustering은 개별 클러스터의 구조가 볼록하지 않거나 볼록한 경우 또는 클러스터의 중심과 확산이 전체 클러스터에 대한 적절한 설명이 아닐 때 매우 유용합니다. 예를 들어, 클러스터가 2D 계획에 중첩 된 원일 경우.</target>
        </trans-unit>
        <trans-unit id="316ef03a3b1d8845e6fcfccde0af625da5037900" translate="yes" xml:space="preserve">
          <source>In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plane.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21d1a0735c97d36c546ea3246065facc34b4f5a9" translate="yes" xml:space="preserve">
          <source>In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance, when clusters are nested circles on the 2D plane.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5662b7bb73c6d21ae298513382716bd3fe526ba2" translate="yes" xml:space="preserve">
          <source>In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.</source>
          <target state="translated">실제로, 국부 밀도는 k- 최근 접 이웃으로부터 얻어진다. 관측치의 LOF 점수는 k- 최근 접 이웃의 평균 국부 밀도와 자체 국부 밀도의 비율과 동일합니다. 정상 인스턴스는 이웃의 것과 유사한 국부 밀도를 가질 것으로 예상되지만 비정상 데이터는 국부 밀도가 훨씬 더 작을 것으로 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="7ab811a62d63edef5c9695fca6cafd8cc266404c" translate="yes" xml:space="preserve">
          <source>In practice those estimates are stored as an attribute named &lt;code&gt;feature_importances_&lt;/code&gt; on the fitted model. This is an array with shape &lt;code&gt;(n_features,)&lt;/code&gt; whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.</source>
          <target state="translated">실제로 이러한 추정치는 적합 모형에 &lt;code&gt;feature_importances_&lt;/code&gt; 라는 속성으로 저장됩니다 . 값이 양수이고 합계가 1.0 인 형태 &lt;code&gt;(n_features,)&lt;/code&gt; 의 배열입니다 . 값이 클수록 예측 기능에 대한 일치 기능의 기여가 더 중요합니다.</target>
        </trans-unit>
        <trans-unit id="2f91c327e74b19930bc0b8d2d9c2f5d99fe44af7" translate="yes" xml:space="preserve">
          <source>In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.</source>
          <target state="translated">실제로 우리는 종종 분포의 형태를 무시하고 각 피쳐의 평균값을 제거하여 데이터를 중앙에 위치하도록 변환 한 다음 일정하지 않은 피쳐를 표준 편차로 나누어 스케일을 조정합니다.</target>
        </trans-unit>
        <trans-unit id="4c632dd9d37d8e850afe2fbbbdbddfedb108d119" translate="yes" xml:space="preserve">
          <source>In practice, \(\mu\) and \(\Sigma\) are replaced by some estimates. The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set and therefor, the corresponding Mahalanobis distances are. One would better have to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set and that the associated Mahalanobis distances accurately reflect the true organisation of the observations.</source>
          <target state="translated">실제로 \ (\ mu \) 및 \ (\ Sigma \)는 몇 가지 추정치로 대체됩니다. 일반적인 공분산 최대 우도 추정치는 데이터 세트에 특이 치의 존재에 매우 민감하므로 해당 Mahalanobis 거리가 있습니다. 추정값이 데이터 세트의 &quot;오차적인&quot;관측치에 저항성이 있고 관련 Mahalanobis 거리가 관측치의 실제 구성을 정확하게 반영하도록 보장하기 위해 강력한 공분산 추정기를 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="7f19bfe5f66f3783151b8147191d95599d9b587d" translate="yes" xml:space="preserve">
          <source>In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That&amp;rsquo;s why it can be useful to restart it several times.</source>
          <target state="translated">실제로 k- 평균 알고리즘은 매우 빠르지 만 (사용 가능한 가장 빠른 클러스터링 알고리즘 중 하나) 로컬 최소값에 해당합니다. 따라서 여러 번 다시 시작하는 것이 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="514529e761d628932a46cfab06e171128c270c12" translate="yes" xml:space="preserve">
          <source>In practice, whether parallelism is helpful at improving runtime depends on many factors. It is usually a good idea to experiment rather than assuming that increasing the number of workers is always a good thing. In some cases it can be highly detrimental to performance to run multiple copies of some estimators or functions in parallel (see oversubscription below).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f131bcc125dc920f59e2c48cc0ec0622c3531f86" translate="yes" xml:space="preserve">
          <source>In practice, you will have to handle yourself the column data type. If you want some columns to be considered as &lt;code&gt;category&lt;/code&gt;, you will have to convert them into categorical columns. If you are using pandas, you can refer to their documentation regarding &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html&quot;&gt;Categorical data&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c017831824360a69856c6ccfffcd3f1b73574d4" translate="yes" xml:space="preserve">
          <source>In practise, a stacking predictor predict as good as the best predictor of the base layer and even sometimes outputperform it by combining the different strength of the these predictors. However, training a stacking predictor is computationally expensive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0aad0cfd8ffb7884222ad0979279d2b7ce81ef15" translate="yes" xml:space="preserve">
          <source>In principle, any function can be passed that provides a &lt;code&gt;rvs&lt;/code&gt; (random variate sample) method to sample a value. A call to the &lt;code&gt;rvs&lt;/code&gt; function should provide independent random samples from possible parameter values on consecutive calls.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9809378b0338436bd7bbe8f2e2070a6272b570d" translate="yes" xml:space="preserve">
          <source>In problems where it is desired to give more importance to certain classes or certain individual samples keywords &lt;code&gt;class_weight&lt;/code&gt; and &lt;code&gt;sample_weight&lt;/code&gt; can be used.</source>
          <target state="translated">특정 클래스 또는 특정 개별 샘플에 더 중요한 것을 요구하는 문제에서 키워드 &lt;code&gt;class_weight&lt;/code&gt; 및 &lt;code&gt;sample_weight&lt;/code&gt; 를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="274191e11959a25ec702f3eb0728b40adf181870" translate="yes" xml:space="preserve">
          <source>In problems where it is desired to give more importance to certain classes or certain individual samples, the parameters &lt;code&gt;class_weight&lt;/code&gt; and &lt;code&gt;sample_weight&lt;/code&gt; can be used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c88cb15e9453fb980c7d19e00322e852632f7bf2" translate="yes" xml:space="preserve">
          <source>In random forests (see &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt; classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cae7f41bfdb577edc832687cbceee9865d3220c5" translate="yes" xml:space="preserve">
          <source>In random forests (see &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt; classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.</source>
          <target state="translated">랜덤 포레스트 ( &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt; &lt;code&gt;RandomForestRegressor&lt;/code&gt; &lt;/a&gt; 클래스 참조 )에서 앙상블의 각 트리는 트레이닝 세트에서 대체 (즉, 부트 스트랩 샘플)로 그린 샘플에서 빌드됩니다. 또한 트리를 구성하는 동안 노드를 분할 할 때 선택한 분할이 더 이상 모든 기능 중에서 가장 잘 분할되지 않습니다. 대신 선택되는 분할이 기능의 임의의 하위 집합 중에서 가장 잘 분할됩니다. 이 무작위성의 결과로, 숲의 편견은 보통 (비 무작위 트리의 편견과 관련하여) 약간 증가하지만 평균화로 인해 편차가 감소합니다. 보통 편향의 증가를 보상하는 것 이상으로, 따라서 전체적으로 더 나은 모델을 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="eee63fdeeb00f1867cdf7e3f336a4276e1d12ae2" translate="yes" xml:space="preserve">
          <source>In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and noise. On average over datasets of the regression problem, the bias term measures the average amount by which the predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes model). The variance term measures the variability of the predictions of the estimator when fit over different instances LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data.</source>
          <target state="translated">회귀 분석에서 추정기의 예상 평균 제곱 오차는 치우침, 분산 및 노이즈 측면에서 분해 될 수 있습니다. 회귀 문제의 평균 데이터 집합에 대해 편향 항은 추정기의 예측이 문제에 대한 가능한 최상의 추정기의 예측과 다른 평균 량을 측정합니다 (예 : 베이 즈 모델). 분산 항은 문제의 다른 사례 LS에 적합 할 때 추정기 예측의 변동성을 측정합니다. 마지막으로 노이즈는 데이터의 변동성으로 인한 오차의 돌이킬 수없는 부분을 측정합니다.</target>
        </trans-unit>
        <trans-unit id="3eae88b0a075df5d4090eee16e911849fedcc7b3" translate="yes" xml:space="preserve">
          <source>In regression, the output remains as \(f(x)\); therefore, output activation function is just the identity function.</source>
          <target state="translated">회귀에서 출력은 \ (f (x) \)로 유지됩니다. 따라서 출력 활성화 기능은 ID 기능 일뿐입니다.</target>
        </trans-unit>
        <trans-unit id="253be6f032627aec5a3b2c4240659e2190b9fba2" translate="yes" xml:space="preserve">
          <source>In scikit-learn a random split into training and test sets can be quickly computed with the &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; helper function. Let&amp;rsquo;s load the iris data set to fit a linear support vector machine on it:</source>
          <target state="translated">scikit-learn에서는 &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt; 도우미 함수를 사용하여 훈련 및 테스트 세트로 무작위 분할을 신속하게 계산할 수 있습니다 . 선형 서포트 벡터 머신에 맞게 홍채 데이터 세트를로드합시다 :</target>
        </trans-unit>
        <trans-unit id="bdcdb5bf0b220e10633a04e3b3d7b23fb832fcf9" translate="yes" xml:space="preserve">
          <source>In scikit-learn, an estimator for classification is a Python object that implements the methods &lt;code&gt;fit(X, y)&lt;/code&gt; and &lt;code&gt;predict(T)&lt;/code&gt;.</source>
          <target state="translated">scikit-learn에서 분류 추정기는 &lt;code&gt;fit(X, y)&lt;/code&gt; 및 &lt;code&gt;predict(T)&lt;/code&gt; 메소드를 구현하는 Python 객체입니다 .</target>
        </trans-unit>
        <trans-unit id="a15700735758e7e1606b13cd4f276e5fe1e0ef96" translate="yes" xml:space="preserve">
          <source>In scikit-learn, bagging methods are offered as a unified &lt;a href=&quot;generated/sklearn.ensemble.baggingclassifier#sklearn.ensemble.BaggingClassifier&quot;&gt;&lt;code&gt;BaggingClassifier&lt;/code&gt;&lt;/a&gt; meta-estimator (resp. &lt;a href=&quot;generated/sklearn.ensemble.baggingregressor#sklearn.ensemble.BaggingRegressor&quot;&gt;&lt;code&gt;BaggingRegressor&lt;/code&gt;&lt;/a&gt;), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, &lt;code&gt;max_samples&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt; control the size of the subsets (in terms of samples and features), while &lt;code&gt;bootstrap&lt;/code&gt; and &lt;code&gt;bootstrap_features&lt;/code&gt; control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting &lt;code&gt;oob_score=True&lt;/code&gt;. As an example, the snippet below illustrates how to instantiate a bagging ensemble of &lt;code&gt;KNeighborsClassifier&lt;/code&gt; base estimators, each built on random subsets of 50% of the samples and 50% of the features.</source>
          <target state="translated">scikit-learn에서 배깅 방법은 통합 된 &lt;a href=&quot;generated/sklearn.ensemble.baggingclassifier#sklearn.ensemble.BaggingClassifier&quot;&gt; &lt;code&gt;BaggingClassifier&lt;/code&gt; &lt;/a&gt; 메타 추정기 ( &lt;a href=&quot;generated/sklearn.ensemble.baggingregressor#sklearn.ensemble.BaggingRegressor&quot;&gt; &lt;code&gt;BaggingRegressor&lt;/code&gt; &lt;/a&gt; . BaggingRegressor ) 로 제공되며, 임의의 하위 집합을 그리는 전략을 지정하는 매개 변수와 함께 사용자 지정 기준 추정기를 입력으로 사용합니다. 특히 &lt;code&gt;max_samples&lt;/code&gt; 및 &lt;code&gt;max_features&lt;/code&gt; 는 하위 집합의 크기 (샘플 및 기능 측면)를 제어 하는 반면, &lt;code&gt;bootstrap&lt;/code&gt; 및 &lt;code&gt;bootstrap_features&lt;/code&gt; 는 샘플과 기능을 교체 여부와 상관없이 제어합니다. 사용 가능한 샘플의 서브 세트를 사용할 때 일반화 정확도는 설정을 통해 가방 외부 샘플로 추정 할 수 있습니다 &lt;code&gt;oob_score=True&lt;/code&gt; . 예를 들어, 아래 스 니펫은 &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 기본 추정기 의 배깅 앙상블을 인스턴스화하는 방법을 보여줍니다 . 각각은 샘플의 50 %와 기능의 50 %의 임의의 하위 집합을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="0848ab34fddbc88dcbfdd395d0519986e8d182eb" translate="yes" xml:space="preserve">
          <source>In scikit-learn, this transformation (with a user-defined shrinkage coefficient) can be directly applied to a pre-computed covariance with the &lt;a href=&quot;generated/sklearn.covariance.shrunk_covariance#sklearn.covariance.shrunk_covariance&quot;&gt;&lt;code&gt;shrunk_covariance&lt;/code&gt;&lt;/a&gt; method. Also, a shrunk estimator of the covariance can be fitted to data with a &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt;&lt;code&gt;ShrunkCovariance&lt;/code&gt;&lt;/a&gt; object and its &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance.fit&quot;&gt;&lt;code&gt;ShrunkCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Again, results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately.</source>
          <target state="translated">scikit-learn에서이 변형 (사용자 정의 수축 계수 포함)은 &lt;a href=&quot;generated/sklearn.covariance.shrunk_covariance#sklearn.covariance.shrunk_covariance&quot;&gt; &lt;code&gt;shrunk_covariance&lt;/code&gt; &lt;/a&gt; 방법 을 사용하여 미리 계산 된 공분산에 직접 적용 할 수 있습니다 . 또한 공분산의 축소 추정값을 &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt; &lt;code&gt;ShrunkCovariance&lt;/code&gt; &lt;/a&gt; 객체와 해당 &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance.fit&quot;&gt; &lt;code&gt;ShrunkCovariance.fit&lt;/code&gt; &lt;/a&gt; 메서드를 사용 하여 데이터에 피팅 할 수 있습니다 . 다시 결과는 데이터가 중심에 있는지 여부에 따라 달라 &lt;code&gt;assume_centered&lt;/code&gt; 매개 변수를 정확하게 사용하려고 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="77c9e3634b9d256acc307751b68c900b0b833a29" translate="yes" xml:space="preserve">
          <source>In single precision, &lt;code&gt;mean&lt;/code&gt; can be inaccurate:</source>
          <target state="translated">단 정밀도에서는 &lt;code&gt;mean&lt;/code&gt; 이 부정확 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="640c5c337251b299605fe0c1704cd43d50fcda84" translate="yes" xml:space="preserve">
          <source>In some cases it&amp;rsquo;s not necessary to include higher powers of any single feature, but only the so-called &lt;em&gt;interaction features&lt;/em&gt; that multiply together at most \(d\) distinct features. These can be gotten from &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; with the setting &lt;code&gt;interaction_only=True&lt;/code&gt;.</source>
          <target state="translated">경우에 따라 단일 기능의 강력한 기능을 모두 포함 할 필요는 없지만 최대 \ (d \) 개의 고유 한 기능을 함께 곱하는 소위 &lt;em&gt;상호 작용 기능&lt;/em&gt; 만 포함 할 수 있습니다. &lt;code&gt;interaction_only=True&lt;/code&gt; 설정으로 &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; &lt;/a&gt; 에서 가져올 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="4976a8ab8df4ced9a6ceb9c8368b484dbc953550" translate="yes" xml:space="preserve">
          <source>In some cases, only interaction terms among features are required, and it can be gotten with the setting &lt;code&gt;interaction_only=True&lt;/code&gt;:</source>
          <target state="translated">경우에 따라 기능 간의 상호 작용 용어 만 필요하며 &lt;code&gt;interaction_only=True&lt;/code&gt; 설정으로 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="fc4d9d9d36cc2fd0e96fb6fded728410f07d4165" translate="yes" xml:space="preserve">
          <source>In some specific cases (when the code that is run in parallel releases the GIL), scikit-learn will indicate to &lt;code&gt;joblib&lt;/code&gt; that a multi-threading backend is preferable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b40a5cc0ca592bd5c1306138f251a81fe534579" translate="yes" xml:space="preserve">
          <source>In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)</source>
          <target state="translated">지나치게 단순화 된 가정에도 불구하고 순진한 Bayes 분류기는 문서 분류 및 스팸 필터링과 같은 많은 실제 상황에서 상당히 잘 작동했습니다. 필요한 매개 변수를 추정하려면 소량의 교육 데이터가 필요합니다. (순진한 Bayes가 잘 작동하는 이론적 인 이유와 어떤 유형의 데이터에 대해서는 아래 참조를 참조하십시오.)</target>
        </trans-unit>
        <trans-unit id="389828ed3005eb646a2fbe827835555cbdc74437" translate="yes" xml:space="preserve">
          <source>In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since \(n - 1\) of the \(n\) samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.</source>
          <target state="translated">정확도 측면에서 LOO는 종종 테스트 오류의 추정값으로 높은 분산을 초래합니다. 직관적으로, \ (n \) 샘플의 \ (n-1 \)는 각 모델을 빌드하는 데 사용되므로 접기로 구성된 모델은 서로 거의 동일하며 전체 교육 세트에서 작성된 모델과 동일합니다.</target>
        </trans-unit>
        <trans-unit id="36132aafc76511f4279f2a1765dcbaeb9d7a44b1" translate="yes" xml:space="preserve">
          <source>In terms of time and space complexity, Theil-Sen scales according to</source>
          <target state="translated">시간과 공간의 복잡성 측면에서 Theil-Sen은</target>
        </trans-unit>
        <trans-unit id="8cac8320893acecd46013a1cd740f5237cabb213" translate="yes" xml:space="preserve">
          <source>In that case, the model with 2 components and full covariance (which corresponds to the true generative model) is selected.</source>
          <target state="translated">이 경우 성분이 2 개인 모델과 완전 공분산 (실제 생성 모델에 해당)이 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="56b0989d4ac400367ce631898b8b32a7aa114deb" translate="yes" xml:space="preserve">
          <source>In that way, we emphasize that the greater the variance of a feature, the larger the weight of the corresponding coefficient on the output, all else being equal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df23be83a828beae97b01644c9cebfd6ec568f81" translate="yes" xml:space="preserve">
          <source>In the &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;smooth_idf=False&lt;/code&gt;, the &amp;ldquo;1&amp;rdquo; count is added to the idf instead of the idf&amp;rsquo;s denominator:</source>
          <target state="translated">에서 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;smooth_idf=False&lt;/code&gt; 의 &quot;1&quot;카운트가 대신 IDF의 분모의 IDF에 추가됩니다</target>
        </trans-unit>
        <trans-unit id="5bd53e1aa867b99daf4cb138797f33e24d9cfda1" translate="yes" xml:space="preserve">
          <source>In the &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, all units are binary stochastic units. This means that the input data should either be binary, or real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model for character recognition, where the interest is on which pixels are active and which aren&amp;rsquo;t. For images of natural scenes it no longer fits because of background, depth and the tendency of neighbouring pixels to take the same values.</source>
          <target state="translated">에서 &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; , 모든 단위는 바이너리 확률 단위입니다. 이는 입력 데이터가 이진이거나 가시적 단위가 켜지거나 꺼질 가능성을 나타내는 0과 1 사이의 실수 값이어야 함을 의미합니다. 이것은 어떤 픽셀이 활성화되어 있고 어떤 픽셀이 활성화되지 않았는 지에 대한 문자 인식을위한 좋은 모델입니다. 자연스러운 장면의 이미지의 경우 배경, 깊이 및 인접 픽셀의 값이 같기 때문에 더 이상 적합하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="729860dcb0b5963ed7d873f5d8718ecbded39d56" translate="yes" xml:space="preserve">
          <source>In the &lt;code&gt;l1&lt;/code&gt; case, theory says that prediction consistency (i.e. that under given hypothesis, the estimator learned predicts as well as a model knowing the true distribution) is not possible because of the bias of the &lt;code&gt;l1&lt;/code&gt;. It does say, however, that model consistency, in terms of finding the right set of non-zero parameters as well as their signs, can be achieved by scaling &lt;code&gt;C1&lt;/code&gt;.</source>
          <target state="translated">에서 &lt;code&gt;l1&lt;/code&gt; 경우, 이론은 예측의 일관성 말한다 (즉, 주어진 가설 아래, 배운 추정이 실제 분포를 아는 모델뿐만 아니라 예측 것을) 때문에의 편견 수 없습니다 &lt;code&gt;l1&lt;/code&gt; . 그러나 &lt;code&gt;C1&lt;/code&gt; 을 스케일링하여 0이 아닌 매개 변수의 올바른 세트와 그 부호를 찾는 관점에서 모델 일관성을 달성 할 수 있다고합니다. 합니다.</target>
        </trans-unit>
        <trans-unit id="3ac113ba1d5ea8579f40e00fa885b5d980c4ba43" translate="yes" xml:space="preserve">
          <source>In the &lt;code&gt;l1&lt;/code&gt; penalty case, the cross-validation-error correlates best with the test-error, when scaling our &lt;code&gt;C&lt;/code&gt; with the number of samples, &lt;code&gt;n&lt;/code&gt;, which can be seen in the first figure.</source>
          <target state="translated">에서 &lt;code&gt;l1&lt;/code&gt; 페널티 경우, 교차 검증 오류의 상관 관계는 시험 오류와 최고의 우리의 확장 &lt;code&gt;C&lt;/code&gt; 를 샘플 수, &lt;code&gt;n&lt;/code&gt; 은 제 도면에서 알 수있다.</target>
        </trans-unit>
        <trans-unit id="4495b7082e60ec7cb3a7d3cf1946536697a0e6b3" translate="yes" xml:space="preserve">
          <source>In the above case, the classifier is fit on a 1d array of multiclass labels and the &lt;code&gt;predict()&lt;/code&gt; method therefore provides corresponding multiclass predictions. It is also possible to fit upon a 2d array of binary label indicators:</source>
          <target state="translated">위의 경우 분류기는 다중 클래스 레이블의 1d 배열과 &lt;code&gt;predict()&lt;/code&gt; 하므로 메서드는 해당 다중 클래스 예측을 제공합니다. 이진 레이블 표시기의 2D 배열에 맞출 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="41d2181ce120b72b14a943b5e6f5608fe64d404d" translate="yes" xml:space="preserve">
          <source>In the above example, &lt;code&gt;char_wb&lt;/code&gt; analyzer is used, which creates n-grams only from characters inside word boundaries (padded with space on each side). The &lt;code&gt;char&lt;/code&gt; analyzer, alternatively, creates n-grams that span across words:</source>
          <target state="translated">위의 예에서, &lt;code&gt;char_wb&lt;/code&gt; 분석기가 사용되며 단어 경계 안에있는 문자 ( 각면에 공백으로 채워짐) 에서만 n- 그램을 만듭니다. &lt;code&gt;char&lt;/code&gt; 분석기, 대안 단어에서 그 범위를 N-그램 만듭니다 :</target>
        </trans-unit>
        <trans-unit id="02dd6b844a6f6c7bb7b63318a0172b18e25d4984" translate="yes" xml:space="preserve">
          <source>In the above example, the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; expects a 1D array as input and therefore the columns were specified as a string (&lt;code&gt;'city'&lt;/code&gt;). However, other transformers generally expect 2D data, and in that case you need to specify the column as a list of strings (&lt;code&gt;['city']&lt;/code&gt;).</source>
          <target state="translated">위의 예에서 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 는 1D 배열을 입력으로 예상하므로 열이 문자열 ( &lt;code&gt;'city'&lt;/code&gt; ) 로 지정되었습니다 . 그러나 다른 변환기에는 일반적으로 2D 데이터가 필요하므로이 경우 열을 문자열 목록으로 지정해야합니다 ( &lt;code&gt;['city']&lt;/code&gt; ) .</target>
        </trans-unit>
        <trans-unit id="af32f30ac84780c46ec03071f0807a02a06be37e" translate="yes" xml:space="preserve">
          <source>In the above example, the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; expects a 1D array as input and therefore the columns were specified as a string (&lt;code&gt;'title'&lt;/code&gt;). However, &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings (&lt;code&gt;['city']&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8be3ffb93abc4e32a08a3f61b709f6ed3e6124c4" translate="yes" xml:space="preserve">
          <source>In the above example-code, we firstly use the &lt;code&gt;fit(..)&lt;/code&gt; method to fit our estimator to the data and secondly the &lt;code&gt;transform(..)&lt;/code&gt; method to transform our count-matrix to a tf-idf representation. These two steps can be combined to achieve the same end result faster by skipping redundant processing. This is done through using the &lt;code&gt;fit_transform(..)&lt;/code&gt; method as shown below, and as mentioned in the note in the previous section:</source>
          <target state="translated">위의 예제 코드에서, 우리는 먼저 &lt;code&gt;fit(..)&lt;/code&gt; 메소드를 사용하여 추정기에 데이터를 맞추고 두번째로 &lt;code&gt;transform(..)&lt;/code&gt; 메소드를 사용하여 count-matrix를 tf-idf 표현으로 변환합니다. 이 두 단계를 중복 처리하여 중복 처리를 건너 뛰어 동일한 최종 결과를 더 빠르게 달성 할 수 있습니다. 아래에 표시된대로 이전 섹션의 참고에서 언급 한대로 &lt;code&gt;fit_transform(..)&lt;/code&gt; 메소드 를 사용하여 수행됩니다 .</target>
        </trans-unit>
        <trans-unit id="370df873906104ebc3c5f7c3ad3752f50f2bb258" translate="yes" xml:space="preserve">
          <source>In the above illustrating figure, we consider some points from a randomly generated dataset. We focus on the stochastic KNN classification of point no. 3. The thickness of a link between sample 3 and another point is proportional to their distance, and can be seen as the relative weight (or probability) that a stochastic nearest neighbor prediction rule would assign to this point. In the original space, sample 3 has many stochastic neighbors from various classes, so the right class is not very likely. However, in the projected space learned by NCA, the only stochastic neighbors with non-negligible weight are from the same class as sample 3, guaranteeing that the latter will be well classified. See the &lt;a href=&quot;#nca-mathematical-formulation&quot;&gt;mathematical formulation&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc1fc9feffcc3ab7062a5968abf403f994be456d" translate="yes" xml:space="preserve">
          <source>In the above process, rejection sampling is used to make sure that n is more than 2, and that the document length is never zero. Likewise, we reject classes which have already been chosen. The documents that are assigned to both classes are plotted surrounded by two colored circles.</source>
          <target state="translated">위의 프로세스에서 거부 샘플링은 n이 2보다 크고 문서 길이가 0이 아닌지 확인하는 데 사용됩니다. 마찬가지로, 우리는 이미 선택된 수업을 거부합니다. 두 클래스에 할당 된 문서는 두 개의 컬러 원으로 둘러싸여 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="35ac86e1f976c024d854c94dc1a07d9250df373b" translate="yes" xml:space="preserve">
          <source>In the above process, rejection sampling is used to make sure that n is never zero or more than &lt;code&gt;n_classes&lt;/code&gt;, and that the document length is never zero. Likewise, we reject classes which have already been chosen.</source>
          <target state="translated">위의 프로세스에서 거부 샘플링은 n이 절대 0 또는 &lt;code&gt;n_classes&lt;/code&gt; 이상이고 문서 길이가 절대 0이 아닌지 확인하는 데 사용 됩니다. 마찬가지로, 우리는 이미 선택된 수업을 거부합니다.</target>
        </trans-unit>
        <trans-unit id="35b3eed71c5956697e4e941c9abda7fa7875d908" translate="yes" xml:space="preserve">
          <source>In the binary (two-class) case, \(tp\), \(tn\), \(fp\) and \(fn\) are respectively the number of true positives, true negatives, false positives and false negatives, the MCC is defined as</source>
          <target state="translated">이항 (2 클래스)의 경우, \ ​​(tp \), \ (tn \), \ (fp \) 및 \ (fn \)은 각각 참 긍정, 참 부정, 거짓 긍정 및 거짓 부정의 수입니다. MCC는</target>
        </trans-unit>
        <trans-unit id="8cf754386b9e93bff61012cc7eebd891fe098125" translate="yes" xml:space="preserve">
          <source>In the binary case, balanced accuracy is equal to the arithmetic mean of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;sensitivity&lt;/a&gt; (true positive rate) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;specificity&lt;/a&gt; (true negative rate), or the area under the ROC curve with binary predictions rather than scores.</source>
          <target state="translated">이진 경우 균형 잡힌 정확도는 산술 평균 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;감도&lt;/a&gt; (진 양성 비율) 및 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;특이성&lt;/a&gt; 과 같습니다. (진 음성 비율) 또는 점수가 아닌 이진 예측이있는 ROC 곡선 아래 영역과 같습니다.</target>
        </trans-unit>
        <trans-unit id="43cd3b6bd763c03854427d73b603fdca48b2b30e" translate="yes" xml:space="preserve">
          <source>In the binary case, balanced accuracy is equal to the arithmetic mean of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;sensitivity&lt;/a&gt; (true positive rate) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;specificity&lt;/a&gt; (true negative rate), or the area under the ROC curve with binary predictions rather than scores:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a998ef5238ae7921fe9486295ae64f00deab3566" translate="yes" xml:space="preserve">
          <source>In the binary case, we can extract true positives, etc as follows:</source>
          <target state="translated">이진 경우 다음과 같이 진양 수 등을 추출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3e94daaa4e8fed019552e1789dc3caf5c267c82f" translate="yes" xml:space="preserve">
          <source>In the binary case:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="193e8f9e646cb769a122ba34f156f35dc1f6d79e" translate="yes" xml:space="preserve">
          <source>In the case of &amp;ldquo;one-vs-one&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt;, the layout of the attributes is a little more involved. In the case of a linear kernel, the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;(n_classes * (n_classes - 1) / 2, n_features)&lt;/code&gt; and &lt;code&gt;(n_classes *
(n_classes - 1) / 2)&lt;/code&gt; respectively. This is similar to the layout for &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is &amp;ldquo;0 vs 1&amp;rdquo;, &amp;ldquo;0 vs 2&amp;rdquo; , &amp;hellip; &amp;ldquo;0 vs n&amp;rdquo;, &amp;ldquo;1 vs 2&amp;rdquo;, &amp;ldquo;1 vs 3&amp;rdquo;, &amp;ldquo;1 vs n&amp;rdquo;, . . . &amp;ldquo;n-1 vs n&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2482e7f51309cef70ec85538824011f95f0813b1" translate="yes" xml:space="preserve">
          <source>In the case of &amp;ldquo;one-vs-one&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, the layout of the attributes is a little more involved. In the case of having a linear kernel, the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class * (n_class - 1) / 2, n_features]&lt;/code&gt; and &lt;code&gt;[n_class * (n_class - 1) / 2]&lt;/code&gt; respectively. This is similar to the layout for &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is &amp;ldquo;0 vs 1&amp;rdquo;, &amp;ldquo;0 vs 2&amp;rdquo; , &amp;hellip; &amp;ldquo;0 vs n&amp;rdquo;, &amp;ldquo;1 vs 2&amp;rdquo;, &amp;ldquo;1 vs 3&amp;rdquo;, &amp;ldquo;1 vs n&amp;rdquo;, . . . &amp;ldquo;n-1 vs n&amp;rdquo;.</source>
          <target state="translated">&quot;일대일&quot; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; 의 경우, 속성의 레이아웃이 약간 더 복잡합니다. 선형 커널이있는 경우 &lt;code&gt;coef_&lt;/code&gt; 및 &lt;code&gt;intercept_&lt;/code&gt; 는 각각 &lt;code&gt;[n_class * (n_class - 1) / 2, n_features]&lt;/code&gt; 및 &lt;code&gt;[n_class * (n_class - 1) / 2]&lt;/code&gt; . 이것은 위에서 설명한 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 의 레이아웃과 유사 하며 각 행은 이제 이진 분류기에 해당합니다. 0에서 n까지의 클래스 순서는 &quot;0 vs 1&quot;, &quot;0 vs 2&quot;,&amp;hellip; &quot;0 vs n&quot;, &quot;1 vs 2&quot;, &quot;1 vs 3&quot;, &quot;1 vs n&quot;입니다. . . &quot;n-1 대 n&quot;.</target>
        </trans-unit>
        <trans-unit id="7befa9fe69dc29e17ce8c14ce1f24dcd596f25dc" translate="yes" xml:space="preserve">
          <source>In the case of Gaussian process classification, &amp;ldquo;one_vs_one&amp;rdquo; might be computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that &amp;ldquo;one_vs_one&amp;rdquo; does not support predicting probability estimates but only plain predictions. Moreover, note that &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt;&lt;code&gt;GaussianProcessClassifier&lt;/code&gt;&lt;/a&gt; does not (yet) implement a true multi-class Laplace approximation internally, but as discussed above is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one.</source>
          <target state="translated">가우시안 프로세스 분류의 경우,&amp;ldquo;one_vs_one&amp;rdquo;은 전체 데이터 세트의 문제를 줄이는 것이 아니라 전체 훈련 세트의 서브 세트 만 포함하는 많은 문제를 해결해야하기 때문에 계산 비용이 저렴할 수 있습니다. 가우스 프로세스 분류는 데이터 세트의 크기에 따라 입방체로 확장되므로 상당히 빠릅니다. 그러나&amp;ldquo;one_vs_one&amp;rdquo;은 예측 확률 예측을 지원하지 않으며 일반 예측 만 지원합니다. 또한 &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt; &lt;code&gt;GaussianProcessClassifier&lt;/code&gt; &lt;/a&gt; 는 내부적으로 진정한 멀티 클래스 라플라스 근사치를 구현하지는 않지만 위에서 논의한 바와 같이 내부적으로 몇 가지 이진 분류 작업을 해결하는 데 기반을두고 있습니다.</target>
        </trans-unit>
        <trans-unit id="35a5ada3d16c18d2778299b423e5960182d45740" translate="yes" xml:space="preserve">
          <source>In the case of LDA, the Gaussians for each class are assumed to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all \(k\). This leads to linear decision surfaces, which can be seen by comparing the log-probability ratios \(\log[P(y=k | X) / P(y=l | X)]\):</source>
          <target state="translated">LDA의 경우 각 클래스의 가우시안은 \ (k \)에 대해 동일한 공분산 행렬 (\ (\ Sigma_k = \ Sigma \))을 공유한다고 가정합니다. 이것은 선형 결정 표면으로 이어지며, 이는 로그 확률 비율 \ (\ log [P (y = k | X) / P (y = l | X)] \)을 비교하여 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9191afe7181654f4c123a5274615786e30f48b2b" translate="yes" xml:space="preserve">
          <source>In the case of QDA, there are no assumptions on the covariance matrices \(\Sigma_k\) of the Gaussians, leading to quadratic decision surfaces. See &lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt; for more details.</source>
          <target state="translated">QDA의 경우 가우시안의 공분산 행렬 \ (\ Sigma_k \)에 대한 가정이 없으므로 2 차 결정 표면이됩니다. 보다&lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt; [3]&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="be50ffb98df62eb79c354d934aa76c5587bf8cba" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification &lt;code&gt;coef_&lt;/code&gt; is a two-dimensional array of &lt;code&gt;shape=[n_classes, n_features]&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; is a one-dimensional array of &lt;code&gt;shape=[n_classes]&lt;/code&gt;. The i-th row of &lt;code&gt;coef_&lt;/code&gt; holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute &lt;code&gt;classes_&lt;/code&gt;). Note that, in principle, since they allow to create a probability model, &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; and &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; are more suitable for one-vs-all classification.</source>
          <target state="translated">다중 클래스 분류의 경우 &lt;code&gt;coef_&lt;/code&gt; 는 &lt;code&gt;shape=[n_classes, n_features]&lt;/code&gt; 의 2 차원 배열 이고 &lt;code&gt;intercept_&lt;/code&gt; 는 &lt;code&gt;shape=[n_classes]&lt;/code&gt; 의 1 차원 배열입니다 . &lt;code&gt;coef_&lt;/code&gt; 의 i 번째 행은 i 번째 클래스에 대한 OVA 분류기의 가중치 벡터를 보유하고; 클래스는 오름차순으로 색인됩니다 ( &lt;code&gt;classes_&lt;/code&gt; 속성 참조 ). 원칙적으로 확률 모델을 작성할 수 있으므로 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; 및 &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; 가 일대일 분류에 더 적합합니다.</target>
        </trans-unit>
        <trans-unit id="a5109fe6449c3757593d6d14759e9857d15d95c4" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification &lt;code&gt;coef_&lt;/code&gt; is a two-dimensional array of shape (n_classes, n_features) and &lt;code&gt;intercept_&lt;/code&gt; is a one-dimensional array of shape (n_classes,). The i-th row of &lt;code&gt;coef_&lt;/code&gt; holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute &lt;code&gt;classes_&lt;/code&gt;). Note that, in principle, since they allow to create a probability model, &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; and &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; are more suitable for one-vs-all classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1959581346965192dc91c946ef9926bceb1c51a" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification, the mean log-marginal likelihood of the one-versus-rest classifiers are returned.</source>
          <target state="translated">멀티 클래스 분류의 경우 1 대 나머지 분류기의 평균 로그-마진 가능성이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="a45d03e82085c1e194b2b19d0700767439a7ac42" translate="yes" xml:space="preserve">
          <source>In the case of one-hot/one-of-K coding, the constructed feature names and values are returned rather than the original ones.</source>
          <target state="translated">one-hot / one-of-K 코딩의 경우, 생성 된 피처 이름과 값이 원래 것보다 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="108b9e0576d5a78538771fb415a46ae76d1a6e26" translate="yes" xml:space="preserve">
          <source>In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. &lt;code&gt;BernoulliNB&lt;/code&gt; might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.</source>
          <target state="translated">텍스트 분류의 경우, 단어 분류 벡터가 아닌 단어 발생 벡터를 사용하여이 분류기를 학습하고 사용할 수 있습니다. &lt;code&gt;BernoulliNB&lt;/code&gt; 는 일부 데이터 세트, 특히 문서가 짧은 데이터 세트에서 성능이 향상 될 수 있습니다. 시간이 허락하면 두 모델을 모두 평가하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="1ee37ddaa2e2b7fb0103fdddd162d9ad76a8f2dd" translate="yes" xml:space="preserve">
          <source>In the case of the digits dataset, the task is to predict, given an image, which digit it represents. We are given samples of each of the 10 possible classes (the digits zero through nine) on which we &lt;em&gt;fit&lt;/em&gt; an &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;estimator&lt;/a&gt; to be able to &lt;em&gt;predict&lt;/em&gt; the classes to which unseen samples belong.</source>
          <target state="translated">숫자 데이터 세트의 경우, 작업은 이미지가 주어진 숫자를 예측하여 예측하는 것입니다. 우리는 &lt;em&gt;예측할&lt;/em&gt; 수 있는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;추정기&lt;/a&gt; 에 &lt;em&gt;맞는&lt;/em&gt; 10 개의 가능한 클래스 (0에서 9까지의 숫자) 각각의 샘플을받습니다.&lt;em&gt;&lt;/em&gt; 보이지 않는 샘플이 속한되는 클래스를.</target>
        </trans-unit>
        <trans-unit id="0484e6facaecfeba6a4ff8e0552fa9a5ac1c52dd" translate="yes" xml:space="preserve">
          <source>In the case that one or more classes are absent in a training portion, a default score needs to be assigned to all instances for that class if &lt;code&gt;method&lt;/code&gt; produces columns per class, as in {&amp;lsquo;decision_function&amp;rsquo;, &amp;lsquo;predict_proba&amp;rsquo;, &amp;lsquo;predict_log_proba&amp;rsquo;}. For &lt;code&gt;predict_proba&lt;/code&gt; this value is 0. In order to ensure finite output, we approximate negative infinity by the minimum finite float value for the dtype in other cases.</source>
          <target state="translated">훈련 부분에 하나 이상의 클래스가없는 경우 { 'decision_function', 'predict_proba', 'predict_log_proba'}와 같이 &lt;code&gt;method&lt;/code&gt; 가 클래스 당 열을 생성하는 경우 기본 점수를 해당 클래스의 모든 인스턴스에 할당해야합니다. . 에 대한 &lt;code&gt;predict_proba&lt;/code&gt; 이 값은 제한된 출력, 다른 경우에 대한 DTYPE 최소 유한 플로트 값만큼 우리 대략 마이너스 무한대을 보장하기 위해 0이다.</target>
        </trans-unit>
        <trans-unit id="74ff5bfdda6b3e93f59169f7c22fc2d68fa3fcf3" translate="yes" xml:space="preserve">
          <source>In the case when the binary labels are fractional (probabilistic), inverse_transform chooses the class with the greatest value. Typically, this allows to use the output of a linear model&amp;rsquo;s decision_function method directly as the input of inverse_transform.</source>
          <target state="translated">이진 레이블이 소수 (확률) 인 경우 inverse_transform은 가장 큰 값을 가진 클래스를 선택합니다. 일반적으로 선형 모델의 decision_function 메소드 출력을 inverse_transform의 입력으로 직접 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e087c17fe61957d86c5cc0e9905f0323cd8dea87" translate="yes" xml:space="preserve">
          <source>In the cases of a tie, the &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; will select the class based on the ascending sort order. E.g., in the following scenario</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d852af4ac330c07eec96a31a9559a88b3b70655" translate="yes" xml:space="preserve">
          <source>In the cases of a tie, the &lt;code&gt;VotingClassifier&lt;/code&gt; will select the class based on the ascending sort order. E.g., in the following scenario</source>
          <target state="translated">동점 인 경우 &lt;code&gt;VotingClassifier&lt;/code&gt; 는 오름차순 정렬 순서에 따라 클래스를 선택합니다. 예를 들어 다음 시나리오에서</target>
        </trans-unit>
        <trans-unit id="84352a0fa93e8d3c09e63ba562819b09bff22e0e" translate="yes" xml:space="preserve">
          <source>In the checkerboard case, each row belongs to all column clusters, and each column belongs to all row clusters. Here is an example of this structure where the variance of the values within each bicluster is small:</source>
          <target state="translated">바둑판의 경우 각 행은 모든 열 클러스터에 속하고 각 열은 모든 행 클러스터에 속합니다. 각 bicluster 내의 값의 분산이 작은이 구조의 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="6a06bacf0f95acd504bad8493dc228833ae72576" translate="yes" xml:space="preserve">
          <source>In the event that the 95% confidence interval based on Fisher transform spans zero, a warning is raised.</source>
          <target state="translated">Fisher 변환을 기반으로하는 95 % 신뢰 구간이 0에 이르면 경고가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="2da0fe066fd806cee05903c8f41b8c38bb726d66" translate="yes" xml:space="preserve">
          <source>In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82.</source>
          <target state="translated">아래 예에서 작은 수축 임계 값을 사용하면 모델의 정확도가 0.81에서 0.82로 증가합니다.</target>
        </trans-unit>
        <trans-unit id="ae3527cc8009043f3459062f8b3ac5f4c7cdc080" translate="yes" xml:space="preserve">
          <source>In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below.</source>
          <target state="translated">아래 그림에서 색상은 클러스터 구성원을 나타내며 큰 원은 알고리즘에서 찾은 핵심 샘플을 나타냅니다. 작은 원은 여전히 ​​클러스터의 일부인 비 핵심 샘플입니다. 또한 특이 치는 아래에 검은 점으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="cfa64cd986932f750b1c68de33ab3971d444bf3d" translate="yes" xml:space="preserve">
          <source>In the first column, first row the learning curve of a naive Bayes classifier is shown for the digits dataset. Note that the training score and the cross-validation score are both not very good at the end. However, the shape of the curve can be found in more complex datasets very often: the training score is very high at the beginning and decreases and the cross-validation score is very low at the beginning and increases. In the second column, first row we see the learning curve of an SVM with RBF kernel. We can see clearly that the training score is still around the maximum and the validation score could be increased with more training samples. The plots in the second row show the times required by the models to train with various sizes of training dataset. The plots in the third row show how much time was required to train the models for each training sizes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbb41405aaf08b233f5d5a9cb37143ee4833765d" translate="yes" xml:space="preserve">
          <source>In the first figure, we visualize the value of the kernel, i.e. the similarity of the sequences, using a colormap. Brighter color here indicates higher similarity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a915ebedbc9fc712ae336eb7409727fc370425dc" translate="yes" xml:space="preserve">
          <source>In the first row, the classifiers are built using the sepal width and the sepal length features only, on the second row using the petal length and sepal length only, and on the third row using the petal width and the petal length only.</source>
          <target state="translated">첫 번째 행에서 분류기는 sepal 너비와 sepal 길이 피처 만 사용하고 두 번째 행은 꽃잎 길이와 sepal 길이 만 사용하고 세 번째 행은 꽃잎 너비와 꽃잎 길이 만 사용하여 작성합니다.</target>
        </trans-unit>
        <trans-unit id="974efdfc7b27c9e04d6e731c42ba40fb3d593ff5" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NearestNeighbors class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1,1,1]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd9366b471cf174a5dc26260b1ee3e4775bd8a95" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1, 1, 1]:</source>
          <target state="translated">다음 예제에서는 데이터 세트를 나타내는 배열에서 NeighborsClassifier 클래스를 구성하고 [1, 1, 1]에 가장 가까운 사람을 묻습니다.</target>
        </trans-unit>
        <trans-unit id="f873541d5e32ccd97b454877a7265b9e862eeb9a" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1,1,1]</source>
          <target state="translated">다음 예제에서는 데이터 세트를 나타내는 배열에서 NeighborsClassifier 클래스를 구성하고 [1,1,1]에 가장 가까운 사람을 묻습니다.</target>
        </trans-unit>
        <trans-unit id="65f9f2f58e8b0fd298381aa88835a40b1607c17f" translate="yes" xml:space="preserve">
          <source>In the following figure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown for three choices of kernels:</source>
          <target state="translated">다음 그림에서는 바이 모달 분포에서 100 개의 점을 가져오고 세 가지 커널 선택에 대한 커널 밀도 추정값이 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="a08df9eebc31ca3edc4756b37a4fdadef1454fe3" translate="yes" xml:space="preserve">
          <source>In the following plot, the maximum effective alpha value is removed, because it is the trivial tree with only one node.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24263a7351535bc4435386a661d4637b721eb5a0" translate="yes" xml:space="preserve">
          <source>In the following plot, we see a function \(f(x) = \cos (\frac{3}{2} \pi x)\) and some noisy samples from that function. We use three different estimators to fit the function: linear regression with polynomial features of degree 1, 4 and 15. We see that the first estimator can at best provide only a poor fit to the samples and the true function because it is too simple (high bias), the second estimator approximates it almost perfectly and the last estimator approximates the training data perfectly but does not fit the true function very well, i.e. it is very sensitive to varying training data (high variance).</source>
          <target state="translated">다음 그림에서 \ (f (x) = \ cos (\ frac {3} {2} \ pi x) \) 함수와 해당 함수의 일부 노이즈 샘플을 볼 수 있습니다. 함수를 맞추기 위해 세 가지 다른 추정값을 사용합니다. 1, 4, 15 차의 다항식 특징을 갖는 선형 회귀 분석. 첫 번째 추정기는 표본에 적합하지 않은 적합도를 제공 할 수 있으며 실제 함수는 너무 단순하기 때문에 ( 두 번째 추정기는 거의 완벽하게 근사하고 마지막 추정기는 훈련 데이터와 완벽하게 근사하지만 실제 기능에 잘 맞지 않습니다. 즉, 다양한 훈련 데이터에 매우 민감합니다 (높은 분산).</target>
        </trans-unit>
        <trans-unit id="605a0ccca31d97f6c29fd62c728a36908756654e" translate="yes" xml:space="preserve">
          <source>In the following section, we will interpret the coefficients of the model. While we do so, we should keep in mind that any conclusion we draw is about the model that we build, rather than about the true (real-world) generative process of the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de54ef0bf525631d31eb0623dcd55f8a9ddc120b" translate="yes" xml:space="preserve">
          <source>In the following sub-sections, we will describe each of those functions, preceded by some notes on common API and metric definition.</source>
          <target state="translated">다음 하위 섹션에서는 이러한 각 기능에 대해 설명하고 공통 API 및 메트릭 정의에 대한 몇 가지 참고 사항을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="7549668dfe247eb9e0e172cc89f620a63604992b" translate="yes" xml:space="preserve">
          <source>In the following we will use the built-in dataset loader for 20 newsgroups from scikit-learn. Alternatively, it is possible to download the dataset manually from the website and use the &lt;a href=&quot;../../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt;&lt;code&gt;sklearn.datasets.load_files&lt;/code&gt;&lt;/a&gt; function by pointing it to the &lt;code&gt;20news-bydate-train&lt;/code&gt; sub-folder of the uncompressed archive folder.</source>
          <target state="translated">다음에서는 scikit-learn의 20 개 뉴스 그룹에 내장 데이터 세트 로더를 사용합니다. 또는 웹 사이트에서 수동으로 데이터 세트를 다운로드 하고 &lt;code&gt;20news-bydate-train&lt;/code&gt; 을 지정 하여 &lt;a href=&quot;../../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt; &lt;code&gt;sklearn.datasets.load_files&lt;/code&gt; &lt;/a&gt; 함수를 사용할 수 있습니다 하고 압축되지 않은 아카이브 폴더 하위 폴더를 .</target>
        </trans-unit>
        <trans-unit id="2cb0b9817ecf09ea4893bb9df9d328ce75ef2d1b" translate="yes" xml:space="preserve">
          <source>In the following, &amp;ldquo;city&amp;rdquo; is a categorical attribute while &amp;ldquo;temperature&amp;rdquo; is a traditional numerical feature:</source>
          <target state="translated">다음에서 &quot;도시&quot;는 범주 적 속성 인 반면 &quot;온도&quot;는 일반적인 숫자 기능입니다.</target>
        </trans-unit>
        <trans-unit id="3e019b4cbe3ce7f1554fa08ce08898c560cb8a3b" translate="yes" xml:space="preserve">
          <source>In the following, we start a Python interpreter from our shell and then load the &lt;code&gt;iris&lt;/code&gt; and &lt;code&gt;digits&lt;/code&gt; datasets. Our notational convention is that &lt;code&gt;$&lt;/code&gt; denotes the shell prompt while &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; denotes the Python interpreter prompt:</source>
          <target state="translated">다음에서는 쉘에서 Python 인터프리터를 시작한 다음 &lt;code&gt;iris&lt;/code&gt; 및 &lt;code&gt;digits&lt;/code&gt; 데이터 세트 를로드합니다 . 우리의 표기법은 &lt;code&gt;$&lt;/code&gt; 가 쉘 프롬프트를 나타내고 &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 나타내고 는 파이썬 인터프리터 프롬프트를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="fe30ab8cfcc104ee94b5fb01793274570a377034" translate="yes" xml:space="preserve">
          <source>In the formula above, \(\mathbf{b}\) and \(\mathbf{c}\) are the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy:</source>
          <target state="translated">위의 공식에서 \ (\ mathbf {b} \) 및 \ (\ mathbf {c} \)는 각각 보이는 레이어와 숨겨진 레이어의 인터셉트 벡터입니다. 모형의 결합 확률은 에너지 측면에서 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="e448c91ec6db1584eec64770c66e9fad7266b47b" translate="yes" xml:space="preserve">
          <source>In the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66070e8da21856ec34fc0b507e5d723e9475a567" translate="yes" xml:space="preserve">
          <source>In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the &lt;code&gt;average&lt;/code&gt; parameter.</source>
          <target state="translated">멀티 클래스 및 멀티 라벨 사례에서 이는 &lt;code&gt;average&lt;/code&gt; 매개 변수 에 따라 가중치를 적용한 각 클래스의 F1 점수 평균 입니다.</target>
        </trans-unit>
        <trans-unit id="47604d7ff8f733fb868ecbca5a77b859a57fe5aa" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the Matthews correlation coefficient can be &lt;a href=&quot;http://rk.kvl.dk/introduction/index.html&quot;&gt;defined&lt;/a&gt; in terms of a &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt;&lt;code&gt;confusion_matrix&lt;/code&gt;&lt;/a&gt;\(C\) for \(K\) classes. To simplify the definition consider the following intermediate variables:</source>
          <target state="translated">다중 클래스의 경우, Matthews 상관 계수는 \ (K \) 클래스 에 대한 &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt; &lt;code&gt;confusion_matrix&lt;/code&gt; &lt;/a&gt; \ (C \)의 관점에서 &lt;a href=&quot;http://rk.kvl.dk/introduction/index.html&quot;&gt;정의&lt;/a&gt; 될 수 있습니다 . 정의를 단순화하려면 다음 중간 변수를 고려하십시오.</target>
        </trans-unit>
        <trans-unit id="b8d01a57cb617acafda7dfb6fdd570d7cb46be7c" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;ovr&amp;rsquo;, and uses the cross- entropy loss if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;multinomial&amp;rsquo;. (Currently the &amp;lsquo;multinomial&amp;rsquo; option is supported only by the &amp;lsquo;lbfgs&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;newton-cg&amp;rsquo; solvers.)</source>
          <target state="translated">멀티 클래스의 경우 훈련 알고리즘은 'multi_class'옵션이 'ovr'로 설정된 경우 OvR (one-vs-rest) 방식을 사용하고 'multi_class'옵션이 '다항식으로 설정된 경우 교차 엔트로피 손실을 사용합니다. '. (현재 '다항식'옵션은 'lbfgs', 'sag'및 'newton-cg'솔버에서만 지원됩니다.)</target>
        </trans-unit>
        <trans-unit id="df1f72c5b54cf7ce11968ba990ade83d8b80475a" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;ovr&amp;rsquo;, and uses the cross-entropy loss if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;multinomial&amp;rsquo;. (Currently the &amp;lsquo;multinomial&amp;rsquo; option is supported only by the &amp;lsquo;lbfgs&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;newton-cg&amp;rsquo; solvers.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fc004eb44e3b89b80f2b6796f35d04551e921cc" translate="yes" xml:space="preserve">
          <source>In the multiclass case:</source>
          <target state="translated">멀티 클래스의 경우 :</target>
        </trans-unit>
        <trans-unit id="d77598124fa8bad46b51e89decd127c4c83bc1e3" translate="yes" xml:space="preserve">
          <source>In the multilabel case with binary label indicators, where the first label set [0,1] has an error:</source>
          <target state="translated">이진 레이블 표시기가있는 다중 레이블 경우 첫 번째 레이블 세트 [0,1]에 오류가 있습니다.</target>
        </trans-unit>
        <trans-unit id="4557110f167a92f3d0af48823270c458eb95839b" translate="yes" xml:space="preserve">
          <source>In the multilabel case with binary label indicators:</source>
          <target state="translated">이진 레이블 표시기가있는 다중 레이블 경우 :</target>
        </trans-unit>
        <trans-unit id="e898613ef6934f2809451f3fa3e427c4a4bd49ce" translate="yes" xml:space="preserve">
          <source>In the multilabel case, this calculates a confusion matrix per sample</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b2a464789db8978cf9674eef13e9bc3f4a4cf79" translate="yes" xml:space="preserve">
          <source>In the multilabel case:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dbb497f0422701e359a6bab16a477019a81aa92" translate="yes" xml:space="preserve">
          <source>In the multilabel learning literature, OvR is also known as the binary relevance method.</source>
          <target state="translated">다중 레이블 학습 문헌에서 OvR은 이진 관련성 방법이라고도합니다.</target>
        </trans-unit>
        <trans-unit id="b098a0ed402e179dee6b5de05c6210f893507735" translate="yes" xml:space="preserve">
          <source>In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the array returned by &lt;code&gt;transform&lt;/code&gt; will typically be dense.</source>
          <target state="translated">새 공간에서 각 차원은 클러스터 중심까지의 거리입니다. X가 희소하더라도 &lt;code&gt;transform&lt;/code&gt; 의해 반환되는 배열 은 일반적으로 밀도가 높습니다.</target>
        </trans-unit>
        <trans-unit id="a90aa674a36fbb7c4f65ac18f5e6a5a0eb4c7bc3" translate="yes" xml:space="preserve">
          <source>In the official &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/README.txt&quot;&gt;README.txt&lt;/a&gt; this task is described as the &amp;ldquo;Restricted&amp;rdquo; task. As I am not sure as to implement the &amp;ldquo;Unrestricted&amp;rdquo; variant correctly, I left it as unsupported for now.</source>
          <target state="translated">공식 &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/README.txt&quot;&gt;README.txt에서&lt;/a&gt; 에서이 작업은 &quot;제한된&quot;작업으로 설명됩니다. &amp;ldquo;제한되지 않은&amp;rdquo;변형을 올바르게 구현할지 확실하지 않으므로 지금은 지원되지 않는 것으로 남겨 두었습니다.</target>
        </trans-unit>
        <trans-unit id="5f046ff1a0210873f4b93ca532a15f1f001fc328" translate="yes" xml:space="preserve">
          <source>In the second figure, we show some regression result on a dataset of 6 sequences. Here we use the 1st, 2nd, 4th, and 5th sequences as the training set to make predictions on the 3rd and 6th sequences.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="901c760a214576d30b4ced7bbcce771984b42233" translate="yes" xml:space="preserve">
          <source>In the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers from bias or variance. However, in high-dimensional spaces, models can become very difficult to visualize. For this reason, it is often helpful to use the tools described below.</source>
          <target state="translated">예제에서 보았던 간단한 1 차원 문제에서 추정기가 편향 또는 분산으로 고통 받는지 쉽게 알 수 있습니다. 그러나 고차원 공간에서는 모델을 시각화하기가 매우 어려워 질 수 있습니다. 이러한 이유로 아래 설명 된 도구를 사용하는 것이 종종 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="bada0a0c8458a65354b2c23e7134e865cc4bf85c" translate="yes" xml:space="preserve">
          <source>In the single label multiclass case, the rows of the returned matrix sum to 1.</source>
          <target state="translated">단일 레이블 멀티 클래스의 경우 반환 된 행렬의 행은 1이됩니다.</target>
        </trans-unit>
        <trans-unit id="696912c12d134eed0fdc2e472302634288905dc5" translate="yes" xml:space="preserve">
          <source>In the small-samples situation, in which &lt;code&gt;n_samples&lt;/code&gt; is on the order of &lt;code&gt;n_features&lt;/code&gt; or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure.</source>
          <target state="translated">&lt;code&gt;n_samples&lt;/code&gt; 가 n_features 이하인 작은 표본 상황에서는 희소 &lt;code&gt;n_features&lt;/code&gt; 분산 추정기가 수축 공분산 추정기보다 더 잘 작동하는 경향이 있습니다. 그러나 반대 상황이나 상관 관계가 높은 데이터의 경우 수치 적으로 불안정 할 수 있습니다. 또한 수축 추정기와 달리 희소 추정기는 비 대각선 구조를 복구 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1c648bbbf8ddd269e7c36de7c1822a2705968fac" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be better to use joblib&amp;rsquo;s replacement of pickle (&lt;code&gt;dump&lt;/code&gt; &amp;amp; &lt;code&gt;load&lt;/code&gt;), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5f33dda5b96ece3c041650e9fa8db02e62bfd46" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be better to use joblib&amp;rsquo;s replacement of pickle (&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:</source>
          <target state="translated">scikit-learn의 특정 경우에는 joblib의 pickle 대체 작업 ( &lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt; )을 사용하는 것이 좋습니다 . 이는 종종 장착 된 scikit- 추정기를 배우지 만 문자열이 아닌 디스크로만 피클 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4d7c6f4a78fb7d42f5b6b076760e4c0fb27e052c" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be more interesting to use joblib&amp;rsquo;s replacement for pickle (&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;), which is more efficient on big data but it can only pickle to the disk and not to a string:</source>
          <target state="translated">scikit-learn의 특정 경우에는 빅 데이터에서 더 효율적이지만 문자열이 아닌 디스크로만 피클 할 수있는 pickle에 대한 joblib의 대체 ( &lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt; )를 사용하는 것이 더 흥미로울 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="6bc18b31ba734fa9a1f609c8898b12e640d531fb" translate="yes" xml:space="preserve">
          <source>In the statistics community, it is common practice to perform multiple imputations, generating, for example, &lt;code&gt;m&lt;/code&gt; separate imputations for a single feature matrix. Each of these &lt;code&gt;m&lt;/code&gt; imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The &lt;code&gt;m&lt;/code&gt; final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3fa57071e687f0bc6b84bf63b957df19fb1d89e" translate="yes" xml:space="preserve">
          <source>In the third figure, we demonstrate a classification model by training on 6 sequences and make predictions on another 5 sequences. The ground truth here is simply whether there is at least one &amp;lsquo;A&amp;rsquo; in the sequence. Here the model makes four correct classifications and fails on one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e8940d2e745f9a24bd23b0a1547dcf715a870bb" translate="yes" xml:space="preserve">
          <source>In the total set of features, only the 4 first ones are significant. We can see that they have the highest score with univariate feature selection. The SVM assigns a large weight to one of these features, but also Selects many of the non-informative features. Applying univariate feature selection before the SVM increases the SVM weight attributed to the significant features, and will thus improve classification.</source>
          <target state="translated">전체 기능 세트에서 처음 4 개의 기능 만 중요합니다. 일 변량 피처 선택에서 점수가 가장 높다는 것을 알 수 있습니다. SVM은 이러한 기능 중 하나에 큰 가중치를 할당하지만 정보가 아닌 많은 기능도 선택합니다. SVM 전에 일 변량 기능 선택을 적용하면 중요한 기능으로 인한 SVM 가중치가 증가하므로 분류가 향상됩니다.</target>
        </trans-unit>
        <trans-unit id="5cab08ab26978fd8cb0ee3262a21c03baca4d379" translate="yes" xml:space="preserve">
          <source>In the transformed &lt;code&gt;X&lt;/code&gt;, the first column is the encoding of the feature with categories &amp;ldquo;male&amp;rdquo;/&amp;rdquo;female&amp;rdquo;, while the remaining 6 columns is the encoding of the 2 features with respectively 3 categories each.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3dd53135ff49dbe7e421a248bf614a3ea8f0d5e" translate="yes" xml:space="preserve">
          <source>In the vector quantization literature, &lt;code&gt;cluster_centers_&lt;/code&gt; is called the code book and each value returned by &lt;code&gt;predict&lt;/code&gt; is the index of the closest code in the code book.</source>
          <target state="translated">벡터 양자화 문헌에서 &lt;code&gt;cluster_centers_&lt;/code&gt; 를 코드북 이라고하며 &lt;code&gt;predict&lt;/code&gt; 에 의해 반환되는 각 값은 코드북 에서 가장 가까운 코드의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="71ea1d0e6870ae14110d577f25dbcd29b63431c3" translate="yes" xml:space="preserve">
          <source>In their 2004 paper &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt;, O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient \(\alpha\) that minimizes the Mean Squared Error between the estimated and the real covariance matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a63d5086bf9614df56a7612405271e0122a8145" translate="yes" xml:space="preserve">
          <source>In their 2004 paper &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;, O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient \(\alpha\) that minimizes the Mean Squared Error between the estimated and the real covariance matrix.</source>
          <target state="translated">2004 년 논문 &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; 에서 O. Ledoit와 M. Wolf는 추정 된 공분산 행렬과 실제 공분산 행렬 사이의 평균 제곱 오차를 최소화하는 최적 수축 계수 \ (\ alpha \)를 계산하는 공식을 제안합니다.</target>
        </trans-unit>
        <trans-unit id="ee9767309b3df05ebf7c392a0bb4e8915eee3d46" translate="yes" xml:space="preserve">
          <source>In these settings, the &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;Spectral clustering&lt;/a&gt; approach solves the problem know as &amp;lsquo;normalized graph cuts&amp;rsquo;: the image is seen as a graph of connected voxels, and the spectral clustering algorithm amounts to choosing graph cuts defining regions while minimizing the ratio of the gradient along the cut, and the volume of the region.</source>
          <target state="translated">이 설정에서 &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;스펙트럼 클러스터링&lt;/a&gt; 접근 방식은 '정규화 된 그래프 컷'으로 알려진 문제를 해결합니다. 이미지는 연결된 복셀의 그래프로 표시되며 스펙트럼 클러스터링 알고리즘은 그래디언트 비율을 최소화하면서 영역을 정의하는 그래프 컷을 선택하는 것과 같습니다. 컷 및 영역의 볼륨.</target>
        </trans-unit>
        <trans-unit id="e2e4475ec0999dd975ba681e19179d817d6681ae" translate="yes" xml:space="preserve">
          <source>In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold.</source>
          <target state="translated">이 경우 특정 그룹 집합에 대해 훈련 된 모델이 보이지 않는 그룹으로 일반화되는지 알고 싶습니다. 이를 측정하기 위해 검증 폴드의 모든 샘플이 짝을 이루는 트레이닝 폴드에서 전혀 표현되지 않은 그룹에서 나온 것인지 확인해야합니다.</target>
        </trans-unit>
        <trans-unit id="8e6586aaac37d3a887b7276aee6797fdd6471b11" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;X_test&lt;/code&gt; are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:</source>
          <target state="translated">이 경우 &lt;code&gt;X_train&lt;/code&gt; 과 &lt;code&gt;X_test&lt;/code&gt; 는 동일한 수의 기능을 갖습니다. 동일한 결과를 얻는 또 다른 방법은 기능 수를 수정하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="c94c921d6a6a8582b29da8ef5a3a44fe1ea80a89" translate="yes" xml:space="preserve">
          <source>In this case, the classifier is fit upon instances each assigned multiple labels. The &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt; is used to binarize the 2d array of multilabels to &lt;code&gt;fit&lt;/code&gt; upon. As a result, &lt;code&gt;predict()&lt;/code&gt; returns a 2d array with multiple predicted labels for each instance.</source>
          <target state="translated">이 경우 분류기는 여러 레이블이 할당 된 인스턴스에 적합합니다. &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; 는&lt;/a&gt; 에 multilabels의 2 차원 배열을 이진화하는 데 사용됩니다 &lt;code&gt;fit&lt;/code&gt; 시. 결과적으로 &lt;code&gt;predict()&lt;/code&gt; 는 각 인스턴스에 대해 여러 개의 예측 레이블이있는 2 차원 배열을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="4367a423584d0b6ba622189fa17b21bb3e206f2c" translate="yes" xml:space="preserve">
          <source>In this case, the cross-validation retained the same ratio of classes across each CV split. Next we&amp;rsquo;ll visualize this behavior for a number of CV iterators.</source>
          <target state="translated">이 경우 교차 검증은 각 CV 분할에서 동일한 비율의 클래스를 유지했습니다. 다음으로 여러 CV 반복자에 대해이 동작을 시각화합니다.</target>
        </trans-unit>
        <trans-unit id="d121f450bc55250670235f93c8cd2083eb40a561" translate="yes" xml:space="preserve">
          <source>In this context, we can define the notions of precision, recall and F-measure:</source>
          <target state="translated">이러한 맥락에서 우리는 정밀도, 리콜 및 F 측정의 개념을 정의 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="308ef10a0e6fa4feab74c9fff8eae0756aa171db" translate="yes" xml:space="preserve">
          <source>In this dataset, each sample corresponds to an insurance policy, i.e. a contract within an insurance company and an individual (policyholder). Available features include driver age, vehicle age, vehicle power, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b69830262e95394b14f5a754f7290cc7372dc2b1" translate="yes" xml:space="preserve">
          <source>In this dataset, each sample corresponds to an insurance policy. Available features include driver age, vehicle age, vehicle power, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b7e5d4a758a26d1b659ba54387246d5cebcf12f" translate="yes" xml:space="preserve">
          <source>In this example the dependent variable Y is set as a function of the input features: y = X*w + c. The coefficient vector w is randomly sampled from a normal distribution, whereas the bias term c is set to a constant.</source>
          <target state="translated">이 예에서 종속 변수 Y는 입력 기능의 함수로 설정됩니다 : y = X * w + c. 계수 벡터 w는 정규 분포에서 무작위로 샘플링되는 반면, 바이어스 항 c는 상수로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="5d530c717737ac885c81ddc70c9c4fe51f2f2e42" translate="yes" xml:space="preserve">
          <source>In this example the silhouette analysis is used to choose an optimal value for &lt;code&gt;n_clusters&lt;/code&gt;. The silhouette plot shows that the &lt;code&gt;n_clusters&lt;/code&gt; value of 3, 5 and 6 are a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. Silhouette analysis is more ambivalent in deciding between 2 and 4.</source>
          <target state="translated">이 예에서, 실루엣 분석은 &lt;code&gt;n_clusters&lt;/code&gt; 에 대한 최적의 값을 선택하는 데 사용됩니다 . 실루엣 플롯은 평균 실루엣 점수가 낮은 클러스터의 존재와 실루엣 플롯의 크기의 변동으로 인해 주어진 데이터에 대해 &lt;code&gt;n_clusters&lt;/code&gt; 값 3, 5 및 6이 잘못된 선택 임을 보여줍니다 . 실루엣 분석은 2와 4 사이에서 더 모호합니다.</target>
        </trans-unit>
        <trans-unit id="0b7aa73d7d4561b0e25989b4fff6f5d53474b724" translate="yes" xml:space="preserve">
          <source>In this example we compare some estimators for the purpose of missing feature imputation with &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="550c896ceafe31d2e76547c4031642097a79581f" translate="yes" xml:space="preserve">
          <source>In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results.</source>
          <target state="translated">이 예에서는 런타임 및 결과 품질 측면에서 K- 평균에 대한 다양한 초기화 전략을 비교합니다.</target>
        </trans-unit>
        <trans-unit id="d9bed364e1f96090d42e72d8ad4e31b8f81dfc1d" translate="yes" xml:space="preserve">
          <source>In this example we prefer the &lt;code&gt;elasticnet&lt;/code&gt; penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the &lt;code&gt;l1_ratio&lt;/code&gt; parameter (in combination with the regularization strength &lt;code&gt;alpha&lt;/code&gt;) to control this tradeoff.</source>
          <target state="translated">이 예제에서 우리 는 모델 소형 성과 예측력 사이에서 좋은 절충안이되기 때문에 &lt;code&gt;elasticnet&lt;/code&gt; 패널티를 선호합니다 . 이 상충 관계를 제어하기 위해 &lt;code&gt;l1_ratio&lt;/code&gt; 파라미터를 (정규화 강도 &lt;code&gt;alpha&lt;/code&gt; 와 함께) 추가로 조정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5f4ca84332e1fc2168e90c43c86e1d784ebd5f8c" translate="yes" xml:space="preserve">
          <source>In this example we see how to robustly fit a linear model to faulty data using the RANSAC algorithm.</source>
          <target state="translated">이 예에서는 RANSAC 알고리즘을 사용하여 선형 모델을 결함이있는 데이터에 강력하게 맞추는 방법을 살펴 봅니다.</target>
        </trans-unit>
        <trans-unit id="8b2f8ba713590c7461bca4df745b9c8578a9ee4a" translate="yes" xml:space="preserve">
          <source>In this example we will illustrate both approaches. We start by defining a few helper functions for loading the data and visualizing results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cb7bd690b6a99c0206d146b05cd9c20c5a47dca" translate="yes" xml:space="preserve">
          <source>In this example we will investigate different imputation techniques:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38fc37287fc51222e73dd7c83e6c92e563107ff6" translate="yes" xml:space="preserve">
          <source>In this example you might try to:</source>
          <target state="translated">이 예에서는 다음을 시도 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8a61e0fa0737723bbfe9d0174ce3aad285419f4d" translate="yes" xml:space="preserve">
          <source>In this example, &lt;code&gt;X&lt;/code&gt; is &lt;code&gt;float32&lt;/code&gt;, which is cast to &lt;code&gt;float64&lt;/code&gt; by &lt;code&gt;fit_transform(X)&lt;/code&gt;.</source>
          <target state="translated">이 예에서 &lt;code&gt;X&lt;/code&gt; 는 &lt;code&gt;float32&lt;/code&gt; 이며 &lt;code&gt;fit_transform(X)&lt;/code&gt; 의해 &lt;code&gt;float64&lt;/code&gt; 로 캐스트됩니다 .</target>
        </trans-unit>
        <trans-unit id="2812da8873763c11175cae962f9ab9000ab381c4" translate="yes" xml:space="preserve">
          <source>In this example, an image with connected circles is generated and spectral clustering is used to separate the circles.</source>
          <target state="translated">이 예에서는 연결된 원이있는 이미지가 생성되고 스펙트럼 클러스터링을 사용하여 원을 구분합니다.</target>
        </trans-unit>
        <trans-unit id="79a59d7ef51f3f34cd5dc94073ebe194acbc66c6" translate="yes" xml:space="preserve">
          <source>In this example, both modeling approaches yield comparable performance metrics. For implementation reasons, the percentage of explained variance \(D^2\) is not available for the product model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69af0b849be70a0524a821dde21a609feb16811a" translate="yes" xml:space="preserve">
          <source>In this example, pixels are represented in a 3D-space and K-means is used to find 64 color clusters. In the image processing literature, the codebook obtained from K-means (the cluster centers) is called the color palette. Using a single byte, up to 256 colors can be addressed, whereas an RGB encoding requires 3 bytes per pixel. The GIF file format, for example, uses such a palette.</source>
          <target state="translated">이 예제에서 픽셀은 3D 공간으로 표현되며 K- 평균은 64 개의 색상 군집을 찾는 데 사용됩니다. 화상 처리 문헌에서, K- 평균 (클러스터 중심)으로부터 얻은 코드북을 컬러 팔레트라고한다. 단일 바이트를 사용하면 최대 256 개의 색상을 처리 할 수 ​​있지만 RGB 인코딩에는 픽셀 당 3 바이트가 필요합니다. 예를 들어 GIF 파일 형식은 이러한 팔레트를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="2f0fb947da0f2bfc5faf32b771a3cb10ff049eda" translate="yes" xml:space="preserve">
          <source>In this example, the numeric data is standard-scaled after mean-imputation, while the categorical data is one-hot encoded after imputing missing values with a new category (&lt;code&gt;'missing'&lt;/code&gt;).</source>
          <target state="translated">이 예에서 숫자 데이터는 평균 측정 후 표준 배율이며, 범주 형 데이터는 새 범주 ( &lt;code&gt;'missing'&lt;/code&gt; )로 결 측값 을 대치 한 후 원 핫 인코딩 됩니다.</target>
        </trans-unit>
        <trans-unit id="354a556d83cef273107f176ebec9db1b19a5c757" translate="yes" xml:space="preserve">
          <source>In this example, the sinusoid is approximated by a polynomial using different pairs of initial values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d88656bc2e040320cf9595554acac12be98f916c" translate="yes" xml:space="preserve">
          <source>In this example, we compare the estimation errors that are made when using various types of location and covariance estimates on contaminated Gaussian distributed data sets:</source>
          <target state="translated">이 예에서는 오염 된 가우시안 분산 데이터 세트에서 다양한 유형의 위치 및 공분산 추정을 사용할 때 발생하는 추정 오차를 비교합니다.</target>
        </trans-unit>
        <trans-unit id="26a8e8ae6383655dcfc18bd00f71528218aa360e" translate="yes" xml:space="preserve">
          <source>In this example, we compute the permutation importance on the Wisconsin breast cancer dataset using &lt;a href=&quot;../../modules/generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;permutation_importance&lt;/code&gt;&lt;/a&gt;. The &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; can easily get about 97% accuracy on a test dataset. Because this dataset contains multicollinear features, the permutation importance will show that none of the features are important. One approach to handling multicollinearity is by performing hierarchical clustering on the features&amp;rsquo; Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b7bcaf87ef3b0730f7083836942b0b038810927" translate="yes" xml:space="preserve">
          <source>In this example, we give an overview of the &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt;&lt;code&gt;sklearn.compose.TransformedTargetRegressor&lt;/code&gt;&lt;/a&gt;. Two examples illustrate the benefit of transforming the targets before learning a linear regression model. The first example uses synthetic data while the second example is based on the Boston housing data set.</source>
          <target state="translated">이 예에서는 &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt; &lt;code&gt;sklearn.compose.TransformedTargetRegressor&lt;/code&gt; &lt;/a&gt; 의 개요를 제공합니다 . 두 가지 예는 선형 회귀 모델을 학습하기 전에 대상을 변환하는 이점을 보여줍니다. 첫 번째 예는 합성 데이터를 사용하고 두 번째 예는 보스턴 주택 데이터 세트를 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="5f4a1f2d68c8f41cbf437ea7e001b2f1285b3f2f" translate="yes" xml:space="preserve">
          <source>In this example, we illustrate the use case in which different regressors are stacked together and a final linear penalized regressor is used to output the prediction. We compare the performance of each individual regressor with the stacking strategy. Stacking slightly improves the overall performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd9410f53a0f1d1aa2f5ff77c7bafaf9751d4c08" translate="yes" xml:space="preserve">
          <source>In this example, we set the value of &lt;code&gt;gamma&lt;/code&gt; manually. To find good values for these parameters, we can use tools such as &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; and &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt;.</source>
          <target state="translated">이 예에서는 &lt;code&gt;gamma&lt;/code&gt; 값을 수동으로 설정합니다 . 이러한 매개 변수에 적합한 값을 찾기 위해 &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;그리드 검색&lt;/a&gt; 및 &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;교차 검증&lt;/a&gt; 과 같은 도구를 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="be5803bd91839824804bc4aadf2784b6ed10723a" translate="yes" xml:space="preserve">
          <source>In this example, we will compare the impurity-based feature importance of &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; with the permutation importance on the titanic dataset using &lt;a href=&quot;../../modules/generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;permutation_importance&lt;/code&gt;&lt;/a&gt;. We will show that the impurity-based feature importance can inflate the importance of numerical features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f66533d22104292e30bad75824b906ee4fc1acd9" translate="yes" xml:space="preserve">
          <source>In this example, we will construct display objects, &lt;a href=&quot;../../modules/generated/sklearn.metrics.confusionmatrixdisplay#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt;&lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.metrics.roccurvedisplay#sklearn.metrics.RocCurveDisplay&quot;&gt;&lt;code&gt;RocCurveDisplay&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.metrics.precisionrecalldisplay#sklearn.metrics.PrecisionRecallDisplay&quot;&gt;&lt;code&gt;PrecisionRecallDisplay&lt;/code&gt;&lt;/a&gt; directly from their respective metrics. This is an alternative to using their corresponding plot functions when a model&amp;rsquo;s predictions are already computed or expensive to compute. Note that this is advanced usage, and in general we recommend using their respective plot functions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e0782ea6d7859077c07d60aaa0a30b1c4373f50" translate="yes" xml:space="preserve">
          <source>In this plot you can see the training scores and validation scores of an SVM for different values of the kernel parameter gamma. For very low values of gamma, you can see that both the training score and the validation score are low. This is called underfitting. Medium values of gamma will result in high values for both scores, i.e. the classifier is performing fairly well. If gamma is too high, the classifier will overfit, which means that the training score is good but the validation score is poor.</source>
          <target state="translated">이 그림에서 커널 매개 변수 감마의 다른 값에 대한 SVM의 교육 점수 및 유효성 검사 점수를 볼 수 있습니다. 감마 값이 매우 낮은 경우 훈련 점수와 유효성 검사 점수가 모두 낮음을 알 수 있습니다. 이것을 언더 피팅이라고합니다. 감마의 중간 값은 두 점수 모두에서 높은 값을 초래합니다. 즉 분류 기가 상당히 잘 수행됩니다. 감마가 너무 높으면 분류 기가 과적 합되므로 훈련 점수는 양호하지만 유효성 검사 점수는 좋지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2c39a03080473177a8509645110953edafebbd76" translate="yes" xml:space="preserve">
          <source>In this scheme, features and samples are defined as follows:</source>
          <target state="translated">이 체계에서 기능과 샘플은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="5941fbb58c226f551ff80660bcd51a84bcc2bae1" translate="yes" xml:space="preserve">
          <source>In this section we will see how to:</source>
          <target state="translated">이 섹션에서는 다음을 수행하는 방법을 살펴 봅니다.</target>
        </trans-unit>
        <trans-unit id="6ce5845b6414a0cfccffc603f3efdd4b47c7ce4b" translate="yes" xml:space="preserve">
          <source>In this section, we introduce the &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt; vocabulary that we use throughout scikit-learn and give a simple learning example.</source>
          <target state="translated">이 섹션에서는 scikit-learn 전체에서 사용 하는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;기계 학습&lt;/a&gt; 어휘를 소개하고 간단한 학습 예를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="e7b54ae8e73f20fa370a273bbb52814367b82582" translate="yes" xml:space="preserve">
          <source>In this snippet we make use of a &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt; coupled with &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt;&lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt;&lt;/a&gt; to evaluate feature importances and select the most relevant features. Then, a &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;sklearn.ensemble.RandomForestClassifier&lt;/code&gt;&lt;/a&gt; is trained on the transformed output, i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also classifiers that provide a way to evaluate feature importances of course. See the &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt; examples for more details.</source>
          <target state="translated">이 스 니펫에서는 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt; 결합 된 &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt; &lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt; &lt;/a&gt; 를 사용하여 기능의 중요도를 평가하고 가장 관련성이 높은 기능을 선택합니다. 그런 다음 &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;sklearn.ensemble.RandomForestClassifier&lt;/code&gt; &lt;/a&gt; 는 변환 된 출력, 즉 관련 기능 만 사용하여 학습 됩니다. 다른 기능 선택 방법 및 기능 중요도를 평가하는 방법을 제공하는 분류기로 유사한 작업을 수행 할 수 있습니다. 자세한 내용은 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; 예제를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="87aee0924ee2a28e2d573efd8e050c2a5c1632a3" translate="yes" xml:space="preserve">
          <source>In unsupervised learning we only have a dataset \(X = \{x_1, x_2, \dots, x_n \}\). How can this dataset be described mathematically? A very simple &lt;code&gt;continuous latent variable&lt;/code&gt; model for \(X\) is</source>
          <target state="translated">비지도 학습에는 데이터 세트 \ (X = \ {x_1, x_2, \ dots, x_n \} \) 만 있습니다. 이 데이터 세트를 수학적으로 어떻게 설명 할 수 있습니까? 매우 간단한 &lt;code&gt;continuous latent variable&lt;/code&gt; \ (X \)에 대한 모델은</target>
        </trans-unit>
        <trans-unit id="a1efe7e891f38316d324e0fe7b8b483308bcbebf" translate="yes" xml:space="preserve">
          <source>Includes values in confusion matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c89a6ca6f29b888687c7afd577a282b5b95a2be5" translate="yes" xml:space="preserve">
          <source>Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as &lt;em&gt;data leakage&lt;/em&gt;), for example in the case of scalers or imputing missing values.</source>
          <target state="translated">예를 들어 스케일러 또는 결 측값을 대치하는 경우와 같이 테스트 데이터의 통계를 전처리기에 통합하면 교차 검증 점수를 신뢰할 수 없게합니다 ( &lt;em&gt;데이터 누출&lt;/em&gt; 이라고 함 ).</target>
        </trans-unit>
        <trans-unit id="4c33f1e1286c254eaae3fbe03197d5578f08f56a" translate="yes" xml:space="preserve">
          <source>Increasing &lt;code&gt;max_depth&lt;/code&gt; for AdaBoost lowers the standard deviation of the scores (but the average score does not improve).</source>
          <target state="translated">AdaBoost에 대해 &lt;code&gt;max_depth&lt;/code&gt; 를 늘리면 점수의 표준 편차가 낮아 지지만 평균 점수는 향상되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e79b1358981354168a853701629e2643ba45bf93" translate="yes" xml:space="preserve">
          <source>Increasing false positive rates such that element i is the false positive rate of predictions with score &amp;gt;= thresholds[i].</source>
          <target state="translated">요소 i가 스코어&amp;gt; = 임계 값 [i]을 갖는 가양 성 예측율이되도록 가양 성 비율을 증가시키는 것.</target>
        </trans-unit>
        <trans-unit id="3ca08d3a2216068596512fa76cc1f85e2464a3a8" translate="yes" xml:space="preserve">
          <source>Increasing thresholds on the decision function used to compute precision and recall.</source>
          <target state="translated">정밀도를 계산하고 호출하는 데 사용되는 의사 결정 기능의 임계 값을 증가시킵니다.</target>
        </trans-unit>
        <trans-unit id="7ae5f53b337e575381bac1d47d2d4a4d2e4839b6" translate="yes" xml:space="preserve">
          <source>Increasing true positive rates such that element i is the true positive rate of predictions with score &amp;gt;= thresholds[i].</source>
          <target state="translated">요소 i가 스코어&amp;gt; = 임계 값 [i]을 갖는 실제 양의 예측 비율이되도록 진 양성 비율을 증가시키는 것.</target>
        </trans-unit>
        <trans-unit id="54206634ab03f8962d59d7c24e12c85ebd45b5e1" translate="yes" xml:space="preserve">
          <source>Incremental PCA</source>
          <target state="translated">증분 PCA</target>
        </trans-unit>
        <trans-unit id="0254682de6b7d13bd44669747bb093c1dca18b21" translate="yes" xml:space="preserve">
          <source>Incremental Principal Component Analysis.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="acaf3165fc4e0ddec759b9648ee12eed48089691" translate="yes" xml:space="preserve">
          <source>Incremental fit on a batch of samples.</source>
          <target state="translated">샘플 배치에 증분 적합.</target>
        </trans-unit>
        <trans-unit id="a79a34aec33f8c8b084e4316cf8e243a4d6601e6" translate="yes" xml:space="preserve">
          <source>Incremental fit with X.</source>
          <target state="translated">X에 증분 적합합니다.</target>
        </trans-unit>
        <trans-unit id="87210470540ea5af2ee40f330fdeea4017f1c0aa" translate="yes" xml:space="preserve">
          <source>Incremental fit with X. All of X is processed as a single batch.</source>
          <target state="translated">X에 증분 적합. 모든 X는 단일 배치로 처리됩니다.</target>
        </trans-unit>
        <trans-unit id="5b9d567927b0a80924b0a28fdea6cf19b23d2e57" translate="yes" xml:space="preserve">
          <source>Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch size allows for control of memory usage.</source>
          <target state="translated">IPCA (증분 주성분 분석)는 일반적으로 분해 할 데이터 세트가 메모리에 맞지 않을 때 주성분 분석 (PCA)을 대체하는 데 사용됩니다. IPCA는 입력 데이터 샘플 수와 무관 한 메모리 양을 사용하여 입력 데이터에 대해 낮은 순위의 근사값을 작성합니다. 여전히 입력 데이터 기능에 의존하지만 배치 크기를 변경하면 메모리 사용을 제어 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="66088e706ece2d903ed2071fa2d42be9315774b6" translate="yes" xml:space="preserve">
          <source>Incremental principal components analysis (IPCA).</source>
          <target state="translated">증분 주성분 분석 (IPCA).</target>
        </trans-unit>
        <trans-unit id="ef7722207a6c2343d08e45f401cd00ccd19381c7" translate="yes" xml:space="preserve">
          <source>Incrementally fit the model to data.</source>
          <target state="translated">모델을 데이터에 점차적으로 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="505bf67b8aa7cec37d64a9ce9b03d73f70b38b8b" translate="yes" xml:space="preserve">
          <source>Incrementally fit the model to data. Fit a separate model for each output variable.</source>
          <target state="translated">모델을 데이터에 점차적으로 맞 춥니 다. 각 출력 변수에 대해 별도의 모델을 피팅하십시오.</target>
        </trans-unit>
        <trans-unit id="e7353e5363cf13ee1c3efdc144e75ff650f6c2d1" translate="yes" xml:space="preserve">
          <source>Incrementally trained logistic regression (when given the parameter &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e12e704fa3eb16be83d58c2167c9c4f83379a1d" translate="yes" xml:space="preserve">
          <source>Indeed many estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales. In particular, metric-based and gradient-based estimators often assume approximately standardized data (centered features with unit variances). A notable exception are decision tree-based estimators that are robust to arbitrary scaling of the data.</source>
          <target state="translated">실제로 많은 추정값은 각 기능의 값이 거의 0에 가까워 지거나 모든 기능이 비슷한 규모로 변한다는 가정하에 설계되었습니다. 특히, 메트릭 기반 및 그라디언트 기반 추정기는 대략 표준화 된 데이터 (단위 차이가있는 중심 형상)를 가정합니다. 데이터의 임의 스케일링에 강력한 의사 결정 트리 기반 추정기는 예외입니다.</target>
        </trans-unit>
        <trans-unit id="9cde6c3dae7189a85444fb86f682cb4ecd226cef" translate="yes" xml:space="preserve">
          <source>Indeed, from the plot above the most important factor in determining WAGE appears to be the variable UNION, even if our intuition might tell us that variables like EXPERIENCE should have more impact.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7933c6d72de999f40e22d3286d9c782fd636305b" translate="yes" xml:space="preserve">
          <source>Independent Component Analysis: ICA</source>
          <target state="translated">독립 성분 분석 : ICA</target>
        </trans-unit>
        <trans-unit id="d170598045cdc9e2df037718a96d1706ed03e640" translate="yes" xml:space="preserve">
          <source>Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt;&lt;code&gt;Fast ICA&lt;/code&gt;&lt;/a&gt; algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants.</source>
          <target state="translated">독립 성분 분석은 다변량 신호를 최대한 독립적 인 부가 성분으로 분리합니다. &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt; &lt;code&gt;Fast ICA&lt;/code&gt; 를&lt;/a&gt; 사용하여 scikit-learn에서 구현됩니다. 알고리즘을 . 일반적으로 ICA는 차원을 줄이는 데 사용되지 않고 중첩 된 신호를 분리하는 데 사용됩니다. ICA 모델에는 노이즈 항이 포함되어 있지 않으므로 모델이 정확하려면 미백을 적용해야합니다. 이는 whiten 인수를 사용하여 내부적으로 수행하거나 PCA 변형 중 하나를 사용하여 수동으로 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="420bdf88d0c37e0c49c684e7be83be0c3065749b" translate="yes" xml:space="preserve">
          <source>Independent component analysis, a latent variable model with non-Gaussian latent variables.</source>
          <target state="translated">독립 성분 분석, 비 가우시안 잠재 변수가있는 잠재 변수 모델.</target>
        </trans-unit>
        <trans-unit id="e81fd2ba1ed4351b51becec6b3e044be03272d29" translate="yes" xml:space="preserve">
          <source>Independent parameter in poly/sigmoid kernel.</source>
          <target state="translated">폴리 / 시그 모이 드 커널의 독립 매개 변수.</target>
        </trans-unit>
        <trans-unit id="75b2e172573134b992419919380eaa4d379125c8" translate="yes" xml:space="preserve">
          <source>Independent parameter in poly/sigmoid kernel. 0 by default.</source>
          <target state="translated">폴리 / 시그 모이 드 커널의 독립 매개 변수. 기본적으로 0입니다.</target>
        </trans-unit>
        <trans-unit id="93ccb8f475af3ead0828a4d704d80fd7c1bcca2a" translate="yes" xml:space="preserve">
          <source>Independent term in decision function.</source>
          <target state="translated">의사 결정 기능에서 독립적 인 용어.</target>
        </trans-unit>
        <trans-unit id="d60b4ce63cb13d9b546e71bb468305b122e1ff12" translate="yes" xml:space="preserve">
          <source>Independent term in decision function. Set to 0.0 if &lt;code&gt;fit_intercept = False&lt;/code&gt;.</source>
          <target state="translated">의사 결정 기능에서 독립적 인 용어. &lt;code&gt;fit_intercept = False&lt;/code&gt; 경우 0.0으로 설정하십시오 .</target>
        </trans-unit>
        <trans-unit id="62ca36e367478a373b265bf6692ac1dd0b87c736" translate="yes" xml:space="preserve">
          <source>Independent term in kernel function. It is only significant in &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">커널 함수에서 독립적 인 용어. 'poly'와 'sigmoid'에서만 중요합니다.</target>
        </trans-unit>
        <trans-unit id="498514c5789196d4e7f38be6d2ccefad9084f660" translate="yes" xml:space="preserve">
          <source>Independent term in poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">폴리 및 시그 모이 드 커널에서 독립적 인 용어. 다른 커널에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="c818388cffefe0c1449b9099e6b8c434f2466b05" translate="yes" xml:space="preserve">
          <source>Independent term in the decision function.</source>
          <target state="translated">의사 결정 기능에서 독립 용어.</target>
        </trans-unit>
        <trans-unit id="b8069da00c91cf6e966b739b57f9cbe347e859d2" translate="yes" xml:space="preserve">
          <source>Independent term in the linear model.</source>
          <target state="translated">선형 모형에서 독립항.</target>
        </trans-unit>
        <trans-unit id="191e8d234bde29edd43499e6f75fe115a0a775a9" translate="yes" xml:space="preserve">
          <source>Independent term in the linear model. Set to 0.0 if &lt;code&gt;fit_intercept = False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c78017fad7dc4be13e21b61b07a09cb5931df33" translate="yes" xml:space="preserve">
          <source>Index of the cluster each sample belongs to.</source>
          <target state="translated">각 샘플이 속한 클러스터의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="79bbc01c7afbd569e88078c06011f6a142dc18ba" translate="yes" xml:space="preserve">
          <source>Index of the column of X to be swapped.</source>
          <target state="translated">교환 될 X의 열 색인.</target>
        </trans-unit>
        <trans-unit id="ed1d58c02de7a13d74564b832a9effc7dd7512f7" translate="yes" xml:space="preserve">
          <source>Index of the row of X to be swapped.</source>
          <target state="translated">교환 될 X 행의 인덱스.</target>
        </trans-unit>
        <trans-unit id="ec76f2d92b2be403363a104dc4a87849c9c331a9" translate="yes" xml:space="preserve">
          <source>Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.</source>
          <target state="translated">인덱싱 가능한 데이터 구조는 일관된 첫 번째 차원을 가진 배열, 목록, 데이터 프레임 또는 scipy 희소 행렬 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="436737ead6b730ec05aa4979d3ab186eb46b0b4a" translate="yes" xml:space="preserve">
          <source>Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where &lt;code&gt;transformer&lt;/code&gt; expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data &lt;code&gt;X&lt;/code&gt; and can return any of the above.</source>
          <target state="translated">두 번째 축의 데이터를 인덱싱합니다. 정수는 위치 열로 해석되는 반면 문자열은 이름별로 DataFrame 열을 참조 할 수 있습니다. &lt;code&gt;transformer&lt;/code&gt; X를 1d 배열과 같은 (벡터) 로 예상 하는 경우 스칼라 문자열 또는 int를 사용해야합니다 . 그렇지 않으면 2d 배열이 변환기로 전달됩니다. 콜 러블에 입력 데이터 &lt;code&gt;X&lt;/code&gt; 가 전달 되고 위의 값 중 하나를 반환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="14c06502f716a74d640158c7430747c2d65e82bc" translate="yes" xml:space="preserve">
          <source>Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where &lt;code&gt;transformer&lt;/code&gt; expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data &lt;code&gt;X&lt;/code&gt; and can return any of the above. To select multiple columns by name or dtype, you can use &lt;a href=&quot;sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt;&lt;code&gt;make_column_selector&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33324894ea0a98c1ef1f880dc97967e00c913318" translate="yes" xml:space="preserve">
          <source>Indicate that func accepts a sparse matrix as input. If validate is False, this has no effect. Otherwise, if accept_sparse is false, sparse matrix inputs will cause an exception to be raised.</source>
          <target state="translated">func이 희소 행렬을 입력으로 받아 들인다는 것을 나타냅니다. validate가 False이면 효과가 없습니다. 그렇지 않으면 accept_sparse가 false이면 희소 행렬 입력으로 인해 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="eb7cd0d8cb7fae1e80a3e28d3771772ae8884b48" translate="yes" xml:space="preserve">
          <source>Indicate that the input X array should be checked before calling &lt;code&gt;func&lt;/code&gt;. The possibilities are:</source>
          <target state="translated">&lt;code&gt;func&lt;/code&gt; 를 호출하기 전에 입력 X 배열을 확인해야 함을 나타냅니다 . 가능성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ae286e88bfa268243cfd38132ccb40e58428bfed" translate="yes" xml:space="preserve">
          <source>Indicate that transform should forward the y argument to the inner callable.</source>
          <target state="translated">변환이 y 인수를 내부 호출 가능으로 전달해야 함을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="9d241566a403a6506d3449cf17d407da2b6e2613" translate="yes" xml:space="preserve">
          <source>Indicates an ordering for the class labels</source>
          <target state="translated">클래스 레이블의 순서를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="5daac4dde04b0b12288306e9a52dc06ec04c0c8f" translate="yes" xml:space="preserve">
          <source>Indicates an ordering for the class labels. All entries should be unique (cannot contain duplicate classes).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1159dbae1b402de01888d0798e1ed328834a2b28" translate="yes" xml:space="preserve">
          <source>Indicates the monotonic constraint to enforce on each feature. -1, 1 and 0 respectively correspond to a positive constraint, negative constraint and no constraint. Read more in the &lt;a href=&quot;../ensemble#monotonic-cst-gbdt&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61bd4246c5310fe8fe14b7816d8991bfb310307e" translate="yes" xml:space="preserve">
          <source>Indicator used to add binary indicators for missing values. &lt;code&gt;None&lt;/code&gt; if add_indicator is False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f992c130abda366b807271662ae2ae17c7305e7" translate="yes" xml:space="preserve">
          <source>Indices according to which X will be subsampled.</source>
          <target state="translated">X가 서브 샘플링 될 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="13d9e5d82e7cab8325fb7843fa12831d259a69f1" translate="yes" xml:space="preserve">
          <source>Indices of &lt;code&gt;components_&lt;/code&gt; in the training set.</source>
          <target state="translated">훈련 세트 의 &lt;code&gt;components_&lt;/code&gt; 지수 .</target>
        </trans-unit>
        <trans-unit id="eeb5984b85169d88759ac10f7fe9d10f5246bc74" translate="yes" xml:space="preserve">
          <source>Indices of active variables at the end of the path.</source>
          <target state="translated">경로 끝에서 활성 변수의 인덱스.</target>
        </trans-unit>
        <trans-unit id="fff8cc57ffbca371ebcb1bd938597e8d339ff509" translate="yes" xml:space="preserve">
          <source>Indices of cluster centers</source>
          <target state="translated">클러스터 센터의 지표</target>
        </trans-unit>
        <trans-unit id="2c68ceeb78b6311d290c266259420efe85138435" translate="yes" xml:space="preserve">
          <source>Indices of columns in the dataset that belong to the bicluster.</source>
          <target state="translated">데이터 세트에서 bicluster에 속하는 열의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="d4d2ad6637f8190da67b396b7e452baac4f7558f" translate="yes" xml:space="preserve">
          <source>Indices of core samples.</source>
          <target state="translated">핵심 샘플의 지표.</target>
        </trans-unit>
        <trans-unit id="82cd4a5510ba380bec1e554cdeb5d721222207d6" translate="yes" xml:space="preserve">
          <source>Indices of features for a given plot. A tuple of one integer will plot a partial dependence curve of one feature. A tuple of two integers will plot a two-way partial dependence curve as a contour plot.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c71c298055da26ecac09942b9115f5fc73f7640d" translate="yes" xml:space="preserve">
          <source>Indices of rows in the dataset that belong to the bicluster.</source>
          <target state="translated">데이터 세트에서 bicluster에 속하는 행의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="d9c7ee7b4f1a89fde0903f47c0111e0985a4d274" translate="yes" xml:space="preserve">
          <source>Indices of samples used when training the estimators. &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;estimator&lt;/code&gt; does not have &lt;code&gt;_pairwise&lt;/code&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5ecd973c55633f90c97a971f74a80fe11af3310" translate="yes" xml:space="preserve">
          <source>Indices of support vectors.</source>
          <target state="translated">지지 벡터의 지표.</target>
        </trans-unit>
        <trans-unit id="9ccd80ce2c5e0529264583d000f7c9651892271d" translate="yes" xml:space="preserve">
          <source>Indices of the approximate nearest points in the population matrix.</source>
          <target state="translated">모집단 행렬에서 가장 가까운 가장 가까운 점의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="94147abfa5127a12fe3c3b0c7153a32b171d401a" translate="yes" xml:space="preserve">
          <source>Indices of the nearest points in the population matrix.</source>
          <target state="translated">모집단 행렬에서 가장 가까운 점의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="3db97f5586a1b88a52d6998bdcb68ce6424bd44e" translate="yes" xml:space="preserve">
          <source>Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models.</source>
          <target state="translated">트리 구조를 시각화하여 개별 의사 결정 트리를 쉽게 해석 할 수 있습니다. 그러나 그라디언트 부스팅 모델은 수백 개의 회귀 트리를 포함하므로 개별 트리의 육안 검사로 쉽게 해석 할 수 없습니다. 다행히도, 기울기 부스팅 모델을 요약하고 해석하기 위해 많은 기술이 제안되었습니다.</target>
        </trans-unit>
        <trans-unit id="dda28c621b6ebb6a75808a25ba823af15c148423" translate="yes" xml:space="preserve">
          <source>Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the feature importance of each tree (see &lt;a href=&quot;#random-forest-feature-importance&quot;&gt;Feature importance evaluation&lt;/a&gt; for more details).</source>
          <target state="translated">개별 의사 결정 트리는 본질적으로 적절한 분리 점을 선택하여 기능 선택을 수행합니다. 이 정보는 각 기능의 중요성을 측정하는 데 사용될 수 있습니다. 기본 아이디어는 : 기능이 트리의 분할 지점에서 자주 사용되는 경우 해당 기능이 더 중요하다는 것입니다. 이 중요성 개념은 각 트리의 기능 중요도를 평균화하여 의사 결정 트리 앙상블까지 확장 할 수 있습니다 (자세한 내용은 &lt;a href=&quot;#random-forest-feature-importance&quot;&gt;기능 중요도 평가&lt;/a&gt; 참조).</target>
        </trans-unit>
        <trans-unit id="a4ca1ad4d7f055ed4989788bfa5efeb8e8f28cc3" translate="yes" xml:space="preserve">
          <source>Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the impurity-based feature importance of each tree (see &lt;a href=&quot;#random-forest-feature-importance&quot;&gt;Feature importance evaluation&lt;/a&gt; for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3bee3019e098ad6b65e2ff793a0e712b529b8f1" translate="yes" xml:space="preserve">
          <source>Individual samples are assumed to be files stored a two levels folder structure such as the following:</source>
          <target state="translated">개별 샘플은 다음과 같은 2 단계 폴더 구조로 저장된 파일 인 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="675958cddf4afedd9b3304a64c0ebf41aefd317c" translate="yes" xml:space="preserve">
          <source>Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to &lt;code&gt;'passthrough'&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0fc01010c6e24d626cecdd62fd33ec75c0c929c" translate="yes" xml:space="preserve">
          <source>Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to &lt;code&gt;None&lt;/code&gt;:</source>
          <target state="translated">개별 단계는 매개 변수로 대체 될 수 있으며, 비 최종 단계는 &lt;code&gt;None&lt;/code&gt; 으로 설정하여 무시할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1005c12f11beba0f3b6f9dd6a7112c31b922588d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample</source>
          <target state="translated">각 샘플에 대한 개별 중량</target>
        </trans-unit>
        <trans-unit id="1fda26bba39629c5adc019bfeebf23b6ea1cae92" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample raises error if sample_weight is passed and base_estimator fit method does not support it.</source>
          <target state="translated">sample_weight가 전달되고 base_estimator fit 방법이이를 지원하지 않으면 각 샘플에 대한 개별 가중치가 오류를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="fa75f1e4d13933a19ded3aa860129fc7cab3ed7d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample, ignored if None is passed.</source>
          <target state="translated">각 샘플에 대한 개별 가중치. 없음이 전달되면 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="00c4a167764cbf2ab25348f070ffdca6c4a1b160" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample. If given a float, every sample will have the same weight.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f445327eb6fc51f2a25624b484ae69f8950143fb" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample. If given a float, every sample will have the same weight. If sample_weight is not None and solver=&amp;rsquo;auto&amp;rsquo;, the solver will be set to &amp;lsquo;cholesky&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="718e842694bb04faf5cc021c83e29c576f98c12d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample. If sample_weight is not None and solver=&amp;rsquo;auto&amp;rsquo;, the solver will be set to &amp;lsquo;cholesky&amp;rsquo;.</source>
          <target state="translated">각 샘플에 대한 개별 중량. sample_weight가 None이 아니고 solver = 'auto'이면 솔버는 'cholesky'로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="080f186426dd365e06ebdb64bb4da000ce313de8" translate="yes" xml:space="preserve">
          <source>Inductive Clustering</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="390c151b41ae038aa81f8d4fccaa7e2c366dd521" translate="yes" xml:space="preserve">
          <source>Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4facccaf3ac8930794a0b5d15e4cd633a166965" translate="yes" xml:space="preserve">
          <source>Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;). Running a dimensionality reduction algorithm such as &lt;a href=&quot;decomposition#pca&quot;&gt;Principal component analysis (PCA)&lt;/a&gt; prior to k-means clustering can alleviate this problem and speed up the computations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f221e5d04a99f60098722e2605a369c8541e6c9c" translate="yes" xml:space="preserve">
          <source>Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;). Running a dimensionality reduction algorithm such as &lt;a href=&quot;pca&quot;&gt;PCA&lt;/a&gt; prior to k-means clustering can alleviate this problem and speed up the computations.</source>
          <target state="translated">관성은 정규화 된 측정 항목이 아닙니다. 낮은 값이 더 좋고 0이 최적이라는 것을 알고 있습니다. 그러나 매우 높은 공간에서 유클리드 거리는 팽창하는 경향이 있습니다 (이것은 소위 &quot;차원의 저주&quot;의 예입니다). k- 평균 군집화 전에 &lt;a href=&quot;pca&quot;&gt;PCA&lt;/a&gt; 와 같은 차원 축소 알고리즘을 실행하면 이 문제를 완화하고 계산 속도를 높일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e010b0c9058bbaf9975d3f14818f3861029f83a1" translate="yes" xml:space="preserve">
          <source>Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.</source>
          <target state="translated">관성은 클러스터가 볼록하고 등방성이라고 가정합니다. 항상 그런 것은 아닙니다. 길쭉한 클러스터 또는 불규칙한 모양의 매니 폴드에는 제대로 반응하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2d57b1c4d1f958efe3710f23677dd552a8a1384c" translate="yes" xml:space="preserve">
          <source>Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:</source>
          <target state="translated">관성 또는 클러스터 내 제곱 기준 기준은 내부적으로 응집성있는 클러스터의 척도로 인식 될 수 있습니다. 여러 가지 단점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="1b9b7d4cd56309d7954eee8c5d88a8d58559a22d" translate="yes" xml:space="preserve">
          <source>Inference of the model can be time consuming.</source>
          <target state="translated">모델의 추론은 시간이 많이 걸릴 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="52570031388abf13077117eb25231bb2412ec6ba" translate="yes" xml:space="preserve">
          <source>Inferred batch size from &lt;code&gt;batch_size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3406f3eade82db35711ae0ecbcd4dea59f7e1be" translate="yes" xml:space="preserve">
          <source>Inferred value for &lt;code&gt;increasing&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f5d2c4b74b9f6de8f1a369f5793fd5ad3db1bd0" translate="yes" xml:space="preserve">
          <source>Influence of outliers on location and covariance estimates</source>
          <target state="translated">위치 및 공분산 추정치에 대한 특이 치의 영향</target>
        </trans-unit>
        <trans-unit id="b8c700f6663aab653644d35fb6aa9a0e53919c01" translate="yes" xml:space="preserve">
          <source>Information on how to contribute. This also contains useful information for advanced users, for example how to build their own estimators.</source>
          <target state="translated">기여 방법에 대한 정보. 여기에는 고급 사용자를위한 유용한 정보 (예 : 자체 견적서를 작성하는 방법)가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="c4fbdb7aab44015fbed39936864f9255a99074c1" translate="yes" xml:space="preserve">
          <source>Information-criterion based model selection is very fast, but it relies on a proper estimation of degrees of freedom, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).</source>
          <target state="translated">정보 기준 기반 모델 선택은 매우 빠르지 만, 자유도의 적절한 추정에 의존하고, 큰 샘플 (점근 적 결과)에 대해 도출되며 모델이 올바른 것으로 가정합니다. 즉,이 모델에 의해 데이터가 실제로 생성된다고 가정합니다. 또한 문제가 심하게 조절되면 (샘플보다 더 많은 기능이있는 경우) 중단되는 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="132062fafb54cb7d1e8b42d922906ff561bc3d9c" translate="yes" xml:space="preserve">
          <source>Inherits from SGDClassifier. &lt;code&gt;Perceptron()&lt;/code&gt; is equivalent to &lt;code&gt;SGDClassifier(loss=&quot;perceptron&quot;, eta0=1, learning_rate=&quot;constant&quot;, penalty=None)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="897de0e7da29095e5d730f2c7102743d023bc056" translate="yes" xml:space="preserve">
          <source>Initial value for alpha (precision of the noise). If not set, alpha_init is 1/Var(y).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5584ccc82484d2c61a85300c7acd35cb5205fef6" translate="yes" xml:space="preserve">
          <source>Initial value for lambda (precision of the weights). If not set, lambda_init is 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="168bf67b39abe0e5515698a4ec915c75e07ca524" translate="yes" xml:space="preserve">
          <source>Initial value for the dictionary for warm restart scenarios.</source>
          <target state="translated">웜 재시작 시나리오에 대한 사전의 초기 값입니다.</target>
        </trans-unit>
        <trans-unit id="64b2b9f72c239478fc1b9e586ac8147218ca87bd" translate="yes" xml:space="preserve">
          <source>Initial value for the sparse code for warm restart scenarios.</source>
          <target state="translated">웜 재시작 시나리오를위한 스파 스 코드의 초기 값입니다.</target>
        </trans-unit>
        <trans-unit id="1a6282c9a9baf1c9231fe6132d9510c0860835e2" translate="yes" xml:space="preserve">
          <source>Initial values for the components for warm restart scenarios.</source>
          <target state="translated">웜 재시작 시나리오를위한 구성 요소의 초기 값.</target>
        </trans-unit>
        <trans-unit id="ff1f0a80e07dd6cd648bd3a5e935a909ec2cb299" translate="yes" xml:space="preserve">
          <source>Initial values for the loadings for warm restart scenarios.</source>
          <target state="translated">웜 재시작 시나리오의로드에 대한 초기 값입니다.</target>
        </trans-unit>
        <trans-unit id="e597ad1e68022a1929058718fd92959e281b3619" translate="yes" xml:space="preserve">
          <source>Initialization of embedding. Possible options are &amp;lsquo;random&amp;rsquo;, &amp;lsquo;pca&amp;rsquo;, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.</source>
          <target state="translated">임베딩 초기화. 가능한 옵션은 'random', 'pca'및 numpy 배열 형태 (n_samples, n_components)입니다. PCA 초기화는 사전 계산 된 거리와 함께 사용할 수 없으며 일반적으로 임의 초기화보다 전체적으로 안정적입니다.</target>
        </trans-unit>
        <trans-unit id="726483459f359a8ededc78286c7835b11430e40a" translate="yes" xml:space="preserve">
          <source>Initialization of the linear transformation. Possible options are &amp;lsquo;auto&amp;rsquo;, &amp;lsquo;pca&amp;rsquo;, &amp;lsquo;lda&amp;rsquo;, &amp;lsquo;identity&amp;rsquo;, &amp;lsquo;random&amp;rsquo;, and a numpy array of shape (n_features_a, n_features_b).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e644154875d3e41452e4b87177ec7a98e4e47540" translate="yes" xml:space="preserve">
          <source>Initialization value for coefficients of logistic regression. Useless for liblinear solver.</source>
          <target state="translated">로지스틱 회귀 계수의 초기화 값입니다. liblinear 솔버에는 쓸모가 없습니다.</target>
        </trans-unit>
        <trans-unit id="ed7011971816dd0f1903ad54a411facedaee4ded" translate="yes" xml:space="preserve">
          <source>Initialization value of the sparse codes. Only used if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">스파 스 코드의 초기화 값. &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 경우에만 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="8989f9ab8650a480a58e2f03b20f1b5428df4549" translate="yes" xml:space="preserve">
          <source>Initialization value of the sparse codes. Only used if &lt;code&gt;algorithm='lasso_cd'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f66672d1a211feccb2663e524389cc7bbdc9544" translate="yes" xml:space="preserve">
          <source>Initialize self. See help(type(self)) for accurate signature.</source>
          <target state="translated">자기를 초기화하십시오. 정확한 서명은 help (type (self))를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="ccac906d8789727d67e839b8a0f34db2f9002a71" translate="yes" xml:space="preserve">
          <source>Initializing components, sampling from layers during fit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4694fddfebcd07057574f3bedbe073aecef8cbe" translate="yes" xml:space="preserve">
          <source>Inliers are labeled 1, while outliers are labeled -1. The predict method makes use of a threshold on the raw scoring function computed by the estimator. This scoring function is accessible through the &lt;code&gt;score_samples&lt;/code&gt; method, while the threshold can be controlled by the &lt;code&gt;contamination&lt;/code&gt; parameter.</source>
          <target state="translated">특이 치는 1로 표시되고 특이 치는 -1로 표시됩니다. 예측 방법은 추정기에 의해 계산 된 원시 스코어링 함수에 대한 임계 값을 사용합니다. 이 스코어링 기능은 &lt;code&gt;score_samples&lt;/code&gt; 방법을 통해 액세스 할 수 있으며 임계 값은 &lt;code&gt;contamination&lt;/code&gt; 매개 변수 로 제어 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2495be2cc170e277a5d9d5dd99a3f779ff0175c9" translate="yes" xml:space="preserve">
          <source>Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid loosing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">알고리즘에 의해 유지되는 내부 통계가 충분합니다. 초기화시이를 전달하는 것은 온라인 설정에서 유용하여 진화의 역사를 잃지 않도록합니다. A (n_components, n_components)는 사전 공분산 행렬입니다. B (n_features, n_components)는 데이터 근사 행렬입니다</target>
        </trans-unit>
        <trans-unit id="06dcf06c32dda54c31aa74eee694be9763d822e4" translate="yes" xml:space="preserve">
          <source>Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid losing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4cb705151cf2a5dcfea8029a0e74d39c67469fa5" translate="yes" xml:space="preserve">
          <source>Inplace column scaling of a CSC/CSR matrix.</source>
          <target state="translated">CSC / CSR 매트릭스의 열 스케일링</target>
        </trans-unit>
        <trans-unit id="15ad21d3f40808b965488683b0faaff07ad4b5e9" translate="yes" xml:space="preserve">
          <source>Inplace column scaling of a CSR matrix.</source>
          <target state="translated">CSR 매트릭스의 열 스케일링</target>
        </trans-unit>
        <trans-unit id="75290bdbc975498c98c7572ec067e43ddceb0c68" translate="yes" xml:space="preserve">
          <source>Inplace row normalize using the l1 norm</source>
          <target state="translated">l1 규범을 사용하여 Inplace row 정규화</target>
        </trans-unit>
        <trans-unit id="bc0180825da8415aba8c76f27c29d7e471b70c2a" translate="yes" xml:space="preserve">
          <source>Inplace row normalize using the l2 norm</source>
          <target state="translated">L2 규범을 사용하여 Inplace Row 정규화</target>
        </trans-unit>
        <trans-unit id="d91ae689358e283ef6d343e24e55244f1fb1cad2" translate="yes" xml:space="preserve">
          <source>Inplace row scaling of a CSR or CSC matrix.</source>
          <target state="translated">CSR 또는 CSC 매트릭스의 적절한 행 스케일링.</target>
        </trans-unit>
        <trans-unit id="16ca749420dd58126c1f3f4ccf95ce2fc49387c9" translate="yes" xml:space="preserve">
          <source>Input array.</source>
          <target state="translated">입력 배열.</target>
        </trans-unit>
        <trans-unit id="f6fcca00499ce6b21e6114d56b450f6d3d43ccb2" translate="yes" xml:space="preserve">
          <source>Input checker utility for building a cross-validator</source>
          <target state="translated">교차 유효성 검사기를 작성하기위한 입력 검사기 유틸리티</target>
        </trans-unit>
        <trans-unit id="3f43a2e4863dbf39da8cfbd5e4e557f3052ec269" translate="yes" xml:space="preserve">
          <source>Input data</source>
          <target state="translated">입력 데이터</target>
        </trans-unit>
        <trans-unit id="a2e2ac083ce3186e216de5448418ffc50e83a494" translate="yes" xml:space="preserve">
          <source>Input data for prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41aa04ac5754100b497c803419573444b7e1d42b" translate="yes" xml:space="preserve">
          <source>Input data representation and sparsity</source>
          <target state="translated">입력 데이터 표현 및 희소성</target>
        </trans-unit>
        <trans-unit id="66a8a4e34fe15bd5eafb5d984d77b9c0e3866728" translate="yes" xml:space="preserve">
          <source>Input data that will be transformed.</source>
          <target state="translated">변환 될 입력 데이터.</target>
        </trans-unit>
        <trans-unit id="fbb05f66a147e8520f226c078931507f60d0caac" translate="yes" xml:space="preserve">
          <source>Input data that will be transformed. It cannot be sparse.</source>
          <target state="translated">변환 될 입력 데이터. 드문 드문 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="1e3e0a570b83c9cbe639106afeb9bedd23e3fcfd" translate="yes" xml:space="preserve">
          <source>Input data to be transformed.</source>
          <target state="translated">변환 할 입력 데이터.</target>
        </trans-unit>
        <trans-unit id="071feefe9143dba47a473de169ba49367bfce443" translate="yes" xml:space="preserve">
          <source>Input data to be transformed. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csr_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">변환 할 입력 데이터. 효율성을 극대화 하려면 &lt;code&gt;dtype=np.float32&lt;/code&gt; 를 사용하십시오 . 희소 행렬도 지원됩니다. 스파 스 &lt;code&gt;csr_matrix&lt;/code&gt; 사용 . 최대 효율성을 위해 를 .</target>
        </trans-unit>
        <trans-unit id="688f24dc1e25fac1dc510cf29e6e51a5cdee42ba" translate="yes" xml:space="preserve">
          <source>Input data used to build forests. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">포리스트를 구축하는 데 사용되는 입력 데이터. 사용 &lt;code&gt;dtype=np.float32&lt;/code&gt; 효율성을 극대화 를 .</target>
        </trans-unit>
        <trans-unit id="ece9df27fcdc2d8935842ef4ed6ac1e8d53f8b6c" translate="yes" xml:space="preserve">
          <source>Input data, of which specified subsets are used to fit the transformers.</source>
          <target state="translated">트랜스포머에 맞도록 지정된 서브 세트가 사용되는 입력 데이터.</target>
        </trans-unit>
        <trans-unit id="8c020a67c0398f86d112987222d02c36283701ba" translate="yes" xml:space="preserve">
          <source>Input data, target values.</source>
          <target state="translated">입력 데이터, 목표 값.</target>
        </trans-unit>
        <trans-unit id="0d15fc28721eb2bcf2d53de59215da674d786463" translate="yes" xml:space="preserve">
          <source>Input data, used to fit transformers.</source>
          <target state="translated">변압기에 맞는 입력 데이터.</target>
        </trans-unit>
        <trans-unit id="6db69e17f58030ae15478a3e504e982203a8ead7" translate="yes" xml:space="preserve">
          <source>Input data, where &amp;ldquo;n_samples&amp;rdquo; is the number of samples and &amp;ldquo;n_features&amp;rdquo; is the number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aee2f5203193bb3069f6e2f7b08e833e91d53841" translate="yes" xml:space="preserve">
          <source>Input data, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="translated">입력 데이터. 여기서 &lt;code&gt;n_samples&lt;/code&gt; 는 샘플 수이고 &lt;code&gt;n_features&lt;/code&gt; 는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="8f9c683e36e31c7c38c7e8d6a2daeccf181d4c94" translate="yes" xml:space="preserve">
          <source>Input data, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">입력 데이터. 여기서 n_samples는 샘플 수이고 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="260753716624ad2077f13f38ada17a33c0f48cba" translate="yes" xml:space="preserve">
          <source>Input data.</source>
          <target state="translated">입력 데이터.</target>
        </trans-unit>
        <trans-unit id="c414149f534c72aa4d2016c93404604f17aa41fd" translate="yes" xml:space="preserve">
          <source>Input data. Columns are assumed to have unit norm.</source>
          <target state="translated">입력 데이터. 열에는 단위 규범이 있다고 가정합니다.</target>
        </trans-unit>
        <trans-unit id="fc921091c020b61d18864b21129c4eb989ef6d69" translate="yes" xml:space="preserve">
          <source>Input data. If &lt;code&gt;None&lt;/code&gt;, the output will be the pairwise similarities between all samples in &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">입력 데이터. 경우 &lt;code&gt;None&lt;/code&gt; , 출력은 모든 샘플 사이의 페어 유사성 될 것입니다 &lt;code&gt;X&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="afaf08d18a1423dc1a78bf3492e5ce0eb50d8638" translate="yes" xml:space="preserve">
          <source>Input data. If &lt;code&gt;dissimilarity=='precomputed'&lt;/code&gt;, the input should be the dissimilarity matrix.</source>
          <target state="translated">입력 데이터. 경우 &lt;code&gt;dissimilarity=='precomputed'&lt;/code&gt; , 입력은 유사성 행렬이어야한다.</target>
        </trans-unit>
        <trans-unit id="c7e2acaca5145e47486c9c2928ab5532aee98fd4" translate="yes" xml:space="preserve">
          <source>Input data. If X is not provided, only the global clustering step is done.</source>
          <target state="translated">입력 데이터. X가 제공되지 않으면 글로벌 클러스터링 단계 만 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="609eafdfc25268bf81369faed57dc8b7d067fce7" translate="yes" xml:space="preserve">
          <source>Input data. Note that if X is None then the Gram matrix must be specified, i.e., cannot be None or False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d39fe8b9c923e952612de77f70ad5aff60c34542" translate="yes" xml:space="preserve">
          <source>Input object to check / convert.</source>
          <target state="translated">확인 / 변환 할 입력 객체.</target>
        </trans-unit>
        <trans-unit id="ca7fd27e447d5e335b2e80d1b2bb1406dceac239" translate="yes" xml:space="preserve">
          <source>Input object to check / convert. Must be two-dimensional and square, otherwise a ValueError will be raised.</source>
          <target state="translated">확인 / 변환 할 입력 개체입니다. 2 차원 및 제곱이어야합니다. 그렇지 않으면 ValueError가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="f24bf584af83fad1f47a5035ff05cdc62fd1daef" translate="yes" xml:space="preserve">
          <source>Input points.</source>
          <target state="translated">입력 포인트.</target>
        </trans-unit>
        <trans-unit id="71b2d21e1f2e71c0dcf88bd09dfe9ef6fd499ea3" translate="yes" xml:space="preserve">
          <source>Input targets</source>
          <target state="translated">입력 목표</target>
        </trans-unit>
        <trans-unit id="b58e4bce1b7ebb41f970a06dbe933522f9ad2c46" translate="yes" xml:space="preserve">
          <source>Input targets multiplied by X: X.T * y</source>
          <target state="translated">입력 대상에 X를 곱한 값 : XT * y</target>
        </trans-unit>
        <trans-unit id="69a0e9f3a009e8ccbba64367545e9fbe88306b19" translate="yes" xml:space="preserve">
          <source>Input targets.</source>
          <target state="translated">입력 대상.</target>
        </trans-unit>
        <trans-unit id="51c9f9fb1fac83429c51404b5e9b2caee57cc2f2" translate="yes" xml:space="preserve">
          <source>Input validation for standard estimators.</source>
          <target state="translated">표준 추정기에 대한 입력 검증.</target>
        </trans-unit>
        <trans-unit id="346da7aa7d7e4eb884706a35a69404b7d3fc9daa" translate="yes" xml:space="preserve">
          <source>Input validation on an array, list, sparse matrix or similar.</source>
          <target state="translated">배열,리스트, 희소 행렬 또는 이와 유사한 것에 대한 입력 검증.</target>
        </trans-unit>
        <trans-unit id="4191049318f63b6c3a85211b91a5c1cecb649bdb" translate="yes" xml:space="preserve">
          <source>Input values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8344beaf285df55c120907e8b74a9a1f87253895" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are 4 independent features uniformly distributed on the intervals:</source>
          <target state="translated">입력 &lt;code&gt;X&lt;/code&gt; 는 간격에 균일하게 분포 된 4 개의 독립적 인 기능입니다.</target>
        </trans-unit>
        <trans-unit id="bcbcb56a88ddeef69ce01bcc9a78a456071e2cf0" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are independent features uniformly distributed on the interval [0, 1]. The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">입력 &lt;code&gt;X&lt;/code&gt; 는 간격 [0, 1]에 균일하게 분포 된 독립적 인 기능입니다. 출력 &lt;code&gt;y&lt;/code&gt; 는 다음 공식에 따라 생성됩니다.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
