<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="deb9c47cdc32b652eb6b8e87508a50e73ddd6520" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;Transformer&lt;/code&gt; API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;code&gt;preprocessing&lt;/code&gt; 모듈은 더 유틸리티 클래스 제공 &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt; 구현하는 것을 &lt;code&gt;Transformer&lt;/code&gt; API는 않도록 훈련 세트에 대한 평균과 표준 편차를 계산하기 위해 테스트 세트에 나중에 다시 적용 같은 변환 할 수 있습니다. 따라서이 클래스는 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; 의 초기 단계에서 사용하기에 적합합니다 .</target>
        </trans-unit>
        <trans-unit id="1c988d2d2cf78f045059c60f8f22ddfe99f6b9b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;random_state&lt;/code&gt; parameter defaults to &lt;code&gt;None&lt;/code&gt;, meaning that the shuffling will be different every time &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; is iterated. However, &lt;code&gt;GridSearchCV&lt;/code&gt; will use the same shuffling for each set of parameters validated by a single call to its &lt;code&gt;fit&lt;/code&gt; method.</source>
          <target state="translated">&lt;code&gt;random_state&lt;/code&gt; 의 에 매개 변수의 기본값은 &lt;code&gt;None&lt;/code&gt; 셔플마다 다를 수 있음을 의미 &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; 반복된다. 그러나 &lt;code&gt;GridSearchCV&lt;/code&gt; 는 한 번의 &lt;code&gt;fit&lt;/code&gt; 메소드 호출로 유효성이 검증 된 각 매개 변수 세트에 대해 동일한 셔플 링을 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="269b993dc29ab1401256004d5e02aa3f5322b3b6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remainder&lt;/code&gt; parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:</source>
          <target state="translated">&lt;code&gt;remainder&lt;/code&gt; 매개 변수는 나머지 평가 열을 변환하는 추정으로 설정 될 수있다. 변환 된 값이 변환 끝에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="ac9a3f7a78a3eea81f462ceabf4e7d7b7695c3e1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;roc_auc_score&lt;/code&gt; function can also be used in multi-class classification. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the multiclass ROC AUC scores are computed from the probability estimates that a sample belongs to a particular class according to the model. The OvO and OvR algorithms support weighting uniformly (&lt;code&gt;average='macro'&lt;/code&gt;) and weighting by the prevalence (&lt;code&gt;average='weighted'&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;roc_auc_score&lt;/code&gt; 의 기능은 다중 클래스 분류에 사용할 수 있습니다. 현재 두 가지 평균화 전략이 지원됩니다 .1 대 1 알고리즘은 쌍별 ROC AUC 점수의 평균을 계산하고 1 대 나머지 알고리즘은 다른 모든 클래스에 대한 각 클래스의 ROC AUC 점수 평균을 계산합니다. 두 경우 모두 다중 클래스 ROC AUC 점수는 표본이 모델에 따라 특정 클래스에 속할 확률 추정치에서 계산됩니다. OvO 및 OvR 알고리즘은 균일 한 가중치 ( &lt;code&gt;average='macro'&lt;/code&gt; )와 유병률에 따른 가중치 ( &lt;code&gt;average='weighted'&lt;/code&gt; )를 지원합니다.</target>
        </trans-unit>
        <trans-unit id="5cf025ac399ddc1c71c004c7cb8bd73171357561" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;shrinkage&lt;/code&gt; parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</source>
          <target state="translated">&lt;code&gt;shrinkage&lt;/code&gt; 파라미터 수동 0없이 수축 0에 대응하는 값 (경험적 공분산 행렬을 의미 사용되는) 특정 1. 완전 수축 한 대응 값 (사이에 설정 될 수있는 수단 대각 그 분산 행렬은 공분산 행렬의 추정치로 사용됩니다). 이 매개 변수를이 두 극값 사이의 값으로 설정하면 공분산 행렬의 축소 된 버전이 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="ac317fd98a77c706ac5d8a7e74a96d7bdbebfe03" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;3&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.covariance&lt;/code&gt; 의 패키지는 공분산의 강력한 추정, 최소 공분산 행렬식 구현 &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;(3)&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="b439b9c10b67475737ae3e151e90a6ed815edbd1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.covariance&lt;/code&gt; 의 패키지 공분산 견고한 추정기 최소 공분산 행렬식 구현한다 &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="6908af1748b7fe666814b53ca6e0a8cab66eb012" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package embeds some small toy datasets as introduced in the &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Getting Started&lt;/a&gt; section.</source>
          <target state="translated">&lt;code&gt;sklearn.datasets&lt;/code&gt; 의 패키지에 도입 된 일부 작은 장난감의 데이터 세트를 내장 &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;시작&lt;/a&gt; 절을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="82c9f1d0e48a88b08f4d85a858e0d70b67f55eec" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package is able to download datasets from the repository using the function &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.datasets&lt;/code&gt; 의 패키지 함수를 사용하여 저장소에서 다운로드 데이터 세트에 수 &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="8a80b4da04f92037fdd1ffd5771651becd3a649e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.preprocessing&lt;/code&gt; package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing&lt;/code&gt; 패키지는 몇 가지 일반적인 유틸리티 함수 변압기 클래스 하류 추정기에 더 적합한 표현으로 원시 특징 벡터를 변경합니다.</target>
        </trans-unit>
        <trans-unit id="9a666cbe94ae4d4ef285ecec57ff29087d1b1c92" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;stop_words_&lt;/code&gt; attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.</source>
          <target state="translated">&lt;code&gt;stop_words_&lt;/code&gt; 속성 이 커지고 모델 크기가 커질 수 있습니다. 이 속성은 검사 용으로 만 제공되며 delattr을 사용하여 안전하게 제거하거나 산 세척 전에 없음으로 설정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a2df492be59e0b5e94ab9ece47f7ab58734f8d4a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;svm.OneClassSVM&lt;/code&gt; is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.</source>
          <target state="translated">&lt;code&gt;svm.OneClassSVM&lt;/code&gt; 는 아웃 라이어 검출 잘 수행하지 않는 따라서 특이점에 민감한 것으로 공지되어있다. 이 추정기는 훈련 세트가 특이 치에 의해 오염되지 않은 경우 신규성 검출에 가장 적합합니다. 즉, 고차원 또는 특이 데이터 분포에 대한 가정이없는 이상 치를 탐지하는 것은 매우 어려운 일이며, 1 클래스 SVM은 하이퍼 파라미터의 값에 따라 이러한 상황에서 유용한 결과를 제공 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ae04908924f55e8ffa997283ff07a48e73585d0d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;tree_disp&lt;/code&gt; and &lt;code&gt;mlp_disp&lt;/code&gt;&lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt; objects contain all the computed information needed to recreate the partial dependence curves. This means we can easily create additional plots without needing to recompute the curves.</source>
          <target state="translated">&lt;code&gt;tree_disp&lt;/code&gt; 및 &lt;code&gt;mlp_disp&lt;/code&gt; &lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay&quot;&gt; &lt;code&gt;PartialDependenceDisplay&lt;/code&gt; 이&lt;/a&gt; 개체 부분 의존도 곡선을 재현하기 위해 필요한 모든 정보를 포함 계산. 즉, 곡선을 다시 계산할 필요없이 추가 플롯을 쉽게 만들 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a0172c625369e43fa830669101114953189c751e" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;kernel function&lt;/em&gt; can be any of the following:</source>
          <target state="translated">&lt;em&gt;커널 함수는&lt;/em&gt; 다음 중 하나가 될 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="9153a9bf6c588ab4d00c19f8aab755b4d4790f32" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;reachability&lt;/em&gt; distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining &lt;em&gt;reachability&lt;/em&gt; distances and data set &lt;code&gt;ordering_&lt;/code&gt; produces a &lt;em&gt;reachability plot&lt;/em&gt;, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. &amp;lsquo;Cutting&amp;rsquo; the reachability plot at a single value produces DBSCAN like results; all points above the &amp;lsquo;cut&amp;rsquo; are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter &lt;code&gt;xi&lt;/code&gt;. There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the &lt;code&gt;cluster_hierarchy_&lt;/code&gt; parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster.</source>
          <target state="translated">&lt;em&gt;도달 성&lt;/em&gt; OPTICS 의해 생성 거리가 하나의 데이터 세트 내의 클러스터의 밀도 변수 추출을 허용. 위의 플롯에서 볼 수 있듯이 &lt;em&gt;도달&lt;/em&gt; 거리와 데이터 세트 &lt;code&gt;ordering_&lt;/code&gt; 을 결합 하면 &lt;em&gt;도달 가능성 플롯이&lt;/em&gt; 생성됩니다.&lt;em&gt;&lt;/em&gt;, 여기서 점 밀도는 Y 축에 표시되고 점은 인접 점이 인접하도록 정렬됩니다. 단일 값에서 도달 가능성 플롯을 '절단'하면 DBSCAN과 유사한 결과가 생성됩니다. '컷'위의 모든 지점은 노이즈로 분류되며 왼쪽에서 오른쪽으로 읽을 때 휴식이있을 때마다 새로운 클러스터를 의미합니다. OPTICS를 사용한 기본 군집 추출은 그래프 내의 가파른 경사를보고 군집을 찾고 사용자는 매개 변수 &lt;code&gt;xi&lt;/code&gt; 를 사용하여 가파른 경사로 계산되는 것을 정의 할 수 있습니다 . 도달 가능성 플롯 덴드로 그램을 통해 데이터의 계층 적 표현을 생성하는 것과 같이 그래프 자체에 대한 분석을위한 다른 가능성도 있으며 알고리즘이 감지 한 클러스터의 계층은 &lt;code&gt;cluster_hierarchy_&lt;/code&gt; 를 통해 액세스 할 수 있습니다.매개 변수. 위의 플롯은 평면 공간의 클러스터 색상이 도달 가능성 플롯의 선형 세그먼트 클러스터와 일치하도록 색상으로 구분되었습니다. 파란색 및 빨간색 클러스터는 도달 가능성 플롯에서 인접하며 더 큰 상위 클러스터의 하위로 계층 적으로 표시 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6eff32d476fb25ebcdfdcb46623a8711aa972809" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;conditional entropy of clusters given class&lt;/strong&gt;\(H(K|C)\) and the &lt;strong&gt;entropy of clusters&lt;/strong&gt;\(H(K)\) are defined in a symmetric manner.</source>
          <target state="translated">&lt;strong&gt;클래스&lt;/strong&gt; \ (H (K | C) \)가 &lt;strong&gt;주어진 클러스터&lt;/strong&gt; 의 &lt;strong&gt;조건부 엔트로피와 클러스터&lt;/strong&gt; 의 &lt;strong&gt;엔트로피&lt;/strong&gt; \ (H (K) \)는 대칭 방식으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="5adbfb7d3b47e961b1e75927796552dd26ad41e4" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;exposure&lt;/strong&gt; is the duration of the insurance coverage of a given policy, in years.</source>
          <target state="translated">&lt;strong&gt;노출은&lt;/strong&gt; 몇 년 동안, 주어진 정책의 보험 기간입니다.</target>
        </trans-unit>
        <trans-unit id="a256791561f56278bffe5a6063bb1e7227590f9a" translate="yes" xml:space="preserve">
          <source>The AGE and EXPERIENCE coefficients are affected by strong variability which might be due to the collinearity between the 2 features: as AGE and EXPERIENCE vary together in the data, their effect is difficult to tease apart.</source>
          <target state="translated">AGE 및 EXPERIENCE 계수는 두 기능 간의 공선 성으로 인해 발생할 수있는 강한 변동성의 영향을받습니다. AGE 및 EXPERIENCE는 데이터에서 서로 다르기 때문에 그 효과를 구분하기가 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="e076b7d1d24895a169d091c6b0d7ae51431e1b5b" translate="yes" xml:space="preserve">
          <source>The AGE coefficient is expressed in &amp;ldquo;dollars/hour per living years&amp;rdquo; while the EDUCATION one is expressed in &amp;ldquo;dollars/hour per years of education&amp;rdquo;. This representation of the coefficients has the benefit of making clear the practical predictions of the model: an increase of \(1\) year in AGE means a decrease of \(0.030867\) dollars/hour, while an increase of \(1\) year in EDUCATION means an increase of \(0.054699\) dollars/hour. On the other hand, categorical variables (as UNION or SEX) are adimensional numbers taking either the value 0 or 1. Their coefficients are expressed in dollars/hour. Then, we cannot compare the magnitude of different coefficients since the features have different natural scales, and hence value ranges, because of their different unit of measure. This is more visible if we plot the coefficients.</source>
          <target state="translated">AGE 계수는 &quot;생활 년당 달러 / 시간&quot;으로 표현되고 교육 1은 &quot;교육 연수당 달러 / 시간&quot;으로 표현됩니다. 계수의 이러한 표현은 모델의 실제 예측을 명확하게하는 이점이 있습니다. AGE가 \ (1 \) 년 증가하면 시간당 \ (0.030867 \) 달러가 감소하고 \ (1 \)이 증가합니다. ) 교육에서의 연도는 시간당 \ (0.054699 \) 달러 증가를 의미합니다. 반면에 범주 형 변수 (UNION 또는 SEX)는 값 0 또는 1을 사용하는 무 차원 숫자입니다. 계수는 달러 / 시간으로 표시됩니다. 그런 다음 측정 단위가 다르기 때문에 특성의 자연 척도가 다르므로 값 범위가 다르기 때문에 다른 계수의 크기를 비교할 수 없습니다. 계수를 플로팅하면 더 잘 보입니다.</target>
        </trans-unit>
        <trans-unit id="2143ba51f2aad99e99e8cd60a62ccfdecbf0f265" translate="yes" xml:space="preserve">
          <source>The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative.</source>
          <target state="translated">AMI는 두 파티션이 동일하면 (즉, 완벽하게 일치하는) 1의 값을 반환합니다. 무작위 파티션 (독립적 인 레이블링)은 평균적으로 약 0의 예상 AMI를 가지므로 음수 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ad838931339007a1bda099c18480308bbb327339" translate="yes" xml:space="preserve">
          <source>The API is experimental (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">API는 실험적이며 (특히 반환 값 구조) 향후 릴리스에서 이전 버전과 호환되지 않는 약간의 변경 사항이있을 수 있습니다. ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ</target>
        </trans-unit>
        <trans-unit id="00e3722885bdadae5430c6fecaacfc1cced893f6" translate="yes" xml:space="preserve">
          <source>The API is experimental in version 0.20 (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">API는 버전 0.20 (특히 반환 값 구조)에서 실험 중이며 이후 릴리스에서 이전 버전과 호환되지 않는 작은 변경 사항이있을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e07138bd94d8d9dd92474342c51f134fa3423d8d" translate="yes" xml:space="preserve">
          <source>The Ames housing dataset is not shipped with scikit-learn and therefore we will fetch it from &lt;a href=&quot;https://www.openml.org/d/42165&quot;&gt;OpenML&lt;/a&gt;.</source>
          <target state="translated">Ames 하우징 데이터 세트는 scikit-learn과 함께 제공되지 않으므로 &lt;a href=&quot;https://www.openml.org/d/42165&quot;&gt;OpenML&lt;/a&gt; 에서 가져옵니다 .</target>
        </trans-unit>
        <trans-unit id="22556f00c2cdfc8c60673e1c663299619a64901c" translate="yes" xml:space="preserve">
          <source>The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a &lt;a href=&quot;#bgmm&quot;&gt;Variational Bayesian Gaussian mixture&lt;/a&gt; avoids the specification of the number of components for a Gaussian mixture model.</source>
          <target state="translated">BIC 기준을 사용하여 가우스 혼합의 성분 수를 효율적으로 선택할 수 있습니다. 이론적으로, 그것은 점근 정권에서만 (즉, 가용 한 많은 데이터가 있고 데이터가 실제로 가우시안 분포의 혼합물에서 iid로 생성되었다고 가정하는 경우)에만 실제 수의 구성 요소를 복구합니다. 사용합니다 &lt;a href=&quot;#bgmm&quot;&gt;변분 베이지안 가우시안 혼합하여&lt;/a&gt; 가우시안 혼합 모델에 대한 부품 수의 사양을 방지 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c912c462511f5921d016fed26135f626922ccc57" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.</source>
          <target state="translated">Barnes-Hut 구현은 목표 차원이 3 이하인 경우에만 작동합니다. 시각화를 구축 할 때는 2D 사례가 일반적입니다.</target>
        </trans-unit>
        <trans-unit id="49027a83447d6307ec530f0dd81bd5ad76360faa" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.</source>
          <target state="translated">Barnes-Hut t-SNE 방법은 2 차원 또는 3 차원 임베딩으로 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="f05f901deabcb1ee376f8697fc79932cbf746ce6" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is \(O[d N log(N)]\), where \(d\) is the number of output dimensions and \(N\) is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is \(O[d N^2]\), but has several other notable differences:</source>
          <target state="translated">여기서 구현 된 Barnes-Hut t-SNE는 일반적으로 다른 매니 폴드 학습 알고리즘보다 훨씬 느립니다. 최적화는 매우 어렵고 그래디언트 계산은 \ (O [d N log (N)] \)입니다. 여기서 \ (d \)는 출력 차원의 수이고 \ (N \)은 샘플 수입니다. Barnes-Hut 방법은 t-SNE 복잡성이 \ (O [d N ^ 2] \) 인 정확한 방법에서 개선되지만 몇 가지 다른 차이점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="e724094f6d5c410991717a9e2fc045d6798def45" translate="yes" xml:space="preserve">
          <source>The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.</source>
          <target state="translated">버치 알고리즘에는 임계 값과 분기 인자라는 두 가지 매개 변수가 있습니다. 분기 계수는 노드의 하위 클러스터 수를 제한하고 임계 값은 입력 샘플과 기존 하위 클러스터 사이의 거리를 제한합니다.</target>
        </trans-unit>
        <trans-unit id="8fa70be719c3475e8e012f73acc572a21c03cdd8" translate="yes" xml:space="preserve">
          <source>The Boston house-price data has been used in many machine learning papers that address regression problems.</source>
          <target state="translated">보스턴 주택 가격 데이터는 회귀 문제를 해결하는 많은 기계 학습 논문에서 사용되었습니다.</target>
        </trans-unit>
        <trans-unit id="455243540177422da87f9c3aa93de30456a2bae3" translate="yes" xml:space="preserve">
          <source>The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &amp;lsquo;Hedonic prices and the demand for clean air&amp;rsquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics &amp;hellip;&amp;rsquo;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.</source>
          <target state="translated">DL'Hedonic 가격과 청정 공기 수요 '인 Harrison, D. 및 Rubinfeld의 보스턴 주택 가격 데이터, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Belsley, Kuh &amp;amp; Welsch, 'Regression diagnostics&amp;hellip;', Wiley, 1980 년에 사용됨. NB 후자 244-261 페이지의 표에는 다양한 변형이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="7bc398b9797cca816dee96d9ed40f902ff743772" translate="yes" xml:space="preserve">
          <source>The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt;&lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">Bunch 객체는 키가 속성임을 노출하는 사전입니다. Bunch 객체에 대한 자세한 내용은 &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt; &lt;code&gt;sklearn.utils.Bunch&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0e6c056d908bee991f648998f41ff72e8a7301c2" translate="yes" xml:space="preserve">
          <source>The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:</source>
          <target state="translated">CF 서브 클러스터는 클러스터링에 필요한 정보를 보유하므로 전체 입력 데이터를 메모리에 보유 할 필요가 없습니다. 이 정보에는 다음이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="b64782d12e8b4e97de28df15435a193ab8a372e9" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Calinski-Harabasz 인덱스는 일반적으로 DBSCAN을 통해 얻은 것과 같은 밀도 기반 클러스터와 같은 클러스터의 다른 개념보다 볼록 클러스터에서 더 높습니다.</target>
        </trans-unit>
        <trans-unit id="b96c855c122e21633007587f3600e092339a5383" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Calinski-Harabaz 지수는 일반적으로 DBSCAN을 통해 얻은 밀도 기반 클러스터와 같은 다른 개념의 클러스터보다 볼록 클러스터에 대해 더 높습니다.</target>
        </trans-unit>
        <trans-unit id="6cdbc3a888667e8344981c2a8742122994627087" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al.</source>
          <target state="translated">Rennie et al.에 기술 된 보완 베이브 분류기.</target>
        </trans-unit>
        <trans-unit id="2b0b8b0a4dd5523203a6ee4b5d195c2e6a35c0fe" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al. (2003).</source>
          <target state="translated">Rennie et al.에 기술 된 보완 베이브 분류기. (2003).</target>
        </trans-unit>
        <trans-unit id="3398d87a3aceb1ea8375c52fcc6a67cec5c63df9" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier was designed to correct the &amp;ldquo;severe assumptions&amp;rdquo; made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.</source>
          <target state="translated">Complement Naive Bayes 분류기는 표준 Multinomial Naive Bayes 분류 기가 만든 &quot;심각한 가정&quot;을 수정하도록 설계되었습니다. 불균형 데이터 세트에 특히 적합합니다.</target>
        </trans-unit>
        <trans-unit id="7c176cfd9628fae51161daa03c193aeae01d49ea" translate="yes" xml:space="preserve">
          <source>The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \(k\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.</source>
          <target state="translated">Contrastive Divergence 방법은 적은 반복 횟수 (\ (k \), 일반적으로 1) 후에 체인을 중지하도록 제안합니다.</target>
        </trans-unit>
        <trans-unit id="1d09474bd461f8756796df6e9a77749a8489b6d3" translate="yes" xml:space="preserve">
          <source>The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than &lt;code&gt;eps&lt;/code&gt; to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than &lt;code&gt;eps&lt;/code&gt; from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering.</source>
          <target state="translated">DBSCAN 알고리즘은 결정 론적이며 동일한 순서로 동일한 데이터가 제공 될 때 항상 동일한 클러스터를 생성합니다. 그러나 데이터가 다른 순서로 제공되면 결과가 다를 수 있습니다. 첫째, 핵심 샘플이 항상 동일한 클러스터에 할당 되더라도 해당 클러스터의 레이블은 해당 샘플이 데이터에서 발견되는 순서에 따라 달라집니다. 두 번째로 중요한 것은 비 핵심 샘플이 할당 된 클러스터는 데이터 순서에 따라 다를 수 있습니다. 이것은 코어가 아닌 샘플이 다른 클러스터의 &lt;code&gt;eps&lt;/code&gt; 에서 2 개의 코어 샘플까지 의 거리를 가질 때 발생 합니다. 삼각 불평등에 의해,이 두 핵심 표본은 &lt;code&gt;eps&lt;/code&gt; 보다 먼 거리에 있어야합니다서로 또는 같은 클러스터에있을 것입니다. 코어가 아닌 샘플은 데이터를 통과 할 때 먼저 생성되는 클러스터에 할당되므로 결과는 데이터 순서에 따라 달라집니다.</target>
        </trans-unit>
        <trans-unit id="b5783662e991f62ab3cc63e2843b11c10be0af6e" translate="yes" xml:space="preserve">
          <source>The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN.</source>
          <target state="translated">Davies-Boulding 지수는 일반적으로 DBSCAN에서 얻은 것과 같은 밀도 기반 클러스터와 같은 다른 개념의 클러스터보다 볼록 클러스터에 대해 더 높습니다.</target>
        </trans-unit>
        <trans-unit id="9dae6187a2e4264ce3764c47d6ced56e22112bc3" translate="yes" xml:space="preserve">
          <source>The Digit Dataset</source>
          <target state="translated">숫자 데이터 세트</target>
        </trans-unit>
        <trans-unit id="15949e73904f01f7a29e0206a3232b388b3af43d" translate="yes" xml:space="preserve">
          <source>The Dirichlet process prior allows to define an infinite number of components and automatically selects the correct number of components: it activates a component only if it is necessary.</source>
          <target state="translated">Dirichlet 프로세스는 사전에 무한한 수의 구성 요소를 정의하고 올바른 수의 구성 요소를 자동으로 선택합니다. 필요한 경우에만 구성 요소를 활성화합니다.</target>
        </trans-unit>
        <trans-unit id="a257bf9309fabcccadc69c1bb14244b979e6f754" translate="yes" xml:space="preserve">
          <source>The Discounted Cumulative Gain divided by the Ideal Discounted Cumulative Gain (the DCG obtained for a perfect ranking), in order to have a score between 0 and 1.</source>
          <target state="translated">0과 1 사이의 점수를 얻기 위해 할인 된 누적 이익을 이상적인 할인 된 누적 이익 (완벽한 순위에 대해 얻은 DCG)으로 나눈 값입니다.</target>
        </trans-unit>
        <trans-unit id="d2e49448333a2cd92baf05825e927765f1835316" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is commonly combined with exponentiation.</source>
          <target state="translated">DotProduct 커널은 일반적으로 지수와 결합됩니다.</target>
        </trans-unit>
        <trans-unit id="842c0c72bca462c8ce3e9ff0cead821c33430a8a" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0^2. For sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">DotProduct 커널은 정지하지 않으며 x_d (d = 1,..., D)의 계수와 N (0, sigma_0 ^ 2 이전의 계수에 N (0, 1)을 두어 선형 회귀에서 얻을 수 있습니다. 편견에. DotProduct 커널은 원점에 대한 좌표 회전에는 변하지 않지만 번역은 아닙니다. sigma_0 ^ 2 매개 변수로 매개 변수화됩니다. sigma_0 ^ 2 = 0의 경우 커널을 동종 선형 커널이라고하며, 그렇지 않으면 동종이 아닙니다. 커널은</target>
        </trans-unit>
        <trans-unit id="078b4d2b566f931479e95c97a38f05103a2ecb7d" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0 \(\sigma\) which controls the inhomogenity of the kernel. For \(\sigma_0^2 =0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">DotProduct 커널은 고정적이지 않으며 \ (x_d (d = 1,..., D) \)의 계수에 \ (N (0, 1) \) 사전을 배치하여 선형 회귀에서 얻을 수 있습니다. 편향에 대한 \ (N (0, \ sigma_0 ^ 2) \)의. DotProduct 커널은 원점에 대한 좌표의 회전에 대해 변하지 않지만 변환이 아닙니다. 커널의 비균질성을 제어하는 ​​매개 변수 sigma_0 \ (\ sigma \)에 의해 매개 변수화됩니다. \ (\ sigma_0 ^ 2 = 0 \)의 경우 커널을 동종 선형 커널이라고하고 그렇지 않으면 동종이 아닙니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="c219b28ff4dd0afb8b865b96896a279315780153" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1 인 Elastic Net 믹싱 매개 변수 l1_ratio = 0은 L2 페널티, l1_ratio = 1 ~ L1에 해당합니다. 기본값은 0.15입니다.</target>
        </trans-unit>
        <trans-unit id="bc71ca7c36fa98be5da488c2c0295a9dca2a49c6" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if &lt;code&gt;penalty&lt;/code&gt; is &amp;lsquo;elasticnet&amp;rsquo;.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1 인 Elastic Net 혼합 매개 변수는 L2 패널티에 해당하고 l1_ratio = 1에서 L1에 해당합니다. &lt;code&gt;penalty&lt;/code&gt; 가 'elasticnet'인 경우에만 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="557cde32667038df92bd17ed0a660f8397aa9f15" translate="yes" xml:space="preserve">
          <source>The Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. Setting &lt;code&gt;l1_ratio=0&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while setting &lt;code&gt;l1_ratio=1&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">&lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; Elastic-Net 혼합 매개 변수 . &lt;code&gt;penalty='elasticnet'&lt;/code&gt; 경우에만 사용됩니다 . 설정 &lt;code&gt;l1_ratio=0&lt;/code&gt; 은 사용하는 것과 동일하다 &lt;code&gt;penalty='l2'&lt;/code&gt; 설정하는 동안, &lt;code&gt;l1_ratio=1&lt;/code&gt; 이 사용하는 것과 동일하다 &lt;code&gt;penalty='l1'&lt;/code&gt; . 들면 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt; 패널티는 L1과 L2의 조합이다.</target>
        </trans-unit>
        <trans-unit id="71317f65b5997839fb7f2d86b8c73dc4399ad254" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2.</source>
          <target state="translated">0 &amp;lt;l1_ratio &amp;lt;= 1 인 ElasticNet 믹싱 매개 변수 l1_ratio = 1 인 경우 페널티는 L1 / L2 페널티입니다. l1_ratio = 0의 경우 L2 페널티입니다. 들면 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 패널티는 L1 / L2와 L2의 조합이다.</target>
        </trans-unit>
        <trans-unit id="f913d5ab29d41b1d0b8510f8960f88320058e481" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in &lt;code&gt;[.1, .5, .7,
.9, .95, .99, 1]&lt;/code&gt;</source>
          <target state="translated">0 &amp;lt;l1_ratio &amp;lt;= 1 인 ElasticNet 믹싱 매개 변수 l1_ratio = 1 인 경우 페널티는 L1 / L2 페널티입니다. l1_ratio = 0의 경우 L2 페널티입니다. 들면 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 패널티는 L1 / L2와 L2의 조합이다. 이 매개 변수는 목록 일 수 있으며,이 경우 다른 값은 교차 유효성 검증에 의해 테스트되고 최상의 예측 점수를 제공하는 값이 사용됩니다. l1_ratio에 대한 값 목록의 올바른 선택은 &lt;code&gt;[.1, .5, .7, .9, .95, .99, 1]&lt;/code&gt; 에서와 같이 더 많은 값을 1 (예 : 올가미)에 가깝고 0 (예 : 릿지)에 두는 경우가 많습니다 . 95, .99, 1]</target>
        </trans-unit>
        <trans-unit id="62ded71b403044434993092527c854b8f53e5c17" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. For &lt;code&gt;l1_ratio = 0&lt;/code&gt; the penalty is an L2 penalty. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; it is an L1 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">&lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; ElasticNet 믹싱 파라미터 . 들면 &lt;code&gt;l1_ratio = 0&lt;/code&gt; 패널티 L2 패널티이다. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; 경우 L1 페널티입니다. 들면 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 패널티는 L1과 L2의 조합이다.</target>
        </trans-unit>
        <trans-unit id="706ede4c7ae02c53bd29139976a5c9ab95013fca" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a periodicity parameter periodicity&amp;gt;0. Only the isotropic variant where l is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">ExpSineSquared 커널은 주기적 기능 모델링을 허용합니다. 길이 스케일 매개 변수 length_scale&amp;gt; 0 및 주기성 매개 변수 주기성&amp;gt; 0으로 매개 변수화됩니다. l이 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="648ff6d48b0697b9c2efa5d7b7f9a5acf1d3efbe" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows one to model functions which repeat themselves exactly. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a periodicity parameter \(p&amp;gt;0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">ExpSineSquared 커널을 사용하면 정확하게 반복되는 함수를 모델링 할 수 있습니다. 길이 스케일 매개 변수 \ (l&amp;gt; 0 \) 및 주기성 매개 변수 \ (p&amp;gt; 0 \)로 매개 변수화됩니다. 현재는 \ (l \)이 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="31f2158684fd941b75a9b2f295ee3be83d4fb497" translate="yes" xml:space="preserve">
          <source>The Exponentiation kernel takes one base kernel and a scalar parameter \(p\) and combines them via</source>
          <target state="translated">지수화 커널은 하나의 기본 커널과 스칼라 매개 변수 \ (p \)를 취하여 다음을 통해 결합합니다.</target>
        </trans-unit>
        <trans-unit id="bd1aecbb19e2286cfafdebb8292f1071a3eb508c" translate="yes" xml:space="preserve">
          <source>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</source>
          <target state="translated">F- 베타 점수는 정밀도 및 리콜의 가중 고조파 평균으로 해석 될 수 있으며, F- 베타 점수는 1에서 최고 값에, 0에서 최악의 점수에 도달합니다.</target>
        </trans-unit>
        <trans-unit id="fcc8a075bfea5f42d0e38256200386182d33cc42" translate="yes" xml:space="preserve">
          <source>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</source>
          <target state="translated">F- 베타 점수는 가중 고조파 정밀도 및 리콜 평균으로 1에서 최적의 값에 도달하고 0에서 최악의 값에 도달합니다.</target>
        </trans-unit>
        <trans-unit id="553773b1d0f7903e424f857dbba13f4b2343a5a1" translate="yes" xml:space="preserve">
          <source>The F-beta score weights recall more than precision by a factor of &lt;code&gt;beta&lt;/code&gt;. &lt;code&gt;beta == 1.0&lt;/code&gt; means recall and precision are equally important.</source>
          <target state="translated">F- 베타 점수 가중치는 &lt;code&gt;beta&lt;/code&gt; 계수로 정밀도 이상의 것을 회상 합니다. &lt;code&gt;beta == 1.0&lt;/code&gt; 은 리콜 및 정밀도가 똑같이 중요하다는 것을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="3f53f44feaa854c0fdf5365ef5e43bc7fa440b01" translate="yes" xml:space="preserve">
          <source>The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:</source>
          <target state="translated">F1 점수는 정밀도 및 회수의 가중 평균으로 해석 될 수 있으며, 여기서 F1 점수는 1에서 최고 값에 도달하고 0에서 최악의 점수에 도달합니다. F1 점수에 대한 정밀도 및 회수의 상대적 기여는 동일합니다. F1 점수의 공식은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="518509d08bd98ece386f11a6583f35b4f559fdac" translate="yes" xml:space="preserve">
          <source>The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:</source>
          <target state="translated">아래 그림은 캘리포니아 주택 데이터 세트에 대한 단방향 및 양방향 부분 의존도 플롯 4 개를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="8d40f6c8f7d760d5a9c3769f72b85293905bcf21" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in a 2-dimensional parameter space (\(m=2\)) when \(R(w) = 1\).</source>
          <target state="translated">아래 그림은 \ (R (w) = 1 \) 일 때 2 차원 매개 변수 공간 (\ (m = 2 \))에서 다른 정규화 항의 윤곽선을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="2280ec0b9d5ba5eef024fd7f6041defdab8c9d48" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in the parameter space when \(R(w) = 1\).</source>
          <target state="translated">아래 그림은 \ (R (w) = 1 \) 인 경우 매개 변수 공간에서 다른 정규화 항의 윤곽을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="af21e4c771514d1aca6cd1c14dcff1ce67a0e477" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt;&lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt;&lt;/a&gt;) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:</source>
          <target state="translated">Fowlkes-Mallows 지수 ( &lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt; &lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt; &lt;/a&gt; )는 샘플의 기본 진리 클래스 할당이 알려진 경우 사용할 수 있습니다. Fowlkes-Mallows 점수 FMI는 쌍별 정밀도의 기하학적 평균으로 정의되며 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="bfdc8ed29be1f90cbfb4dc9576ce17a736f93294" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall:</source>
          <target state="translated">Fowlkes-Mallows 지수 (FMI)는 정밀도와 회수 사이의 기하학적 평균으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="fb7ab21b6034e9b9d6397353d40d09f0a02e126e" translate="yes" xml:space="preserve">
          <source>The French Motor Third-Party Liability Claims dataset</source>
          <target state="translated">French Motor Third-Party Liability Claims 데이터 세트</target>
        </trans-unit>
        <trans-unit id="90c804ef552ac504772d87a0c90e2454ea50931f" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">GP 사전 평균은 0으로 가정합니다. 사전의 공분산은 &lt;a href=&quot;#gp-kernels&quot;&gt;커널&lt;/a&gt; 객체를 전달하여 지정됩니다 . 커널의 하이퍼 파라미터는 전달 &lt;code&gt;optimizer&lt;/code&gt; 기반으로 LLM (log-marginal-likelihood)을 최대화하여 GaussianProcessRegressor를 피팅하는 동안 최적화 됩니다. LML에 여러 개의 로컬 옵티마가있을 수 있으므로 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; 를 지정하여 옵티 마이저를 반복적으로 시작할 수 있습니다 . 첫 번째 실행은 항상 커널의 초기 하이퍼 파라미터 값에서 시작하여 수행됩니다. 후속 실행은 허용 된 값의 범위에서 무작위로 선택된 하이퍼 파라미터 값에서 수행됩니다. 초기 하이퍼 파라미터를 고정 상태로 유지해야하는 경우 최적화 프로그램으로 &lt;code&gt;None&lt;/code&gt; 을 전달할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="060746131ee7579fe50d723eccf0216e72cc0775" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">GP 사전 평균은 0으로 가정합니다. 사전의 공분산은 &lt;a href=&quot;#gp-kernels&quot;&gt;커널&lt;/a&gt; 객체 를 전달하여 지정됩니다 . 커널의 하이퍼 파라미터는 전달 &lt;code&gt;optimizer&lt;/code&gt; 기반으로 로그 한계 가능성 (LML)을 최대화하여 GaussianProcessRegressor 피팅 중에 최적화 됩니다. LML에 여러 로컬 옵티마가있을 수 있으므로 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; 를 지정하여 옵티 마이저를 반복적으로 시작할 수 있습니다 . 첫 번째 실행은 항상 커널의 초기 하이퍼 파라미터 값에서 시작하여 수행됩니다. 후속 실행은 허용 된 값 범위에서 무작위로 선택된 하이퍼 파라미터 값에서 수행됩니다. 초기 하이퍼 파라미터를 고정해야하는 경우 &lt;code&gt;None&lt;/code&gt; 을 옵티 마이저로 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d9ff401cc389e116b429c312d580f50e1ed79479" translate="yes" xml:space="preserve">
          <source>The Gini coefficient (based on the area under the curve) can be used as a model selection metric to quantify the ability of the model to rank policyholders. Note that this metric does not reflect the ability of the models to make accurate predictions in terms of absolute value of total claim amounts but only in terms of relative amounts as a ranking metric.</source>
          <target state="translated">Gini 계수 (곡선 아래 영역을 기반으로 함)는 모델 선택 메트릭으로 사용되어 보험 계약자의 순위를 매기는 모델의 능력을 정량화 할 수 있습니다. 이 메트릭은 총 청구 금액의 절대 값 측면에서 정확한 예측을 수행하는 모델의 능력을 반영하지 않고 순위 메트릭으로서의 상대적인 금액 측면에서만 반영합니다.</target>
        </trans-unit>
        <trans-unit id="18dcaaaea80b191d9100a9a4ae22f95aa636616a" translate="yes" xml:space="preserve">
          <source>The Gini index reflects the ability of a model to rank predictions irrespective of their absolute values, and therefore only assess their ranking power.</source>
          <target state="translated">Gini 지수는 절대 값에 관계없이 예측 순위를 매기는 모델의 능력을 반영하므로 순위 파워 만 평가합니다.</target>
        </trans-unit>
        <trans-unit id="01a974069deaad9f4a696951139fa6f180c4610d" translate="yes" xml:space="preserve">
          <source>The HLLE algorithm comprises three stages:</source>
          <target state="translated">HLLE 알고리즘은 3 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="eaec5120030b979edc1dcb8e844874e0db8c76f1" translate="yes" xml:space="preserve">
          <source>The Hamming loss is the fraction of labels that are incorrectly predicted.</source>
          <target state="translated">해밍 손실은 잘못 예측 된 레이블의 일부입니다.</target>
        </trans-unit>
        <trans-unit id="304ac7ce83417558cbe44e7723fbf3a93a036677" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss, when &lt;code&gt;normalize&lt;/code&gt; parameter is set to True. It is always between 0 and 1, lower being better.</source>
          <target state="translated">해밍 손실은 &lt;code&gt;normalize&lt;/code&gt; 매개 변수가 True로 설정된 경우 서브 세트 제로 1 손실에 의해 상한이됩니다 . 항상 0과 1 사이이며 낮을수록 좋습니다.</target>
        </trans-unit>
        <trans-unit id="ee89b1587a7c7b8d189cccf628a41e55c5d7ebe4" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1.</source>
          <target state="translated">해밍 손실은 부분적인 제로원 손실에 의해 상한이됩니다. 샘플에 대해 정규화 할 때 해밍 손실은 항상 0과 1 사이입니다.</target>
        </trans-unit>
        <trans-unit id="e3001f46869621e71f2b9c72a2ca6b3e38966a32" translate="yes" xml:space="preserve">
          <source>The Haversine (or great circle) distance is the angular distance between two points on the surface of a sphere. The first distance of each point is assumed to be the latitude, the second is the longitude, given in radians. The dimension of the data must be 2.</source>
          <target state="translated">Haversine (또는 대원) 거리는 구 표면에있는 두 점 사이의 각도 거리입니다. 각 지점의 첫 번째 거리는 위도, 두 번째는 라디안 단위의 경도로 간주됩니다. 데이터의 차원은 2 여야합니다.</target>
        </trans-unit>
        <trans-unit id="2f575d5541e123fdf35ce0dd5c6fb4373f14dde3" translate="yes" xml:space="preserve">
          <source>The Huber Regressor optimizes the squared loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; and the absolute loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt;, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.</source>
          <target state="translated">Huber Regressor는 &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; 샘플에서 제곱 손실을 최적화합니다. &amp;lt;엡실론 및 샘플에 대한 절대 손실 &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt; , 여기서 w 및 sigma는 최적화 할 매개 변수입니다. 매개 변수 sigma는 y가 특정 요소에 의해 확대 또는 축소되는 경우 동일한 견고성을 달성하기 위해 엡실론의 크기를 조정할 필요가 없도록합니다. 이것은 X의 다른 기능이 다른 스케일 일 수 있다는 사실을 고려하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a428acf86562c566ebfc0a4a1d3e42c700a1a75f" translate="yes" xml:space="preserve">
          <source>The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter &lt;code&gt;epsilon&lt;/code&gt;. This parameter depends on the scale of the target variables.</source>
          <target state="translated">후버 및 엡실론에 둔감 한 손실 함수를 사용하여 강력한 회귀 분석을 수행 할 수 있습니다. 민감하지 않은 영역의 너비는 &lt;code&gt;epsilon&lt;/code&gt; 매개 변수를 통해 지정해야합니다 . 이 매개 변수는 대상 변수의 척도에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="0a2150cee6da11610a14f68e745e5d0639a39459" translate="yes" xml:space="preserve">
          <source>The Iris Dataset</source>
          <target state="translated">아이리스 데이터 셋</target>
        </trans-unit>
        <trans-unit id="7c0ef25371cb6aecb434ff0290ad88905a1c2367" translate="yes" xml:space="preserve">
          <source>The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.</source>
          <target state="translated">홍채 데이터 세트는 꽃받침 길이, 꽃받침 너비, 꽃잎 길이 및 꽃잎 너비의 4 가지 속성을 가진 3 가지 종류의 홍채 꽃 (Setosa, Versicolour 및 Virginica)을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="3877561d1fbee6aff8708ef4ec5fcabe6115833e" translate="yes" xml:space="preserve">
          <source>The IsolationForest &amp;lsquo;isolates&amp;rsquo; observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</source>
          <target state="translated">IsolationForest는 피처를 임의로 선택한 다음 선택한 피처의 최대 값과 최소값 사이의 분할 값을 임의로 선택하여 관찰을 '격리'합니다.</target>
        </trans-unit>
        <trans-unit id="9e47d533732129c7b308a9673031913c04e17afd" translate="yes" xml:space="preserve">
          <source>The Isomap algorithm comprises three stages:</source>
          <target state="translated">Isomap 알고리즘은 세 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="43f0627e56441244b0d44b83d4abbf8510d9eac0" translate="yes" xml:space="preserve">
          <source>The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">교차점의 크기를 두 레이블 집합의 합집합 크기로 나눈 것으로 정의 된 Jaccard 색인 [1] 또는 Jaccard 유사성 계수는 &lt;code&gt;y_true&lt;/code&gt; 의 샘플에 대해 예측 레이블 집합을 해당 레이블 집합과 비교하는 데 사용됩니다. .</target>
        </trans-unit>
        <trans-unit id="54b0fb2f21ad6f516f16dd3562af3d3b867660fe" translate="yes" xml:space="preserve">
          <source>The Jaccard similarity coefficient of the \(i\)-th samples, with a ground truth label set \(y_i\) and predicted label set \(\hat{y}_i\), is defined as</source>
          <target state="translated">지면 진실 레이블 세트 \ (y_i \) 및 예측 레이블 세트 \ (\ hat {y} _i \)를 가진 \ (i \) 번째 샘플의 Jaccard 유사성 계수는 ​​다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="1b6ab133b67b3354e1eaee98fb0609deafb4a4e0" translate="yes" xml:space="preserve">
          <source>The Johnson-Lindenstrauss bound for embedding with random projections</source>
          <target state="translated">임의 투영법으로 임베딩 할 수있는 Johnson-Lindenstrauss</target>
        </trans-unit>
        <trans-unit id="e9607194f44934f99bc18e8a1d649dcabdd2e40a" translate="yes" xml:space="preserve">
          <source>The K-means algorithm aims to choose centroids that minimise the &lt;strong&gt;inertia&lt;/strong&gt;, or &lt;strong&gt;within-cluster sum-of-squares criterion&lt;/strong&gt;:</source>
          <target state="translated">K- 평균 알고리즘은 &lt;strong&gt;관성&lt;/strong&gt; 을 최소화하는 중심 또는 &lt;strong&gt;클러스터 내 제곱합 기준&lt;/strong&gt; 을 선택하는 것을 목표로합니다 .</target>
        </trans-unit>
        <trans-unit id="2e420c9d276fc6531e2d5a1fd3367ac4465f247c" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">KDD Cup '99 데이터 세트는 MIT Lincoln Lab [1]이 만든 1998 DARPA 침입 탐지 시스템 (IDS) 평가 데이터 세트의 tcpdump 부분을 처리하여 생성되었습니다. 인공 데이터 ( &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;데이터 세트 홈페이지&lt;/a&gt; 에 설명되어 있음 )는 폐쇄 된 네트워크와 손으로 주입 된 공격을 사용하여 생성되어 백그라운드에서 정상적인 활동으로 여러 유형의 공격을 생성했습니다. 초기 목표는 감독 학습 알고리즘에 대한 대규모 훈련 세트를 생성하는 것이기 때문에 실제 세계에서는 비현실적이며 비정상적인 데이터 탐지를 목표로하는 감독되지 않은 이상 탐지에는 부적합한 비정상 데이터의 비율 (80.1 %)이 많으며, 즉</target>
        </trans-unit>
        <trans-unit id="c580c78b1750c851ebf65b6b32d643554e25c44f" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">KDD Cup '99 데이터 세트는 MIT Lincoln Lab [1]에서 만든 1998 DARPA IDS (Intrusion Detection System) 평가 데이터 세트의 tcpdump 부분을 처리하여 생성되었습니다. 인공 데이터 ( &lt;a href=&quot;https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;데이터 세트의 홈페이지&lt;/a&gt; 에 설명 됨 )는 백그라운드에서 정상적인 활동으로 다양한 유형의 공격을 생성하기 위해 폐쇄 된 네트워크 및 수동 주입 공격을 사용하여 생성되었습니다. 초기 목표는지도 학습 알고리즘을위한 대규모 학습 세트를 생성하는 것이었기 때문에 실제 세계에서는 비현실적이며 '비정상'데이터를 탐지하는 것을 목표로하는 감독되지 않은 이상 탐지에 부적합한 비정상 데이터가 많은 비율 (80.1 %)로 존재합니다. 즉</target>
        </trans-unit>
        <trans-unit id="42488af2a88df53e7b3301c7d5a125afdc9cd2ce" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.</source>
          <target state="translated">원래 공간과 포함 된 공간에서 관절 확률의 KL (Kullback-Leibler) 발산은 경사 하강에 의해 최소화됩니다. KL 분기는 볼록하지 않습니다. 즉, 초기화가 다른 여러 번 다시 시작하면 KL 분기의 로컬 최소값이됩니다. 따라서 다른 시드를 시도하고 KL 분기가 가장 낮은 임베딩을 선택하는 것이 유용한 경우가 있습니다.</target>
        </trans-unit>
        <trans-unit id="c1f022c5d8701b9b00fce305ae262139841dede3" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use 0 for no regularization.</source>
          <target state="translated">L2 정규화 매개 변수입니다. 정규화하지 않으려면 0을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="5b439efc6a95c460b62f48e9b2ed58ff5ae6d780" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use &lt;code&gt;0&lt;/code&gt; for no regularization (default).</source>
          <target state="translated">L2 정규화 매개 변수입니다. 정규화하지 않으려면 &lt;code&gt;0&lt;/code&gt; 을 사용합니다 (기본값).</target>
        </trans-unit>
        <trans-unit id="9a35ab88c062292779ce626a30b50f45159613b2" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">LARS 모델은 추정기 &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt; 또는 하위 수준 구현 &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt; &lt;code&gt;lars_path_gram&lt;/code&gt; 을&lt;/a&gt; 사용하여 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5c5e824f9a2e5664a81c2abf8d8de696419cc018" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">LARS 모델은 추정기 &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt; 또는 저수준 구현 &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; 를&lt;/a&gt; 사용하여 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5604e6549ba6e538c77a361942c228433b92a65e" translate="yes" xml:space="preserve">
          <source>The LTSA algorithm comprises three stages:</source>
          <target state="translated">LTSA 알고리즘은 3 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="5921a49032d4468242b223b9e118c35472eff0fa" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation consist of retrieving the path with function &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Lars 알고리즘은 정규화 매개 변수를 따라 계수의 전체 경로를 거의 무료로 제공하므로 일반적인 작업은 &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 경로를 검색하는 것으로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="066ac8e9a006935363e41b727fa078b6cbe4a689" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation is to retrieve the path with one of the functions &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Lars 알고리즘은 정규화 매개 변수를 따라 계수의 전체 경로를 거의 무료로 제공하므로 일반적인 작업은 &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt; &lt;code&gt;lars_path_gram&lt;/code&gt; &lt;/a&gt; 함수 중 하나를 사용하여 경로를 검색하는 것입니다 .</target>
        </trans-unit>
        <trans-unit id="9503ea7db44738c355b5bdf4d0264864cc9e2161" translate="yes" xml:space="preserve">
          <source>The Lasso is a linear model that estimates sparse coefficients with l1 regularization.</source>
          <target state="translated">Lasso는 l1 정규화로 희소 계수를 추정하는 선형 모델입니다.</target>
        </trans-unit>
        <trans-unit id="0f5aa687d48724082c940128def6bdb3f6066405" translate="yes" xml:space="preserve">
          <source>The Lasso optimization function varies for mono and multi-outputs.</source>
          <target state="translated">올가미 최적화 기능은 모노 및 멀티 출력에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="3c1b2c3b1551762e6f1da2d8a29e9acf6404c123" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">사용할 올가미 솔버 : 좌표 하강 또는 LARS. 피처 수가 샘플 수보다 큰 희소 기본 그래프에는 LARS를 사용하십시오. 다른 곳에서는 더 수치 적으로 안정적인 cd를 선호합니다.</target>
        </trans-unit>
        <trans-unit id="2de8b3228aef7c3b031e79cb56d858268d66f64e" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p &amp;gt; n. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">사용할 올가미 솔버 : 좌표 하강 또는 LARS. p&amp;gt; n 인 매우 희박한 기본 그래프에는 LARS를 사용하십시오. 다른 곳에서는 더 수치 적으로 안정적인 cd를 선호합니다.</target>
        </trans-unit>
        <trans-unit id="59ce670bca31c79340517b84d85b37c8ca4886a2" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">공분산 행렬의 Ledoit-Wolf 추정기 는 &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; &lt;/a&gt; 패키지 의 &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 샘플에서 계산 하거나 &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt; 객체를 동일한 샘플 에 피팅하여 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6c4c48b93f6a66f7991382ba54fcfe5fb18a6ebb" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">공분산 행렬의 Ledoit-Wolf 추정기 는 &lt;code&gt;sklearn.covariance&lt;/code&gt; 패키지 의 &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 샘플에서 계산 하거나 &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt; 객체를 동일한 샘플 에 피팅하여 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="aec2f6a4d0fb4e3ecc2ba56ea33e122b851a535b" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset constains two small dataset:</source>
          <target state="translated">Linnerud 데이터 세트는 두 개의 작은 데이터 세트를 구성합니다.</target>
        </trans-unit>
        <trans-unit id="6be9aee52b8392e35ed4aeda9bdbad80150f0295" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club:</source>
          <target state="translated">Linnerud 데이터 세트는 다중 출력 회귀 데이터 세트입니다. 피트니스 클럽의 중년 남성 20 명으로부터 수집 된 세 가지 운동 (데이터)과 세 가지 생리적 (목표) 변수로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="0460202c91e1eee28482597354a8366af4e5eb02" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for novelty detection. Note that when LOF is used for novelty detection you MUST not use predict, decision_function and score_samples on the training set as this would lead to wrong results. You must only use these methods on new unseen data (which are not in the training set). See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for outlier detection.</source>
          <target state="translated">LOF (Local Outlier Factor) 알고리즘은 감독되지 않은 이상 탐지 방법으로, 주변에 대한 주어진 데이터 포인트의 로컬 밀도 편차를 계산합니다. 이웃보다 밀도가 실질적으로 낮은 샘플을 특이 치로 간주합니다. 이 예는 참신 탐지에 LOF를 사용하는 방법을 보여줍니다. LOF가 참신 탐지에 사용되는 경우 훈련 결과에 대해 predict, decision_function 및 score_samples를 사용해서는 안됩니다. 결과가 잘못 될 수 있습니다. 보이지 않는 새로운 데이터 (훈련 세트에없는 데이터)에만 이러한 방법을 사용해야합니다. 이상치 탐지와 신규성 탐지의 차이점 및 이상치 탐지에 LOF를 사용하는 방법에 대한 자세한 내용 은 &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;사용자 안내서&lt;/a&gt; :를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="d0e3831e71a419f2c6e3df80ad462189c4208703" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection.</source>
          <target state="translated">LOF (Local Outlier Factor) 알고리즘은 감독되지 않은 이상 탐지 방법으로, 주변에 대한 주어진 데이터 포인트의 로컬 밀도 편차를 계산합니다. 이웃보다 밀도가 실질적으로 낮은 샘플을 특이 치로 간주합니다. 이 예는 scikit-learn에서이 추정기의 기본 사용 사례 인 이상치 탐지에 LOF를 사용하는 방법을 보여줍니다. LOF가 이상치 탐지에 사용될 때는 predict, decision_function 및 score_samples 메소드가 없습니다. 특이 치 탐지와 참신 탐지의 차이점과 참신 탐지에 LOF를 사용하는 방법에 대한 자세한 내용은 사용 &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;설명서를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e334ea68d0fb9bc258e189ffac7524671f8600d5" translate="yes" xml:space="preserve">
          <source>The MLLE algorithm comprises three stages:</source>
          <target state="translated">MLLE 알고리즘은 3 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="514baa8efb372e193dd4209109d33c4d6c755f88" translate="yes" xml:space="preserve">
          <source>The Matplotlib Figure object.</source>
          <target state="translated">Matplotlib Figure 객체입니다.</target>
        </trans-unit>
        <trans-unit id="5adf2471ff49dbf27acc1a8b4862b530e5b52366" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).</source>
          <target state="translated">Matthews 상관 계수 (+1은 완벽한 예측, 0은 평균 임의 예측, -1 및 역 예측)를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="21efcbf7188af02e8a17818fd892c631c3cc436c" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. [source: Wikipedia]</source>
          <target state="translated">Matthews 상관 계수는 이진 및 멀티 클래스 분류의 품질 측정으로 기계 학습에 사용됩니다. 그것은 참과 거짓 긍정과 부정을 고려하고 일반적으로 클래스의 크기가 매우 다른 경우에도 사용할 수있는 균형 잡힌 측정으로 간주됩니다. MCC는 본질적으로 -1과 +1 사이의 상관 계수 값입니다. +1의 계수는 완전 예측, 0은 평균 랜덤 예측, -1은 역 예측을 나타냅니다. 통계량은 파이 계수라고도합니다. [출처 : Wikipedia]</target>
        </trans-unit>
        <trans-unit id="6faebd1cede47746805364be40ce28b059dea40d" translate="yes" xml:space="preserve">
          <source>The Mean Squared Error (in the sense of the Frobenius norm) between &lt;code&gt;self&lt;/code&gt; and &lt;code&gt;comp_cov&lt;/code&gt; covariance estimators.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 및 &lt;code&gt;comp_cov&lt;/code&gt; 공분산 추정기 간의 평균 제곱 오차 (Frobenius 표준의 의미에서) .</target>
        </trans-unit>
        <trans-unit id="d3bca24cc7fb644916020d52b90a1ddb7136c5be" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.</source>
          <target state="translated">최소 공분산 결정 공분산 추정값은 가우스 분포 데이터에 적용되지만 단봉 대칭 대칭 분포에서 가져온 데이터와 관련이있을 수 있습니다. 다중 모달 데이터와 함께 사용하기위한 것이 아닙니다 (이 경우 MinCovDet 객체에 맞는 알고리즘이 실패 할 수 있음). 멀티 모달 데이터 셋을 다루기위한 프로젝션 추구 방법을 고려해야합니다.</target>
        </trans-unit>
        <trans-unit id="451133ac20cdcd95892f17968e1c348a4b0f4b8f" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">PJRousseuw는 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 에서 최소 공분산 결정 인자 (MCD)를 도입했습니다 .</target>
        </trans-unit>
        <trans-unit id="e452597962db33c03eccea44c483adb03cd8dfa7" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;3&lt;/a&gt;.</source>
          <target state="translated">MCD (Minimum Covariance Determinant estimator)는 &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;3&lt;/a&gt; 에서 PJRousseuw에 의해 도입되었습니다 .</target>
        </trans-unit>
        <trans-unit id="d811e159b509e69af06fa8b1a6b421d8a9ea40ca" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in [1].</source>
          <target state="translated">PJRousseuw는 [1]에서 최소 공분산 결정 인자 (MCD)를 도입했습니다.</target>
        </trans-unit>
        <trans-unit id="ecfb18631be5aab9ee26a60642b369e5e4e48a3b" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;3&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">최소 공분산 결정 추정치는 PJ Rousseeuw가 &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;3&lt;/a&gt; 에서 도입 한 데이터 세트의 공분산에 대한 강력한 추정기입니다 . 아이디어는 특이 치가 아닌 &quot;좋은&quot;관측치의 주어진 비율 (h)을 찾고 경험적 공분산 행렬을 계산하는 것입니다. 이 경험적 공분산 행렬은 수행 된 관측치 선택을 보상하기 위해 재조정됩니다 ( &quot;일관성 단계&quot;). 최소 공분산 결정 추정기를 계산하면 Mahalanobis 거리에 따라 관측치에 가중치를 부여하여 데이터 세트의 공분산 행렬을 재가 중 추정 할 수 있습니다 (&amp;ldquo;재가 중 단계&amp;rdquo;).</target>
        </trans-unit>
        <trans-unit id="92f2af418b12a7a58727045874e736340d33efbd" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">최소 공분산 결정 인자 추정기는 &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]의&lt;/a&gt; PJ Rousseeuw에 의해 도입 된 데이터 세트의 공분산에 대한 강력한 추정기입니다 . 아이디어는 특이 치가 아닌 &quot;좋은&quot;관측치의 주어진 비율 (h)을 찾고 경험적 공분산 행렬을 계산하는 것입니다. 이 경험적 공분산 행렬은 수행 된 관측의 선택을 보상하기 위해 재조정된다 ( &quot;일관성 단계&quot;). 최소 공분산 결정 요인 추정값을 계산 한 후, Mahalanobis 거리에 따라 관측치에 가중치를 부여하여 데이터 세트의 공분산 행렬의 가중치를 재 추정 할 수 있습니다 ( &quot;가중치 단계&quot;).</target>
        </trans-unit>
        <trans-unit id="26a5ec87ffc45e2d236a64ca891cc8c307910b23" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples} - n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples} + n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance. After a correction step aiming at compensating the fact that the estimates were learned from only a portion of the initial data, we end up with robust estimates of the data set location and covariance.</source>
          <target state="translated">최소 공분산 결정 인자 추정값은 강력하고 높은 고 장점입니다 (즉, 최대 \ (\ frac {n_ \ text {samples}-n_ \ text {features}-). 1} {2} \) 특이 치) 공분산 추정값. 아이디어의 경험적 공분산이 가장 작은 결정 인 \ (\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \) 관측 값을 찾아서 &quot;순수한&quot;관측 값의 하위 집합을 생성하는 것입니다. 위치 및 공분산의 표준 추정치를 계산합니다. 초기 데이터의 일부에서만 추정값을 학습했다는 사실을 보상하기 위해 수정 단계를 수행 한 후 데이터 세트 위치 및 공분산에 대한 강력한 추정치가됩니다.</target>
        </trans-unit>
        <trans-unit id="868c6b821132444289ad5893f70525225594997c" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples}-n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples}+n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance.</source>
          <target state="translated">최소 공분산 결정 인자 추정값은 강력하고 높은 고 장점입니다 (즉, \ (\ frac {n_ \ text {samples} -n_ \ text {features}-)까지 오염도가 높은 데이터 세트의 공분산 행렬을 추정하는 데 사용할 수 있습니다. 1} {2} \) 특이 치) 공분산 추정값. 아이디어의 경험적 공분산이 가장 작은 결정 인 \ (\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \) 관측 값을 찾아서 &quot;순수한&quot;관측 값의 하위 집합을 생성하는 것입니다. 위치 및 공분산의 표준 추정치를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="b2ce758bd900de6631654a32b9b8962161ffb45f" translate="yes" xml:space="preserve">
          <source>The Mutual Information is a measure of the similarity between two labels of the same data. Where \(|U_i|\) is the number of the samples in cluster \(U_i\) and \(|V_j|\) is the number of the samples in cluster \(V_j\), the Mutual Information between clusterings \(U\) and \(V\) is given as:</source>
          <target state="translated">상호 정보는 동일한 데이터의 두 레이블 간의 유사성을 측정 한 것입니다. 여기서 \ (| U_i | \)는 클러스터 \ (U_i \)의 샘플 수이고 \ (| V_j | \)는 클러스터 \ (V_j \)의 샘플 수이며, 클러스터링 간의 상호 정보 \ ( U \) 및 \ (V \)는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4607a569600ef039f2071e80730b55efccd8dd8d" translate="yes" xml:space="preserve">
          <source>The Nystroem method, as implemented in &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; uses the &lt;code&gt;rbf&lt;/code&gt; kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter &lt;code&gt;n_components&lt;/code&gt;.</source>
          <target state="translated">Nystroem에서 구현 된 &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; 방법은 커널의 낮은 순위 근사를위한 일반적인 방법입니다. 커널이 평가되는 데이터를 본질적으로 서브 샘플링하여이를 달성합니다. 기본적으로 &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; 은 &lt;code&gt;rbf&lt;/code&gt; 커널을 사용하지만 모든 커널 기능이나 사전 계산 된 커널 매트릭스를 사용할 수 있습니다. 사용 된 샘플의 수는 계산 된 형상의 차원이기도하며 매개 변수 &lt;code&gt;n_components&lt;/code&gt; 로 제공 됩니다.</target>
        </trans-unit>
        <trans-unit id="acd97cd02b3cf6e9137b2a9dda0c8e62c31c52e4" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">공분산 행렬의 OAS 추정기는 &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; &lt;/a&gt; 패키지 의 &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 샘플에서 계산 하거나 &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt; 객체를 동일한 샘플 에 피팅하여 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ad47de32445041d6ef99f77621d867a03cd9751f" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">공분산 행렬의 OAS 추정기는 &lt;code&gt;sklearn.covariance&lt;/code&gt; 패키지 의 &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 샘플에서 계산 하거나 &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt; 객체를 동일한 샘플 에 피팅하여 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6f45a4e82bc4c1b489f40318ecca4028acd0f46e" translate="yes" xml:space="preserve">
          <source>The One-Class SVM has been introduced by Sch&amp;ouml;lkopf et al. for that purpose and implemented in the &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; module in the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The \(\nu\) parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.</source>
          <target state="translated">One-Class SVM은 Sch&amp;ouml;lkopf et al. 이를 위해 &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt; 객체 의 &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; 모듈에서 구현 합니다. 프론티어를 정의하려면 커널과 스칼라 매개 변수를 선택해야합니다. RBF 커널은 대역폭 매개 변수를 설정하기위한 정확한 공식이나 알고리즘이 없지만 일반적으로 선택됩니다. 이것이 scikit-learn 구현의 기본값입니다. One-Class SVM의 마진이라고도하는 \ (\ nu \) 매개 변수는 프론티어 외부에서 새롭지 만 규칙적인 관찰을 찾을 가능성에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="ce00ae4f245475e7026810e89464783863843edd" translate="yes" xml:space="preserve">
          <source>The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.</source>
          <target state="translated">로지스틱 회귀는 예측을 수행하는 반면 PCA는 감독되지 않은 차원 축소를 수행합니다.</target>
        </trans-unit>
        <trans-unit id="04f0f7d7b53f41cd954be6291735d195a21f52b7" translate="yes" xml:space="preserve">
          <source>The Poisson deviance cannot be computed on non-positive values predicted by the model. For models that do return a few non-positive predictions (e.g. &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;) we ignore the corresponding samples, meaning that the obtained Poisson deviance is approximate. An alternative approach could be to use &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt;&lt;code&gt;TransformedTargetRegressor&lt;/code&gt;&lt;/a&gt; meta-estimator to map &lt;code&gt;y_pred&lt;/code&gt; to a strictly positive domain.</source>
          <target state="translated">포아송 이탈도는 모델에서 예측 한 양수가 아닌 값에서 계산할 수 없습니다. 몇 가지 비 양성 예측 (예 : &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; )을 반환하는 모델의 경우 해당 샘플을 무시합니다. 즉, 얻은 포아송 편차가 근사치임을 의미합니다. 대체 접근 방식은 &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt; &lt;code&gt;TransformedTargetRegressor&lt;/code&gt; &lt;/a&gt; 메타 추정기를 사용 하여 &lt;code&gt;y_pred&lt;/code&gt; 를 엄격하게 양의 도메인 에 매핑 하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="bc5a34aa26f7428f3e35a346a70f3689822fe771" translate="yes" xml:space="preserve">
          <source>The Poisson deviance computed as an evaluation metric reflects both the calibration and the ranking power of the model. It also makes a linear assumption on the ideal relationship between the expected value and the variance of the response variable. For the sake of conciseness we did not check whether this assumption holds.</source>
          <target state="translated">평가 메트릭으로 계산 된 푸 아송 이탈도는 모델의 보정 및 순위 파워를 모두 반영합니다. 또한 예상 값과 반응 변수의 분산 간의 이상적인 관계에 대한 선형 가정을합니다. 간결함을 위해이 가정이 유효한지 확인하지 않았습니다.</target>
        </trans-unit>
        <trans-unit id="450889f232106f285e0f6ca8296ef879dd035563" translate="yes" xml:space="preserve">
          <source>The Probability Density Functions (PDF) of these distributions are illustrated in the following figure,</source>
          <target state="translated">이러한 분포의 확률 밀도 함수 (PDF)는 다음 그림에 나와 있습니다.</target>
        </trans-unit>
        <trans-unit id="935532caca213849eac9af7588fc52963397d517" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">회귀 자에서 &lt;code&gt;score&lt;/code&gt; 를 호출 할 때 사용되는 R2 점수 는 버전 0.23 &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; 를 사용하여 기본값 &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt; 와 일관성을 유지 합니다. 이는 모든 다중 출력 회귀 변수 의 &lt;code&gt;score&lt;/code&gt; 방법에 영향을줍니다 ( &lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor&quot;&gt; &lt;code&gt;MultiOutputRegressor&lt;/code&gt; &lt;/a&gt; 제외 ).</target>
        </trans-unit>
        <trans-unit id="11e4fa9675506dfc6e48ea958dcc499e9784ad90" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;sklearn.multioutput.multioutputregressor#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">회귀 자에서 &lt;code&gt;score&lt;/code&gt; 를 호출 할 때 사용되는 R2 점수 는 버전 0.23 &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; 를 사용하여 기본값 &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt; 와 일관성을 유지 합니다. 이는 모든 다중 출력 회귀 변수 의 &lt;code&gt;score&lt;/code&gt; 방법에 영향을줍니다 ( &lt;a href=&quot;sklearn.multioutput.multioutputregressor#sklearn.multioutput.MultiOutputRegressor&quot;&gt; &lt;code&gt;MultiOutputRegressor&lt;/code&gt; &lt;/a&gt; 제외 ).</target>
        </trans-unit>
        <trans-unit id="a83459e6ef7baec271df4e0325c8da4e8711242a" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">RBF 커널은 고정 커널입니다. &quot;제곱 지수&quot;커널이라고도합니다. 길이 스케일 매개 변수 \ (l&amp;gt; 0 \)에 의해 매개 변수화되며, 이는 스칼라 (커널의 등방성 변형) 또는 입력 X (커널의 이방성 변형)와 같은 차원 수를 가진 벡터 일 수 있습니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="33c279bdcb922f55f225b113a682e1a5cf28cc6d" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter length_scale&amp;gt;0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">RBF 커널은 고정 커널입니다. &quot;제곱 지수&quot;커널이라고도합니다. 스칼라 (커널의 등방성 변형) 또는 입력 X (커널의 이방성 변형)와 동일한 수의 차원을 가진 벡터 일 수있는 길이 스케일 매개 변수 length_scale&amp;gt; 0으로 매개 변수화됩니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="f47997c4010f2e20accd04690970d6547a71bfc2" translate="yes" xml:space="preserve">
          <source>The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.</source>
          <target state="translated">RBF 커널은 밀도가 높은 매트릭스로 메모리에 표시되는 완전히 연결된 그래프를 생성합니다. 이 행렬은 매우 클 수 있으며 알고리즘의 각 반복에 대해 전체 행렬 곱셈 계산을 수행하는 비용과 결합하면 실행 시간이 엄청나게 길어질 수 있습니다. 반면, KNN 커널은 훨씬 더 메모리 친화적 인 희소 행렬을 생성하여 실행 시간을 크게 줄일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="512de356729b7d551b43d5d76bd8681dea941e19" translate="yes" xml:space="preserve">
          <source>The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.</source>
          <target state="translated">RBM은 특정 그래픽 모델을 사용하여 데이터의 가능성을 최대화하려고합니다. 사용 된 모수 학습 알고리즘 ( &lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt; )은 표현이 입력 데이터에서 멀리 벗어나는 것을 방지하여 흥미로운 규칙을 캡처하지만 모형이 작은 데이터 세트에는 유용하지 않으며 밀도 추정에는 유용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="95d48012cfd5cdfda48a80ad45de8292db32665c" translate="yes" xml:space="preserve">
          <source>The R^2 score or ndarray of scores if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">'multioutput'이 'raw_values'인 경우 R ^ 2 점수 또는 점수의 ndarray입니다.</target>
        </trans-unit>
        <trans-unit id="57cf85e67b74d4b36ce4e00a60cc2afb37409d42" translate="yes" xml:space="preserve">
          <source>The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.</source>
          <target state="translated">Rand Index는 예측 및 실제 군집화에서 동일하거나 다른 군집에 할당 된 모든 표본 쌍과 계수 쌍을 고려하여 두 군집 간의 유사성 측정을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="21d3f8892ca41bf337636e90759152be22d9788c" translate="yes" xml:space="preserve">
          <source>The RandomTreesEmbedding, from the &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module, is not technically a manifold embedding method, as it learn a high-dimensional representation on which we apply a dimensionality reduction method. However, it is often useful to cast a dataset into a representation in which the classes are linearly-separable.</source>
          <target state="translated">&lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 모듈 의 RandomTreesEmbedding 은 차원 축소 방법을 적용하는 고차원 표현을 배우기 때문에 기술적으로 매니 폴드 임베딩 방법이 아닙니다. 그러나 데이터 세트를 클래스를 선형으로 분리 할 수있는 표현으로 캐스트하는 것이 종종 유용합니다.</target>
        </trans-unit>
        <trans-unit id="9863f2af9163cbec042d3a8db1d9d0da7a79bddf" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length scales. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a scale mixture parameter \(\alpha&amp;gt;0\). Only the isotropic variant where length_scale \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">RationalQuadratic 커널은 다양한 특성 길이 척도를 가진 RBF 커널의 척도 혼합 (무한 합)으로 볼 수 있습니다. 길이 스케일 매개 변수 \ (l&amp;gt; 0 \) 및 스케일 혼합 매개 변수 \ (\ alpha&amp;gt; 0 \)로 매개 변수화됩니다. 현재 length_scale \ (l \)이 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="ddaea903844387569c2d558e504bb67163af0e53" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a scale mixture parameter alpha&amp;gt;0. Only the isotropic variant where length_scale is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">RationalQuadratic 커널은 특성 길이가 다른 RBF 커널의 스케일 혼합 (무한 합)으로 볼 수 있습니다. 길이 스케일 파라미터 length_scale&amp;gt; 0 및 스케일 혼합 파라미터 alpha&amp;gt; 0으로 파라미터 화됩니다. length_scale이 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="4290d451a4bc3d8974b88b0877f706f5c5e0d4c1" translate="yes" xml:space="preserve">
          <source>The SAGA solver supports both float64 and float32 bit arrays.</source>
          <target state="translated">SAGA 솔버는 float64 및 float32 비트 배열을 모두 지원합니다.</target>
        </trans-unit>
        <trans-unit id="66cebb865fc92b18216814d7cca47981b6cb6b41" translate="yes" xml:space="preserve">
          <source>The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the &lt;em&gt;stress&lt;/em&gt;) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.</source>
          <target state="translated">SMACOF (COmplicated Function을 MAjorizing하여 스케일링) 알고리즘은 주요 기술을 사용하여 목적 함수 ( &lt;em&gt;스트레스&lt;/em&gt; )를 최소화하는 다차원 스케일링 알고리즘입니다 . Guttman Transform이라고도하는 응력 주요 화는 응력의 모노톤 수렴을 보장하며 경사 하강과 같은 전통적인 기술보다 강력합니다.</target>
        </trans-unit>
        <trans-unit id="e2c4971923aff3f7fe2f8678fb678a8a1fe12716" translate="yes" xml:space="preserve">
          <source>The SMACOF algorithm for metric MDS can summarized by the following steps:</source>
          <target state="translated">메트릭 MDS에 대한 SMACOF 알고리즘은 다음 단계로 요약 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9991accadfafce42fd0097ac5bf1452a92166503" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient &lt;em&gt;s&lt;/em&gt; for a single sample is then given as:</source>
          <target state="translated">실루엣 계수는 &lt;em&gt;s의&lt;/em&gt; 단일 샘플에 대해 다음과 같이 주어진다 :</target>
        </trans-unit>
        <trans-unit id="977018d27e576be441891586d29895a67820e9bb" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.</source>
          <target state="translated">샘플 세트에 대한 실루엣 계수는 각 샘플에 대한 실루엣 계수의 평균으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="a8aa62253fcb64807b1a0f2a014811a76f4c09cc" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.</source>
          <target state="translated">실루엣 계수는 샘플이 자신과 유사한 샘플로 얼마나 잘 클러스터링되는지를 측정합니다. 높은 실루엣 계수를 갖는 군집 모델은 동일한 군집의 샘플이 서로 유사하고 서로 다른 군집의 샘플이 서로 매우 유사하지 않은 경우 분리되어 밀도가 높다고합니다.</target>
        </trans-unit>
        <trans-unit id="5da8604230381cdf66d3aec27ea08ace466e606c" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">실루엣 계수는 각 샘플에 대한 평균 클러스터 내 거리 ( &lt;code&gt;a&lt;/code&gt; ) 및 평균 가장 가까운 클러스터 거리 ( &lt;code&gt;b&lt;/code&gt; )를 사용하여 계산됩니다 . 표본의 실루엣 계수는 &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; 입니다. Silhouette Coefficient는 레이블 수가 2 &amp;lt;= n_labels &amp;lt;= n_samples-1 인 경우에만 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="4273a368cac49099b8caa7c819b214a9d85c0ff4" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. To clarify, &lt;code&gt;b&lt;/code&gt; is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">실루엣 계수는 각 샘플에 대한 평균 클러스터 내 거리 ( &lt;code&gt;a&lt;/code&gt; ) 및 평균 가장 가까운 클러스터 거리 ( &lt;code&gt;b&lt;/code&gt; )를 사용하여 계산됩니다 . 표본의 실루엣 계수는 &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; 입니다. 명확히하기 위해, &lt;code&gt;b&lt;/code&gt; 는 샘플과 샘플이 포함되지 않은 가장 가까운 군집 사이의 거리입니다. Silhouette Coefficient는 레이블 수가 2 &amp;lt;= n_labels &amp;lt;= n_samples-1 인 경우에만 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="74a577cf3f9facf0caa36a3cd46ce91a25197ef1" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Silhouette Coefficient는 DBSCAN을 통해 얻은 것과 같은 밀도 기반 클러스터와 같은 다른 개념의 클러스터보다 볼록 클러스터에 일반적으로 더 높습니다.</target>
        </trans-unit>
        <trans-unit id="b08373a116f8a28b8f59b4a939f8e86bc18a285c" translate="yes" xml:space="preserve">
          <source>The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result.</source>
          <target state="translated">Spearman 상관 계수는 데이터에서 추정되며 결과 추정의 부호가 결과로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="1aca461bec942a3ea49c28ca350e80c03610ed92" translate="yes" xml:space="preserve">
          <source>The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:</source>
          <target state="translated">스펙트럼 임베딩 (Laplacian Eigenmaps) 알고리즘은 3 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="5c2cf1989e094ea2eafd60dd4bfcc701bf5b4819" translate="yes" xml:space="preserve">
          <source>The Stack Exchange family of sites hosts &lt;a href=&quot;https://meta.stackexchange.com/q/130524&quot;&gt;multiple subdomains for Machine Learning questions&lt;/a&gt;.</source>
          <target state="translated">Stack Exchange 사이트 제품군은 &lt;a href=&quot;https://meta.stackexchange.com/q/130524&quot;&gt;머신 러닝 질문에 대한 여러 하위 도메인을&lt;/a&gt; 호스팅합니다 .</target>
        </trans-unit>
        <trans-unit id="530048073ab2f0fd4beae1a41150629fce3bbc04" translate="yes" xml:space="preserve">
          <source>The TF-IDF vectorized posts form a word frequency matrix, which is then biclustered using Dhillon&amp;rsquo;s Spectral Co-Clustering algorithm. The resulting document-word biclusters indicate subsets words used more often in those subsets documents.</source>
          <target state="translated">TF-IDF 벡터화 된 포스트는 워드 주파수 매트릭스를 형성 한 다음 Dhillon의 Spectral Co-Clustering 알고리즘을 사용하여 bicluster됩니다. 결과적인 문서 단어 biclusters는 해당 하위 집합 문서에서 더 자주 사용되는 하위 집합 단어를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="a51b4bd7337a8a43bf76bb00c70355d636e98cf2" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">V- 측정은 실제로 상기 논의 된 상호 정보 (NMI)와 동일하며, 집계 함수는 산술 평균이다 &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="4af22841a0f64abdacec5694995ece03a0f97488" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id16&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">V- 측정은 실제로 위에서 설명한 상호 정보 (NMI)와 동일하며 집계 함수는 산술 평균입니다 &lt;a href=&quot;#b2011&quot; id=&quot;id16&quot;&gt;[B2011]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="28e22a992327910c0281c7997fd0381a055b2da5" translate="yes" xml:space="preserve">
          <source>The V-measure is the harmonic mean between homogeneity and completeness:</source>
          <target state="translated">V- 측정은 균질성과 완전성 사이의 조화 평균입니다.</target>
        </trans-unit>
        <trans-unit id="8e9e469c4bec48fbecca2280bcc57b2302181e26" translate="yes" xml:space="preserve">
          <source>The WAGE is increasing when EDUCATION is increasing. Note that the dependence between WAGE and EDUCATION represented here is a marginal dependence, i.e., it describes the behavior of a specific variable without keeping the others fixed.</source>
          <target state="translated">교육이 증가하면 임금이 증가합니다. 여기에 표시된 임금과 교육 간의 의존성은 한계 의존성입니다. 즉, 다른 변수를 고정하지 않고 특정 변수의 행동을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="9d831d6fbe54cab7c78889ea027eda02af846327" translate="yes" xml:space="preserve">
          <source>The Yeo-Johnson transform is given by:</source>
          <target state="translated">Yeo-Johnson 변환은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="dd310b3ef8cade9b7b44463cf24d9960d1a07902" translate="yes" xml:space="preserve">
          <source>The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \(Z\):</source>
          <target state="translated">\ (\ ell = \ lceil \ log_2 k \ rceil \) 특이 벡터는 두 번째부터 시작하여 원하는 분할 정보를 제공합니다. 행렬 \ (Z \)를 구성하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0151312e57232e026343f2a42d71b34eecec6ec6" translate="yes" xml:space="preserve">
          <source>The \(\nu\)-SVC formulation &lt;a href=&quot;#id17&quot; id=&quot;id8&quot;&gt;15&lt;/a&gt; is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="translated">\ (\ nu \)-SVC 공식 &lt;a href=&quot;#id17&quot; id=&quot;id8&quot;&gt;15&lt;/a&gt; 는 \ (C \)-SVC의 재 매개 변수화이므로 수학적으로 동등합니다.</target>
        </trans-unit>
        <trans-unit id="6bae7eee8e57958cb7a015c09e85982ab959fe35" translate="yes" xml:space="preserve">
          <source>The \(k\)-neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; is the most commonly used technique. The optimal choice of the value \(k\) is highly data-dependent: in general a larger \(k\) suppresses the effects of noise, but makes the classification boundaries less distinct.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt; 의 \ (k \)-이웃 분류 는 가장 일반적으로 사용되는 기술입니다. \ (k \) 값의 최적 선택은 데이터에 따라 크게 달라집니다. 일반적으로 큰 \ (k \)는 노이즈의 영향을 억제하지만 분류 경계를 덜 뚜렷하게 만듭니다.</target>
        </trans-unit>
        <trans-unit id="e8862ed89c87547157e194d96a8f490026b2e0fb" translate="yes" xml:space="preserve">
          <source>The ability to reproduce the data of the regularized model is similar to the one of the non-regularized model.</source>
          <target state="translated">정규화 된 모델의 데이터를 재현하는 기능은 비정규 화 된 모델의 데이터와 유사합니다.</target>
        </trans-unit>
        <trans-unit id="7ea48a47287b56e3f66c9ec81d51c291e8fedf59" translate="yes" xml:space="preserve">
          <source>The above vectorization scheme is simple but the fact that it holds an &lt;strong&gt;in- memory mapping from the string tokens to the integer feature indices&lt;/strong&gt; (the &lt;code&gt;vocabulary_&lt;/code&gt; attribute) causes several &lt;strong&gt;problems when dealing with large datasets&lt;/strong&gt;:</source>
          <target state="translated">위의 벡터화 체계는 간단하지만 &lt;strong&gt;문자열 토큰에서 정수 피처 인덱스&lt;/strong&gt; ( &lt;code&gt;vocabulary_&lt;/code&gt; 속성) &lt;strong&gt;로의 메모리 내 맵핑을&lt;/strong&gt; 보유한다는 사실은 &lt;strong&gt;큰 데이터 세트를 처리 할 때&lt;/strong&gt; 몇 가지 &lt;strong&gt;문제점을&lt;/strong&gt; 발생시킵니다 .</target>
        </trans-unit>
        <trans-unit id="f78fe42a387595f69f94294d9712aa8fff9cf7ee" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores during early stopping. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="translated">조기 중지 중에 점수를 비교할 때 사용할 절대 허용 오차입니다. 허용 오차가 높을수록 조기에 중지 할 가능성이 높습니다. 허용 오차가 높을수록 후속 반복이 참조 점수에 대한 개선으로 간주되기가 더 어려워집니다.</target>
        </trans-unit>
        <trans-unit id="a971e24181d9be44e4bb51f3963013ec9c9d01d5" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="translated">점수를 비교할 때 사용할 절대 허용 오차입니다. 허용 오차가 높을수록 조기에 중지 할 가능성이 높습니다. 허용 오차가 높을수록 후속 반복이 참조 점수에 대한 개선으로 간주되기가 더 어려워집니다.</target>
        </trans-unit>
        <trans-unit id="58279b870becff96bab0fc34e17d7f045af03b34" translate="yes" xml:space="preserve">
          <source>The abstract base class for all kernels is &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt;. Kernel implements a similar interface as &lt;code&gt;Estimator&lt;/code&gt;, providing the methods &lt;code&gt;get_params()&lt;/code&gt;, &lt;code&gt;set_params()&lt;/code&gt;, and &lt;code&gt;clone()&lt;/code&gt;. This allows setting kernel values also via meta-estimators such as &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;GridSearch&lt;/code&gt;. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with &lt;code&gt;k1__&lt;/code&gt; and parameters of the right operand with &lt;code&gt;k2__&lt;/code&gt;. An additional convenience method is &lt;code&gt;clone_with_theta(theta)&lt;/code&gt;, which returns a cloned version of the kernel but with the hyperparameters set to &lt;code&gt;theta&lt;/code&gt;. An illustrative example:</source>
          <target state="translated">모든 커널의 추상 기본 클래스는 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt; 입니다. 커널은 &lt;code&gt;Estimator&lt;/code&gt; 와 유사한 인터페이스를 구현하여 &lt;code&gt;get_params()&lt;/code&gt; , &lt;code&gt;set_params()&lt;/code&gt; 및 &lt;code&gt;clone()&lt;/code&gt; 메소드를 제공합니다 . 이를 통해 &lt;code&gt;Pipeline&lt;/code&gt; 또는 &lt;code&gt;GridSearch&lt;/code&gt; 와 같은 메타 추정기를 통해 커널 값을 설정할 수 있습니다 . 커널의 중첩 구조 (커널 연산자를 적용하여 아래 참조)로 인해 커널 매개 변수의 이름이 상대적으로 복잡해질 수 있습니다. 일반적으로 이진 커널 연산자의 경우 왼쪽 피연산자의 매개 변수 앞에 &lt;code&gt;k1__&lt;/code&gt; 이 있고 오른쪽 피연산자의 매개 변수 앞에 &lt;code&gt;k2__&lt;/code&gt; 가 있습니다. 있습니다. 추가적인 편의 방법은 &lt;code&gt;clone_with_theta(theta)&lt;/code&gt; 커널의 복제 된 버전을 반환하지만 하이퍼 파라미터는 &lt;code&gt;theta&lt;/code&gt; 로 설정됩니다 . 예시적인 예 :</target>
        </trans-unit>
        <trans-unit id="d6dfb2a865fc7561c9990f60831354c8c2b2acf1" translate="yes" xml:space="preserve">
          <source>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: &amp;ldquo;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&amp;rdquo;, Optimization Methods and Software 1, 1992, 23-34].</source>
          <target state="translated">3 차원 공간에서 분리 평면을 얻는 데 사용되는 실제 선형 프로그램은 다음에 설명되어 있습니다. 34].</target>
        </trans-unit>
        <trans-unit id="5c16c787f9e7f87c21ce4f86533b13082a6022ce" translate="yes" xml:space="preserve">
          <source>The actual number of iteration performed by the solver. Only returned if &lt;code&gt;return_n_iter&lt;/code&gt; is True.</source>
          <target state="translated">솔버가 수행 한 실제 반복 횟수입니다. &lt;code&gt;return_n_iter&lt;/code&gt; 가 True 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="9cc109786e99b4c544e4621619329291c5062f5c" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion.</source>
          <target state="translated">중지 기준에 도달하기 전 실제 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="68fff8e45d31e6c0edd22bde7c766ffe083896ee" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">중지 기준에 도달하기 전 실제 반복 횟수입니다. 다중 클래스 적합의 경우 모든 이진 적합에 대한 최대 값입니다.</target>
        </trans-unit>
        <trans-unit id="c6a597b70b1c3dba7fc91a33b2b458a1718ae376" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion.</source>
          <target state="translated">중지 기준에 도달하기위한 실제 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="be3e6ed927427ff958a3dc691ec6f2ca38b153e8" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">중지 기준에 도달하기위한 실제 반복 횟수입니다. 멀티 클래스 적합의 경우 모든 이진 적합보다 최대 값입니다.</target>
        </trans-unit>
        <trans-unit id="905122fa4ae495ec172d69201d288d9cbd8eb107" translate="yes" xml:space="preserve">
          <source>The actual number of neighbors used for &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; queries.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt; 쿼리에 사용 된 실제 이웃 수입니다 .</target>
        </trans-unit>
        <trans-unit id="d1e2217f69f6ffa7102e10d2dcd6eed0b5a03524" translate="yes" xml:space="preserve">
          <source>The actual number of quantiles used to discretize the cumulative distribution function.</source>
          <target state="translated">누적 분포 함수를 이산화하는 데 사용되는 실제 분위수입니다.</target>
        </trans-unit>
        <trans-unit id="99f948dd8ff8ebdf8ccc1268f27f992c4d54a1e0" translate="yes" xml:space="preserve">
          <source>The actual number of samples</source>
          <target state="translated">실제 샘플 수</target>
        </trans-unit>
        <trans-unit id="83f4574f37ccf239bd98ad8929c323f413ece25a" translate="yes" xml:space="preserve">
          <source>The actual number of samples.</source>
          <target state="translated">실제 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="257683b273bf01e4ad40af7f18f19c426c837478" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel as used here is given by</source>
          <target state="translated">여기에 사용 된 추가 카이 제곱 커널은</target>
        </trans-unit>
        <trans-unit id="721e722946fa16cdc1d4e80a122a59df383d3b35" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel is a kernel on histograms, often used in computer vision.</source>
          <target state="translated">추가 카이 제곱 커널은 히스토그램의 커널로 컴퓨터 비전에 자주 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="509930cd9616089e31b59f87837fde9a2850682a" translate="yes" xml:space="preserve">
          <source>The additive version of this kernel</source>
          <target state="translated">이 커널의 추가 버전</target>
        </trans-unit>
        <trans-unit id="479259eedb54024948ef963b0d114b00fbb05f1b" translate="yes" xml:space="preserve">
          <source>The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split the graph into comparably sized components.</source>
          <target state="translated">인접 행렬은 스펙트럼 (특히 가장 작은 고유 값과 관련된 고유 벡터)이 그래프를 비슷한 크기의 구성 요소로 나누는 데 필요한 최소한의 컷 수로 해석되는 정규화 된 그래프 라플라시안을 계산하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="b4f6e52ff52334d95b6f16934aecd26800758ca5" translate="yes" xml:space="preserve">
          <source>The adjacency matrix of the graph to embed.</source>
          <target state="translated">포함 할 그래프의 인접 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="f8c06d6c9bebb104301c962acd25687eac4074b4" translate="yes" xml:space="preserve">
          <source>The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).</source>
          <target state="translated">따라서 조정 된 Rand 지수는 군집 및 표본 수에 관계없이 임의의 표식에 대해 0.0에 가깝고 군집이 동일 할 때 정확히 1.0 (최대 순열)에 가까운 값을 갖도록 보장됩니다.</target>
        </trans-unit>
        <trans-unit id="83e2303d70450b875b04241e83f4a4153932e0a9" translate="yes" xml:space="preserve">
          <source>The advantage of using approximate explicit feature maps compared to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;kernel trick&lt;/a&gt;, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; can make non-linear learning on large datasets possible.</source>
          <target state="translated">암시 적으로 기능 맵을 사용 하는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;커널 트릭에&lt;/a&gt; 비해 대략적인 명시 적 기능 맵을 사용할 경우의 장점은 명시 적 맵핑이 온라인 학습에 더 적합 할 수 있고 매우 큰 데이터 세트에서 학습 비용을 크게 줄일 수 있다는 것입니다. 표준 커널 화 된 SVM은 대규모 데이터 세트로 확장 할 수 없지만 대략적인 커널 맵을 사용하면 훨씬 더 효율적인 선형 SVM을 사용할 수 있습니다. 특히 커널 맵 근사치와 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 를 결합하면 대규모 데이터 세트에 대한 비선형 학습이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="e8cf8d3d0c8dfd7b6a3c8351c56803f2fdf0d2d8" translate="yes" xml:space="preserve">
          <source>The advantages of Bayesian Regression are:</source>
          <target state="translated">베이지안 회귀의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d7269fefe21b18a1d9b1e8a11da2f35edf3e36ec" translate="yes" xml:space="preserve">
          <source>The advantages of GBRT are:</source>
          <target state="translated">GBRT의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="6bdcff89c23af609c3f964058bbddc38e45558f8" translate="yes" xml:space="preserve">
          <source>The advantages of Gaussian processes are:</source>
          <target state="translated">가우스 프로세스의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c632bf8abb99b63a04554183dcc32e66bb0cf004" translate="yes" xml:space="preserve">
          <source>The advantages of LARS are:</source>
          <target state="translated">LARS의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="54923c815d40d3138e9fbe2ad92c97dd60ef1b2e" translate="yes" xml:space="preserve">
          <source>The advantages of Multi-layer Perceptron are:</source>
          <target state="translated">다층 퍼셉트론의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="1dabd14761114fadc83f1114e989744a341117db" translate="yes" xml:space="preserve">
          <source>The advantages of Stochastic Gradient Descent are:</source>
          <target state="translated">확률 적 경사 하강의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a12cea148a9927c20c87185a7ad428a4bd953760" translate="yes" xml:space="preserve">
          <source>The advantages of support vector machines are:</source>
          <target state="translated">서포트 벡터 머신의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="b3ce019dea8f1fa7178760b3ed7e1bed715f9894" translate="yes" xml:space="preserve">
          <source>The affinity matrix describing the relationship of the samples to embed. &lt;strong&gt;Must be symmetric&lt;/strong&gt;.</source>
          <target state="translated">삽입 할 샘플의 관계를 설명하는 선호도 매트릭스입니다. &lt;strong&gt;대칭이어야합니다&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="f8f51936a34abb4d8304c65310991fbeb781a2b1" translate="yes" xml:space="preserve">
          <source>The algorithm automatically sets the number of clusters, instead of relying on a parameter &lt;code&gt;bandwidth&lt;/code&gt;, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided &lt;code&gt;estimate_bandwidth&lt;/code&gt; function, which is called if the bandwidth is not set.</source>
          <target state="translated">알고리즘은 매개 변수 &lt;code&gt;bandwidth&lt;/code&gt; 에 의존하지 않고 클러스터 수를 자동으로 설정하여 검색 할 영역의 크기를 결정합니다. 이 매개 변수는 수동으로 설정 될 수 있지만 제공된 &lt;code&gt;estimate_bandwidth&lt;/code&gt; _ 대역폭 기능을 사용하여 추정 할 수 있습니다.이 기능은 대역폭이 설정되지 않은 경우에 호출됩니다.</target>
        </trans-unit>
        <trans-unit id="999ae100a463cc0ff60234ca1a1687522770fa40" translate="yes" xml:space="preserve">
          <source>The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is &amp;ldquo;n_samples choose n_subsamples&amp;rdquo;, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.</source>
          <target state="translated">이 알고리즘은 X에서 샘플의 n_subsamples 크기를 가진 부분 집합에 대해 최소 제곱 해를 계산합니다. 피처 수와 샘플 사이의 n_subsamples 값은 견고성과 효율성 사이의 절충으로 추정기로 이어집니다. 최소 제곱 해의 수는&amp;ldquo;n_samples choose n_subsamples&amp;rdquo;이므로 매우 클 수 있으므로 max_subpopulation으로 제한 될 수 있습니다. 이 한계에 도달하면 서브 세트가 무작위로 선택됩니다. 최종 단계에서, 공간 중앙값 (또는 L1 중앙값)은 모든 최소 제곱 솔루션으로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="19207c781cd295df6565aab588711bf1a9323bef" translate="yes" xml:space="preserve">
          <source>The algorithm can also be understood through the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi diagrams&lt;/a&gt;. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.</source>
          <target state="translated">이 알고리즘은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi 다이어그램&lt;/a&gt; 의 개념을 통해서도 이해 될 수 있습니다 . 먼저 점의 보로 노이 다이어그램은 현재 중심을 사용하여 계산됩니다. Voronoi 다이어그램의 각 세그먼트는 별도의 클러스터가됩니다. 둘째, 중심이 각 세그먼트의 평균으로 업데이트됩니다. 그런 다음 알고리즘은 중지 기준이 충족 될 때까지이를 반복합니다. 일반적으로 반복 사이의 목적 함수의 상대적 감소가 주어진 공차 값보다 작 으면 알고리즘이 중지됩니다. 이 구현에서는 그렇지 않습니다. 중심이 공차보다 작게 이동하면 반복이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="ac387733191797f4111e67a021bb9641f7899ac2" translate="yes" xml:space="preserve">
          <source>The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">이 문제를 해결하기 위해 사용 된 알고리즘은 Friedman 2008 Biostatistics 논문의 GLasso 알고리즘입니다. R &lt;code&gt;glasso&lt;/code&gt; 패키지 와 동일한 알고리즘 입니다.</target>
        </trans-unit>
        <trans-unit id="2d7b95845283c4d98cc4167d4e0748ee6c69e84f" translate="yes" xml:space="preserve">
          <source>The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. &amp;ldquo;Algorithms for computing the sample variance: Analysis and recommendations.&amp;rdquo; The American Statistician 37.3 (1983): 242-247:</source>
          <target state="translated">증분 평균 및 표준에 대한 알고리즘은 Chan, Tony F., Gene H. Golub 및 Randall J. LeVeque의 방정식 1.5a, b에 나와 있습니다. &quot;샘플 분산 계산 알고리즘 : 분석 및 권장 사항.&quot; 미국 통계 학자 37.3 (1983) : 242-247 :</target>
        </trans-unit>
        <trans-unit id="c4e655f6aa7cf22a58a123dc60b12c815d9f7df3" translate="yes" xml:space="preserve">
          <source>The algorithm is adapted from Guyon [1] and was designed to generate the &amp;ldquo;Madelon&amp;rdquo; dataset.</source>
          <target state="translated">이 알고리즘은 Guyon [1]에서 채택되었으며 &quot;Madelon&quot;데이터 세트를 생성하도록 설계되었습니다.</target>
        </trans-unit>
        <trans-unit id="da28c971693f926f3030184039a925bcc2b1ca76" translate="yes" xml:space="preserve">
          <source>The algorithm is from Marsland [1].</source>
          <target state="translated">알고리즘은 Marsland에서 온 것입니다 [1].</target>
        </trans-unit>
        <trans-unit id="94c105f8aebba6a360eae1c93878348f10f58fca" translate="yes" xml:space="preserve">
          <source>The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.</source>
          <target state="translated">이 알고리즘은 알고리즘을 실행하는 동안 가장 가까운 인접 검색을 여러 개 필요로하기 때문에 확장 성이 뛰어나지 않습니다. 알고리즘 수렴이 보장되지만 중심 변화가 적을 때 알고리즘 반복이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="da3ddd72ab3f78c12c541d3f34c2389bc8e810e8" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including features at each step, the estimated coefficients are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">이 알고리즘은 전진 단계적 회귀와 유사하지만 각 단계에서 특징을 포함하는 대신 추정 계수가 잔차와의 상관 관계와 같은 방향으로 증가합니다.</target>
        </trans-unit>
        <trans-unit id="3dc6012ab672dce7fe920dc60add16368612e345" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">이 알고리즘은 정방향 회귀 회귀와 유사하지만 각 단계에서 변수를 포함하는 대신 추정 된 매개 변수는 잔차와의 상관 관계에 대해 등각 방향으로 증가합니다.</target>
        </trans-unit>
        <trans-unit id="0335c4d6784d824f2c45b2464a3517f723de00c5" translate="yes" xml:space="preserve">
          <source>The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.</source>
          <target state="translated">알고리즘은 확률 론적이며 다른 시드로 여러 번 다시 시작하면 다른 임베딩이 생성 될 수 있습니다. 그러나 오류가 가장 적은 임베딩을 선택하는 것은 완벽합니다.</target>
        </trans-unit>
        <trans-unit id="7c7de93467b285ca3f0b63a2764ebf7e5ab511ed" translate="yes" xml:space="preserve">
          <source>The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \(b\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.</source>
          <target state="translated">이 알고리즘은 바닐라 k- 평균과 유사한 두 가지 주요 단계를 반복합니다. 첫 번째 단계에서 \ (b \) 샘플은 데이터 집합에서 무작위로 추출되어 미니 배치를 형성합니다. 그런 다음 가장 가까운 중심에 할당됩니다. 두 번째 단계에서는 중심이 업데이트됩니다. k- 평균과 대조적으로, 이것은 샘플마다 수행된다. 미니 배치의 각 샘플에 대해 할당 된 중심은 샘플 및 해당 중심에 할당 된 모든 이전 샘플의 스트리밍 평균을 취하여 업데이트됩니다. 이것은 시간이 지남에 따라 중심에 대한 변화율을 감소시키는 효과가 있습니다. 이러한 단계는 수렴 또는 미리 결정된 반복 횟수에 도달 할 때까지 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="eac8adf7ae77ba9768138371c6c71edd5a3a76a7" translate="yes" xml:space="preserve">
          <source>The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.</source>
          <target state="translated">이 알고리즘은 행렬의 행과 열을 분할하여 대응하는 블록 단위 상수 바둑판 행렬이 원래 행렬에 대한 근사치를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="36533938d0e94afa89a69e7b9609c61646ab71f5" translate="yes" xml:space="preserve">
          <source>The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.</source>
          <target state="translated">이 알고리즘은 전체 입력 샘플 데이터를 노이즈의 영향을받을 수있는 일련의 인라이너와 데이터에 대한 잘못된 가설로 인해 발생하는 이상 값으로 나눕니다. 결과 모델은 결정된 이너만으로 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="76e87cc3ec2713663d8449fef17b4e0c4101c305" translate="yes" xml:space="preserve">
          <source>The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.</source>
          <target state="translated">미리 설정된 최대 반복 횟수에 도달하면 알고리즘이 중지됩니다. 또는 손실의 개선이 특정의 작은 수보다 낮은 경우.</target>
        </trans-unit>
        <trans-unit id="841b170fb7ba2429816cb7c51af4dbae57b471af" translate="yes" xml:space="preserve">
          <source>The algorithm supports sample weights, which can be given by a parameter &lt;code&gt;sample_weight&lt;/code&gt;. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \(X\).</source>
          <target state="translated">이 알고리즘은 &lt;code&gt;sample_weight&lt;/code&gt; 매개 변수로 제공 될 수있는 샘플 가중치를 지원합니다 . 따라서 클러스터 중심과 관성 값을 계산할 때 일부 샘플에 더 많은 가중치를 할당 할 수 있습니다. 예를 들어 샘플에 가중치 2를 할당하는 것은 해당 샘플의 복제본을 데이터 세트 \ (X \)에 추가하는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a8238e48cf484817aaf23aa2adc5e4a70681d78f" translate="yes" xml:space="preserve">
          <source>The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.</source>
          <target state="translated">NearestNeighbors 모듈에서 포인트 단위 거리를 계산하고 가장 가까운 이웃을 찾는 데 사용하는 알고리즘입니다. 자세한 내용은 NearestNeighbors 모듈 설명서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="3602b334476c395c578aab7a7d09d75f92100f28" translate="yes" xml:space="preserve">
          <source>The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.</source>
          <target state="translated">이 알고리즘은 입력 데이터 행렬을 이분 그래프로 취급합니다. 행렬의 행과 열은 두 정점 세트에 해당하고 각 항목은 행과 열 사이의 가장자리에 해당합니다. 알고리즘은이 그래프의 정규화 된 컷을 근사하여 무거운 하위 그래프를 찾습니다.</target>
        </trans-unit>
        <trans-unit id="e4086364ea17b4442e0265a08361358aa304a247" translate="yes" xml:space="preserve">
          <source>The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop.</source>
          <target state="translated">가중치를 추정하는 데 사용되는 알고리즘입니다. n_components 번, 즉 외부 루프가 반복 될 때마다 한 번씩 호출됩니다.</target>
        </trans-unit>
        <trans-unit id="e4e3e486ab0114bca02d02635f9410b77ca99a25" translate="yes" xml:space="preserve">
          <source>The algorithm used to fit the model is coordinate descent.</source>
          <target state="translated">모델에 맞는 알고리즘은 좌표 하강입니다.</target>
        </trans-unit>
        <trans-unit id="17e35eb9c6004e2d60b73104c93585aacf4a2e98" translate="yes" xml:space="preserve">
          <source>The algorithmic complexity of affinity propagation is quadratic in the number of points.</source>
          <target state="translated">친화도 전파의 알고리즘 복잡도는 포인트 수에서 2 차적입니다.</target>
        </trans-unit>
        <trans-unit id="4c657d81ec38ba678d89f7b7de187e376e50c0f3" translate="yes" xml:space="preserve">
          <source>The algorithms for regression and classification only differ in the concrete loss function used.</source>
          <target state="translated">회귀 및 분류 알고리즘은 사용 된 구체적 손실 함수 만 다릅니다.</target>
        </trans-unit>
        <trans-unit id="50542b2bc530bb15fc1bcd67d7b073d61bea03ab" translate="yes" xml:space="preserve">
          <source>The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.</source>
          <target state="translated">안정성 선택 기사의 알파 매개 변수는 기능을 임의로 스케일하는 데 사용됩니다. 0과 1 사이 여야합니다.</target>
        </trans-unit>
        <trans-unit id="35d70c04237d10e5da496ff67ac57fce44665490" translate="yes" xml:space="preserve">
          <source>The alpha parameter of the GraphicalLasso setting the sparsity of the model is set by internal cross-validation in the GraphicalLassoCV. As can be seen on figure 2, the grid to compute the cross-validation score is iteratively refined in the neighborhood of the maximum.</source>
          <target state="translated">모델 희소성을 설정하는 GraphicalLasso의 알파 매개 변수는 GraphicalLassoCV에서 내부 교차 검증에 의해 설정됩니다. 그림 2에서 볼 수 있듯이 교차 검증 점수를 계산하는 그리드는 최대 값 근처에서 반복적으로 구체화됩니다.</target>
        </trans-unit>
        <trans-unit id="2075fb3135d145905cc5a41db16871a4bf5168da" translate="yes" xml:space="preserve">
          <source>The alpha-quantile of the huber loss function and the quantile loss function. Only if &lt;code&gt;loss='huber'&lt;/code&gt; or &lt;code&gt;loss='quantile'&lt;/code&gt;.</source>
          <target state="translated">후버 손실 함수 및 양자 손실 함수의 알파 양자. &lt;code&gt;loss='huber'&lt;/code&gt; 또는 &lt;code&gt;loss='quantile'&lt;/code&gt; 인 경우에만 해당됩니다 .</target>
        </trans-unit>
        <trans-unit id="28a2d4f18d224d5f96bb30d3faa50cdca5ce2935" translate="yes" xml:space="preserve">
          <source>The alphas along the path where models are computed.</source>
          <target state="translated">모델이 계산되는 경로의 알파입니다.</target>
        </trans-unit>
        <trans-unit id="4390acc7061cc013783802ff89b6303810b0044d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set.</source>
          <target state="translated">데이터 세트의 오염량, 즉 데이터 세트의 특이 치 비율.</target>
        </trans-unit>
        <trans-unit id="a3cf6bdbcc68281bb427886cdd573766d75b2a0b" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Range is (0, 0.5).</source>
          <target state="translated">데이터 세트의 오염 정도, 즉 데이터 세트의 이상 값 비율. 범위는 (0, 0.5)입니다.</target>
        </trans-unit>
        <trans-unit id="db48eba23a1b19c738dbbe1eb3626efb11bd507f" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. If &amp;lsquo;auto&amp;rsquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">데이터 세트의 오염량, 즉 데이터 세트의 특이 치 비율. 의사 결정 기능에서 임계 값을 정의하기 위해 적합 할 때 사용됩니다. '자동'인 경우 결정 기능 임계 값은 원본 용지에서와 같이 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="f37071a773077642311acdcd66bbd39bda02514d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.</source>
          <target state="translated">데이터 세트의 오염 정도, 즉 데이터 세트의 이상 값 비율. 샘플 점수에 대한 임계 값을 정의하기 위해 피팅 할 때 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0b69accc98489279e23dd931886f63d4aafd913d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. If &amp;ldquo;auto&amp;rdquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">데이터 세트의 오염량, 즉 데이터 세트의 특이 치 비율. 피팅 할 때 결정 기능에 대한 임계 값을 정의하는 데 사용됩니다. &quot;자동&quot;인 경우 결정 기능 임계 값은 원본 용지에서와 같이 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="a618727acf5955128fa616ec196fb65bef331d06" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the scores of the samples.</source>
          <target state="translated">데이터 세트의 오염 정도, 즉 데이터 세트의 이상 값 비율. 피팅 할 때 이것은 샘플 점수에 대한 임계 값을 정의하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="057670e523b801c82d76306d2019fbc0b1e55fa7" translate="yes" xml:space="preserve">
          <source>The amount of penalization chosen by cross validation</source>
          <target state="translated">교차 검증에 의해 선택된 처벌 금액</target>
        </trans-unit>
        <trans-unit id="0a8889ae5955aac97641248ca40c9031408985ae" translate="yes" xml:space="preserve">
          <source>The amount of variance explained by each of the selected components.</source>
          <target state="translated">선택한 각 성분에 의해 설명 된 분산 량.</target>
        </trans-unit>
        <trans-unit id="0c35119665a5a383bf2a2ea2491d6548297b77b9" translate="yes" xml:space="preserve">
          <source>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</source>
          <target state="translated">입력 샘플의 이상 점수는 숲에서 나무의 평균 이상 점수로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="f5bff141bb39bd3c0bc3c7fd23ee875e735e27a8" translate="yes" xml:space="preserve">
          <source>The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.</source>
          <target state="translated">각 표본의 이상 점수를 국소 이상치라고합니다. 주변에 대한 주어진 샘플의 밀도의 국소 편차를 측정합니다. 이상 점수는 물체가 주변 이웃과 얼마나 고립되어 있는지에 달려 있습니다. 보다 정확하게는, 국소성은 k- 최근 접 이웃에 의해 주어지며, 그 거리는 국부 밀도를 추정하는데 사용된다. 샘플의 국부 밀도를 이웃의 국부 밀도와 비교함으로써, 이웃보다 밀도가 실질적으로 낮은 샘플을 식별 할 수있다. 이들은 이상치로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="9d2b05e02bf58b122225781451ec470ef3bbad43" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal.</source>
          <target state="translated">입력 샘플의 이상 점수. 낮을수록 비정상적입니다.</target>
        </trans-unit>
        <trans-unit id="7ea9f6456fff678e2acc40131f68202326a3c878" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">입력 샘플의 이상 점수. 낮을수록 비정상적입니다. 음수 점수는 특이 치를 나타내고 양수 점수는 특이 치를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="f8f5cba472ebac3a5205adfc98746f94cfb55c44" translate="yes" xml:space="preserve">
          <source>The approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; can be combined with the approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; to yield an approximate feature map for the exponentiated chi squared kernel. See the &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; for details and &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; for combination with the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; 에서&lt;/a&gt; 제공 하는 대략적인 기능 맵을 &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 에서 제공하는 대략적인 기능 맵과 결합 하여 지수 카이 제곱 커널에 대한 대략적인 기능 맵을 생성 할 수 있습니다 . 참조 &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; 자세한 내용과에 대한 &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; 와 조합 &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="98f8783bf76339c13180f5a0c21989e5d9843d3d" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the data by linear combinations.</source>
          <target state="translated">선형 조합으로 대부분의 데이터를 설명하는 데 필요한 대략적인 특이 벡터 수입니다.</target>
        </trans-unit>
        <trans-unit id="01540c9a0c0d75da953bc96aafe62b761a8754d9" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.</source>
          <target state="translated">대부분의 입력 데이터를 선형 조합으로 설명하는 데 필요한 대략적인 특이 벡터 수입니다. 입력에 이런 종류의 단일 스펙트럼을 사용하면 발생기가 실제로 관찰되는 상관 관계를 재현 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9b1212b40ba5f9b8205d7b64e2e9c20d35b415b3" translate="yes" xml:space="preserve">
          <source>The arithmetic mean is the sum of the elements along the axis divided by the number of elements.</source>
          <target state="translated">산술 평균은 축을 따라 요소의 합을 요소 수로 나눈 값입니다.</target>
        </trans-unit>
        <trans-unit id="35a07a5d9cec4bce6db2b0fe073857a7979c5b16" translate="yes" xml:space="preserve">
          <source>The array has 0.16% of non zero values.</source>
          <target state="translated">이 배열에는 0이 아닌 값의 0.16 %가 있습니다.</target>
        </trans-unit>
        <trans-unit id="a77140549775c60e078b0c26c1b965d02c08ebb3" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations</source>
          <target state="translated">(로그) 밀도 평가의 배열</target>
        </trans-unit>
        <trans-unit id="449fd4c0217076d90d22633d6cb32b7300c30336" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations, shape = X.shape[:-1]</source>
          <target state="translated">(log) 밀도 평가의 배열, 모양 = X.shape [:-1]</target>
        </trans-unit>
        <trans-unit id="d00a652e1f004fd3ddfd653b6794ad2aba8108c7" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations.</source>
          <target state="translated">로그 (밀도) 평가 배열입니다.</target>
        </trans-unit>
        <trans-unit id="0e2b34732ef7f3aed76135ded438ba1b0e2806d3" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations. These are normalized to be probability densities, so values will be low for high-dimensional data.</source>
          <target state="translated">로그 (밀도) 평가의 배열입니다. 이들은 확률 밀도로 정규화되므로 고차원 데이터의 경우 값이 낮습니다.</target>
        </trans-unit>
        <trans-unit id="531360783ebbdd4b65c83458a2d186f114e4086b" translate="yes" xml:space="preserve">
          <source>The automatic estimation from Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604 by Thomas P. Minka is also compared.</source>
          <target state="translated">PCA에 대한 차원 자동 선택의 자동 추정. Thomas P. Minka의 NIPS 2000 : 598-604도 비교됩니다.</target>
        </trans-unit>
        <trans-unit id="37bf2d4736a8a0a6781e876ebdb656796aca8913" translate="yes" xml:space="preserve">
          <source>The available cross validation iterators are introduced in the following section.</source>
          <target state="translated">사용 가능한 교차 유효성 검증 반복기는 다음 섹션에서 소개됩니다.</target>
        </trans-unit>
        <trans-unit id="52d681893d146fe7dad6c4424a94a534bab69431" translate="yes" xml:space="preserve">
          <source>The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.</source>
          <target state="translated">평균 복잡도는 O (kn T)로 주어지며, n은 샘플 수이고 T는 반복 수입니다.</target>
        </trans-unit>
        <trans-unit id="d9092bb0da47977d9b43f41f12fd7dcc75d26801" translate="yes" xml:space="preserve">
          <source>The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with &lt;code&gt;n_labels&lt;/code&gt; as its expected value, but samples are bounded (using rejection sampling) by &lt;code&gt;n_classes&lt;/code&gt;, and must be nonzero if &lt;code&gt;allow_unlabeled&lt;/code&gt; is False.</source>
          <target state="translated">인스턴스 당 평균 레이블 수입니다. 보다 정확하게는 샘플 당 레이블 수 는 예상 값 으로 &lt;code&gt;n_labels&lt;/code&gt; 를 사용하여 Poisson 분포에서 가져 오지만 샘플은 &lt;code&gt;n_classes&lt;/code&gt; 에 의해 제한되고 (거부 샘플링 사용) , &lt;code&gt;allow_unlabeled&lt;/code&gt; 가 False 이면 0이 아니어야합니다 .</target>
        </trans-unit>
        <trans-unit id="50521765649b7ccece3cb452f18896c14c55c5d8" translate="yes" xml:space="preserve">
          <source>The average precision score in multi-label settings</source>
          <target state="translated">다중 레이블 설정의 평균 정밀도 점수</target>
        </trans-unit>
        <trans-unit id="66fdff3c7719e9049c28a125538dc0d9761fb599" translate="yes" xml:space="preserve">
          <source>The averaged NDCG scores for all samples.</source>
          <target state="translated">모든 샘플에 대한 평균 NDCG 점수입니다.</target>
        </trans-unit>
        <trans-unit id="7f192c1d1db49dfae3574eb70a5374ae8608eb71" translate="yes" xml:space="preserve">
          <source>The averaged intercept term.</source>
          <target state="translated">평균 절편 항입니다.</target>
        </trans-unit>
        <trans-unit id="9595497d17dae79a02bc4117e4b89bb9f68b4736" translate="yes" xml:space="preserve">
          <source>The averaged intercept term. Only available if &lt;code&gt;average=True&lt;/code&gt;.</source>
          <target state="translated">평균 절편 기간입니다. &lt;code&gt;average=True&lt;/code&gt; 인 경우에만 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c831f95b759b1e5121cd7c90ec4906a408d65670" translate="yes" xml:space="preserve">
          <source>The averaged sample DCG scores.</source>
          <target state="translated">평균 샘플 DCG 점수입니다.</target>
        </trans-unit>
        <trans-unit id="5f1e3570b35bda8ded1b9651eb310bcefe0589f6" translate="yes" xml:space="preserve">
          <source>The axes with which the grid has been created or None if the grid has been given.</source>
          <target state="translated">그리드가 생성 된 축 또는 그리드가 제공된 경우 없음</target>
        </trans-unit>
        <trans-unit id="da553a7d6d695e12dca5e56a77d2e07dce1c7a46" translate="yes" xml:space="preserve">
          <source>The axis along which &lt;code&gt;X&lt;/code&gt; will be subsampled. &lt;code&gt;axis=0&lt;/code&gt; will select rows while &lt;code&gt;axis=1&lt;/code&gt; will select columns.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; 가 서브 샘플링 되는 축 입니다. &lt;code&gt;axis=0&lt;/code&gt; 은 행을 선택하고 &lt;code&gt;axis=1&lt;/code&gt; 은 열을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="2af1a0461163ef9a917dd17f9a6c7032c07c79bb" translate="yes" xml:space="preserve">
          <source>The axis along which to impute.</source>
          <target state="translated">대치 할 축입니다.</target>
        </trans-unit>
        <trans-unit id="efae1008b127cbb418b23469c7f222e2a6660b67" translate="yes" xml:space="preserve">
          <source>The bag of words representation is quite simplistic but surprisingly useful in practice.</source>
          <target state="translated">단어 모음은 매우 단순하지만 실제로는 놀랍게도 유용합니다.</target>
        </trans-unit>
        <trans-unit id="a41734ff5b2ce49bfc6f83ebb1bb324e76d37d70" translate="yes" xml:space="preserve">
          <source>The bags of words representation implies that &lt;code&gt;n_features&lt;/code&gt; is the number of distinct words in the corpus: this number is typically larger than 100,000.</source>
          <target state="translated">단어 백은 &lt;code&gt;n_features&lt;/code&gt; 가 코퍼스의 고유 단어 수임을 나타냅니다 .이 숫자는 일반적으로 100,000보다 큽니다.</target>
        </trans-unit>
        <trans-unit id="eac13f6898f72e0eaad5c85bd1b456a5882d1a72" translate="yes" xml:space="preserve">
          <source>The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.</source>
          <target state="translated">이진 및 멀티 클래스 분류 문제의 균형 잡힌 정확성은 불균형 데이터 세트를 처리합니다. 각 클래스에서 얻은 리콜의 평균으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="fd188197709d9753bf6d2d7d8ebbb1b211377cd3" translate="yes" xml:space="preserve">
          <source>The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.</source>
          <target state="translated">여기서 대역폭은 평활화 매개 변수로 작용하여 결과에서 바이어스와 분산 간의 균형을 제어합니다. 큰 대역폭은 매우 매끄러운 (즉, 높은 바이어스) 밀도 분포로 이어집니다. 대역폭이 작 으면 불균형 (즉, 고 분산) 밀도 분포가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="220a3cae014ca1f6f44493020405f4e460f4038d" translate="yes" xml:space="preserve">
          <source>The bandwidth of the kernel.</source>
          <target state="translated">커널의 대역폭.</target>
        </trans-unit>
        <trans-unit id="eaa1089b1fdc51cb43f528072a19cd3f3ec50c61" translate="yes" xml:space="preserve">
          <source>The bandwidth parameter.</source>
          <target state="translated">대역폭 매개 변수</target>
        </trans-unit>
        <trans-unit id="ca9efc037882c6303cbf300aed185a1d9a7dd7ae" translate="yes" xml:space="preserve">
          <source>The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classifier.</source>
          <target state="translated">막대 그림은 각 분류기의 정확도, 훈련 시간 (정규화) 및 테스트 시간 (정규화)을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="6ef0fbe42e5306306c086ebee574632386d4e293" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center. This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">기본 분류기는 25 개의 기본 추정기 (트리)가있는 임의의 포리스트 분류기입니다. 이 분류 기가 800 개의 훈련 데이터 포인트 모두에 대해 훈련 된 경우 예측에 대한 확신이 많으므로 큰 로그 손실이 발생합니다. 나머지 200 개 데이터 포인트에 대해 method = 'sigmoid'를 사용하여 600 개 데이터 포인트에 대해 학습 된 동일한 분류기를 교정하면 예측의 신뢰도가 감소합니다. 즉, 확률 벡터가 단면의 가장자리에서 중심으로 이동합니다. 이 교정으로 로그 손실이 줄어 듭니다. 대안은 기본 추정기의 수를 늘려서 로그 손실이 비슷하게 감소했음을 유의하십시오.</target>
        </trans-unit>
        <trans-unit id="3f89ef1e6cb2a54f74eb0a982beea8ffd3333592" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center:</source>
          <target state="translated">기본 분류기는 25 개의 기본 추정기 (트리)가있는 임의의 포리스트 분류기입니다. 이 분류 기가 800 개의 훈련 데이터 포인트 모두에 대해 훈련 된 경우 예측에 대한 확신이 많으므로 큰 로그 손실이 발생합니다. 나머지 200 개의 데이터 포인트에 대해 method = 'sigmoid'를 사용하여 600 개의 데이터 포인트에 대해 학습 된 동일한 분류기를 교정하면 예측의 신뢰도가 감소합니다. 즉, 확률 벡터를 단면의 가장자리에서 중심으로 이동합니다.</target>
        </trans-unit>
        <trans-unit id="2000944fc04d6f6e7393d659c7173e725c27d458" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;.</source>
          <target state="translated">강화 된 앙상블이 구축되는 기본 추정량입니다. 경우 &lt;code&gt;None&lt;/code&gt; 후베이스 추정기는 &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cdab13e5017fd96224833fdcfcd75615e152e1af" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</source>
          <target state="translated">부스트 앙상블이 만들어지는 기본 추정기. 적절한 &lt;code&gt;classes_&lt;/code&gt; 및 &lt;code&gt;n_classes_&lt;/code&gt; 속성 뿐만 아니라 샘플 가중치를 지원해야 합니다. &lt;code&gt;None&lt;/code&gt; 인 경우 기본 추정기는 &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7ad7363185e9af79d948e6b752e7cc25ad567f4b" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;.</source>
          <target state="translated">강화 된 앙상블이 구축되는 기본 추정량입니다. 적절한 &lt;code&gt;classes_&lt;/code&gt; 및 &lt;code&gt;n_classes_&lt;/code&gt; 속성 뿐만 아니라 샘플 가중치에 대한 지원이 필요 합니다. 경우 &lt;code&gt;None&lt;/code&gt; 후베이스 추정기는 &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="39ba9d176a83a208ed96d60f33bec321ac051ac5" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</source>
          <target state="translated">부스트 앙상블이 만들어지는 기본 추정기. 샘플 가중치 지원이 필요합니다. 경우 &lt;code&gt;None&lt;/code&gt; , 그베이스 추정기는 없다 &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="09e53f3d87743d060ad05fb04ae38587d00307b0" translate="yes" xml:space="preserve">
          <source>The base estimator from which the classifier chain is built.</source>
          <target state="translated">분류기 체인이 작성되는 기본 추정기입니다.</target>
        </trans-unit>
        <trans-unit id="c229b061f41bb16a20d56916c466508eaa778c17" translate="yes" xml:space="preserve">
          <source>The base estimator from which the ensemble is grown.</source>
          <target state="translated">앙상블이 성장하는 기본 추정기.</target>
        </trans-unit>
        <trans-unit id="5066de8ea8cb5c40493cb3ffbb0e912b0ff83ff9" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This can be both a fitted (if &lt;code&gt;prefit&lt;/code&gt; is set to True) or a non-fitted estimator. The estimator must have either a &lt;code&gt;feature_importances_&lt;/code&gt; or &lt;code&gt;coef_&lt;/code&gt; attribute after fitting.</source>
          <target state="translated">변압기가 구축되는 기본 추정기입니다. 이는 &lt;code&gt;prefit&lt;/code&gt; ( 사전 적합도 가 True로 설정된 경우 )이거나 적합하지 않은 추정기입니다. 추정기에는 피팅 후 &lt;code&gt;feature_importances_&lt;/code&gt; 또는 &lt;code&gt;coef_&lt;/code&gt; 속성 이 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="8935e197daa0e6c85ff9f9b0e235723b5ff13893" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the &lt;code&gt;SelectFromModel&lt;/code&gt;, i.e when prefit is False.</source>
          <target state="translated">변압기가 구축되는 기본 추정기입니다. 이것은 적합하지 않은 추정기가 &lt;code&gt;SelectFromModel&lt;/code&gt; 로 전달 될 때, 즉 prefit이 False 인 경우 에만 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="8062156aa44a8a655e18edffbba0a99e0e098c8a" translate="yes" xml:space="preserve">
          <source>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.</source>
          <target state="translated">데이터 세트의 임의의 부분 집합에 맞는 기본 추정량입니다. None이면, 기본 추정기는 의사 결정 트리입니다.</target>
        </trans-unit>
        <trans-unit id="d80c39c018abc387662fda22711c8b7bb4707717" translate="yes" xml:space="preserve">
          <source>The base kernel</source>
          <target state="translated">기본 커널</target>
        </trans-unit>
        <trans-unit id="1b7bc12e1d1eb0bb8e39e15748d7f9b27d93ef1a" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns uniform weights to each neighbor. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.</source>
          <target state="translated">기본 가장 가까운 이웃 분류는 균일 한 가중치를 사용합니다. 즉, 쿼리 지점에 지정된 값은 가장 가까운 이웃의 단순 과반수 투표에서 계산됩니다. 어떤 상황에서는 이웃이 더 가까워 지도록 이웃을 가중시키는 것이 좋습니다. 이것은 &lt;code&gt;weights&lt;/code&gt; 키워드를 통해 달성 할 수 있습니다 . 기본값 &lt;code&gt;weights = 'uniform'&lt;/code&gt; 은 각 이웃에 균일 한 가중치를 지정합니다. &lt;code&gt;weights = 'distance'&lt;/code&gt; 는 쿼리 지점으로부터의 거리의 역 비례에 가중치를 할당합니다. 대안 적으로, 가중치를 계산하기 위해 거리의 사용자 정의 함수가 제공 될 수있다.</target>
        </trans-unit>
        <trans-unit id="650ceb2a1485e5f890a3dcff86c2748037640c3b" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns equal weights to all points. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.</source>
          <target state="translated">가장 가까운 기본 이웃 회귀 분석은 균일 한 가중치를 사용합니다. 즉, 로컬 이웃의 각 포인트는 쿼리 포인트의 분류에 균일하게 기여합니다. 일부 상황에서, 가까운 지점보다 가까운 지점이 회귀에 더 많이 기여하도록 가중치를 적용하는 것이 유리할 수 있습니다. 이것은 &lt;code&gt;weights&lt;/code&gt; 키워드를 통해 달성 할 수 있습니다 . 기본값 &lt;code&gt;weights = 'uniform'&lt;/code&gt; 은 모든 포인트에 동일한 가중치를 할당합니다. &lt;code&gt;weights = 'distance'&lt;/code&gt; 는 쿼리 지점으로부터의 거리의 역 비례하는 가중치를 할당합니다. 대안 적으로, 거리의 사용자 정의 함수가 제공 될 수 있으며, 이는 가중치를 계산하는데 사용될 것이다.</target>
        </trans-unit>
        <trans-unit id="1cb5ab68855135ab83ff44b066a7bba092358a07" translate="yes" xml:space="preserve">
          <source>The behavior of &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; is summarized in the following table.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; 의 동작은 다음 표에 요약되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="9c1d95adae9acea07102a8bfe1db513459ebeaee" translate="yes" xml:space="preserve">
          <source>The behavior of the model is very sensitive to the &lt;code&gt;gamma&lt;/code&gt; parameter. If &lt;code&gt;gamma&lt;/code&gt; is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with &lt;code&gt;C&lt;/code&gt; will be able to prevent overfitting.</source>
          <target state="translated">모델의 동작은 &lt;code&gt;gamma&lt;/code&gt; 매개 변수에 매우 민감합니다 . &lt;code&gt;gamma&lt;/code&gt; 가 너무 큰 경우 , 서포트 벡터의 영향 영역의 반경은 서포트 벡터 자체 만 포함하며 &lt;code&gt;C&lt;/code&gt; 를 사용한 정규화 양은 과적 합을 방지 할 수 없습니다 .</target>
        </trans-unit>
        <trans-unit id="2ab99fa4c73a04aa03f4ac31b15851c1ddbe51a1" translate="yes" xml:space="preserve">
          <source>The below plot uses the first two features. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;here&lt;/a&gt; for more information on this dataset.</source>
          <target state="translated">아래 그림은 처음 두 기능을 사용합니다. 이 데이터 세트에 대한 자세한 내용 은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;여기&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="4992d67c6e1aff2322ba4c5bb3c9e0c82a7a36cf" translate="yes" xml:space="preserve">
          <source>The best model is selected by cross-validation.</source>
          <target state="translated">교차 검증에 의해 최상의 모델이 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="1a3f42d6835c4f6686d54e99d9f9cd0e3a597fec" translate="yes" xml:space="preserve">
          <source>The best performance is 1 with &lt;code&gt;normalize == True&lt;/code&gt; and the number of samples with &lt;code&gt;normalize == False&lt;/code&gt;.</source>
          <target state="translated">최고의 성능은 &lt;code&gt;normalize == True&lt;/code&gt; 1 이고 &lt;code&gt;normalize == False&lt;/code&gt; 인 샘플 수입니다 .</target>
        </trans-unit>
        <trans-unit id="4b2ab3d16cd4ed6ed82ae137ea7b0ef6d79ff832" translate="yes" xml:space="preserve">
          <source>The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.</source>
          <target state="translated">가장 좋은 p- 값은 1 / (n_permutations + 1)이고 최악은 1.0입니다.</target>
        </trans-unit>
        <trans-unit id="ed9ac623a873633695e926c14de64904b54167f4" translate="yes" xml:space="preserve">
          <source>The best possible score is 1.0, lower values are worse.</source>
          <target state="translated">가장 좋은 점수는 1.0이며 값이 낮을수록 나쁩니다.</target>
        </trans-unit>
        <trans-unit id="9230aaef9f0a1fa5e57a82e7f44b36cb5d04d36e" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.</source>
          <target state="translated">가장 좋은 값은 1이고 가장 나쁜 값은 -1입니다. 0에 가까운 값은 겹치는 클러스터를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="6e2eac60da3011cdc30f86be85be8ffa7e5f1da5" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.</source>
          <target state="translated">가장 좋은 값은 1이고 가장 나쁜 값은 -1입니다. 0에 가까운 값은 겹치는 클러스터를 나타냅니다. 음수 값은 일반적으로 다른 군집이 더 유사하므로 표본이 잘못된 군집에 할당되었음을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="b4a689b86b5cda51ef6628ef622a0daf756424bd" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0 when &lt;code&gt;adjusted=False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;adjusted=False&lt;/code&gt; 때 가장 좋은 값은 1이고 가장 나쁜 값은 0 입니다.</target>
        </trans-unit>
        <trans-unit id="488558d8da77f986fd351608404cb6a07bd4fe58" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0.</source>
          <target state="translated">가장 좋은 값은 1이고 가장 나쁜 값은 0입니다.</target>
        </trans-unit>
        <trans-unit id="b82813aa45a878f8df07ac95972e2e5116e63bed" translate="yes" xml:space="preserve">
          <source>The bias term in the underlying linear model.</source>
          <target state="translated">기본 선형 모형의 바이어스 항입니다.</target>
        </trans-unit>
        <trans-unit id="d5675ace96b0ccc75bff7a0b67255343283bf5b9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each column.</source>
          <target state="translated">각 열의 bicluster 레이블.</target>
        </trans-unit>
        <trans-unit id="d135838b65ddeda2dac8521941211167551d33d9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each row.</source>
          <target state="translated">각 행의 bicluster 레이블.</target>
        </trans-unit>
        <trans-unit id="b216c2ddafb03b97c09e36e1d0b96ac53ff1f7ba" translate="yes" xml:space="preserve">
          <source>The bins have identical widths.</source>
          <target state="translated">저장소의 너비는 동일합니다.</target>
        </trans-unit>
        <trans-unit id="c0eb1458ca2a62b6254994f184c01ee65afc52b4" translate="yes" xml:space="preserve">
          <source>The bins have the same number of samples and depend on &lt;code&gt;y_prob&lt;/code&gt;.</source>
          <target state="translated">Bin은 동일한 수의 샘플을 &lt;code&gt;y_prob&lt;/code&gt; 의존합니다 .</target>
        </trans-unit>
        <trans-unit id="4f153639172a63c8dba2ed27e16715aaa100936d" translate="yes" xml:space="preserve">
          <source>The bipartite structure allows for the use of efficient block Gibbs sampling for inference.</source>
          <target state="translated">이분자 구조는 추론을 위해 효율적인 블록 Gibbs 샘플링을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a902e8cbc26950a56283f916fe814a1e5f8ed790" translate="yes" xml:space="preserve">
          <source>The bottleneck of a gradient boosting procedure is building the decision trees. Building a traditional decision tree (as in the other GBDTs &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;) requires sorting the samples at each node (for each feature). Sorting is needed so that the potential gain of a split point can be computed efficiently. Splitting a single node has thus a complexity of \(\mathcal{O}(n_\text{features} \times n \log(n))\) where \(n\) is the number of samples at the node.</source>
          <target state="translated">그래디언트 부스팅 절차의 병목 현상은 의사 결정 트리를 구축하는 것입니다. 기존 의사 결정 트리 (다른 GBDT &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; 에서&lt;/a&gt; 와 같이 )를 구축하려면 각 노드 (각 기능에 대해)에서 샘플을 정렬해야합니다. 분할 점의 잠재적 이득을 효율적으로 계산할 수 있도록 정렬이 필요합니다. 따라서 단일 노드를 분할하면 \ (\ mathcal {O} (n_ \ text {features} \ times n \ log (n)) \)의 복잡성이 있습니다. 여기서 \ (n \)은 노드의 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="b104bf424ac624d7987a6846ffafb73fcb6a3323" translate="yes" xml:space="preserve">
          <source>The bounding box for each cluster center when centers are generated at random.</source>
          <target state="translated">중심이 무작위로 생성 될 때 각 클러스터 중심의 경계 상자입니다.</target>
        </trans-unit>
        <trans-unit id="e73634a1f979122b85e5e2e94c3d89b631e61d63" translate="yes" xml:space="preserve">
          <source>The boxes represent repeated sampling.</source>
          <target state="translated">상자는 반복 된 샘플링을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="de87510aaa8a4634fe7b670444f1141d0b934e04" translate="yes" xml:space="preserve">
          <source>The breast cancer dataset is a classic and very easy binary classification dataset.</source>
          <target state="translated">유방암 데이터 세트는 고전적이고 매우 쉬운 이진 분류 데이터 세트입니다.</target>
        </trans-unit>
        <trans-unit id="2045b20286b76d60e5fa1f74a1df8fc02a45c66b" translate="yes" xml:space="preserve">
          <source>The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the &amp;ldquo;calibration&amp;rdquo; of a set of probabilistic predictions.</source>
          <target state="translated">치어 점수 손실도 0에서 1 사이이며 점수가 낮을수록 (평균 제곱 차이가 작을수록) 예측이 더 정확합니다. 그것은 확률 론적 예측의 &quot;보정&quot;의 척도로서 생각 될 수있다.</target>
        </trans-unit>
        <trans-unit id="b76bd58217534fc8775d360bccb78e720219d44b" translate="yes" xml:space="preserve">
          <source>The calibration is based on the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; method of the &lt;code&gt;base_estimator&lt;/code&gt; if it exists, else on &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="translated">교정은에 기초 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function에서&lt;/a&gt; 의 방법 &lt;code&gt;base_estimator&lt;/code&gt; 는 다른에 있으면 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="6452eadf0058adcc76ac85d9ade38659e522fb0c" translate="yes" xml:space="preserve">
          <source>The calibration of the model can be assessed by plotting the mean observed value vs the mean predicted value on groups of test samples binned by predicted risk.</source>
          <target state="translated">모델의 보정은 예측 위험으로 분류 된 테스트 샘플 그룹에 대해 평균 관측 값 대 평균 예측값을 플로팅하여 평가할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="74481417cca6e6adb9f25dfbb8e31b8fc201eb01" translate="yes" xml:space="preserve">
          <source>The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function.</source>
          <target state="translated">역변환에 사용할 수 있습니다. 이것은 args와 kwargs가 전달 된 역변환과 같은 인수로 전달됩니다. inverse_func가 None이면 inverse_func는 항등 함수입니다.</target>
        </trans-unit>
        <trans-unit id="81ea3433c521d90153f147dfae64d0df290c9757" translate="yes" xml:space="preserve">
          <source>The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function.</source>
          <target state="translated">변환에 사용할 호출 가능 이것은 args와 kwargs를 전달하여 transform과 동일한 인수로 전달됩니다. func가 None이면 func은 항등 함수입니다.</target>
        </trans-unit>
        <trans-unit id="a1419fd6aa69dd9ba89b3d48a7257eafd52e186a" translate="yes" xml:space="preserve">
          <source>The categorical Naive Bayes classifier is suitable for classification with discrete features that are categorically distributed. The categories of each feature are drawn from a categorical distribution.</source>
          <target state="translated">범주 형 Naive Bayes 분류기는 범주별로 분포 된 이산 기능을 사용하여 분류하는 데 적합합니다. 각 기능의 범주는 범주 분포에서 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="1f38539ea641dd8b50f2f7d85a7f466ba670ba50" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;).</source>
          <target state="translated">피팅하는 동안 결정된 각 지형지 물의 범주 (X의 지형지 물 순서 및 &lt;code&gt;transform&lt;/code&gt; 의 출력에 해당 ).</target>
        </trans-unit>
        <trans-unit id="12b8e6304a220ce13ab40041e41390ae67d86b4e" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;). This includes the category specified in &lt;code&gt;drop&lt;/code&gt; (if any).</source>
          <target state="translated">피팅하는 동안 결정된 각 기능의 범주 (X의 기능 순서 및 &lt;code&gt;transform&lt;/code&gt; 의 출력에 해당 ). 여기에는 &lt;code&gt;drop&lt;/code&gt; (있는 경우)에 지정된 범주가 포함됩니다 .</target>
        </trans-unit>
        <trans-unit id="11dc40b09acf079281dfd9a81c3951e435409b7d" translate="yes" xml:space="preserve">
          <source>The centers of each cluster. Only returned if &lt;code&gt;return_centers=True&lt;/code&gt;.</source>
          <target state="translated">각 클러스터의 중심. &lt;code&gt;return_centers=True&lt;/code&gt; 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="9d4177cfd25fe2d42f6a09b10ba8f700ec03cc45" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is given by</source>
          <target state="translated">카이 제곱 커널은</target>
        </trans-unit>
        <trans-unit id="e654b311ef425d553f0f6705f2dd43fc95968d41" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is most commonly used on histograms (bags) of visual words.</source>
          <target state="translated">카이 제곱 커널은 시각적 단어의 히스토그램 (가방)에서 가장 일반적으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="085e153138bf3768d8fc53856c1f1238c6d3caff" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt;&lt;code&gt;chi2_kernel&lt;/code&gt;&lt;/a&gt; and then passed to an &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt;:</source>
          <target state="translated">카이 제곱 커널은 컴퓨터 비전 응용 프로그램에서 비선형 SVM을 교육하는 데 매우 널리 사용됩니다. 그것은 사용하여 계산 될 수있다 &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt; &lt;code&gt;chi2_kernel&lt;/code&gt; 을&lt;/a&gt; 한 다음에 전달 &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; &lt;/a&gt; 와 함께 &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="9233e345f2318b5fc92b12dd745f5adf2df3c33c" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative. This kernel is most commonly applied to histograms.</source>
          <target state="translated">카이 제곱 커널은 X와 Y의 각 행 쌍 사이에서 계산됩니다. X와 Y는 음이 아니어야합니다. 이 커널은 히스토그램에 가장 일반적으로 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="a8ba729b106653acd0bc97ad998e8dac7b2b635e" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is given by:</source>
          <target state="translated">카이 제곱 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="959a186ffdea8e09c8857bbd475b2cf453d363af" translate="yes" xml:space="preserve">
          <source>The child estimator template used to create the collection of fitted sub-estimators.</source>
          <target state="translated">피팅 된 하위 추정기 모음을 만드는 데 사용되는 하위 추정기 템플릿입니다.</target>
        </trans-unit>
        <trans-unit id="5ac1548c51d0c4529f9b6250f5d16db6b392dc40" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_features&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_features&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_features]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_features + i&lt;/code&gt;</source>
          <target state="translated">각 비 리프 노드의 자식 &lt;code&gt;n_features&lt;/code&gt; 보다 작은 값 은 원래 샘플 인 나무의 잎에 해당합니다. &lt;code&gt;n_features&lt;/code&gt; 보다 크거나 같은 노드 &lt;code&gt;i&lt;/code&gt; 는 비 리프 노드이며 자식 &lt;code&gt;children_[i - n_features]&lt;/code&gt; 있습니다. 대안으로 i 번째 반복에서, children [i] [0] 및 children [i] [1]이 병합되어 노드 &lt;code&gt;n_features + i&lt;/code&gt; 를 형성합니다.</target>
        </trans-unit>
        <trans-unit id="c8989abb78a441b49758b99ec48d8b090a3ce248" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_samples&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_samples&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_samples]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_samples + i&lt;/code&gt;</source>
          <target state="translated">각 비 리프 노드의 자식 &lt;code&gt;n_samples&lt;/code&gt; 보다 작은 값 은 원래 샘플 인 트리의 잎에 해당합니다. &lt;code&gt;n_samples&lt;/code&gt; 보다 크거나 같은 노드 &lt;code&gt;i&lt;/code&gt; 는 리프가 아닌 노드이며 자식 &lt;code&gt;children_[i - n_samples]&lt;/code&gt; 있습니다. 대안으로 i 번째 반복에서, children [i] [0] 및 children [i] [1]이 병합되어 노드 &lt;code&gt;n_samples + i&lt;/code&gt; 를 형성합니다.</target>
        </trans-unit>
        <trans-unit id="f3473fe6f2b500963d93290672a7a3ef3f212da2" translate="yes" xml:space="preserve">
          <source>The choice of features is not particularly helpful, but serves to illustrate the technique.</source>
          <target state="translated">기능 선택은 특별히 도움이되지 않지만 기술을 설명하는 데 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="15475f4de31a0dbbaa5b0396fc18010629c57dd3" translate="yes" xml:space="preserve">
          <source>The choice of the distribution depends on the problem at hand:</source>
          <target state="translated">배포판의 선택은 당면한 문제에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="1f460fe2e0e34556a3c6a8a6d017a5f63eeb9ea9" translate="yes" xml:space="preserve">
          <source>The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">각 혼합물 성분의 정밀 매트릭스의 콜레 스키 분해. 정밀 행렬은 공분산 행렬의 역수입니다. 공분산 행렬은 대칭 양수로 한정되므로 가우스 혼합은 정밀 행렬에 의해 동등하게 매개 변수화 될 수 있습니다. 공분산 행렬 대신 정밀 행렬을 저장하면 테스트 시간에 새로운 샘플의 로그 가능성을보다 효율적으로 계산할 수 있습니다. 모양은 &lt;code&gt;covariance_type&lt;/code&gt; 에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="8e7b3855f3de3292a39f43c9800aebcf16bc77cb" translate="yes" xml:space="preserve">
          <source>The claim &lt;strong&gt;frequency&lt;/strong&gt; is the number of claims divided by the exposure, typically measured in number of claims per year.</source>
          <target state="translated">클레임 &lt;strong&gt;빈도&lt;/strong&gt; 는 일반적으로 연간 클레임 수로 측정되는 클레임 ​​수를 노출로 나눈 값입니다.</target>
        </trans-unit>
        <trans-unit id="54b709e28c0bc302f278f75cf2f9281781fc40ea" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; used with the optional parameter &lt;code&gt;svd_solver='randomized'&lt;/code&gt; is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.</source>
          <target state="translated">선택적 매개 변수 &lt;code&gt;svd_solver='randomized'&lt;/code&gt; 와 함께 사용되는 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 클래스 는이 경우에 매우 유용합니다. 대부분의 특이 벡터를 제거 할 것이기 때문에 계산을 단일 벡터의 대략적인 추정치로 제한하는 것이 훨씬 효율적입니다. 실제로 변환을 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="9afc238719eb2575519db8e476eb36c39ec071a2" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt;&lt;code&gt;DictVectorizer&lt;/code&gt;&lt;/a&gt; can be used to convert feature arrays represented as lists of standard Python &lt;code&gt;dict&lt;/code&gt; objects to the NumPy/SciPy representation used by scikit-learn estimators.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt; &lt;code&gt;DictVectorizer&lt;/code&gt; &lt;/a&gt; 클래스를 사용하면 표준 Python &lt;code&gt;dict&lt;/code&gt; 객체 목록으로 표시되는 피처 배열 을 scikit-learn 추정기에서 사용하는 NumPy / SciPy 표현 으로 변환 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0b3d7d58279a2952a288a94db8a135a534d78ec7" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; is a high-speed, low-memory vectorizer that uses a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;feature hashing&lt;/a&gt;, or the &amp;ldquo;hashing trick&amp;rdquo;. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no &lt;code&gt;inverse_transform&lt;/code&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt; 클래스 는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;기능 해싱&lt;/a&gt; 또는 &quot;해싱 트릭&quot; 이라고하는 기술을 사용하는 고속 저 메모리 벡터 라이저 입니다 . 벡터화 프로그램과 마찬가지로 교육에서 발생하는 기능의 해시 테이블을 작성하는 대신 &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt; 인스턴스는 기능에 해시 함수를 적용하여 샘플 행렬에서 열 인덱스를 직접 결정합니다. 결과적으로 검사 성을 희생하면서 속도를 높이고 메모리 사용량을 줄입니다. hasher는 입력 기능이 어떻게 생겼는지 기억하지 &lt;code&gt;inverse_transform&lt;/code&gt; 메소드 가 없습니다 .</target>
        </trans-unit>
        <trans-unit id="46148e4d160f9510def06597f1921af425f74fcb" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing function to data. It solves the following problem:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; &lt;/a&gt; 클래스 는 비 감소 함수를 데이터에 맞 춥니 다. 다음과 같은 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="0773ec8e22bf216d4df607f3329b18b1168f7c09" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing real function to 1-dimensional data. It solves the following problem:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; &lt;/a&gt; 클래스 는 감소하지 않는 실수 함수를 1 차원 데이터에 맞 춥니 다. 다음 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="04846022f8485d75461ddcb301a4717897728672" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; implements this component wise deterministic sampling. Each component is sampled \(n\) times, yielding \(2n+1\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \(n\) is usually chosen to be 1 or 2, transforming the dataset to size &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (in the case of \(n=2\)).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt; 클래스 는이 컴포넌트의 현명한 결정적 샘플링을 구현합니다. 각 구성 요소는 \ (n \) 번 샘플링되어 입력 차원 당 \ (2n + 1 \) 차원을 생성합니다 (두 개의 배수는 푸리에 변환의 실수 부분과 복잡한 부분에서 유래 함). 문헌에서 \ (n \)은 일반적으로 1 또는 2로 선택되어 데이터 세트를 크기 &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (\ (n = 2 \)의 경우)로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="9fdc0ed638cb0889fa5320ee5ada72b80a16402f" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt;&lt;code&gt;ElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt; &lt;code&gt;ElasticNetCV&lt;/code&gt; &lt;/a&gt; 클래스를 사용하여 교차 유효성 검사를 통해 &lt;code&gt;alpha&lt;/code&gt; (\ (\ alpha \)) 및 &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ rho \)) 매개 변수를 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c89182d0633b09742e745213024a53ccec4fee0a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt;&lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt; &lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt; &lt;/a&gt; 클래스를 사용하여 교차 유효성 검증을 통해 매개 변수 &lt;code&gt;alpha&lt;/code&gt; (\ (\ alpha \)) 및 &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ rho \)) 를 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6c66ee2c7a53fbda2b2b2da602d1e782a1fcc1b3" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 클래스 는 1 차 SGD 학습 루틴을 구현합니다. 알고리즘은 학습 예제를 반복하고 각 예제마다 다음에 의해 제공된 업데이트 규칙에 따라 모델 매개 변수를 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="f128e57d09cf8b511f002f70c40429b250044323" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 클래스 는 분류에 대한 다양한 손실 함수 및 처벌을 지원하는 일반 확률 적 경사 하강 학습 루틴을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="3fc72b8fffc5a512701e2a944bd472c169bc1771" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; trained with the hinge loss, equivalent to a linear SVM.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 클래스 는 분류에 대한 다양한 손실 함수와 페널티를 지원하는 일반 확률 적 경사 하강 법 학습 루틴을 구현합니다. 다음은 선형 SVM에 해당하는 힌지 손실로 훈련 된 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 의 결정 경계입니다 .</target>
        </trans-unit>
        <trans-unit id="9be5b4c1c13de0290ee1bddf8fed519138b844e9" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; is well suited for regression problems with a large number of training samples (&amp;gt; 10.000), for other problems we recommend &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 클래스 는 선형 손실 회귀 모델에 맞게 다양한 손실 함수 및 위약금을 지원하는 일반 확률 적 경사 하강 학습 루틴을 구현합니다. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 는 많은 훈련 샘플 (&amp;gt; 10.000)의 회귀 문제에 적합하며 &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt; &lt;code&gt;ElasticNet&lt;/code&gt; 을&lt;/a&gt; 권장하는 다른 문제에 적합 합니다.</target>
        </trans-unit>
        <trans-unit id="5ca16e4c08c0574f508b6c526beeb2111eade54a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;OneClassSVM&lt;/code&gt;&lt;/a&gt; implements a One-Class SVM which is used in outlier detection.</source>
          <target state="translated">클래스 &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;OneClassSVM&lt;/code&gt; 의&lt;/a&gt; 구현 이상치 탐지에 사용되는 하나의 클래스 SVM.</target>
        </trans-unit>
        <trans-unit id="5eaffd69df806c5b8353c88358cfd0a49b275168" translate="yes" xml:space="preserve">
          <source>The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in &lt;code&gt;gbrt.classes_&lt;/code&gt;.</source>
          <target state="translated">PDP를 계산할 클래스 레이블입니다. gbrt가 다중 클래스 모델 인 경우에만 해당됩니다. &lt;code&gt;gbrt.classes_&lt;/code&gt; 에 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="2a31ab08188f24bd31e99f218cab5467eddf2ef0" translate="yes" xml:space="preserve">
          <source>The class labels.</source>
          <target state="translated">클래스 라벨.</target>
        </trans-unit>
        <trans-unit id="7c57202048886b9c3874e17b392c104a7a21da98" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="translated">입력 샘플의 클래스 로그 확률입니다. 클래스의 순서는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; 속성의 순서와 일치합니다 .</target>
        </trans-unit>
        <trans-unit id="075fd1f474c95d9f3895b75ead4b1f9c99be54a8" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">입력 샘플의 클래스 로그 확률. 클래스의 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성의 순서와 일치합니다 .</target>
        </trans-unit>
        <trans-unit id="b5b69c523a5547b26d366d35f11f7dab77d71024" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. The smaller \(\nu\), the less smooth the approximated function is. As \(\nu\rightarrow\infty\), the kernel becomes equivalent to the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel. When \(\nu = 1/2\), the Mat&amp;eacute;rn kernel becomes identical to the absolute exponential kernel. Important intermediate values are \(\nu=1.5\) (once differentiable functions) and \(\nu=2.5\) (twice differentiable functions).</source>
          <target state="translated">Matern 커널의 클래스는 &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt; 의 일반화입니다 . 결과 함수의 부드러움을 제어하는 ​​추가 매개 변수 \ (\ nu \)가 있습니다. \ (\ nu \)가 작을수록 근사 함수가 덜 부드럽습니다. \ (\ nu \ rightarrow \ infty \)로 커널은 &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt; 커널 과 동일합니다 . \ (\ nu = 1/2 \) 일 때 Mat&amp;eacute;rn 커널은 절대 지수 커널과 동일하게됩니다. 중요한 중간 값은 \ (\ nu = 1.5 \) (한 번 미분 할 수있는 함수) 및 \ (\ nu = 2.5 \) (두 번의 미분 할 수있는 함수)입니다.</target>
        </trans-unit>
        <trans-unit id="f9e49144e04860d9a1f4a8512901a503e5d41c6f" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the RBF and the absolute exponential kernel parameterized by an additional parameter nu. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).</source>
          <target state="translated">Matern 커널 클래스는 RBF의 일반화이며 추가 매개 변수 nu로 매개 변수화 된 절대 지수 커널입니다. nu가 작을수록 근사 함수의 부드러움이 줄어 듭니다. nu = inf의 경우 커널은 RBF 커널과 같고 nu = 0.5의 경우 절대 지수 커널과 같습니다. 중요한 중간 값은 nu = 1.5 (한 번 차별화 함수)와 nu = 2.5 (두 번 차별화 함수)입니다.</target>
        </trans-unit>
        <trans-unit id="e48205f573bda857c27ec3937eb80960daed3556" translate="yes" xml:space="preserve">
          <source>The class ordering is preserved:</source>
          <target state="translated">수업 순서는 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="0401a90c4f50bc0cacd4ce11e3ddd76a2d543405" translate="yes" xml:space="preserve">
          <source>The class prior probabilities. By default, the class proportions are inferred from the training data.</source>
          <target state="translated">클래스 사전 확률입니다. 기본적으로 클래스 비율은 훈련 데이터에서 추론됩니다.</target>
        </trans-unit>
        <trans-unit id="e9e493bffc66549584cbf656024fe268e745fcd4" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples.</source>
          <target state="translated">입력 샘플의 클래스 확률입니다.</target>
        </trans-unit>
        <trans-unit id="6a7a001ab832443f9ea75be4af1e68e4b9eb028c" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute.</source>
          <target state="translated">입력 샘플의 클래스 확률입니다. 출력 순서는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; 속성 의 순서와 동일 합니다.</target>
        </trans-unit>
        <trans-unit id="81478ef84d2d645ae9c4e632247f7f3bc9052a92" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute.</source>
          <target state="translated">입력 샘플의 클래스 확률. 출력 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성 의 순서와 동일 합니다.</target>
        </trans-unit>
        <trans-unit id="eb02560f00e2596a11150c32ba96675012f41151" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="translated">입력 샘플의 클래스 확률입니다. 클래스의 순서는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; 속성의 순서와 일치합니다 .</target>
        </trans-unit>
        <trans-unit id="8ea2403e3ddde25cdaa7213df112f25928f5232d" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">입력 샘플의 클래스 확률. 클래스의 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성의 순서와 일치합니다 .</target>
        </trans-unit>
        <trans-unit id="64b22dfb54911895a611d3b9cfbaf3a3f14d97f5" translate="yes" xml:space="preserve">
          <source>The class to report if &lt;code&gt;average='binary'&lt;/code&gt; and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting &lt;code&gt;labels=[pos_label]&lt;/code&gt; and &lt;code&gt;average != 'binary'&lt;/code&gt; will report scores for that label only.</source>
          <target state="translated">&lt;code&gt;average='binary'&lt;/code&gt; 이고 데이터가 이진 인지보고 할 클래스 입니다. 데이터가 멀티 클래스 또는 멀티 라벨 인 경우 무시됩니다. 설정 &lt;code&gt;labels=[pos_label]&lt;/code&gt; 및 &lt;code&gt;average != 'binary'&lt;/code&gt; 라벨에만 점수를보고합니다.</target>
        </trans-unit>
        <trans-unit id="b02b6e7e5cca0ff24c2691f6e7d2bc9c247d17d1" translate="yes" xml:space="preserve">
          <source>The class to use to build the returned adjacency matrix.</source>
          <target state="translated">리턴 된 인접 행렬을 빌드하는 데 사용할 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="08bf02b9f7a917eed20c9c94c384791c0ed4578f" translate="yes" xml:space="preserve">
          <source>The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.</source>
          <target state="translated">우리가 일대일 적합을 수행하는 클래스. None이면 주어진 문제가 이진 인 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="26dc8660d1112d77620f09027ce7b9fbee6027a3" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; fits a logistic regression model, while with &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; it fits a linear support vector machine (SVM).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 클래스는 서로 다른 (볼록한) 손실 함수와 서로 다른 위약금을 사용하여 분류 및 회귀에 대한 선형 모델에 맞는 기능을 제공합니다. 예를 들어 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; 로 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 는 로지스틱 회귀 모델에 적합하고 &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; 로 선형 지원 벡터 머신 (SVM)에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="d29e2c71b01d97c9c93dc5aad937ec38e91b7ca9" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide two criteria to stop the algorithm when a given level of convergence is reached:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 클래스 는 주어진 수렴 수준에 도달하면 알고리즘을 중지하는 두 가지 기준을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="d7ef8ada32aed500913b0cf053368e18ea306f33" translate="yes" xml:space="preserve">
          <source>The classes in &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can handle either NumPy arrays or &lt;code&gt;scipy.sparse&lt;/code&gt; matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt; 의 클래스는 NumPy 배열 또는 &lt;code&gt;scipy.sparse&lt;/code&gt; 행렬을 입력으로 처리 할 수 ​​있습니다 . 밀도가 높은 매트릭스의 경우 가능한 많은 거리 메트릭이 지원됩니다. 희소 행렬의 경우 검색을 위해 임의의 Minkowski 메트릭이 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="f39d71815c55889dd46cb64855cc1d40f837d2d9" translate="yes" xml:space="preserve">
          <source>The classes in the &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators&amp;rsquo; accuracy scores or to boost their performance on very high-dimensional datasets.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; &lt;/a&gt; 모듈 의 클래스는 샘플 세트의 기능 선택 / 차원 감소에 사용되어 추정기의 정확도 점수를 높이거나 매우 높은 차원의 데이터 집합에서 성능을 향상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="df672af473b055efb339763c1ea56e27edeec8c9" translate="yes" xml:space="preserve">
          <source>The classes in this submodule allow to approximate the embedding \(\phi\), thereby working explicitly with the representations \(\phi(x_i)\), which obviates the need to apply the kernel or store training examples.</source>
          <target state="translated">이 서브 모듈의 클래스는 임베드 \ (\ phi \)를 근사화하여 \ (\ phi (x_i) \) 표현과 함께 명시 적으로 작동하므로 커널을 적용하거나 교육 예제를 저장할 필요가 없습니다.</target>
        </trans-unit>
        <trans-unit id="41647bba96e968e0bd58965ae65f2bc0365d42c6" translate="yes" xml:space="preserve">
          <source>The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</source>
          <target state="translated">클래스 레이블 (단일 출력 문제) 또는 클래스 레이블 배열 목록 (다중 출력 문제)</target>
        </trans-unit>
        <trans-unit id="772daeb9b0eaa408b264b0fa548b1cbd24779805" translate="yes" xml:space="preserve">
          <source>The classes labels.</source>
          <target state="translated">클래스 레이블.</target>
        </trans-unit>
        <trans-unit id="13f18ce429d97546a8b16cf132e8e6c03147cb50" translate="yes" xml:space="preserve">
          <source>The classic implementation of the clustering method based on the Lloyd&amp;rsquo;s algorithm. It consumes the whole set of input data at each iteration.</source>
          <target state="translated">로이드 알고리즘을 기반으로 한 클러스터링 방법의 고전적인 구현. 각 반복에서 전체 입력 데이터 세트를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="b3e4f700832d6cdb3a215961499bbf68e1aa40b8" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">분류는 시각화 목적으로 PCA 및 CCA에서 발견 한 처음 두 개의 주요 구성 요소로 투영 한 다음 선형 커널이있는 두 개의 SVC를 사용하여 각 클래스에 대한 차별 모델을 학습하는 &lt;a href=&quot;../../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt; 메타 분류자를 사용하여 수행됩니다 . PCA는 감독되지 않은 차원 감소를 수행하는 데 사용되는 반면 CCA는 감독되는 차원 감소를 수행하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="4b0c76ea0ae326d19f535ab9520b9dd024cf9124" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">분류는 시각화 목적으로 PCA와 CCA에서 찾은 첫 번째 두 가지 주요 구성 요소에 투영 한 다음 선형 커널이있는 두 개의 SVC를 사용하여 각 클래스의 차별적 모델을 배우는 &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt; 메타 분류기 를 사용하여 수행됩니다 . PCA는 감독되지 않은 차원 축소를 수행하는 데 사용되고 CCA는 감독 된 차원 축소를 수행하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="3f7b4b923ee0ab9dbf257d91692081f3d6b6945f" translate="yes" xml:space="preserve">
          <source>The classification target. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas Series.</source>
          <target state="translated">분류 대상입니다. 경우 &lt;code&gt;as_frame=True&lt;/code&gt; , &lt;code&gt;target&lt;/code&gt; 팬더 시리즈가 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="7c83c4f2209684d200eb3e2732a4357005bab63e" translate="yes" xml:space="preserve">
          <source>The classifier which predicts given the output of &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;estimators_&lt;/code&gt; 의 출력을 예측하는 분류기입니다 .</target>
        </trans-unit>
        <trans-unit id="d9bcc8debd316bfd1c4b9e73630f6ba7215b9e3d" translate="yes" xml:space="preserve">
          <source>The classifier whose output decision function needs to be calibrated to offer more accurate predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.</source>
          <target state="translated">보다 정확한 predict_proba 출력을 제공하기 위해 출력 결정 기능을 교정해야하는 분류기. cv = prefit 인 경우 분류기는 이미 데이터에 적합해야합니다.</target>
        </trans-unit>
        <trans-unit id="4188695350bfd2d35a500d1eee9968d78806da89" translate="yes" xml:space="preserve">
          <source>The classifier whose output need to be calibrated to provide more accurate &lt;code&gt;predict_proba&lt;/code&gt; outputs.</source>
          <target state="translated">보다 정확한 &lt;code&gt;predict_proba&lt;/code&gt; 출력 을 제공하기 위해 출력을 보정해야하는 분류기입니다 .</target>
        </trans-unit>
        <trans-unit id="917e6ef2892859cfeed06565c1a2e19769195da1" translate="yes" xml:space="preserve">
          <source>The cluster ordered list of sample indices.</source>
          <target state="translated">샘플 인덱스의 클러스터 순서 목록입니다.</target>
        </trans-unit>
        <trans-unit id="88e39a3dfd7087282361f3b39109354f7ff32ede" translate="yes" xml:space="preserve">
          <source>The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs.</source>
          <target state="translated">아래 코드는 예측의 구성과 계산이 여러 작업 내에서 병렬화되는 방법도 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="6a1cc4b5491e9b42fd445850086762fcb248e836" translate="yes" xml:space="preserve">
          <source>The code below plots the dependency of y against individual x_i and normalized values of univariate F-tests statistics and mutual information.</source>
          <target state="translated">아래 코드는 개별 x_i에 대한 y의 종속성과 일 변량 F- 검정 통계량 및 상호 정보의 정규화 된 값을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="4b0a65eb61a5277b5e28aef5c3b0e7ea577bdc07" translate="yes" xml:space="preserve">
          <source>The code-examples in the above tutorials are written in a &lt;em&gt;python-console&lt;/em&gt; format. If you wish to easily execute these examples in &lt;strong&gt;IPython&lt;/strong&gt;, use:</source>
          <target state="translated">위 튜토리얼의 코드 예제는 &lt;em&gt;python-console&lt;/em&gt; 형식으로 작성되었습니다 . &lt;strong&gt;IPython&lt;/strong&gt; 에서 이러한 예제를 쉽게 실행하려면 다음을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="ce298b790cf6df954f5ce5058b04debe04deb707" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">계수 R ^ 2는 (1-u / v)로 정의되며, 여기서 u는 잔차 제곱합 ((y_true-y_pred) ** 2) .sum ()이고 v는 회귀 제곱합 ((y_true- y_true.mean ()) ** 2) .sum (). 최고 점수는 1.0이며 음수가 될 수 있습니다 (모델이 임의로 악화 될 수 있기 때문에). 입력 특성을 무시하고 항상 y의 예상 값을 예측하는 상수 모델은 0.0의 R ^ 2 점수를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="48121e6567bc65fc6ea38acd7d461ac73e2ea472" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">계수 R ^ 2는 (1-u / v)로 정의되며, 여기서 u는 잔차 제곱합 ((y_true-y_pred) ** 2) .sum ()이고 v는 총 제곱합 ((y_true- y_true.mean ()) ** 2) .sum (). 최고 점수는 1.0이며 음수가 될 수 있습니다 (모델이 임의로 악화 될 수 있기 때문에). 입력 특성을 무시하고 항상 y의 예상 값을 예측하는 상수 모델은 0.0의 R ^ 2 점수를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="a7b8f1c4eb5f60ca5d7c57d0dcd2ad470def6e62" translate="yes" xml:space="preserve">
          <source>The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="translated">Ordinary Least Squares에 대한 계수 추정치는 특성의 독립성에 의존합니다. 특성이 상관되고 설계 행렬 \ (X \)의 열이 근사적인 선형 의존성을 가질 때, 설계 행렬은 특이점에 가까워지고 결과적으로 최소 제곱 추정값은 관측 된 대상의 임의 오류에 매우 민감 해집니다. 큰 차이를 생성합니다. 이러한 &lt;em&gt;다중 공선 성&lt;/em&gt; 상황은 예를 들어 실험 설계없이 데이터를 수집 할 때 발생할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f0dabbd241b37afcd99edc97579580c22f2703f9" translate="yes" xml:space="preserve">
          <source>The coefficient of the underlying linear model. It is returned only if coef is True.</source>
          <target state="translated">기본 선형 모형의 계수입니다. coef가 True 인 경우에만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="8b0531322a0447d32da022aa29f51bdb5853bf2b" translate="yes" xml:space="preserve">
          <source>The coefficients \(w\) of the model can be accessed:</source>
          <target state="translated">모델의 계수 \ (w \)에 액세스 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8bf2184b15d694a8d6ef65832d1a84477f9388bd" translate="yes" xml:space="preserve">
          <source>The coefficients are significantly different. AGE and EXPERIENCE coefficients are both positive but they now have less influence on the prediction.</source>
          <target state="translated">계수가 크게 다릅니다. AGE 및 EXPERIENCE 계수는 모두 양수이지만 이제 예측에 미치는 영향이 적습니다.</target>
        </trans-unit>
        <trans-unit id="b2933da174c8fa332cc9ad841c0c25a1ab1996d2" translate="yes" xml:space="preserve">
          <source>The coefficients can be forced to be positive.</source>
          <target state="translated">계수는 양수로 강제 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3c89d0ba57b5481659a50cc73b873694c355fd9b" translate="yes" xml:space="preserve">
          <source>The coefficients of the linear model: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</source>
          <target state="translated">선형 모형의 계수 : &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="5395d49e7d69c07e1b2416a99bbd87627621ae2f" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the coefficient of determination are also calculated.</source>
          <target state="translated">계수, 잔차 제곱합 및 결정 계수도 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="a7757730afe97cb8593dea4757d82727872c7529" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the variance score are also calculated.</source>
          <target state="translated">계수, 잔차 제곱합 및 분산 점수도 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="f15e1f2c7fb96cd43c32056a6c407c9c1c570883" translate="yes" xml:space="preserve">
          <source>The collection of fitted base estimators.</source>
          <target state="translated">적합한 기본 추정기 모음입니다.</target>
        </trans-unit>
        <trans-unit id="4d0721afe1a467c5d2eb2443395ac7ab7a4e3f8b" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &amp;lsquo;drop&amp;rsquo;.</source>
          <target state="translated">항에 장착 서브 추정기의 컬렉션 &lt;code&gt;estimators&lt;/code&gt; '드롭'아니다.</target>
        </trans-unit>
        <trans-unit id="e97d4f2642ef3372648a38fef3350e771647c3d6" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; 이 아닌 &lt;code&gt;estimators&lt;/code&gt; 정의 된 적합 추정치 모음입니다 .</target>
        </trans-unit>
        <trans-unit id="0b6e390487ac628e8895818624f2b32fd0c95d36" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators.</source>
          <target state="translated">적합 추정치 모음입니다.</target>
        </trans-unit>
        <trans-unit id="929cef17b33a8aadbb4cff8f87d4813d9e794dca" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators. &lt;code&gt;loss_.K&lt;/code&gt; is 1 for binary classification, otherwise n_classes.</source>
          <target state="translated">적합 추정치 모음입니다. &lt;code&gt;loss_.K&lt;/code&gt; 는 이진 분류의 경우 1이고, 그렇지 않으면 n_classes입니다.</target>
        </trans-unit>
        <trans-unit id="34213cac4f6d6096a4a72655d4c46d8da28f907c" translate="yes" xml:space="preserve">
          <source>The collection of fitted transformers as tuples of (name, fitted_transformer, column). &lt;code&gt;fitted_transformer&lt;/code&gt; can be an estimator, &amp;lsquo;drop&amp;rsquo;, or &amp;lsquo;passthrough&amp;rsquo;. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (&amp;lsquo;remainder&amp;rsquo;, transformer, remaining_columns) corresponding to the &lt;code&gt;remainder&lt;/code&gt; parameter. If there are remaining columns, then &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt;, otherwise &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt;.</source>
          <target state="translated">튜플 (이름, 장착 _ 변압기, 열)로 장착 된 변압기 모음입니다. &lt;code&gt;fitted_transformer&lt;/code&gt; 는 추정기, 'drop'또는 'passthrough'일 수 있습니다. 열이 선택되지 않은 경우 이것은 적합하지 않은 변압기입니다. 나머지 열이있는 경우 마지막 요소는 &lt;code&gt;remainder&lt;/code&gt; 매개 변수에 해당하는 양식 ( 'remainder', 변환기, 잔여 _ 열)의 튜플입니다 . 남은 열이 있으면 &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt; 이고, 그렇지 않으면 &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="3626dc1999e74681fe5d5d9d41be14f043b3b76d" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the image, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="translated">이미지에서 추출 된 패치 모음. 여기서 &lt;code&gt;n_patches&lt;/code&gt; 는 &lt;code&gt;max_patches&lt;/code&gt; 또는 추출 할 수있는 총 패치 수입니다.</target>
        </trans-unit>
        <trans-unit id="cd0348cc380532b01c9102e25a743ff6f1c3fc49" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the images, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;n_samples * max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="translated">이미지에서 추출 된 패치 모음. 여기서 &lt;code&gt;n_patches&lt;/code&gt; 는 &lt;code&gt;n_samples * max_patches&lt;/code&gt; 또는 추출 할 수있는 총 패치 수입니다.</target>
        </trans-unit>
        <trans-unit id="d6d471d12278a874c4225d83bbd25d7f04a0a976" translate="yes" xml:space="preserve">
          <source>The color map illustrates the decision function learned by the SVC.</source>
          <target state="translated">컬러 맵은 SVC가 학습 한 결정 기능을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="093f5986f8d055d5b47f11ad021ce6ecfcef91e9" translate="yes" xml:space="preserve">
          <source>The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.</source>
          <target state="translated">표시기 [n_nodes_ptr [i] : n_nodes_ptr [i + 1]]의 열은 i 번째 추정기에 대한 표시기 값을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="f0cca3e664aa52c0e892615c12365c0a6af40452" translate="yes" xml:space="preserve">
          <source>The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.</source>
          <target state="translated">이 예에서 사용 된 조합은이 데이터 세트에서 특별히 도움이되지 않으며 FeatureUnion의 사용법을 보여주기 위해서만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="238b85659a6f96b691291ac49250c2a48af41530" translate="yes" xml:space="preserve">
          <source>The complete set of patches. If the patches contain colour information, channels are indexed along the last dimension: RGB patches would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="translated">완전한 패치 세트. 패치에 색상 정보가 포함 된 경우 채널은 마지막 차원을 따라 인덱싱됩니다. RGB 패치는 &lt;code&gt;n_channels=3&lt;/code&gt; 을 갖 습니다 .</target>
        </trans-unit>
        <trans-unit id="b4849a6d39c196dd952c4e98da8a227fce685751" translate="yes" xml:space="preserve">
          <source>The complexity parameter \(\alpha \geq 0\) controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="translated">복잡도 매개 변수 \ (\ alpha \ geq 0 \)는 수축량을 제어합니다. \ (\ alpha \) 값이 클수록 수축량이 커지므로 계수가 공선성에 더 강해집니다.</target>
        </trans-unit>
        <trans-unit id="4524a6f67fb21e2d5fb712c219044b7460dbad0e" translate="yes" xml:space="preserve">
          <source>The components of the random matrix are drawn from N(0, 1 / n_components).</source>
          <target state="translated">랜덤 행렬의 성분은 N (0, 1 / n_components)에서 추출됩니다.</target>
        </trans-unit>
        <trans-unit id="4d179dd17ec2e9e8ac79ee8fb26a8b8727033655" translate="yes" xml:space="preserve">
          <source>The compromise between l1 and l2 penalization chosen by cross validation</source>
          <target state="translated">교차 검증에 의해 선택된 l1과 l2 불이익의 절충</target>
        </trans-unit>
        <trans-unit id="37c8e8aa42af430070b2b87be4c5db5db294cc7b" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;fit&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 중 계산 은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cc728704a5b1deaa221b61ada3dd42ad8e661fbd" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;predict&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;predict&lt;/code&gt; 중 계산 은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cc5659751ed96e6a9a511b247760eba7bd9fab63" translate="yes" xml:space="preserve">
          <source>The computation of Davies-Bouldin is simpler than that of Silhouette scores.</source>
          <target state="translated">Davies-Bouldin의 계산은 Silhouette 점수의 계산보다 간단합니다.</target>
        </trans-unit>
        <trans-unit id="d526bd934b93818cdf8a3fccada8e6b2b34d84f2" translate="yes" xml:space="preserve">
          <source>The computational overhead of each SVD is &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt;, but only 2 * batch_size samples remain in memory at a time. There will be &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD computations to get the principal components, versus 1 large SVD of complexity &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; for PCA.</source>
          <target state="translated">각 SVD의 계산 오버 헤드는 &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt; 이지만 한 번에 2 * batch_size 샘플 만 메모리에 남아 있습니다. 주요 구성 요소를 얻기위한 &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD 계산 이 있으며 PCA 의 경우 복잡한 &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; 의 1 개의 큰 SVD가 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2fa163515de812b4d78a2bbcef87e239a36ae4d1" translate="yes" xml:space="preserve">
          <source>The concept of early stopping is simple. We specify a &lt;code&gt;validation_fraction&lt;/code&gt; which denotes the fraction of the whole dataset that will be kept aside from training to assess the validation loss of the model. The gradient boosting model is trained using the training set and evaluated using the validation set. When each additional stage of regression tree is added, the validation set is used to score the model. This is continued until the scores of the model in the last &lt;code&gt;n_iter_no_change&lt;/code&gt; stages do not improve by atleast &lt;code&gt;tol&lt;/code&gt;. After that the model is considered to have converged and further addition of stages is &amp;ldquo;stopped early&amp;rdquo;.</source>
          <target state="translated">조기 중지의 개념은 간단합니다. 모델의 유효성 검증 손실을 평가하기 위해 학습과 별도로 유지 될 전체 데이터 세트의 비율을 나타내는 &lt;code&gt;validation_fraction&lt;/code&gt; 을 지정합니다 . 그라디언트 부스팅 모델은 훈련 세트를 사용하여 훈련되고 검증 세트를 사용하여 평가됩니다. 회귀 트리의 각 추가 단계가 추가되면 유효성 검사 세트가 모델 스코어링에 사용됩니다. 이것은 마지막 &lt;code&gt;n_iter_no_change&lt;/code&gt; 단계 에서 모델의 점수 가 적어도 &lt;code&gt;tol&lt;/code&gt; 에 의해 향상되지 않을 때까지 계속 됩니다 . 그 후 모델이 수렴 된 것으로 간주되고 단계의 추가가 &quot;조기 중지&quot;됩니다.</target>
        </trans-unit>
        <trans-unit id="feb60f640c62fd0856cb50e8401267d4f55d4774" translate="yes" xml:space="preserve">
          <source>The concrete &lt;code&gt;LossFunction&lt;/code&gt; object.</source>
          <target state="translated">구체적인 &lt;code&gt;LossFunction&lt;/code&gt; 객체입니다.</target>
        </trans-unit>
        <trans-unit id="4f21e1340272b60cf06a69153cc19d0140d625cf" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">콘크리트 손실 기능은 &lt;code&gt;loss&lt;/code&gt; 매개 변수 를 통해 설정할 수 있습니다 . &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 는 다음과 같은 손실 함수를 지원합니다.</target>
        </trans-unit>
        <trans-unit id="c24a50928ec0729629e45e8b53b6d8b9f0112f79" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">콘크리트 손실 기능은 &lt;code&gt;loss&lt;/code&gt; 매개 변수 를 통해 설정할 수 있습니다 . &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 는 다음과 같은 손실 기능을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="a5b98bb304bc3ec6a422167859a2f5da5580fa47" translate="yes" xml:space="preserve">
          <source>The concrete penalty can be set via the &lt;code&gt;penalty&lt;/code&gt; parameter. SGD supports the following penalties:</source>
          <target state="translated">구체적 패널티는 &lt;code&gt;penalty&lt;/code&gt; 파라미터 를 통해 설정할 수 있습니다 . SGD는 다음과 같은 처벌을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="c0c66ccf788e88932593e02308e1dcae0bb5c1f0" translate="yes" xml:space="preserve">
          <source>The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:</source>
          <target state="translated">각 유닛의 조건부 확률 분포는 다음과 같은 입력을받는 로지스틱 시그 모이 드 활성화 함수에 의해 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="6e94f7f28c2cc9bab8c93a85a4438b802d82d10d" translate="yes" xml:space="preserve">
          <source>The confidence score for a sample is the signed distance of that sample to the hyperplane.</source>
          <target state="translated">표본의 신뢰 점수는 해당 표본과 초평면 사이의 부호있는 거리입니다.</target>
        </trans-unit>
        <trans-unit id="b129f98741f30e6ae05e0872d86b02b5a921e905" translate="yes" xml:space="preserve">
          <source>The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;&lt;/a&gt; to restrict merging to nearest neighbors as in &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;this example&lt;/a&gt;, or using &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt;&lt;/a&gt; to enable only merging of neighboring pixels on an image, as in the &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;coin&lt;/a&gt; example.</source>
          <target state="translated">연결 제약 조건은 연결 매트릭스를 통해 적용됩니다. 연결해야하는 데이터 집합의 인덱스가있는 행과 열의 교차 지점에만 요소가있는 scipy 스파 스 행렬. 이 매트릭스는 사전 정보로 구성 할 수 있습니다. 예를 들어 링크가있는 페이지 만 서로 링크하여 웹 페이지를 클러스터링 할 수 있습니다. &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;예를 들어 &lt;/a&gt;&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt; &lt;/a&gt; 를 사용 하여이 예제 와 같이 가장 가까운 이웃으로의 병합을 제한 &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; &lt;/a&gt; 를 사용하여 데이터에서 배울 수 있습니다 . &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;동전&lt;/a&gt; 예.</target>
        </trans-unit>
        <trans-unit id="cba08514635a6da99fcbf046720afd85a65680ea" translate="yes" xml:space="preserve">
          <source>The constant value which defines the covariance: k(x_1, x_2) = constant_value</source>
          <target state="translated">공분산을 정의하는 상수 값 : k (x_1, x_2) = constant_value</target>
        </trans-unit>
        <trans-unit id="83836055dd86bf18d0180179c00f49511034950c" translate="yes" xml:space="preserve">
          <source>The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings.</source>
          <target state="translated">계산 된 분할 표는 일반적으로 두 군집 간의 유사성 통계 (이 문서에 나열된 다른 통계)를 계산하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="ca0505a0687635a00a190438e7d26956e48c468c" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</source>
          <target state="translated">수렴 임계 값입니다. 하한 평균 게인이이 임계 값보다 낮 으면 EM 반복이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="0e26ef32f94bda64e8f6e396c3c303a3311ac4e1" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold.</source>
          <target state="translated">수렴 임계 값입니다. 모델에 대한 훈련 데이터의 가능성에 대한 하한 평균 이득이이 임계 값보다 낮 으면 EM 반복이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="80f05388942910150044c6d47584d8bddc73ef4c" translate="yes" xml:space="preserve">
          <source>The converse mapping from feature name to column index is stored in the &lt;code&gt;vocabulary_&lt;/code&gt; attribute of the vectorizer:</source>
          <target state="translated">기능 이름에서 열 인덱스로의 대화 매핑 은 벡터 화기 의 &lt;code&gt;vocabulary_&lt;/code&gt; 속성에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="42e30f7491cd0bbd1be9eaa1008d97761b319a5f" translate="yes" xml:space="preserve">
          <source>The converted and validated X.</source>
          <target state="translated">변환 및 검증 된 X</target>
        </trans-unit>
        <trans-unit id="f4a00aeed47e5a0cd669edcdec893bcad4bc87b6" translate="yes" xml:space="preserve">
          <source>The converted and validated array.</source>
          <target state="translated">변환되고 유효성이 검사 된 배열입니다.</target>
        </trans-unit>
        <trans-unit id="043f2ec629cc510b9c1127fe6b10b6fe4448d241" translate="yes" xml:space="preserve">
          <source>The converted and validated y.</source>
          <target state="translated">변환되고 검증 된 y.</target>
        </trans-unit>
        <trans-unit id="81a7bc326e697c66d8802c043c85fabeabbf241b" translate="yes" xml:space="preserve">
          <source>The converted dataname.</source>
          <target state="translated">변환 된 데이터 이름</target>
        </trans-unit>
        <trans-unit id="223ada656bcdd3fee604b90acc8ae4baf52723e0" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is</source>
          <target state="translated">UCI ML 유방암 위스콘신 (진단) 데이터 세트의 사본은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="44e5d228f37bd8405cfefcfa348ce3d27d939cb4" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit</source>
          <target state="translated">UCI ML Wine 데이터 세트 데이터 세트의 사본이 다운로드되고 이에 맞게 수정됩니다.</target>
        </trans-unit>
        <trans-unit id="e11d4d7608fe1fa28c8a673b1389ac6181cb7877" translate="yes" xml:space="preserve">
          <source>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights \(w_1\), \(w_2\), &amp;hellip;, \(w_N\) to each of the training samples. Initially, those weights are all set to \(w_i = 1/N\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence &lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;.</source>
          <target state="translated">AdaBoost의 핵심 원칙은 반복적으로 수정 된 데이터 버전에 약한 학습자 순서 (즉, 작은 의사 결정 트리와 같이 임의 추측보다 약간 더 나은 모델)를 맞추는 것입니다. 그런 다음 이들 모두의 예측은 가중 과반수 투표 (또는 합계)를 통해 결합되어 최종 예측을 생성합니다. 소위 부스팅 반복에서 데이터 수정은 각 트레이닝 샘플에 가중치 \ (w_1 \), \ (w_2 \),&amp;hellip;, \ (w_N \)를 적용하는 것으로 구성됩니다. 처음에는 이러한 가중치가 모두 \ (w_i = 1 / N \)으로 설정되어 있으므로 첫 번째 단계는 단순히 약한 학습자에게 원본 데이터를 학습시킵니다. 연속적인 반복마다 샘플 가중치가 개별적으로 수정되고 학습 알고리즘이 가중치가 적용된 데이터에 다시 적용됩니다. 주어진 단계에서이전 단계에서 유도 된 부스트 모델에 의해 잘못 예측 된 트레이닝 예제는 가중치가 증가하는 반면, 올바르게 예측 된 가중치는 감소합니다. 반복이 진행됨에 따라 예측하기 어려운 예는 계속 증가하는 영향을받습니다. 이에 따라 각각의 약한 학습자는 순서에서 이전 예제에서 놓친 예제에 집중해야합니다.&lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2a94b871b5efce504e5077120d25fb83e224b7ee" translate="yes" xml:space="preserve">
          <source>The corpus is a collection of \(D\) documents.</source>
          <target state="translated">말뭉치는 \ (D \) 문서 모음입니다.</target>
        </trans-unit>
        <trans-unit id="81757ae28a30627b83ded3cb76d108f655541caf" translate="yes" xml:space="preserve">
          <source>The correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y)).</source>
          <target state="translated">각 회귀 변수와 목표 간의 상관 관계가 계산됩니다. 즉, ((XX :: i)-mean (X [:, i]) * (y-mean_y)) / (std (X [:, i]) ) * std (y)).</target>
        </trans-unit>
        <trans-unit id="302d596f93db30183d7b9abc1999d9cfb95f2d1f" translate="yes" xml:space="preserve">
          <source>The corresponding image is:</source>
          <target state="translated">해당 이미지는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9a9313289c48f29b6d60eb0302d43d099de5f93d" translate="yes" xml:space="preserve">
          <source>The corresponding training labels.</source>
          <target state="translated">해당 교육 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="8f6395cc147644bb3051e08a46acbc9cd47145af" translate="yes" xml:space="preserve">
          <source>The cosine distance is defined as &lt;code&gt;1 - cosine_similarity&lt;/code&gt;: the lowest value is 0 (identical point) but it is bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only on their relative angles.</source>
          <target state="translated">코사인 거리는 &lt;code&gt;1 - cosine_similarity&lt;/code&gt; 로 정의됩니다 . 가장 낮은 값은 0 (동일 포인트)이지만 가장 먼 포인트의 경우 2로 제한됩니다. 그 값은 벡터 점의 규범에 의존하지 않고 상대 각도에만 의존합니다.</target>
        </trans-unit>
        <trans-unit id="c4b95b481d689de7dde58522ad60a16deab9f841" translate="yes" xml:space="preserve">
          <source>The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm</source>
          <target state="translated">코사인 거리는 각 표본이 단위 규범으로 정규화되는 경우 제곱 유클리드 거리의 절반에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="59fadedc84e4bf70f9182651c0ff609a02cebdec" translate="yes" xml:space="preserve">
          <source>The cost complexity measure of a single node is \(R_\alpha(t)=R(t)+\alpha\). The branch, \(T_t\), is defined to be a tree where node \(t\) is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, \(R(T_t)&amp;lt;R(t)\). However, the cost complexity measure of a node, \(t\), and its branch, \(T_t\), can be equal depending on \(\alpha\). We define the effective \(\alpha\) of a node to be the value where they are equal, \(R_\alpha(T_t)=R_\alpha(t)\) or \(\alpha_{eff}(t)=\frac{R(t)-R(T_t)}{|T|-1}\). A non-terminal node with the smallest value of \(\alpha_{eff}\) is the weakest link and will be pruned. This process stops when the pruned tree&amp;rsquo;s minimal \(\alpha_{eff}\) is greater than the &lt;code&gt;ccp_alpha&lt;/code&gt; parameter.</source>
          <target state="translated">단일 노드의 비용 복잡성 측정 값은 \ (R_ \ alpha (t) = R (t) + \ alpha \)입니다. 분기 \ (T_t \)는 노드 \ (t \)가 루트 인 트리로 정의됩니다. 일반적으로 노드의 불순물은 터미널 노드의 불순물 합계 \ (R (T_t) &amp;lt;R (t) \)보다 큽니다. 그러나 노드 \ (t \) 및 분기 \ (T_t \)의 비용 복잡성 측정은 \ (\ alpha \)에 따라 동일 할 수 있습니다. 노드의 유효 \ (\ alpha \)를 동일한 값인 \ (R_ \ alpha (T_t) = R_ \ alpha (t) \) 또는 \ (\ alpha_ {eff} (t)로 정의합니다. = \ frac {R (t) -R (T_t)} {| T | -1} \). 가장 작은 값이 \ (\ alpha_ {eff} \) 인 비 터미널 노드는 가장 약한 링크이며 제거됩니다. 이 프로세스는 잘라낸 트리의 최소 \ (\ alpha_ {eff} \)가 &lt;code&gt;ccp_alpha&lt;/code&gt; 매개 변수 보다 클 때 중지 됩니다.</target>
        </trans-unit>
        <trans-unit id="de99d6d126cf03f253c85bcda8fea4fdd47045eb" translate="yes" xml:space="preserve">
          <source>The cost function of an isomap embedding is</source>
          <target state="translated">아이소 맵 임베딩의 비용 함수는</target>
        </trans-unit>
        <trans-unit id="40200d49debb9a752670d9f4e91dc77cd9d66f0a" translate="yes" xml:space="preserve">
          <source>The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.</source>
          <target state="translated">트리를 사용하는 비용 (즉, 데이터 예측)은 트리를 훈련시키는 데 사용되는 데이터 포인트 수에 로그입니다.</target>
        </trans-unit>
        <trans-unit id="c9b6813659bffb0c08aeb974fcecdaaf97e786b0" translate="yes" xml:space="preserve">
          <source>The covariance matrix of a data set is known to be well approximated by the classical &lt;em&gt;maximum likelihood estimator&lt;/em&gt; (or &amp;ldquo;empirical covariance&amp;rdquo;), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population&amp;rsquo;s covariance matrix.</source>
          <target state="translated">관측치 수가 피처 개수 (관측치를 설명하는 변수)에 비해 충분히 클 경우, 데이터 세트의 공분산 행렬은 고전적인 &lt;em&gt;최대 우도 추정기&lt;/em&gt; (또는 &quot;임시 공분산&quot;)에 의해 근사화되는 것으로 알려져 있습니다 . 보다 정확하게는 표본의 최대 우도 추정값은 해당 모집단의 공분산 행렬의 편향 추정량입니다.</target>
        </trans-unit>
        <trans-unit id="ff9747571dfcab155502f0d51c20ced0a4eda87c" translate="yes" xml:space="preserve">
          <source>The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions.</source>
          <target state="translated">공분산 행렬은이 값에 단위 행렬을 곱한 값입니다. 이 데이터 세트는 대칭 정규 분포 만 생성합니다.</target>
        </trans-unit>
        <trans-unit id="d4f25f1f739fb6455b136c8d6d48a91032e89970" translate="yes" xml:space="preserve">
          <source>The covariance of each mixture component. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">각 혼합 성분의 공분산. 모양은 &lt;code&gt;covariance_type&lt;/code&gt; 에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="37e0aa90ea88b46d75d1f0f3b55b29aae27815f8" translate="yes" xml:space="preserve">
          <source>The covariance to compare with.</source>
          <target state="translated">비교할 공분산입니다.</target>
        </trans-unit>
        <trans-unit id="a8664fdb20de58ef24fb2b254f60a073e2dad9f4" translate="yes" xml:space="preserve">
          <source>The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the canonical correlation analysis (CCA).</source>
          <target state="translated">교차 분해 모듈에는 부분 최소 제곱 (PLS)과 표준 상관 분석 (CCA)이라는 두 가지 주요 알고리즘 제품군이 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="5725093c72b0120c682574584b056d0b295efdd9" translate="yes" xml:space="preserve">
          <source>The cross validation score obtained on the training data</source>
          <target state="translated">교육 데이터에서 얻은 교차 검증 점수</target>
        </trans-unit>
        <trans-unit id="7d9ee6d7f0b44964f013a45e604e6c29cc8a3f8f" translate="yes" xml:space="preserve">
          <source>The cross-validation can then be performed easily:</source>
          <target state="translated">그런 다음 교차 검증을 쉽게 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="58fa7e854fcd94e774f9d9c000c917c76d680086" translate="yes" xml:space="preserve">
          <source>The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores:</source>
          <target state="translated">Platt 스케일링과 관련된 교차 검증은 대규모 데이터 세트에 대한 비용이 많이 드는 작업입니다. 또한 확률 추정치가 점수와 일치하지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b91dc81955b1dbc1a750c7c0ae1e516f1cb5789e" translate="yes" xml:space="preserve">
          <source>The cross-validation score can be directly calculated using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper. Given an estimator, the cross-validation object and the input dataset, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</source>
          <target state="translated">교차 유효성 검사 점수는 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 도우미를 사용하여 직접 계산할 수 있습니다 . 추정기, 교차 유효성 검사 객체 및 입력 데이터 집합이 주어지면 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 는 데이터를 학습 및 테스트 집합으로 반복 분할하고 학습 집합을 사용하여 추정 자를 학습하고 교차의 각 반복에 대한 테스트 집합을 기반으로 점수를 계산합니다. 확인.</target>
        </trans-unit>
        <trans-unit id="0c9e67494c034c3aceceb77302443d5c2bb54301" translate="yes" xml:space="preserve">
          <source>The cross-validation scores such that &lt;code&gt;grid_scores_[i]&lt;/code&gt; corresponds to the CV score of the i-th subset of features.</source>
          <target state="translated">&lt;code&gt;grid_scores_[i]&lt;/code&gt; 가 특징의 i 번째 부분 집합의 CV 점수에 해당하도록 교차 검증 점수 .</target>
        </trans-unit>
        <trans-unit id="7186db76e260a93ceff6e7a012a18028bce16f5b" translate="yes" xml:space="preserve">
          <source>The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see &lt;code&gt;NearestNeighbors&lt;/code&gt;.</source>
          <target state="translated">현재 구현에서는 볼 트리와 kd- 트리를 사용하여 포인트의 주변을 결정하여 전체 거리 매트릭스 계산을 피합니다 (0.14 이전의 scikit-learn 버전에서 수행됨). 맞춤 측정 항목을 사용할 가능성이 유지됩니다. 자세한 내용은 &lt;code&gt;NearestNeighbors&lt;/code&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e3ec89890503565d6b8dae28b11789379133ea78" translate="yes" xml:space="preserve">
          <source>The current loss computed with the loss function.</source>
          <target state="translated">손실 함수로 계산 된 현재 손실.</target>
        </trans-unit>
        <trans-unit id="a415ef6f77731ce51df5a94c5015135ebd3f462c" translate="yes" xml:space="preserve">
          <source>The curse of dimensionality</source>
          <target state="translated">차원의 저주</target>
        </trans-unit>
        <trans-unit id="3eaad00bc0bba0577db9b826b646317a24a90409" translate="yes" xml:space="preserve">
          <source>The data</source>
          <target state="translated">자료</target>
        </trans-unit>
        <trans-unit id="973622d96c176a0a77b0dde8f6175466c5afec2c" translate="yes" xml:space="preserve">
          <source>The data is always a 2D array, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt;, although the original data may have had a different shape. In the case of the digits, each original sample is an image of shape &lt;code&gt;(8, 8)&lt;/code&gt; and can be accessed using:</source>
          <target state="translated">원래 데이터의 모양이 다를 수 있지만 데이터는 항상 2D 배열 인 shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; 입니다. 숫자의 경우 각 원본 샘플은 모양 &lt;code&gt;(8, 8)&lt;/code&gt; 의 이미지이며 다음을 사용하여 액세스 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0e6af14908ee7619e613da95e5ef5e8434ef93d9" translate="yes" xml:space="preserve">
          <source>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.</source>
          <target state="translated">데이터는 음이 아닌 것으로 가정되며, 종종 L1- 노름이 1로 정규화됩니다. 정규화는 카이 제곱 거리 (이산 확률 분포 사이의 거리)에 연결하여 합리화됩니다.</target>
        </trans-unit>
        <trans-unit id="aa2161d468d436ce31e1b1a4a535d19d4acf7995" translate="yes" xml:space="preserve">
          <source>The data is generated with the &lt;code&gt;make_checkerboard&lt;/code&gt; function, then shuffled and passed to the Spectral Biclustering algorithm. The rows and columns of the shuffled matrix are rearranged to show the biclusters found by the algorithm.</source>
          <target state="translated">&lt;code&gt;make_checkerboard&lt;/code&gt; 함수를 사용하여 데이터를 생성 한 다음 섞어서 스펙트럼 바이 클러스터링 알고리즘으로 전달합니다. 셔플 링 된 행렬의 행과 열은 알고리즘에서 찾은 biclusters를 보여주기 위해 재 배열됩니다.</target>
        </trans-unit>
        <trans-unit id="cbdb41e13849fa39e43a0c0831b005495a0d0342" translate="yes" xml:space="preserve">
          <source>The data is split according to the cv parameter. Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.</source>
          <target state="translated">데이터는 cv 매개 변수에 따라 분할됩니다. 각 샘플은 정확히 하나의 테스트 세트에 속하며 예측은 해당 학습 세트에 맞는 추정기로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="549dd87c3787b6b83f2456e4cff9d8aa392d12c1" translate="yes" xml:space="preserve">
          <source>The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.</source>
          <target state="translated">이 데이터는 세 개의 다른 경운기에 의해 이탈리아의 같은 지역에서 재배 된 와인의 화학적 분석 결과입니다. 세 가지 유형의 와인에서 발견되는 다른 성분에 대해 13 가지의 다른 측정이 이루어집니다.</target>
        </trans-unit>
        <trans-unit id="92b1cf272aa9964f4bcd0a98099303474f1eb6db" translate="yes" xml:space="preserve">
          <source>The data list to learn.</source>
          <target state="translated">학습 할 데이터 목록입니다.</target>
        </trans-unit>
        <trans-unit id="279fbf7584e00e713412e8ff4546300536ffeb5a" translate="yes" xml:space="preserve">
          <source>The data matrix</source>
          <target state="translated">데이터 매트릭스</target>
        </trans-unit>
        <trans-unit id="8a93d3171d08360f1c2b565ec2f68ceaa0cb40fb" translate="yes" xml:space="preserve">
          <source>The data matrix to learn.</source>
          <target state="translated">학습 할 데이터 매트릭스입니다.</target>
        </trans-unit>
        <trans-unit id="a36ea8c8057a9ae699a69f099ee316c3295afaf3" translate="yes" xml:space="preserve">
          <source>The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.</source>
          <target state="translated">p 개의 특징과 n 개의 샘플이있는 데이터 매트릭스. 데이터 세트는 원시 추정치를 계산하는 데 사용 된 데이터 세트 여야합니다.</target>
        </trans-unit>
        <trans-unit id="f0ed1480aeed96966c83d245c0e82839723677ea" translate="yes" xml:space="preserve">
          <source>The data matrix.</source>
          <target state="translated">데이터 매트릭스.</target>
        </trans-unit>
        <trans-unit id="3e01104c886db328397aa7550432fcf04c711803" translate="yes" xml:space="preserve">
          <source>The data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="translated">데이터 매트릭스입니다. 경우 &lt;code&gt;as_frame=True&lt;/code&gt; , &lt;code&gt;data&lt;/code&gt; 팬더 DataFrame 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="70cee63c42777050ee29bb5c41f5ed813382b6c9" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is int</source>
          <target state="translated">반환 된 희소 행렬의 데이터입니다. 기본적으로 int입니다</target>
        </trans-unit>
        <trans-unit id="13b8870b16632bb144f99a987da0a17b912fd261" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is the dtype of img</source>
          <target state="translated">반환 된 희소 행렬의 데이터입니다. 기본적으로 img의 dtype입니다.</target>
        </trans-unit>
        <trans-unit id="a9c1f6d9c961581c21421066f6ce8f7c709084a9" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained.</source>
          <target state="translated">&lt;code&gt;gbrt&lt;/code&gt; 가 훈련 된 데이터 .</target>
        </trans-unit>
        <trans-unit id="c66d20ce5405cff4e02a00c321afd4016f52379a" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained. It is used to generate a &lt;code&gt;grid&lt;/code&gt; for the &lt;code&gt;target_variables&lt;/code&gt;. The &lt;code&gt;grid&lt;/code&gt; comprises &lt;code&gt;grid_resolution&lt;/code&gt; equally spaced points between the two &lt;code&gt;percentiles&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;gbrt&lt;/code&gt; 가 훈련 된 데이터 . 생성하는 데 사용되는 &lt;code&gt;grid&lt;/code&gt; 에 대한 &lt;code&gt;target_variables&lt;/code&gt; 을 . &lt;code&gt;grid&lt;/code&gt; 는 두 &lt;code&gt;percentiles&lt;/code&gt; 사이에 동일한 간격 의 그리드 &lt;code&gt;grid_resolution&lt;/code&gt; 구성 됩니다.</target>
        </trans-unit>
        <trans-unit id="2ffaf51af185adce5e53447e39433481435abf03" translate="yes" xml:space="preserve">
          <source>The data samples transformed.</source>
          <target state="translated">변환 된 데이터 샘플.</target>
        </trans-unit>
        <trans-unit id="65c0e8f3b46fedbf2f41b7e1589997d8de30db77" translate="yes" xml:space="preserve">
          <source>The data set contains images of hand-written digits: 10 classes where each class refers to a digit.</source>
          <target state="translated">데이터 세트에는 손으로 쓴 숫자의 이미지가 포함됩니다. 각 클래스는 숫자를 나타내는 10 개의 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="a49ccf8db4843f7af3fe75d2e385d4c5ae4a247b" translate="yes" xml:space="preserve">
          <source>The data that should be scaled.</source>
          <target state="translated">확장해야 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="4362ce6dec3d09ef8de03aa7af16c30845dd1b85" translate="yes" xml:space="preserve">
          <source>The data that should be transformed back.</source>
          <target state="translated">다시 변환해야하는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="47ba75ee664e21a51401b4c1203a08e7a876f44e" translate="yes" xml:space="preserve">
          <source>The data to be transformed by subset.</source>
          <target state="translated">서브 세트로 변환 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="03bb048cfbbc3212fd60edf266a3822d3176ef87" translate="yes" xml:space="preserve">
          <source>The data to be transformed using a power transformation.</source>
          <target state="translated">전력 변환을 사용하여 변환 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="73ce115221fb870e05c90b8f043ca278fdee74a1" translate="yes" xml:space="preserve">
          <source>The data to be transformed.</source>
          <target state="translated">변환 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="df7f442215a9627450351a5239ccb7b837681c69" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">요소별로 이진화 할 데이터입니다. 불필요한 사본을 피하기 위해 scipy.sparse 행렬은 CSR 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="5401bf0fdb954957c9a3f345344e508dbd6bac54" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy.</source>
          <target state="translated">요소별로 이진화 할 데이터입니다. scipy.sparse 매트릭스는 불필요한 사본을 피하기 위해 CSR 또는 CSC 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="515108ae5aad51ada5b234ccff65d93cdd42711e" translate="yes" xml:space="preserve">
          <source>The data to center and scale.</source>
          <target state="translated">중심 및 스케일링 할 데이터.</target>
        </trans-unit>
        <trans-unit id="975edd1be086184330a8a3ebbf935588408d4a98" translate="yes" xml:space="preserve">
          <source>The data to determine the categories of each feature.</source>
          <target state="translated">각 기능의 범주를 결정하는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="a77a7e1c81043cc97f700ad0d213253f486b563a" translate="yes" xml:space="preserve">
          <source>The data to encode.</source>
          <target state="translated">인코딩 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="d843c1d7e5e8986d5ad23251cff2a94ef8ee7955" translate="yes" xml:space="preserve">
          <source>The data to fit.</source>
          <target state="translated">맞는 데이터.</target>
        </trans-unit>
        <trans-unit id="e7fbe72ba2fcf8fff679b5e28b0fef9589e7589d" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be for example a list, or an array.</source>
          <target state="translated">맞는 데이터. 예를 들어 목록 또는 배열 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="00725d5de44209593a99040fefb16433276154c2" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be, for example a list, or an array at least 2d.</source>
          <target state="translated">맞는 데이터. 예를 들어 목록 또는 2d 이상의 배열 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="53846ac28d489b02b9949ef562a942f522e52d48" translate="yes" xml:space="preserve">
          <source>The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">요소별로 정규화 할 데이터입니다. 불필요한 사본을 피하기 위해 scipy.sparse 행렬은 CSR 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="c13e1ef3ee3dcba79bb567a64bd572c32814ce01" translate="yes" xml:space="preserve">
          <source>The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">정규화 할 데이터입니다. 불필요한 사본을 피하기 위해 scipy.sparse 행렬은 CSR 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="89435a85b63a4550536a3843494521cdfd812011" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row.</source>
          <target state="translated">행 단위로 변환 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="a092ffb0fb8aec81b90c0802c73b2ae6cfa80dcc" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row. Sparse input should preferably be in CSC format.</source>
          <target state="translated">행 단위로 변환 할 데이터입니다. 스파 스 입력은 CSC 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="3f0555e5cf2c3f2eb80a653fe1f67911e223ec90" translate="yes" xml:space="preserve">
          <source>The data to transform.</source>
          <target state="translated">변환 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="f0a016c9443bbddef85796fd3691360a3c817ea0" translate="yes" xml:space="preserve">
          <source>The data used to compute the mean and standard deviation used for later scaling along the features axis.</source>
          <target state="translated">피쳐 축을 따라 나중에 스케일링하는 데 사용되는 평균 및 표준 편차를 계산하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="55438370e51e74e9e8aa5e6ed6a37b2c111bd898" translate="yes" xml:space="preserve">
          <source>The data used to compute the median and quantiles used for later scaling along the features axis.</source>
          <target state="translated">피쳐 축을 따라 나중에 스케일링하는 데 사용되는 중앙값 및 Quantile을 계산하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="0ae3bec0fd19bdece5355d54cf80d1ae7bcc7c26" translate="yes" xml:space="preserve">
          <source>The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</source>
          <target state="translated">피쳐 축을 따라 나중에 스케일링하는 데 사용되는 기능별 최소값 및 최대 값을 계산하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="e26762b4f27727d3210d3fe2a64ad24b366c1062" translate="yes" xml:space="preserve">
          <source>The data used to estimate the optimal transformation parameters.</source>
          <target state="translated">최적 변환 매개 변수를 추정하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="56c320de81ba4f246c360453cbd2da4e63d63c7f" translate="yes" xml:space="preserve">
          <source>The data used to fit the model. If &lt;code&gt;copy_X=False&lt;/code&gt;, then &lt;code&gt;X_fit_&lt;/code&gt; is a reference. This attribute is used for the calls to transform.</source>
          <target state="translated">모형을 맞추는 데 사용 된 데이터입니다. 경우 &lt;code&gt;copy_X=False&lt;/code&gt; 다음 &lt;code&gt;X_fit_&lt;/code&gt; 은 참조입니다. 이 속성은 변환 호출에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="03376550c822ad450e2d14840686210c4804ebc6" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis.</source>
          <target state="translated">형상 축을 따라 스케일링하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="c4374e3fd9ee0863ad791a18672a80272e651c9f" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;. Additionally, the sparse matrix needs to be nonnegative if &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; is False.</source>
          <target state="translated">형상 축을 따라 스케일링하는 데 사용되는 데이터입니다. 희소 행렬이 제공되면 희소 &lt;code&gt;csc_matrix&lt;/code&gt; 로 변환됩니다 . &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; 가 False 인 경우 희소 행렬은 음수가 아니어야 합니다.</target>
        </trans-unit>
        <trans-unit id="848647eb355f23af31ef1bf06cbb1e0d945eea47" translate="yes" xml:space="preserve">
          <source>The data used to scale along the specified axis.</source>
          <target state="translated">지정된 축을 따라 스케일링하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="db728424571c6d0aa73c940cbe613870747b4067" translate="yes" xml:space="preserve">
          <source>The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)</source>
          <target state="translated">이 데이터는 다양한 분류기를 비교하기 위해 다른 많은 사람들과 함께 사용되었습니다. RDA만이 100 % 정확한 분류를 달성했지만 클래스는 분리 가능합니다. (RDA : 100 %, QDA 99.4 %, LDA 98.9 %, 1NN 96.1 % (z 변환 된 데이터)) (Leave-One-Out 기술을 사용한 모든 결과)</target>
        </trans-unit>
        <trans-unit id="660fe59aa4f1107f7d968ac4c0471ddbff4317b5" translate="yes" xml:space="preserve">
          <source>The data.</source>
          <target state="translated">자료.</target>
        </trans-unit>
        <trans-unit id="02bda070eb404181c9796aadeceacf78aac05356" translate="yes" xml:space="preserve">
          <source>The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a &lt;code&gt;sample_weight&lt;/code&gt; when fitting DBSCAN.</source>
          <target state="translated">데이터에서 중복이 발생하는 경우 정확한 중복을 제거하거나 BIRCH를 사용하여 데이터 세트를 압축 할 수 있습니다. 그런 다음 많은 포인트에 대해 상대적으로 적은 수의 담당자 만 있습니다. 그런 다음 &lt;code&gt;sample_weight&lt;/code&gt; 을 피팅 할 때 sample_weight 를 제공 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6bbac4d66b74c67e38ab2b8e6e075ffda05e6522" translate="yes" xml:space="preserve">
          <source>The dataset is called &amp;ldquo;Twenty Newsgroups&amp;rdquo;. Here is the official description, quoted from the &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;website&lt;/a&gt;:</source>
          <target state="translated">데이터 세트를 &quot;20 개의 뉴스 그룹&quot;이라고합니다. 다음은 &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;웹 사이트&lt;/a&gt; 에서 인용 한 공식 설명입니다 .</target>
        </trans-unit>
        <trans-unit id="c4573e45f21f00151e7512eaf1b6ae1cd41e22bd" translate="yes" xml:space="preserve">
          <source>The dataset is from Zhu et al [1].</source>
          <target state="translated">데이터 세트는 Zhu et al [1]에서 가져온 것입니다.</target>
        </trans-unit>
        <trans-unit id="1a602d6b3832c4acaa760453be6cb4d26914f03f" translate="yes" xml:space="preserve">
          <source>The dataset is generated using the &lt;code&gt;make_biclusters&lt;/code&gt; function, which creates a matrix of small values and implants bicluster with large values. The rows and columns are then shuffled and passed to the Spectral Co-Clustering algorithm. Rearranging the shuffled matrix to make biclusters contiguous shows how accurately the algorithm found the biclusters.</source>
          <target state="translated">&lt;code&gt;make_biclusters&lt;/code&gt; 함수를 사용하여 데이터 세트를 생성합니다 .이 함수는 작은 값의 행렬을 작성하고 큰 값으로 bicluster를 이식합니다. 그런 다음 행과 열이 섞여 스펙트럼 공동 클러스터링 알고리즘으로 전달됩니다. 셔플 된 행렬을 재 배열하여 biclusters를 연속적으로 만들면 알고리즘이 biclusters를 얼마나 정확하게 찾았는지 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4131a4e690502c19383d02f0465052ba8df886e1" translate="yes" xml:space="preserve">
          <source>The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">데이터 세트는 인덱스 순서로 근처의 포인트가 파라미터 공간에서 근처에 있도록 구성되어 K- 인접 이웃의 대략적인 블록 대각선 매트릭스로 이어진다. 이러한 희소 그래프는 &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt; &lt;/a&gt; 학습을위한 포인트 간의 공간 관계를 사용하는 다양한 상황에서 유용합니다. 특히 sklearn.manifold.Isomap , &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt; 을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="774649dea26c780c5e43d2464445b85ef137fcca" translate="yes" xml:space="preserve">
          <source>The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classification).</source>
          <target state="translated">데이터 집합은 회귀 (각 분류)에 대한 Boston Housing 데이터 집합 (각 20 개 뉴스 그룹)입니다.</target>
        </trans-unit>
        <trans-unit id="c0f7ef482ab49522d3cd87d64587ff804f6727c1" translate="yes" xml:space="preserve">
          <source>The dataset used for evaluation is a 2D grid of isotropic Gaussian clusters widely spaced.</source>
          <target state="translated">평가에 사용 된 데이터 세트는 넓은 간격으로 등방성 가우스 클러스터의 2D 그리드입니다.</target>
        </trans-unit>
        <trans-unit id="c07599d3a55d8254ba468cc109830533370f5dcf" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is Reuters-21578 as provided by the UCI ML repository. It will be automatically downloaded and uncompressed on first run.</source>
          <target state="translated">이 예에서 사용 된 데이터 집합은 UCI ML 리포지토리에서 제공 한 Reuters-21578입니다. 처음 실행하면 자동으로 다운로드되고 압축 해제됩니다.</target>
        </trans-unit>
        <trans-unit id="f610543312a82d7ead69951d61b82950d647ad3c" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, aka &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">이 예제에서 사용 된 데이터 세트는 사전 처리 된 &quot;야생의 레이블이있는 얼굴&quot;(일명 &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW)&lt;/a&gt; 발췌입니다 .</target>
        </trans-unit>
        <trans-unit id="257296081e1c4b1a90a4ff7a918d81d5746f6705" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, also known as &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">이 예제에서 사용 된 데이터 세트는 &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt; 라고도 알려진 &quot;야생의 레이블이있는 얼굴&quot;의 사전 처리 된 발췌입니다 .</target>
        </trans-unit>
        <trans-unit id="0e491b7a83df0aa734f40ae016e056d9d5ef3d92" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.</source>
          <target state="translated">이 예에서 사용 된 데이터 세트는 20 개의 뉴스 그룹 데이터 세트이며 자동으로 다운로드 된 후 문서 분류 예에 대해 캐시 및 재사용됩니다.</target>
        </trans-unit>
        <trans-unit id="b391209f39032c4c37a7d267548501e64198b514" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.</source>
          <target state="translated">이 예에서 사용 된 데이터 집합은 20 개의 뉴스 그룹 데이터 집합입니다. 자동으로 다운로드 된 다음 캐시됩니다.</target>
        </trans-unit>
        <trans-unit id="b6261e60f08853c79c52801fadd1554784335b15" translate="yes" xml:space="preserve">
          <source>The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).</source>
          <target state="translated">사용 된 데이터 세트는 UCI에서 사용 가능한 와인 데이터 세트입니다. 이 데이터 세트에는 측정하는 특성 (알코올 함량 및 말산)이 다르기 때문에 이질적인 규모의 이종 특성이 있습니다.</target>
        </trans-unit>
        <trans-unit id="2320c3b81471635dac46a3209d55417f5e2427c2" translate="yes" xml:space="preserve">
          <source>The dataset will be downloaded from the &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 homepage&lt;/a&gt; if necessary. The compressed size is about 656 MB.</source>
          <target state="translated">필요한 경우 &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 홈페이지&lt;/a&gt; 에서 데이터 세트가 다운로드됩니다 . 압축 된 크기는 약 656MB입니다.</target>
        </trans-unit>
        <trans-unit id="aadaaf4f9f002ac5fc13c03e41ad65b9ea025276" translate="yes" xml:space="preserve">
          <source>The dataset: wages</source>
          <target state="translated">데이터 세트 : 임금</target>
        </trans-unit>
        <trans-unit id="a104c196a5a6b4961911da3fe75edb0c176425f0" translate="yes" xml:space="preserve">
          <source>The datasets also contain a full description in their &lt;code&gt;DESCR&lt;/code&gt; attribute and some contain &lt;code&gt;feature_names&lt;/code&gt; and &lt;code&gt;target_names&lt;/code&gt;. See the dataset descriptions below for details.</source>
          <target state="translated">데이터 세트에는 &lt;code&gt;DESCR&lt;/code&gt; 속성 에 대한 전체 설명이 포함되어 있으며 일부에는 &lt;code&gt;feature_names&lt;/code&gt; 및 &lt;code&gt;target_names&lt;/code&gt; 가 포함되어 있습니다 . 자세한 내용은 아래 데이터 세트 설명을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="266ce3ccfa4a57091d07a7fe8bfc007064ff062e" translate="yes" xml:space="preserve">
          <source>The decision function computed the final estimator.</source>
          <target state="translated">결정 함수는 최종 추정치를 계산했습니다.</target>
        </trans-unit>
        <trans-unit id="26c7542245412a79b0296d695d58227ed1491d25" translate="yes" xml:space="preserve">
          <source>The decision function is equal (up to a constant factor) to the log-posterior of the model, i.e. &lt;code&gt;log p(y = k | x)&lt;/code&gt;. In a binary classification setting this instead corresponds to the difference &lt;code&gt;log p(y = 1 | x) - log p(y = 0 | x)&lt;/code&gt;. See &lt;a href=&quot;../lda_qda#lda-qda-math&quot;&gt;Mathematical formulation of the LDA and QDA classifiers&lt;/a&gt;.</source>
          <target state="translated">결정 함수는 모델의 log-posterior와 같습니다 (상수 인자까지), 즉 &lt;code&gt;log p(y = k | x)&lt;/code&gt; . 이진 분류 설정에서 이것은 대신 차이 &lt;code&gt;log p(y = 1 | x) - log p(y = 0 | x)&lt;/code&gt; 합니다. &lt;a href=&quot;../lda_qda#lda-qda-math&quot;&gt;LDA 및 QDA 분류기의 수학적 공식화를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="f5da29d39d3005818dbe52be4adbd180f7105540" translate="yes" xml:space="preserve">
          <source>The decision function is:</source>
          <target state="translated">결정 기능은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="fdcfe75a710515596cfa4af6113e103288190672" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">앙상블의 트리에서 예측 된 원시 값에 해당하는 입력 샘플의 결정 함수입니다. 클래스는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; 속성에 해당합니다 . 회귀 및 이진 분류는 &lt;code&gt;k == 1&lt;/code&gt; 이고 그렇지 않으면 &lt;code&gt;k==n_classes&lt;/code&gt; 특수한 경우입니다 .</target>
        </trans-unit>
        <trans-unit id="13c22c73db1fc45ace2498b233349bae422df959" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">앙상블의 트리에서 예측 된 원시 값에 해당하는 입력 샘플의 결정 함수입니다. 클래스의 순서는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; 속성의 순서와 일치합니다 . 회귀 및 이진 분류는 [n_samples] 모양의 배열을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="c0e6f8aea5cc6c4291ed60eb8cccabc092233917" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The columns correspond to the classes in sorted order, as they appear in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">입력 샘플의 결정 기능. 열은 &lt;code&gt;classes_&lt;/code&gt; 속성에 표시된 대로 정렬 된 순서로 클래스에 해당합니다 . 회귀 및 이진 분류는 &lt;code&gt;k == 1&lt;/code&gt; 인 특수한 경우이며 , 그렇지 않으면 &lt;code&gt;k==n_classes&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="ae769935874da2534c2ccb6ca3e5794a84e4d439" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">입력 샘플의 결정 기능. 출력 순서는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; 속성 의 순서와 동일 합니다. 이진 분류는 &lt;code&gt;k == 1&lt;/code&gt; 이고 그렇지 않으면 &lt;code&gt;k==n_classes&lt;/code&gt; 특수한 경우입니다 . 이진 분류의 경우 -1 또는 1에 더 가까운 값은 각각 &lt;code&gt;classes_&lt;/code&gt; 의 첫 번째 또는 두 번째 클래스와 더 비슷하다는 것을 의미 합니다.</target>
        </trans-unit>
        <trans-unit id="06ac27333a17bb0e5a70b7bdd7a4a5eafb52284d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">입력 샘플의 결정 기능. 출력 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성 의 순서와 동일 합니다. 이진 분류는 &lt;code&gt;k == 1&lt;/code&gt; 인 특수한 경우 이고, 그렇지 않으면 &lt;code&gt;k==n_classes&lt;/code&gt; 입니다. 이진 분류의 경우 -1 또는 1에 가까운 값은 각각 &lt;code&gt;classes_&lt;/code&gt; 의 첫 번째 또는 두 번째 클래스와 비슷 합니다.</target>
        </trans-unit>
        <trans-unit id="7a79704b70e2487a7278def4173b216c7abdf67d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">입력 샘플의 결정 기능. 클래스의 순서는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; 속성의 순서와 일치합니다 . 회귀 및 이진 분류는 [n_samples] 모양의 배열을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="271144c81e33420b0281bbd1b4c29fbb1374c6ac" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">입력 샘플의 결정 기능. 클래스의 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성의 순서와 일치합니다 . 회귀 및 이진 분류는 &lt;code&gt;k == 1&lt;/code&gt; 인 특수한 경우이며 , 그렇지 않으면 &lt;code&gt;k==n_classes&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="c04fb9b1c64a374fbc2424dc1dc2e69edec92fae" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">입력 샘플의 결정 기능. 클래스의 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성의 순서와 일치합니다 . 회귀 및 이진 분류는 모양 [n_samples]의 배열을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="7a4d1c2d06404b468f740213ed27426daa73066a" translate="yes" xml:space="preserve">
          <source>The decision rule for Bernoulli naive Bayes is based on</source>
          <target state="translated">Bernoulli 순진 Bayes의 결정 규칙은</target>
        </trans-unit>
        <trans-unit id="5c243c13540ddb2ab9ba5e07515f5ccc5bddcdc4" translate="yes" xml:space="preserve">
          <source>The decision tree estimator to be exported. It can be an instance of DecisionTreeClassifier or DecisionTreeRegressor.</source>
          <target state="translated">내보낼 의사 결정 트리 추정기입니다. DecisionTreeClassifier 또는 DecisionTreeRegressor의 인스턴스 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0e0b3ca71cdfb4dae3b0e05518ff98bb98da9e44" translate="yes" xml:space="preserve">
          <source>The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve:</source>
          <target state="translated">의사 결정 트리 구조를 분석하여 피처와 예측 대상 간의 관계에 대한 추가 정보를 얻을 수 있습니다. 이 예에서는 다음을 검색하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="7930dea6ad7960124a06f25452c69dc408a6af03" translate="yes" xml:space="preserve">
          <source>The decision tree to be exported to GraphViz.</source>
          <target state="translated">GraphViz로 내보낼 의사 결정 트리.</target>
        </trans-unit>
        <trans-unit id="096b758652a55c20db43d0b3794cde42f9ae935e" translate="yes" xml:space="preserve">
          <source>The decision tree to be plotted.</source>
          <target state="translated">플로팅 할 의사 결정 트리입니다.</target>
        </trans-unit>
        <trans-unit id="41d54cbac8ff4bb674faa62fe3a273da70a2c8ec" translate="yes" xml:space="preserve">
          <source>The decision values for the samples are computed by adding the normalized sum of pair-wise classification confidence levels to the votes in order to disambiguate between the decision values when the votes for all the classes are equal leading to a tie.</source>
          <target state="translated">표본에 대한 결정 값은 모든 등급에 대한 투표가 동일 할 때 결정 값을 명확하게하기 위해 표준화 된 쌍별 분류 신뢰 수준의 합을 투표에 추가하여 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="9db4a0cde49703606f721a9d732403d53161167d" translate="yes" xml:space="preserve">
          <source>The decoding strategy depends on the vectorizer parameters.</source>
          <target state="translated">디코딩 전략은 벡터 라이저 매개 변수에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="8ab5c7033c87e7cea9882c536950d8e60cf30550" translate="yes" xml:space="preserve">
          <source>The default coding of images is based on the &lt;code&gt;uint8&lt;/code&gt; dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; don&amp;rsquo;t forget to scale to the range 0 - 1 as done in the following example.</source>
          <target state="translated">이미지의 기본 코딩은 메모리를 절약하기 위한 &lt;code&gt;uint8&lt;/code&gt; dtype을 기반으로합니다 . 입력이 부동 소수점 표현으로 먼저 변환되는 경우 종종 기계 학습 알고리즘이 가장 잘 작동합니다. 또한 &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; 를 사용하려는 경우 다음 예에서와 같이 0-1 범위로 조정하는 것을 잊지 마십시오.</target>
        </trans-unit>
        <trans-unit id="ec537a685ef4f8a1d5936e658cd9e5b93a584d77" translate="yes" xml:space="preserve">
          <source>The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:</source>
          <target state="translated">기본 구성은 2 자 이상의 단어를 추출하여 문자열을 토큰 화합니다. 이 단계를 수행하는 특정 기능을 명시 적으로 요청할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3d025c574ea3cc62ecf87d5b0a9f83d7b3c75d6a" translate="yes" xml:space="preserve">
          <source>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module for the list of possible cross-validation objects.</source>
          <target state="translated">사용되는 기본 교차 유효성 검사 생성기는 Stratified K-Folds입니다. 정수가 제공되면 사용 된 접기 수입니다. 가능한 교차 유효성 검사 객체 목록은 &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt; 모듈을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="24e256639376dfcfda93b74d1117680d7d248e59" translate="yes" xml:space="preserve">
          <source>The default dataset is the 20 newsgroups dataset. To run the example on the digits dataset, pass the &lt;code&gt;--use-digits-dataset&lt;/code&gt; command line argument to this script.</source>
          <target state="translated">기본 데이터 세트는 20 개의 뉴스 그룹 데이터 세트입니다. 숫자 데이터 세트에서 예제를 실행하려면 &lt;code&gt;--use-digits-dataset&lt;/code&gt; 명령 줄 인수를이 스크립트에 전달합니다.</target>
        </trans-unit>
        <trans-unit id="53050da8c5d49a141b5e2d80a70c0fa67fd7eb00" translate="yes" xml:space="preserve">
          <source>The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the &amp;ndash;twenty-newsgroups command line argument to this script.</source>
          <target state="translated">기본 데이터 세트는 숫자 데이터 세트입니다. 20 개의 뉴스 그룹 데이터 세트에서 예제를 실행하려면 &amp;ndash;twenty-newsgroups 명령 행 인수를이 스크립트에 전달하십시오.</target>
        </trans-unit>
        <trans-unit id="8018aa0e057c3b81a78860eae3f5892dde5e1c45" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this estimator.&amp;rdquo;</source>
          <target state="translated">기본 오류 메시지는&amp;ldquo;이 % (name) s 인스턴스는 아직 장착되지 않았습니다. 이 추정기를 사용하기 전에 적절한 인수와 함께 'fit'을 호출하십시오.&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="713044bd0fefe6c8ff38e7a00814412176e5caf0" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this method.&amp;rdquo;</source>
          <target state="translated">기본 오류 메시지는&amp;ldquo;이 % (name) s 인스턴스가 아직 설치되지 않았습니다. 이 방법을 사용하기 전에 적절한 인수로 'fit'을 호출하십시오.&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="bd1a08c69ea4f3f536ea9f567759a2eca8958b5b" translate="yes" xml:space="preserve">
          <source>The default parameters (n_samples / n_features / n_components) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).</source>
          <target state="translated">기본 매개 변수 (n_samples / n_features / n_components)는 수십 초 안에 예제를 실행할 수 있어야합니다. 문제의 차원을 늘리려 고 시도 할 수 있지만 NMF에서 시간 복잡성은 다항식입니다. LDA에서 시간 복잡도는 (n_samples * 반복)에 비례합니다.</target>
        </trans-unit>
        <trans-unit id="6bb3234d6819185e21b8b8e0778371ddb1ae1e23" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net &lt;a href=&quot;#id15&quot; id=&quot;id1&quot;&gt;11&lt;/a&gt; solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">기본 설정은 &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 입니다. L1 패널티는 희소 솔루션으로 이어져 대부분의 계수를 0으로 만듭니다. Elastic Net &lt;a href=&quot;#id15&quot; id=&quot;id1&quot;&gt;11&lt;/a&gt; 은 상관 관계가 높은 속성이있는 경우 L1 패널티의 일부 결함을 해결합니다. &lt;code&gt;l1_ratio&lt;/code&gt; 매개 변수 는 L1 및 L2 패널티의 볼록 조합을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="e964bf4dcf7d784d0dee842886fe27c5060b1af1" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">기본 설정은 &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 입니다. L1 페널티는 스파 스 해를 유발하여 대부분의 계수를 0으로 만듭니다. Elastic Net은 상관 관계가 높은 속성이있는 경우 L1 페널티의 일부 결함을 해결합니다. &lt;code&gt;l1_ratio&lt;/code&gt; 매개 변수 는 L1 및 L2 페널티의 볼록한 조합을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="d600d29df52c52da3fc6f38fdcf73036ecf8cbdf" translate="yes" xml:space="preserve">
          <source>The default slice is a rectangular shape around the face, removing most of the background:</source>
          <target state="translated">기본 슬라이스는 얼굴 주위의 직사각형 모양으로 대부분의 배경을 제거합니다.</target>
        </trans-unit>
        <trans-unit id="f2f8fa0d6181b12438bd7660924ca4c2f89302ad" translate="yes" xml:space="preserve">
          <source>The default solver is &amp;lsquo;svd&amp;rsquo;. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the &amp;lsquo;svd&amp;rsquo; solver cannot be used with shrinkage.</source>
          <target state="translated">기본 솔버는 'svd'입니다. 분류와 변환을 모두 수행 할 수 있으며 공분산 행렬의 계산에 의존하지 않습니다. 이는 기능 수가 많은 상황에서 이점이 될 수 있습니다. 그러나 'svd'솔버는 축소와 함께 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="2307e8c1575f885543e9c12c3e37adbd9d66c3dd" translate="yes" xml:space="preserve">
          <source>The default strategy implements one step of the bootstrapping procedure.</source>
          <target state="translated">기본 전략은 부트 스트랩 절차의 한 단계를 구현합니다.</target>
        </trans-unit>
        <trans-unit id="f17725906dcf13fd5a8abccea776a77c1270d7a9" translate="yes" xml:space="preserve">
          <source>The default value &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; uses &lt;code&gt;n_features&lt;/code&gt; rather than &lt;code&gt;n_features / 3&lt;/code&gt;. The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].</source>
          <target state="translated">기본 값 &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; 사용의 &lt;code&gt;n_features&lt;/code&gt; 보다는 &lt;code&gt;n_features / 3&lt;/code&gt; . 후자는 원래 [1]에서 제안되었지만 전자는 [2]에서 경험적으로 더 정당화되었다.</target>
        </trans-unit>
        <trans-unit id="71b5a78b2592e847cac7e9a1c29294e2be25d4eb" translate="yes" xml:space="preserve">
          <source>The default value of &lt;code&gt;copy&lt;/code&gt; changed from False to True in 0.23.</source>
          <target state="translated">&lt;code&gt;copy&lt;/code&gt; 의 기본값 은 0.23에서 False에서 True로 변경되었습니다.</target>
        </trans-unit>
        <trans-unit id="8ad5c264779356671425d0f732ca8de7347758f3" translate="yes" xml:space="preserve">
          <source>The default values for the parameters controlling the size of the trees (e.g. &lt;code&gt;max_depth&lt;/code&gt;, &lt;code&gt;min_samples_leaf&lt;/code&gt;, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.</source>
          <target state="translated">트리 크기를 제어하는 ​​매개 변수의 기본값 (예 : &lt;code&gt;max_depth&lt;/code&gt; , &lt;code&gt;min_samples_leaf&lt;/code&gt; 등)은 완전히 자르고 정리되지 않은 트리로 이어지며 일부 데이터 세트에서 잠재적으로 매우 클 수 있습니다. 메모리 소비를 줄이려면 트리의 복잡성과 크기를 해당 매개 변수 값을 설정하여 제어해야합니다.</target>
        </trans-unit>
        <trans-unit id="4209f696edb2b5615bc39c07cc72cc1b347ebd54" translate="yes" xml:space="preserve">
          <source>The definitive description of key concepts and API elements for using scikit-learn and developing compatible tools.</source>
          <target state="translated">scikit-learn을 사용하고 호환 가능한 도구를 개발하기위한 주요 개념과 API 요소에 대한 명확한 설명.</target>
        </trans-unit>
        <trans-unit id="767ee5df767908c4f8729c932c4a3d808fe558cd" translate="yes" xml:space="preserve">
          <source>The degree of the polynomial features. Default = 2.</source>
          <target state="translated">다항식 특징의 정도입니다. 기본값은 2입니다.</target>
        </trans-unit>
        <trans-unit id="c0ae4038f8b612638e22a63002b9a0592ebf4ed6" translate="yes" xml:space="preserve">
          <source>The density of w, between 0 and 1</source>
          <target state="translated">0과 1 사이의 w의 밀도</target>
        </trans-unit>
        <trans-unit id="1385416aaed19b5930c017ff407294aed6e786b1" translate="yes" xml:space="preserve">
          <source>The depth of a tree is the maximum distance between the root and any leaf.</source>
          <target state="translated">나무의 깊이는 뿌리와 잎 사이의 최대 거리입니다.</target>
        </trans-unit>
        <trans-unit id="64211b28a272c8cc95fc17412f0b29158a9e57aa" translate="yes" xml:space="preserve">
          <source>The desired absolute tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 0.</source>
          <target state="translated">결과의 원하는 절대 공차. 공차가 클수록 일반적으로 실행 속도가 빨라집니다. 기본값은 0입니다.</target>
        </trans-unit>
        <trans-unit id="d976f3ec62dae27876361a8dc74b8b1f3089859a" translate="yes" xml:space="preserve">
          <source>The desired relative tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 1E-8.</source>
          <target state="translated">결과의 원하는 상대 공차. 공차가 클수록 일반적으로 실행 속도가 빨라집니다. 기본값은 1E-8입니다.</target>
        </trans-unit>
        <trans-unit id="e06750706d15ac4ccfc7d440948abe624b2a5a8e" translate="yes" xml:space="preserve">
          <source>The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:</source>
          <target state="translated">당뇨병 데이터 세트는 442 명의 환자에 대한 10 가지 생리 학적 변수 (연령, 성별, 체중, 혈압) 측정치와 1 년 후 질병 진행의 지표로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="9fc4977037d4a4b0eddabbe726bd2d0de7d06481" translate="yes" xml:space="preserve">
          <source>The dict at &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; gives the parameter setting for the best model, that gives the highest mean score (&lt;code&gt;search.best_score_&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; 의 dict 는 가장 높은 평균 점수 ( &lt;code&gt;search.best_score_&lt;/code&gt; ) 를 제공하는 최상의 모델에 대한 매개 변수 설정을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="bfaa3d320392a9f6c8addb9130a129b654790105" translate="yes" xml:space="preserve">
          <source>The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.</source>
          <target state="translated">희소 코딩에 사용되는 사전 원자. 선은 단위 규범으로 정규화되는 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="7373631cc038017694fe3a2fdfb8d4bfc8e42605" translate="yes" xml:space="preserve">
          <source>The dictionary factor in the matrix factorization.</source>
          <target state="translated">행렬 분해의 사전 인수입니다.</target>
        </trans-unit>
        <trans-unit id="09a0fc68f904b631d6aa1871e7b81e42ac764443" translate="yes" xml:space="preserve">
          <source>The dictionary is fitted on the distorted left half of the image, and subsequently used to reconstruct the right half. Note that even better performance could be achieved by fitting to an undistorted (i.e. noiseless) image, but here we start from the assumption that it is not available.</source>
          <target state="translated">사전은 이미지의 왜곡 된 왼쪽 절반에 맞춰지고 오른쪽 절반을 재구성하는 데 사용됩니다. 왜곡되지 않은 (즉, 노이즈가없는) 이미지에 맞춰 더 나은 성능을 얻을 수 있지만 여기서는 사용할 수 없다는 가정에서 시작합니다.</target>
        </trans-unit>
        <trans-unit id="03531d72d72ed916646e794ac15c0622fe9c9b59" translate="yes" xml:space="preserve">
          <source>The dictionary learning objects offer, via the &lt;code&gt;split_code&lt;/code&gt; parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.</source>
          <target state="translated">사전 학습 객체는 &lt;code&gt;split_code&lt;/code&gt; 매개 변수 를 통해 희소 코딩 결과에서 양수 값과 음수 값을 분리 할 수있는 가능성을 제공합니다. 이것은 사전 학습이 감독 학습에 사용될 기능을 추출하는 데 사용될 때 유용합니다. 학습 알고리즘은 학습 알고리즘이 특정 원자의 음의 하중에 해당하는 양의 하중에 다른 가중치를 할당 할 수 있기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="c5012873b1ea68e5deed2b04cdb4e90687b2c340" translate="yes" xml:space="preserve">
          <source>The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.</source>
          <target state="translated">데이터의 희소 코딩을 해결하기위한 사전 매트릭스. 일부 알고리즘은 의미있는 출력을 위해 정규화 된 행을 가정합니다.</target>
        </trans-unit>
        <trans-unit id="f137f3a53fdf8d92f7463e89f6383b5a88f3c320" translate="yes" xml:space="preserve">
          <source>The dictionary with normalized components (D).</source>
          <target state="translated">정규화 된 구성 요소가있는 사전 (D)</target>
        </trans-unit>
        <trans-unit id="d49d4bb0bf7407d291ade3beb50a3cbdf55548f8" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size &lt;code&gt;p&lt;/code&gt; unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups.</source>
          <target state="translated">LeavePGroupsOut과 GroupShuffleSplit의 차이점은 전자가 크기 &lt;code&gt;p&lt;/code&gt; 고유 그룹 의 모든 서브 세트를 사용하여 분할을 생성하는 반면 GroupShuffleSplit은 사용자가 결정한 고유 그룹의 일부를 사용하여 사용자가 결정한 임의의 테스트 분할을 생성한다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="f8e596d2cfcc0c0ae56904e18bdffdb0d463a655" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with all the samples assigned to &lt;code&gt;p&lt;/code&gt; different values of the groups while the latter uses samples all assigned the same groups.</source>
          <target state="translated">LeavePGroupsOut과 LeaveOneGroupOut의 차이점은 전자 는 그룹에 서로 다른 &lt;code&gt;p&lt;/code&gt; 개의 값에 할당 된 모든 샘플을 사용하여 테스트 세트를 작성하는 반면, 후자는 동일한 그룹에 할당 된 샘플을 사용한다는 점입니다.</target>
        </trans-unit>
        <trans-unit id="123fc66dcb39304255b8e25c6c10c654dbb2b0f3" translate="yes" xml:space="preserve">
          <source>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \(P(x_i \mid y)\).</source>
          <target state="translated">다른 순진 Bayes 분류기는 주로 \ (P (x_i \ mid y) \)의 분포에 대한 가정에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="fc85907a08e94b0f17af8c0c01aad9ef06729185" translate="yes" xml:space="preserve">
          <source>The digits dataset is made of 1797 8x8 images of hand-written digits</source>
          <target state="translated">자릿수 데이터 세트는 손으로 쓴 자릿수의 1797 8x8 이미지로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="46ad3d3589f97f144056aa3785c730497272c03e" translate="yes" xml:space="preserve">
          <source>The dimension of the projected subspace.</source>
          <target state="translated">투영 된 부분 공간의 치수.</target>
        </trans-unit>
        <trans-unit id="5572a493038acd1179abd12e1cd0c48e6709dea1" translate="yes" xml:space="preserve">
          <source>The dimension of the projection subspace.</source>
          <target state="translated">투영 부분 공간의 치수.</target>
        </trans-unit>
        <trans-unit id="984d56bddca949d9f1aa278aca87eb46a4072ffd" translate="yes" xml:space="preserve">
          <source>The dimensionality of the resulting representation is &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt;. If &lt;code&gt;max_leaf_nodes == None&lt;/code&gt;, the number of leaf nodes is at most &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt;.</source>
          <target state="translated">결과 표현의 차원은 &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt; 입니다. 경우 &lt;code&gt;max_leaf_nodes == None&lt;/code&gt; 대부분에서, 리프 노드의 수입니다 &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="068779d9c792d7cec6633fa1a442091cb7f6f6f3" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.</source>
          <target state="translated">랜덤 프로젝션 행렬의 크기와 분포는 데이터 집합의 두 샘플 사이의 쌍별 거리를 유지하도록 제어됩니다.</target>
        </trans-unit>
        <trans-unit id="1fc3554ec8e7cb3510bb9bcb462301a368807c4a" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.</source>
          <target state="translated">랜덤 프로젝션 행렬의 크기와 분포는 데이터 집합의 두 샘플 사이의 쌍별 거리를 유지하도록 제어됩니다. 따라서 랜덤 프로젝션은 거리 기반 방법에 적합한 근사 기법입니다.</target>
        </trans-unit>
        <trans-unit id="42010c6a5240a459a14ffe4007a4a9d645a82c6f" translate="yes" xml:space="preserve">
          <source>The dimensions of one patch.</source>
          <target state="translated">한 패치의 치수입니다.</target>
        </trans-unit>
        <trans-unit id="16b41d4b4452bd551af457fee2f8b60407215654" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet).</source>
          <target state="translated">중량 분포 (Dirichlet)에 대한 각 성분의 디리클레 농도.</target>
        </trans-unit>
        <trans-unit id="6b13240ddd6531c8fdb797758cabfbb4633f5e80" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;:</source>
          <target state="translated">중량 분포 (Dirichlet)에 대한 각 성분의 디리클레 농도. 유형은 &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; 에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="b470809fd29296951902d887ca553f032b8a40c4" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to &lt;code&gt;1. / n_components&lt;/code&gt;.</source>
          <target state="translated">중량 분포 (Dirichlet)에 대한 각 성분의 디리클레 농도. 이것은 일반적으로 문헌에서 감마라고합니다. 농도가 높을수록 중앙에 더 많은 질량이 들어가고 더 많은 성분이 활성화되고 농도가 낮을수록 혼합물 무게의 가장 자리에 더 많은 질량이 생성됩니다. 매개 변수의 값은 0보다 커야합니다. None이면 &lt;code&gt;1. / n_components&lt;/code&gt; 설정됩니다 . / n_components .</target>
        </trans-unit>
        <trans-unit id="cbf40d766d38685ebd59a9222ccafd32de07a34f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Bayesian regression include:</source>
          <target state="translated">베이지안 회귀의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ec84046fbad625e7f2f2f6744eab5866f4d72369" translate="yes" xml:space="preserve">
          <source>The disadvantages of GBRT are:</source>
          <target state="translated">GBRT의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cfaf1a3af8c2483843ec10c36faa80968b886bf2" translate="yes" xml:space="preserve">
          <source>The disadvantages of Gaussian processes include:</source>
          <target state="translated">가우스 프로세스의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c8e0c1974308fc5ae09e195effcd7396a8db8601" translate="yes" xml:space="preserve">
          <source>The disadvantages of Multi-layer Perceptron (MLP) include:</source>
          <target state="translated">MLP (Multi-layer Perceptron)의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c71179ab39a085455ac6f6888a2f6fd3afe84d6f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Stochastic Gradient Descent include:</source>
          <target state="translated">확률 적 그라데이션 하강의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="84c28c658fb7eb00eb9347365efe97c2f04c30f3" translate="yes" xml:space="preserve">
          <source>The disadvantages of decision trees include:</source>
          <target state="translated">의사 결정 트리의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="47feb0b4f4476b90d13a7091a56a1b0f68ad3a62" translate="yes" xml:space="preserve">
          <source>The disadvantages of support vector machines include:</source>
          <target state="translated">서포트 벡터 머신의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="266f0e7f0f21de8e86a9f2ce2320de66f17358df" translate="yes" xml:space="preserve">
          <source>The disadvantages of the LARS method include:</source>
          <target state="translated">LARS 방법의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="fe9e13a1ad2c431f3f290607f881625ead9408c0" translate="yes" xml:space="preserve">
          <source>The disadvantages to using t-SNE are roughly:</source>
          <target state="translated">t-SNE 사용의 단점은 대략 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cd2f4d25623f857cb298f31868b60b26f2d776d4" translate="yes" xml:space="preserve">
          <source>The display objects store the computed values that were passed as arguments. This allows for the visualizations to be easliy combined using matplotlib&amp;rsquo;s API. In the following example, we place the displays next to each other in a row.</source>
          <target state="translated">표시 객체는 인수로 전달 된 계산 된 값을 저장합니다. 이를 통해 matplotlib의 API를 사용하여 시각화를 쉽게 결합 할 수 있습니다. 다음 예에서는 디스플레이를 한 줄로 나란히 배치합니다.</target>
        </trans-unit>
        <trans-unit id="a7ea673185d2a40501e91d63f573c417ac055cd1" translate="yes" xml:space="preserve">
          <source>The distance metric to use</source>
          <target state="translated">사용할 거리 측정 항목</target>
        </trans-unit>
        <trans-unit id="fade8506c426ca456f7b5f3ebd671dcd06e51421" translate="yes" xml:space="preserve">
          <source>The distance metric to use. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="translated">사용할 거리 측정법입니다. 그것과 동일 할 것이다 &lt;code&gt;metric&lt;/code&gt; 등의 파라미터 또는 동의어 '유클리드'만약 &lt;code&gt;metric&lt;/code&gt; '민코프 스키'와, 파라미터 세트 &lt;code&gt;p&lt;/code&gt; 2 파라미터 세트.</target>
        </trans-unit>
        <trans-unit id="0063e1a06973d2332dc280d77426f57f299b9005" translate="yes" xml:space="preserve">
          <source>The distance metric to use. Note that not all metrics are valid with all algorithms. Refer to the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; for a description of available algorithms. Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is &amp;lsquo;euclidean&amp;rsquo;.</source>
          <target state="translated">사용할 거리 측정 항목입니다. 모든 메트릭에 모든 메트릭이 유효한 것은 아닙니다. 사용 가능한 알고리즘에 대한 설명은 &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; 의 설명서를 참조하십시오 . 밀도 출력의 정규화는 유클리드 거리 측정법에 대해서만 정확합니다. 기본값은 '유클리드'입니다.</target>
        </trans-unit>
        <trans-unit id="3c4790f8b52fd2ea34a9c9dfcea72e27e7f13da0" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the p param equal to 2.)</source>
          <target state="translated">각 샘플 포인트에 대한 k- 이웃을 계산하는 데 사용되는 거리 메트릭입니다. DistanceMetric 클래스는 사용 가능한 메트릭 목록을 제공합니다. 기본 거리는 '유클리드'(p 매개 변수가 2 인 'minkowski'측정 항목)입니다.</target>
        </trans-unit>
        <trans-unit id="f687007319ee4f163c7b6668932fdc048712a932" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the param equal to 2.)</source>
          <target state="translated">각 샘플 포인트에 대해 주어진 반경 내에서 이웃을 계산하는 데 사용되는 거리 메트릭입니다. DistanceMetric 클래스는 사용 가능한 메트릭 목록을 제공합니다. 기본 거리는 '유클리드'(매개 변수가 2 인 'minkowski'측정 항목)입니다.</target>
        </trans-unit>
        <trans-unit id="8c96fb568d2fd154ab3cf2ef1e009cb92f123321" translate="yes" xml:space="preserve">
          <source>The distance metric used. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="translated">사용 된 거리 측정법입니다. 그것과 동일 할 것이다 &lt;code&gt;metric&lt;/code&gt; 등의 파라미터 또는 동의어 '유클리드'만약 &lt;code&gt;metric&lt;/code&gt; '민코프 스키'와, 파라미터 세트 &lt;code&gt;p&lt;/code&gt; 2 파라미터 세트.</target>
        </trans-unit>
        <trans-unit id="a36fd43a3aae80628a6e6b30a7a5575b2bb66390" translate="yes" xml:space="preserve">
          <source>The distances between the row vectors of &lt;code&gt;X&lt;/code&gt; and the row vectors of &lt;code&gt;Y&lt;/code&gt; can be evaluated using &lt;a href=&quot;generated/sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;pairwise_distances&lt;/code&gt;&lt;/a&gt;. If &lt;code&gt;Y&lt;/code&gt; is omitted the pairwise distances of the row vectors of &lt;code&gt;X&lt;/code&gt; are calculated. Similarly, &lt;a href=&quot;generated/sklearn.metrics.pairwise.pairwise_kernels#sklearn.metrics.pairwise.pairwise_kernels&quot;&gt;&lt;code&gt;pairwise.pairwise_kernels&lt;/code&gt;&lt;/a&gt; can be used to calculate the kernel between &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; using different kernel functions. See the API reference for more details.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; 의 행 벡터와 &lt;code&gt;Y&lt;/code&gt; 의 행 벡터 사이의 거리는 &lt;a href=&quot;generated/sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;pairwise_distances&lt;/code&gt; 를&lt;/a&gt; 사용하여 계산할 수 있습니다 . 경우 &lt;code&gt;Y&lt;/code&gt; 는 의 행 벡터의 페어 거리 생략 &lt;code&gt;X&lt;/code&gt; 를 산출한다. 마찬가지로 &lt;a href=&quot;generated/sklearn.metrics.pairwise.pairwise_kernels#sklearn.metrics.pairwise.pairwise_kernels&quot;&gt; &lt;code&gt;pairwise.pairwise_kernels&lt;/code&gt; &lt;/a&gt; 는 다른 커널 함수를 사용하여 &lt;code&gt;X&lt;/code&gt; 와 &lt;code&gt;Y&lt;/code&gt; 사이의 커널을 계산하는 데 사용할 수 있습니다 . 자세한 내용은 API 참조를 참조하세요.</target>
        </trans-unit>
        <trans-unit id="7e12a027c556a152948ba7c771e2cb65d4ac3e99" translate="yes" xml:space="preserve">
          <source>The distinct labels used in classifying instances.</source>
          <target state="translated">인스턴스 분류에 사용되는 고유 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="2333c2cc37e50157e20c383f74b00f781cb37a1b" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; is asserted by the fact that &lt;code&gt;p&lt;/code&gt; is defining an eps-embedding with good probability as defined by:</source>
          <target state="translated">랜덤 프로젝션 &lt;code&gt;p&lt;/code&gt; 에 의해 야기 된 왜곡 은 &lt;code&gt;p&lt;/code&gt; 가 다음에 의해 정의 된 바와 같이 좋은 확률로 eps 임베딩을 정의 한다는 사실에 의해 주장 된다 :</target>
        </trans-unit>
        <trans-unit id="befef2d542b10663820383e2f1e59843332d14a0" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; only changes the distance between two points by a factor (1 +- eps) in an euclidean space with good probability. The projection &lt;code&gt;p&lt;/code&gt; is an eps-embedding as defined by:</source>
          <target state="translated">랜덤 프로젝션 &lt;code&gt;p&lt;/code&gt; 에 의해 도입 된 왜곡 은 유클리드 공간에서 두 점 사이의 거리를 유클리드 공간에서 인자 (1 + -eps)만큼만 변경합니다. 투영 &lt;code&gt;p&lt;/code&gt; 는 다음에 의해 정의 된 eps 임베딩입니다.</target>
        </trans-unit>
        <trans-unit id="b6d88e183a439e17415e4b06c82155c9e34fa344" translate="yes" xml:space="preserve">
          <source>The distributions in &lt;code&gt;scipy.stats&lt;/code&gt; prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via &lt;code&gt;np.random.seed&lt;/code&gt; or set using &lt;code&gt;np.random.set_state&lt;/code&gt;. However, beginning scikit-learn 0.18, the &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module sets the random state provided by the user if scipy &amp;gt;= 0.16 is also available.</source>
          <target state="translated">버전 scipy 0.16 이전의 &lt;code&gt;scipy.stats&lt;/code&gt; 에 있는 분포에서는 임의 상태를 지정할 수 없습니다. 대신, &lt;code&gt;np.random.seed&lt;/code&gt; 를 통해 시드 하거나 &lt;code&gt;np.random.set_state&lt;/code&gt; 를 사용하여 설정할 수있는 전역 numpy 임의 상태를 사용 합니다 . 그러나 scikit-learn 0.18부터는&lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; 의&lt;/a&gt; 모듈 scipy&amp;gt; = 0.16도 가능한 경우, 사용자에 의해 제공되는 임의의 상태를 설정한다.</target>
        </trans-unit>
        <trans-unit id="e4d3a4f449c4df99f288fa9346370fef4e50edc3" translate="yes" xml:space="preserve">
          <source>The dual gap at the end of the optimization for the optimal alpha (&lt;code&gt;alpha_&lt;/code&gt;).</source>
          <target state="translated">최적 알파에 대한 최적화 종료시 이중 간격 ( &lt;code&gt;alpha_&lt;/code&gt; ) 입니다.</target>
        </trans-unit>
        <trans-unit id="1c5b7990e4f078095970dbd44954a4ff0f6bdb22" translate="yes" xml:space="preserve">
          <source>The dual gaps at the end of the optimization for each alpha.</source>
          <target state="translated">각 알파에 대한 최적화 종료시 이중 간격.</target>
        </trans-unit>
        <trans-unit id="eceb770d3ab0f05ce2328559f3653f372ef1e2c9" translate="yes" xml:space="preserve">
          <source>The dual problem is</source>
          <target state="translated">이중 문제는</target>
        </trans-unit>
        <trans-unit id="aa2b6f5781584ece88d8eb541efa4219ed937f23" translate="yes" xml:space="preserve">
          <source>The dual problem to the primal is</source>
          <target state="translated">원시에 대한 이중 문제는</target>
        </trans-unit>
        <trans-unit id="c16a68f2d6169f4f9aa27b4b94f9b3db38d73c1f" translate="yes" xml:space="preserve">
          <source>The dummy regression model predicts a constant frequency. This model does not attribute the same tied rank to all samples but is none-the-less globally well calibrated (to estimate the mean frequency of the entire population).</source>
          <target state="translated">더미 회귀 모델은 일정한 빈도를 예측합니다. 이 모델은 모든 샘플에 동일한 순위를 부여하지는 않지만 그럼에도 불구하고 전체 모집단의 평균 빈도를 추정하기 위해 전 세계적으로 잘 보정되었습니다.</target>
        </trans-unit>
        <trans-unit id="b5eac57a33eeb0d09a6d5ebfcbe611bef2da07d8" translate="yes" xml:space="preserve">
          <source>The edges of each bin. Contain arrays of varying shapes &lt;code&gt;(n_bins_, )&lt;/code&gt; Ignored features will have empty arrays.</source>
          <target state="translated">각 용지함의 가장자리. 다양한 모양의 배열을 포함 &lt;code&gt;(n_bins_, )&lt;/code&gt; 무시 된 피처에는 빈 배열이 있습니다.</target>
        </trans-unit>
        <trans-unit id="50c07dc6eab301b9d9953e2bd5849c6aa3abe8d7" translate="yes" xml:space="preserve">
          <source>The effect of the transformer is weaker than on the synthetic data. However, the transform induces a decrease of the MAE.</source>
          <target state="translated">변압기의 효과는 합성 데이터보다 약합니다. 그러나, 변환은 MAE의 감소를 유도한다.</target>
        </trans-unit>
        <trans-unit id="8b7256d0b08ed885751aee08f85ac36bb70ae336" translate="yes" xml:space="preserve">
          <source>The effective size of the batch is computed here. If there are no more jobs to dispatch, return False, else return True.</source>
          <target state="translated">배치의 유효 크기가 여기에서 계산됩니다. 디스패치 할 작업이 더 이상 없으면 False를, 그렇지 않으면 True를 리턴하십시오.</target>
        </trans-unit>
        <trans-unit id="a044cf1265a1684d79e7b48898279d20ad6f0dac" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities</source>
          <target state="translated">사용할 고유 값 분해 전략입니다. AMG를 사용하려면 pyamg를 설치해야합니다. 매우 큰 희소 문제에서 더 빠를 수 있지만 불안정성을 초래할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="1dc685b8ff4dd3ccaa26f5f653c44ef0324bd82f" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities.</source>
          <target state="translated">사용할 고유 값 분해 전략입니다. AMG를 사용하려면 pyamg를 설치해야합니다. 매우 큰 희소 문제에서 더 빠를 수 있지만 불안정성을 초래할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="e9d800f8ff5b9787b828397b459d27c3b21cb754" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems.</source>
          <target state="translated">사용할 고유 값 분해 전략입니다. AMG를 사용하려면 pyamg를 설치해야합니다. 매우 크고 희소 한 문제에서는 더 빠를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f14ed9796539634c3492800089020cfe2ebbd7af" translate="yes" xml:space="preserve">
          <source>The elastic net optimization function varies for mono and multi-outputs.</source>
          <target state="translated">탄성 순 최적화 기능은 모노 및 멀티 출력에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="4d9a50594cec642f130c977c2c5c095fc619b302" translate="yes" xml:space="preserve">
          <source>The elements of the estimators parameter, having been fitted on the training data. If an estimator has been set to &lt;code&gt;'drop'&lt;/code&gt;, it will not appear in &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="translated">훈련 데이터에 맞춰진 추정자 매개 변수의 요소입니다. 추정량로 설정되어있는 경우 &lt;code&gt;'drop'&lt;/code&gt; , 그것은에 표시되지 않습니다 &lt;code&gt;estimators_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e0191353bc3bca50b627784afbd555eb11c1f3dd" translate="yes" xml:space="preserve">
          <source>The empirical covariance matrix of a sample can be computed using the &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt;&lt;code&gt;empirical_covariance&lt;/code&gt;&lt;/a&gt; function of the package, or by fitting an &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; object to the data sample with the &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt;&lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Be careful that results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately. More precisely, if &lt;code&gt;assume_centered=False&lt;/code&gt;, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and &lt;code&gt;assume_centered=True&lt;/code&gt; should be used.</source>
          <target state="translated">샘플의 경험적 공분산 행렬은 패키지 의 &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt; &lt;code&gt;empirical_covariance&lt;/code&gt; &lt;/a&gt; 함수를 사용 하거나 &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt; &lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt; &lt;/a&gt; 방법으로 데이터 샘플에 &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; &lt;/a&gt; 객체를 피팅 하여 계산할 수 있습니다 . 결과는 데이터가 중심에 있는지 여부에 따라 달라 &lt;code&gt;assume_centered&lt;/code&gt; 매개 변수를 정확하게 사용할 수 있습니다 . 보다 정확하게, &lt;code&gt;assume_centered=False&lt;/code&gt; 인 경우 테스트 세트는 학습 세트와 동일한 평균 벡터를 갖습니다. 그렇지 않은 경우, 둘 다 사용자가 중심에 두어야하며 &lt;code&gt;assume_centered=True&lt;/code&gt; 중심 = 참을 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="d4b0da98d5cb047aa932ef62858edca40e188517" translate="yes" xml:space="preserve">
          <source>The encoded signal (Y).</source>
          <target state="translated">인코딩 된 신호 (Y).</target>
        </trans-unit>
        <trans-unit id="cc2fdf3023f61dc3df7bba02538cce691e7cf2cd" translate="yes" xml:space="preserve">
          <source>The energy function measures the quality of a joint assignment:</source>
          <target state="translated">에너지 함수는 공동 할당의 품질을 측정합니다.</target>
        </trans-unit>
        <trans-unit id="1bfb5cae50ef0b1032c729ea4ab68ff502e7f906" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;test_fold[i]&lt;/code&gt; represents the index of the test set that sample &lt;code&gt;i&lt;/code&gt; belongs to. It is possible to exclude sample &lt;code&gt;i&lt;/code&gt; from any test set (i.e. include sample &lt;code&gt;i&lt;/code&gt; in every training set) by setting &lt;code&gt;test_fold[i]&lt;/code&gt; equal to -1.</source>
          <target state="translated">&lt;code&gt;test_fold[i]&lt;/code&gt; 항목 은 샘플 &lt;code&gt;i&lt;/code&gt; 가 속하는 테스트 세트의 색인을 나타냅니다 . &lt;code&gt;test_fold[i]&lt;/code&gt; 설정 하여 모든 테스트 세트에서 샘플 &lt;code&gt;i&lt;/code&gt; 를 제외 할 수 있습니다 (예 : 모든 트레이닝 세트에 샘플 &lt;code&gt;i&lt;/code&gt; 포함 ) . 를 -1 .</target>
        </trans-unit>
        <trans-unit id="503425d1f7e07b98f32316b1d85343640a4e1294" translate="yes" xml:space="preserve">
          <source>The equivalence between &lt;code&gt;alpha&lt;/code&gt; and the regularization parameter of SVM, &lt;code&gt;C&lt;/code&gt; is given by &lt;code&gt;alpha = 1 / C&lt;/code&gt; or &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt;, depending on the estimator and the exact objective function optimized by the model.</source>
          <target state="translated">간의 등가 &lt;code&gt;alpha&lt;/code&gt; 및 SVM의 정규화 파라미터는, &lt;code&gt;C&lt;/code&gt; 가 주어진다 &lt;code&gt;alpha = 1 / C&lt;/code&gt; 또는 &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt; 추정기 모델과 정확한 최적화 목적 함수에 따라.</target>
        </trans-unit>
        <trans-unit id="2fec157ea85a1f513b11852f437f8dc13c9361bb" translate="yes" xml:space="preserve">
          <source>The error message or a substring of the error message.</source>
          <target state="translated">오류 메시지 또는 오류 메시지의 하위 문자열</target>
        </trans-unit>
        <trans-unit id="67a9569df158acc253e75d9fa812424d1f2b58c6" translate="yes" xml:space="preserve">
          <source>The estimated (sparse) precision matrix.</source>
          <target state="translated">추정 된 (희소 한) 정밀 행렬.</target>
        </trans-unit>
        <trans-unit id="cdf0d65945f589fd5da3781e66a0a81a30e934b1" translate="yes" xml:space="preserve">
          <source>The estimated covariance matrix.</source>
          <target state="translated">추정 된 공분산 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="4b02a2f0d176ac6fb9b484de9fcc2a6b0be67a5a" translate="yes" xml:space="preserve">
          <source>The estimated labels.</source>
          <target state="translated">예상 라벨입니다.</target>
        </trans-unit>
        <trans-unit id="77a18d9bfc3d1fc8448c4fb559b59c860611bbc7" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;.</source>
          <target state="translated">Tipping and Bishop 1999의 확률 적 PCA 모델에 따른 잡음 공분산 추정치. C. Bishop의&amp;ldquo;패턴 인식 및 기계 학습&amp;rdquo;, 12.2.1 p. 574 또는 &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="735cdf6c1c61f38b1092aa1bd4262b34c0d5f0f3" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;. It is required to compute the estimated data covariance and score samples.</source>
          <target state="translated">Tipping and Bishop 1999의 확률 적 PCA 모델에 따른 잡음 공분산 추정치. C. Bishop의&amp;ldquo;패턴 인식 및 기계 학습&amp;rdquo;, 12.2.1 p. 574 또는 &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt; . 추정 된 데이터 공분산 및 스코어 샘플을 계산해야합니다.</target>
        </trans-unit>
        <trans-unit id="cc49065d7efcb582d87899f7997c53b1f8d60c78" translate="yes" xml:space="preserve">
          <source>The estimated noise variance for each feature.</source>
          <target state="translated">각 기능에 대한 추정 노이즈 분산입니다.</target>
        </trans-unit>
        <trans-unit id="960e70b3d078f74487289ecbb537290d1f7473d4" translate="yes" xml:space="preserve">
          <source>The estimated number of components. Relevant when &lt;code&gt;n_components=None&lt;/code&gt;.</source>
          <target state="translated">예상 구성 요소 수입니다. &lt;code&gt;n_components=None&lt;/code&gt; 일 때 관련 .</target>
        </trans-unit>
        <trans-unit id="df011fa4ccaad62cb4a83a0955409a4c4b3035c8" translate="yes" xml:space="preserve">
          <source>The estimated number of components. When n_components is set to &amp;lsquo;mle&amp;rsquo; or a number between 0 and 1 (with svd_solver == &amp;lsquo;full&amp;rsquo;) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.</source>
          <target state="translated">예상 구성 요소 수입니다. n_components가 'mle'로 설정되거나 0과 1 사이의 숫자 (svd_solver == 'full'사용) 인 경우이 숫자는 입력 데이터에서 추정됩니다. 그렇지 않으면 매개 변수 n_components 또는 n_components가 없음 인 경우 더 작은 n_features 및 n_samples 값과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cd106f5afe35a52902113758dbc47d95941530e6" translate="yes" xml:space="preserve">
          <source>The estimated number of connected components in the graph.</source>
          <target state="translated">그래프에서 연결된 구성 요소의 예상 수입니다.</target>
        </trans-unit>
        <trans-unit id="54994eb61f1dce6c8894f0827b84346580c11cb2" translate="yes" xml:space="preserve">
          <source>The estimation of the EXPERIENCE coefficient is now less variable and remain important for all models trained during cross-validation.</source>
          <target state="translated">EXPERIENCE 계수의 추정은 이제 덜 가변적이며 교차 검증 중에 훈련 된 모든 모델에 대해 중요합니다.</target>
        </trans-unit>
        <trans-unit id="a98c4d1bc7caeb1163bf14c014655348909c79f3" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts.</source>
          <target state="translated">모델의 추정은 p 개의 서브 샘플 포인트의 모든 가능한 조합의 서브 인구의 기울기와 절편을 계산하여 수행됩니다. 절편이 적합하면 p는 n_features + 1 이상이어야합니다. 그런 다음 최종 기울기와 절편은 이러한 기울기와 절편의 공간 중앙값으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="aed02faa2e2402cc32a5968b7e86a54cff535c4e" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.</source>
          <target state="translated">모형의 추정은 관측치의 한계 로그 우도를 반복적으로 최대화하여 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="44fe06813d3627aaa56531c52961387365e10d66" translate="yes" xml:space="preserve">
          <source>The estimation of the number of degrees of freedom is given by:</source>
          <target state="translated">자유도의 추정은 다음과 같이 주어진다 :</target>
        </trans-unit>
        <trans-unit id="2bed56c931836016f42a33afdedfac5cb8f555f4" translate="yes" xml:space="preserve">
          <source>The estimator also implements &lt;code&gt;partial_fit&lt;/code&gt;, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.</source>
          <target state="translated">추정기는 또한 &lt;code&gt;partial_fit&lt;/code&gt; 을 구현합니다. 하여 미니 배치에서 한 번만 반복하여 사전을 업데이트합니다. 데이터를 처음부터 쉽게 사용할 수 없거나 데이터가 메모리에 맞지 않을 때 온라인 학습에 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="efc1e3841cf31ff119d8e77fb55d7093c1df4dd4" translate="yes" xml:space="preserve">
          <source>The estimator objects for each cv split. This is available only if &lt;code&gt;return_estimator&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">각 cv 스플릿에 대한 추정기 객체. &lt;code&gt;return_estimator&lt;/code&gt; 매개 변수가 &lt;code&gt;True&lt;/code&gt; 로 설정된 경우에만 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f33cc8b973e26f8a535386dbd610021f87dfdf8f" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned</source>
          <target state="translated">복제 할 추정기 또는 추정기 그룹</target>
        </trans-unit>
        <trans-unit id="c1ff61d32d7a7a2deb658878854ba14950f2c891" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned.</source>
          <target state="translated">복제 할 추정기 또는 추정기 그룹입니다.</target>
        </trans-unit>
        <trans-unit id="65944411996b76786bcb63db12ac31b08b55648a" translate="yes" xml:space="preserve">
          <source>The estimator that provides the initial predictions. Set via the &lt;code&gt;init&lt;/code&gt; argument or &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">초기 예측을 제공하는 추정기입니다. &lt;code&gt;init&lt;/code&gt; 인수 또는 &lt;code&gt;loss.init_estimator&lt;/code&gt; 를 통해 설정하십시오 .</target>
        </trans-unit>
        <trans-unit id="f7ae9218cda2abb5c086ad75a4ab1e94b87ebec1" translate="yes" xml:space="preserve">
          <source>The estimator to use at each step of the round-robin imputation. If &lt;code&gt;sample_posterior&lt;/code&gt; is True, the estimator must support &lt;code&gt;return_std&lt;/code&gt; in its &lt;code&gt;predict&lt;/code&gt; method.</source>
          <target state="translated">라운드 로빈 대치의 각 단계에서 사용할 추정기입니다. 경우 &lt;code&gt;sample_posterior&lt;/code&gt; 가 True 인, 추가 지원해야 &lt;code&gt;return_std&lt;/code&gt; 을 자사의 &lt;code&gt;predict&lt;/code&gt; 방법.</target>
        </trans-unit>
        <trans-unit id="e68c5f93929a570a29c52208fc20a3ad933e6e26" translate="yes" xml:space="preserve">
          <source>The estimator to visualize.</source>
          <target state="translated">시각화 할 추정기입니다.</target>
        </trans-unit>
        <trans-unit id="5314881b9357d8103e571e2b27bb56496dd3b052" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute, but can be accessed by index or name by indexing (with &lt;code&gt;[idx]&lt;/code&gt;) the Pipeline:</source>
          <target state="translated">파이프 라인의 추정자는 &lt;code&gt;steps&lt;/code&gt; 속성에 목록으로 저장되지만 파이프 라인 을 인덱싱 ( &lt;code&gt;[idx]&lt;/code&gt; 사용 )하여 인덱스 또는 이름으로 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ade400270e658832b6e128c7ef187979eae33356" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute:</source>
          <target state="translated">파이프 라인의 추정기는 &lt;code&gt;steps&lt;/code&gt; 속성에 목록으로 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="bc948c82039e32a77f975544660f3043a6111d3d" translate="yes" xml:space="preserve">
          <source>The estimators of the pipeline can be retrieved by index:</source>
          <target state="translated">파이프 라인의 추정치는 인덱스로 검색 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a46349c122935a13db8091d877d4179b37995289" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. For example, it is possible to use these estimators to turn a binary classifier or a regressor into a multiclass classifier. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime performance improves.</source>
          <target state="translated">이 모듈에 제공된 추정기는 메타 추정기입니다. 생성자에 기본 추정기가 제공되어야합니다. 예를 들어,이 추정기를 사용하여 이진 분류기 또는 회귀자를 멀티 클래스 분류기로 바꿀 수 있습니다. 정확성 또는 런타임 성능이 향상되기를 희망하여 이러한 추정기를 멀티 클래스 추정기와 함께 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="1868049da2c813b87c2e3d48a5c71f72cc892852" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. The meta-estimator extends single output estimators to multioutput estimators.</source>
          <target state="translated">이 모듈에 제공된 추정기는 메타 추정기입니다. 생성자에 기본 추정기가 제공되어야합니다. 메타 추정기는 단일 출력 추정기를 다중 출력 추정기로 확장합니다.</target>
        </trans-unit>
        <trans-unit id="fe77b9f934c3d4135d1310eb25c7f2c922f35971" translate="yes" xml:space="preserve">
          <source>The exact API of all functions and classes, as given by the docstrings. The API documents expected types and allowed features for all functions, and all parameters available for the algorithms.</source>
          <target state="translated">docstring이 제공하는 모든 함수 및 클래스의 정확한 API. API는 모든 기능에 대해 예상되는 유형과 허용 된 기능 및 알고리즘에 사용 가능한 모든 매개 변수를 문서화합니다.</target>
        </trans-unit>
        <trans-unit id="213a31afd656f2a0c8b37dd55b40fc5308db3ca1" translate="yes" xml:space="preserve">
          <source>The exact additive chi squared kernel.</source>
          <target state="translated">정확한 추가 카이 제곱 커널.</target>
        </trans-unit>
        <trans-unit id="14b1a49c77fdcb8a0d3244090e7c6df0b2a6cd07" translate="yes" xml:space="preserve">
          <source>The exact chi squared kernel.</source>
          <target state="translated">정확한 카이 제곱 커널.</target>
        </trans-unit>
        <trans-unit id="21c56a998f59a2c6be6550db485efcc395359e33" translate="yes" xml:space="preserve">
          <source>The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of &lt;code&gt;n_estimators&lt;/code&gt; at which the error stabilizes.</source>
          <target state="translated">아래 예제는 훈련 중 새 트리를 추가 할 때 OOB 오류를 측정하는 방법을 보여줍니다. 결과 플롯을 통해 실무자는 적절한 &lt;code&gt;n_estimators&lt;/code&gt; 값을 추정 할 수 있습니다. 오류가 안정화되는 .</target>
        </trans-unit>
        <trans-unit id="556b3073adbd8a84d1d4b2fe21db4807c2daf4c8" translate="yes" xml:space="preserve">
          <source>The example below uses a support vector classifier with a non-linear kernel to build a model with optimized hyperparameters by grid search. We compare the performance of non-nested and nested CV strategies by taking the difference between their scores.</source>
          <target state="translated">아래 예제는 비선형 커널과 함께 지원 벡터 분류기를 사용하여 그리드 검색에 의해 최적화 된 하이퍼 파라미터가있는 모델을 만듭니다. 중첩되지 않은 CV 전략과 중첩 된 CV 전략의 성과를 점수 간의 차이로 비교합니다.</target>
        </trans-unit>
        <trans-unit id="4cd198cf307950e0cb10d5d2e6b86c91624772aa" translate="yes" xml:space="preserve">
          <source>The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features.</source>
          <target state="translated">이 예에서는 실수 피처의 이산화 유무에 관계없이 선형 회귀 (선형 모델) 및 의사 결정 트리 (트리 기반 모델)의 예측 결과를 비교합니다.</target>
        </trans-unit>
        <trans-unit id="3101cd4fceb55d1be63d1bff5cb3f45996fd9aa0" translate="yes" xml:space="preserve">
          <source>The example demonstrates syntax and speed only; it doesn&amp;rsquo;t actually do anything useful with the extracted vectors. See the example scripts {document_classification_20newsgroups,clustering}.py for actual learning on text documents.</source>
          <target state="translated">이 예제는 구문과 속도 만 보여줍니다. 실제로 추출 된 벡터에는 유용한 기능이 없습니다. 텍스트 문서에 대한 실제 학습은 예제 스크립트 {document_classification_20newsgroups, clustering} .py를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="32ed3af5edbdaa27f9eba4e5a30255c6afab4414" translate="yes" xml:space="preserve">
          <source>The example is engineered to show the effect of the choice of different metrics. It is applied to waveforms, which can be seen as high-dimensional vector. Indeed, the difference between metrics is usually more pronounced in high dimension (in particular for euclidean and cityblock).</source>
          <target state="translated">이 예제는 다양한 메트릭 선택의 효과를 보여 주도록 설계되었습니다. 고차원 벡터로 볼 수있는 파형에 적용됩니다. 실제로 지표 간 차이는 일반적으로 높은 차원에서 특히 두드러집니다 (특히 유클리드 및 도시 블록).</target>
        </trans-unit>
        <trans-unit id="36ed82faabe70da57df289b8a54bb16e8830773a" translate="yes" xml:space="preserve">
          <source>The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge.</source>
          <target state="translated">이 예는 능선의 예측이 데이터 세트에 존재하는 특이 치에 의해 크게 영향을 받는다는 것을 보여줍니다. 모델이 선형 손실을 사용하기 때문에 Huber 회귀 분석은 특이 치의 영향을 덜받습니다. Huber 회귀 변수의 매개 변수 epsilon이 증가함에 따라 의사 결정 기능은 능선의 결정 함수에 접근합니다.</target>
        </trans-unit>
        <trans-unit id="889bc091af3b0106e91a81884d921d1b2df9bdd6" translate="yes" xml:space="preserve">
          <source>The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected &lt;code&gt;n_components=5&lt;/code&gt; which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.</source>
          <target state="translated">아래의 예는 고정 된 개수의 성분을 가진 가우스 혼합 모델을 이전의 디 리클 렛 공정을 사용하는 변형 가우스 혼합 모델과 비교합니다. 여기서 클래식 가우시안 혼합물에는 2 개의 군집으로 구성된 데이터 집합에 5 개의 구성 요소가 장착되어 있습니다. 우리는 이전에 Dirichlet 공정을 사용하는 변형 가우시안 혼합물이 2 개의 성분으로 만 제한 할 수있는 반면 가우시안 혼합물은 사용자가 우선적으로 설정해야하는 고정 된 수의 성분으로 데이터에 적합하다는 것을 알 수 있습니다. 이 경우 사용자는 이 완구 데이터 세트의 실제 생성 분포와 일치하지 않는 &lt;code&gt;n_components=5&lt;/code&gt; 를 선택 했습니다 . 관측이 거의없는 경우, 사전에 Dirichlet 공정을 사용하는 변형 가우스 혼합 모델은 보수적 인 입장을 취하고 하나의 구성 요소에만 적합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2cf55b61801dba638e3df4951d7127011c13c263" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation &lt;a href=&quot;#veb2009&quot; id=&quot;id13&quot;&gt;[VEB2009]&lt;/a&gt;. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">상호 정보에 대한 기대 값은 다음 식 &lt;a href=&quot;#veb2009&quot; id=&quot;id13&quot;&gt;[VEB2009]를&lt;/a&gt; 사용하여 계산할 수 있습니다 . 이 방정식에서 \ (a_i = | U_i | \) (\ (U_i \)의 요소 수) 및 \ (b_j = | V_j | \) (\ (V_j \)의 요소 수).</target>
        </trans-unit>
        <trans-unit id="e5c4a6233958dced6338c85ecba7ef5860ffcbbc" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">상호 정보에 대한 예상 값은 다음 방정식 [VEB2009]을 사용하여 계산할 수 있습니다. 이 방정식에서 \ (a_i = | U_i | \) (\ (U_i \)의 요소 수) 및 \ (b_j = | V_j | \) (\ (V_j \)의 요소 수)</target>
        </trans-unit>
        <trans-unit id="5075a63ec7be1cbe42a641e5ae277f9e4b63f160" translate="yes" xml:space="preserve">
          <source>The experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The first figure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the assumption of feature-independence and result in an overly confident classifier, which is indicated by the typical transposed-sigmoid curve.</source>
          <target state="translated">이 실험은 20 개의 특징을 가진 100,000 개의 샘플 (1,000 개가 모델 피팅에 사용됨)로 이진 분류를위한 인공 데이터 세트에서 수행됩니다. 20 개의 기능 중 2 개만 유익하고 10 개는 중복입니다. 첫 번째 그림은 등장 성 교정과 시그 모이 드 교정이 모두 포함 된 로지스틱 회귀 분석, 가우시안 순진 베이 및 가우시안 순 베이로 얻은 추정 확률을 보여줍니다. 범례에서보고 된 Brier 점수로 교정 성능을 평가합니다 (더 작을수록 좋습니다). 여기서 원시 가우시안 순진 베이 즈가 매우 나쁘게 수행하는 동안 로지스틱 회귀 분석이 잘 조정되어 있음을 알 수 있습니다. 이는 기능 독립성의 가정을 위반하는 중복 기능으로 인해 과도하게 신뢰할 수있는 분류기를 생성하며, 이는 전형적인 조옮김 시그 모이 드 곡선으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="b61a812b707dfd6196631d345ad23c1f697f044e" translate="yes" xml:space="preserve">
          <source>The experimental data presents a long tail distribution for &lt;code&gt;y&lt;/code&gt;. In all models, we predict the expected frequency of a random variable, so we will have necessarily fewer extreme values than for the observed realizations of that random variable. This explains that the mode of the histograms of model predictions doesn&amp;rsquo;t necessarily correspond to the smallest value. Additionally, the normal distribution used in &lt;code&gt;Ridge&lt;/code&gt; has a constant variance, while for the Poisson distribution used in &lt;code&gt;PoissonRegressor&lt;/code&gt; and &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;, the variance is proportional to the predicted expected value.</source>
          <target state="translated">실험 데이터는 &lt;code&gt;y&lt;/code&gt; 에 대한 긴 꼬리 분포를 나타냅니다 . 모든 모델에서 랜덤 변수의 예상 빈도를 예측하므로 해당 랜덤 변수의 관측 된 실현보다 극단 값이 반드시 더 적을 것입니다. 이것은 모델 예측의 히스토그램 모드가 반드시 가장 작은 값에 해당하는 것은 아니라는 것을 설명합니다. 또한 &lt;code&gt;Ridge&lt;/code&gt; 에서 사용되는 정규 분포 는 분산이 일정하지만 &lt;code&gt;PoissonRegressor&lt;/code&gt; 및 &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; 에 사용되는 Poisson 분포의 경우 분산은 예측 된 기대 값에 비례합니다.</target>
        </trans-unit>
        <trans-unit id="e1db213d528f30cf745c1554ad2a43991932646f" translate="yes" xml:space="preserve">
          <source>The explained variance or ndarray if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">'multioutput'이 'raw_values'인 경우 설명 된 분산 또는 ndarray입니다.</target>
        </trans-unit>
        <trans-unit id="dff7adf6114ae66681ccc96cdb5c8abce36c0555" translate="yes" xml:space="preserve">
          <source>The explicit constant as predicted by the &amp;ldquo;constant&amp;rdquo; strategy. This parameter is useful only for the &amp;ldquo;constant&amp;rdquo; strategy.</source>
          <target state="translated">&quot;일정한&quot;전략에 의해 예측 된 명시 적 상수. 이 매개 변수는 &quot;일정한&quot;전략에만 유용합니다.</target>
        </trans-unit>
        <trans-unit id="96483cbe0c7b434b8387960f89d9c2700f0af59b" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate [default 0.5].</source>
          <target state="translated">역 스케일링 학습률의 지수 [기본값 0.5].</target>
        </trans-unit>
        <trans-unit id="fe088102cec241da3f7e07b043fd901378b61425" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate.</source>
          <target state="translated">역 스케일링 학습률에 대한 지수입니다.</target>
        </trans-unit>
        <trans-unit id="1fa85799d065de026e0253585d451384c9ee6013" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to &amp;lsquo;invscaling&amp;rsquo;. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">역 스케일링 학습률의 지수입니다. learning_rate가 'invscaling'으로 설정된 경우 효과적인 학습 속도를 업데이트하는 데 사용됩니다. solver = 'sgd'인 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="c7acf68e47ad4fe8cc8cf54d82e56ffc97c60a11" translate="yes" xml:space="preserve">
          <source>The exponent for the base kernel</source>
          <target state="translated">기본 커널의 지수</target>
        </trans-unit>
        <trans-unit id="fe68de8b995171a61c550ec2f7815b01a2d28ec7" translate="yes" xml:space="preserve">
          <source>The exponentiated version of the kernel, which is usually preferable.</source>
          <target state="translated">커널의 지수 버전으로 일반적으로 바람직합니다.</target>
        </trans-unit>
        <trans-unit id="d1490a4bcb15c22959ee30800b231e4413480004" translate="yes" xml:space="preserve">
          <source>The external estimator fit on the reduced dataset.</source>
          <target state="translated">외부 추정기는 축소 된 데이터 세트에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="18d0cd5609d8f78695dc43242ffb9cf995a65ee9" translate="yes" xml:space="preserve">
          <source>The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):</source>
          <target state="translated">추출 된 TF-IDF 벡터는 30000 차원 이상의 공간에서 샘플에 의해 평균 159 개의 0이 아닌 구성 요소 (0이 아닌 0이 아닌 특징)로 매우 희박합니다.</target>
        </trans-unit>
        <trans-unit id="e5bc1ae1e0e90623cdbf461e900aded771c1b79d" translate="yes" xml:space="preserve">
          <source>The extracted dataset will only retain pictures of people that have at least &lt;code&gt;min_faces_per_person&lt;/code&gt; different pictures.</source>
          <target state="translated">추출 된 데이터 세트는 최소한 &lt;code&gt;min_faces_per_person&lt;/code&gt; 다른 사진 을 가진 사람의 사진 만 유지 합니다.</target>
        </trans-unit>
        <trans-unit id="413a1ee9e7f0b0741202aa45f471425edb5c2085" translate="yes" xml:space="preserve">
          <source>The extraction method used to extract clusters using the calculated reachability and ordering. Possible values are &amp;ldquo;xi&amp;rdquo; and &amp;ldquo;dbscan&amp;rdquo;.</source>
          <target state="translated">계산 된 도달 가능성 및 순서를 사용하여 클러스터를 추출하는 데 사용되는 추출 방법입니다. 가능한 값은 &quot;xi&quot;및 &quot;dbscan&quot;입니다.</target>
        </trans-unit>
        <trans-unit id="7cb7e9ef70987101280d538c40ac3f0463557635" translate="yes" xml:space="preserve">
          <source>The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.</source>
          <target state="translated">하이퍼 큐브 크기를 곱하는 요소입니다. 값이 클수록 클러스터 / 클래스가 분산되어 분류 작업이 쉬워집니다.</target>
        </trans-unit>
        <trans-unit id="65d18b0a37e6414a591311e75d9ded04a4293e5e" translate="yes" xml:space="preserve">
          <source>The factory can be any callable that takes no argument and return an instance of &lt;code&gt;ParallelBackendBase&lt;/code&gt;.</source>
          <target state="translated">팩토리는 인수를 취하지 않고 &lt;code&gt;ParallelBackendBase&lt;/code&gt; 인스턴스를 반환하는 모든 콜 러블이 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0cd5de0c793252ae54dc423f82b86e165f05af5f" translate="yes" xml:space="preserve">
          <source>The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&amp;rsquo;s paper. Note that it&amp;rsquo;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.</source>
          <target state="translated">RA Fisher 선생님이 처음 사용한 유명한 Iris 데이터베이스. 데이터 세트는 Fisher의 논문에서 가져 왔습니다. R과 동일하지만 UCI Machine Learning Repository와는 다르며 두 개의 잘못된 데이터 포인트가 있습니다.</target>
        </trans-unit>
        <trans-unit id="64135ddd2f4a94ed0611a8692e8099bf4451f815" translate="yes" xml:space="preserve">
          <source>The feature (e.g. &lt;code&gt;[0]&lt;/code&gt;) or pair of interacting features (e.g. &lt;code&gt;[(0, 1)]&lt;/code&gt;) for which the partial dependency should be computed.</source>
          <target state="translated">부분 종속성이 계산되어야 하는 특성 (예 : &lt;code&gt;[0]&lt;/code&gt; ) 또는 상호 작용하는 특성 쌍 (예 : &lt;code&gt;[(0, 1)]&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="e9c1f9439f6a49e182ea739a4c5151e4783dfb88" translate="yes" xml:space="preserve">
          <source>The feature importance scores of a fit gradient boosting model can be accessed via the &lt;code&gt;feature_importances_&lt;/code&gt; property:</source>
          <target state="translated">fit gradient boosting model의 기능 중요도 점수는 &lt;code&gt;feature_importances_&lt;/code&gt; 속성을 통해 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="29293679b6f2571a09f6b43f7ca55454fa94d5ff" translate="yes" xml:space="preserve">
          <source>The feature importances.</source>
          <target state="translated">기능의 중요성.</target>
        </trans-unit>
        <trans-unit id="596d7f9b612d1877ad65dac2d6189030efea06e6" translate="yes" xml:space="preserve">
          <source>The feature matrix &lt;code&gt;X&lt;/code&gt; should be standardized before fitting. This ensures that the penalty treats features equally.</source>
          <target state="translated">기능 행렬 &lt;code&gt;X&lt;/code&gt; 는 적합하기 전에 표준화되어야합니다. 이렇게하면 패널티가 기능을 동등하게 취급합니다.</target>
        </trans-unit>
        <trans-unit id="40d3ffb0824ad292679a9b4129b9e33a1cef7858" translate="yes" xml:space="preserve">
          <source>The feature matrix. Categorical features are encoded as ordinals.</source>
          <target state="translated">기능 매트릭스. 범주 기능은 서수로 인코딩됩니다.</target>
        </trans-unit>
        <trans-unit id="0a487efba080872bb151e33f60795da2b55ed658" translate="yes" xml:space="preserve">
          <source>The feature ranking, such that &lt;code&gt;ranking_[i]&lt;/code&gt; corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.</source>
          <target state="translated">&lt;code&gt;ranking_[i]&lt;/code&gt; 와 같은 특징 순위 는 i 번째 특징의 순위 위치에 대응한다. 선택된 (즉, 최상의 추정) 기능에는 순위 1이 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="c61b5bb2da747f9d4f147f5fd2e2f8308d20750d" translate="yes" xml:space="preserve">
          <source>The features and estimators that are experimental aren&amp;rsquo;t subject to deprecation cycles. Use them at your own risks!</source>
          <target state="translated">실험적인 기능 및 추정치는 지원 중단주기의 영향을받지 않습니다. 위험을 감수하고 사용하십시오!</target>
        </trans-unit>
        <trans-unit id="acbf27a6428b599cd8900c8ca4f4ecae3f0cc8b0" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and &lt;code&gt;max_features=n_features&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">각 분할에서 기능이 항상 임의로 변경됩니다. 따라서 최상의 스플릿을 검색하는 동안 열거 된 여러 스플릿에 대해 기준의 개선이 동일한 경우, 동일한 트레이닝 데이터 및 &lt;code&gt;max_features=n_features&lt;/code&gt; 로도 최적의 스플릿이 달라질 수 있습니다 . 피팅하는 동안 결정적인 동작을 얻으려면 &lt;code&gt;random_state&lt;/code&gt; 를 수정해야합니다.</target>
        </trans-unit>
        <trans-unit id="c25846da01d7420f77d01daefe6ea3229aead8d7" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, &lt;code&gt;max_features=n_features&lt;/code&gt; and &lt;code&gt;bootstrap=False&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">각 분할에서 기능이 항상 임의로 변경됩니다. 따라서 최상의 스플릿을 검색하는 동안 열거 된 여러 스플릿에 대해 기준의 개선이 동일 하면 동일한 학습 데이터 인 &lt;code&gt;max_features=n_features&lt;/code&gt; 및 &lt;code&gt;bootstrap=False&lt;/code&gt; 로도 가장 잘 찾은 스플릿이 달라질 수 있습니다 . 피팅하는 동안 결정적인 동작을 얻으려면 &lt;code&gt;random_state&lt;/code&gt; 를 수정해야합니다.</target>
        </trans-unit>
        <trans-unit id="79b5615baa935539329fdffd05466c4966130183" translate="yes" xml:space="preserve">
          <source>The features indices which will be returned when calling &lt;code&gt;transform&lt;/code&gt;. They are computed during &lt;code&gt;fit&lt;/code&gt;. For &lt;code&gt;features='all'&lt;/code&gt;, it is to &lt;code&gt;range(n_features)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; 을 호출 할 때 반환되는 기능 인덱스입니다 . 그들은 &lt;code&gt;fit&lt;/code&gt; 하는 동안 계산됩니다 . 들어 &lt;code&gt;features='all'&lt;/code&gt; , 그것은이다 &lt;code&gt;range(n_features)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7bb18d685bec7903d9b61cd6026a227300d22db3" translate="yes" xml:space="preserve">
          <source>The features of &lt;code&gt;X&lt;/code&gt; have been transformed from \([x_1, x_2]\) to \([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within any linear model.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; 의 기능이 \ ([x_1, x_2] \)에서 \ ([1, x_1, x_2, x_1 ^ 2, x_1 x_2, x_2 ^ 2] \)로 변환되었으며 이제 모든 선형 모델 내에서 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b791adba0e25e8ba284f5da492871659f500ccc6" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2)\) to \((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).</source>
          <target state="translated">X의 기능이 \ ((X_1, X_2) \)에서 \ ((1, X_1, X_2, X_1 ^ 2, X_1X_2, X_2 ^ 2) \)로 변환되었습니다.</target>
        </trans-unit>
        <trans-unit id="0ec9e3dee3b599b742aa9c67dee3f4cab82e4d00" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2, X_3)\) to \((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).</source>
          <target state="translated">X의 기능이 \ ((X_1, X_2, X_3) \)에서 \ ((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3) \)로 변환되었습니다.</target>
        </trans-unit>
        <trans-unit id="eaccd9379a9dbc6cce41fc318001c8bf42339abe" translate="yes" xml:space="preserve">
          <source>The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.</source>
          <target state="translated">아래 그림은 수축 및 서브 샘플링이 모델의 적합도에 미치는 영향을 보여줍니다. 수축이 비 수축보다 우수한 것을 알 수 있습니다. 수축이있는 서브 샘플링은 모델의 정확도를 더욱 높일 수 있습니다. 반면에 수축이없는 서브 샘플링은 제대로 수행되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="409a640cc1c72ef47cfd3f6ea68f986d77585eba" translate="yes" xml:space="preserve">
          <source>The figure below shows four one-way and one two-way partial dependence plots for the California housing dataset, with a &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">아래 그림은 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 를 사용하여 캘리포니아 주택 데이터 세트에 대한 4 개의 단방향 및 1 개의 양방향 부분 종속성 플롯을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="50a711fcdd2b061e3d348535a1d7a4d9bddb7ed5" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">아래 그림은 최소 제곱 손실과 500 명의 기본 학습자가있는 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 를 보스턴 주택 가격 데이터 세트 ( &lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt; ) 에 적용한 결과를 보여줍니다 . 왼쪽의 그림은 각 반복에서 열차 및 테스트 오류를 ​​보여줍니다. 각 반복에서 열차 오차 는 그라디언트 부스팅 모델 의 &lt;code&gt;train_score_&lt;/code&gt; 속성에 저장됩니다 . 각 반복에서 테스트 오류는 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt; 메소드를 통해 얻을 수 있으며 각 단계에서 예측을 생성하는 생성기를 반환합니다. 이와 같은 플롯을 사용하여 최적의 트리 수 (예 : &lt;code&gt;n_estimators&lt;/code&gt; 를 결정할 수 있습니다 조기 중지로 ) . 오른쪽 그림은 기능의 중요성을 보여줍니다. &lt;code&gt;feature_importances_&lt;/code&gt; 속성</target>
        </trans-unit>
        <trans-unit id="76597dd5c3aa4f5be5177f9ebf1af69e02fc15a8" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the impurity-based feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">아래 그림 은 보스턴 주택 가격 데이터 세트 ( &lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt; )에 최소 제곱 손실과 500 개의 기본 학습자가있는 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 를 적용한 결과를 보여줍니다 . 왼쪽의 플롯은 각 반복에서 훈련 및 테스트 오류를 ​​보여줍니다. 각 반복의 기차 오류 는 경사 부스팅 모델 의 &lt;code&gt;train_score_&lt;/code&gt; 속성에 저장됩니다 . 각 반복의 테스트 오류 는 각 단계에서 예측을 산출하는 생성기를 반환하는 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt; 메서드를 통해 얻을 수 있습니다 . 이와 같은 플롯 은 조기 중지를 통해 최적의 트리 수 (예 : &lt;code&gt;n_estimators&lt;/code&gt; ) 를 결정하는 데 사용할 수 있습니다 . 오른쪽의 플롯은 다음을 통해 얻을 수있는 불순물 기반 기능 중요도를 보여줍니다. &lt;code&gt;feature_importances_&lt;/code&gt; 속성.</target>
        </trans-unit>
        <trans-unit id="cc8de6d2f1f6034186ab0af10d91754eadd34954" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">그림은 두 가지 방법이 목표 함수의 합리적인 모델을 배우는 것을 보여줍니다. GPR은 함수의 주기성을 대략 2 * pi (6.28)로 정확하게 식별하는 반면 KRR은 두 배 주기성 4 * pi를 선택합니다. 또한, GPR은 KRR에 사용할 수없는 예측에 대한 합리적인 신뢰 한계를 제공합니다. 두 방법의 주요 차이점은 피팅과 ​​예측에 필요한 시간입니다. KRR을 피팅하는 것은 원칙적으로 빠르지 만 하이퍼 파라미터 최적화에 대한 그리드 검색은 하이퍼 파라미터의 수에 따라 지수 적으로 확장됩니다 ( &quot;차원의 저주&quot;). GPR에서 파라미터의 기울기 기반 최적화는 이러한 지수 스케일링을 겪지 않으므로 3 차원 하이퍼 파라미터 공간이있는이 예에서 훨씬 빠릅니다. 예측 시간은 비슷합니다. 하나,GPR의 예측 분포의 분산을 생성하는 것은 단지 평균을 예측하는 것보다 훨씬 오래 걸립니다.</target>
        </trans-unit>
        <trans-unit id="7a70cd6dc569a4ad4c8d12310e45e3bf56f13e7d" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly \(2*\pi\) (6.28), while KRR chooses the doubled periodicity \(4*\pi\) . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">그림은 두 가지 방법 모두 목표 함수의 합리적인 모델을 학습 함을 보여줍니다. GPR은 함수의 주기성을 대략 \ (2 * \ pi \) (6.28)로 올바르게 식별하는 반면 KRR은 두 배 주기성 \ (4 * \ pi \)를 선택합니다. 또한, GPR은 KRR에 사용할 수없는 예측에 대한 합리적인 신뢰 한계를 제공합니다. 두 방법의 주요 차이점은 피팅과 ​​예측에 필요한 시간입니다. KRR을 피팅하는 것은 원칙적으로 빠르지 만 하이퍼 파라미터 최적화에 대한 그리드 검색은 하이퍼 파라미터의 수에 따라 지수 적으로 확장됩니다 ( &quot;차원의 저주&quot;). GPR에서 파라미터의 기울기 기반 최적화는 이러한 지수 스케일링을 겪지 않으므로 3 차원 하이퍼 파라미터 공간이있는이 예에서 훨씬 빠릅니다. 예측 시간은 비슷합니다. 하나,GPR의 예측 분포의 분산을 생성하는 것은 단지 평균을 예측하는 것보다 훨씬 오래 걸립니다.</target>
        </trans-unit>
        <trans-unit id="4c819c571e9cf27ccc7a880b7ae493959bac8667" translate="yes" xml:space="preserve">
          <source>The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding.</source>
          <target state="translated">그림은 부스팅 반복의 함수로서 음의 OOB 개선의 누적 합계를 보여줍니다. 보시다시피, 처음 100 회 반복에 대한 테스트 손실을 추적 한 다음 비관적 인 방식으로 분기합니다. 이 그림은 또한 일반적으로 테스트 손실을 더 잘 추정하지만 계산 상 더 까다로운 3 배 교차 검증의 성능을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="37bfa29a2e04a43879779df5389399bec3580356" translate="yes" xml:space="preserve">
          <source>The figure shows the trade-off between cross-validated score and the number of PCA components. The balanced case is when n_components=10 and accuracy=0.88, which falls into the range within 1 standard deviation of the best accuracy score.</source>
          <target state="translated">그림은 교차 검증 된 점수와 PCA 구성 요소 수 간의 균형을 보여줍니다. 균형 잡힌 경우는 n_components = 10이고 정확도 = 0.88이며, 이는 최고 정확도 점수의 1 표준 편차 범위에 속합니다.</target>
        </trans-unit>
        <trans-unit id="fc5bec6cdf25a030ca7f5736bc605af5657e6e59" translate="yes" xml:space="preserve">
          <source>The figures below are used to illustrate the effect of scaling our &lt;code&gt;C&lt;/code&gt; to compensate for the change in the number of samples, in the case of using an &lt;code&gt;l1&lt;/code&gt; penalty, as well as the &lt;code&gt;l2&lt;/code&gt; penalty.</source>
          <target state="translated">아래 그림은 우리의 스케일링 효과를 설명하기 위해 사용되는 &lt;code&gt;C&lt;/code&gt; 를 사용하는 경우, 샘플의 수의 변화를 보상하기 위해 &lt;code&gt;l1&lt;/code&gt; 패널티뿐만 아니라 &lt;code&gt;l2&lt;/code&gt; 페널티.</target>
        </trans-unit>
        <trans-unit id="0b0d592cc5daeb578d9b82714e3800c3092f375f" translate="yes" xml:space="preserve">
          <source>The figures illustrate the interpolating property of the Gaussian Process model as well as its probabilistic nature in the form of a pointwise 95% confidence interval.</source>
          <target state="translated">그림은 가우시안 프로세스 모델의 보간 속성과 점별 95 % 신뢰 구간 형식의 확률 적 속성을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="e96dfdb18e509ef4ea2f5731757d5c9fff16f17b" translate="yes" xml:space="preserve">
          <source>The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.</source>
          <target state="translated">그림은 클래스 지원 크기 (각 클래스의 요소 수)에 따른 정규화 유무에 관계없이 혼동 행렬을 보여줍니다. 이러한 종류의 정규화는 클래스 불균형의 경우 어떤 클래스가 잘못 분류되고 있는지에 대한 시각적으로 해석하기에 흥미로울 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e0f5d06a7d2ed2f3c6fb0ae21cf62a8ad73fbd7f" translate="yes" xml:space="preserve">
          <source>The filenames for the images.</source>
          <target state="translated">이미지의 파일 이름입니다.</target>
        </trans-unit>
        <trans-unit id="4c01e855d5c880b7b53c9ce8ac56ca5941d4bf79" translate="yes" xml:space="preserve">
          <source>The filenames holding the dataset.</source>
          <target state="translated">데이터 세트를 보유하는 파일 이름입니다.</target>
        </trans-unit>
        <trans-unit id="83d2d2e5c2f316a3531e949e8bac6210f7e21821" translate="yes" xml:space="preserve">
          <source>The files themselves are loaded in memory in the &lt;code&gt;data&lt;/code&gt; attribute. For reference the filenames are also available:</source>
          <target state="translated">파일 자체는 &lt;code&gt;data&lt;/code&gt; 속성의 메모리에로드됩니다 . 참고로 파일 이름도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4834eeb362cb8f80edd0fa52b738fe7db255a20e" translate="yes" xml:space="preserve">
          <source>The filesystem path to the root folder where MLComp datasets are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead.</source>
          <target state="translated">MLComp 데이터 세트가 저장된 루트 폴더의 파일 시스템 경로입니다. mlcomp_root가 None 인 경우 MLCOMP_DATASETS_HOME 환경 변수가 대신 검색됩니다.</target>
        </trans-unit>
        <trans-unit id="5e83705f6f5cf8453734b26cfa75b02cefbf9a06" translate="yes" xml:space="preserve">
          <source>The final sum of similarities is divided by the size of the larger set.</source>
          <target state="translated">유사성의 최종 합은 더 큰 세트의 크기로 나뉩니다.</target>
        </trans-unit>
        <trans-unit id="6023cd4bbdadd0261fe4fb93bbac4f79dd623983" translate="yes" xml:space="preserve">
          <source>The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set).</source>
          <target state="translated">관성 기준의 최종 값 (훈련 세트의 모든 관측치에 대해 가장 가까운 중심까지의 제곱 거리의 합).</target>
        </trans-unit>
        <trans-unit id="65db46a7a2384cdd4de97d3c70536cd9da4c8616" translate="yes" xml:space="preserve">
          <source>The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points).</source>
          <target state="translated">응력의 최종 값 (변위의 제곱 거리와 모든 구속 점의 거리의 합).</target>
        </trans-unit>
        <trans-unit id="379910ba877ee4fe0038b0877574596b2d4e7156" translate="yes" xml:space="preserve">
          <source>The first 4 plots use the &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; with different numbers of informative features, clusters per class and classes. The final 2 plots use &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">처음 4 개의 플롯 은 서로 다른 수의 정보 기능, 클래스 및 클래스 별 클러스터와 함께 &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt; &lt;code&gt;make_classification&lt;/code&gt; &lt;/a&gt; 을 사용합니다 . 마지막 2 개의 플롯은 &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt; &lt;code&gt;make_blobs&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt; &lt;code&gt;make_gaussian_quantiles&lt;/code&gt; 를 사용&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="7e7a148ceab046969e5524f650bd9948ad6ae0ac" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;[.9, .1]&lt;/code&gt; in &lt;code&gt;y_pred&lt;/code&gt; denotes 90% probability that the first sample has label 0. The log loss is non-negative.</source>
          <target state="translated">제 &lt;code&gt;[.9, .1]&lt;/code&gt; 에 &lt;code&gt;y_pred&lt;/code&gt; 첫번째 샘플 라벨 0 로그 손실을 갖는 것을 나타내고, 90 %의 확률은 음수이다.</target>
        </trans-unit>
        <trans-unit id="c41f4a219241dbe01041bd4816c28f057f1f8fc9" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;n_samples % n_splits&lt;/code&gt; folds have size &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt;, other folds have size &lt;code&gt;n_samples // n_splits&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">첫 번째 &lt;code&gt;n_samples % n_splits&lt;/code&gt; 폴드의 크기는 &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt; 이고 다른 폴드의 크기는 &lt;code&gt;n_samples // n_splits&lt;/code&gt; . 여기서 &lt;code&gt;n_samples&lt;/code&gt; 는 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="318062ceb48883ba19b024b3ac85479b5d766c8b" translate="yes" xml:space="preserve">
          <source>The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices. In general, multiple points can be queried at the same time.</source>
          <target state="translated">반환 된 첫 번째 배열에는 1.6보다 가까운 모든 점까지의 거리가 포함되고 반환 된 두 번째 배열에는 인덱스가 포함됩니다. 일반적으로 여러 지점을 동시에 쿼리 할 수 ​​있습니다.</target>
        </trans-unit>
        <trans-unit id="b072f11f22e8fd40f8f011ced740f859610c5839" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the product-kernel</source>
          <target state="translated">제품 커널의 첫 번째 기본 커널</target>
        </trans-unit>
        <trans-unit id="2843deb348cef2ebb64b5cb6c478da3efa931340" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the sum-kernel</source>
          <target state="translated">합 커널의 첫 번째 기본 커널</target>
        </trans-unit>
        <trans-unit id="23baf5dbd5fca89380aba285e5a6dca77d9ff4b8" translate="yes" xml:space="preserve">
          <source>The first column of images shows true faces. The next columns illustrate how extremely randomized trees, k nearest neighbors, linear regression and ridge regression complete the lower half of those faces.</source>
          <target state="translated">이미지의 첫 번째 열은 실제 얼굴을 보여줍니다. 다음 열은 극도로 무작위 화 된 나무, k 개의 가장 가까운 이웃, 선형 회귀 및 능선 회귀가 해당면의 아래쪽 절반을 어떻게 완성하는지 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="758127cc4d0b762bc4a77876dd4551f141ccd7cd" translate="yes" xml:space="preserve">
          <source>The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.</source>
          <target state="translated">첫 번째는 노이즈 수준이 높고 길이가 큰 모델에 해당하며 노이즈별로 데이터의 모든 변형을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="2d71e99749591e16661c08d2c19738355ed2fad8" translate="yes" xml:space="preserve">
          <source>The first element of each line can be used to store a target variable to predict.</source>
          <target state="translated">각 라인의 첫 번째 요소는 예측할 대상 변수를 저장하는 데 사용될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d151d2a5867c5ba125b375d964535691cae0b0d6" translate="yes" xml:space="preserve">
          <source>The first example illustrates how robust covariance estimation can help concentrating on a relevant cluster when another one exists. Here, many observations are confounded into one and break down the empirical covariance estimation. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">첫 번째 예는 강력한 공분산 추정이 다른 클러스터가 존재할 때 관련 클러스터에 집중하는 데 어떻게 도움이 될 수 있는지 보여줍니다. 여기에서 많은 관측치가 하나에 혼동되어 경험적 공분산 추정치를 세분화합니다. 물론 일부 선별 도구는 두 개의 군집 (지원 벡터 머신, 가우스 혼합 모델, 일 변량 이상치 탐지 등)의 존재를 지적했을 것입니다. 그러나 그것이 고차원적인 예라면 이것들 중 어느 것도 쉽게 적용 할 수 없었습니다.</target>
        </trans-unit>
        <trans-unit id="3ed755a713ce6230a16c443825e8cd0dee1503e8" translate="yes" xml:space="preserve">
          <source>The first example illustrates how the Minimum Covariance Determinant robust estimator can help concentrate on a relevant cluster when outlying points exist. Here the empirical covariance estimation is skewed by points outside of the main cluster. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">첫 번째 예는 최소 공분산 결정 로버 스트 추정기가 이상 점이 존재할 때 관련 클러스터에 집중하는 데 어떻게 도움이 될 수 있는지 보여줍니다. 여기서 경험적 공분산 추정은 주 군집 외부의 점에 의해 왜곡됩니다. 물론 일부 스크리닝 도구는 두 개의 클러스터 (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection,&amp;hellip;)의 존재를 지적했을 것입니다. 그러나 그것이 고차원 적 예 였다면 이들 중 어느 것도 쉽게 적용 할 수 없었습니다.</target>
        </trans-unit>
        <trans-unit id="ecfbbb8300d863a918a602e1c4f90841d4b71a3f" translate="yes" xml:space="preserve">
          <source>The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):</source>
          <target state="translated">첫 번째 로더는 얼굴 식별 작업에 사용됩니다 : 멀티 클래스 분류 작업 (따라서지도 학습) :</target>
        </trans-unit>
        <trans-unit id="1272790baab931c65e23f13d4b17128820578899" translate="yes" xml:space="preserve">
          <source>The first model is a classical Gaussian Mixture Model with 10 components fit with the Expectation-Maximization algorithm.</source>
          <target state="translated">첫 번째 모델은 Expectation-Maximization 알고리즘에 맞는 10 개의 구성 요소가 포함 된 클래식 가우스 혼합 모델입니다.</target>
        </trans-unit>
        <trans-unit id="a28da4c4f339f2d2823befd3907c74a3bd7b1459" translate="yes" xml:space="preserve">
          <source>The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.</source>
          <target state="translated">첫 번째 플롯은 2 개의 입력 기능과 2 개의 가능한 대상 클래스 (2 진 분류) 만 포함 된 단순화 된 분류 문제에 대한 다양한 매개 변수 값에 대한 결정 함수의 시각화입니다. 이러한 종류의 플롯은 더 많은 기능이나 대상 클래스의 문제에 대해서는 수행 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="fdb1a20b80ba5f009466ef7acd3785afda979734" translate="yes" xml:space="preserve">
          <source>The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit &amp;ldquo;block&amp;rdquo; is stacked above each point on a regular grid. As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution. If we instead center each block on the point it represents, we get the estimate shown in the bottom left panel. This is a kernel density estimation with a &amp;ldquo;top hat&amp;rdquo; kernel. This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution.</source>
          <target state="translated">첫 번째 플롯은 히스토그램을 사용하여 1D의 포인트 밀도를 시각화하는 데 따른 문제 중 하나를 보여줍니다. 직관적으로, 히스토그램은 단위 &quot;블록&quot;이 규칙적인 그리드에서 각 점 위에 쌓이는 방식으로 생각할 수 있습니다. 그러나 상위 2 개의 패널에서 볼 수 있듯이 이러한 블록에 대해 그리드를 선택하면 밀도 분포의 기본 모양에 대한 격렬한 아이디어가 생길 수 있습니다. 대신 각 블록을 나타내는 점의 중심에 놓으면 왼쪽 하단 패널에 추정치가 표시됩니다. 이것은 &quot;top hat&quot;커널을 사용한 커널 밀도 추정입니다. 이 아이디어는 다른 커널 형태로 일반화 될 수 있습니다. 첫 번째 그림의 오른쪽 아래 패널은 동일한 분포에 대한 가우스 커널 밀도 추정치를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="80a6f35d6de94c1655a0ab570a2d81b8e9f0c281" translate="yes" xml:space="preserve">
          <source>The first plot shows that with an increasing number of samples &lt;code&gt;n_samples&lt;/code&gt;, the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; increased logarithmically in order to guarantee an &lt;code&gt;eps&lt;/code&gt;-embedding.</source>
          <target state="translated">샘플의 증가와 함께 제 플롯 도시 &lt;code&gt;n_samples&lt;/code&gt; , 치수의 최소 수 &lt;code&gt;n_components&lt;/code&gt; 을 위해 대수적으로 증가는 보장 &lt;code&gt;eps&lt;/code&gt; -embedding한다.</target>
        </trans-unit>
        <trans-unit id="0fa5069414b33f645c4a3a3261f97c50a380bf97" translate="yes" xml:space="preserve">
          <source>The first plot shows the best inertia reached for each combination of the model (&lt;code&gt;KMeans&lt;/code&gt; or &lt;code&gt;MiniBatchKMeans&lt;/code&gt;) and the init method (&lt;code&gt;init=&quot;random&quot;&lt;/code&gt; or &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt;) for increasing values of the &lt;code&gt;n_init&lt;/code&gt; parameter that controls the number of initializations.</source>
          <target state="translated">첫 번째 플롯은 초기화 횟수를 제어하는 &lt;code&gt;n_init&lt;/code&gt; 매개 변수의 값을 늘리기 위해 모델 ( &lt;code&gt;KMeans&lt;/code&gt; 또는 &lt;code&gt;MiniBatchKMeans&lt;/code&gt; )과 init 메소드 ( &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; 또는 &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt; ) 의 각 조합에 대해 도달 한 최상의 관성을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="35a5988878bfd98d1cc6f738d4af80878102b501" translate="yes" xml:space="preserve">
          <source>The first row of output array indicates that there are three samples whose true cluster is &amp;ldquo;a&amp;rdquo;. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is &amp;ldquo;b&amp;rdquo;. Of them, none is in predicted cluster 0, one is in 1 and two are in 2.</source>
          <target state="translated">출력 배열의 첫 번째 행은 실제 클러스터가 &quot;a&quot;인 3 개의 샘플이 있음을 나타냅니다. 그중 두 개는 예측 된 클러스터 0에 있고, 하나는 1에 있고, 아무도는 2에 없습니다. 그리고 두 번째 행은 실제 클러스터가 &quot;b&quot;인 세 개의 샘플이 있음을 나타냅니다. 이 중 예측 된 클러스터 0에는 아무것도없고 1은 1에, 2는 2에 있습니다.</target>
        </trans-unit>
        <trans-unit id="94d22c244cece5d36684b54eb3a3c36ef460564d" translate="yes" xml:space="preserve">
          <source>The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.</source>
          <target state="translated">처음 두 손실 함수는 게으 르며, 예제가 마진 제약 조건을 위반하는 경우에만 모델 매개 변수를 업데이트하므로 L2 페널티가 사용되는 경우에도 훈련이 매우 효율적으로 이루어지고 스파 스 모델이 생성 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="90159f341e51eef650aa99c9ddd91af9915c59a3" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the &lt;code&gt;transform&lt;/code&gt; method.</source>
          <target state="translated">적합 모델은 또한 &lt;code&gt;transform&lt;/code&gt; 방법을 사용하여 입력을 가장 차별적 인 방향으로 투영함으로써 입력의 차원을 줄이는 데 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="865ac3c349894331005e5c9a9918fbe1c4576705" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.</source>
          <target state="translated">또한 장착 된 모델을 사용하여 입력을 가장 차별적 인 방향으로 투사하여 입력의 치수를 줄일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="47ade7eb6fd49b2a025d9a87896bfcf3ba24402d" translate="yes" xml:space="preserve">
          <source>The fitted model.</source>
          <target state="translated">적합 모델.</target>
        </trans-unit>
        <trans-unit id="474bb96d2aba10d56f1c494aef7da54c3ce49fa5" translate="yes" xml:space="preserve">
          <source>The flattened data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="translated">평면화 된 데이터 행렬입니다. 경우 &lt;code&gt;as_frame=True&lt;/code&gt; , &lt;code&gt;data&lt;/code&gt; 팬더 DataFrame 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="14120dbe50cfe7a93d61eb3782205eb1e9322e9d" translate="yes" xml:space="preserve">
          <source>The flexibility of controlling the smoothness of the learned function via \(\nu\) allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Mat&amp;eacute;rn kernel are shown in the following figure:</source>
          <target state="translated">\ (\ nu \)를 통해 학습 된 기능의 부드러움을 제어 할 수있는 유연성 덕분에 진정한 기본 기능 관계의 속성에 적응할 수 있습니다. Mat&amp;eacute;rn 커널로 인한 GP의 앞뒤는 다음 그림과 같습니다.</target>
        </trans-unit>
        <trans-unit id="130cb7c407aba5d31bddc566759f6c2692fa934a" translate="yes" xml:space="preserve">
          <source>The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.</source>
          <target state="translated">아래의 순서도는 사용자가 데이터를 평가할 추정 방법과 관련하여 문제에 접근하는 방법에 대한 대략적인 가이드를 제공하기 위해 마련된 것입니다.</target>
        </trans-unit>
        <trans-unit id="f162534c96a36fe6b7c86b6775d49d882723bf35" translate="yes" xml:space="preserve">
          <source>The folder names are used as supervised signal label names. The individual file names are not important.</source>
          <target state="translated">폴더 이름은 감독 된 신호 레이블 이름으로 사용됩니다. 개별 파일 이름은 중요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="f2db8b6a5929e1458b6943c5d307e1ce8a02db89" translate="yes" xml:space="preserve">
          <source>The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.</source>
          <target state="translated">폴드는 별개의 그룹의 수가 각각의 폴드에서 대략 동일하다는 의미에서 대략 균형이 잡힌다.</target>
        </trans-unit>
        <trans-unit id="7910d480ac1425d50ff9fab631fdf912810ec26e" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">다음은 목표 값이 특성의 선형 조합이 될 것으로 예상되는 회귀 분석을위한 일련의 방법입니다. 수학적 표기법에서 \ (\ hat {y} \)가 예측 값인 경우.</target>
        </trans-unit>
        <trans-unit id="b43c7b928f1a786f0281c067a04292ccfddc7963" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">다음은 목표 값이 입력 변수의 선형 조합 일 것으로 예상되는 회귀를위한 일련의 방법입니다. 수학 개념에서 \ (\ hat {y} \)가 예측 된 값인 경우.</target>
        </trans-unit>
        <trans-unit id="81301a81f070c1a1a05c02b69a367906c9b88d21" translate="yes" xml:space="preserve">
          <source>The following clustering assignment is slightly better, since it is homogeneous but not complete:</source>
          <target state="translated">다음 클러스터링 할당은 균질하지만 완전하지 않기 때문에 약간 더 좋습니다.</target>
        </trans-unit>
        <trans-unit id="563a9a6f3a15e2fd51ba9e9b647870430bf12e58" translate="yes" xml:space="preserve">
          <source>The following code defines a linear kernel and creates a classifier instance that will use that kernel:</source>
          <target state="translated">다음 코드는 선형 커널을 정의하고 해당 커널을 사용할 분류 자 ​​인스턴스를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="b8e362395586349d1fcb8b526001a536e06f5d5d" translate="yes" xml:space="preserve">
          <source>The following code is a bit verbose, feel free to jump directly to the analysis of the &lt;a href=&quot;#results&quot;&gt;results&lt;/a&gt;.</source>
          <target state="translated">다음 코드는 약간 장황하며 &lt;a href=&quot;#results&quot;&gt;결과&lt;/a&gt; 분석으로 바로 이동할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b414b22c1d4fa210fe910463f00d0b843e42262f" translate="yes" xml:space="preserve">
          <source>The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the &lt;code&gt;groups&lt;/code&gt; parameter.</source>
          <target state="translated">이를 위해 다음과 같은 교차 유효성 검사 스플리터를 사용할 수 있습니다. 샘플의 그룹화 식별자는 &lt;code&gt;groups&lt;/code&gt; 매개 변수 를 통해 지정됩니다 .</target>
        </trans-unit>
        <trans-unit id="6757b5cfa24458748d25f81af60d324180a74dc4" translate="yes" xml:space="preserve">
          <source>The following cross-validators can be used in such cases.</source>
          <target state="translated">이러한 경우 다음 교차 유효성 검사기를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6da3fe3659ba9973062031cbded9ac1ee9e80559" translate="yes" xml:space="preserve">
          <source>The following dataset has integer features, two of which are the same in every sample. These are removed with the default setting for threshold:</source>
          <target state="translated">다음 데이터 세트에는 정수 피처가 있으며이 중 두 개는 모든 샘플에서 동일합니다. 이들은 임계 값에 대한 기본 설정으로 제거됩니다.</target>
        </trans-unit>
        <trans-unit id="54024b35aa398c3f2441f7e2cd8d0e6044cd9d46" translate="yes" xml:space="preserve">
          <source>The following estimators have built-in variable selection fitting procedures, but any estimator using a L1 or elastic-net penalty also performs variable selection: typically &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with an appropriate penalty.</source>
          <target state="translated">다음 추정기에는 기본 제공 변수 선택 피팅 절차가 있지만 L1 또는 Elastic-net 패널티를 사용하는 모든 추정기는 변수 선택도 수행합니다. 일반적으로 적절한 패널티가있는 &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="f50629b49593a5773731e8a4d6d7fbab3d06afa6" translate="yes" xml:space="preserve">
          <source>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</source>
          <target state="translated">다음 예제는 데이터를 분할하고 모델을 피팅하며 5 회 연속 점수를 매번 (각각 다른 분할로) 계산하여 홍채 데이터 세트에서 선형 커널 지원 벡터 시스템의 정확도를 추정하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="cd40f6b0be33059696fbdf36c2c2af62f27e4578" translate="yes" xml:space="preserve">
          <source>The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="translated">다음 예제는 순열 기반 기능 중요도와 대조적으로 불순물 기반 기능 중요도의 한계를 강조합니다. &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;순열 중요도 vs 랜덤 포레스트 기능 중요도 (MDI)&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="50761302c7bb4483750fc53752f14a1bf26cb023" translate="yes" xml:space="preserve">
          <source>The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.</source>
          <target state="translated">다음 예는 Olivetti면 데이터 세트에서 스파 스 PCA를 사용하여 추출한 16 개의 구성 요소를 보여줍니다. 정규화 용어가 많은 영 (0)을 유도하는 방법을 볼 수 있습니다. 또한, 데이터의 자연적 구조는 0이 아닌 계수가 수직으로 인접하게한다. 이 모델은 수학적으로이를 강제하지 않습니다. 각 구성 요소는 벡터 \ (h \ in \ mathbf {R} ^ {4096} \)이며 64x64 픽셀 이미지로 인간 친화적 인 시각화를 제외하고는 수직 인접성 개념이 없습니다. 아래에 표시된 구성 요소가 로컬로 표시된다는 사실은 데이터의 고유 한 구조의 영향으로 이러한 로컬 패턴으로 인해 재구성 오류가 최소화됩니다. 인접성과 다른 종류의 구조를 고려한 희소성을 유발하는 규범이 있습니다. &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; 참조그러한 방법의 검토를 위해. Sparse PCA 사용 방법에 대한 자세한 내용은 아래 예 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="218baecbc1e50d08c530ba0bc6ca1335ad228789" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">다음 예제 는 선형 지원 벡터 머신, 의사 결정 트리 및 K- 최근 접 이웃 분류기를 기반으로 소프트 &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; &lt;/a&gt; 를 사용할 때 결정 영역이 어떻게 변경 될 수 있는지 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="0b4e5d16e2ae8075091f7ec9c797f8dd043b5cfe" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;code&gt;VotingClassifier&lt;/code&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">다음 예 는 선형 지원 벡터 머신, 의사 결정 트리 및 K- 최근 접 이웃 분류기를 기반으로 소프트 &lt;code&gt;VotingClassifier&lt;/code&gt; 를 사용할 때 결정 영역이 변경되는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="fcb0630f60eb0a1b1e5514ec57d9a7f6cbd21ce7" translate="yes" xml:space="preserve">
          <source>The following example illustrates the effect of scaling the regularization parameter when using &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; for &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;classification&lt;/a&gt;. For SVC classification, we are interested in a risk minimization for the equation:</source>
          <target state="translated">다음 예는 &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machine&lt;/a&gt; 을 사용하여 &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;분류&lt;/a&gt; 할 때 정규화 매개 변수를 조정하는 효과를 보여줍니다 . SVC 분류의 경우 방정식에 대한 위험 최소화에 관심이 있습니다.</target>
        </trans-unit>
        <trans-unit id="35227ebb51bdeaa9a401c5e44c0ca9ca35e1f0e6" translate="yes" xml:space="preserve">
          <source>The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">다음 예제는 &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt; 모델을 사용하여 얼굴 인식 작업에 대한 각 개별 픽셀의 상대적 중요도를 색상으로 표시 한 것입니다 .</target>
        </trans-unit>
        <trans-unit id="e4e3a609b2ed09432a121fc3917b934875c3544c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit an AdaBoost classifier with 100 weak learners:</source>
          <target state="translated">다음 예제는 약한 학습자 100 명과 AdaBoost 분류기를 맞추는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="08beea172467f4b9200a143a3d6a2c2ed164664c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the VotingRegressor:</source>
          <target state="translated">다음 예제는 VotingRegressor를 맞추는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="486be98f63ccda08601d245e4a85066d3abba297" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the majority rule classifier:</source>
          <target state="translated">다음 예는 다수 규칙 분류자를 맞추는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="6a236adbadcca90d4c1ec977d69fa3c364957c12" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 most informative features in the Friedman #1 dataset.</source>
          <target state="translated">다음 예는 Friedman # 1 데이터 세트에서 가장 유익한 5 가지 기능을 검색하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="376fe34989034d77dfd504e8201c3d4b2dfa1573" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.</source>
          <target state="translated">다음 예는 Friedman # 1 데이터 세트에서 5 가지 올바른 정보 기능을 검색하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="25b5efc453e06d92e8572618fefd1930c5c8aab3" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.</source>
          <target state="translated">다음 예는 Friedman # 1 데이터 세트에서 알려지지 않은 5 가지 유익한 기능을 검색하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="e99af0f029f79715308d4d1c9ecbe2acb8983e4d" translate="yes" xml:space="preserve">
          <source>The following example will, for instance, transform some British spelling to American spelling:</source>
          <target state="translated">예를 들어 다음 예제는 영국식 철자를 미국식 철자로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="ad9262c5496a82045b0e3b64071f384beed06659" translate="yes" xml:space="preserve">
          <source>The following experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The figure shows the estimated probabilities obtained with logistic regression, a linear support-vector classifier (SVC), and linear SVC with both isotonic calibration and sigmoid calibration. The Brier score is a metric which is a combination of calibration loss and refinement loss, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt;, reported in the legend (the smaller the better). Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve.</source>
          <target state="translated">다음 실험은 20 개의 특징을 가진 100,000 개의 샘플 (1,000 개가 모델 피팅에 사용됨)로 이진 분류를위한 인공 데이터 세트에서 수행됩니다. 20 개의 기능 중 2 개만 유익하고 10 개는 중복입니다. 그림은 로지스틱 회귀 분석, 선형 지원-벡터 분류기 (SVC) 및 등장 교정 및 시그 모이 드 교정이 모두 포함 된 선형 SVC를 통해 얻은 추정 확률을 보여줍니다. Brier 점수는 교정 손실과 정제 손실, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt; 의 조합 인 메트릭입니다., 범례에보고되었습니다 (더 작을수록 좋습니다). 교정 손실은 ROC 세그먼트의 기울기에서 파생 된 경험적 확률의 평균 제곱 편차로 정의됩니다. 정제 손실은 최적 비용 곡선 아래 면적에 의해 측정 된 예상 최적 손실로 정의 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7e7b7eaddffc735bc9fdc4de0de59e2218717940" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approximately seven times faster than fitting &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; since it has learned a sparse model using only approximately 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">다음 그림은 정현파 타겟 함수와 5 번째 데이터 포인트마다 추가되는 강력한 노이즈로 구성된 인공 데이터 세트의 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; 을 비교합니다 . &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; 의 학습 된 모델이 표시 되며 , 여기서 RBF 커널의 복잡성 / 정규화 및 대역폭이 그리드 검색을 사용하여 최적화되었습니다. 학습 된 기능은 매우 유사합니다. 그러나 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; 를 맞추는&lt;/a&gt; 것은 &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; 을&lt;/a&gt; 맞추는 것보다 약 7 배 더 빠릅니다 (둘 다 그리드 검색 사용). 그러나 &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; 을 사용하면&lt;/a&gt; 목표 값 100000 개에 대한 예측이 3 배 이상 빠릅니다. 100 개의 훈련 데이터 포인트 중 약 1/3만을 지원 벡터로 사용하여 희소 모델을 학습했기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="2b3bbe94b0947452c8fc360afc3d0e6f8832ae22" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approx. seven times faster than fitting &lt;code&gt;SVR&lt;/code&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">다음 그림은 사인파 목표 함수와 매 5 번째 데이터 포인트에 추가 된 강한 노이즈로 구성된 인공 데이터 세트에서 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;SVR&lt;/code&gt; 을 비교합니다 . 학습 된 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 및 &lt;code&gt;SVR&lt;/code&gt; 모델 이 그려져 있으며 RBF 커널의 복잡성 / 규정 화 및 대역폭이 그리드 검색을 사용하여 최적화되었습니다. 학습 된 기능은 매우 유사합니다. 그러나 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 피팅 은 약입니다. &lt;code&gt;SVR&lt;/code&gt; 피팅보다 7 배 더 빠릅니다 (둘 다 그리드 검색 사용). 그러나 SVR을 사용하면 100000 개의 목표 값을 예측하는 데 약 3 배만 빠릅니다. 100 개의 교육 데이터 포인트 중 1/3이 지원 벡터로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="7f576baedfe43e5a873ff30c0a921c0894cd8a99" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zero entries in the coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yield scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">다음 그림은 단순 Lasso 또는 MultiTaskLasso를 사용하여 얻은 계수 행렬 W에서 0이 아닌 항목의 위치를 ​​비교합니다. Lasso는 MultiTaskLasso의 0이 아닌 열이 전체 열인 동안 흩어져있는 0이 아닌 값을 산출합니다.</target>
        </trans-unit>
        <trans-unit id="b11b5ffd2f0d8dbd878722867b9b243b2e73a5ea" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zeros in W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yields scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">다음 그림은 W에서 0이 아닌 위치를 간단한 올가미 또는 MultiTaskLasso로 비교 한 것입니다. 올가미는 0이 아닌 산란을 산출하지만 0이 아닌 MultiTaskLasso는 전체 열입니다.</target>
        </trans-unit>
        <trans-unit id="c66df17d5ba5efe635efde7f93de1fc62e75c222" translate="yes" xml:space="preserve">
          <source>The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">다음 그림은 사인파 목표 함수와 강한 노이즈로 구성된 인공 데이터 세트의 두 가지 방법을 보여줍니다. 이 그림은 주기적 기능 학습에 적합한 ExpSineSquared 커널을 기반으로 KRR 및 GPR의 학습 모델을 비교합니다. 커널의 하이퍼 파라미터는 커널의 부드러움 (길이 _ 스케일) 및주기 (주기)를 제어합니다. 또한 데이터의 노이즈 레벨은 커널의 추가 WhiteKernel 구성 요소와 KRR의 정규화 매개 변수 alpha에 의해 GPR에 의해 명시 적으로 학습됩니다.</target>
        </trans-unit>
        <trans-unit id="7af807be2b2e68501b3d7cc153c6244aca835299" translate="yes" xml:space="preserve">
          <source>The following guide focuses on &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, which might be preferred for small sample sizes since binning may lead to split points that are too approximate in this setting.</source>
          <target state="translated">다음 가이드는 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 에 초점을 맞 춥니 다. 이는 비닝으로 인해이 설정에서 너무 근사한 분할 지점으로 이어질 수 있기 때문에 작은 샘플 크기에 선호 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c2a01fd3fe0b5b9cd28910783764aacd71509b19" translate="yes" xml:space="preserve">
          <source>The following illustrate the probability density functions of the target before and after applying the logarithmic functions.</source>
          <target state="translated">다음은 로그 함수를 적용하기 전후에 목표의 확률 밀도 함수를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="b0293fdb9cc21af9db21d1f9f61ccde2a4565147" translate="yes" xml:space="preserve">
          <source>The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.</source>
          <target state="translated">다음 이미지는 너구리 얼굴 이미지의 일부에서 추출 된 4x4 픽셀 이미지 패치에서 배운 사전이 어떻게 보이는지 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="a988a0a50289dff96522065b976c597b6e4e63e8" translate="yes" xml:space="preserve">
          <source>The following image shows on the data above the estimated probability using a Gaussian naive Bayes classifier without calibration, with a sigmoid calibration and with a non-parametric isotonic calibration. One can observe that the non-parametric model provides the most accurate probability estimates for samples in the middle, i.e., 0.5.</source>
          <target state="translated">다음 이미지는 교정없이, 시그 모이 드 교정 및 비모수 적 등장 교정으로 가우시안 순진 베이 즈 분류기를 사용하여 추정 된 확률 위의 데이터를 보여줍니다. 비모수 적 모델은 중간에있는 샘플, 즉 0.5에 대해 가장 정확한 확률 추정치를 제공한다는 것을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b45c55863d114ed9459a8475837831820ff65766" translate="yes" xml:space="preserve">
          <source>The following images demonstrate the benefit of probability calibration. The first image present a dataset with 2 classes and 3 blobs of data. The blob in the middle contains random samples of each class. The probability for the samples in this blob should be 0.5.</source>
          <target state="translated">다음 이미지는 확률 보정의 이점을 보여줍니다. 첫 번째 이미지는 2 개의 클래스와 3 개의 데이터 Blob이있는 데이터 집합을 나타냅니다. 가운데의 얼룩에는 각 클래스의 무작위 샘플이 포함되어 있습니다. 이 얼룩의 샘플 확률은 0.5 여야합니다.</target>
        </trans-unit>
        <trans-unit id="ba49479bf1cc39dce48fe5a925ee39da6796af0a" translate="yes" xml:space="preserve">
          <source>The following lists the string metric identifiers and the associated distance metric classes:</source>
          <target state="translated">다음은 문자열 메트릭 식별자 및 관련 거리 메트릭 클래스를 나열합니다.</target>
        </trans-unit>
        <trans-unit id="807c0e6975f025a025e2951d9c9164c1f3454c71" translate="yes" xml:space="preserve">
          <source>The following loss functions are supported and can be specified using the parameter &lt;code&gt;loss&lt;/code&gt;:</source>
          <target state="translated">다음 손실 기능이 지원되며 매개 변수 &lt;code&gt;loss&lt;/code&gt; 를 사용하여 지정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f56ebb3690526678e7e0211eddd43dd69c89de3e" translate="yes" xml:space="preserve">
          <source>The following parts are parallelized:</source>
          <target state="translated">다음 부분은 병렬화됩니다.</target>
        </trans-unit>
        <trans-unit id="69eaa3387fa8263cde077e4bb7511faccc7173e0" translate="yes" xml:space="preserve">
          <source>The following plot compares how well the probabilistic predictions of different classifiers are calibrated, using &lt;a href=&quot;generated/sklearn.calibration.calibration_curve#sklearn.calibration.calibration_curve&quot;&gt;&lt;code&gt;calibration_curve&lt;/code&gt;&lt;/a&gt;. The x axis represents the average predicted probability in each bin. The y axis is the &lt;em&gt;fraction of positives&lt;/em&gt;, i.e. the proportion of samples whose class is the positive class (in each bin).</source>
          <target state="translated">다음 플롯하여, 서로 다른 분류의 확률 예측은 교정 얼마나 잘 비교 &lt;a href=&quot;generated/sklearn.calibration.calibration_curve#sklearn.calibration.calibration_curve&quot;&gt; &lt;code&gt;calibration_curve&lt;/code&gt; 를&lt;/a&gt; . x 축은 각 빈의 평균 예측 확률을 나타냅니다. y 축은 &lt;em&gt;긍정&lt;/em&gt; 의 비율입니다. 즉, 클래스가 (각 빈에서) 긍정 클래스 인 샘플의 비율입니다.</target>
        </trans-unit>
        <trans-unit id="70624e811991b8fac875c2f0dd0db6a8fb2e1e2b" translate="yes" xml:space="preserve">
          <source>The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.</source>
          <target state="translated">다음 그림은 다양한 군집 성능 평가 지표에 대한 군집 수 및 샘플 수의 영향을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="661cd7d1cca4025c0fd4f4a1e938e7140f6ef24b" translate="yes" xml:space="preserve">
          <source>The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; differ slightly from the standard textbook notation that defines the idf as</source>
          <target state="translated">다음 섹션에는 tf-idfs가 정확하게 계산되는 방법과 scikit-learn의 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; &lt;/a&gt; 에서 계산 된 tf-idfs 가 idf를</target>
        </trans-unit>
        <trans-unit id="78ad5f101e233b65633dffc5c68bec29c1eae0b0" translate="yes" xml:space="preserve">
          <source>The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.</source>
          <target state="translated">다음 섹션에는 다양한 교차 검증 전략에 따라 데이터 세트 분할을 생성하는 데 사용할 수있는 인덱스를 생성하는 유틸리티가 나열되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="999c74375c41b701ce62b520b704cf4b739a66b1" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean feature value of the two nearest neighbors of samples with missing values:</source>
          <target state="translated">다음 스 니펫은 누락 된 값이있는 두 개의 가장 가까운 이웃 샘플의 평균 특성 값을 사용하여 &lt;code&gt;np.nan&lt;/code&gt; 로 인코딩 된 누락 된 값을 대체하는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="5c75d78bf5e5b93a438a72e22f9eee2db09a82ed" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean value of the columns (axis 0) that contain the missing values:</source>
          <target state="translated">다음 스 니펫은 &lt;code&gt;np.nan&lt;/code&gt; 이 포함 된 열 (축 0)의 평균값을 사용하여 결 측값을 np.nan으로 인코딩하여 바꾸는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="35e4b9d817818dd84b86f006c91682ff56db6021" translate="yes" xml:space="preserve">
          <source>The following subsections are only rough guidelines: the same estimator can fall into multiple categories, depending on its parameters.</source>
          <target state="translated">다음 하위 섹션은 대략적인 지침 일뿐입니다. 동일한 추정자가 매개 변수에 따라 여러 범주에 속할 수 있습니다. ㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ</target>
        </trans-unit>
        <trans-unit id="b6c9fc0377f0a1e8adb0ffabc9be6ea63e8b1dd6" translate="yes" xml:space="preserve">
          <source>The following table lists some specific EDMs and their unit deviance (all of these are instances of the Tweedie family):</source>
          <target state="translated">다음 표에는 특정 EDM과 해당 단위 편차가 나열되어 있습니다 (모두 Tweedie 제품군의 인스턴스 임).</target>
        </trans-unit>
        <trans-unit id="8aa5b699e7b8122b521ff7ffa5779f4500a14f28" translate="yes" xml:space="preserve">
          <source>The following table summarizes the penalties supported by each solver:</source>
          <target state="translated">다음 표는 각 솔버가 지원하는 패널티를 요약 한 것입니다.</target>
        </trans-unit>
        <trans-unit id="9d55ab5466e0380dae4e49e3b2bfc5d99624d130" translate="yes" xml:space="preserve">
          <source>The following toy example demonstrates how the model ignores the samples with zero sample weights:</source>
          <target state="translated">다음 장난감 예제는 모델이 샘플 가중치가 0 인 샘플을 무시하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="46781e9d9b616f25e94b9f1718ce2c06cffa693e" translate="yes" xml:space="preserve">
          <source>The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.</source>
          <target state="translated">다음의 두 참조는 컨시어지 제어에 사용되는 이중성 갭 계산뿐만 아니라 scikit-learn의 좌표 하강 솔버에 사용 된 반복을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="5c7ef4cd14485e27e01a8eae72d8f974a84e41ab" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; 에서&lt;/a&gt; 학습 한 모델의 형태는 &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; (Support Vector Regression)과 동일합니다 . 그러나 다른 손실 함수가 사용됩니다. KRR은 제곱 오류 손실을 사용하는 반면 지원 벡터 회귀는 l2 정규화와 결합 된 \ (\ epsilon \)-insensitive loss를 사용합니다. &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; 과 달리 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 피팅 은 닫힌 형식으로 수행 할 수 있으며 일반적으로 중간 크기의 데이터 세트에 대해 더 빠릅니다. 반면에 학습 된 모델은 희소가 아니므로 예측 시간에 \ (\ epsilon&amp;gt; 0 \)에 대한 희소 모델을 학습하는 &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; 보다 느립니다 .</target>
        </trans-unit>
        <trans-unit id="d492404a63c6d65da32b70e6dcf87e75a94b6a4d" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;code&gt;SVR&lt;/code&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;code&gt;SVR&lt;/code&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 가 학습 한 모델의 형식은 &lt;code&gt;SVR&lt;/code&gt; (벡터 회귀 ) 지원과 동일합니다 . 그러나 서로 다른 손실 함수가 사용됩니다. KRR은 제곱 오차 손실을 사용하지만 지원 벡터 회귀는 \ (\ epsilon \)에 둔감 한 손실을 l2 정규화와 함께 사용합니다. &lt;code&gt;SVR&lt;/code&gt; 과 달리 피팅 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 는 닫힌 형태로 수행 할 수 있으며 일반적으로 중간 규모의 데이터 세트에 더 빠릅니다. 반면에 학습 된 모델은 희소성이 아니므로 SVR보다 느리므로 예측 시간에 \ (\ epsilon&amp;gt; 0 \)에 대한 희소 모델을 학습합니다.</target>
        </trans-unit>
        <trans-unit id="cb6eb14fd1b6ffac38e7d6c15d36b7076beafa85" translate="yes" xml:space="preserve">
          <source>The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon &amp;gt; 0, at prediction-time.</source>
          <target state="translated">KRR이 학습 한 모델의 형식은 SVR (벡터 회귀) 지원과 동일합니다. 그러나 서로 다른 손실 함수가 사용됩니다. KRR은 제곱 오차 손실을 사용하고 지원 벡터 회귀는 엡실론에 둔감 한 손실을 l2 정규화와 함께 사용합니다. SVR과 달리 KRR 모델의 맞춤은 닫힌 형식으로 수행 할 수 있으며 일반적으로 중간 규모의 데이터 집합에 더 빠릅니다. 반면에 학습 된 모델은 희소성이 아니므로 SVR보다 느리므로 예측 시간에 엡실론&amp;gt; 0에 대한 희소 모델을 학습합니다.</target>
        </trans-unit>
        <trans-unit id="785bfd19f7d4d298d07ec2673131bad97bee7c7c" translate="yes" xml:space="preserve">
          <source>The form of these kernels is as follows:</source>
          <target state="translated">이 커널의 형태는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cecc632cfa798838e6560a2f37e713a3ae87c1a6" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as idf(t) = log [ n / df(t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]).</source>
          <target state="translated">문서 세트에서 문서 d의 용어 t에 대한 tf-idf를 계산하는 데 사용되는 공식은 tf-idf (t, d) = tf (t, d) * idf (t)이고 idf가 계산됩니다. as idf (t) = log [n / df (t)] + 1 ( &lt;code&gt;smooth_idf=False&lt;/code&gt; 인 경우 ), 여기서 n은 문서 세트의 총 문서 수이고 df (t)는 t의 문서 빈도입니다. 문서 빈도는 용어 t를 포함하는 문서 세트의 문서 수입니다. 위 방정식에서 idf에 &quot;1&quot;을 추가하면 idf가 0 인 용어, 즉 학습 세트의 모든 문서에서 발생하는 용어가 완전히 무시되지 않습니다. (위의 idf 공식은 idf를 idf (t) = log [n / (df (t) + 1)])로 정의하는 표준 교과서 표기법과 다릅니다.</target>
        </trans-unit>
        <trans-unit id="99fa4cbad7a403537a73165a2b1d717c58bb9b6b" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).</source>
          <target state="translated">용어 t의 tf-idf를 계산하는 데 사용되는 공식은 tf-idf (d, t) = tf (t) * idf (d, t)이고 idf는 idf (d, t) = log로 계산됩니다. [n / df (d, t)] + 1 ( &lt;code&gt;smooth_idf=False&lt;/code&gt; 인 경우 ), 여기서 n은 총 문서 수이고 df (d, t)는 문서 빈도입니다. 문서 빈도는 용어 t를 포함하는 문서 d의 수입니다. 위 방정식에서 idf에 &quot;1&quot;을 추가하면 idf가 0 인 항, 즉 훈련 세트의 모든 문서에서 발생하는 항이 완전히 무시되지 않습니다. 위의 idf 공식은 idf를 idf (d, t) = log [n / (df (d, t) + 1)]로 정의하는 표준 교과서 표기법과 다릅니다.</target>
        </trans-unit>
        <trans-unit id="2757038bccf5826fff274cfe8684f779a3893302" translate="yes" xml:space="preserve">
          <source>The formula used here does not correspond to the one given in the article. In the original article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn&amp;rsquo;t affect the value of the estimator.</source>
          <target state="translated">여기에 사용 된 공식은 기사에 제공된 공식과 일치하지 않습니다. 원래 기사에서 수식 (23)은 분자와 분모 모두에서 2 / p에 Trace (cov * cov)를 곱한 값을 나타내지 만 큰 p의 경우 2 / p의 값이 너무 작으므로이 연산이 생략됩니다. 추정기의 가치에 영향을 미치지 않습니다.</target>
        </trans-unit>
        <trans-unit id="25426ebad0f81610af376f208a1d7f7cdb7c96c5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. &lt;code&gt;subsample&lt;/code&gt; interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;. Choosing &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; leads to a reduction of variance and an increase in bias.</source>
          <target state="translated">개별 기본 학습자를 맞추는 데 사용되는 샘플의 비율입니다. 1.0보다 작 으면 확률 적 그라디언트 부스팅이 발생합니다. &lt;code&gt;subsample&lt;/code&gt; 은 &lt;code&gt;n_estimators&lt;/code&gt; 매개 변수와 상호 작용합니다 . &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; 선택하면 분산이 감소하고 바이어스가 증가합니다.</target>
        </trans-unit>
        <trans-unit id="568fbbbfe83b2ef390104a99310641880cb62ce5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.</source>
          <target state="translated">각 무작위 설계에 사용될 샘플의 비율. 0과 1 사이 여야합니다. 1이면 모든 샘플이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="52b16a6d5efc38a8b1d594773d860718286a7706" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class are randomly exchanged. Larger values introduce noise in the labels and make the classification task harder.</source>
          <target state="translated">클래스가 무작위로 교환되는 샘플의 비율. 값이 클수록 레이블에 노이즈가 발생하고 분류 작업이 더 어려워집니다.</target>
        </trans-unit>
        <trans-unit id="3e4c940191ac2ce629537cccbeb4e1290fc15133" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder. Note that the default setting flip_y &amp;gt; 0 might lead to less than n_classes in y in some cases.</source>
          <target state="translated">클래스가 무작위로 할당 된 샘플의 비율입니다. 값이 클수록 레이블에 노이즈가 발생하고 분류 작업이 더 어려워집니다. 기본 설정 flip_y&amp;gt; 0은 경우에 따라 y에서 n_classes 미만으로 이어질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2b8586795acd1de1d253165ce5ff74dbee8020e0" translate="yes" xml:space="preserve">
          <source>The free parameters in the model are C and epsilon.</source>
          <target state="translated">모델의 자유 매개 변수는 C 및 엡실론입니다.</target>
        </trans-unit>
        <trans-unit id="852901dbf93c375963721fc84a4888e6b45c78dd" translate="yes" xml:space="preserve">
          <source>The full description of the dataset</source>
          <target state="translated">데이터 세트에 대한 전체 설명</target>
        </trans-unit>
        <trans-unit id="d9127a02d65d598dd00fad372a9aa50b138962b6" translate="yes" xml:space="preserve">
          <source>The full description of the dataset.</source>
          <target state="translated">데이터 세트에 대한 전체 설명입니다.</target>
        </trans-unit>
        <trans-unit id="19877feb57bb09f6d25fbaaa466f42d45ca4d944" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt;&lt;code&gt;img_to_graph&lt;/code&gt;&lt;/a&gt; returns such a matrix from a 2D or 3D image. Similarly, &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;grid_to_graph&lt;/code&gt;&lt;/a&gt; build a connectivity matrix for images given the shape of these image.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt; &lt;code&gt;img_to_graph&lt;/code&gt; &lt;/a&gt; 함수 는 2D 또는 3D 이미지에서 이러한 행렬을 반환합니다. 마찬가지로 &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;grid_to_graph&lt;/code&gt; &lt;/a&gt; 는 이러한 이미지의 모양이 지정된 이미지에 대한 연결성 매트릭스를 작성합니다.</target>
        </trans-unit>
        <trans-unit id="0a9dad7d233dabf5b0bcafb3e330d5014a951e15" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt;&lt;code&gt;lasso_path&lt;/code&gt;&lt;/a&gt; is useful for lower-level tasks, as it computes the coefficients along the full path of possible values.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt; &lt;code&gt;lasso_path&lt;/code&gt; &lt;/a&gt; 함수 는 가능한 값의 전체 경로를 따라 계수를 계산하므로 하위 수준 작업에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="6bbdd98a7d5d87ddd3bbdfb8569481a5e5e04495" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt;&lt;code&gt;cohen_kappa_score&lt;/code&gt;&lt;/a&gt; computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen&amp;rsquo;s kappa&lt;/a&gt; statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt; &lt;code&gt;cohen_kappa_score&lt;/code&gt; &lt;/a&gt; 함수는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen의 카파&lt;/a&gt; 통계량을 계산 합니다. 이 측정법은 분류 자와 대립 진실이 아닌 다른 인간 주석에 의한 레이블을 비교하기위한 것입니다.</target>
        </trans-unit>
        <trans-unit id="93f21bf6b976a0d8004246b9140ac3f9b0310fba" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt;&lt;code&gt;laplacian_kernel&lt;/code&gt;&lt;/a&gt; is a variant on the radial basis function kernel defined as:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt; &lt;code&gt;laplacian_kernel&lt;/code&gt; &lt;/a&gt; 함수 는 다음과 같이 정의 된 방사형 기본 함수 커널의 변형입니다.</target>
        </trans-unit>
        <trans-unit id="3fe1ad7d2ae4a1d31d30a7b5c63e81d53b7568bb" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt; computes the linear kernel, that is, a special case of &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;degree=1&lt;/code&gt; and &lt;code&gt;coef0=0&lt;/code&gt; (homogeneous). If &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are column vectors, their linear kernel is:</source>
          <target state="translated">함수 &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; 가&lt;/a&gt; 인 선형 커널의 특별한 경우 계산 &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;degree=1&lt;/code&gt; 및 &lt;code&gt;coef0=0&lt;/code&gt; (균일)를. 경우 &lt;code&gt;x&lt;/code&gt; 및 &lt;code&gt;y&lt;/code&gt; 열 벡터이고, 그 선형 커널은 :</target>
        </trans-unit>
        <trans-unit id="72295044331d14363e1e1fe37cea504256d7c67f" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt; 함수 는 두 벡터 사이의 차수 d 다항식 커널을 계산합니다. 다항식 커널은 두 벡터 간의 유사성을 나타냅니다. 개념적으로 다항식 커널은 동일한 차원에서 벡터 간의 유사성뿐만 아니라 차원에서도 유사성을 고려합니다. 머신 러닝 알고리즘에 사용될 때 기능 상호 작용을 설명 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2f5b051b13aa3478b0b4a17587fa0bec1e6b87b4" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt;&lt;code&gt;rbf_kernel&lt;/code&gt;&lt;/a&gt; computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:</source>
          <target state="translated">함수 &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt; &lt;code&gt;rbf_kernel&lt;/code&gt; 은&lt;/a&gt; 두 벡터 사이의 방사형 기저 함수 (RBF) 커널을 계산한다. 이 커널은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="a4ac87fe4bd82908551f017a1e984b845dfe00ca" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt;&lt;code&gt;sigmoid_kernel&lt;/code&gt;&lt;/a&gt; computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt; &lt;code&gt;sigmoid_kernel&lt;/code&gt; &lt;/a&gt; 함수 는 두 벡터 사이의 시그 모이 드 커널을 계산합니다. 시그 모이 드 커널은 쌍곡 탄젠트 (hyperbolic tangent) 또는 다층 퍼셉트론 (Multilayer Perceptron)으로도 알려져 있습니다 (신경망 분야에서는 종종 뉴런 활성화 기능으로 사용되기 때문에). 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="2e327cd188c9064345dfa4a6a6764985ae429bc7" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;receiver operating characteristic curve, or ROC curve&lt;/a&gt;. Quoting Wikipedia :</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt; 함수 는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;수신기 작동 특성 곡선 또는 ROC 곡선을&lt;/a&gt; 계산합니다 . 인용 위키 백과 :</target>
        </trans-unit>
        <trans-unit id="a52cb65c85990e02ab77ee98f8132a1c3b7a7b6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; has a similar interface to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; 함수는 cross_val_score 와 유사한 인터페이스를 &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 입력의 각 요소에 대해 테스트 세트에있을 때 해당 요소에 대해 얻은 예측을 리턴합니다. 모든 요소를 ​​테스트 세트에 정확히 한 번만 할당하는 교차 검증 전략 만 사용할 수 있습니다 (그렇지 않으면 예외가 발생 함).</target>
        </trans-unit>
        <trans-unit id="9da333e152484b3ac872458a2a577c489d5042de" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt;&lt;code&gt;validation_curve&lt;/code&gt;&lt;/a&gt; can help in this case:</source>
          <target state="translated">이 경우 &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt; &lt;code&gt;validation_curve&lt;/code&gt; &lt;/a&gt; 함수 가 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ef1bfedb8d96897b52fb746234adf67b6ec8f0b2" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;normalize&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset, either using the &lt;code&gt;l1&lt;/code&gt; or &lt;code&gt;l2&lt;/code&gt; norms:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt; &lt;code&gt;normalize&lt;/code&gt; &lt;/a&gt; 함수 는 &lt;code&gt;l1&lt;/code&gt; 또는 &lt;code&gt;l2&lt;/code&gt; 규범을 사용하여 단일 배열과 유사한 데이터 세트에서이 작업을 빠르고 쉽게 수행 할 수있는 방법을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="621182b188f45e3dd8884ee1d03015ee530041c8" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset:</source>
          <target state="translated">함수 &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt; 은 단일 배열 형 데이터 세트에서이 작업을 빠르고 쉽게 수행 할 수있는 방법을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="6ffd4cf30ad5a8e8b6ec54710ec8f9345471233e" translate="yes" xml:space="preserve">
          <source>The function &lt;code&gt;plot_regression_results&lt;/code&gt; is used to plot the predicted and true targets.</source>
          <target state="translated">&lt;code&gt;plot_regression_results&lt;/code&gt; 함수 는 예측 대상과 실제 대상을 표시하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="14656519a034ad24c06a19460128dde85e00e72d" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">이 함수는 &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; 및 &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]에&lt;/a&gt; 설명 된 k- 최근 접 이웃 거리로부터의 엔트로피 추정에 기초한 비모수 적 방법에 의존합니다 . 두 방법 모두 &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; 에서 원래 제안 된 아이디어를 기반으로합니다 .</target>
        </trans-unit>
        <trans-unit id="cf1d60e35345e80734f9e0bef9342c1dede910a1" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">이 함수는 &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; 및 &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]에&lt;/a&gt; 설명 된 k- 최근 접 이웃 거리로부터의 엔트로피 추정에 기초한 비모수 적 방법에 의존합니다 . 두 방법 모두 &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; 에서 원래 제안 된 아이디어를 기반으로합니다 .</target>
        </trans-unit>
        <trans-unit id="fb8314efd5d7c36c4d228413206cd5331b82e456" translate="yes" xml:space="preserve">
          <source>The function requires either the argument &lt;code&gt;grid&lt;/code&gt; which specifies the values of the target features on which the partial dependence function should be evaluated or the argument &lt;code&gt;X&lt;/code&gt; which is a convenience mode for automatically creating &lt;code&gt;grid&lt;/code&gt; from the training data. If &lt;code&gt;X&lt;/code&gt; is given, the &lt;code&gt;axes&lt;/code&gt; value returned by the function gives the axis for each target feature.</source>
          <target state="translated">이 함수에는 부분 의존 함수를 평가할 대상 피처의 값을 지정 하는 인수 &lt;code&gt;grid&lt;/code&gt; 또는 훈련 데이터에서 &lt;code&gt;grid&lt;/code&gt; 를 자동으로 생성하기위한 편리한 모드 인 인수 &lt;code&gt;X&lt;/code&gt; 가 필요합니다 . 경우 &lt;code&gt;X&lt;/code&gt; 가 부여되어 상기 &lt;code&gt;axes&lt;/code&gt; 이 함수에 의해 리턴 된 값은 각각의 타겟 피쳐에 대한 축을 제공한다.</target>
        </trans-unit>
        <trans-unit id="d2fedea237bdc334f6322c4692b523df3b5f6fbf" translate="yes" xml:space="preserve">
          <source>The function to be decorated</source>
          <target state="translated">꾸미는 기능</target>
        </trans-unit>
        <trans-unit id="f7fc9606332ff37416d86060e52aa56595a5e3ce" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;friedman_mse&amp;rdquo; for the mean squared error with improvement score by Friedman, &amp;ldquo;mse&amp;rdquo; for mean squared error, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error. The default value of &amp;ldquo;friedman_mse&amp;rdquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원 기준은 Friedman이 개선 한 평균 제곱 오차의 경우 &quot;friedman_mse&quot;, 평균 제곱 오차의 경우 &quot;mse&quot;, 평균 절대 오차의 경우 &quot;mae&quot;입니다. &quot;friedman_mse&quot;의 기본값은 경우에 따라 더 나은 근사치를 제공 할 수 있으므로 일반적으로 최고입니다.</target>
        </trans-unit>
        <trans-unit id="8c0bede693f27f1257cedfa964bd89180e6bfada" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원되는 기준은 Gini 불순물에 대한&amp;ldquo;지니&amp;rdquo;및 정보 획득에 대한&amp;ldquo;엔트로피&amp;rdquo;입니다.</target>
        </trans-unit>
        <trans-unit id="03e746b06910ee8c07bc239c0b780e5294140dfb" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain. Note: this parameter is tree-specific.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원되는 기준은 Gini 불순물에 대한&amp;ldquo;지니&amp;rdquo;및 정보 획득에 대한&amp;ldquo;엔트로피&amp;rdquo;입니다. 참고 :이 매개 변수는 트리마다 다릅니다.</target>
        </trans-unit>
        <trans-unit id="c211c5fb9289c19b7ab7fb609c5f42d0fd7fb417" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, &amp;ldquo;friedman_mse&amp;rdquo;, which uses mean squared error with Friedman&amp;rsquo;s improvement score for potential splits, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원되는 기준은 평균 제곱 오차에 대해 &quot;mse&quot;이며, 이는 기능 선택 기준으로 분산 감소와 동일하며 각 터미널 노드의 평균 인 &quot;friedman_mse&quot;를 사용하여 L2 손실을 최소화합니다. 평균 절대 오차의 경우 &quot;mae&quot;를 나누고 각 터미널 노드의 중앙값을 사용하여 L1 손실을 최소화합니다.</target>
        </trans-unit>
        <trans-unit id="bf422f8c78875448c6e34d2c008baf4a5621c47d" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원되는 기준은 평균 제곱 오차에 대해 &quot;mse&quot;이며, 이는 특징 선택 기준으로 분산 감소와 동일하며 평균 절대 오차에 대해서는 &quot;mae&quot;입니다.</target>
        </trans-unit>
        <trans-unit id="8599be908b4b8dfee70cafa139c14ed8be2f9fde" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;lsquo;friedman_mse&amp;rsquo; for the mean squared error with improvement score by Friedman, &amp;lsquo;mse&amp;rsquo; for mean squared error, and &amp;lsquo;mae&amp;rsquo; for the mean absolute error. The default value of &amp;lsquo;friedman_mse&amp;rsquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">분할의 품질을 측정하는 기능입니다. 지원되는 기준은 Friedman의 개선 점수가있는 평균 제곱 오차의 경우 'friedman_mse', 평균 제곱 오차의 경우 'mse', 평균 절대 오차의 경우 'mae'입니다. 'friedman_mse'의 기본값은 경우에 따라 더 나은 근사치를 제공 할 수 있으므로 일반적으로 가장 좋습니다.</target>
        </trans-unit>
        <trans-unit id="8b3c9548b1b42f5af71c7bfe5f885c712284a1e6" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;, or a tuple of such objects.</source>
          <target state="translated">거리 행렬의 각 청크에 적용되는 함수로 필요한 값으로 줄입니다. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; 가 반복적으로 호출됩니다. 여기서 &lt;code&gt;D_chunk&lt;/code&gt; 는 행 &lt;code&gt;start&lt;/code&gt; 에서 시작하여 쌍방향 거리 행렬의 연속적인 수직 슬라이스입니다 . 길이가 &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; 인 배열, 목록 또는 희소 행렬 또는 이러한 객체의 튜플을 반환해야 합니다.</target>
        </trans-unit>
        <trans-unit id="0114f911a9d8b12f31f5da2c60626b44d9e0ba26" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return one of: None; an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;; or a tuple of such objects. Returning None is useful for in-place operations, rather than reductions.</source>
          <target state="translated">거리 행렬의 각 청크에 적용되어 필요한 값으로 줄이는 함수입니다. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; 가 반복적으로 호출됩니다. 여기서 &lt;code&gt;D_chunk&lt;/code&gt; 는 행 &lt;code&gt;start&lt;/code&gt; 에서 시작하는 쌍별 거리 행렬의 연속적인 수직 슬라이스입니다 . 다음 중 하나를 반환해야합니다. None; 길이가 &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; 인 배열, 목록 또는 희소 행렬 ; 또는 그러한 객체의 튜플. None을 반환하는 것은 감소보다는 내부 작업에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="34f983a96e6e1ba7a36ae52fb70df739e3a5aca9" translate="yes" xml:space="preserve">
          <source>The functional form of the G function used in the approximation to neg-entropy. Could be either &amp;lsquo;logcosh&amp;rsquo;, &amp;lsquo;exp&amp;rsquo;, or &amp;lsquo;cube&amp;rsquo;. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:</source>
          <target state="translated">음의 엔트로피 근사에 사용되는 G 함수의 기능적 형식입니다. 'logcosh', 'exp'또는 'cube'일 수 있습니다. 자신의 기능을 제공 할 수도 있습니다. 함수의 값과 그 파생물을 포함하는 튜플을 해당 지점에 반환해야합니다. 예:</target>
        </trans-unit>
        <trans-unit id="f3580ae3ff67a44925f6075f1fc46034cd1f3d14" translate="yes" xml:space="preserve">
          <source>The generated array.</source>
          <target state="translated">생성 된 배열.</target>
        </trans-unit>
        <trans-unit id="22dd341f8c92381f47326cd6fbe4fff46dd59a6b" translate="yes" xml:space="preserve">
          <source>The generated matrix.</source>
          <target state="translated">생성 된 행렬.</target>
        </trans-unit>
        <trans-unit id="b670a7959baa25f4a9c290d5df951114cede8bad" translate="yes" xml:space="preserve">
          <source>The generated samples.</source>
          <target state="translated">생성 된 샘플.</target>
        </trans-unit>
        <trans-unit id="61a0705074aa05bf429a2ff6dd9f09842d1c86f2" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">센터를 초기화하는 데 사용되는 생성기. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="17ea6c727e52953fb5e03ae24f1b3fcbc132ab70" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">센터를 초기화하는 데 사용되는 생성기입니다. 여러 함수 호출에서 재현 가능한 출력을 위해 int를 전달합니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="473c7ed23812d4bf8fcf6c97ab0551b7301ccf95" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">코드북을 초기화하는 데 사용되는 생성기입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a0fb8deb3b7c972152c6c4cb8a6e10d0c386fcd5" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">코드북을 초기화하는 데 사용되는 생성기입니다. 여러 함수 호출에서 재현 가능한 출력을 위해 int를 전달합니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="4e831547e83750929880db075c40fc4aa61eb4b8" translate="yes" xml:space="preserve">
          <source>The generator used to randomize the design. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">설계를 무작위 화하는 데 사용되는 생성기. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cd84f22d9e60e3f86bb2455d78aaf109647d76b1" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select a subset of samples. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;sample_size is not None&lt;/code&gt;.</source>
          <target state="translated">생성기는 샘플의 하위 집합을 무작위로 선택하는 데 사용됩니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;sample_size is not None&lt;/code&gt; 경우에 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="36aa9d7d033388d006150bbc613bf9f61afb2f31" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">대역폭 추정을 위해 입력 포인트에서 샘플을 무작위로 선택하는 데 사용되는 생성기입니다. int를 사용하여 임의성을 결정 론적으로 만드십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="eebbb245bcabbcd7cdd05b74aa699ac85737839b" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">대역폭 추정을 위해 입력 포인트에서 샘플을 무작위로 선택하는 데 사용되는 생성기입니다. 임의성을 결정적으로 만들려면 int를 사용하십시오. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="ff752b880533b887a66601fd8d804091da8b44b7" translate="yes" xml:space="preserve">
          <source>The goal is to compare different estimators to see which one is best for the &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt; when using a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt;&lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt;&lt;/a&gt; estimator on the California housing dataset with a single value randomly removed from each row.</source>
          <target state="translated">목표는 캘리포니아 주택 데이터 세트에서 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt; &lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt; &lt;/a&gt; 추정기를 사용할 때 &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt; 에&lt;/a&gt; 가장 적합한 추정기를 비교 하여 각 행에서 무작위로 제거 된 단일 값을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="6c9c479a6511fc655c70a7c51ab0a85c04034b21" translate="yes" xml:space="preserve">
          <source>The goal is to measure the latency one can expect when doing predictions either in bulk or atomic (i.e. one by one) mode.</source>
          <target state="translated">목표는 대량 또는 원자 (예 : 하나씩) 모드에서 예측을 수행 할 때 예상 할 수있는 대기 시간을 측정하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="95b3715b7309073ce23a5e1ae6bd1c83fb5ab128" translate="yes" xml:space="preserve">
          <source>The goal of &lt;strong&gt;ensemble methods&lt;/strong&gt; is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</source>
          <target state="translated">&lt;strong&gt;앙상블 방법&lt;/strong&gt; 의 목표 는 단일 추정기에 대한 일반 화성 / 강건성을 향상시키기 위해 주어진 학습 알고리즘으로 구축 된 여러 기본 추정기의 예측을 결합하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="a736cc9a27a226bf9910cdd3892bdfd0fe07d378" translate="yes" xml:space="preserve">
          <source>The goal of NCA is to learn an optimal linear transformation matrix of size &lt;code&gt;(n_components, n_features)&lt;/code&gt;, which maximises the sum over all samples \(i\) of the probability \(p_i\) that \(i\) is correctly classified, i.e.:</source>
          <target state="translated">NCA의 목표는 \ (i \)가 올바르게 분류 될 확률 \ (p_i \)의 모든 샘플 \ (i \)에 대한 합계를 최대화 하는 최적의 선형 변환 행렬 크기 &lt;code&gt;(n_components, n_features)&lt;/code&gt; 를 학습하는 것입니다. 즉 :</target>
        </trans-unit>
        <trans-unit id="4be143708c968fbd1f8e4e8288622de6f1251f81" translate="yes" xml:space="preserve">
          <source>The goal of this example is to analyze the graph of links inside wikipedia articles to rank articles by relative importance according to this eigenvector centrality.</source>
          <target state="translated">이 예제의 목표는이 고유 벡터 중심성에 따라 위키피디아 기사 내의 링크 그래프를 분석하여 상대적 중요성에 따라 기사의 순위를 정하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="5095c3c5239296131fc48d25860f3771ab9b91e3" translate="yes" xml:space="preserve">
          <source>The goal of this example is to show intuitively how the metrics behave, and not to find good clusters for the digits. This is why the example works on a 2D embedding.</source>
          <target state="translated">이 예의 목표는 지표의 작동 방식을 직관적으로 표시하고 숫자에 적합한 클러스터를 찾지 않는 것입니다. 이것이 예제가 2D 임베딩에서 작동하는 이유입니다.</target>
        </trans-unit>
        <trans-unit id="20bd11aa678c9e04777a40ec0f194ab6b7581aa6" translate="yes" xml:space="preserve">
          <source>The goal of this guide is to explore some of the main &lt;code&gt;scikit-learn&lt;/code&gt; tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics.</source>
          <target state="translated">이 안내서의 목표는 20 가지 주제에 대한 텍스트 문서 (뉴스 그룹 게시물) 모음 분석이라는 단일 실제 작업 에서 주요 &lt;code&gt;scikit-learn&lt;/code&gt; 도구 중 일부를 탐색하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="c224302bdf144aacfdfc260a6638fe1107c2e8d4" translate="yes" xml:space="preserve">
          <source>The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</source>
          <target state="translated">주어진 문서에서 토큰의 원시 발생 빈도 대신 tf-idf를 사용하는 목표는 주어진 코퍼스에서 매우 자주 발생하고 따라서 경험적으로 덜 유용한 토큰의 영향을 축소하는 것입니다. 훈련 말뭉치의 작은 부분.</target>
        </trans-unit>
        <trans-unit id="7c4e551c63ec44d66aa3a175c12b51d46fb0ca3e" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when &lt;code&gt;eval_gradient&lt;/code&gt; is True.</source>
          <target state="translated">커널의 하이퍼 파라미터에 대한 커널 k (X, X)의 기울기입니다. &lt;code&gt;eval_gradient&lt;/code&gt; 가 True 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="3325084d0d02139eb921a2545ba48cff39657720" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</source>
          <target state="translated">커널의 하이퍼 파라미터에 대한 커널 k (X, X)의 기울기. eval_gradient가 True 인 경우에만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="1d28aeaa2593827c70ee0cbf35f8d5891fd13e08" translate="yes" xml:space="preserve">
          <source>The graph data is fetched from the DBpedia dumps. DBpedia is an extraction of the latent structured data of the Wikipedia content.</source>
          <target state="translated">그래프 데이터는 DBpedia 덤프에서 가져옵니다. DBpedia는 Wikipedia 컨텐츠의 잠재 구조화 된 데이터를 추출한 것입니다.</target>
        </trans-unit>
        <trans-unit id="94a8641249b9d38c741294dd700ba9c013e60d15" translate="yes" xml:space="preserve">
          <source>The graph should contain only one connect component, elsewhere the results make little sense.</source>
          <target state="translated">그래프에는 하나의 연결 구성 요소 만 포함되어야하며, 그렇지 않으면 결과가 의미가 없습니다.</target>
        </trans-unit>
        <trans-unit id="0aca214d3abec7143ce0e3db3703c7759829a517" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level Bayesian model:</source>
          <target state="translated">LDA의 그래픽 모델은 3 단계 베이지안 모델입니다.</target>
        </trans-unit>
        <trans-unit id="53c200c93d4342eee587d3c6f82bd52b691b78c8" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level generative model:</source>
          <target state="translated">LDA의 그래픽 모델은 3 단계 생성 모델입니다.</target>
        </trans-unit>
        <trans-unit id="b70b496cfa90301063390d7d501e40e03165e505" translate="yes" xml:space="preserve">
          <source>The graphical model of an RBM is a fully-connected bipartite graph.</source>
          <target state="translated">RBM의 그래픽 모델은 완전히 연결된 이분 그래프입니다.</target>
        </trans-unit>
        <trans-unit id="43e8165a75805bcff11ce07ae2103365830550d8" translate="yes" xml:space="preserve">
          <source>The grid of &lt;code&gt;target_variables&lt;/code&gt; values for which the partial dependecy should be evaluated (either &lt;code&gt;grid&lt;/code&gt; or &lt;code&gt;X&lt;/code&gt; must be specified).</source>
          <target state="translated">부분 의존성을 평가해야하는 &lt;code&gt;target_variables&lt;/code&gt; 값 의 그리드입니다 ( &lt;code&gt;grid&lt;/code&gt; 또는 &lt;code&gt;X&lt;/code&gt; 를 지정해야 함).</target>
        </trans-unit>
        <trans-unit id="1fb55e8edd66947697c84e91d6ac3fa247bd29e9" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting</source>
          <target state="translated">피팅에 사용되는 알파 그리드</target>
        </trans-unit>
        <trans-unit id="c8ce42c94fb7c4644fc397d481823f721280ec22" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio</source>
          <target state="translated">각 l1_ratio에 대해 피팅에 사용되는 알파 그리드</target>
        </trans-unit>
        <trans-unit id="92edf0193fba09badba27bf7739525bce6da7601" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio.</source>
          <target state="translated">각 l1_ratio에 대해 피팅에 사용되는 알파 격자입니다.</target>
        </trans-unit>
        <trans-unit id="22daba72217df04ec1af1a8a340277c45da4b54e" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting.</source>
          <target state="translated">피팅에 사용되는 알파 격자입니다.</target>
        </trans-unit>
        <trans-unit id="37c7f40c413d2aeb11b8645a838ea3c671b821b6" translate="yes" xml:space="preserve">
          <source>The grid points between 0 and 1: alpha/alpha_max</source>
          <target state="translated">그리드 포인트는 0과 1 사이입니다. alpha / alpha_max</target>
        </trans-unit>
        <trans-unit id="1e90c7186240eacb6693334f5a2cb603522a3c95" translate="yes" xml:space="preserve">
          <source>The grid search instance behaves like a normal &lt;code&gt;scikit-learn&lt;/code&gt; model. Let&amp;rsquo;s perform the search on a smaller subset of the training data to speed up the computation:</source>
          <target state="translated">그리드 검색 인스턴스는 일반적인 &lt;code&gt;scikit-learn&lt;/code&gt; 모델 처럼 동작 합니다. 계산 속도를 높이기 위해 훈련 데이터의 작은 하위 집합에서 검색을 수행해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="0fb72e7d32db09c24209afb30ebd0c3ef9469a12" translate="yes" xml:space="preserve">
          <source>The grid search provided by &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; exhaustively generates candidates from a grid of parameter values specified with the &lt;code&gt;param_grid&lt;/code&gt; parameter. For instance, the following &lt;code&gt;param_grid&lt;/code&gt;:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; 가&lt;/a&gt; 제공하는 그리드 검색 은 &lt;code&gt;param_grid&lt;/code&gt; 파라미터로 지정된 파라미터 값의 그리드에서 후보를 철저하게 생성합니다 . 예를 들어, 다음 &lt;code&gt;param_grid&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="5d877967ec11cd12214237a4561547e48123553d" translate="yes" xml:space="preserve">
          <source>The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.</source>
          <target state="translated">메트릭 선택에 대한 지침은 서로 다른 클래스의 샘플 간 거리를 최대화하고 각 클래스 내에서이를 최소화하는 메트릭을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="42b20fdbf19bf6fb28b9336fb1c931e7ebd9ff54" translate="yes" xml:space="preserve">
          <source>The handwritten digit dataset has 1797 total points. The model will be trained using all points, but only 30 will be labeled. Results in the form of a confusion matrix and a series of metrics over each class will be very good.</source>
          <target state="translated">손으로 쓴 숫자 데이터 집합에는 총 1797 개의 점이 있습니다. 모델은 모든 점을 사용하여 학습되지만 30 만 레이블이 지정됩니다. 혼동 행렬 형태와 각 클래스에 대한 일련의 메트릭 결과가 매우 좋습니다.</target>
        </trans-unit>
        <trans-unit id="1dbebc05eff701a160c5a2149017afcbb50998d6" translate="yes" xml:space="preserve">
          <source>The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">사용 된 해시 함수는 서명 된 32 비트 버전의 Murmurhash3입니다.</target>
        </trans-unit>
        <trans-unit id="86a9e346ee809bf0b27001ea98677db7fe898a9e" translate="yes" xml:space="preserve">
          <source>The higher &lt;code&gt;p&lt;/code&gt; the less weight is given to extreme deviations between true and predicted targets.</source>
          <target state="translated">&lt;code&gt;p&lt;/code&gt; 가 높을수록 실제 목표와 예상 목표 사이의 극심한 편차에 더 적은 가중치가 부여됩니다.</target>
        </trans-unit>
        <trans-unit id="6ddf3e17ce9800f43a67078b41067ddc0d8c6a4c" translate="yes" xml:space="preserve">
          <source>The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex.</source>
          <target state="translated">농도가 높을수록 중앙에 더 많은 질량이 들어가고 더 많은 성분이 활성화되고 농도가 낮을수록 단면 가장자리에 더 많은 질량이 생깁니다.</target>
        </trans-unit>
        <trans-unit id="d55d0516aec7ebe6efc3bc1e9f0b83c115b36898" translate="yes" xml:space="preserve">
          <source>The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">높을수록 기능이 더 중요합니다. 기능의 중요성은 해당 기능이 가져온 기준의 (정규화 된) 총 감소로 계산됩니다. 지니 중요성이라고도합니다.</target>
        </trans-unit>
        <trans-unit id="1006ebebebbef4245b7568132e1eabf566a79140" translate="yes" xml:space="preserve">
          <source>The highest p-value for features to be kept.</source>
          <target state="translated">유지할 피처의 가장 높은 p- 값입니다.</target>
        </trans-unit>
        <trans-unit id="0c1d204d0071e8e2a8101579dd644bade4aef0d9" translate="yes" xml:space="preserve">
          <source>The highest uncorrected p-value for features to keep.</source>
          <target state="translated">유지할 피쳐에 대해 수정되지 않은 가장 높은 p- 값입니다.</target>
        </trans-unit>
        <trans-unit id="3708900f6a079bc16cd76c37da5647812d1a0427" translate="yes" xml:space="preserve">
          <source>The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights.</source>
          <target state="translated">희소성을 유발하는 이전의 가중치가 암시되기 때문에 추정 된 가중치의 히스토그램은 매우 정점에 이릅니다.</target>
        </trans-unit>
        <trans-unit id="d24bdc7c6b1e4dbeb474c6e9f76042366464386e" translate="yes" xml:space="preserve">
          <source>The hyperparameters</source>
          <target state="translated">하이퍼 파라미터</target>
        </trans-unit>
        <trans-unit id="25f29cead3cfba36338bcf026da5a846edf25636" translate="yes" xml:space="preserve">
          <source>The i-th score &lt;code&gt;train_score_[i]&lt;/code&gt; is the deviance (= loss) of the model at iteration &lt;code&gt;i&lt;/code&gt; on the in-bag sample. If &lt;code&gt;subsample == 1&lt;/code&gt; this is the deviance on the training data.</source>
          <target state="translated">i 번째 점수 &lt;code&gt;train_score_[i]&lt;/code&gt; 는 백 내 샘플 에서 반복 &lt;code&gt;i&lt;/code&gt; 에서 모델의 이탈 (= 손실)입니다 . &lt;code&gt;subsample == 1&lt;/code&gt; 경우 훈련 데이터에 대한 편차입니다.</target>
        </trans-unit>
        <trans-unit id="7768180d6d5c29f9be6eb9d80f99cbf93007e25a" translate="yes" xml:space="preserve">
          <source>The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.</source>
          <target state="translated">기본 생성 프로세스가 종속 샘플 그룹을 생성하면 iid 가정이 깨집니다.</target>
        </trans-unit>
        <trans-unit id="d5e94b9dd17268ac8457a9d26f68d07b8365584a" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; &lt;/a&gt; 의 기본 개념은 개념적으로 다른 기계 학습 분류기를 결합하고 과반수 투표 또는 평균 예측 확률 (소프트 투표)을 사용하여 클래스 레이블을 예측하는 것입니다. 이러한 분류기는 개별 약점의 균형을 맞추기 위해 똑같이 잘 수행되는 모델 세트에 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1f4d07aca22136c4b5a250fb6a846554681fb509" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt; &lt;code&gt;VotingRegressor&lt;/code&gt; &lt;/a&gt; 의 기본 개념은 개념적으로 다른 기계 학습 회귀자를 결합하고 평균 예측 값을 반환하는 것입니다. 이러한 회귀 변수는 개별 약점의 균형을 맞추기 위해 똑같이 잘 수행되는 모델 세트에 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="190c7529dfd22ae7ed70d4f84d1deb499abbe749" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;code&gt;VotingClassifier&lt;/code&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; 의 기본 개념은 개념적으로 다른 머신 러닝 분류기를 결합하고 다 수표 또는 평균 예측 확률 (소프트 투표)을 사용하여 클래스 레이블을 예측하는 것입니다. 이러한 분류기는 개별 약점의 균형을 맞추기 위해 동일한 성능의 모델 세트에 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f4f9efa7d25a183372e2e9ce75bfe20581578033" translate="yes" xml:space="preserve">
          <source>The image as a numpy array: height x width x color</source>
          <target state="translated">numpy 배열로서의 이미지 : 높이 x 너비 x 색상</target>
        </trans-unit>
        <trans-unit id="77bae2f833c03e6f85b6a43b3aa3ead8f81ce2e5" translate="yes" xml:space="preserve">
          <source>The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.</source>
          <target state="translated">이미지는 256 그레이 레벨로 양자화되고 부호없는 8 비트 정수로 저장됩니다. 로더는 간격 [0, 1]에서이 값을 부동 소수점 값으로 변환하므로 많은 알고리즘에서 작업하기가 더 쉽습니다.</target>
        </trans-unit>
        <trans-unit id="6db946ce103c288b47eadb4f1488a8fdc91a7488" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients. See &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; for another implementation:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; 클래스의 구현 은 계수에 맞는 알고리즘으로 좌표 하강을 사용합니다. 다른 구현에 대해서는 &lt;a href=&quot;#least-angle-regression&quot;&gt;최소 각도 회귀&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="64dabeaa3f8e38c30333615b1b174cbd1348b11e" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; &lt;/a&gt; 클래스의 구현 에서는 계수에 맞는 알고리즘으로 좌표 하강을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="555e24352725a2133c35e7a52f56a702cd7f81c4" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; &lt;/a&gt; 클래스의 구현 에서는 계수에 맞는 알고리즘으로 좌표 하강을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="f9cb17c43bed4736104925d68075ebd6b07ace68" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt;. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:</source>
          <target state="translated">구현은 &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt; 의 알고리즘 2.1을 기반으로 합니다. 표준 scikit-learn 추정기의 API 외에도 GaussianProcessRegressor는 :</target>
        </trans-unit>
        <trans-unit id="70780cc3dac260d0fb4e85faed94f4134c9dc0ae" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">이 구현은 Rasmussen과 Williams의 GPML (Gaussian Processes for Machine Learning) 알고리즘 2.1을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="1904d3648b9818da40270920e1855bd9dcd752d1" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">이 구현은 Rasmussen과 Williams의 GPML (Gaussian Processes for Machine Learning)의 알고리즘 3.1, 3.2 및 5.1을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="0eeedaeb1d413ea7240926a8ca5811e478d2d62b" translate="yes" xml:space="preserve">
          <source>The implementation is based on an ensemble of ExtraTreeRegressor. The maximum depth of each tree is set to &lt;code&gt;ceil(log_2(n))&lt;/code&gt; where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="translated">구현은 ExtraTreeRegressor의 앙상블을 기반으로합니다. 각 트리의 최대 깊이는 &lt;code&gt;ceil(log_2(n))&lt;/code&gt; 로 설정됩니다. 여기서 \ (n \)은 트리를 만드는 데 사용되는 샘플 수입니다 (자세한 내용은 (Liu et al., 2008) 참조).</target>
        </trans-unit>
        <trans-unit id="166f55526ba60cbc129406f3899569f7b0eb4efd" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm.</source>
          <target state="translated">구현은 libsvm을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="cb75b7ddb016087965f5fad7be27fc6c1cb16290" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.</source>
          <target state="translated">구현은 libsvm을 기반으로합니다. 적합 시간 복잡도는 샘플 수에 따라 2 차 이상이며 2 만 개가 넘는 샘플로 데이터 세트에 맞게 확장하기가 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="fa8767a0fc5ecb59976beb65f6a5637eaddaef83" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVR&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDRegressor&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="translated">구현은 libsvm을 기반으로합니다. 적합 시간 복잡도는 샘플 수가 2 차 이상이므로 10000 개 이상의 샘플이있는 데이터 세트로 확장하기가 어렵습니다. 대규모 데이터 세트의 경우 &lt;a href=&quot;sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVR&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDRegressor&lt;/code&gt; 를&lt;/a&gt; 대신 사용 하는 것이 &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt; &lt;/a&gt; . sklearn.kernel_approximation.Nystroem 변환기 이후에 가능 합니다.</target>
        </trans-unit>
        <trans-unit id="c37eaa47201071ab85ed7ee61d61deb2b696067c" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="translated">구현은 libsvm을 기반으로합니다. 적합 시간은 샘플 수에 따라 최소한 2 차적으로 확장되며 수만 개의 샘플을 넘어서는 비실용적 일 수 있습니다. 대규모 데이터 세트의 경우 &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; 를&lt;/a&gt; 대신 사용 하는 것이 &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt; &lt;/a&gt; . sklearn.kernel_approximation.Nystroem 변환기 이후에 가능 합니다.</target>
        </trans-unit>
        <trans-unit id="a4dec30aaf65f05fda35ca6a3928a075e5f1587d" translate="yes" xml:space="preserve">
          <source>The implementation is designed to:</source>
          <target state="translated">구현은 다음을 위해 설계되었습니다.</target>
        </trans-unit>
        <trans-unit id="e44f638bc5a5bdc8405d919df1e1a354ca31f099" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; is based on an ensemble of &lt;a href=&quot;generated/sklearn.tree.extratreeregressor#sklearn.tree.ExtraTreeRegressor&quot;&gt;&lt;code&gt;tree.ExtraTreeRegressor&lt;/code&gt;&lt;/a&gt;. Following Isolation Forest original paper, the maximum depth of each tree is set to \(\lceil \log_2(n) \rceil\) where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt; &lt;code&gt;ensemble.IsolationForest&lt;/code&gt; &lt;/a&gt; 구현은 &lt;a href=&quot;generated/sklearn.tree.extratreeregressor#sklearn.tree.ExtraTreeRegressor&quot;&gt; &lt;code&gt;tree.ExtraTreeRegressor&lt;/code&gt; &lt;/a&gt; 앙상블을 기반으로합니다 . Isolation Forest 원본 논문에 따라 각 나무의 최대 깊이는 \ (\ lceil \ log_2 (n) \ rceil \)로 설정됩니다. 여기서 \ (n \)은 나무를 만드는 데 사용 된 샘플 수입니다 ((Liu et al ., 2008)).</target>
        </trans-unit>
        <trans-unit id="08d84e0cf0c38a38100a9dad48e0efc744fd2db8" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;.</source>
          <target state="translated">구현 &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt; - 학습에 scikit은 다변량 선형 회귀 모델로 일반화 다음 &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; 다차원의 중앙값을 일반화하여 중간 공간 &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="fda50d88e9c81601cfc445f1c078677bdd2f3d40" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id36&quot;&gt;12&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id37&quot;&gt;13&lt;/a&gt;.</source>
          <target state="translated">구현 &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt; - 학습에 scikit은 다변량 선형 회귀 모델을 일반화 다음 &lt;a href=&quot;#f1&quot; id=&quot;id36&quot;&gt;12&lt;/a&gt; 다차원의 중앙값을 일반화하여 중간 공간 &lt;a href=&quot;#f2&quot; id=&quot;id37&quot;&gt;(13)&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="eb185b2e148e3613cdde933dd05a6b32e3b9acbe" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM&lt;/a&gt; of L&amp;eacute;on Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">SGD의 구현은 &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM의&lt;/a&gt; 영향을받습니다.L&amp;eacute;on Bottou의 SvmSGD와 유사하게 가중치 벡터는 스칼라와 L2 정규화의 경우 효율적인 가중치 업데이트를 허용하는 벡터의 곱으로 표시됩니다. 희소 특징 벡터의 경우, 인터셉트는 더 자주 업데이트된다는 사실을 설명하기 위해 더 작은 학습 속도 (0.0을 곱한)로 업데이트됩니다. 학습 예제는 순차적으로 선택되며 학습 된 각 예제 후에 학습 속도가 낮아집니다. 우리는 Shalev-Shwartz 등의 학습률 일정을 채택했습니다. 2007. 다중 등급 분류의 경우 &quot;1 대 전체&quot;접근 방식이 사용됩니다. 우리는 Tsuruoka et al. L1 정규화 (및 Elastic Net)의 경우 2009 년. 코드는 Cython으로 작성되었습니다.</target>
        </trans-unit>
        <trans-unit id="3db47613e45248213b8540e2c5b681f6afec29a2" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;code&gt;Stochastic Gradient SVM&lt;/code&gt; of &lt;a href=&quot;#id10&quot; id=&quot;id7&quot;&gt;7&lt;/a&gt;. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse input &lt;code&gt;X&lt;/code&gt;, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from &lt;a href=&quot;#id12&quot; id=&quot;id8&quot;&gt;8&lt;/a&gt;. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed in &lt;a href=&quot;#id13&quot; id=&quot;id9&quot;&gt;9&lt;/a&gt; for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">SGD의 구현은에 의해 영향을 받는다 &lt;code&gt;Stochastic Gradient SVM&lt;/code&gt; 의 &lt;a href=&quot;#id10&quot; id=&quot;id7&quot;&gt;7&lt;/a&gt; . SvmSGD와 유사하게 가중치 벡터는 스칼라와 L2 정규화의 경우 효율적인 가중치 업데이트를 허용하는 벡터의 곱으로 표시됩니다. 희소 입력 &lt;code&gt;X&lt;/code&gt; 의 경우 더 자주 업데이트된다는 사실을 고려하여 더 작은 학습률 (0.01 곱하기)로 절편이 업데이트됩니다. 학습 예는 순차적으로 선택되며 각 관찰 예 후에 학습률이 낮아집니다. 우리는 &lt;a href=&quot;#id12&quot; id=&quot;id8&quot;&gt;8&lt;/a&gt; 에서 학습률 일정을 채택했습니다 . 다중 클래스 분류의 경우 &quot;일대 다&quot;접근 방식이 사용됩니다. &lt;a href=&quot;#id13&quot; id=&quot;id9&quot;&gt;9&lt;/a&gt; 에서 제안한 잘린 기울기 알고리즘을 사용합니다.L1 정규화 (및 Elastic Net) 용. 코드는 Cython으로 작성되었습니다.</target>
        </trans-unit>
        <trans-unit id="234c6abfe6fe107b5770fe372b7d9e14789fc069" translate="yes" xml:space="preserve">
          <source>The implementation of logistic regression in scikit-learn can be accessed from class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.</source>
          <target state="translated">scikit-learn에서 로지스틱 회귀 구현은 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 클래스에서 액세스 할 수 있습니다 . 이 구현은 선택적인 L2 또는 L1 정규화를 사용하여 이진, 일대일 또는 다항 로지스틱 회귀에 적합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="11a45f793137837fb140d6499ba2bfe32aacbae3" translate="yes" xml:space="preserve">
          <source>The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">지형지 물의 중요성은 해당 지형지 물이 가져온 기준의 (정규화 된) 총 축소로 계산됩니다. 지니 중요도라고도합니다.</target>
        </trans-unit>
        <trans-unit id="285ba1aeef83b4f72aaed81188dabeb94680bb69" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator.</source>
          <target state="translated">이전 반복에 비해 백 아웃 샘플의 손실 개선 (= 이탈). &lt;code&gt;oob_improvement_[0]&lt;/code&gt; 오버 첫번째 단계의 손실의 개선 &lt;code&gt;init&lt;/code&gt; 추정기.</target>
        </trans-unit>
        <trans-unit id="820150544fe550e2e074294e51088eda9cdc8014" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator. Only available if &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;</source>
          <target state="translated">이전 반복에 비해 out-of-bag 샘플의 손실 (= 편차) 개선. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; 은 &lt;code&gt;init&lt;/code&gt; 추정기에 비해 첫 번째 단계의 손실 개선입니다 . &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; 경우에만 사용 가능</target>
        </trans-unit>
        <trans-unit id="d60773790f2e7bcb156876ca46ea3f1191f69076" translate="yes" xml:space="preserve">
          <source>The impurity at \(m\) is computed using an impurity function \(H()\), the choice of which depends on the task being solved (classification or regression)</source>
          <target state="translated">\ (m \)의 불순물은 불순물 함수 \ (H () \)를 사용하여 계산되며, 선택하는 문제는 해결중인 작업 (분류 또는 회귀)에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="d750c1db4cd012b1969f8fbe25c31782ef1879ba" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importance ranks the numerical features to be the most important features. As a result, the non-predictive &lt;code&gt;random_num&lt;/code&gt; variable is ranked the most important!</source>
          <target state="translated">불순물 기반 기능 중요도는 숫자 기능을 가장 중요한 기능으로 분류합니다. 결과적으로 비 예측 &lt;code&gt;random_num&lt;/code&gt; 변수가 가장 중요하게 순위가 매겨집니다!</target>
        </trans-unit>
        <trans-unit id="eb74ef34fa0b599fafdb1111102755a10d538f2b" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore &lt;strong&gt;do not necessarily inform us on which features are most important to make good predictions on held-out dataset&lt;/strong&gt;. Secondly, &lt;strong&gt;they favor high cardinality features&lt;/strong&gt;, that is features with many unique values. &lt;a href=&quot;permutation_importance#permutation-importance&quot;&gt;Permutation feature importance&lt;/a&gt; is an alternative to impurity-based feature importance that does not suffer from these flaws. These two methods of obtaining feature importance are explored in: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="translated">트리 기반 모델에서 계산 된 불순물 기반 기능 중요도에는 잘못된 결론으로 ​​이어질 수있는 두 가지 결함이 있습니다. 먼저 훈련 데이터 세트에서 파생 된 통계를 기반으로 계산되므로 &lt;strong&gt;보류 된 데이터 세트에서 좋은 예측을 수행하는 데 가장 중요한 기능이 무엇인지 반드시 알려주지는 않습니다&lt;/strong&gt; . 둘째, 고유 한 값이 많은 기능인 &lt;strong&gt;높은 카디널리티 기능을 선호&lt;/strong&gt; 합니다. &lt;a href=&quot;permutation_importance#permutation-importance&quot;&gt;순열 기능 중요도&lt;/a&gt; 는 이러한 결함이없는 불순물 기반 기능 중요도의 대안입니다. 기능 중요도를 얻는이 두 가지 방법은 &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;순열 중요도와 MDI (Random Forest Feature Importance)&lt;/a&gt; 에서 살펴 봅니다.</target>
        </trans-unit>
        <trans-unit id="a85ca2be296db64c4eea5e4c85e7d3c8770ab473" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances.</source>
          <target state="translated">불순물 기반 기능의 중요성.</target>
        </trans-unit>
        <trans-unit id="f89a5a9d5e4b0a0db4c9a50975418d448408f475" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature if axis == 0.</source>
          <target state="translated">축 == 0 인 경우 각 피처의 대치 채우기 값입니다.</target>
        </trans-unit>
        <trans-unit id="52ad716a60ba342ef84206f96283ef6240a59825" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature.</source>
          <target state="translated">각 기능에 대한 대치 채우기 값입니다.</target>
        </trans-unit>
        <trans-unit id="66edd575acaaa453b1fb3cfd5f9ed2a63e5987ae" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature. Computing statistics can result in &lt;code&gt;np.nan&lt;/code&gt; values. During &lt;a href=&quot;#sklearn.impute.SimpleImputer.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt;, features corresponding to &lt;code&gt;np.nan&lt;/code&gt; statistics will be discarded.</source>
          <target state="translated">각 기능에 대한 대치 채우기 값입니다. 통계를 계산하면 &lt;code&gt;np.nan&lt;/code&gt; 값이 생성 될 수 있습니다 . &lt;a href=&quot;#sklearn.impute.SimpleImputer.transform&quot;&gt; &lt;code&gt;transform&lt;/code&gt; &lt;/a&gt; 중에 &lt;code&gt;np.nan&lt;/code&gt; 통계에 해당하는 기능 이 삭제됩니다.</target>
        </trans-unit>
        <trans-unit id="7cd5e57b66f65afed5b5b066bdba074e0cee3d73" translate="yes" xml:space="preserve">
          <source>The imputation strategy.</source>
          <target state="translated">대치 전략.</target>
        </trans-unit>
        <trans-unit id="d67cf9b9c87d5f16b287aedd5a057873a2e85c3c" translate="yes" xml:space="preserve">
          <source>The imputed dataset. &lt;code&gt;n_output_features&lt;/code&gt; is the number of features that is not always missing during &lt;code&gt;fit&lt;/code&gt;.</source>
          <target state="translated">대치 된 데이터 세트입니다. &lt;code&gt;n_output_features&lt;/code&gt; 는 &lt;code&gt;fit&lt;/code&gt; 중에 항상 누락되지 않는 기능의 수입니다 .</target>
        </trans-unit>
        <trans-unit id="fc59b667379a9f19464aefa0413c2d5b611edad6" translate="yes" xml:space="preserve">
          <source>The imputed input data.</source>
          <target state="translated">대치 된 입력 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="aa1b27a8286b1b98805decd6657b8fa02347921e" translate="yes" xml:space="preserve">
          <source>The index (of the &lt;code&gt;cv_results_&lt;/code&gt; arrays) which corresponds to the best candidate parameter setting.</source>
          <target state="translated">최상의 후보 매개 변수 설정에 해당하는 인덱스 ( &lt;code&gt;cv_results_&lt;/code&gt; 배열의)입니다.</target>
        </trans-unit>
        <trans-unit id="d8f1885dbc976f2ceb3f7711b10b324e6546b97b" translate="yes" xml:space="preserve">
          <source>The index is computed only quantities and features inherent to the dataset.</source>
          <target state="translated">인덱스는 데이터 세트 고유의 수량 및 기능 만 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="eaf4dd06369196819331db8b96a42731aed3d8e0" translate="yes" xml:space="preserve">
          <source>The index is defined as the average similarity between each cluster \(C_i\) for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of this index, similarity is defined as a measure \(R_{ij}\) that trades off:</source>
          <target state="translated">인덱스는 \ (i = 1, ..., k \)에 대한 각 클러스터 \ (C_i \)와 가장 유사한 하나의 \ (C_j \) 사이의 평균 유사성으로 정의됩니다. 이 지수의 맥락에서 유사성은 다음과 같은 트레이드 \ (R_ {ij} \)로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="7ad5522250591909d8f74ff707e1fa5837de3ae6" translate="yes" xml:space="preserve">
          <source>The index is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared):</source>
          <target state="translated">인덱스는 모든 군집에 대한 군집 간 분산과 군집 간 분산의 합의 비율입니다 (분산은 거리 제곱의 합으로 정의 됨).</target>
        </trans-unit>
        <trans-unit id="f4af6c67193225ac6c1c304797e9ea3402785083" translate="yes" xml:space="preserve">
          <source>The index of the cluster.</source>
          <target state="translated">클러스터의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="bf1ee02857437dc2bc3e741c8e3e56ef69d1ae10" translate="yes" xml:space="preserve">
          <source>The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.</source>
          <target state="translated">어휘에서 단어의 색인 값은 전체 훈련 모음에서 빈도와 연결됩니다.</target>
        </trans-unit>
        <trans-unit id="03c2241f87945c783a2d50da20a997d6e73ac147" translate="yes" xml:space="preserve">
          <source>The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don&amp;rsquo;t use this parameter unless you know what to do.</source>
          <target state="translated">정렬 된 훈련 입력 샘플의 인덱스. 동일한 데이터 세트에서 많은 트리가 성장하면 트리간에 순서를 캐시 할 수 있습니다. None이면 여기에서 데이터가 정렬됩니다. 수행 할 작업을 모르면이 매개 변수를 사용하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="0f615a1a241d635bbd7f8073a9962f88455d5840" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each column.</source>
          <target state="translated">각 열의 클러스터 멤버쉽 표시기.</target>
        </trans-unit>
        <trans-unit id="7c7c8e5dd3219610397a571246f91a523e9b0296" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each row.</source>
          <target state="translated">각 행의 클러스터 멤버쉽 표시기.</target>
        </trans-unit>
        <trans-unit id="1d09056572af0ab7d260b8a9a458cace43041c54" translate="yes" xml:space="preserve">
          <source>The inertia matrix uses a Heapq-based representation.</source>
          <target state="translated">관성 행렬은 힙 기반 표현을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="4f68a65921a2da6924b081b35cf28ef22fa8adaa" translate="yes" xml:space="preserve">
          <source>The inferred value of max_features.</source>
          <target state="translated">max_features의 유추 된 값입니다.</target>
        </trans-unit>
        <trans-unit id="9af964a5bdc6c7f6e3e23dbd4ccea9f871ad0d0c" translate="yes" xml:space="preserve">
          <source>The initial coefficients to warm-start the optimization.</source>
          <target state="translated">최적화를 웜 스타트하기위한 초기 계수입니다.</target>
        </trans-unit>
        <trans-unit id="e326d26a40d08d9ebf56e15d175f98f61ace6983" translate="yes" xml:space="preserve">
          <source>The initial guess for the covariance.</source>
          <target state="translated">공분산에 대한 초기 추측.</target>
        </trans-unit>
        <trans-unit id="a31ad0b6b83b24ae472799017e15e5984848ac41" translate="yes" xml:space="preserve">
          <source>The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)</source>
          <target state="translated">각 특징에 대한 잡음 분산의 초기 추측. None이면 기본값은 np.ones (n_features)입니다.</target>
        </trans-unit>
        <trans-unit id="5af1f38d3d578b440a3ef0c3e0d6f491e678143f" translate="yes" xml:space="preserve">
          <source>The initial intercept to warm-start the optimization.</source>
          <target state="translated">최적화를 웜 스타트하기위한 초기 인터셉트.</target>
        </trans-unit>
        <trans-unit id="ffb7aede167c9f36a232ef307d2df81471bb1b4f" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.0 as eta0 is not used by the default schedule &amp;lsquo;optimal&amp;rsquo;.</source>
          <target state="translated">'일정한', '진정한'또는 '적응적인'일정의 초기 학습 속도. 기본 일정 'optimal'에서 eta0을 사용하지 않으므로 기본값은 0.0입니다.</target>
        </trans-unit>
        <trans-unit id="3434635df97d9946f02b9082e79d95e6999ccc03" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.01.</source>
          <target state="translated">'constant', 'invscaling'또는 'Adaptive'일정에 대한 초기 학습률입니다. 기본값은 0.01입니다.</target>
        </trans-unit>
        <trans-unit id="f396af141edab02acfc3d8c05ea1f6e174dd0d5b" translate="yes" xml:space="preserve">
          <source>The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">사용 된 초기 학습 속도. 가중치를 업데이트 할 때 단계 크기를 제어합니다. solver = 'sgd'또는 'adam'인 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8aa315cba5e51992abb63a7c33e92703aa91a364" translate="yes" xml:space="preserve">
          <source>The initial model \(F_{0}\) is problem specific, for least-squares regression one usually chooses the mean of the target values.</source>
          <target state="translated">초기 모델 \ (F_ {0} \)은 문제에 따라 다르므로 최소 제곱 회귀 분석에서는 일반적으로 목표 값의 평균을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="fcbcdc2001232ebf2d1267dbd5c1db8c05bef33b" translate="yes" xml:space="preserve">
          <source>The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument. The passed object has to implement &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">초기 모델은 &lt;code&gt;init&lt;/code&gt; 인수 를 통해 지정할 수도 있습니다 . 전달 된 객체는 &lt;code&gt;fit&lt;/code&gt; 과 &lt;code&gt;predict&lt;/code&gt; 를 구현해야 합니다.</target>
        </trans-unit>
        <trans-unit id="2b0323f433eec5618cbc80c984794d0b5575bec5" translate="yes" xml:space="preserve">
          <source>The initial transformation will be a random array of shape &lt;code&gt;(n_components, n_features)&lt;/code&gt;. Each value is sampled from the standard normal distribution.</source>
          <target state="translated">초기 변환은 임의의 모양 배열 &lt;code&gt;(n_components, n_features)&lt;/code&gt; 입니다. 각 값은 표준 정규 분포에서 샘플링됩니다.</target>
        </trans-unit>
        <trans-unit id="17bd189eec62d21587058ccd88b444461d73119b" translate="yes" xml:space="preserve">
          <source>The initial values of the coefficients.</source>
          <target state="translated">계수의 초기 값</target>
        </trans-unit>
        <trans-unit id="32a92ee0ef2ffe7a34cba56e64f123bd9b2ca181" translate="yes" xml:space="preserve">
          <source>The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the first layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the weight matrix as a 28x28 pixel image.</source>
          <target state="translated">입력 데이터는 28x28 픽셀의 필기 숫자로 구성되어 데이터 세트에 784 개의 기능이 있습니다. 그러므로 제 1 레이어 가중치 행렬은 (784, hidden_layer_sizes [0]) 모양을 갖는다. 따라서 가중치 행렬의 단일 열을 28x28 픽셀 이미지로 시각화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c8ff70c87eb4edb0d6df0b02362dd0f826eb86c0" translate="yes" xml:space="preserve">
          <source>The input data matrix</source>
          <target state="translated">입력 데이터 매트릭스</target>
        </trans-unit>
        <trans-unit id="28402823f8037a445e34f883189d18c9c8586801" translate="yes" xml:space="preserve">
          <source>The input data to complete.</source>
          <target state="translated">완료 할 입력 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="10ea3f8f2392cebb3ea568adacc2ca87c3d24f21" translate="yes" xml:space="preserve">
          <source>The input data to project into a smaller dimensional space.</source>
          <target state="translated">더 작은 차원 공간으로 투사 할 입력 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="8b941f334dfce6c774093743936f754bf8137c6b" translate="yes" xml:space="preserve">
          <source>The input data.</source>
          <target state="translated">입력 데이터</target>
        </trans-unit>
        <trans-unit id="a383a11075a72cc4c498a6da492b12429e0d8bd0" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods:</source>
          <target state="translated">바둑판 패턴을보다 명확하게하기 위해 입력 행렬 \ (A \)가 먼저 정규화됩니다. 세 가지 가능한 방법이 있습니다.</target>
        </trans-unit>
        <trans-unit id="5296b71b9c59f1ea88d1300f56d80b11acdd4263" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is preprocessed as follows:</source>
          <target state="translated">입력 행렬 \ (A \)는 다음과 같이 전처리됩니다.</target>
        </trans-unit>
        <trans-unit id="1dfee05c320291671e1507b730cbc56e48fcc05b" translate="yes" xml:space="preserve">
          <source>The input samples with only the selected features.</source>
          <target state="translated">선택된 기능 만있는 입력 샘플.</target>
        </trans-unit>
        <trans-unit id="aa9c51d1052bb07178d99dc6b9998838f3fd04f4" translate="yes" xml:space="preserve">
          <source>The input samples.</source>
          <target state="translated">입력 샘플.</target>
        </trans-unit>
        <trans-unit id="691f5d8dd518149d763a76be65224ad6a47a3de2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">입력 샘플. 내부적으로 는 희소 행렬이 희소 &lt;code&gt;csr_matrix&lt;/code&gt; 에 제공되는 경우 &lt;code&gt;dtype=np.float32&lt;/code&gt; 로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="80a0f34908e0a755318c3eb528444068386165b2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">입력 샘플. 내부적으로 dtype은 &lt;code&gt;dtype=np.float32&lt;/code&gt; 로 변환됩니다 . 희소 행렬이 제공되면 희소 &lt;code&gt;csr_matrix&lt;/code&gt; 로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="ad241eeecc4b3d71885329d69a3ecbb931dd0903" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">입력 샘플. 내부적으로 dtype은 &lt;code&gt;dtype=np.float32&lt;/code&gt; 로 변환됩니다 . 희소 행렬이 제공되면 희소 &lt;code&gt;csr_matrix&lt;/code&gt; 로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="574b784828dbe4184d33a76077708bbde6ac5b78" translate="yes" xml:space="preserve">
          <source>The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.</source>
          <target state="translated">입력 샘플입니다. 희소 행렬은 CSC, CSR, COO, DOK 또는 LIL 일 수 있습니다. COO, DOK 및 LIL이 CSR로 전환됩니다.</target>
        </trans-unit>
        <trans-unit id="b99e327cb34f5c84fcbd4e91cfb496e38c8c302f" translate="yes" xml:space="preserve">
          <source>The input samples. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csc_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">입력 샘플. 효율성을 극대화 하려면 &lt;code&gt;dtype=np.float32&lt;/code&gt; 를 사용하십시오 . 희소 행렬도 지원됩니다 . 최대 효율성을 위해 희소 &lt;code&gt;csc_matrix&lt;/code&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="6e6a9dea87222450502841b10c214ed0343cd360" translate="yes" xml:space="preserve">
          <source>The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt;&lt;code&gt;make_low_rank_matrix&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">입력 세트는 잘 조절되거나 (기본적으로) 저지방 꼬리 단일 프로파일을 가질 수 있습니다. 자세한 내용은 &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt; &lt;code&gt;make_low_rank_matrix&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="289f5749e232fcbed1540450cf860647da777cb8" translate="yes" xml:space="preserve">
          <source>The input set is well conditioned, centered and gaussian with unit variance.</source>
          <target state="translated">입력 세트는 잘 조정되고 중앙에 있으며 단위 편차가있는 가우스입니다.</target>
        </trans-unit>
        <trans-unit id="9196dd161fc035e27746b1e485ce67d2bc4a2f97" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.</source>
          <target state="translated">이 변환기의 입력은 범주 형 (이산) 기능에 의해 취해진 값을 나타내는 정수 또는 문자열의 배열과 유사해야합니다. 피처는 서수로 변환됩니다. 이로 인해 기능 당 단일 열의 정수 (0-n_categories-1)가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="a912a205d28c73bb81095b58a51d5b9233f6732c" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the &lt;code&gt;sparse&lt;/code&gt; parameter)</source>
          <target state="translated">이 변환기에 대한 입력은 정수 또는 문자열의 배열과 유사해야하며 범주 형 (이산) 기능에서 사용하는 값을 나타냅니다. 기능은 원-핫 (일명 'one-of-K'또는 '더미') 인코딩 체계를 사용하여 인코딩됩니다. 이렇게하면 각 범주에 대한 이진 열이 생성되고 희소 행렬 또는 조밀 배열이 반환됩니다 ( &lt;code&gt;sparse&lt;/code&gt; 매개 변수 에 따라 다름 ).</target>
        </trans-unit>
        <trans-unit id="6fca358f78b22e3193c17b331b39a22e0cf2c917" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array.</source>
          <target state="translated">이 변환기의 입력은 범주 형 (이산) 기능에 의해 취해진 값을 나타내는 정수 또는 문자열의 배열과 유사해야합니다. 기능은 one-hot (일명 'K'또는 '더미') 인코딩 체계를 사용하여 인코딩됩니다. 그러면 각 범주에 대한 이진 열이 생성되고 희소 행렬 또는 밀도 배열이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="c9721cff229ea27eaddea830cb215d9ecb24c54d" translate="yes" xml:space="preserve">
          <source>The instance.</source>
          <target state="translated">인스턴스입니다.</target>
        </trans-unit>
        <trans-unit id="d6846270bbc73ec820141552eb32b90be4589180" translate="yes" xml:space="preserve">
          <source>The integer id or the string name metadata of the MLComp dataset to load</source>
          <target state="translated">로드 할 MLComp 데이터 세트의 정수 ID 또는 문자열 이름 메타 데이터</target>
        </trans-unit>
        <trans-unit id="0ff5072970d94a45dfd0b3dc59cc200193becdfd" translate="yes" xml:space="preserve">
          <source>The integer labels (0 or 1) for class membership of each sample.</source>
          <target state="translated">각 샘플의 클래스 멤버쉽에 대한 정수 레이블 (0 또는 1)입니다.</target>
        </trans-unit>
        <trans-unit id="a6afcce2561a80c34c914f95ed66620399eb0d7c" translate="yes" xml:space="preserve">
          <source>The integer labels for class membership of each sample.</source>
          <target state="translated">각 샘플의 클래스 멤버쉽에 대한 정수 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="868af1be557e4830c6f25b4ff3211b7a9aad7eb0" translate="yes" xml:space="preserve">
          <source>The integer labels for cluster membership of each sample.</source>
          <target state="translated">각 샘플의 클러스터 멤버쉽에 대한 정수 레이블.</target>
        </trans-unit>
        <trans-unit id="05d4e37f01ed47119c132248581df1196214fb7d" translate="yes" xml:space="preserve">
          <source>The integer labels for quantile membership of each sample.</source>
          <target state="translated">각 샘플의 Quantile 멤버쉽에 대한 정수 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="7da460eaa0239b5647e6b97a087bcede08e5a0c5" translate="yes" xml:space="preserve">
          <source>The intercept of the model. Only returned if &lt;code&gt;return_intercept&lt;/code&gt; is True and if X is a scipy sparse array.</source>
          <target state="translated">모델의 절편입니다. &lt;code&gt;return_intercept&lt;/code&gt; 가 True이고 X가 scipy 스파 스 배열 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="f0408bacce1626a183a8eadd09dc2d6f8b9e549c" translate="yes" xml:space="preserve">
          <source>The intercept term.</source>
          <target state="translated">절편 용어.</target>
        </trans-unit>
        <trans-unit id="39d99a92784fd6547680c69e2a029d63ab730943" translate="yes" xml:space="preserve">
          <source>The inverse document frequency (IDF) vector; only defined if &lt;code&gt;use_idf&lt;/code&gt; is True.</source>
          <target state="translated">역 문서 빈도 (IDF) 벡터; &lt;code&gt;use_idf&lt;/code&gt; 가 True 인 경우에만 정의됩니다 .</target>
        </trans-unit>
        <trans-unit id="70e42545b4f301fb133ffa67ef01dbd1c81a14a6" translate="yes" xml:space="preserve">
          <source>The inverse of the Box-Cox transformation is given by:</source>
          <target state="translated">Box-Cox 변환의 역수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="1296b0fc0246edf109c97645c02261b1d3d028bb" translate="yes" xml:space="preserve">
          <source>The inverse of the Yeo-Johnson transformation is given by:</source>
          <target state="translated">Yeo-Johnson 변환의 역수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="8621ba40f721d68ec6f12f67b5e3cc856e6fc6d9" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">아이리스 데이터 셋은 클래식하고 매우 쉬운 멀티 클래스 분류 데이터 셋입니다.</target>
        </trans-unit>
        <trans-unit id="82f3a3d98edcec3969a3e26fc0789c7c6c1f74ef" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:</source>
          <target state="translated">홍채 데이터 세트는 꽃잎과 꽃받침 길이 및 너비에서 3 가지 종류의 홍채 (Setosa, Versicolour 및 Virginica)를 식별하는 분류 작업입니다.</target>
        </trans-unit>
        <trans-unit id="968fa226a66bed28f98c84df2ac306ac992e59c2" translate="yes" xml:space="preserve">
          <source>The isotonic regression optimization problem is defined by:</source>
          <target state="translated">등장 성 회귀 최적화 문제는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="710a9703f51d61fb5f41b8c77a926ca070febd6c" translate="yes" xml:space="preserve">
          <source>The iteration will stop when &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; where pg_i is the i-th component of the projected gradient.</source>
          <target state="translated">반복이 중지됩니다 때 &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; 여기서 pg_i는 투영 된 기울기의 i 번째 성분입니다.</target>
        </trans-unit>
        <trans-unit id="d1f978f02193a9d7a16e2ac4f28051f1be20fa33" translate="yes" xml:space="preserve">
          <source>The iterator consumption and dispatching is protected by the same lock so calling this function should be thread safe.</source>
          <target state="translated">반복자 소비 및 디스패치는 동일한 잠금으로 보호되므로이 함수를 호출하면 스레드로부터 안전해야합니다.</target>
        </trans-unit>
        <trans-unit id="e6bcf49b626a9dedb0663face3bd765b3dedb378" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the bias vector corresponding to layer i + 1.</source>
          <target state="translated">목록의 i 번째 요소는 레이어 i + 1에 해당하는 바이어스 벡터를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="4b396b269a89e1a7151872f147950b58bd31d2c2" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the weight matrix corresponding to layer i.</source>
          <target state="translated">목록의 i 번째 요소는 레이어 i에 해당하는 가중치 행렬을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="8ce9197f7d2eba2389cfe44c6d9806383e84232a" translate="yes" xml:space="preserve">
          <source>The ith element represents the number of neurons in the ith hidden layer.</source>
          <target state="translated">i 번째 요소는 i 번째 숨겨진 레이어의 뉴런 수를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="110dde91ce5735296cfcdbdf4849eb2634f008ea" translate="yes" xml:space="preserve">
          <source>The justification for the formula in the loss=&amp;rdquo;modified_huber&amp;rdquo; case is in the appendix B in: &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&lt;/a&gt;</source>
          <target state="translated">loss =&amp;rdquo;modified_huber&amp;rdquo;사례의 공식에 대한 정당성은 부록 B ( &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf)에 있습니다.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="08f434f0a998f2afa3af3de6e921bb2935ec3174" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space.</source>
          <target state="translated">k- 평균 알고리즘은 \ (N \) 샘플 \ (X \) 세트를 \ (K \) 분리 된 클러스터 \ (C \)로 나눕니다. 클러스터. 이 수단은 일반적으로 클러스터 &quot;중심&quot;이라고합니다. 동일한 공간에 있지만 일반적으로 \ (X \)의 포인트는 아닙니다.</target>
        </trans-unit>
        <trans-unit id="2ad3a7b0acdb773b3a5a78e7f86e7401f887f5a4" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space. The K-means algorithm aims to choose centroids that minimise the &lt;em&gt;inertia&lt;/em&gt;, or within-cluster sum of squared criterion:</source>
          <target state="translated">k- 평균 알고리즘은 일련의 \ (N \) 샘플 \ (X \)를 \ (K \) 분리 된 클러스터 \ (C \)로 나눕니다. 클러스터. 이 수단을 일반적으로 클러스터 &quot;중심점&quot;이라고합니다. 비록 같은 공간에 살고 있지만 일반적으로 \ (X \)를 가리키는 것은 아닙니다. K- 평균 알고리즘은 &lt;em&gt;관성&lt;/em&gt; 또는 클러스터 내 제곱 기준의 합 을 최소화하는 중심을 선택하는 것을 목표로합니다 .</target>
        </trans-unit>
        <trans-unit id="3042a74be3a11ce6ced2d21d393c8c55a0b044ff" translate="yes" xml:space="preserve">
          <source>The k-means problem is solved using either Lloyd&amp;rsquo;s or Elkan&amp;rsquo;s algorithm.</source>
          <target state="translated">k- 평균 문제는 Lloyd 또는 Elkan 알고리즘을 사용하여 해결됩니다.</target>
        </trans-unit>
        <trans-unit id="698fe2144ac8be0b801483c0a1d13dc4d07c73ce" translate="yes" xml:space="preserve">
          <source>The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).</source>
          <target state="translated">카파 점수 (docstring 참조)는 -1과 1 사이의 숫자입니다. 0.8보다 높은 점수는 일반적으로 좋은 일치로 간주됩니다. 0 이하는 동의가 없음을 의미합니다 (실제로 임의의 레이블).</target>
        </trans-unit>
        <trans-unit id="951d69c60ea24190bd0c5a4dcae9aeb94c1c913d" translate="yes" xml:space="preserve">
          <source>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</source>
          <target state="translated">카파 통계량 -1과 1 사이의 숫자입니다. 최대 값은 완전한 동의를 의미합니다. 0 이하는 기회 동의를 의미합니다.</target>
        </trans-unit>
        <trans-unit id="e7d87035112e23c601a476b538c50df786049966" translate="yes" xml:space="preserve">
          <source>The kernel density estimator can be used with any of the valid distance metrics (see &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt;&lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt;&lt;/a&gt; for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine distance&lt;/a&gt; which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:</source>
          <target state="translated">커널 밀도 추정기는 유효한 거리 메트릭 ( 사용 가능한 메트릭 목록 은 &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt; &lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt; &lt;/a&gt; 참조 )과 함께 사용할 수 있지만 결과는 유클리드 메트릭에 대해서만 올바르게 정규화됩니다. 특히 유용한 지표 중 하나 는 구의 점 사이의 각도 거리를 측정 하는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine 거리&lt;/a&gt; 입니다. 다음은 지리 공간 데이터의 시각화를 위해 커널 밀도 추정값을 사용하는 예입니다.이 경우 남미 대륙에서 서로 다른 두 종의 관측치 분포가 있습니다.</target>
        </trans-unit>
        <trans-unit id="f5acb5ee70e93822d6214e7faa0671c1edbb5e04" translate="yes" xml:space="preserve">
          <source>The kernel is composed of several terms that are responsible for explaining different properties of the signal:</source>
          <target state="translated">커널은 신호의 다른 속성을 설명하는 여러 용어로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="075d7f089a3306b950fac8e515a19a5954a7251d" translate="yes" xml:space="preserve">
          <source>The kernel is given by:</source>
          <target state="translated">커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="b2a264496619ccec6a15428322dff95c6847afef" translate="yes" xml:space="preserve">
          <source>The kernel specifying the covariance function of the GP. If None is passed, the kernel &amp;ldquo;1.0 * RBF(1.0)&amp;rdquo; is used as default. Note that the kernel&amp;rsquo;s hyperparameters are optimized during fitting.</source>
          <target state="translated">GP의 공분산 함수를 지정하는 커널. None이 전달되면 커널&amp;ldquo;1.0 * RBF (1.0)&amp;rdquo;이 기본값으로 사용됩니다. 커널의 하이퍼 파라미터는 피팅하는 동안 최적화됩니다.</target>
        </trans-unit>
        <trans-unit id="3f4b36434e95db38ce416f2948e370303e8d7ce4" translate="yes" xml:space="preserve">
          <source>The kernel to use. Valid kernels are [&amp;lsquo;gaussian&amp;rsquo;|&amp;rsquo;tophat&amp;rsquo;|&amp;rsquo;epanechnikov&amp;rsquo;|&amp;rsquo;exponential&amp;rsquo;|&amp;rsquo;linear&amp;rsquo;|&amp;rsquo;cosine&amp;rsquo;] Default is &amp;lsquo;gaussian&amp;rsquo;.</source>
          <target state="translated">사용할 커널. 유효한 커널은 [ 'gaussian'| 'tophat'| 'epanechnikov'| 'exponential'| 'linear'| 'cosine']입니다. 기본값은 'gaussian'입니다.</target>
        </trans-unit>
        <trans-unit id="7d7a3328bfc6be5d4c52eb80a02db6810f6af97a" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers.</source>
          <target state="translated">예측에 사용 된 커널. 이진 분류의 경우 커널 구조는 매개 변수로 전달 된 것과 동일하지만 최적화 된 하이퍼 파라미터가 있습니다. 멀티 클래스 분류의 경우 one-restus-rest 분류기에서 사용되는 다른 커널로 구성된 CompoundKernel이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="5a1810e4f4370ea9d4b3eb4b69c5661264f9680f" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters</source>
          <target state="translated">예측에 사용 된 커널. 커널의 구조는 매개 변수로 전달 된 것과 동일하지만 최적화 된 하이퍼 파라미터가 있습니다</target>
        </trans-unit>
        <trans-unit id="317ceb705f764af792a5c1b02b0c75da4f7767d2" translate="yes" xml:space="preserve">
          <source>The key &lt;code&gt;'params'&lt;/code&gt; is used to store a list of parameter settings dicts for all the parameter candidates.</source>
          <target state="translated">&lt;code&gt;'params'&lt;/code&gt; 키 는 모든 매개 변수 후보에 대한 매개 변수 설정 목록을 저장하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="467dfd2cbee563f1dc16d74e0763e99176db4d68" translate="yes" xml:space="preserve">
          <source>The l1-penalized estimator can recover part of this off-diagonal structure. It learns a sparse precision. It is not able to recover the exact sparsity pattern: it detects too many non-zero coefficients. However, the highest non-zero coefficients of the l1 estimated correspond to the non-zero coefficients in the ground truth. Finally, the coefficients of the l1 precision estimate are biased toward zero: because of the penalty, they are all smaller than the corresponding ground truth value, as can be seen on the figure.</source>
          <target state="translated">l1 처벌 추정기는이 비 대각선 구조의 일부를 복구 할 수 있습니다. 희소 정밀도를 배웁니다. 정확한 희소성 패턴을 복구 할 수 없습니다. 너무 많은 0이 아닌 계수를 감지합니다. 그러나, 추정 된 l1의 최고 비제로 계수는 지상 진실의 비제로 계수에 대응한다. 마지막으로, l1 정밀 추정의 계수는 0으로 편향됩니다. 페널티로 인해 그림에서 볼 수 있듯이 모두 해당지면 정확도 값보다 작습니다.</target>
        </trans-unit>
        <trans-unit id="3f115dd7f6ab226f3d1efb2261d982c44eb021f0" translate="yes" xml:space="preserve">
          <source>The label of the positive class</source>
          <target state="translated">포지티브 클래스의 레이블</target>
        </trans-unit>
        <trans-unit id="1daeeb6b0700e2caf583964dcec09c381eeb8ea9" translate="yes" xml:space="preserve">
          <source>The label of the positive class. Only applied to binary &lt;code&gt;y_true&lt;/code&gt;. For multilabel-indicator &lt;code&gt;y_true&lt;/code&gt;, &lt;code&gt;pos_label&lt;/code&gt; is fixed to 1.</source>
          <target state="translated">포지티브 클래스의 레이블입니다. 이진 &lt;code&gt;y_true&lt;/code&gt; 에만 적용됩니다 . 다중 레이블 표시기 &lt;code&gt;y_true&lt;/code&gt; 의 경우 &lt;code&gt;pos_label&lt;/code&gt; 은 1로 고정됩니다.</target>
        </trans-unit>
        <trans-unit id="e8fa4295eafd0b055339770364309144cc85ebe4" translate="yes" xml:space="preserve">
          <source>The label of the positive class. When &lt;code&gt;pos_label=None&lt;/code&gt;, if y_true is in {-1, 1} or {0, 1}, &lt;code&gt;pos_label&lt;/code&gt; is set to 1, otherwise an error will be raised.</source>
          <target state="translated">포지티브 클래스의 레이블입니다. &lt;code&gt;pos_label=None&lt;/code&gt; 일 때 y_true 가 {-1, 1} 또는 {0, 1}에 &lt;code&gt;pos_label&lt;/code&gt; 이 1로 설정되고 그렇지 않으면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="844ad03ed5a5991044f5c2a9015a7d46ffc83176" translate="yes" xml:space="preserve">
          <source>The label sets.</source>
          <target state="translated">라벨이 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="4e35273308f7d89cd0147256c1b5ea034960bf40" translate="yes" xml:space="preserve">
          <source>The labels assigned to samples. Points which are not included in any cluster are labeled as -1.</source>
          <target state="translated">샘플에 할당 된 레이블입니다. 클러스터에 포함되지 않은 포인트는 -1로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="34e6595f06fcaf1ac9d996e42fb91cf1dc35ee40" translate="yes" xml:space="preserve">
          <source>The labels of the clusters.</source>
          <target state="translated">클러스터의 레이블.</target>
        </trans-unit>
        <trans-unit id="f920ebdbb736d4fac6e624f1f0a24fae7f686c27" translate="yes" xml:space="preserve">
          <source>The laplacian kernel is defined as:</source>
          <target state="translated">라플라시안 커널은 다음과 같이 정의됩니다 :</target>
        </trans-unit>
        <trans-unit id="06271ccbc4593ebfa05907a5273f644dc124ef42" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the coefficient vector.</source>
          <target state="translated">따라서 올가미 추정은 \ (\ alpha || w || _1 \)이 추가 된 최소 제곱 페널티의 최소화를 해결합니다. 여기서 \ (\ alpha \)는 상수이고 \ (|| w || _1 \)은 다음과 같습니다. 계수 벡터의 \ (\ ell_1 \)-노름.</target>
        </trans-unit>
        <trans-unit id="321fd6d2804d3c8ea8df389dda895240e812e24c" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the parameter vector.</source>
          <target state="translated">따라서 올가미 추정은 \ (\ alpha || w || _1 \)가 추가 된 최소 제곱 페널티의 최소화를 해결합니다. 여기서 \ (\ alpha \)는 상수이고 \ (|| w || _1 \)는 모수 벡터의 \ (\ ell_1 \)-norm</target>
        </trans-unit>
        <trans-unit id="374010cdf4f6f26c62c956d36c36442b1f12b646" translate="yes" xml:space="preserve">
          <source>The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.</source>
          <target state="translated">마지막 특징은 Perceptron이 힌지 손실로 SGD보다 약간 훈련 속도가 빠르며 결과 모델이 희소하다는 것을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="6312a138a2adc5ee223c24bc72b6c9e5099fd7ea" translate="yes" xml:space="preserve">
          <source>The last dataset is an example of a &amp;lsquo;null&amp;rsquo; situation for clustering: the data is homogeneous, and there is no good clustering. For this example, the null dataset uses the same parameters as the dataset in the row above it, which represents a mismatch in the parameter values and the data structure.</source>
          <target state="translated">마지막 데이터 세트는 클러스터링에 대한 '널 (null)'상황의 예입니다. 데이터가 균등하며 클러스터링이 양호하지 않습니다. 이 예에서 null 데이터 집합은 위의 행에있는 데이터 집합과 동일한 매개 변수를 사용합니다. 이는 매개 변수 값과 데이터 구조의 불일치를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="2cc9bc385685f028c8287b17d0d958b5050b56a0" translate="yes" xml:space="preserve">
          <source>The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.</source>
          <target state="translated">마지막 정밀도 및 리콜 값은 각각 1과 0이며 해당 임계 값이 없습니다. 그래야 y 축에서 그래프가 시작됩니다.</target>
        </trans-unit>
        <trans-unit id="9ddff80583e9f2c8a48f3b64e4a9a06a8a8cefcd" translate="yes" xml:space="preserve">
          <source>The last two panels show how we can sample from the last two models. The resulting samples distributions do not look exactly like the original data distribution. The difference primarily stems from the approximation error we made by using a model that assumes that the data was generated by a finite number of Gaussian components instead of a continuous noisy sine curve.</source>
          <target state="translated">마지막 두 패널은 마지막 두 모델에서 샘플링하는 방법을 보여줍니다. 결과 표본 분포는 원래 데이터 분포와 정확히 같지 않습니다. 그 차이는 주로 연속적인 잡음이있는 사인 곡선 대신 유한 한 수의 가우스 성분에 의해 생성되었다고 가정하는 모델을 사용하여 만든 근사 오차에서 비롯됩니다.</target>
        </trans-unit>
        <trans-unit id="a4514c8e853c44e1e4a78bf33662f9b32277ba44" translate="yes" xml:space="preserve">
          <source>The latent variables of X.</source>
          <target state="translated">X의 잠재 변수</target>
        </trans-unit>
        <trans-unit id="03a214378b3ef2e13a3cc6617d05b3fe221146c8" translate="yes" xml:space="preserve">
          <source>The learning rate \(\eta\) can be either constant or gradually decaying. For classification, the default learning rate schedule (&lt;code&gt;learning_rate='optimal'&lt;/code&gt;) is given by</source>
          <target state="translated">학습 속도 \ (\ eta \)는 일정하거나 점진적으로 감소 할 수 있습니다. 분류의 경우 기본 학습 속도 일정 ( &lt;code&gt;learning_rate='optimal'&lt;/code&gt; )은</target>
        </trans-unit>
        <trans-unit id="36c5b5659a911c2db600cd9fbf9c0485b5053e95" translate="yes" xml:space="preserve">
          <source>The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a &amp;lsquo;ball&amp;rsquo; with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.</source>
          <target state="translated">t-SNE의 학습 속도는 일반적으로 [10.0, 1000.0] 범위입니다. 학습률이 너무 높으면 데이터가 가장 가까운 이웃과 거의 같은 거리에있는 '볼'처럼 보일 수 있습니다. 학습 속도가 너무 낮 으면 대부분의 점수가 특이 치가 적은 짙은 구름에서 압축 된 것처럼 보일 수 있습니다. 비용 함수가 로컬 최소값에 못 미치는 경우 학습률을 높이는 것이 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="55a4138aa9f2827acbf6f24a7d7d78c5c9231271" translate="yes" xml:space="preserve">
          <source>The learning rate for weight updates. It is &lt;em&gt;highly&lt;/em&gt; recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.</source>
          <target state="translated">체중 업데이트에 대한 학습률. 그것은되어 &lt;em&gt;높은&lt;/em&gt; 이 하이퍼 매개 변수를 조정하는 것이 좋습니다. 합리적인 값은 10 ** [0., -3.] 범위에 있습니다.</target>
        </trans-unit>
        <trans-unit id="aa4707838d034cc31029160760df726f17933ef5" translate="yes" xml:space="preserve">
          <source>The learning rate schedule:</source>
          <target state="translated">학습 속도 일정 :</target>
        </trans-unit>
        <trans-unit id="3497cec934b609e53594ddceecdfac3a47086873" translate="yes" xml:space="preserve">
          <source>The learning rate, also known as &lt;em&gt;shrinkage&lt;/em&gt;. This is used as a multiplicative factor for the leaves values. Use &lt;code&gt;1&lt;/code&gt; for no shrinkage.</source>
          <target state="translated">&lt;em&gt;축소&lt;/em&gt; 라고도하는 학습률 입니다. 이것은 잎 값에 대한 곱셈 요소로 사용됩니다. 수축이없는 경우 &lt;code&gt;1&lt;/code&gt; 을 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="0f65b2c5f287b4c02eec9ec5be4e57eb68fdc4c9" translate="yes" xml:space="preserve">
          <source>The least squares loss (along with the implicit use of the identity link function) of the Ridge regression model seems to cause this model to be badly calibrated. In particular, it tends to underestimate the risk and can even predict invalid negative frequencies.</source>
          <target state="translated">Ridge 회귀 모델의 최소 제곱 손실 (Identity Link 함수의 암시 적 사용과 함께)으로 인해이 모델이 잘못 보정되는 것으로 보입니다. 특히 위험을 과소 평가하는 경향이 있으며 유효하지 않은 음의 빈도를 예측할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="ed4ad540aa3d79251b7dbdba80cd56b31d18c164" translate="yes" xml:space="preserve">
          <source>The least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; this method has a cost of \(O(n_{\text{samples}} n_{\text{features}}^2)\), assuming that \(n_{\text{samples}} \geq n_{\text{features}}\).</source>
          <target state="translated">최소 제곱 솔루션은 X의 특이 값 분해를 사용하여 계산됩니다. X가 모양의 행렬 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; 경우이 방법의 비용은 \ (O (n _ {\ text {samples}} n _ {\ text {features)입니다. }} ^ 2) \), \ (n _ {\ text {samples}} \ geq n _ {\ text {features}} \)라고 가정합니다.</target>
        </trans-unit>
        <trans-unit id="866660dd294846977c41d9eb05a5f823762d12dd" translate="yes" xml:space="preserve">
          <source>The left and right examples highlight the &lt;code&gt;n_labels&lt;/code&gt; parameter: more of the samples in the right plot have 2 or 3 labels.</source>
          <target state="translated">왼쪽과 오른쪽의 예는 &lt;code&gt;n_labels&lt;/code&gt; 매개 변수를 강조 표시합니다 . 오른쪽 그림의 많은 샘플에는 2 개 또는 3 개의 레이블이 있습니다.</target>
        </trans-unit>
        <trans-unit id="3656a1b3513126216409c003f2c19b4a1bde366e" translate="yes" xml:space="preserve">
          <source>The leftmost layer, known as the input layer, consists of a set of neurons \(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.</source>
          <target state="translated">입력 레이어라고하는 가장 왼쪽 레이어는 입력 기능을 나타내는 뉴런 \ (\ {x_i | x_1, x_2, ..., x_m \} \)으로 구성됩니다. 숨겨진 레이어의 각 뉴런은 가중 선형 합계 \ (w_1x_1 + w_2x_2 + ... + w_mx_m \)와 비선형 활성화 함수 \ (g (\ cdot) : R \로 이전 레이어의 값을 변환합니다. 오른쪽 화살표 R \)-쌍곡선 tan 함수와 유사합니다. 출력 레이어는 마지막 숨겨진 레이어에서 값을 받아 출력 값으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="7b2e6a226728025cdbf60737ae02746b4f5067e4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel.</source>
          <target state="translated">커널의 길이 스케일.</target>
        </trans-unit>
        <trans-unit id="658641ffb44e40c4155905b8a662123f9fbe36a4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension.</source>
          <target state="translated">커널의 길이 스케일. 플로트 인 경우 등방성 커널이 사용됩니다. 어레이 인 경우, 이방성 커널이 사용되며, 여기서 각각의 치수 l은 각각의 피쳐 치수의 길이 스케일을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="c8dac18109c7577d20154429eb0e5e79a703a997" translate="yes" xml:space="preserve">
          <source>The likelihood of the data set with &lt;code&gt;self.covariance_&lt;/code&gt; as an estimator of its covariance matrix.</source>
          <target state="translated">공분산 행렬의 추정값 으로 &lt;code&gt;self.covariance_&lt;/code&gt; 를 사용한 데이터 세트의 가능성 .</target>
        </trans-unit>
        <trans-unit id="9d0f580fc795d85a96fff8300e9ac1a9bc3bbb0b" translate="yes" xml:space="preserve">
          <source>The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.</source>
          <target state="translated">다항식 피처에 대해 훈련 된 선형 모델은 입력 다항식 계수를 정확하게 복구 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9cf1fb22576b6c52a81ed3fb3cd57a1b7c534cc0" translate="yes" xml:space="preserve">
          <source>The linear models &lt;code&gt;LinearSVC()&lt;/code&gt; and &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; yield slightly different decision boundaries. This can be a consequence of the following differences:</source>
          <target state="translated">선형 모델 &lt;code&gt;LinearSVC()&lt;/code&gt; 와 &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; 약간 다른 결정 경계를 생성합니다. 이는 다음과 같은 차이점의 결과 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e79cf5350a0d3a4c8abb9ba5736a33d5a0c53fd5" translate="yes" xml:space="preserve">
          <source>The linear models assume no interactions between the input variables which likely causes under-fitting. Inserting a polynomial feature extractor (&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt;) indeed increases their discrimative power by 2 points of Gini index. In particular it improves the ability of the models to identify the top 5% riskiest profiles.</source>
          <target state="translated">선형 모델은 과소 적합을 유발할 가능성이있는 입력 변수 간의 상호 작용이 없다고 가정합니다. 다항식 특징 추출기 ( &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; &lt;/a&gt; )를 삽입하면 실제로 2 점의 Gini 인덱스만큼 디스크 리머 티브 파워가 증가합니다. 특히 상위 5 % 가장 위험한 프로필을 식별하는 모델의 능력을 향상시킵니다.</target>
        </trans-unit>
        <trans-unit id="086fe9ce38700dda3eefe00cc625a2fbfd905812" translate="yes" xml:space="preserve">
          <source>The linear operator to apply to the data to get the independent sources. This is equal to the unmixing matrix when &lt;code&gt;whiten&lt;/code&gt; is False, and equal to &lt;code&gt;np.dot(unmixing_matrix, self.whitening_)&lt;/code&gt; when &lt;code&gt;whiten&lt;/code&gt; is True.</source>
          <target state="translated">독립 소스를 얻기 위해 데이터에 적용 할 선형 연산자입니다. 이 경우는 unmixing 매트릭스 같다 &lt;code&gt;whiten&lt;/code&gt; 거짓이고, 동일 &lt;code&gt;np.dot(unmixing_matrix, self.whitening_)&lt;/code&gt; 때 &lt;code&gt;whiten&lt;/code&gt; 참이다.</target>
        </trans-unit>
        <trans-unit id="a3db33e367d642c4ed744d98677d2b41b0f8967e" translate="yes" xml:space="preserve">
          <source>The linear transformation learned during fitting.</source>
          <target state="translated">피팅하는 동안 학습 된 선형 변환입니다.</target>
        </trans-unit>
        <trans-unit id="c3af4059661dbcc46e497f8eb762a99fb7562b4b" translate="yes" xml:space="preserve">
          <source>The link function is determined by the &lt;code&gt;link&lt;/code&gt; parameter.</source>
          <target state="translated">링크 기능은 &lt;code&gt;link&lt;/code&gt; 매개 변수에 의해 결정됩니다 .</target>
        </trans-unit>
        <trans-unit id="7828a03a3850299c9821c4410a12edc3f4715fc7" translate="yes" xml:space="preserve">
          <source>The link function of the GLM, i.e. mapping from linear predictor &lt;code&gt;X @ coeff + intercept&lt;/code&gt; to prediction &lt;code&gt;y_pred&lt;/code&gt;. Option &amp;lsquo;auto&amp;rsquo; sets the link depending on the chosen family as follows:</source>
          <target state="translated">GLM의 연결 함수, 즉 선형 예측 자 &lt;code&gt;X @ coeff + intercept&lt;/code&gt; 에서 예측 &lt;code&gt;y_pred&lt;/code&gt; 로 매핑 합니다. 'auto'옵션은 선택한 제품군에 따라 다음과 같이 링크를 설정합니다.</target>
        </trans-unit>
        <trans-unit id="df16104005b0ad36b12ecec77e07a777907b9657" translate="yes" xml:space="preserve">
          <source>The linkage distance threshold above which, clusters will not be merged. If not &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;n_clusters&lt;/code&gt; must be &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;compute_full_tree&lt;/code&gt; must be &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">클러스터가 병합되지 않는 연결 거리 임계 값입니다. 그렇지 않은 경우 &lt;code&gt;None&lt;/code&gt; , &lt;code&gt;n_clusters&lt;/code&gt; 이 없어야합니다 &lt;code&gt;None&lt;/code&gt; 과 &lt;code&gt;compute_full_tree&lt;/code&gt; 은 해야 &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e68f70b09864b10f1aea37e78800b7b9e97f9614" translate="yes" xml:space="preserve">
          <source>The list of Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. A value of 0 is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while 1 is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">&lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; 인 Elastic-Net 혼합 매개 변수의 목록입니다 . &lt;code&gt;penalty='elasticnet'&lt;/code&gt; 경우에만 사용됩니다 . 값이 0 인 경우에는 &lt;code&gt;penalty='l2'&lt;/code&gt; 를 사용하는 것과 같고, 1 인 경우에는 &lt;code&gt;penalty='l1'&lt;/code&gt; 을 사용하는 것과 같습니다 . 들면 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt; 패널티는 L1과 L2의 조합이다.</target>
        </trans-unit>
        <trans-unit id="7b28aee2c83cc27005872232d54b931e2f0eba84" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each cross-validation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">보정 된 분류기 목록 (각 교차 검증 접기에 대해 하나씩). 검증 접기를 제외한 모든 접기에 적합하고 검증 접기에서 보정되었습니다.</target>
        </trans-unit>
        <trans-unit id="02b9c4dcf92e3aac8d7274c2bb8e453c5501153c" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each crossvalidation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">교정 교차점마다 하나씩 교정 분류기의 목록으로, 검증 영역을 제외한 모든 영역에 적합하고 검증 영역에서 교정됩니다.</target>
        </trans-unit>
        <trans-unit id="89a2f4c0656f755e155226e23cfb6b592b80d152" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end,
-start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after such nested smaller clusters. Since &lt;code&gt;labels&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(clusters) &amp;gt;
np.unique(labels)&lt;/code&gt;.</source>
          <target state="translated">모든 인덱스가 포함 된 각 행 의 &lt;code&gt;[start, end]&lt;/code&gt; 형식의 클러스터 목록입니다 . 클러스터는 &lt;code&gt;(end, -start)&lt;/code&gt; (오름차순)에 따라 정렬되므로 더 작은 클러스터를 포함하는 더 큰 클러스터는 이러한 중첩 된 더 작은 클러스터 다음에옵니다. 이후 &lt;code&gt;labels&lt;/code&gt; 계층, 일반적으로 반영하지 않습니다 &lt;code&gt;len(clusters) &amp;gt; np.unique(labels)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4b68d75689eb0ac219c6fca1ed0b8741a6d57de9" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end, -start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after those smaller ones. Since &lt;code&gt;labels_&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(cluster_hierarchy_) &amp;gt; np.unique(optics.labels_)&lt;/code&gt;. Please also note that these indices are of the &lt;code&gt;ordering_&lt;/code&gt;, i.e. &lt;code&gt;X[ordering_][start:end + 1]&lt;/code&gt; form a cluster. Only available when &lt;code&gt;cluster_method='xi'&lt;/code&gt;.</source>
          <target state="translated">모든 인덱스가 포함 된 각 행 의 &lt;code&gt;[start, end]&lt;/code&gt; 형식의 클러스터 목록입니다 . 클러스터는 &lt;code&gt;(end, -start)&lt;/code&gt; (오름차순)에 따라 정렬되므로 더 작은 클러스터를 포함하는 더 큰 클러스터가 더 작은 클러스터 다음에옵니다. 이후 &lt;code&gt;labels_&lt;/code&gt; 계층, 일반적으로 반영하지 않습니다 &lt;code&gt;len(cluster_hierarchy_) &amp;gt; np.unique(optics.labels_)&lt;/code&gt; . 또한 이러한 인덱스는 &lt;code&gt;ordering_&lt;/code&gt; 입니다 . 즉, &lt;code&gt;X[ordering_][start:end + 1]&lt;/code&gt; 이 클러스터를 형성합니다. &lt;code&gt;cluster_method='xi'&lt;/code&gt; 인 경우에만 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="662dd2cdb21b969ce9a44b42150a945acbfed523" translate="yes" xml:space="preserve">
          <source>The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.</source>
          <target state="translated">목적 함수의 값 목록과 각 반복에서 이중 간격입니다. return_costs가 True 인 경우에만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="177835998b74cd6b15a39a13b3bf0448af9a1ccf" translate="yes" xml:space="preserve">
          <source>The local outlier factor (LOF) of a sample captures its supposed &amp;lsquo;degree of abnormality&amp;rsquo;. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.</source>
          <target state="translated">표본의 국소 이상치 (LOF)는 추정 된 '이상도'를 포착합니다. 이는 샘플과 k- 최근 접 이웃의 로컬 도달 가능성 밀도의 비율의 평균입니다.</target>
        </trans-unit>
        <trans-unit id="f5a19f67242aeecdaccb15b2c01bb7b8a32da9cd" translate="yes" xml:space="preserve">
          <source>The log likelihood at each iteration.</source>
          <target state="translated">각 반복에서 로그 가능성.</target>
        </trans-unit>
        <trans-unit id="d6423c4071c8619e84a84f9bcba64b070be431d3" translate="yes" xml:space="preserve">
          <source>The log-marginal-likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;self.kernel_.theta&lt;/code&gt; 의 로그 한계 가능성</target>
        </trans-unit>
        <trans-unit id="28c02970f0172adf1161a050ed9ce282eb08888a" translate="yes" xml:space="preserve">
          <source>The log-posterior of LDA can also be written &lt;a href=&quot;#id7&quot; id=&quot;id1&quot;&gt;3&lt;/a&gt; as:</source>
          <target state="translated">LDA의 log-posterior는 다음과 같이 &lt;a href=&quot;#id7&quot; id=&quot;id1&quot;&gt;3&lt;/a&gt; 으로 쓸 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b9f3852ad5acaf7d90c57ecdaa290bd2f37a1610" translate="yes" xml:space="preserve">
          <source>The log-transformed bounds on the kernel&amp;rsquo;s hyperparameters theta</source>
          <target state="translated">커널 하이퍼 파라미터 세타의 로그 변환 된 경계</target>
        </trans-unit>
        <trans-unit id="997c53b0e19e31f9cbe762633463d7411a04ec12" translate="yes" xml:space="preserve">
          <source>The logarithm used is the natural logarithm (base-e).</source>
          <target state="translated">사용 된 로그는 자연 로그입니다 (base-e).</target>
        </trans-unit>
        <trans-unit id="b4b4750e8021650b4da5de07fd1cb495068051c3" translate="yes" xml:space="preserve">
          <source>The logistic regression with One-Vs-Rest is not a multiclass classifier out of the box. As a result it has more trouble in separating class 2 and 3 than the other estimators.</source>
          <target state="translated">One-Vs-Rest를 사용한 로지스틱 회귀는 기본적으로 멀티 클래스 분류 기가 아닙니다. 결과적으로 다른 추정기보다 클래스 2와 3을 분리하는 데 더 많은 어려움이 있습니다.</target>
        </trans-unit>
        <trans-unit id="658bb63624739c29e98c4d11ac78e5bf2f1fae2c" translate="yes" xml:space="preserve">
          <source>The loss function that &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; minimizes is given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; 가&lt;/a&gt; 최소화 하는 손실 함수 는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="47dc1dfa051244f9df37eb81c616d6773441e3ed" translate="yes" xml:space="preserve">
          <source>The loss function to be used. Defaults to &amp;lsquo;hinge&amp;rsquo;, which gives a linear SVM.</source>
          <target state="translated">사용할 손실 기능. 기본값은 'hinge'이며 선형 SVM을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="ced89f4970549dd56e97e03e89e06fddefc0d81d" translate="yes" xml:space="preserve">
          <source>The loss function to be used. The possible values are &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;</source>
          <target state="translated">사용할 손실 기능. 가능한 값은 'squared_loss', 'huber', 'epsilon_insensitive'또는 'squared_epsilon_insensitive'입니다.</target>
        </trans-unit>
        <trans-unit id="dcd8a12c459702396e7eec22715d06aed2aee153" translate="yes" xml:space="preserve">
          <source>The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper.</source>
          <target state="translated">사용되는 손실 함수 : epsilon_insensitive : 참조 용지의 PA-I와 동일합니다. squared_epsilon_insensitive : 참조 용지의 PA-II와 동일합니다.</target>
        </trans-unit>
        <trans-unit id="4c1d6cfb55920bc5e055d28ac3a6b6a90335ca56" translate="yes" xml:space="preserve">
          <source>The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper.</source>
          <target state="translated">사용되는 손실 기능 : 힌지 : 참조 용지의 PA-I와 동일합니다. squared_hinge : 참조 용지의 PA-II와 동일합니다.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
