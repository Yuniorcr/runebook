<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="20a9e2ae79f377ec69f0ec221a6bcb99fe892698" translate="yes" xml:space="preserve">
          <source>Inputs: fitted predictive model \(m\), tabular dataset (training or validation) \(D\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fffa8f8e3b740ecfc583b9bf477ffcbdb298b533" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest.</source>
          <target state="translated">Inserta nuevos datos en el ya instalado Bosque LSH.</target>
        </trans-unit>
        <trans-unit id="e78cacac23222d74508b7d4b79fbb8a5cb79c6fc" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest. Cost is proportional to new total size, so additions should be batched.</source>
          <target state="translated">Inserta nuevos datos en el ya instalado Bosque LSH.El costo es proporcional al nuevo tamaño total,por lo que las adiciones deben ser agrupadas.</target>
        </trans-unit>
        <trans-unit id="aa15440a6446ecaf7603f9c0287316507f4328a1" translate="yes" xml:space="preserve">
          <source>Inspecting coefficients across the folds of a cross-validation loop gives an idea of their stability.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3ce4618efaae8bf391d0768eaf2c4b834adfb9b" translate="yes" xml:space="preserve">
          <source>Inspection</source>
          <target state="translated">Inspection</target>
        </trans-unit>
        <trans-unit id="4c0fbc7b0ca330086776985f409e7f037b2f9494" translate="yes" xml:space="preserve">
          <source>Instance of the estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58768f013d8600aed4da42a9f67c30c0b0e7f2be" translate="yes" xml:space="preserve">
          <source>Instead of computing with a set of cardinality &amp;lsquo;n choose k&amp;rsquo;, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if &amp;lsquo;n choose k&amp;rsquo; is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.</source>
          <target state="translated">En lugar de calcular con un conjunto de cardinalidad 'n elija k', donde n es el n&amp;uacute;mero de muestras yk es el n&amp;uacute;mero de submuestras (al menos el n&amp;uacute;mero de caracter&amp;iacute;sticas), considere solo una subpoblaci&amp;oacute;n estoc&amp;aacute;stica de un tama&amp;ntilde;o m&amp;aacute;ximo dado si 'n elige k 'es m&amp;aacute;s grande que max_subpopulation. Para problemas que no sean peque&amp;ntilde;os, este par&amp;aacute;metro determinar&amp;aacute; el uso de la memoria y el tiempo de ejecuci&amp;oacute;n si no se cambia n_subsamples.</target>
        </trans-unit>
        <trans-unit id="ce6171dee8019fcd810326710a2a425d2ef2e21c" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the L1 norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="translated">En lugar de dar un resultado vectorial, la soluci&amp;oacute;n LARS consiste en una curva que denota la soluci&amp;oacute;n para cada valor de la norma L1 del vector de par&amp;aacute;metros. La ruta completa de los coeficientes se almacena en la matriz &lt;code&gt;coef_path_&lt;/code&gt; , que tiene un tama&amp;ntilde;o (n_features, max_features + 1). La primera columna siempre es cero.</target>
        </trans-unit>
        <trans-unit id="848ef7b85f04c1e0179836725b124a8c68948c34" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the \(\ell_1\) norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c47d2e573aac76a94273f4c46c066cf6f2a8ad1" translate="yes" xml:space="preserve">
          <source>Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. We try out all classifiers on either words or bigrams, with or without idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:</source>
          <target state="translated">En lugar de ajustar los parámetros de los diversos componentes de la cadena,es posible realizar una búsqueda exhaustiva de los mejores parámetros en una cuadrícula de posibles valores.Probamos todos los clasificadores en palabras o en bigrams,con o sin idf,y con un parámetro de penalización de 0,01 o 0,001 para el SVM lineal:</target>
        </trans-unit>
        <trans-unit id="db33f6d449a5c5c7a074dd03bb12ec7fc077641c" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_centering=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSR matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">En su lugar, se espera que la persona que llama establezca expl&amp;iacute;citamente &lt;code&gt;with_centering=False&lt;/code&gt; (en ese caso, solo se realizar&amp;aacute; el escalado de varianza en las caracter&amp;iacute;sticas de la matriz CSR) o que llame a &lt;code&gt;X.toarray()&lt;/code&gt; si espera que la matriz densa materializada se ajuste en memoria.</target>
        </trans-unit>
        <trans-unit id="f080b277d95a6b1142abd6eb9ea11a07abcb1917" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_mean=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSC matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">En su lugar, se espera que la persona que llama establezca expl&amp;iacute;citamente &lt;code&gt;with_mean=False&lt;/code&gt; (en ese caso, solo se realizar&amp;aacute; el escalado de varianza en las caracter&amp;iacute;sticas de la matriz CSC) o que llame a &lt;code&gt;X.toarray()&lt;/code&gt; si espera que la matriz densa materializada se ajuste en memoria.</target>
        </trans-unit>
        <trans-unit id="b11f1ba476938b01d18dd66d0c3826617a20151e" translate="yes" xml:space="preserve">
          <source>Instead, the distribution over \(w\) is assumed to be an axis-parallel, elliptical Gaussian distribution.</source>
          <target state="translated">En su lugar,se supone que la distribución sobre \ ~ (w)es un eje paralelo,elíptica distribución Gaussiana.</target>
        </trans-unit>
        <trans-unit id="33a2873657f7cc53fbafced5857dd217868f1368" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. By default, it is &amp;lsquo;strict&amp;rsquo;, meaning that a UnicodeDecodeError will be raised. Other values are &amp;lsquo;ignore&amp;rsquo; and &amp;lsquo;replace&amp;rsquo;.</source>
          <target state="translated">Instrucciones sobre qu&amp;eacute; hacer si se proporciona una secuencia de bytes para analizar que contiene caracteres que no pertenecen a la &lt;code&gt;encoding&lt;/code&gt; dada . De forma predeterminada, es 'estricto', lo que significa que se generar&amp;aacute; un UnicodeDecodeError. Otros valores son 'ignorar' y 'reemplazar'.</target>
        </trans-unit>
        <trans-unit id="d22b7ba366228e805a5817961de5812cf7af3a5e" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. Passed as keyword argument &amp;lsquo;errors&amp;rsquo; to bytes.decode.</source>
          <target state="translated">Instrucciones sobre qu&amp;eacute; hacer si se proporciona una secuencia de bytes para analizar que contiene caracteres que no pertenecen a la &lt;code&gt;encoding&lt;/code&gt; dada . Pasado como argumento de palabra clave 'errores' a bytes.decode.</target>
        </trans-unit>
        <trans-unit id="98ae123013fca86e4cc21f01a470888e055215cc" translate="yes" xml:space="preserve">
          <source>Integer array of labels. If not provided, labels will be inferred from y_true and y_pred.</source>
          <target state="translated">Una serie de etiquetas enteras.Si no se proporcionan,las etiquetas se deducirán de y_true y y_pred.</target>
        </trans-unit>
        <trans-unit id="e031a894709099be1ecbe448974105f94db94157" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to linear predictor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb86aae8ca1d5ea8c3a2f0216a09b115ca2c4371" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to the decision function.</source>
          <target state="translated">La intercepción (también conocida como sesgo)se añadió a la función de decisión.</target>
        </trans-unit>
        <trans-unit id="02c60e7ce23b1ba7da9aadaca682e74dd23bd987" translate="yes" xml:space="preserve">
          <source>Intercept term.</source>
          <target state="translated">Término de intercepción.</target>
        </trans-unit>
        <trans-unit id="077392291decf12f1b024c471b5bea6bcd10e56c" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid loosing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">Estad&amp;iacute;sticas internas suficientes que mantiene el algoritmo. Mantenerlos es &amp;uacute;til en entornos online, para no perder el historial de evoluci&amp;oacute;n, pero no deber&amp;iacute;an tener ning&amp;uacute;n uso para el usuario final. A (n_components, n_components) es la matriz de covarianza del diccionario. B (n_features, n_components) es la matriz de aproximaci&amp;oacute;n de datos</target>
        </trans-unit>
        <trans-unit id="e1895bccbde849f2ce31dc6715c34549e1152575" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid losing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14f5f43f255d2aa36ff5598f3fb3ace3d6d04389" translate="yes" xml:space="preserve">
          <source>Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.</source>
          <target state="translated">Internamente,la aproximación de Laplace se utiliza para aproximar el posterior no gaussiano por un gaussiano.</target>
        </trans-unit>
        <trans-unit id="0b925a293764508f95547bba83dbd960f81b58e6" translate="yes" xml:space="preserve">
          <source>Internally, the target &lt;code&gt;y&lt;/code&gt; is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">Internamente, el objetivo &lt;code&gt;y&lt;/code&gt; siempre se convierte en una matriz bidimensional para ser utilizada por transformadores scikit-learn. En el momento de la predicci&amp;oacute;n, la salida se reformar&amp;aacute; para que tenga el mismo n&amp;uacute;mero de dimensiones que &lt;code&gt;y&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="465a9fa03a440d5f1b8441512ea129ccebe5933c" translate="yes" xml:space="preserve">
          <source>Internally, this method uses &lt;code&gt;max_iter = 1&lt;/code&gt;. Therefore, it is not guaranteed that a minimum of the cost function is reached after calling it once. Matters such as objective convergence and early stopping should be handled by the user.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a6c7ce41d2f8fb06b74993c6b6972d365c014219" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython.</source>
          <target state="translated">Internamente, usamos &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; y &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; para manejar todos los c&amp;aacute;lculos. Estas bibliotecas est&amp;aacute;n empaquetadas con C y Cython.</target>
        </trans-unit>
        <trans-unit id="921b6b42e9e212246385b90b6e2081ffae4bdd4d" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;&lt;a href=&quot;#id14&quot; id=&quot;id9&quot;&gt;12&lt;/a&gt; and &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;&lt;a href=&quot;#id13&quot; id=&quot;id10&quot;&gt;11&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython. For a description of the implementation and details of the algorithms used, please refer to their respective papers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a02157db035ff864370a2c436b6c81a38e8d8a3c" translate="yes" xml:space="preserve">
          <source>Interpreting coefficients: scale matters</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2d983855292bfa7e006da9cc5e0020136bdcd0e" translate="yes" xml:space="preserve">
          <source>Interruption of multiprocesses jobs with &amp;lsquo;Ctrl-C&amp;rsquo;</source>
          <target state="translated">Interrupci&amp;oacute;n de trabajos multiprocesos con 'Ctrl-C'</target>
        </trans-unit>
        <trans-unit id="c8666d7061618ff72086e37218ea77619df4e168" translate="yes" xml:space="preserve">
          <source>Intuitive interpretation: clustering with bad V-measure can be &lt;strong&gt;qualitatively analyzed in terms of homogeneity and completeness&lt;/strong&gt; to better feel what &amp;lsquo;kind&amp;rsquo; of mistakes is done by the assignment.</source>
          <target state="translated">Interpretaci&amp;oacute;n intuitiva: la agrupaci&amp;oacute;n con una mala medida V se puede &lt;strong&gt;analizar cualitativamente en t&amp;eacute;rminos de homogeneidad e integridad&lt;/strong&gt; para sentir mejor qu&amp;eacute; &quot;tipo&quot; de errores comete la asignaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="b3bf13a5a75c5bcae60f4d54f651f7f504b37960" translate="yes" xml:space="preserve">
          <source>Intuitively, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;precision&lt;/a&gt; is the ability of the classifier not to label as positive a sample that is negative, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;recall&lt;/a&gt; is the ability of the classifier to find all the positive samples.</source>
          <target state="translated">Intuitivamente, la &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;precisi&amp;oacute;n&lt;/a&gt; es la capacidad del clasificador de no etiquetar como positiva una muestra negativa, y la &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;recuperaci&amp;oacute;n&lt;/a&gt; es la capacidad del clasificador de encontrar todas las muestras positivas.</target>
        </trans-unit>
        <trans-unit id="d7d0867c1bea54b1fdaded0f6d4a137c7b95792e" translate="yes" xml:space="preserve">
          <source>Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data.</source>
          <target state="translated">Intuitivamente,también se puede pensar en un histograma como una pila de bloques,un bloque por punto.Apilando los bloques en el espacio de la cuadrícula apropiada,recuperamos el histograma.¿Pero qué pasa si,en lugar de apilar los bloques en una cuadrícula regular,centramos cada bloque en el punto que representa,y sumamos la altura total en cada lugar? Esta idea nos lleva a la visualización de la parte inferior izquierda.Tal vez no sea tan limpia como un histograma,pero el hecho de que los datos impulsen las ubicaciones de los bloques significa que es una representación mucho mejor de los datos subyacentes.</target>
        </trans-unit>
        <trans-unit id="a413ab311fb3ee6ba0089ad38e522b4769b873e8" translate="yes" xml:space="preserve">
          <source>Intuitively, the &lt;code&gt;gamma&lt;/code&gt; parameter defines how far the influence of a single training example reaches, with low values meaning &amp;lsquo;far&amp;rsquo; and high values meaning &amp;lsquo;close&amp;rsquo;. The &lt;code&gt;gamma&lt;/code&gt; parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.</source>
          <target state="translated">De manera intuitiva, el par&amp;aacute;metro &lt;code&gt;gamma&lt;/code&gt; define hasta d&amp;oacute;nde llega la influencia de un solo ejemplo de entrenamiento, con valores bajos que significan &quot;lejos&quot; y valores altos que significan &quot;cerca&quot;. Los par&amp;aacute;metros &lt;code&gt;gamma&lt;/code&gt; pueden verse como la inversa del radio de influencia de las muestras seleccionadas por el modelo como vectores de apoyo.</target>
        </trans-unit>
        <trans-unit id="0af317bc827b64b57bcc63f42ad5928a61b8cb1f" translate="yes" xml:space="preserve">
          <source>Intuitively, this matrix can be interpreted as a matrix of pseudo features (the points raised to some power). The matrix is akin to (but different from) the matrix induced by a polynomial kernel.</source>
          <target state="translated">Intuitivamente,esta matriz puede ser interpretada como una matriz de seudo características (los puntos elevados a cierta potencia).La matriz es similar (pero diferente)a la matriz inducida por un núcleo polinómico.</target>
        </trans-unit>
        <trans-unit id="d0136f60343b9ddfe4e95ff298a3f11e6a44a13d" translate="yes" xml:space="preserve">
          <source>Intuitively, we&amp;rsquo;re trying to maximize the margin (by minimizing \(||w||^2 = w^Tw\)), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value \(y_i (w^T \phi (x_i) + b)\) would be \(\geq 1\) for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance \(\zeta_i\) from their correct margin boundary. The penalty term &lt;code&gt;C&lt;/code&gt; controls the strengh of this penalty, and as a result, acts as an inverse regularization parameter (see note below).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcf37d79a0d7f3a40e6e7bdc86aa285b256f5c04" translate="yes" xml:space="preserve">
          <source>Inverse Gaussian</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20dc7b25181635b005eb94a34d79a1d1ef88f5eb" translate="yes" xml:space="preserve">
          <source>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</source>
          <target state="translated">Invertir la fuerza de regularización;debe ser una flotación positiva.Al igual que en las máquinas de vectores de apoyo,los valores más pequeños especifican una mayor regularización.</target>
        </trans-unit>
        <trans-unit id="33bf667eeef9f8f87ba0b221f0610de05f350c0d" translate="yes" xml:space="preserve">
          <source>Inverse the transformation.</source>
          <target state="translated">Invertir la transformación.</target>
        </trans-unit>
        <trans-unit id="c6d1024dc4c416573a81f58d53b390ce79e27d74" translate="yes" xml:space="preserve">
          <source>Inverse the transformation. Return a vector of size nb_features with the values of Xred assigned to each group of features</source>
          <target state="translated">Invertir la transformación.Devolver un vector de tamaño nb_funciones con los valores de Xred asignados a cada grupo de características</target>
        </trans-unit>
        <trans-unit id="68776e7556a932d7c1772f163bcd0ea5d3036f2f" translate="yes" xml:space="preserve">
          <source>Inverse transform matrix. Only available when &lt;code&gt;fit_inverse_transform&lt;/code&gt; is True.</source>
          <target state="translated">Matriz de transformaci&amp;oacute;n inversa. Solo disponible cuando &lt;code&gt;fit_inverse_transform&lt;/code&gt; es True.</target>
        </trans-unit>
        <trans-unit id="a53229d5506328691d3b32e8898ac28b845cf1d2" translate="yes" xml:space="preserve">
          <source>Inverse transformed array.</source>
          <target state="translated">Matriz transformada inversa.</target>
        </trans-unit>
        <trans-unit id="6d0db9202e10d4b2a1eb16356a668a8027a929fc" translate="yes" xml:space="preserve">
          <source>Invokes the passed method name of the passed estimator. For method=&amp;rsquo;predict_proba&amp;rsquo;, the columns correspond to the classes in sorted order.</source>
          <target state="translated">Invoca el nombre del m&amp;eacute;todo pasado del estimador pasado. Para method = 'predict_proba', las columnas corresponden a las clases en orden ordenado.</target>
        </trans-unit>
        <trans-unit id="93ce645e781a1eaea74d358c1f7aa54ffb25426b" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;'drop'&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d37270b3f9f32ae673296712eb4a194d52812d8f" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;None&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">Invocar el m&amp;eacute;todo de &lt;code&gt;fit&lt;/code&gt; en el &lt;code&gt;VotingClassifier&lt;/code&gt; ajustar&amp;aacute; los clones de esos estimadores originales que se almacenar&amp;aacute;n en el atributo de clase &lt;code&gt;self.estimators_&lt;/code&gt; . Un estimador se puede establecer en &lt;code&gt;None&lt;/code&gt; usando &lt;code&gt;set_params&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0f0eee1c2a0f3878912e931a58a1d92e46d13c5a" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingRegressor&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;'drop'&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42b4a555867c758d3e1c4078b74a325ea5729a8f" translate="yes" xml:space="preserve">
          <source>Iris-Setosa</source>
          <target state="translated">Iris-Setosa</target>
        </trans-unit>
        <trans-unit id="0e4a66fb06fc31fa26bb267122a303163869bd83" translate="yes" xml:space="preserve">
          <source>Iris-Versicolour</source>
          <target state="translated">Iris-Versicolour</target>
        </trans-unit>
        <trans-unit id="c11352543468838c7f536aa067f758dd5cf065cc" translate="yes" xml:space="preserve">
          <source>Iris-Virginica</source>
          <target state="translated">Iris-Virginica</target>
        </trans-unit>
        <trans-unit id="bb0f5655f4fe0f8adc1a787c53ae1e836f4be186" translate="yes" xml:space="preserve">
          <source>Iso-probability lines for Gaussian Processes classification (GPC)</source>
          <target state="translated">Líneas de isoprobabilidad para la clasificación de los Procesos Gausianos (GPC)</target>
        </trans-unit>
        <trans-unit id="2b50512539d0e21a6687a0e4968f704ff8cc80fe" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm</source>
          <target state="translated">Algoritmo de aislamiento forestal</target>
        </trans-unit>
        <trans-unit id="90b7e1d9dae263e13bf54b6eb5bbce295b25c458" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00617c131e78d4c4ef41c400773154d235217731" translate="yes" xml:space="preserve">
          <source>IsolationForest example</source>
          <target state="translated">AislamientoEjemplo de bosque</target>
        </trans-unit>
        <trans-unit id="3a2755971bbebbe11d424139f5382799c401f262" translate="yes" xml:space="preserve">
          <source>Isomap Embedding</source>
          <target state="translated">Incrustación de Isomap</target>
        </trans-unit>
        <trans-unit id="fe769adce6faebe1974c95ecc576637486cbe643" translate="yes" xml:space="preserve">
          <source>Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik, Mair Journal of Statistical Software 2009</source>
          <target state="translated">Optimización de Isotonos en R:Algoritmo de los violadores-ajustadores de piscina (PAVA)y métodos de conjuntos activos Leeuw,Hornik,Mair Journal of Statistical Software 2009</target>
        </trans-unit>
        <trans-unit id="906c68921cb26d68c13066c88efbe4d7d97d1205" translate="yes" xml:space="preserve">
          <source>Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol. 14, No. 2 (May, 1989), pp. 303-308</source>
          <target state="translated">Regresión Isotónica Mediana:A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol.14,No.2 (May,1989),pp.303-308</target>
        </trans-unit>
        <trans-unit id="73b36c35655a3846d59943ac16d2df052178f43b" translate="yes" xml:space="preserve">
          <source>Isotonic Regression</source>
          <target state="translated">Regresión isotónica</target>
        </trans-unit>
        <trans-unit id="c214056f848cd4e39c52f175df94ac0d422815da" translate="yes" xml:space="preserve">
          <source>Isotonic fit of y.</source>
          <target state="translated">Ajuste isotónico de y.</target>
        </trans-unit>
        <trans-unit id="350a83a6eea9b1b3e9903b81e34485a4ebed4999" translate="yes" xml:space="preserve">
          <source>Isotonic regression model.</source>
          <target state="translated">Modelo de regresión isotónica.</target>
        </trans-unit>
        <trans-unit id="7c5ae8804283297e052b100d9986cbd5cd009701" translate="yes" xml:space="preserve">
          <source>Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.</source>
          <target state="translated">Emite una advertencia cuando la función es llamada/la clase es instanciada y añade una advertencia a la cadena de doctores.</target>
        </trans-unit>
        <trans-unit id="4de98053a0f4264ca5362b17521388fcee7300ef" translate="yes" xml:space="preserve">
          <source>It adapts to the data at hand.</source>
          <target state="translated">Se adapta a los datos que tenemos a mano.</target>
        </trans-unit>
        <trans-unit id="d00cd2eb4ac76616c412b13d0e3140cdba7905a2" translate="yes" xml:space="preserve">
          <source>It allows specifying multiple metrics for evaluation.</source>
          <target state="translated">Permite especificar múltiples métricas para la evaluación.</target>
        </trans-unit>
        <trans-unit id="f7ac040f9311efb440d25da16c027a81ab8e3ad5" translate="yes" xml:space="preserve">
          <source>It also can be expressed in set cardinality formulation:</source>
          <target state="translated">También puede expresarse en la formulación de la cardinalidad establecida:</target>
        </trans-unit>
        <trans-unit id="aedad5338d2a0edf1701c1d5c20ad5954bfd8c84" translate="yes" xml:space="preserve">
          <source>It can also be directly used as the &lt;code&gt;kernel&lt;/code&gt; argument:</source>
          <target state="translated">Tambi&amp;eacute;n se puede usar directamente como argumento del &lt;code&gt;kernel&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="9678c3fa14b59b03394b92e8e0080149cf3f64c8" translate="yes" xml:space="preserve">
          <source>It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).</source>
          <target state="translated">También puede utilizarse como paso de preprocesamiento para los estimadores que consideran las variables aleatorias booleanas (por ejemplo,modeladas utilizando la distribución de Bernoulli en un entorno bayesiano).</target>
        </trans-unit>
        <trans-unit id="94554b8e34efbb328f639daf4ccda2adc301f69d" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.</source>
          <target state="translated">También puede utilizarse para transformar las etiquetas no numéricas (siempre y cuando sean hashable y comparables)en etiquetas numéricas.</target>
        </trans-unit>
        <trans-unit id="f96e6d208d3d13906cbf9cd9c045b4122e99a4e4" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels:</source>
          <target state="translated">También puede utilizarse para transformar las etiquetas no numéricas (siempre y cuando sean hashable y comparables)en etiquetas numéricas:</target>
        </trans-unit>
        <trans-unit id="52f6dad43e1775ee0bbb04be9ef515bae958e0a2" translate="yes" xml:space="preserve">
          <source>It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting.</source>
          <target state="translated">También puede tener un término de regularización añadido a la función de pérdida que encoge los parámetros del modelo para evitar el sobreajuste.</target>
        </trans-unit>
        <trans-unit id="998bd5d13863b9f1e85f5a6708bf38f625d563b0" translate="yes" xml:space="preserve">
          <source>It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.</source>
          <target state="translated">También puede utilizar la aplicación scipy.sparse.linalg ARPACK de la SVD truncada.</target>
        </trans-unit>
        <trans-unit id="a5f9c7ba1af0aaff84e6645b602de8095311d995" translate="yes" xml:space="preserve">
          <source>It can be called with parameters &lt;code&gt;(estimator, X, y)&lt;/code&gt;, where &lt;code&gt;estimator&lt;/code&gt; is the model that should be evaluated, &lt;code&gt;X&lt;/code&gt; is validation data, and &lt;code&gt;y&lt;/code&gt; is the ground truth target for &lt;code&gt;X&lt;/code&gt; (in the supervised case) or &lt;code&gt;None&lt;/code&gt; (in the unsupervised case).</source>
          <target state="translated">Se puede llamar con par&amp;aacute;metros &lt;code&gt;(estimator, X, y)&lt;/code&gt; , donde el &lt;code&gt;estimator&lt;/code&gt; es el modelo que se debe evaluar, &lt;code&gt;X&lt;/code&gt; son los datos de validaci&amp;oacute;n e &lt;code&gt;y&lt;/code&gt; es el objetivo de verdad del terreno para &lt;code&gt;X&lt;/code&gt; (en el caso supervisado) o &lt;code&gt;None&lt;/code&gt; (en el caso no supervisado). caso).</target>
        </trans-unit>
        <trans-unit id="9a6afc7a825a539f282e6908ea3004d59da105e7" translate="yes" xml:space="preserve">
          <source>It can be downloaded/loaded using the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Se puede descargar / cargar usando la funci&amp;oacute;n &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="afe5a10e4cd1db3d3b82e37290c3a4b0be9670c8" translate="yes" xml:space="preserve">
          <source>It can be interpreted as a weighted difference per entry.</source>
          <target state="translated">Puede interpretarse como una diferencia ponderada por cada entrada.</target>
        </trans-unit>
        <trans-unit id="d898e853ebb8a8ce7531765c1307531f5ab826e6" translate="yes" xml:space="preserve">
          <source>It can be noted that k-means (and minibatch k-means) are very sensitive to feature scaling and that in this case the IDF weighting helps improve the quality of the clustering by quite a lot as measured against the &amp;ldquo;ground truth&amp;rdquo; provided by the class label assignments of the 20 newsgroups dataset.</source>
          <target state="translated">Se puede observar que k-means (y minibatch k-means) son muy sensibles a la escala de caracter&amp;iacute;sticas y que, en este caso, la ponderaci&amp;oacute;n IDF ayuda a mejorar la calidad de la agrupaci&amp;oacute;n en bastante medida en comparaci&amp;oacute;n con la &quot;verdad b&amp;aacute;sica&quot; proporcionada por las asignaciones de etiquetas de clase del conjunto de datos de 20 grupos de noticias.</target>
        </trans-unit>
        <trans-unit id="074f1a9d1908eeea94dd9624b8e4e74f70971f1f" translate="yes" xml:space="preserve">
          <source>It can be seen from the plots that the results of &lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;Orthogonal Matching Pursuit (OMP)&lt;/a&gt; with two non-zero coefficients is a bit less biased than when keeping only one (the edges look less prominent). It is in addition closer from the ground truth in Frobenius norm.</source>
          <target state="translated">Se puede ver en los gr&amp;aacute;ficos que los resultados de la &lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;b&amp;uacute;squeda de correspondencia ortogonal (OMP)&lt;/a&gt; con dos coeficientes distintos de cero son un poco menos sesgados que cuando se mantiene solo uno (los bordes se ven menos prominentes). Adem&amp;aacute;s, est&amp;aacute; m&amp;aacute;s cerca de la verdad b&amp;aacute;sica en la norma Frobenius.</target>
        </trans-unit>
        <trans-unit id="0cf8fb702abea7c91fd29d6847c4f9bb34be57f9" translate="yes" xml:space="preserve">
          <source>It can be shown that the \(\nu\)-SVC formulation is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="translated">Se puede demostrar que la formulación del CVC es una reparameterización del CVC y por lo tanto matemáticamente equivalente.</target>
        </trans-unit>
        <trans-unit id="157aa7190191e4be1be236c86eabe4e67d5e1efd" translate="yes" xml:space="preserve">
          <source>It can be used for univariate features selection, read more in the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">Se puede utilizar para la selecci&amp;oacute;n de funciones univariadas; lea m&amp;aacute;s en la &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;Gu&amp;iacute;a del usuario&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="711c50760d4f6c264d6b8a92b5297202a600fa0b" translate="yes" xml:space="preserve">
          <source>It can be used to include regularization parameters in the estimation procedure.</source>
          <target state="translated">Puede utilizarse para incluir parámetros de regularización en el procedimiento de estimación.</target>
        </trans-unit>
        <trans-unit id="e52b5bc871c7db656a1b43abcce93011714c74d2" translate="yes" xml:space="preserve">
          <source>It does not require a learning rate.</source>
          <target state="translated">No requiere un ritmo de aprendizaje.</target>
        </trans-unit>
        <trans-unit id="4d19424efe5e9e20338f3273e68fb2ccbb132c12" translate="yes" xml:space="preserve">
          <source>It doesn&amp;rsquo;t give a single metric to use as an objective for clustering optimisation.</source>
          <target state="translated">No proporciona una sola m&amp;eacute;trica para usar como objetivo para la optimizaci&amp;oacute;n de la agrupaci&amp;oacute;n en cl&amp;uacute;steres.</target>
        </trans-unit>
        <trans-unit id="53127b98db145a1107f55ea45dbcbf9eb40fb387" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;2&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a46e66645323d1f8dad68441e68b478eacb85f4" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="translated">Se ha observado en [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt; que, cuando se restringe cuidadosamente, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; puede producir una representaci&amp;oacute;n basada en partes del conjunto de datos, dando como resultado modelos interpretables. El siguiente ejemplo muestra 16 componentes dispersos encontrados por &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; de las im&amp;aacute;genes en el conjunto de datos de caras de Olivetti, en comparaci&amp;oacute;n con las caras propias de PCA.</target>
        </trans-unit>
        <trans-unit id="8dcb00db48002a7fdf6f8f4ffd6c64f833e7dfb1" translate="yes" xml:space="preserve">
          <source>It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map.</source>
          <target state="translated">Tiene propiedades que son similares a las del núcleo de chi cuadrado exponencial que se utiliza a menudo en la visión por computador,pero permite una simple aproximación de Monte Carlo del mapa de características.</target>
        </trans-unit>
        <trans-unit id="37dc8b6f979214e286ace27e5272dd91d61126bc" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="translated">Ha demostrado su utilidad en ML aplicado a datos silenciosos. Ver, por ejemplo, &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;aprendizaje autom&amp;aacute;tico para la mec&amp;aacute;nica cu&amp;aacute;ntica en pocas palabras&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="bbe0780585e0153715a866cbcbfa3d1d5e8429c4" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3ea912466304dbf8e7c52052ad02a2c286c1ad1" translate="yes" xml:space="preserve">
          <source>It implements a variant of Random Kitchen Sinks.[1]</source>
          <target state="translated">Implementa una variante de los fregaderos de cocina al azar.[1]</target>
        </trans-unit>
        <trans-unit id="74d404b8e11acc4d9a6402146bf70c71d78d2e94" translate="yes" xml:space="preserve">
          <source>It is a Linear Model trained with an L1 prior as regularizer.</source>
          <target state="translated">Es un Modelo Lineal entrenado con un L1 previo como regularizador.</target>
        </trans-unit>
        <trans-unit id="971eff281c404ac7ff23799c2f2e17c93f769de1" translate="yes" xml:space="preserve">
          <source>It is a memory-efficient, online-learning algorithm provided as an alternative to &lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt;. It constructs a tree data structure with the cluster centroids being read off the leaf. These can be either the final cluster centroids or can be provided as input to another clustering algorithm such as &lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Es un algoritmo de aprendizaje en l&amp;iacute;nea que ahorra memoria y se proporciona como alternativa a &lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; &lt;/a&gt; . Construye una estructura de datos de &amp;aacute;rbol con los centroides del cl&amp;uacute;ster que se leen de la hoja. Estos pueden ser los centroides del cl&amp;uacute;ster final o pueden proporcionarse como entrada a otro algoritmo de agrupamiento como &lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="1a9933b24a1c1c056c0577574d6613078e271127" translate="yes" xml:space="preserve">
          <source>It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is &lt;code&gt;n_samples&lt;/code&gt;, the update method is same as batch learning. In the literature, this is called kappa.</source>
          <target state="translated">Es un par&amp;aacute;metro que controla la tasa de aprendizaje en el m&amp;eacute;todo de aprendizaje en l&amp;iacute;nea. El valor debe establecerse entre (0.5, 1.0] para garantizar la convergencia asint&amp;oacute;tica. Cuando el valor es 0.0 y el tama&amp;ntilde;o del lote es &lt;code&gt;n_samples&lt;/code&gt; , el m&amp;eacute;todo de actualizaci&amp;oacute;n es el mismo que el aprendizaje por lotes. En la literatura, esto se llama kappa.</target>
        </trans-unit>
        <trans-unit id="15f125826dc7be5a6512e2415a2ab7dc87afbdb7" translate="yes" xml:space="preserve">
          <source>It is advised to set the parameter &lt;code&gt;epsilon&lt;/code&gt; to 1.35 to achieve 95% statistical efficiency.</source>
          <target state="translated">Se recomienda establecer el par&amp;aacute;metro &lt;code&gt;epsilon&lt;/code&gt; en 1,35 para lograr una eficiencia estad&amp;iacute;stica del 95%.</target>
        </trans-unit>
        <trans-unit id="542f7392581e6c4610879b36a37978fc74650959" translate="yes" xml:space="preserve">
          <source>It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.</source>
          <target state="translated">También es común entre la comunidad de procesamiento de textos utilizar valores de características binarias (probablemente para simplificar el razonamiento probabilístico)aunque los recuentos normalizados (también conocidos como frecuencias de término)o las características valoradas de TF-IDF suelen tener un rendimiento ligeramente superior en la práctica.</target>
        </trans-unit>
        <trans-unit id="005dab4eb22b6ead110b29a8850c3898f552d977" translate="yes" xml:space="preserve">
          <source>It is also known as the Variance Ratio Criterion.</source>
          <target state="translated">También se conoce como el Criterio de la Relación de Variación.</target>
        </trans-unit>
        <trans-unit id="657bf821e2dc05fc87b192deecf1d4c429b7d563" translate="yes" xml:space="preserve">
          <source>It is also possible to compute the permutation importances on the training set. This reveals that &lt;code&gt;random_num&lt;/code&gt; gets a significantly higher importance ranking than when computed on the test set. The difference between those two plots is a confirmation that the RF model has enough capacity to use that random numerical feature to overfit. You can further confirm this by re-running this example with constrained RF with min_samples_leaf=10.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4221678f503b8b29e4ba191986027706279048ac" translate="yes" xml:space="preserve">
          <source>It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros.</source>
          <target state="translated">También es posible restringir el diccionario y/o el código para que sean positivos y se ajusten a las limitaciones que puedan estar presentes en los datos.A continuación se presentan las caras con diferentes restricciones de positividad aplicadas.El rojo indica valores negativos,el azul indica valores positivos y el blanco representa ceros.</target>
        </trans-unit>
        <trans-unit id="cd4e94997f77b01819c6451ba1d9a9d98a78db7c" translate="yes" xml:space="preserve">
          <source>It is also possible to efficiently produce a sparse graph showing the connections between neighboring points:</source>
          <target state="translated">También es posible producir eficientemente un gráfico disperso que muestre las conexiones entre los puntos vecinos:</target>
        </trans-unit>
        <trans-unit id="0396c69b921d6f190c09b79532ebbdc31b35e115" translate="yes" xml:space="preserve">
          <source>It is also possible to encode each column into &lt;code&gt;n_categories - 1&lt;/code&gt; columns instead of &lt;code&gt;n_categories&lt;/code&gt; columns by using the &lt;code&gt;drop&lt;/code&gt; parameter. This parameter allows the user to specify a category for each feature to be dropped. This is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression (&lt;a href=&quot;generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt;&lt;code&gt;LinearRegression&lt;/code&gt;&lt;/a&gt;), since co-linearity would cause the covariance matrix to be non-invertible. When this parameter is not None, &lt;code&gt;handle_unknown&lt;/code&gt; must be set to &lt;code&gt;error&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5cc911e1a3213d4a6ea82327423c6b7195a9251" translate="yes" xml:space="preserve">
          <source>It is also possible to map data to a normal distribution using &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; by setting &lt;code&gt;output_distribution='normal'&lt;/code&gt;. Using the earlier example with the iris dataset:</source>
          <target state="translated">Tambi&amp;eacute;n es posible asignar los datos a una distribuci&amp;oacute;n normal usando &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt; configurando &lt;code&gt;output_distribution='normal'&lt;/code&gt; . Usando el ejemplo anterior con el conjunto de datos de iris:</target>
        </trans-unit>
        <trans-unit id="f35eb3fdcbe153523a6b78440df1aad8edf3b026" translate="yes" xml:space="preserve">
          <source>It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:</source>
          <target state="translated">También es posible utilizar otras estrategias de validación cruzada aprobando en su lugar un iterador de validación cruzada,por ejemplo:</target>
        </trans-unit>
        <trans-unit id="c62a692f1bef88aa9ea1dc55b02f68e6ac2b429f" translate="yes" xml:space="preserve">
          <source>It is classically used to separate mixed signals (a problem known as &lt;em&gt;blind source separation&lt;/em&gt;), as in the example below:</source>
          <target state="translated">Se utiliza cl&amp;aacute;sicamente para separar se&amp;ntilde;ales mixtas (un problema conocido como &lt;em&gt;separaci&amp;oacute;n ciega de fuentes&lt;/em&gt; ), como en el ejemplo siguiente:</target>
        </trans-unit>
        <trans-unit id="579f13cf2a54010546e31ecfaa7ced83f4da4e12" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as an ordinary least squares.</source>
          <target state="translated">Es computacionalmente tan rápido como la selección hacia adelante y tiene el mismo orden de complejidad que un mínimo cuadrado ordinario.</target>
        </trans-unit>
        <trans-unit id="eaad2537722d5ddb17252eb65683de60a4e9ec00" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as ordinary least squares.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a0b5a3028e23e8f66ba4845cf98605a35e74ca3" translate="yes" xml:space="preserve">
          <source>It is converted to an F score then to a p-value.</source>
          <target state="translated">Se convierte en una puntuación F y luego en un valor p.</target>
        </trans-unit>
        <trans-unit id="9da4ca4cacbca0baec3287f1b2124c4dcd00df7a" translate="yes" xml:space="preserve">
          <source>It is easily modified to produce solutions for other estimators, like the Lasso.</source>
          <target state="translated">Se modifica fácilmente para producir soluciones para otros estimadores,como el Lazo.</target>
        </trans-unit>
        <trans-unit id="a180f7cced602efdc3c3224733570427c990972f" translate="yes" xml:space="preserve">
          <source>It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren&amp;rsquo;t from this window of time.</source>
          <target state="translated">Es f&amp;aacute;cil para un clasificador sobreajustarse en cosas particulares que aparecen en los datos de los 20 grupos de noticias, como los encabezados de los grupos de noticias. Muchos clasificadores logran puntuaciones F muy altas, pero sus resultados no se generalizar&amp;iacute;an a otros documentos que no pertenecen a esta ventana de tiempo.</target>
        </trans-unit>
        <trans-unit id="f3493e2a2c4e4ad9e265942ad2fd137cc9804a32" translate="yes" xml:space="preserve">
          <source>It is generally recommended to avoid using significantly more processes or threads than the number of CPUs on a machine. Over-subscription happens when a program is running too many threads at the same time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e5aa329cff0eb3a121eaf66246e864cad7413ee" translate="yes" xml:space="preserve">
          <source>It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten&amp;rsquo;s FAQ [2].</source>
          <target state="translated">Se recomienda utilizar otro m&amp;eacute;todo de reducci&amp;oacute;n de dimensionalidad (por ejemplo, PCA para datos densos o TruncatedSVD para datos escasos) para reducir el n&amp;uacute;mero de dimensiones a una cantidad razonable (por ejemplo, 50) si el n&amp;uacute;mero de caracter&amp;iacute;sticas es muy alto. Esto suprimir&amp;aacute; algo de ruido y acelerar&amp;aacute; el c&amp;aacute;lculo de distancias por pares entre muestras. Para obtener m&amp;aacute;s consejos, consulte las preguntas frecuentes de Laurens van der Maaten [2].</target>
        </trans-unit>
        <trans-unit id="4c694641d1b1cd9e68259bd2f7aabf747615adda" translate="yes" xml:space="preserve">
          <source>It is important to assign an identifier to unlabeled points along with the labeled data when training the model with the &lt;code&gt;fit&lt;/code&gt; method. The identifier that this implementation uses is the integer value \(-1\).</source>
          <target state="translated">Es importante asignar un identificador a los puntos sin etiquetar junto con los datos etiquetados al entrenar el modelo con el m&amp;eacute;todo de &lt;code&gt;fit&lt;/code&gt; . El identificador que utiliza esta implementaci&amp;oacute;n es el valor entero \ (- 1 \).</target>
        </trans-unit>
        <trans-unit id="643f8f6ee250eb138ea3f0ff80df853cb01ed9c1" translate="yes" xml:space="preserve">
          <source>It is important to keep in mind that the coefficients that have been dropped may still be related to the outcome by themselves: the model chose to suppress them because they bring little or no additional information on top of the other features. Additionnaly, this selection is unstable for correlated features, and should be interpreted with caution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a067b4f8fd8c4002a8fc9abd7aa015e146d303ad" translate="yes" xml:space="preserve">
          <source>It is important to note that when the number of samples is much larger than the number of features, one would expect that no shrinkage would be necessary. The intuition behind this is that if the population covariance is full rank, when the number of sample grows, the sample covariance will also become positive definite. As a result, no shrinkage would necessary and the method should automatically do this.</source>
          <target state="translated">Es importante señalar que cuando el número de muestras es mucho mayor que el número de características,es de esperar que no sea necesario reducirlo.La intuición que subyace a esto es que si la covarianza de la población es de rango completo,cuando el número de muestras aumente,la covarianza de la muestra también se convertirá en definitiva positiva.Como resultado,no sería necesaria ninguna contracción y el método debería hacer esto automáticamente.</target>
        </trans-unit>
        <trans-unit id="d9e45bb570908f10d72f7b51c91c236b78670a3c" translate="yes" xml:space="preserve">
          <source>It is made of 150 observations of irises, each described by 4 features: their sepal and petal length and width, as detailed in &lt;code&gt;iris.DESCR&lt;/code&gt;.</source>
          <target state="translated">Se compone de 150 observaciones de iris, cada una descrita por 4 caracter&amp;iacute;sticas: su s&amp;eacute;palo y el largo y ancho de los p&amp;eacute;talos, como se detalla en &lt;code&gt;iris.DESCR&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1856ef8269f110a1ccc7c37c9db810b8176fc5f8" translate="yes" xml:space="preserve">
          <source>It is more efficient than the LassoCV if only a small number of features are selected compared to the total number, for instance if there are very few samples compared to the number of features.</source>
          <target state="translated">Es más eficiente que el LassoCV si sólo se selecciona un pequeño número de características en comparación con el número total,por ejemplo si hay muy pocas muestras en comparación con el número de características.</target>
        </trans-unit>
        <trans-unit id="1d58fe1839ae301f10f6b9aaac159ed67a9eabfe" translate="yes" xml:space="preserve">
          <source>It is not appropriate to pass these predictions into an evaluation metric. Use &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt;&lt;code&gt;cross_validate&lt;/code&gt;&lt;/a&gt; to measure generalization error.</source>
          <target state="translated">No es apropiado pasar estas predicciones a una m&amp;eacute;trica de evaluaci&amp;oacute;n. Utilice &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt; &lt;code&gt;cross_validate&lt;/code&gt; &lt;/a&gt; para medir el error de generalizaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="9af2e9f8fa22ff915f29e1a168987ff536812b91" translate="yes" xml:space="preserve">
          <source>It is not recommended to hard-code the backend name in a call to Parallel in a library. Instead it is recommended to set soft hints (prefer) or hard constraints (require) so as to make it possible for library users to change the backend from the outside using the parallel_backend context manager.</source>
          <target state="translated">No es recomendable codificar el nombre del backend en una llamada a Paralelo en una biblioteca.En su lugar se recomienda establecer sugerencias suaves (preferir)o restricciones duras (requerir)para hacer posible que los usuarios de la biblioteca cambien el backend desde el exterior usando el gestor de contexto parallel_backend.</target>
        </trans-unit>
        <trans-unit id="b26cf025ddad5c1f883715bf24d85887eccade22" translate="yes" xml:space="preserve">
          <source>It is not regularized (penalized).</source>
          <target state="translated">No se regulariza (penaliza).</target>
        </trans-unit>
        <trans-unit id="3023247377e7882a0cbda1c2d8280926be6aa8ba" translate="yes" xml:space="preserve">
          <source>It is now possible to prune most tree-based estimators once the trees are built. The pruning is based on minimal cost-complexity. Read more in the &lt;a href=&quot;../../modules/tree#minimal-cost-complexity-pruning&quot;&gt;User Guide&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f18c5e38092754d2adb7bb6eb5c0799854e297b3" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where p &amp;gt;&amp;gt; n (i.e., when the number of dimensions is significantly greater than the number of points)</source>
          <target state="translated">Es num&amp;eacute;ricamente eficiente en contextos donde p &amp;gt;&amp;gt; n (es decir, cuando el n&amp;uacute;mero de dimensiones es significativamente mayor que el n&amp;uacute;mero de puntos)</target>
        </trans-unit>
        <trans-unit id="3387956abcbcbc8c7f2e1b04a07247bbce69a743" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where the number of features is significantly greater than the number of samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c7122bd43c891f087ca247f2fcde5236f637b0c" translate="yes" xml:space="preserve">
          <source>It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values.</source>
          <target state="translated">A menudo es interesante proyectar los datos a un espacio de dimensiones más bajas que conserve la mayor parte de la varianza,dejando caer el vector singular de componentes asociados con valores singulares más bajos.</target>
        </trans-unit>
        <trans-unit id="5ed0af274291a2311daa7ee05d7bd79f85fc7e49" translate="yes" xml:space="preserve">
          <source>It is possible and recommended to search the hyper-parameter space for the best &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt; score.</source>
          <target state="translated">Es posible y recomendable buscar en el espacio de hiperpar&amp;aacute;metros para obtener la mejor puntuaci&amp;oacute;n de &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;validaci&amp;oacute;n cruzada&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="19b21329d1ba1e2fd90b4634d03905cf0f5e7826" translate="yes" xml:space="preserve">
          <source>It is possible to adjust the threshold of the binarizer:</source>
          <target state="translated">Es posible ajustar el umbral del binarizador:</target>
        </trans-unit>
        <trans-unit id="5fdc4b2c9af36a36fca156373f6cb7c574f550b3" translate="yes" xml:space="preserve">
          <source>It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:</source>
          <target state="translated">Es posible calcular las precisiones,retiradas,puntuaciones F1 y soportes por etiqueta en lugar de hacer un promedio:</target>
        </trans-unit>
        <trans-unit id="c890fcaf4f6baafc6ccf39a67fce7daf92b8b950" translate="yes" xml:space="preserve">
          <source>It is possible to control the randomness for reproducibility of the results by explicitly seeding the &lt;code&gt;random_state&lt;/code&gt; pseudo random number generator.</source>
          <target state="translated">Es posible controlar la aleatoriedad para la reproducibilidad de los resultados mediante la siembra expl&amp;iacute;cita del generador de n&amp;uacute;meros pseudoaleatorios &lt;code&gt;random_state&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8c436001d07579c89b669f127dbaf0c3bd65de34" translate="yes" xml:space="preserve">
          <source>It is possible to customize the behavior by passing a callable to the vectorizer constructor:</source>
          <target state="translated">Es posible personalizar el comportamiento pasando una llamada al constructor del vectorizador:</target>
        </trans-unit>
        <trans-unit id="0839b4d3a34db46e778e581b781425b62631583b" translate="yes" xml:space="preserve">
          <source>It is possible to disable either centering or scaling by either passing &lt;code&gt;with_mean=False&lt;/code&gt; or &lt;code&gt;with_std=False&lt;/code&gt; to the constructor of &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Es posible deshabilitar el centrado o el escalado pasando &lt;code&gt;with_mean=False&lt;/code&gt; o &lt;code&gt;with_std=False&lt;/code&gt; al constructor de &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="85a1eed4b8a0f5199be27070848fbc013f8f8638" translate="yes" xml:space="preserve">
          <source>It is possible to get back the category names as follows:</source>
          <target state="translated">Es posible recuperar los nombres de las categorías de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="6b11842b410c7ed9014abd60118219965dd51782" translate="yes" xml:space="preserve">
          <source>It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:</source>
          <target state="translated">Es posible hacer una introspección de los atributos del escalador para averiguar la naturaleza exacta de la transformación aprendida en los datos de entrenamiento:</target>
        </trans-unit>
        <trans-unit id="d994dbf018869cdf387e647211852d55a08f6930" translate="yes" xml:space="preserve">
          <source>It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">Es posible cargar solo una sub-selecci&amp;oacute;n de las categor&amp;iacute;as pasando la lista de categor&amp;iacute;as para cargar a la funci&amp;oacute;n &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="03749a52f5e2e7e976928d01767369407c7307c4" translate="yes" xml:space="preserve">
          <source>It is possible to mix sparse and dense arrays in the same run:</source>
          <target state="translated">Es posible mezclar conjuntos dispersos y densos en la misma carrera:</target>
        </trans-unit>
        <trans-unit id="54adadb321f18cc462f6fa41bc0c4a25f1f6b29d" translate="yes" xml:space="preserve">
          <source>It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The &lt;code&gt;statsmodels
package &amp;lt;https://pypi.org/project/statsmodels/&amp;gt;&lt;/code&gt; natively supports this. Within sklearn, one could use bootstrapping instead as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be16ce674bae3bf54f6cdc3d4d41c5990ca746b6" translate="yes" xml:space="preserve">
          <source>It is possible to overcome those limitations by combining the &amp;ldquo;hashing trick&amp;rdquo; (&lt;a href=&quot;#feature-hashing&quot;&gt;Feature hashing&lt;/a&gt;) implemented by the &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt;&lt;/a&gt; class and the text preprocessing and tokenization features of the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Es posible superar esas limitaciones combinando el &quot;truco de hash&quot; ( &lt;a href=&quot;#feature-hashing&quot;&gt;Feature hashing&lt;/a&gt; ) implementado por la clase &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt; &lt;/a&gt; y las caracter&amp;iacute;sticas de preprocesamiento y tokenizaci&amp;oacute;n de texto del &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="737f1fd475d1dc22b14b4896563d476e36ccb4e8" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Es posible guardar un modelo en scikit-learn usando el modelo de persistencia incorporado de Python, &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="1d319918af937e7b588f1bdf4ba0c9bc1d2e6a8f" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Es posible guardar un modelo en scikit-learn usando el modelo de persistencia incorporado de Python, a saber, &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="c1dfe5304fca32594b4f7b15a0ed1671355448d1" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/3/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33bdca276514e666ea92e40ef8d9c04e5206a96b" translate="yes" xml:space="preserve">
          <source>It is possible to specify this explicitly using the parameter &lt;code&gt;categories&lt;/code&gt;. There are two genders, four possible continents and four web browsers in our dataset:</source>
          <target state="translated">Es posible especificar esto expl&amp;iacute;citamente utilizando las &lt;code&gt;categories&lt;/code&gt; par&amp;aacute;metros . Hay dos g&amp;eacute;neros, cuatro continentes posibles y cuatro navegadores web en nuestro conjunto de datos:</target>
        </trans-unit>
        <trans-unit id="7291e604dadcc649d0d77ccb4ebf5e3c457ba713" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt;&lt;code&gt;plot_confusion_matrix&lt;/code&gt;&lt;/a&gt; to create a &lt;a href=&quot;#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt;&lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt;&lt;/a&gt;. All parameters are stored as attributes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab8a32e7f4d09197b24de455e36e8e5fe3df1e84" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt;&lt;code&gt;plot_precision_recall_curve&lt;/code&gt;&lt;/a&gt; to create a visualizer. All parameters are stored as attributes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80b150a43cd5aba26116528fb4f30933db56b582" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt;&lt;code&gt;plot_roc_curve&lt;/code&gt;&lt;/a&gt; to create a visualizer. All parameters are stored as attributes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f18c0fc60439524a8f745b0ef99ce87a71814897" translate="yes" xml:space="preserve">
          <source>It is recommended to use &lt;a href=&quot;sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create a &lt;a href=&quot;#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt;. All parameters are stored as attributes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbfa3a3539d1ad40958cd50540686b2891b5e349" translate="yes" xml:space="preserve">
          <source>It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features.</source>
          <target state="translated">A veces no basta con centrar y escalar los rasgos de forma independiente,ya que un modelo posterior puede hacer algunas suposiciones sobre la independencia lineal de los rasgos.</target>
        </trans-unit>
        <trans-unit id="b49f552a6383ab2c79a28eb8ae358eb905c8df59" translate="yes" xml:space="preserve">
          <source>It is sometimes tedious to find the model which will best perform on a given dataset. Stacking provide an alternative by combining the outputs of several learners, without the need to choose a model specifically. The performance of stacking is usually close to the best model and sometimes it can outperform the prediction performance of each individual model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="311a1593daf1f1187805481bab88c0d399c3cecf" translate="yes" xml:space="preserve">
          <source>It is sometimes worthwhile storing the state of a specific transformer since it could be used again. Using a pipeline in &lt;code&gt;GridSearchCV&lt;/code&gt; triggers such situations. Therefore, we use the argument &lt;code&gt;memory&lt;/code&gt; to enable caching.</source>
          <target state="translated">A veces vale la pena almacenar el estado de un transformador espec&amp;iacute;fico, ya que podr&amp;iacute;a usarse nuevamente. El uso de una canalizaci&amp;oacute;n en &lt;code&gt;GridSearchCV&lt;/code&gt; desencadena este tipo de situaciones. Por lo tanto, usamos la &lt;code&gt;memory&lt;/code&gt; argumentos para habilitar el almacenamiento en cach&amp;eacute;.</target>
        </trans-unit>
        <trans-unit id="9d1619fcc011ef5a461992b135770b43d5982f12" translate="yes" xml:space="preserve">
          <source>It is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba51434e495bfdf85ff2401c563345468fae8389" translate="yes" xml:space="preserve">
          <source>It is the fastest algorithm for learning mixture models</source>
          <target state="translated">Es el algoritmo más rápido para el aprendizaje de modelos de mezcla</target>
        </trans-unit>
        <trans-unit id="5c9cedaa4c291702a05bee05d8b7517536cf8c97" translate="yes" xml:space="preserve">
          <source>It is the opposite as as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">Es lo contrario,ya que cuanto más grande es mejor,es decir,los valores grandes corresponden a los inliers.</target>
        </trans-unit>
        <trans-unit id="eec2b41e384c85c1e4587a1c1f6fa007f24ac337" translate="yes" xml:space="preserve">
          <source>It is the opposite as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0c456c256349cc53ca134d105c8be601465dd39" translate="yes" xml:space="preserve">
          <source>It is worth noting that RandomForests and ExtraTrees can be fitted in parallel on many cores as each tree is built independently of the others. AdaBoost&amp;rsquo;s samples are built sequentially and so do not use multiple cores.</source>
          <target state="translated">Vale la pena se&amp;ntilde;alar que RandomForests y ExtraTrees se pueden instalar en paralelo en muchos n&amp;uacute;cleos, ya que cada &amp;aacute;rbol se construye independientemente de los dem&amp;aacute;s. Las muestras de AdaBoost se crean secuencialmente y, por lo tanto, no utilizan varios n&amp;uacute;cleos.</target>
        </trans-unit>
        <trans-unit id="9876d3b6328cf0e41b8f18a6a35d45d86ad7b5f1" translate="yes" xml:space="preserve">
          <source>It is worth noting that more than 93% of policyholders have zero claims. If we were to convert this problem into a binary classification task, it would be significantly imbalanced, and even a simplistic model that would only predict mean can achieve an accuracy of 93%.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86441e9f0bfca4823b62f4a6d4cecce7c1b80a8e" translate="yes" xml:space="preserve">
          <source>It might be possible to trade some accuracy on the training set for a slightly better accuracy on the test set by limiting the capacity of the trees (for instance by setting &lt;code&gt;min_samples_leaf=5&lt;/code&gt; or &lt;code&gt;min_samples_leaf=10&lt;/code&gt;) so as to limit overfitting while not introducing too much underfitting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1e3cdc828409a9c2610db18343fc252165aed81" translate="yes" xml:space="preserve">
          <source>It might seem questionable to use a (penalized) Least Squares loss to fit a classification model instead of the more traditional logistic or hinge losses. However in practice all those models can lead to similar cross-validation scores in terms of accuracy or precision/recall, while the penalized least squares loss used by the &lt;a href=&quot;generated/sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier&quot;&gt;&lt;code&gt;RidgeClassifier&lt;/code&gt;&lt;/a&gt; allows for a very different choice of the numerical solvers with distinct computational performance profiles.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4029704b1c865bc18db0f7f71b472d5421882ac" translate="yes" xml:space="preserve">
          <source>It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model.</source>
          <target state="translated">Produce un camino de solución lineal completo,que es útil en la validación cruzada o intentos similares para afinar el modelo.</target>
        </trans-unit>
        <trans-unit id="dec67b5f65557893043d8c253cdd2dab65f3a96a" translate="yes" xml:space="preserve">
          <source>It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4dee1947755b5fe4ca1a29e0b9b0f0b85817660" translate="yes" xml:space="preserve">
          <source>It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.</source>
          <target state="translated">Devuelve un dictado que contiene tiempos de ajuste,tiempos de puntuación (y opcionalmente puntuaciones de entrenamiento así como estimadores ajustados)además de la puntuación de la prueba.</target>
        </trans-unit>
        <trans-unit id="be2089f68dcca4fd6c2250f878425dd499fc444a" translate="yes" xml:space="preserve">
          <source>It returns a dictionary-like object, with the following attributes:</source>
          <target state="translated">Devuelve un objeto parecido a un diccionario,con los siguientes atributos:</target>
        </trans-unit>
        <trans-unit id="6f65f619abd94296c7075a4b5d91a76ac1e641bc" translate="yes" xml:space="preserve">
          <source>It returns a floating point number that quantifies the &lt;code&gt;estimator&lt;/code&gt; prediction quality on &lt;code&gt;X&lt;/code&gt;, with reference to &lt;code&gt;y&lt;/code&gt;. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.</source>
          <target state="translated">Devuelve un n&amp;uacute;mero de punto flotante que cuantifica la calidad de la predicci&amp;oacute;n del &lt;code&gt;estimator&lt;/code&gt; en &lt;code&gt;X&lt;/code&gt; , con referencia &lt;code&gt;y&lt;/code&gt; . Nuevamente, por convenci&amp;oacute;n, los n&amp;uacute;meros m&amp;aacute;s altos son mejores, por lo que si su anotador devuelve p&amp;eacute;rdidas, ese valor debe negarse.</target>
        </trans-unit>
        <trans-unit id="58c0c1b9288f5ba70bfdf3e509c8376ea38265d4" translate="yes" xml:space="preserve">
          <source>It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.</source>
          <target state="translated">Cabe señalar que el lema de Johnson-Lindenstrauss puede dar una estimación muy conservadora del número necesario de componentes,ya que no hace ninguna suposición sobre la estructura del conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="af7347a7c717add0101a2649bad5550dc47a184a" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bcecde02163a3a6b9fb69b7700a66c21be36347" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="translated">Muestra c&amp;oacute;mo usar &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; para aproximar el mapa de caracter&amp;iacute;sticas de un kernel RBF para la clasificaci&amp;oacute;n con una SVM en el conjunto de datos de d&amp;iacute;gitos. Se comparan los resultados usando una SVM lineal en el espacio original, una SVM lineal usando las asignaciones aproximadas y usando una SVM kernelizada. Se muestran los tiempos y la precisi&amp;oacute;n para cantidades variables de muestreos de Monte Carlo (en el caso de &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; , que usa caracter&amp;iacute;sticas aleatorias de Fourier) y subconjuntos de diferentes tama&amp;ntilde;os del conjunto de entrenamiento (para &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; ) para el mapeo aproximado.</target>
        </trans-unit>
        <trans-unit id="45249c231a1e8d583e28deb277d22f5fe88e16b7" translate="yes" xml:space="preserve">
          <source>It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=&amp;rsquo;l1&amp;rsquo; or projected on the euclidean unit sphere if norm=&amp;rsquo;l2&amp;rsquo;.</source>
          <target state="translated">Convierte una colecci&amp;oacute;n de documentos de texto en una matriz scipy.sparse que contiene recuentos de ocurrencia de tokens (o informaci&amp;oacute;n de ocurrencia binaria), posiblemente normalizada como frecuencias de tokens si norm = 'l1' o proyectada en la esfera de la unidad euclidiana si norm = 'l2'.</target>
        </trans-unit>
        <trans-unit id="9b65a724f589693294d8b39fded9beef68bf84ef" translate="yes" xml:space="preserve">
          <source>It updates its model only on mistakes.</source>
          <target state="translated">Actualiza su modelo sólo en los errores.</target>
        </trans-unit>
        <trans-unit id="4ef1ebaa3d2757730ff62ab1b50211fc95aec89a" translate="yes" xml:space="preserve">
          <source>It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.</source>
          <target state="translated">Utiliza la implementación LAPACK de la SVD completa o una SVD truncada aleatoriamente por el método de Halko et al.2009,dependiendo de la forma de los datos de entrada y el número de componentes a extraer.</target>
        </trans-unit>
        <trans-unit id="74ae47bdcf2723d7a82146ada9f167c02a150388" translate="yes" xml:space="preserve">
          <source>It will plot the class decision boundaries given by a Nearest Neighbors classifier when using the Euclidean distance on the original features, versus using the Euclidean distance after the transformation learned by Neighborhood Components Analysis. The latter aims to find a linear transformation that maximises the (stochastic) nearest neighbor classification accuracy on the training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80b76c72ce07f72ff1bcc8279eceffb68f3a73b2" translate="yes" xml:space="preserve">
          <source>It would be possible to get even higher predictive performance with a larger neural network but the training would also be significantly more expensive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb35f7366145b28ea8e69aba84c19929cb4fd162" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the &lt;code&gt;return_X_y&lt;/code&gt; parameter to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Tambi&amp;eacute;n es posible que casi todas estas funciones restrinjan la salida para que sea una tupla que contenga solo los datos y el destino, estableciendo el par&amp;aacute;metro &lt;code&gt;return_X_y&lt;/code&gt; en &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="522ad20a1aa12ab3e8e796322f388a9318be6368" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows:</source>
          <target state="translated">Est&amp;aacute; claro c&amp;oacute;mo la forma del grano afecta la suavidad de la distribuci&amp;oacute;n resultante. El estimador de densidad del kernel de scikit-learn se puede utilizar de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="35600165765d17d14f9a53e3a40d6c087a8e15cc" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="262f72bd253b7e8f886f3645ecd5afaaf624d7fc" translate="yes" xml:space="preserve">
          <source>Iterate 2 and 3 until convergence.</source>
          <target state="translated">Iterar 2 y 3 hasta la convergencia.</target>
        </trans-unit>
        <trans-unit id="e39adff24d3659cea88912062bf2425d11890a07" translate="yes" xml:space="preserve">
          <source>Iterative imputation of the missing values</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2f172891cc8c1241e8513ed23c4f46ceb939f0f" translate="yes" xml:space="preserve">
          <source>Iterative procedure to maximize the evidence</source>
          <target state="translated">Procedimiento iterativo para maximizar la evidencia</target>
        </trans-unit>
        <trans-unit id="1e87dcaf344d15783f1af4ad18b162b497d772d4" translate="yes" xml:space="preserve">
          <source>Its dual is</source>
          <target state="translated">Su doble es</target>
        </trans-unit>
        <trans-unit id="ce6398892ce7bfa57c8075a52d29f534a23469a6" translate="yes" xml:space="preserve">
          <source>Its validation performance, measured via the \(R^2\) score, is significantly larger than the chance level. This makes it possible to use the &lt;a href=&quot;generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;permutation_importance&lt;/code&gt;&lt;/a&gt; function to probe which features are most predictive:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07da5fcab12f57a49e864df0a2dcb43ec8cd118b" translate="yes" xml:space="preserve">
          <source>J&amp;oslash;rgensen, B. (1992). The theory of exponential dispersion models and analysis of deviance. Monografias de matem&amp;aacute;tica, no. 51. See also &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_dispersion_model&quot;&gt;Exponential dispersion model.&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7e5d74ebe16b65b422b57dd9089de563aa7d4b5" translate="yes" xml:space="preserve">
          <source>J. Cohen (1960). &amp;ldquo;A coefficient of agreement for nominal scales&amp;rdquo;. Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ba292a3729a3ffa6797e98ae7a24bba4f0e087f" translate="yes" xml:space="preserve">
          <source>J. Davis, M. Goadrich, &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;The Relationship Between Precision-Recall and ROC Curves&lt;/a&gt;, ICML 2006.</source>
          <target state="translated">J. Davis, M. Goadrich, &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;La relaci&amp;oacute;n entre precisi&amp;oacute;n-recuperaci&amp;oacute;n y curvas ROC&lt;/a&gt; , ICML 2006.</target>
        </trans-unit>
        <trans-unit id="9f9ca6a90c561398be254053aedc4a945c9160d7" translate="yes" xml:space="preserve">
          <source>J. Friedman, &amp;ldquo;Multivariate adaptive regression splines&amp;rdquo;, The Annals of Statistics 19 (1), pages 1-67, 1991.</source>
          <target state="translated">J. Friedman, &quot;Splines de regresi&amp;oacute;n adaptativa multivariante&quot;, The Annals of Statistics 19 (1), p&amp;aacute;ginas 1-67, 1991.</target>
        </trans-unit>
        <trans-unit id="f3a4e2abf1b3937c134504328e857f33b32a50ea" translate="yes" xml:space="preserve">
          <source>J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</source>
          <target state="translated">J.Friedman,Aproximación de la Función Codiciosa:A Gradient Boosting Machine,The Annals of Statistics,Vol.29,No.5,2001.</target>
        </trans-unit>
        <trans-unit id="f06167e7b529cb39087bd6b97f521b456419a7cd" translate="yes" xml:space="preserve">
          <source>J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov. &amp;ldquo;Neighbourhood Components Analysis&amp;rdquo;. Advances in Neural Information Processing Systems. 17, 513-520, 2005. &lt;a href=&quot;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&quot;&gt;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37cd6f13ac969b2cba8a5a7a242580515d06793b" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="translated">J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Aprendizaje de diccionarios en l&amp;iacute;nea para codificaci&amp;oacute;n dispersa ( &lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="8417a497b98a8d480d1cb9818d1f322bb7565268" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d67bb0042b556d1819c5bcf6e551c8393e0d921f" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="translated">J. Nothman, H. Qin y R. Yurchak (2018). &lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&quot;Detener listas de palabras en paquetes de software de c&amp;oacute;digo abierto gratuitos&quot;&lt;/a&gt; . En &lt;em&gt;Proc. Taller de software de c&amp;oacute;digo abierto de PNL&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="40bd5fef8dd5af699d79639063d7832cf1e45e49" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;https://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="692e866d29ab5ac27b12eb939942a283756e56c6" translate="yes" xml:space="preserve">
          <source>J. Zhu, H. Zou, S. Rosset, T. Hastie. &amp;ldquo;Multi-class AdaBoost&amp;rdquo;, 2009.</source>
          <target state="translated">J. Zhu, H. Zou, S. Rosset, T. Hastie. &amp;ldquo;AdaBoost de varias clases&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="5d92020b429e9c336d3ae7d33c4ba163d63036bb" translate="yes" xml:space="preserve">
          <source>J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.</source>
          <target state="translated">J.R.Quinlan.C4.5:programas para el aprendizaje de la máquina.Morgan Kaufmann,1993.</target>
        </trans-unit>
        <trans-unit id="b259e0488f66e10ad76098d33440aa4c5f23c876" translate="yes" xml:space="preserve">
          <source>JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case&lt;/a&gt;</source>
          <target state="translated">JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;Una encuesta de m&amp;eacute;todos de m&amp;iacute;nimos cuadrados parciales (PLS), con &amp;eacute;nfasis en el caso de dos bloques&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89591613ce2ead27076b0dd7b68b18da1f4e31d9" translate="yes" xml:space="preserve">
          <source>Jaccard similarity coefficient score</source>
          <target state="translated">La puntuación del coeficiente de similitud de Jaccard</target>
        </trans-unit>
        <trans-unit id="3dd35b446a7d3de6ee5688cfabde9bb7cc55f61a" translate="yes" xml:space="preserve">
          <source>JaccardDistance</source>
          <target state="translated">JaccardDistance</target>
        </trans-unit>
        <trans-unit id="493395686693db33a59d5eea00e82ad6c02c5742" translate="yes" xml:space="preserve">
          <source>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.</source>
          <target state="translated">Jacob A.Wegelin.Un estudio de los métodos de mínimos cuadrados parciales (PLS),con énfasis en el caso de los dos bloques.Informe técnico 371,Departamento de Estadística,Universidad de Washington,Seattle,2000.</target>
        </trans-unit>
        <trans-unit id="b9a5b6145a558ec82725430f200fe46fa35f6aac" translate="yes" xml:space="preserve">
          <source>Jarvelin, K., &amp;amp; Kekalainen, J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS), 20(4), 422-446.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80af07b09c2acb231d89e62f1382b88433574f92" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,</source>
          <target state="translated">Jesse Read,Bernhard Pfahringer,Geoff Holmes,Yew Frank,</target>
        </trans-unit>
        <trans-unit id="6f9c9a3eee3a8f7459d68946df6ef289f22fee94" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, &amp;ldquo;Classifier Chains for Multi-label Classification&amp;rdquo;, 2009.</source>
          <target state="translated">Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, &amp;ldquo;Cadenas clasificadoras para clasificaci&amp;oacute;n de etiquetas m&amp;uacute;ltiples&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="43121fdb3391941437c9f79e4fe2a93d4f69bed7" translate="yes" xml:space="preserve">
          <source>Joblib also tries to limit the oversubscription by limiting the number of threads usable in some third-party library threadpools like OpenBLAS, MKL or OpenMP. The default limit in each worker is set to &lt;code&gt;max(cpu_count() // effective_n_jobs, 1)&lt;/code&gt; but this limit can be overwritten with the &lt;code&gt;inner_max_num_threads&lt;/code&gt; argument which will be used to set this limit in the child processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbd429df4e4127b1956a8217cca140ac1f4576de" translate="yes" xml:space="preserve">
          <source>Joblib is able to support both multi-processing and multi-threading. Whether joblib chooses to spawn a thread or a process depends on the &lt;strong&gt;backend&lt;/strong&gt; that it&amp;rsquo;s using.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dbbc7351b3326fa09a2af62a2a8c482a7f498e4a" translate="yes" xml:space="preserve">
          <source>Joblib is currently unable to avoid oversubscription in a multi-threading context. It can only do so with the &lt;code&gt;loky&lt;/code&gt; backend (which spawns processes).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2da78ef6529cd970b51628e985f5f2ea249ac134" translate="yes" xml:space="preserve">
          <source>Johanna Hardin, David M Rocke. The distribution of robust distances. Journal of Computational and Graphical Statistics. December 1, 2005, 14(4): 928-946.</source>
          <target state="translated">Johanna Hardin,David M.Rocke.La distribución de las distancias robustas.Revista de Estadísticas Gráficas y Computacionales.1 de diciembre de 2005,14(4):928-946.</target>
        </trans-unit>
        <trans-unit id="3cd3820aa7670cf9157f83a7b518b28ca957a3fc" translate="yes" xml:space="preserve">
          <source>John K. Dixon, &amp;ldquo;Pattern Recognition with Partly Missing Data&amp;rdquo;, IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue: 10, pp. 617 - 621, Oct. 1979. &lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/4310090/&quot;&gt;http://ieeexplore.ieee.org/abstract/document/4310090/&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f669c43cc07002b0d2c74696c2e577f06e2f830d" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, (2015). &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;.</source>
          <target state="translated">Juan. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015). &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentos del aprendizaje autom&amp;aacute;tico para el an&amp;aacute;lisis predictivo de datos: algoritmos, ejemplos resueltos y estudios de casos&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7aafef76ed32e6bfff8b0b682dc86da3ac5a13fa" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;, 2015.</source>
          <target state="translated">Juan. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentos del aprendizaje autom&amp;aacute;tico para el an&amp;aacute;lisis predictivo de datos: algoritmos, ejemplos resueltos y estudios de casos&lt;/a&gt; , 2015.</target>
        </trans-unit>
        <trans-unit id="19e6bf8efc7133dd97d0abbd89569a0495139bdc" translate="yes" xml:space="preserve">
          <source>Joint feature selection with multi-task Lasso</source>
          <target state="translated">Selección conjunta de características con el lazo multitarea</target>
        </trans-unit>
        <trans-unit id="6e210d8e33bded6f565ddf30568ce6ee46546dcb" translate="yes" xml:space="preserve">
          <source>Joint parameter selection</source>
          <target state="translated">Selección de parámetros conjuntos</target>
        </trans-unit>
        <trans-unit id="5dcd2dd79faa568a08732dcdc7a1c5d001632db6" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="translated">Journal of Machine Learning Research 15 (octubre): 3221-3245, 2014. &lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7eaac587d1f40d409b66183976f554af82049338" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6689749f561220cbe925de6f0809b1dc75c6258d" translate="yes" xml:space="preserve">
          <source>July, 1988</source>
          <target state="translated">Julio de 1988</target>
        </trans-unit>
        <trans-unit id="854e66ede1ccc0e35f92ec3068666dcad934aaf9" translate="yes" xml:space="preserve">
          <source>July; 1998</source>
          <target state="translated">Julio;1998</target>
        </trans-unit>
        <trans-unit id="a2a7da9b458fe4f43b31552673f0b66352445d61" translate="yes" xml:space="preserve">
          <source>Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN Error Measures in MultiClass Prediction</source>
          <target state="translated">Jurman,Riccadonna,Furlanello,(2012).Una comparación de las medidas de error del MCC y el CEN en la predicción de multiclases</target>
        </trans-unit>
        <trans-unit id="a8dcc7a6052d083397dd89c88efdae4d16610c1e" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="translated">Del mismo modo que es importante probar un predictor en datos retenidos durante el entrenamiento, el preprocesamiento (como la estandarizaci&amp;oacute;n, la selecci&amp;oacute;n de caracter&amp;iacute;sticas, etc.) y &lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;transformaciones de datos&lt;/a&gt; similares tambi&amp;eacute;n deben aprenderse de un conjunto de entrenamiento y aplicarse a los datos retenidos para la predicci&amp;oacute;n. :</target>
        </trans-unit>
        <trans-unit id="bb93e8a53cb9de4ad35209178664abb04cf0e5f7" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;https://scikit-learn.org/0.23/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19ba747b37c5ad6ac1cc4689022bdfff83622716" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a in b), but with a nicer default message.</source>
          <target state="translated">Igual que self.assertTrue(a en b),pero con un mejor mensaje por defecto.</target>
        </trans-unit>
        <trans-unit id="a7abce2837c2684f6308e0a3abb93654038ccc5f" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a not in b), but with a nicer default message.</source>
          <target state="translated">Como self.assertTrue(a no en b),pero con un mejor mensaje por defecto.</target>
        </trans-unit>
        <trans-unit id="57113affe2edb0e21a97719d086746970040c08f" translate="yes" xml:space="preserve">
          <source>K&amp;auml;rkk&amp;auml;inen and S. &amp;Auml;yr&amp;auml;m&amp;ouml;: &lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;On Computation of Spatial Median for Robust Data Mining.&lt;/a&gt;</source>
          <target state="translated">K&amp;auml;rkk&amp;auml;inen y S. &amp;Auml;yr&amp;auml;m&amp;ouml;: &lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;sobre el c&amp;aacute;lculo de la mediana espacial para la miner&amp;iacute;a de datos robusta.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c859ded2bd8aa408f6c0369beaf2c2cf3f0ddde" translate="yes" xml:space="preserve">
          <source>K(X, Y) = &amp;lt;X, Y&amp;gt; / (||X||*||Y||)</source>
          <target state="translated">K (X, Y) = &amp;lt;X, Y&amp;gt; / (|| X || * || Y ||)</target>
        </trans-unit>
        <trans-unit id="86beb78a3bdf4132202cbc165378339bb7f278e3" translate="yes" xml:space="preserve">
          <source>K-Folds cross-validator</source>
          <target state="translated">El validador cruzado de los pliegues K</target>
        </trans-unit>
        <trans-unit id="66e29f0aeaaf6f3b77934175874c79014b658ea2" translate="yes" xml:space="preserve">
          <source>K-Means</source>
          <target state="translated">K-Means</target>
        </trans-unit>
        <trans-unit id="bc6e2dbca5eeaca5cfd908f6085c13e70dbbe207" translate="yes" xml:space="preserve">
          <source>K-Means clustering</source>
          <target state="translated">Agrupación de K-Means</target>
        </trans-unit>
        <trans-unit id="57bc3e1ca5db2c1c7f140e0c9654a7e1bd0c4cd4" translate="yes" xml:space="preserve">
          <source>K-Means clustering.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4dcab3f446cd66684df43251a7a52921a5b665f1" translate="yes" xml:space="preserve">
          <source>K-dimensional tree for fast generalized N-point problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c532c5671424d23a3a3bc85d7cee5f6f8a964404" translate="yes" xml:space="preserve">
          <source>K-fold iterator variant with non-overlapping groups.</source>
          <target state="translated">Variante del iterador K con grupos no superpuestos.</target>
        </trans-unit>
        <trans-unit id="8434c9f312099287fd33427192dcba6bdae1583b" translate="yes" xml:space="preserve">
          <source>K-means Clustering</source>
          <target state="translated">K-means Clustering</target>
        </trans-unit>
        <trans-unit id="16176fa529a1e6d30521129cdc8f04353aaff22e" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient by using the triangle inequality, but currently doesn&amp;rsquo;t support sparse data. &amp;ldquo;auto&amp;rdquo; chooses &amp;ldquo;elkan&amp;rdquo; for dense data and &amp;ldquo;full&amp;rdquo; for sparse data.</source>
          <target state="translated">Algoritmo K-means a utilizar. El algoritmo cl&amp;aacute;sico de estilo EM es &quot;completo&quot;. La variaci&amp;oacute;n &quot;elkan&quot; es m&amp;aacute;s eficiente al usar la desigualdad del tri&amp;aacute;ngulo, pero actualmente no admite datos escasos. &quot;Auto&quot; elige &quot;elkan&quot; para datos densos y &quot;completo&quot; para datos escasos.</target>
        </trans-unit>
        <trans-unit id="c00998c8eabed1e931fa81c6f69faf59aad07506" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient on data with well-defined clusters, by using the triangle inequality. However it&amp;rsquo;s more memory intensive due to the allocation of an extra array of shape (n_samples, n_clusters).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="848dff73d6f69d92cd5b01b40f76a731abde9743" translate="yes" xml:space="preserve">
          <source>K-means can be used for vector quantization. This is achieved using the transform method of a trained model of &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Se pueden usar K-medias para la cuantificaci&amp;oacute;n de vectores. Esto se logra utilizando el m&amp;eacute;todo de transformaci&amp;oacute;n de un modelo entrenado de &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="ba78203e9e9f38ce3f7e015938283eb704622fc1" translate="yes" xml:space="preserve">
          <source>K-means clustering</source>
          <target state="translated">K significa agrupación</target>
        </trans-unit>
        <trans-unit id="4c31918fe250fba32eafec9c8bd2408d0665baa0" translate="yes" xml:space="preserve">
          <source>K-means clustering algorithm.</source>
          <target state="translated">K significa algoritmo de agrupación.</target>
        </trans-unit>
        <trans-unit id="3f9399be9d9993e05f4712a210efb7bcf391430f" translate="yes" xml:space="preserve">
          <source>K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.</source>
          <target state="translated">K-means es equivalente al algoritmo de maximización de expectativas con una pequeña matriz de covarianza diagonal,totalmente igual.</target>
        </trans-unit>
        <trans-unit id="f931e58c5b02fb6c60e80955646f359bee6ac7ee" translate="yes" xml:space="preserve">
          <source>K-means is often referred to as Lloyd&amp;rsquo;s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose \(k\) samples from the dataset \(X\). After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.</source>
          <target state="translated">K-means a menudo se conoce como algoritmo de Lloyd. En t&amp;eacute;rminos b&amp;aacute;sicos, el algoritmo tiene tres pasos. El primer paso elige los centroides iniciales, siendo el m&amp;eacute;todo m&amp;aacute;s b&amp;aacute;sico elegir \ (k \) muestras del conjunto de datos \ (X \). Despu&amp;eacute;s de la inicializaci&amp;oacute;n, K-means consiste en recorrer los otros dos pasos. El primer paso asigna cada muestra a su centroide m&amp;aacute;s cercano. El segundo paso crea nuevos centroides tomando el valor medio de todas las muestras asignadas a cada centroide anterior. Se calcula la diferencia entre el antiguo y el nuevo centroide y el algoritmo repite estos dos &amp;uacute;ltimos pasos hasta que este valor es menor que un umbral. En otras palabras, se repite hasta que los centroides no se mueven significativamente.</target>
        </trans-unit>
        <trans-unit id="c91b0be65ee9c7db25b71aa279369cca08edc7ca" translate="yes" xml:space="preserve">
          <source>K-means quantization</source>
          <target state="translated">K significa cuantificación</target>
        </trans-unit>
        <trans-unit id="5cf295fcd230ab825b1fa5bcf82b9dac494d126c" translate="yes" xml:space="preserve">
          <source>KDTree for fast generalized N-point problems</source>
          <target state="translated">KDTree para problemas de puntos N rápidamente generalizados</target>
        </trans-unit>
        <trans-unit id="34d74f913e8bd68fa4a9d1c4d3966f34ca72fc15" translate="yes" xml:space="preserve">
          <source>KDTree(X, leaf_size=40, metric=&amp;rsquo;minkowski&amp;rsquo;, **kwargs)</source>
          <target state="translated">KDTree (X, tama&amp;ntilde;o_hoja = 40, m&amp;eacute;trico = 'minkowski', ** kwargs)</target>
        </trans-unit>
        <trans-unit id="e6f48940c5e34202cb03c9567fde221973b85eb8" translate="yes" xml:space="preserve">
          <source>KNN Based Imputation</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a130d990c735953536ce43a1c5cf50c4989bca1" translate="yes" xml:space="preserve">
          <source>Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.</source>
          <target state="translated">Las puntuaciones Kappa pueden calcularse para problemas binarios o de varias clases,pero no para problemas de varias etiquetas (excepto si se calcula manualmente una puntuación por etiqueta)y no para más de dos anotadores.</target>
        </trans-unit>
        <trans-unit id="7642aa421288e310d04c4d2f1c88ac24e935cd02" translate="yes" xml:space="preserve">
          <source>Ke et. al. &lt;a href=&quot;https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree&quot;&gt;&amp;ldquo;LightGBM: A Highly Efficient Gradient BoostingDecision Tree&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="992bd2f88020b43ae9d881f8a8ecb43504cd4a74" translate="yes" xml:space="preserve">
          <source>Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False.</source>
          <target state="translated">Mantén los 3 canales RGB en lugar de promediarlos en un solo canal de nivel de gris.Si el color es Verdadero,la forma de los datos tiene una dimensión más que la forma con color=Falso.</target>
        </trans-unit>
        <trans-unit id="e85b73c38883acb129e419e1a2f1719d75a444c5" translate="yes" xml:space="preserve">
          <source>Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005.</source>
          <target state="translated">Ken Tang y Ponnuthurai N.Suganthan y Xi Yao y A.Kai Qin.Reducción de la dimensionalidad lineal utilizando LDA ponderado por relevancia.Escuela de Ingeniería Eléctrica y Electrónica de la Universidad Tecnológica de Nanyang.2005.</target>
        </trans-unit>
        <trans-unit id="4ac337776123607052d628758806e2172a140241" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimate of Species Distributions</source>
          <target state="translated">Estimación de la densidad del núcleo de la distribución de las especies</target>
        </trans-unit>
        <trans-unit id="1794dd0445cf0665650fb5446983f4ef8a3519d3" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation</source>
          <target state="translated">Estimación de la densidad del núcleo</target>
        </trans-unit>
        <trans-unit id="9837c7505d0f3a8c028c3a0430177597bb56e3e2" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bd4b1d4f074cf6b0f30ea849b2a75ad1d3777d9" translate="yes" xml:space="preserve">
          <source>Kernel PCA</source>
          <target state="translated">Núcleo PCA</target>
        </trans-unit>
        <trans-unit id="e5cb129fc99d7ba99fe28de6d8de36380920334b" translate="yes" xml:space="preserve">
          <source>Kernel PCA was introduced in:</source>
          <target state="translated">Se introdujo el PCA del núcleo:</target>
        </trans-unit>
        <trans-unit id="2064482c4c2332e23a8df06a915448e7e780cd46" translate="yes" xml:space="preserve">
          <source>Kernel Principal Component Analysis.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba5a4a64bda1b4288aa7730d4a3cc2a5a99cf5dc" translate="yes" xml:space="preserve">
          <source>Kernel Principal component analysis (KPCA)</source>
          <target state="translated">Análisis del componente principal del núcleo (KPCA)</target>
        </trans-unit>
        <trans-unit id="f9f3967ca79560e0b7bba219989bbd17450e2f6e" translate="yes" xml:space="preserve">
          <source>Kernel bandwidth.</source>
          <target state="translated">Ancho de banda del núcleo.</target>
        </trans-unit>
        <trans-unit id="8f8874978483d89d1eb3e15131193d11bfd798e3" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for &amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">Coeficiente de kernel para 'rbf', 'poli' y 'sigmoide'.</target>
        </trans-unit>
        <trans-unit id="97392135d656893f41c86b28ea3abd0d9e018bae" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf kernel.</source>
          <target state="translated">Coeficiente del núcleo para el núcleo rbf.</target>
        </trans-unit>
        <trans-unit id="0ae546d11d3317bcab299286e34e2f35ebfa8832" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">Coeficiente de núcleo para los núcleos rbf,poli y sigmoide.Ignorado por otros núcleos.</target>
        </trans-unit>
        <trans-unit id="4a5a36cb73b6fa8cb90e406a9b203038f766b3f9" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt;.</source>
          <target state="translated">Coeficiente de kernel para kernels rbf, poli, sigmoide, laplaciano y chi2. Ignorado por &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7ed139f80ae0f25db98c92c5cce6311e8435271b" translate="yes" xml:space="preserve">
          <source>Kernel density estimation in scikit-learn is implemented in the &lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; estimator, which uses the Ball Tree or KD Tree for efficient queries (see &lt;a href=&quot;neighbors#neighbors&quot;&gt;Nearest Neighbors&lt;/a&gt; for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions.</source>
          <target state="translated">La estimaci&amp;oacute;n de la densidad del kernel en scikit-learn se implementa en el estimador &lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt; , que usa el &amp;aacute;rbol de bolas o el &amp;aacute;rbol KD para consultas eficientes (consulte &lt;a href=&quot;neighbors#neighbors&quot;&gt;Vecinos m&amp;aacute;s cercanos&lt;/a&gt; para una discusi&amp;oacute;n sobre estos). Aunque el ejemplo anterior utiliza un conjunto de datos 1D para simplificar, la estimaci&amp;oacute;n de la densidad del n&amp;uacute;cleo se puede realizar en cualquier n&amp;uacute;mero de dimensiones, aunque en la pr&amp;aacute;ctica la maldici&amp;oacute;n de la dimensionalidad hace que su rendimiento se degrade en dimensiones altas.</target>
        </trans-unit>
        <trans-unit id="55e8fbe20e17e26ae0f3d4e88a1aeba0651c9393" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">Hiperpar&amp;aacute;metros de kernel para los que se eval&amp;uacute;a la probabilidad log-marginal. Si es None, se devuelve el &lt;code&gt;self.kernel_.theta&lt;/code&gt; precalculado de self.kernel_.theta .</target>
        </trans-unit>
        <trans-unit id="aef459d7999942524bf342d4727b96b71e8fe80a" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. In the case of multi-class classification, theta may be the hyperparameters of the compound kernel or of an individual kernel. In the latter case, all individual kernel get assigned the same theta values. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">Hiperpar&amp;aacute;metros de kernel para los que se eval&amp;uacute;a la probabilidad log-marginal. En el caso de la clasificaci&amp;oacute;n de clases m&amp;uacute;ltiples, theta pueden ser los hiperpar&amp;aacute;metros del n&amp;uacute;cleo compuesto o de un n&amp;uacute;cleo individual. En el &amp;uacute;ltimo caso, a todos los n&amp;uacute;cleos individuales se les asignan los mismos valores theta. Si es None, se devuelve el &lt;code&gt;self.kernel_.theta&lt;/code&gt; precalculado de self.kernel_.theta .</target>
        </trans-unit>
        <trans-unit id="e5fe7d4b4a2b4b1f4287c0408af092681ae17306" translate="yes" xml:space="preserve">
          <source>Kernel k(X, Y)</source>
          <target state="translated">Kernel k(X,Y)</target>
        </trans-unit>
        <trans-unit id="3ec24bca52509370dd13e99805241c7649952db1" translate="yes" xml:space="preserve">
          <source>Kernel map to be approximated. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number.</source>
          <target state="translated">Mapa del núcleo para ser aproximado.Un llamable debe aceptar dos argumentos y los argumentos de la palabra clave pasados a este objeto como kernel_params,y debe devolver un número de punto flotante.</target>
        </trans-unit>
        <trans-unit id="819d0e343c77a54a44f85a514be1d98b92643c33" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. Set to &amp;ldquo;precomputed&amp;rdquo; in order to pass a precomputed kernel matrix to the estimator methods instead of samples.</source>
          <target state="translated">Mapeo de kernel utilizado internamente. Un invocable debe aceptar dos argumentos y los argumentos de palabra clave pasados ​​a este objeto como kernel_params, y debe devolver un n&amp;uacute;mero de punto flotante. Establ&amp;eacute;zcalo en &quot;precalculado&quot; para pasar una matriz de kernel precalculada a los m&amp;eacute;todos de estimador en lugar de a las muestras.</target>
        </trans-unit>
        <trans-unit id="438349cc7f220c5c2d499eba1d3d6c5414057ba4" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. This parameter is directly passed to &lt;code&gt;sklearn.metrics.pairwise.pairwise_kernel&lt;/code&gt;. If &lt;code&gt;kernel&lt;/code&gt; is a string, it must be one of the metrics in &lt;code&gt;pairwise.PAIRWISE_KERNEL_FUNCTIONS&lt;/code&gt;. If &lt;code&gt;kernel&lt;/code&gt; is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if &lt;code&gt;kernel&lt;/code&gt; is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from &lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5470105c2039f2210b1a2c9d8e55edfd818f2e42" translate="yes" xml:space="preserve">
          <source>Kernel matrix.</source>
          <target state="translated">Matriz del núcleo.</target>
        </trans-unit>
        <trans-unit id="839a7f66845e964bf2afbaec5611c3402217b903" translate="yes" xml:space="preserve">
          <source>Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function \(k\) (a so called Mercer kernel), it is guaranteed that there exists a mapping \(\phi\) into a Hilbert space \(\mathcal{H}\), such that</source>
          <target state="translated">Los métodos de núcleo como las máquinas de vector de apoyo o el PCA kernelizado se basan en la propiedad de reproducir los espacios del núcleo Hilbert.Para cualquier función positiva definida del kernel (un llamado kernel Mercer),se garantiza que existe un mapeo en el espacio Hilbert,de tal manera que</target>
        </trans-unit>
        <trans-unit id="a3bb404582c234b1b5161269097e65342126edc8" translate="yes" xml:space="preserve">
          <source>Kernel methods to project data into alternate dimensional spaces</source>
          <target state="translated">Métodos de núcleo para proyectar datos en espacios dimensionales alternativos</target>
        </trans-unit>
        <trans-unit id="7853e504e205e94517ed94484ade6d5285c25255" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{sum}(X, Y) = k1(X, Y) + k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{product}(X, Y) = k1(X, Y) * k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(exponent\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^\text{exponent}\).</source>
          <target state="translated">Los operadores de kernel toman uno o dos kernels base y los combinan en un nuevo kernel. El n&amp;uacute;cleo &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt; toma dos n&amp;uacute;cleos \ (k1 \) y \ (k2 \) y los combina a trav&amp;eacute;s de \ (k_ {sum} (X, Y) = k1 (X, Y) + k2 (X, Y) \). El n&amp;uacute;cleo del &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; o&lt;/a&gt; toma dos n&amp;uacute;cleos \ (k1 \) y \ (k2 \) y los combina mediante \ (k_ {producto} (X, Y) = k1 (X, Y) * k2 (X, Y) \). El kernel de &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt; &lt;code&gt;Exponentiation&lt;/code&gt; &lt;/a&gt; toma un kernel base y un par&amp;aacute;metro escalar \ (exponente \) y los combina mediante \ (k_ {exp} (X, Y) = k (X, Y) ^ \ text {exponent} \).</target>
        </trans-unit>
        <trans-unit id="6d4564c4032221fcc796901a96d1a6cf81d6aa2f" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(p\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^p\). Note that magic methods &lt;code&gt;__add__&lt;/code&gt;, &lt;code&gt;__mul___&lt;/code&gt; and &lt;code&gt;__pow__&lt;/code&gt; are overridden on the Kernel objects, so one can use e.g. &lt;code&gt;RBF() + RBF()&lt;/code&gt; as a shortcut for &lt;code&gt;Sum(RBF(), RBF())&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9dc320ddac29ab60da57cafc47693079e4b6b082" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge Regression&lt;/a&gt; (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">La regresi&amp;oacute;n de la cresta del n&amp;uacute;cleo (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combina la &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;regresi&amp;oacute;n de la cresta&lt;/a&gt; (m&amp;iacute;nimos cuadrados lineales con regularizaci&amp;oacute;n de la norma l2) con el truco del n&amp;uacute;cleo. De este modo, aprende una funci&amp;oacute;n lineal en el espacio inducida por el n&amp;uacute;cleo respectivo y los datos. Para los n&amp;uacute;cleos no lineales, esto corresponde a una funci&amp;oacute;n no lineal en el espacio original.</target>
        </trans-unit>
        <trans-unit id="3fcbd037e0e31c66e26fa73fca7d9588d56b4cd8" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge regression and classification&lt;/a&gt; (linear least squares with l2-norm regularization) with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method&quot;&gt;kernel trick&lt;/a&gt;. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d585be11bb912be319b898c908d63ce568dd8c0" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">La regresión de la cresta del núcleo (KRR)combina la regresión de la cresta (mínimos cuadrados lineales con regularización de la norma l2)con el truco del núcleo.De este modo,aprende una función lineal en el espacio inducido por el núcleo respectivo y los datos.En el caso de los núcleos no lineales,esto corresponde a una función no lineal en el espacio original.</target>
        </trans-unit>
        <trans-unit id="262cee695a2ed79939315817b4a3a26823167afe" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression combines ridge regression with the kernel trick</source>
          <target state="translated">La regresión de la cresta del núcleo combina la regresión de la cresta con el truco del núcleo</target>
        </trans-unit>
        <trans-unit id="589ad014b6254add975c198ac204f9560e253ac1" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression.</source>
          <target state="translated">Regresión de la cresta del núcleo.</target>
        </trans-unit>
        <trans-unit id="a797077c9a6730a652ad75f039a934d138c2b41f" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed.</source>
          <target state="translated">Núcleo a utilizar en el modelo:lineal,polinomio,RBF,sigmoide o precalculado.</target>
        </trans-unit>
        <trans-unit id="407ab400408caf91955e34873fdbfe1f6ae14b07" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed. &amp;lsquo;rbf&amp;rsquo; by default.</source>
          <target state="translated">Kernel a utilizar en el modelo: lineal, polinomial, RBF, sigmoide o precomputado. 'rbf' por defecto.</target>
        </trans-unit>
        <trans-unit id="716837a63a81bd1da24c9f2580ff0581777fc381" translate="yes" xml:space="preserve">
          <source>Kernel which is composed of a set of other kernels.</source>
          <target state="translated">Núcleo que se compone de un conjunto de otros núcleos.</target>
        </trans-unit>
        <trans-unit id="a170413f32a293189023e0700b83d22ea6042972" translate="yes" xml:space="preserve">
          <source>Kernel. Default=&amp;rdquo;linear&amp;rdquo;.</source>
          <target state="translated">N&amp;uacute;cleo. Predeterminado = &amp;rdquo;lineal&amp;rdquo;.</target>
        </trans-unit>
        <trans-unit id="e3cb275740ef8ee4f25f4b8b1bb2cb56094f01c1" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">Los kernels (tambi&amp;eacute;n llamados &quot;funciones de covarianza&quot; en el contexto de los GP) son un ingrediente crucial de los GP que determinan la forma del anterior y posterior del GP. Codifican las suposiciones sobre la funci&amp;oacute;n que se est&amp;aacute; aprendiendo definiendo la &quot;similitud&quot; de dos puntos de datos combinados con la suposici&amp;oacute;n de que los puntos de datos similares deben tener valores objetivo similares. Se pueden distinguir dos categor&amp;iacute;as de n&amp;uacute;cleos: los n&amp;uacute;cleos estacionarios dependen solo de la distancia de dos puntos de datos y no de sus valores absolutos \ (k (x_i, x_j) = k (d (x_i, x_j)) \) y por lo tanto son invariantes a las traducciones en el espacio de entrada, mientras que los n&amp;uacute;cleos no estacionarios dependen tambi&amp;eacute;n de los valores espec&amp;iacute;ficos de los puntos de datos. Los granos estacionarios se pueden subdividir en granos isotr&amp;oacute;picos y anisotr&amp;oacute;picos, donde los granos isotr&amp;oacute;picos tambi&amp;eacute;n son invariantes a las rotaciones en el espacio de entrada. Para m&amp;aacute;s detalles,nos referimos al Cap&amp;iacute;tulo 4 de&lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="98cdf159fe1dcd2fc6aef984c01878bfe7d52c10" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;. For guidance on how to best combine different kernels, we refer to &lt;a href=&quot;#duv2014&quot; id=&quot;id6&quot;&gt;[Duv2014]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ad8dd8fec70a9d46c4f724f1ce47b4b45810363" translate="yes" xml:space="preserve">
          <source>Kernels are measures of similarity, i.e. &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; if objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are considered &amp;ldquo;more similar&amp;rdquo; than objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;. A kernel must also be positive semi-definite.</source>
          <target state="translated">Kernels son medidas de similitud, es decir, &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; si los objetos &lt;code&gt;a&lt;/code&gt; y &lt;code&gt;b&lt;/code&gt; se consideran &amp;ldquo;m&amp;aacute;s similares&amp;rdquo; que los objetos &lt;code&gt;a&lt;/code&gt; y &lt;code&gt;c&lt;/code&gt; . Un grano tambi&amp;eacute;n debe ser positivo semi-definido.</target>
        </trans-unit>
        <trans-unit id="cd28143394596209b24bd87df6806973641c2997" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="translated">Los kernels est&amp;aacute;n parametrizados por un vector \ (\ theta \) de hiperpar&amp;aacute;metros. Estos hiperpar&amp;aacute;metros pueden, por ejemplo, controlar las escalas de longitud o la periodicidad de un kernel (ver m&amp;aacute;s abajo). Todos los n&amp;uacute;cleos admiten la computaci&amp;oacute;n de gradientes anal&amp;iacute;ticos de la autocovarianza del n&amp;uacute;cleo con respecto a \ (\ theta \) mediante la configuraci&amp;oacute;n de &lt;code&gt;eval_gradient=True&lt;/code&gt; en el m&amp;eacute;todo &lt;code&gt;__call__&lt;/code&gt; . Este gradiente es utilizado por el proceso gaussiano (tanto regresor como clasificador) para calcular el gradiente de la probabilidad log-marginal, que a su vez se usa para determinar el valor de \ (\ theta \), que maximiza el log-marginal- probabilidad, mediante ascenso en pendiente. Para cada hiperpar&amp;aacute;metro, el valor inicial y los l&amp;iacute;mites deben especificarse al crear una instancia del kernel. El valor actual de \ (\ theta \) se puede obtener y establecer mediante la propiedad &lt;code&gt;theta&lt;/code&gt; del objeto del kernel. Adem&amp;aacute;s, se puede acceder a los l&amp;iacute;mites de los hiperpar&amp;aacute;metros mediante los &lt;code&gt;bounds&lt;/code&gt; de propiedad del kernel. Tenga en cuenta que ambas propiedades (theta y l&amp;iacute;mites) devuelven valores transformados logar&amp;iacute;tmicamente de los valores utilizados internamente, ya que suelen ser m&amp;aacute;s susceptibles a la optimizaci&amp;oacute;n basada en gradientes. La especificaci&amp;oacute;n de cada hiperpar&amp;aacute;metro se almacena en forma de una instancia de &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt; &lt;code&gt;Hyperparameter&lt;/code&gt; &lt;/a&gt; en el kernel respectivo. Tenga en cuenta que un kernel que utiliza un hiperpar&amp;aacute;metro con el nombre &quot;x&quot; debe tener los atributos self.xy self.x_bounds.</target>
        </trans-unit>
        <trans-unit id="4aed51cbfb6629b3c22c46504375ed74bc60a033" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a754d09a87b01a5043bf319d676ca0f6cb6a853" translate="yes" xml:space="preserve">
          <source>Kernels:</source>
          <target state="translated">Kernels:</target>
        </trans-unit>
        <trans-unit id="c3b9fc0d0d17c07a841795715ed044ed9e710926" translate="yes" xml:space="preserve">
          <source>Kevin P. Murphy &amp;ldquo;Machine Learning: A Probabilistic Perspective&amp;rdquo;, The MIT Press chapter 14.4.3, pp. 492-493</source>
          <target state="translated">Kevin P. Murphy &quot;Machine Learning: A Probabilistic Perspective&quot;, The MIT Press cap&amp;iacute;tulo 14.4.3, p&amp;aacute;gs. 492-493</target>
        </trans-unit>
        <trans-unit id="1ebff3fd3bf929976eef25f0da78c334d18a2c1d" translate="yes" xml:space="preserve">
          <source>Keys are parameter names that can be passed to &lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;set_config&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Las claves son nombres de par&amp;aacute;metros que se pueden pasar a &lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;set_config&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c16cf0c8b95cb6641127d4ecde39c2d13ee54107" translate="yes" xml:space="preserve">
          <source>Keyword arguments allow to adapt these defaults to specific data sets (see parameters &lt;code&gt;target_name&lt;/code&gt;, &lt;code&gt;data_name&lt;/code&gt;, &lt;code&gt;transpose_data&lt;/code&gt;, and the examples below).</source>
          <target state="translated">Los argumentos de palabras clave permiten adaptar estos valores predeterminados a conjuntos de datos espec&amp;iacute;ficos (consulte los par&amp;aacute;metros &lt;code&gt;target_name&lt;/code&gt; , &lt;code&gt;data_name&lt;/code&gt; , &lt;code&gt;transpose_data&lt;/code&gt; y los ejemplos a continuaci&amp;oacute;n).</target>
        </trans-unit>
        <trans-unit id="6a687df4f73e66be23d8d5cd9810da872c9b92e2" translate="yes" xml:space="preserve">
          <source>Keyword arguments passed to the coordinate descent solver.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee035295632767669037b1fd1546556e8af6cebd" translate="yes" xml:space="preserve">
          <source>Keyword arguments to be passed to matplotlib&amp;rsquo;s &lt;code&gt;plot&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd2209e677c2e2331711a5337dc06706ac2ee537" translate="yes" xml:space="preserve">
          <source>Keyword arguments to pass to specified metric function.</source>
          <target state="translated">Argumentos de palabras clave para pasar a una función métrica específica.</target>
        </trans-unit>
        <trans-unit id="b6574be8c6baa963e814d600a049a18b07924f05" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="translated">Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola y Josh Attenberg (2009). &lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Funci&amp;oacute;n hash para el aprendizaje multitarea a gran escala&lt;/a&gt; . Proc. ICML.</target>
        </trans-unit>
        <trans-unit id="aaf2909b07b71367a7207c2f93060ee37cc58e6c" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;https://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35a95e3949c1091022c84b09bdfaee477e2ca247" translate="yes" xml:space="preserve">
          <source>Kingma, Diederik, and Jimmy Ba. &amp;ldquo;Adam: A method for stochastic</source>
          <target state="translated">Kingma, Diederik y Jimmy Ba. &quot;Adam: un m&amp;eacute;todo para estoc&amp;aacute;stico</target>
        </trans-unit>
        <trans-unit id="7bf0d4f9044d36fbabdb373fe028824c8f48b797" translate="yes" xml:space="preserve">
          <source>Kluger, Y., Basri, R., Chang, J. T., &amp;amp; Gerstein, M. (2003). Spectral biclustering of microarray data: coclustering genes and conditions. Genome research, 13(4), 703-716.</source>
          <target state="translated">Kluger, Y., Basri, R., Chang, JT y Gerstein, M. (2003). Biclustering espectral de datos de microarrays: genes y condiciones de coclustering. Investigaci&amp;oacute;n del genoma, 13 (4), 703-716.</target>
        </trans-unit>
        <trans-unit id="454573718b795c598350a3ed3c4e500004992423" translate="yes" xml:space="preserve">
          <source>Kluger, Yuval, et. al., 2003. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;Spectral biclustering of microarray data: coclustering genes and conditions&lt;/a&gt;.</source>
          <target state="translated">Kluger, Yuval y col. al., 2003. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;Biclustering espectral de datos de microarrays: genes y condiciones de coagrupaci&amp;oacute;n&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c956cdb3811d15bc82b9ab562e4744234449e302" translate="yes" xml:space="preserve">
          <source>Knowing only the number of samples, the &lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt;&lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;&lt;/a&gt; estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection:</source>
          <target state="translated">Conociendo solo el n&amp;uacute;mero de muestras, &lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt; &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt; &lt;/a&gt; estima de forma conservadora el tama&amp;ntilde;o m&amp;iacute;nimo del subespacio aleatorio para garantizar una distorsi&amp;oacute;n acotada introducida por la proyecci&amp;oacute;n aleatoria:</target>
        </trans-unit>
        <trans-unit id="dc8be79b794b57340c1a9b2bf6e67594910f3213" translate="yes" xml:space="preserve">
          <source>Koby Crammer, Yoram Singer. On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal of Machine Learning Research 2, (2001), 265-292</source>
          <target state="translated">Koby Crammer,Yoram Singer.Sobre la implementación algorítmica de las máquinas vectoriales basadas en núcleos multiclases.Journal of Machine Learning Research 2,(2001),265-292</target>
        </trans-unit>
        <trans-unit id="5c3682641cb862b7b72f47a7d095c9e12f698d72" translate="yes" xml:space="preserve">
          <source>Kullback-Leibler divergence after optimization.</source>
          <target state="translated">Divergencia Kullback-Leibler después de la optimización.</target>
        </trans-unit>
        <trans-unit id="58f9065948558949c0307af59f2acaf3f9203c82" translate="yes" xml:space="preserve">
          <source>KulsinskiDistance</source>
          <target state="translated">KulsinskiDistance</target>
        </trans-unit>
        <trans-unit id="cb6565437657bdf8e9b94faf7a832064c7b5f242" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="translated">L-BFGS es un solucionador que se aproxima a la matriz hessiana que representa la derivada parcial de segundo orden de una funci&amp;oacute;n. Adem&amp;aacute;s, se aproxima a la inversa de la matriz de Hesse para realizar actualizaciones de par&amp;aacute;metros. La implementaci&amp;oacute;n utiliza la versi&amp;oacute;n Scipy de &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="554ef38240e48c4335936815621409310d3aac71" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bcf28acc035a046a1884f5a68a7e0642aed3a3f" translate="yes" xml:space="preserve">
          <source>L-BFGS-B &amp;ndash; Software for Large-scale Bound-constrained Optimization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9d5151f1c406ba9642eb6d20ad7472462d4b8c9" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning 24, pages 123-140, 1996.</source>
          <target state="translated">L. Breiman, &quot;Bagging predictors&quot;, Machine Learning 24, p&amp;aacute;ginas 123-140, 1996.</target>
        </trans-unit>
        <trans-unit id="05401786a74b32c74f5aaf77879ff5fe2a1ce4dc" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning, 24(2), 123-140, 1996.</source>
          <target state="translated">L. Breiman, &quot;Bagging predictors&quot;, Machine Learning, 24 (2), 123-140, 1996.</target>
        </trans-unit>
        <trans-unit id="ae813a657051355d781d3ca7a4417546370b5fb0" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Pasting small votes for classification in large databases and on-line&amp;rdquo;, Machine Learning, 36(1), 85-103, 1999.</source>
          <target state="translated">L. Breiman, &quot;Pegar peque&amp;ntilde;os votos para la clasificaci&amp;oacute;n en grandes bases de datos y en l&amp;iacute;nea&quot;, Machine Learning, 36 (1), 85-103, 1999.</target>
        </trans-unit>
        <trans-unit id="97e482bcc046e44b1e543a4a852a328e802cd962" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Random Forests&amp;rdquo;, Machine Learning, 45(1), 5-32, 2001. &lt;a href=&quot;https://doi.org/10.1023/A:1010933404324&quot;&gt;https://doi.org/10.1023/A:1010933404324&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93aaad4c8bcdef78f99bc463e879b251fb063491" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &amp;ldquo;Classification and Regression Trees&amp;rdquo;, Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L. Breiman, J. Friedman, R. Olshen y C. Stone, &quot;&amp;Aacute;rboles de clasificaci&amp;oacute;n y regresi&amp;oacute;n&quot;, Wadsworth, Belmont, CA, 1984.</target>
        </trans-unit>
        <trans-unit id="728ad1a9616394c8f19b0d53311780e8eed780ec" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L.Breiman,J.Friedman,R.Olshen y C.Stone.Árboles de clasificación y regresión.Wadsworth,Belmont,CA,1984.</target>
        </trans-unit>
        <trans-unit id="26e831dbfd841f8bca5cddecddc5d95f765adc3b" translate="yes" xml:space="preserve">
          <source>L. Breiman, P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;Submodel selection and evaluation in regression: The X-random case&lt;/a&gt;, International Statistical Review 1992;</source>
          <target state="translated">L. Breiman, P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;Selecci&amp;oacute;n y evaluaci&amp;oacute;n de submodelos en regresi&amp;oacute;n: el caso X-random&lt;/a&gt; , International Statistical Review 1992;</target>
        </trans-unit>
        <trans-unit id="da524759b928a0c6c0410a2ba55315d0723efbf9" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="translated">L. Breiman y A. Cutler, &quot;Random Forests&quot;, &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c982df17d29f8aba32aa6a03bfa726201c066e60" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ecce2961bb8a3f0ec10fc321ad8811dfc6312ac" translate="yes" xml:space="preserve">
          <source>L. F. Kozachenko, N. N. Leonenko, &amp;ldquo;Sample Estimate of the Entropy of a Random Vector&amp;rdquo;, Probl. Peredachi Inf., 23:2 (1987), 9-16</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d97aac3a80efb6d42fc11cefc484a2c681583627" translate="yes" xml:space="preserve">
          <source>L. F. Kozachenko, N. N. Leonenko, &amp;ldquo;Sample Estimate of the Entropy of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f170f61c9bead94cf287d881c88071c0e3a5501e" translate="yes" xml:space="preserve">
          <source>L. Hubert and P. Arabie, Comparing Partitions, Journal of Classification 1985 &lt;a href=&quot;https://link.springer.com/article/10.1007%2FBF01908075&quot;&gt;https://link.springer.com/article/10.1007%2FBF01908075&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9026b644be7a2edc54521dca8f44e2af501befa2" translate="yes" xml:space="preserve">
          <source>L. Mosley, &lt;a href=&quot;https://lib.dr.iastate.edu/etd/13537/&quot;&gt;A balanced approach to the multi-class imbalance problem&lt;/a&gt;, IJCV 2010.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27e9c034667fd587e63afe1b6bd9ac5dd761c4eb" translate="yes" xml:space="preserve">
          <source>L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C. Moore, John DeNero.</source>
          <target state="translated">Regularización de L1 y L2 para modelos de pérdida de bisagras multiclase por Robert C.Moore,John DeNero.</target>
        </trans-unit>
        <trans-unit id="739dce23f089e2bc4737d849cf6e6812aaac6b25" translate="yes" xml:space="preserve">
          <source>L1 Penalty and Sparsity in Logistic Regression</source>
          <target state="translated">L1 Penalización y escasez en la regresión logística</target>
        </trans-unit>
        <trans-unit id="8d79d7e84774c8797e94aafbcec78896f21a814d" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{i=1}^{n} |w_i|\), which leads to sparse solutions.</source>
          <target state="translated">Norma L1:\N-R(w):=\N-suma de las cifras.|que conduce a soluciones escasas.</target>
        </trans-unit>
        <trans-unit id="b5bea6bb158694d1df1789af92353788b4accacc" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{j=1}^{m} |w_j|\), which leads to sparse solutions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae8ca0f194d88f499adeb94f8b5c01af62268b9f" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2\),</source>
          <target state="translated">Norma L2:\ ~-(R(w):=\ ~ \ ~ fracaso 1}{\a6}{\a6}\N-w_i^2\N,</target>
        </trans-unit>
        <trans-unit id="2f4d6f15c348b07a6c4412e8ecac45c72eb1770b" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{j=1}^{m} w_j^2 = ||w||_2^2\),</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e55996560b375d2b1311657b3550d521d2224094" translate="yes" xml:space="preserve">
          <source>L2 penalty (regularization term) parameter.</source>
          <target state="translated">Parámetro de penalización L2 (término de regularización).</target>
        </trans-unit>
        <trans-unit id="512ddf6d4bbcf9517a6def433a9acfdaefb1e3cd" translate="yes" xml:space="preserve">
          <source>LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all \(k\). This reduces the log posterior to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d7eb4b58ee70885659f8b6dfa6b739d18b840b6" translate="yes" xml:space="preserve">
          <source>LIBLINEAR &amp;ndash; A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR: una biblioteca para grandes clasificaciones lineales</target>
        </trans-unit>
        <trans-unit id="23f600324ae930d885bf27049a430c382dc77087" translate="yes" xml:space="preserve">
          <source>LIBLINEAR: A Library for Large Linear Classification</source>
          <target state="translated">LIBERAL:Una biblioteca para la clasificación lineal grande</target>
        </trans-unit>
        <trans-unit id="919f2c891fd7b6ae4005ef3cab68511f9b26c031" translate="yes" xml:space="preserve">
          <source>LIBSVM: A Library for Support Vector Machines</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f7204b5759b40e38407ab9bdcb1553f2d733475" translate="yes" xml:space="preserve">
          <source>LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes.</source>
          <target state="translated">El LSA también se conoce como indexación semántica latente,LSI,aunque estrictamente eso se refiere a su uso en índices persistentes para fines de recuperación de información.</target>
        </trans-unit>
        <trans-unit id="f4a5095ae748443324845cf5a2f1b28d147ed2ca" translate="yes" xml:space="preserve">
          <source>LSH Forest being an approximate method, some true neighbors from the indexed dataset might be missing from the results.</source>
          <target state="translated">Siendo el Bosque LSH un método aproximado,algunos verdaderos vecinos del conjunto de datos indexados podrían faltar en los resultados.</target>
        </trans-unit>
        <trans-unit id="afceea8d4c81422ac802414c94f3f49075a51ec4" translate="yes" xml:space="preserve">
          <source>LSH Forest: Locality Sensitive Hashing forest [1] is an alternative method for vanilla approximate nearest neighbor search methods. LSH forest data structure has been implemented using sorted arrays and binary search and 32 bit fixed-length hashes. Random projection is used as the hash family which approximates cosine distance.</source>
          <target state="translated">Bosque LSH:Bosque Hashing sensible a la localidad [1]es un método alternativo para la búsqueda de vainilla aproximada del vecino más cercano.La estructura de datos del bosque LSH ha sido implementada usando matrices ordenadas y búsqueda binaria y hashes de 32 bits de longitud fija.Se utiliza la proyección aleatoria como la familia de hashes que se aproxima a la distancia del coseno.</target>
        </trans-unit>
        <trans-unit id="497cbd9196f20980eefacbc5b295901fb0a6c25f" translate="yes" xml:space="preserve">
          <source>LSTAT % lower status of the population</source>
          <target state="translated">LSTAT % estatus inferior de la población</target>
        </trans-unit>
        <trans-unit id="10e8ec7cf1b34af007bc1d6b016abc85aa0b454d" translate="yes" xml:space="preserve">
          <source>Label Propagation classifier</source>
          <target state="translated">Etiqueta Clasificador de propagación</target>
        </trans-unit>
        <trans-unit id="abaf5a09ed6812e5734e77c1313bb44d953f5d5d" translate="yes" xml:space="preserve">
          <source>Label Propagation digits active learning</source>
          <target state="translated">Propagación de la etiqueta dígitos aprendizaje activo</target>
        </trans-unit>
        <trans-unit id="a45a75b5c87b437cf487b153831ce5b94e5322d0" translate="yes" xml:space="preserve">
          <source>Label Propagation digits: Demonstrating performance</source>
          <target state="translated">Cifras de propagación de la etiqueta:Demostración de rendimiento</target>
        </trans-unit>
        <trans-unit id="f15baf6416f92a52b1527f1d28d49a335fe3d388" translate="yes" xml:space="preserve">
          <source>Label Propagation learning a complex structure</source>
          <target state="translated">Propagación de etiquetas aprendiendo una estructura compleja</target>
        </trans-unit>
        <trans-unit id="5f24cba3626113f57fbd8f2c1a1dac90f055831d" translate="yes" xml:space="preserve">
          <source>Label assigned to each item via the transduction.</source>
          <target state="translated">Etiqueta asignada a cada artículo a través de la transducción.</target>
        </trans-unit>
        <trans-unit id="0154541a5d5e8e0b2444f876377737f91ad447a9" translate="yes" xml:space="preserve">
          <source>Label considered as positive and others are considered negative.</source>
          <target state="translated">La etiqueta se considera positiva y las demás se consideran negativas.</target>
        </trans-unit>
        <trans-unit id="e1c383c45e91a1b41ae4aea8504e1ff71ada889a" translate="yes" xml:space="preserve">
          <source>Label is 1 for an inlier and -1 for an outlier according to the LOF score and the contamination parameter.</source>
          <target state="translated">La etiqueta es 1 para un valor atípico y -1 para un valor atípico según la puntuación de la LOF y el parámetro de contaminación.</target>
        </trans-unit>
        <trans-unit id="0facd2ec455a2234134eaa8ce0e172bf077ca396" translate="yes" xml:space="preserve">
          <source>Label of the positive class. Defaults to the greater label unless y_true is all 0 or all -1 in which case pos_label defaults to 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a4a633c5d3b5ebf2a9c4453fb41f8475e350bc9" translate="yes" xml:space="preserve">
          <source>Label of the positive class. If None, the maximum label is used as positive class</source>
          <target state="translated">Etiqueta de la clase positiva.Si no hay ninguno,la etiqueta máxima se utiliza como clase positiva</target>
        </trans-unit>
        <trans-unit id="91ed314c98998b774c857769b601470c2a4233d0" translate="yes" xml:space="preserve">
          <source>Label propagation denotes a few variations of semi-supervised graph inference algorithms.</source>
          <target state="translated">La propagación de etiquetas denota unas pocas variaciones de algoritmos de inferencia gráfica semisupervisados.</target>
        </trans-unit>
        <trans-unit id="3a4c36d2f1914cbaa6f86d2f3e759e05f747e6f8" translate="yes" xml:space="preserve">
          <source>Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available:</source>
          <target state="translated">Los modelos de propagación de etiquetas tienen dos métodos de núcleo incorporados.La elección del núcleo afecta tanto a la escalabilidad como al rendimiento de los algoritmos.Los siguientes están disponibles:</target>
        </trans-unit>
        <trans-unit id="d9c8943fba1565dfa00ecc788417147c59e84b5a" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;mean reciprocal rank&lt;/a&gt;.</source>
          <target state="translated">La precisi&amp;oacute;n promedio de clasificaci&amp;oacute;n de etiquetas (LRAP) promedia sobre las muestras la respuesta a la siguiente pregunta: para cada etiqueta de verdad del terreno, &amp;iquest;qu&amp;eacute; fracci&amp;oacute;n de las etiquetas de mayor rango eran etiquetas verdaderas? Esta medida de rendimiento ser&amp;aacute; m&amp;aacute;s alta si puede dar una mejor clasificaci&amp;oacute;n a las etiquetas asociadas con cada muestra. La puntuaci&amp;oacute;n obtenida es siempre estrictamente mayor que 0, y el mejor valor es 1. Si hay exactamente una etiqueta relevante por muestra, la precisi&amp;oacute;n promedio de clasificaci&amp;oacute;n de etiquetas es equivalente a la &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;clasificaci&amp;oacute;n rec&amp;iacute;proca media&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="12d27d4c8cd4504c7079d27029449977fea3fa44" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score.</source>
          <target state="translated">La precisión media de la clasificación de etiquetas (LRAP)es el promedio de cada etiqueta de verdad asignada a cada muestra,de la relación entre las etiquetas de verdad y las etiquetas totales con menor puntuación.</target>
        </trans-unit>
        <trans-unit id="57882529b52287495d04cf4c6bba559a970b02d4" translate="yes" xml:space="preserve">
          <source>Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected.</source>
          <target state="translated">Etiqueta,que se da para las muestras atípicas (muestras sin vecinos en un radio determinado).Si se establece en Ninguno,se eleva el valor de Error,cuando se detecta un valor atípico.</target>
        </trans-unit>
        <trans-unit id="a3ea7d5af24c9f7706e04a90b4cc006ad64537bf" translate="yes" xml:space="preserve">
          <source>LabelSpreading model for semi-supervised learning</source>
          <target state="translated">Modelo de difusión de etiquetas para el aprendizaje semisupervisado</target>
        </trans-unit>
        <trans-unit id="82b6583f37d4a090f2277f71261de91f41eff15e" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized:</source>
          <target state="translated">Las etiquetas que asignan a todos los miembros de las clases a los mismos grupos son completas no ser siempre puras,por lo tanto se penalizan:</target>
        </trans-unit>
        <trans-unit id="a59d28cce33bc578e32e7790445917276a69fe16" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized:</source>
          <target state="translated">Las etiquetas que asignan a todos los miembros de las clases a los mismos grupos son completas no son homogéneas,por lo que se penalizan:</target>
        </trans-unit>
        <trans-unit id="2625047637f13a503b1aa26353d53ce007980d47" translate="yes" xml:space="preserve">
          <source>Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well:</source>
          <target state="translated">Los etiquetados que tienen agrupaciones puras con miembros que provienen de las mismas clases son homogéneos,pero las divisiones innecesarias perjudican la integridad y,por lo tanto,penalizan también la medida V:</target>
        </trans-unit>
        <trans-unit id="040e8af7f9faa240f939c7eb15dd2f3691882d68" translate="yes" xml:space="preserve">
          <source>Labelled data.</source>
          <target state="translated">Datos etiquetados.</target>
        </trans-unit>
        <trans-unit id="a8a910f7e8e66128e5f0f93a7ebe3b1d5812067b" translate="yes" xml:space="preserve">
          <source>Labelling a new sample is performed by finding the nearest centroid for a given sample.</source>
          <target state="translated">El etiquetado de una nueva muestra se realiza encontrando el centroide más cercano para una muestra determinada.</target>
        </trans-unit>
        <trans-unit id="47fc9fa69e29f326a363aa6376f6761fa85e0797" translate="yes" xml:space="preserve">
          <source>Labels assigned by the first annotator.</source>
          <target state="translated">Etiquetas asignadas por el primer anotador.</target>
        </trans-unit>
        <trans-unit id="bdb7346e56bb733f97e8f0b9d11cce2ffadf9042" translate="yes" xml:space="preserve">
          <source>Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping &lt;code&gt;y1&lt;/code&gt; and &lt;code&gt;y2&lt;/code&gt; doesn&amp;rsquo;t change the value.</source>
          <target state="translated">Etiquetas asignadas por el segundo anotador. La estad&amp;iacute;stica kappa es sim&amp;eacute;trica, por lo que intercambiar &lt;code&gt;y1&lt;/code&gt; e &lt;code&gt;y2&lt;/code&gt; no cambia el valor.</target>
        </trans-unit>
        <trans-unit id="202396c3dbc4d15cb0462523b4fd7f2f49834479" translate="yes" xml:space="preserve">
          <source>Labels assigned to the centroids of the subclusters after they are clustered globally.</source>
          <target state="translated">Etiquetas asignadas a los centroides de los subconjuntos después de que se agrupen globalmente.</target>
        </trans-unit>
        <trans-unit id="86a5303314971b15773b1ad8460967a7978fc1e6" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels are ranging from 0-39 and correspond to the Subject IDs.</source>
          <target state="translated">Etiquetas asociadas a cada imagen de la cara.Esas etiquetas van de 0 a 39 y corresponden a las identificaciones de los sujetos.</target>
        </trans-unit>
        <trans-unit id="b8a8237c586e7a43e02e7a221af16786bca65b16" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels range from 0-5748 and correspond to the person IDs.</source>
          <target state="translated">Etiquetas asociadas a cada imagen de la cara.Esas etiquetas van de 0 a 5748 y corresponden a las identificaciones de las personas.</target>
        </trans-unit>
        <trans-unit id="639c7a5f12221be9fa16d4184a91d960ee8d5fb6" translate="yes" xml:space="preserve">
          <source>Labels associated to each pair of images. The two label values being different persons or the same person.</source>
          <target state="translated">Etiquetas asociadas a cada par de imágenes.Los dos valores de la etiqueta son personas diferentes o la misma persona.</target>
        </trans-unit>
        <trans-unit id="dd9359ae6e29bf7b087516560ad1a2e91d10cfb0" translate="yes" xml:space="preserve">
          <source>Labels for X.</source>
          <target state="translated">Etiquetas para la X.</target>
        </trans-unit>
        <trans-unit id="0b53b6571e9267409e85ff23873e0a0824df02a7" translate="yes" xml:space="preserve">
          <source>Labels of each point</source>
          <target state="translated">Las etiquetas de cada punto</target>
        </trans-unit>
        <trans-unit id="4350a7104cda6c17ed013efe2d00ebaae03eeb73" translate="yes" xml:space="preserve">
          <source>Labels of each point (if compute_labels is set to True).</source>
          <target state="translated">Etiquetas de cada punto (si compute_labels se establece en True).</target>
        </trans-unit>
        <trans-unit id="8c76fdcbe4be61c2bbf79d2e67413441e31eb988" translate="yes" xml:space="preserve">
          <source>Labels of each point.</source>
          <target state="translated">Etiquetas de cada punto.</target>
        </trans-unit>
        <trans-unit id="9caa2dbfb17c8c2f4ae17aa6bab878223c8520e3" translate="yes" xml:space="preserve">
          <source>Labels to constrain permutation within groups, i.e. &lt;code&gt;y&lt;/code&gt; values are permuted among samples with the same group identifier. When not specified, &lt;code&gt;y&lt;/code&gt; values are permuted among all samples.</source>
          <target state="translated">Etiquetas para restringir la permutaci&amp;oacute;n dentro de los grupos, es decir, los valores de &lt;code&gt;y&lt;/code&gt; se permutan entre muestras con el mismo identificador de grupo. Cuando no se especifica, los valores de &lt;code&gt;y&lt;/code&gt; se permutan entre todas las muestras.</target>
        </trans-unit>
        <trans-unit id="c21c4f0b2fc516030c767721367e1d2fba51e007" translate="yes" xml:space="preserve">
          <source>Labels.</source>
          <target state="translated">Labels.</target>
        </trans-unit>
        <trans-unit id="45efe9972f3bf7c62e3db1678d501faf12d10c1b" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_clusters&lt;/code&gt; and &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">Grandes &lt;code&gt;n_clusters&lt;/code&gt; y &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28e08fa26129e68210c4b016ca6da1b08a1a37e9" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">Grandes &lt;code&gt;n_samples&lt;/code&gt; y &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d40959dcecc27d1d44b2e4cffa59a304e9a052a1" translate="yes" xml:space="preserve">
          <source>Large dataset, outlier removal, data reduction.</source>
          <target state="translated">Gran conjunto de datos,eliminación de valores atípicos,reducción de datos.</target>
        </trans-unit>
        <trans-unit id="8f1784e927c9c4e578edb46d860596eed4a90b35" translate="yes" xml:space="preserve">
          <source>Large outliers</source>
          <target state="translated">Grandes valores atípicos</target>
        </trans-unit>
        <trans-unit id="20dfcd03ef69fe3c6c6e8549019c43956f87d5db" translate="yes" xml:space="preserve">
          <source>Lars computes a path solution only for each kink in the path. As a result, it is very efficient when there are only of few kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting any meta parameter. On the opposite, coordinate descent compute the path points on a pre-specified grid (here we use the default). Thus it is more efficient if the number of grid points is smaller than the number of kinks in the path. Such a strategy can be interesting if the number of features is really large and there are enough samples to select a large amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the coordinate descent algorithm will only sample the path on a grid.</source>
          <target state="translated">Lars calcula una solución del camino sólo para cada curva del camino.Como resultado,es muy eficiente cuando hay sólo unos pocos pliegues,que es el caso si hay pocas características o muestras.Además,es capaz de calcular el camino completo sin establecer ningún meta parámetro.Por el contrario,el descenso de coordenadas calcula los puntos del camino en una cuadrícula preespecificada (aquí utilizamos el valor por defecto).Por lo tanto,es más eficiente si el número de puntos de la cuadrícula es menor que el número de pliegues del camino.Tal estrategia puede ser interesante si el número de características es realmente grande y hay suficientes muestras para seleccionar una gran cantidad.En cuanto a los errores numéricos,para las variables muy correlacionadas,Lars acumulará más errores,mientras que el algoritmo de descenso de coordenadas sólo muestreará el camino en una cuadrícula.</target>
        </trans-unit>
        <trans-unit id="fafbf93538200568ab2506c2a63168c161506b4f" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net</source>
          <target state="translated">Lazo y red elástica</target>
        </trans-unit>
        <trans-unit id="64045413f4cce0f6cc0a64e33254b9beab1142d8" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net for Sparse Signals</source>
          <target state="translated">Lazo y red elástica para señales dispersas</target>
        </trans-unit>
        <trans-unit id="02b3c1dbfc5f6c26007e2282ba4be10a77581a65" translate="yes" xml:space="preserve">
          <source>Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent.</source>
          <target state="translated">Lazo y red elástica (penalización L1 y L2)implementados mediante un descenso coordinado.</target>
        </trans-unit>
        <trans-unit id="721bb6d50a67145009b7e81abd6add7dc9980ff6" translate="yes" xml:space="preserve">
          <source>Lasso computed by least-angle regression</source>
          <target state="translated">El lazo calculado por la regresión del ángulo menor</target>
        </trans-unit>
        <trans-unit id="c805258f4c266592bbe9892ca4c6fe8fe41525e3" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path</source>
          <target state="translated">Modelo lineal de lazo con ajuste iterativo a lo largo de un camino de regularización</target>
        </trans-unit>
        <trans-unit id="47657b9ded4cc2a2b0764459c9d492e6cc3f4eb7" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af3dece2cf6ae684f46dbebc7279e4f62e00335d" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Lars using BIC or AIC for model selection</source>
          <target state="translated">El modelo de lazo encaja con Lars usando BIC o AIC para la selección del modelo</target>
        </trans-unit>
        <trans-unit id="050a0d126029facc258b43169ac1e55a978389bf" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a.</source>
          <target state="translated">El modelo de lazo encaja con la Regresión de Menor Ángulo,también conocida como...</target>
        </trans-unit>
        <trans-unit id="9cd5532bfae0b1e27ef3555196bbd1195b2078fe" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a. Lars</source>
          <target state="translated">El modelo de lazo encaja con la regresión del ángulo mínimo,también conocido como Lars.</target>
        </trans-unit>
        <trans-unit id="7cbdf91f396ae23c8822ab30bdd340882655aa26" translate="yes" xml:space="preserve">
          <source>Lasso model selection: Cross-Validation / AIC / BIC</source>
          <target state="translated">Selección de modelo de lazo:Validación cruzada/AIC/BIC</target>
        </trans-unit>
        <trans-unit id="5632592b831e91b94b8f3294c0115a52d64872b7" translate="yes" xml:space="preserve">
          <source>Lasso models (see the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; User Guide section) estimates sparse coefficients. LassoCV applies cross validation in order to determine which value of the regularization parameter (&lt;code&gt;alpha&lt;/code&gt;) is best suited for the model estimation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51c5bc73e17f640c8a180ee453dbdca923d8c408" translate="yes" xml:space="preserve">
          <source>Lasso on dense and sparse data</source>
          <target state="translated">Lazo en datos densos y escasos</target>
        </trans-unit>
        <trans-unit id="4222e17e965145615293d33dd92e1394e71c2b5b" translate="yes" xml:space="preserve">
          <source>Lasso path using LARS</source>
          <target state="translated">Trayectoria del lazo usando LARS</target>
        </trans-unit>
        <trans-unit id="1acac83cf58491df993404acd51caed4c4458648" translate="yes" xml:space="preserve">
          <source>Lasso using coordinate descent (&lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;)</source>
          <target state="translated">Lazo con descenso de coordenadas ( &lt;a href=&quot;linear_model#lasso&quot;&gt;Lazo&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="e33c33e9d593ce188f7d437b3dd829993a23358f" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.</source>
          <target state="translated">La Asignación de Dirichlets Latentes es un modelo probabilístico generativo para colecciones de conjuntos de datos discretos como corpúsculos de texto.También es un modelo temático que se utiliza para descubrir temas abstractos de una colección de documentos.</target>
        </trans-unit>
        <trans-unit id="b259b9fed25933f3361602dc71394efbeb9d0882" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation with online variational Bayes algorithm</source>
          <target state="translated">Asignación de Dirichlet latente con un algoritmo Bayes de variación en línea</target>
        </trans-unit>
        <trans-unit id="691257140e4ed31a708c6cf301cec44aee34c69f" translate="yes" xml:space="preserve">
          <source>Latent representations of the data.</source>
          <target state="translated">Representaciones latentes de los datos.</target>
        </trans-unit>
        <trans-unit id="7972223ce1d5a83652f334b349de24d196516da5" translate="yes" xml:space="preserve">
          <source>Later you can load back the pickled model (possibly in another Python process) with:</source>
          <target state="translated">Más tarde puedes volver a cargar el modelo encurtido (posiblemente en otro proceso de Python)con:</target>
        </trans-unit>
        <trans-unit id="af705669290f66a0c593b0deebc97c8dff7d4996" translate="yes" xml:space="preserve">
          <source>Later, you can reload the pickled model (possibly in another Python process) with:</source>
          <target state="translated">Más tarde,puedes recargar el modelo encurtido (posiblemente en otro proceso de Python)con:</target>
        </trans-unit>
        <trans-unit id="87b4154b3c380b9ca1fa8f1419dd8e2c1d34065a" translate="yes" xml:space="preserve">
          <source>Latitude house block latitude</source>
          <target state="translated">La casa de la latitud bloquea la latitud</target>
        </trans-unit>
        <trans-unit id="5d6517da9252e690b07eb861ecaf7b79646512be" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">El tama&amp;ntilde;o de la hoja pas&amp;oacute; a &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; o &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; . Esto puede afectar la velocidad de construcci&amp;oacute;n y consulta, as&amp;iacute; como la memoria requerida para almacenar el &amp;aacute;rbol. El valor &amp;oacute;ptimo depende de la naturaleza del problema.</target>
        </trans-unit>
        <trans-unit id="90341c46ba90925b69433ce5faecb3e1b8c85d8c" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;code&gt;BallTree&lt;/code&gt; or &lt;code&gt;KDTree&lt;/code&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adfd1a5c3117b99a14c45a4ae06038fd4593b137" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">El tamaño de la hoja pasó a BallTree o KDTree.Esto puede afectar a la velocidad de la construcción y la consulta,así como la memoria necesaria para almacenar el árbol.El valor óptimo depende de la naturaleza del problema.</target>
        </trans-unit>
        <trans-unit id="2f4f4f9d9992d30c454ebca3af5182554c5dd5d3" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">El tamaño de la hoja pasó a BallTree o cKDTree.Esto puede afectar a la velocidad de la construcción y la consulta,así como la memoria necesaria para almacenar el árbol.El valor óptimo depende de la naturaleza del problema.</target>
        </trans-unit>
        <trans-unit id="31743e5f5ee8b348cb24154ab26179446399d075" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X and returns the transformed data.</source>
          <target state="translated">Aprende un modelo NMF para los datos X y devuelve los datos transformados.</target>
        </trans-unit>
        <trans-unit id="a49199fe15b3d192e2f8e78d2cfcc004b5bb592f" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X.</source>
          <target state="translated">Aprende un modelo NMF para los datos X.</target>
        </trans-unit>
        <trans-unit id="f28a5a2a8197ba712162f1642134c8c32dff12de" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings and transform X.</source>
          <target state="translated">Aprenda una lista de nombres de funciones -&amp;gt; asignaciones de &amp;iacute;ndices y transforme X.</target>
        </trans-unit>
        <trans-unit id="8c410f4ecac33d5545793d5deb3e8b1121157db0" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings.</source>
          <target state="translated">Conozca una lista de nombres de funciones -&amp;gt; asignaciones de &amp;iacute;ndices.</target>
        </trans-unit>
        <trans-unit id="a753afaf1f2a5a0c1c19f381e2c844f4e69ccf16" translate="yes" xml:space="preserve">
          <source>Learn a vocabulary dictionary of all tokens in the raw documents.</source>
          <target state="translated">Aprende un diccionario de vocabulario de todas las fichas de los documentos en bruto.</target>
        </trans-unit>
        <trans-unit id="d9349583a45dc48570d0d3236e8faf8ecfef570b" translate="yes" xml:space="preserve">
          <source>Learn and apply the dimension reduction on the train data.</source>
          <target state="translated">Aprende y aplica la reducción de dimensiones en los datos del tren.</target>
        </trans-unit>
        <trans-unit id="c8e9cfdd99f37695b9bb2a2cf234f653fe10a376" translate="yes" xml:space="preserve">
          <source>Learn empirical variances from X.</source>
          <target state="translated">Aprende las variaciones empíricas de X.</target>
        </trans-unit>
        <trans-unit id="650a6ae9c550e7f32470024973e3b36aee2841fa" translate="yes" xml:space="preserve">
          <source>Learn model for the data X with variational Bayes method.</source>
          <target state="translated">Aprende el modelo para los datos X con el método variacional de Bayes.</target>
        </trans-unit>
        <trans-unit id="dacb80f7c7ce4a5db80b953f259da8b386886101" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights)</source>
          <target state="translated">Aprende el vector idf (pesos de término global)</target>
        </trans-unit>
        <trans-unit id="fb7b507c119ba0834ef410110951b80a51da9b63" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b331e0a3149fc26c2099c41ac3e9655d530c7a47" translate="yes" xml:space="preserve">
          <source>Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)</source>
          <target state="translated">Aprende la transformación inversa para los núcleos no precalculados.(es decir,aprender a encontrar la imagen previa de un punto)</target>
        </trans-unit>
        <trans-unit id="351ef97b73f57653764681dfe2a94603d5d1cbab" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return document-term matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee96e1f94ac61b3bff29cbb75afd2fdb8a437bed" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return term-document matrix.</source>
          <target state="translated">Aprende el diccionario de vocabulario y devuelve la matriz de términos-documentos.</target>
        </trans-unit>
        <trans-unit id="65adc2e107d7d619d7107cf14005ab5e9c9cd5ef" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf from training set.</source>
          <target state="translated">Aprende el vocabulario y el idf del set de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="51c9b9cb5d5b207d3afa5b6e39e7500c1ff633ed" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return document-term matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9ba5b6f4cc6cff1198de21973fdc3c62d64336f" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return term-document matrix.</source>
          <target state="translated">Aprende el vocabulario y el idf,devuelve la matriz de términos-documentos.</target>
        </trans-unit>
        <trans-unit id="5b86400dde56a045e486ecc10d7618c7daf0f573" translate="yes" xml:space="preserve">
          <source>Learning a graph structure</source>
          <target state="translated">Aprendiendo la estructura de un gráfico</target>
        </trans-unit>
        <trans-unit id="4ecad9f15b8037b0486e20e0cb489ddd61caeca5" translate="yes" xml:space="preserve">
          <source>Learning an embedding</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f89176d3f1741099f1479699aa585a0c6906b634" translate="yes" xml:space="preserve">
          <source>Learning and predicting</source>
          <target state="translated">Aprender y predecir</target>
        </trans-unit>
        <trans-unit id="5087c606edcdf30c07ac8bd6a14c9b96c0975b25" translate="yes" xml:space="preserve">
          <source>Learning curve.</source>
          <target state="translated">Curva de aprendizaje.</target>
        </trans-unit>
        <trans-unit id="af86142d107ea3e7d568509ca68cbef348748b05" translate="yes" xml:space="preserve">
          <source>Learning problems fall into a few categories:</source>
          <target state="translated">Los problemas de aprendizaje se dividen en varias categorías:</target>
        </trans-unit>
        <trans-unit id="213b18cf4e4c891522544c2231435e470a8853a1" translate="yes" xml:space="preserve">
          <source>Learning rate schedule for weight updates.</source>
          <target state="translated">Calendario de tasas de aprendizaje para las actualizaciones de peso.</target>
        </trans-unit>
        <trans-unit id="ad3970bc51aa8e2c82bc13dcb9d4922e01a06590" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each classifier by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">La tasa de aprendizaje reduce la contribuci&amp;oacute;n de cada clasificador por tasa de &lt;code&gt;learning_rate&lt;/code&gt; . Existe una compensaci&amp;oacute;n entre &lt;code&gt;learning_rate&lt;/code&gt; y &lt;code&gt;n_estimators&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cc055e36b16b7ea8669e5252649d8e3ee1af0b11" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each regressor by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">La tasa de aprendizaje reduce la contribuci&amp;oacute;n de cada regresor en tasa de &lt;code&gt;learning_rate&lt;/code&gt; . Existe una compensaci&amp;oacute;n entre &lt;code&gt;learning_rate&lt;/code&gt; y &lt;code&gt;n_estimators&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8982fb177d3b5a895540d84670a324c9b8376572" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.</source>
          <target state="translated">Aprender los par&amp;aacute;metros de una funci&amp;oacute;n de predicci&amp;oacute;n y probarlos con los mismos datos es un error metodol&amp;oacute;gico: un modelo que simplemente repetir&amp;iacute;a las etiquetas de las muestras que acaba de ver tendr&amp;iacute;a una puntuaci&amp;oacute;n perfecta, pero no podr&amp;iacute;a predecir nada &amp;uacute;til todav&amp;iacute;a. datos no vistos. Esta situaci&amp;oacute;n se llama &lt;strong&gt;sobreajuste&lt;/strong&gt; . Para evitarlo, es una pr&amp;aacute;ctica com&amp;uacute;n cuando se realiza un experimento de aprendizaje autom&amp;aacute;tico (supervisado) para mantener parte de los datos disponibles como un &lt;strong&gt;conjunto de prueba &lt;/strong&gt; &lt;code&gt;X_test, y_test&lt;/code&gt; . Tenga en cuenta que la palabra &quot;experimento&quot; no est&amp;aacute; destinada a denotar solo el uso acad&amp;eacute;mico, porque incluso en entornos comerciales, el aprendizaje autom&amp;aacute;tico generalmente comienza de manera experimental.</target>
        </trans-unit>
        <trans-unit id="7ed56c1456833faed4202c795166989f5307ee69" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by &lt;a href=&quot;grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; techniques.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93c3e1794e48ba7d8637b32d813e97686cf36d4f" translate="yes" xml:space="preserve">
          <source>Learns each output independently rather than chaining.</source>
          <target state="translated">Aprende cada salida de forma independiente en lugar de encadenarla.</target>
        </trans-unit>
        <trans-unit id="5fce8b00092369b98dfb920b76a7ee0efe5e00b1" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a.</source>
          <target state="translated">Modelo de Regresión de Menor Ángulo,también conocido como...</target>
        </trans-unit>
        <trans-unit id="3b28e26eb21f16fdbdefabf1ed5ad375edeafb8b" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a. LAR</source>
          <target state="translated">Modelo de regresión de ángulo mínimo,también conocido como LAR.</target>
        </trans-unit>
        <trans-unit id="b8ab306ac662259fba4aa6725b193c75be140b61" translate="yes" xml:space="preserve">
          <source>Least Squares projection of the data onto the sparse components.</source>
          <target state="translated">Proyección de los datos en los componentes dispersos.</target>
        </trans-unit>
        <trans-unit id="2c3aa035aea93ac3dc79ecee5528b7c8dcfba4ab" translate="yes" xml:space="preserve">
          <source>Least absolute deviation (&lt;code&gt;'lad'&lt;/code&gt;): A robust loss function for regression. The initial model is given by the median of the target values.</source>
          <target state="translated">Desviaci&amp;oacute;n m&amp;iacute;nima absoluta ( &lt;code&gt;'lad'&lt;/code&gt; ): una funci&amp;oacute;n de p&amp;eacute;rdida robusta para la regresi&amp;oacute;n. El modelo inicial viene dado por la mediana de los valores objetivo.</target>
        </trans-unit>
        <trans-unit id="3aeaacb76e6b5d496047f324133ddd0747e1d2c6" translate="yes" xml:space="preserve">
          <source>Least squares (&lt;code&gt;'ls'&lt;/code&gt;): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.</source>
          <target state="translated">M&amp;iacute;nimos cuadrados ( &lt;code&gt;'ls'&lt;/code&gt; ): la elecci&amp;oacute;n natural para la regresi&amp;oacute;n debido a sus propiedades computacionales superiores. El modelo inicial viene dado por la media de los valores objetivo.</target>
        </trans-unit>
        <trans-unit id="972ad47a68ab0dd02c8d4f9c32f25ef79120c408" translate="yes" xml:space="preserve">
          <source>Least-Squares: Linear regression (Ridge or Lasso depending on \(R\)). \(L(y_i, f(x_i)) = \frac{1}{2}(y_i - f(x_i))^2\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7963186b092849241b779637d34ce64214b0375a" translate="yes" xml:space="preserve">
          <source>Least-Squares: Ridge Regression.</source>
          <target state="translated">Cuadrados mínimos:Regresión de la cresta.</target>
        </trans-unit>
        <trans-unit id="acf6db0396d489bb160af474285d57fb823df68a" translate="yes" xml:space="preserve">
          <source>Least-angle regression (&lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt;)</source>
          <target state="translated">Regresi&amp;oacute;n de &amp;aacute;ngulo m&amp;iacute;nimo (regresi&amp;oacute;n de &lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;&amp;aacute;ngulo m&amp;iacute;nimo&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="8427893bdf84ecb2b84085547aa1cfdd8fd0e807" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="818f02ffe71d576f8833c06d3318f5d50790be37" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the predictor most correlated with the response. When there are multiple predictors having equal correlation, instead of continuing along the same predictor, it proceeds in a direction equiangular between the predictors.</source>
          <target state="translated">La regresión de ángulo mínimo (LARS)es un algoritmo de regresión para datos de alta dimensión,desarrollado por Bradley Efron,Trevor Hastie,Iain Johnstone y Robert Tibshirani.LARS es similar a la regresión por pasos hacia adelante.En cada paso,encuentra el predictor más correlacionado con la respuesta.Cuando hay múltiples predictores que tienen igual correlación,en lugar de continuar a lo largo del mismo predictor,procede en una dirección equiangular entre los predictores.</target>
        </trans-unit>
        <trans-unit id="5cc9936fd171dfb4c941611970a01e31c9182cee" translate="yes" xml:space="preserve">
          <source>Leave One Group Out cross-validator</source>
          <target state="translated">Deje un grupo fuera del validador cruzado</target>
        </trans-unit>
        <trans-unit id="708b3ff9ed12b2c6f3635d37f516d672f76ad26e" translate="yes" xml:space="preserve">
          <source>Leave P Group(s) Out cross-validator</source>
          <target state="translated">Deje el grupo(s)P fuera del validador cruzado</target>
        </trans-unit>
        <trans-unit id="b1d423c90dfa79c0db1cf2e91d8b80c110d2debb" translate="yes" xml:space="preserve">
          <source>Leave P groups out.</source>
          <target state="translated">Deje los grupos P fuera.</target>
        </trans-unit>
        <trans-unit id="2e788c12c63436d5bbf2b3d54792d07b4ad5906d" translate="yes" xml:space="preserve">
          <source>Leave P observations out.</source>
          <target state="translated">Deje las observaciones P fuera.</target>
        </trans-unit>
        <trans-unit id="23a4dfbb0e55172e2c29fa75763519463b465b57" translate="yes" xml:space="preserve">
          <source>Leave one observation out.</source>
          <target state="translated">Deje una observación fuera.</target>
        </trans-unit>
        <trans-unit id="96e7c056605d5580183d915f0e8250d81cc4028b" translate="yes" xml:space="preserve">
          <source>Leave-One-Out cross-validator</source>
          <target state="translated">El validador cruzado de &quot;Leave-One-Out&quot;...</target>
        </trans-unit>
        <trans-unit id="a3d5fb094bf6540a5945dfebfc612a40422d0970" translate="yes" xml:space="preserve">
          <source>Leave-P-Out cross-validator</source>
          <target state="translated">El validador cruzado Leave-P-Out</target>
        </trans-unit>
        <trans-unit id="7fa92633d7eb4070a1a9e7f3ffd6a6dd808d5514" translate="yes" xml:space="preserve">
          <source>Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.</source>
          <target state="translated">Ledoit O,Wolf M.Honey,encogí la matriz de covarianza de la muestra.The Journal of Portfolio Management 30(4),110-119,2004.</target>
        </trans-unit>
        <trans-unit id="b6a08e295c1dafc447ef93ac82d0e6a70b01528e" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf&amp;rsquo;s formula as described in &amp;ldquo;A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices&amp;rdquo;, Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.</source>
          <target state="translated">Ledoit-Wolf es una forma particular de contracci&amp;oacute;n, donde el coeficiente de contracci&amp;oacute;n se calcula utilizando la f&amp;oacute;rmula de O. Ledoit y M. Wolf como se describe en &quot;Un estimador bien condicionado para matrices de covarianza de gran dimensi&amp;oacute;n&quot;, Ledoit y Wolf, Journal of Multivariate Analysis , Volumen 88, N&amp;uacute;mero 2, febrero de 2004, p&amp;aacute;ginas 365-411.</target>
        </trans-unit>
        <trans-unit id="b450ff5574aa7547a2d2804a59fde9043d1f11e3" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf vs OAS estimation</source>
          <target state="translated">Estimación de Ledoit-Wolf vs.OAS</target>
        </trans-unit>
        <trans-unit id="74b56641357b357e1a04f8ba20caa0211258f1b9" translate="yes" xml:space="preserve">
          <source>LedoitWolf Estimator</source>
          <target state="translated">Estimador LedoitWolf</target>
        </trans-unit>
        <trans-unit id="a7127a921977497178bbe9d19b374d5b3660e695" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y)</source>
          <target state="translated">Argumento izquierdo del kernel devuelto k(X,Y)</target>
        </trans-unit>
        <trans-unit id="db46139863e59ff060f10e61ce009637ab35e3fe" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9bf558c9b2f1ab98bdd863d46c825f77b8bcb622" translate="yes" xml:space="preserve">
          <source>Left to right.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75bf879e9683d8e42f9cdbce4ac2378477aafa4c" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;</source>
          <target state="translated">Longitud del camino. &lt;code&gt;eps=1e-3&lt;/code&gt; significa que &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c25fd55e85b58584519914fbcbbc8e0b71dacb4b" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;.</source>
          <target state="translated">Longitud del camino. &lt;code&gt;eps=1e-3&lt;/code&gt; significa que &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="817bbec7d68f2acac91e1c4383d071e50b618e53" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="820f47ab7dc5f77aa60df5d42cf3669d7140be19" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters:</source>
          <target state="translated">Menos sensibilidad al número de parámetros:</target>
        </trans-unit>
        <trans-unit id="5f1b602bd58c70400c6cc3123702692e92eee6c0" translate="yes" xml:space="preserve">
          <source>Lessons learned</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="650648dcfa58ca5d69540fc9d7c76c71c03cdd8d" translate="yes" xml:space="preserve">
          <source>Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a function mapping x to a Hilbert space. KernelCenterer centers (i.e., normalize to have zero mean) the data without explicitly computing phi(x). It is equivalent to centering phi(x) with sklearn.preprocessing.StandardScaler(with_std=False).</source>
          <target state="translated">Dejemos que K(x,z)sea un núcleo definido por phi(x)^T phi(z),donde phi es una función que asigna x a un espacio Hilbert.El KernelCenterer centra (es decir,normaliza para tener una media cero)los datos sin computar explícitamente phi(x).Es equivalente a centrar phi(x)con sklearn.preprocessing.StandardScaler(with_std=False).</target>
        </trans-unit>
        <trans-unit id="821c4d001d9578c562e1b0af64f8be6da39e632f" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(\sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d3e4f22e7563805ce13fd93e247765bb5c10653" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="translated">Sea \ (S \) la matriz de similitud y \ (X \) las coordenadas de los \ (n \) puntos de entrada. Las disparidades \ (\ hat {d} _ {ij} \) son la transformaci&amp;oacute;n de las similitudes elegidas de algunas formas &amp;oacute;ptimas. El objetivo, llamado estr&amp;eacute;s, es definido por \ (sum_ {i &amp;lt;j} d_ {ij} (X) - \ hat {d} _ {ij} (X) \)</target>
        </trans-unit>
        <trans-unit id="5ab8a1b7961470064849295cd500b6579b4fa397" translate="yes" xml:space="preserve">
          <source>Let \(X_S\) be the set of target features (i.e. the &lt;code&gt;features&lt;/code&gt; parameter) and let \(X_C\) be its complement.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0a7cf3804b0fab7efc2f61eebe3b5930a870c77" translate="yes" xml:space="preserve">
          <source>Let the data at node \(m\) be represented by \(Q\). For each candidate split \(\theta = (j, t_m)\) consisting of a feature \(j\) and threshold \(t_m\), partition the data into \(Q_{left}(\theta)\) and \(Q_{right}(\theta)\) subsets</source>
          <target state="translated">Deja que los datos del nodo sean representados por &quot;Q&quot;.Para cada candidato dividido=(j,t_m)\Nque consiste en una característica y un umbral,divide los datos en subconjuntos...</target>
        </trans-unit>
        <trans-unit id="0eec5761b9ded7fa59a47d01c0fcb2883cae7dd4" translate="yes" xml:space="preserve">
          <source>Let us now try to reconstruct the original image from the patches by averaging on overlapping areas:</source>
          <target state="translated">Intentemos ahora reconstruir la imagen original a partir de los parches,haciendo un promedio de las áreas superpuestas:</target>
        </trans-unit>
        <trans-unit id="b5fecaca6e2e29d4dbee3904d66ce06eade247b5" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s compute the performance of this constant prediction baseline with 3 different regression metrics:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="253db59ca1134838191e01d302a48262e9ba6374" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s consider the following trained regression model:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b44b9f209d8c4175d9e9b8f1619b5304447365c9" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s fit a MLPRegressor and compute single-variable partial dependence plots</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c338f438f84e4277f756f203f24830c86a10546d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s load data from the newsgroups dataset which comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b852fb92508c2f8d11c716ed1054a5db1d558210" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s load the motor claim dataset from OpenML: &lt;a href=&quot;https://www.openml.org/d/41214&quot;&gt;https://www.openml.org/d/41214&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed3c266c74a01e549482a05ad99384adcdbf59a9" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s make the same partial dependence plot for the 2 features interaction, this time in 3 dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f393e9b32e94a400250ae55bd302c219802fcb3" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s now compute the partial dependence plots for this neural network using the model-agnostic (brute-force) method:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c401ba9464abf901fd51a8e8666504a0d45adbfb" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s now fit a GradientBoostingRegressor and compute the partial dependence plots either or one or two variables at a time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="106ecb5f7c6bb4669d70caeae32d17518696e61b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s print the first lines of the first loaded file:</source>
          <target state="translated">Imprimamos las primeras l&amp;iacute;neas del primer archivo cargado:</target>
        </trans-unit>
        <trans-unit id="bcd495b6fedeb570ab62363e5dbb308b08a2e969" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 25, and 50, and want to know their class name.</source>
          <target state="translated">Supongamos que est&amp;aacute; interesado en las muestras 10, 25 y 50, y desea saber el nombre de su clase.</target>
        </trans-unit>
        <trans-unit id="65595eba1f80a7173dc24674f2afc2d5837968b2" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 50, and 85, and want to know their class name.</source>
          <target state="translated">Supongamos que est&amp;aacute; interesado en las muestras 10, 50 y 85 y desea saber el nombre de su clase.</target>
        </trans-unit>
        <trans-unit id="fb46983e946ca9f3803c9b6fd00931719bb67d7b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 80, and 140, and want to know their class name.</source>
          <target state="translated">Supongamos que est&amp;aacute; interesado en las muestras 10, 80 y 140 y desea saber el nombre de su clase.</target>
        </trans-unit>
        <trans-unit id="e8b31884f71e52e12b72fcb054664d8ab80d318b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; cross-validation object:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36ae1b66774b74a6fe504ba4aa0655c5c58c7d06" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;code&gt;KFold&lt;/code&gt; cross-validation object:</source>
          <target state="translated">Veamos c&amp;oacute;mo se ve el objeto de validaci&amp;oacute;n cruzada de &lt;code&gt;KFold&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="9c035fe2592c35e4264add67f9ea318300cfbf19" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take a look at what the most informative features are:</source>
          <target state="translated">Echemos un vistazo a cu&amp;aacute;les son las caracter&amp;iacute;sticas m&amp;aacute;s informativas:</target>
        </trans-unit>
        <trans-unit id="fc26adc7a4427a7251d50d414ace6a50d7fbe76d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents:</source>
          <target state="translated">Tomemos un ejemplo con los siguientes recuentos. El primer t&amp;eacute;rmino est&amp;aacute; presente el 100% del tiempo, por lo que no es muy interesante. Las otras dos caracter&amp;iacute;sticas solo en menos del 50% del tiempo, por lo que probablemente sean m&amp;aacute;s representativas del contenido de los documentos:</target>
        </trans-unit>
        <trans-unit id="2f95c182cf5fa2e13ce2622defe8af992d765313" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s try again with the default setting:</source>
          <target state="translated">Intentemos de nuevo con la configuraci&amp;oacute;n predeterminada:</target>
        </trans-unit>
        <trans-unit id="c3a6b9996c4370e6a8670703084aa093c6face20" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:</source>
          <target state="translated">Us&amp;eacute;moslo para tokenizar y contar las apariciones de palabras de un corpus minimalista de documentos de texto:</target>
        </trans-unit>
        <trans-unit id="292ee05108cd151612c92edea6da34acc9a56662" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use pandas to load a copy of the titanic dataset. The following shows how to apply separate preprocessing on numerical and categorical features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c77ba9dd4a1f63d6e3e47c6ccfdd6637c48fa284" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s visually compare the cross validation behavior for many scikit-learn cross-validation objects. Below we will loop through several common cross-validation objects, visualizing the behavior of each.</source>
          <target state="translated">Comparemos visualmente el comportamiento de validaci&amp;oacute;n cruzada para muchos objetos de validaci&amp;oacute;n cruzada de scikit-learn. A continuaci&amp;oacute;n, recorreremos varios objetos comunes de validaci&amp;oacute;n cruzada, visualizando el comportamiento de cada uno.</target>
        </trans-unit>
        <trans-unit id="6c69807d4e78cfb8da9f8e8c21f378d88124782a" translate="yes" xml:space="preserve">
          <source>Level of verbosity.</source>
          <target state="translated">Nivel de verbosidad.</target>
        </trans-unit>
        <trans-unit id="339e81226696b46d7377960244af7f4dcb540176" translate="yes" xml:space="preserve">
          <source>Lewis, D. D., Yang, Y., Rose, T. G., &amp;amp; Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee9267aef527ceaeed70f092da783571e1b2536d" translate="yes" xml:space="preserve">
          <source>Libsvm GUI</source>
          <target state="translated">Libsvm GUI</target>
        </trans-unit>
        <trans-unit id="87b3d037e844d0b272ce5bfc0fb0a06e31f13827" translate="yes" xml:space="preserve">
          <source>License: BSD 3 clause</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="538c09161b8497f998404cafc34964ed3a445575" translate="yes" xml:space="preserve">
          <source>Licensed under the 3-clause BSD License.</source>
          <target state="translated">Licenciado bajo la licencia BSD de 3 cláusulas.</target>
        </trans-unit>
        <trans-unit id="d119b02c417272fad54f56fb5c480a5a866c4e2e" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="translated">Lichman, M. (2013). Repositorio de aprendizaje autom&amp;aacute;tico de la UCI [ &lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt; ]. Irvine, CA: Universidad de California, Facultad de Informaci&amp;oacute;n y Ciencias de la Computaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="e76bfa901cdd22255b64ebacf70ec6f76b7f04ab" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;https://archive.ics.uci.edu/ml&quot;&gt;https://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a2cabe35d47f4d173451a3dc4bce594fc9e8434" translate="yes" xml:space="preserve">
          <source>Like &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;, forests of trees also extend to &lt;a href=&quot;tree#tree-multioutput&quot;&gt;multi-output problems&lt;/a&gt; (if Y is an array of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt;).</source>
          <target state="translated">Al igual &lt;a href=&quot;tree#tree&quot;&gt;que los &amp;aacute;rboles de decisi&amp;oacute;n&lt;/a&gt; , los bosques de &amp;aacute;rboles tambi&amp;eacute;n se extienden a &lt;a href=&quot;tree#tree-multioutput&quot;&gt;problemas de m&amp;uacute;ltiples salidas&lt;/a&gt; (si Y es una matriz de tama&amp;ntilde;o &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="f8c94a1d14fd76e50a18d8c5112493b7c41a75b8" translate="yes" xml:space="preserve">
          <source>Like &lt;code&gt;Pipeline&lt;/code&gt;, individual steps may be replaced using &lt;code&gt;set_params&lt;/code&gt;, and ignored by setting to &lt;code&gt;'drop'&lt;/code&gt;:</source>
          <target state="translated">Al igual que &lt;code&gt;Pipeline&lt;/code&gt; , los pasos individuales se pueden reemplazar usando &lt;code&gt;set_params&lt;/code&gt; , e ignorarse estableciendo en &lt;code&gt;'drop'&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="09c07eeb7023cd495f9b67aa8c5832e9de0a3634" translate="yes" xml:space="preserve">
          <source>Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.</source>
          <target state="translated">Al igual que el MultinomialNB,este clasificador es adecuado para datos discretos.La diferencia es que mientras que MultinomialNB trabaja con recuentos de ocurrencia,BernoulliNB está diseñado para características binarias/booleanas.</target>
        </trans-unit>
        <trans-unit id="33e640491ab99ccee8501c0c94b8deb9f93a420b" translate="yes" xml:space="preserve">
          <source>Like fit(X) followed by transform(X), but does not require materializing X in memory.</source>
          <target state="translated">Como fit(X)seguido de transform(X),pero no requiere materializar X en la memoria.</target>
        </trans-unit>
        <trans-unit id="07a7d71492c9370f4c5f214183352ca2f48a9590" translate="yes" xml:space="preserve">
          <source>Like in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using &lt;code&gt;set_params&lt;/code&gt; and searched in grid search.</source>
          <target state="translated">Como en Pipeline y FeatureUnion, esto permite que el transformador y sus par&amp;aacute;metros se establezcan usando &lt;code&gt;set_params&lt;/code&gt; y se busquen en la b&amp;uacute;squeda de cuadr&amp;iacute;cula.</target>
        </trans-unit>
        <trans-unit id="ace16ab25f8f0e2cb278ad02989604150a81258c" translate="yes" xml:space="preserve">
          <source>Like pipelines, feature unions have a shorthand constructor called &lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt;&lt;code&gt;make_union&lt;/code&gt;&lt;/a&gt; that does not require explicit naming of the components.</source>
          <target state="translated">Al igual que las canalizaciones, las uniones de caracter&amp;iacute;sticas tienen un constructor abreviado llamado &lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt; &lt;code&gt;make_union&lt;/code&gt; &lt;/a&gt; que no requiere un nombre expl&amp;iacute;cito de los componentes.</target>
        </trans-unit>
        <trans-unit id="29685b73b0fbefa3dc8a3378779801b7f7266cf2" translate="yes" xml:space="preserve">
          <source>Like scalers, &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; puts all features into the same, known range or distribution. However, by performing a rank transformation, it smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.</source>
          <target state="translated">Al igual que los escaladores, &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt; coloca todas las funciones en el mismo rango o distribuci&amp;oacute;n conocidos. Sin embargo, al realizar una transformaci&amp;oacute;n de rango, suaviza distribuciones inusuales y est&amp;aacute; menos influenciado por valores at&amp;iacute;picos que los m&amp;eacute;todos de escala. Sin embargo, distorsiona las correlaciones y distancias dentro y entre entidades.</target>
        </trans-unit>
        <trans-unit id="73666b411bce492e14d6ba5736c0ca5eebf6eb1b" translate="yes" xml:space="preserve">
          <source>Like the Poisson GLM above, the gradient boosted trees model minimizes the Poisson deviance. However, because of a higher predictive power, it reaches lower values of Poisson deviance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ae4ed5af04ee97eab148462e283fb7149bd9d04" translate="yes" xml:space="preserve">
          <source>Limit in bytes of the size of the cache.</source>
          <target state="translated">Límite en bytes del tamaño del caché.</target>
        </trans-unit>
        <trans-unit id="bbd76c46a461ce6867ca433ec8697501cc65b137" translate="yes" xml:space="preserve">
          <source>Limiting distance of neighbors to return. (default is the value passed to the constructor).</source>
          <target state="translated">Limitando la distancia de los vecinos para regresar.(por defecto es el valor pasado al constructor).</target>
        </trans-unit>
        <trans-unit id="62c917554a7197d63486db913ca90de577c0bfe0" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis</source>
          <target state="translated">Análisis discriminante lineal</target>
        </trans-unit>
        <trans-unit id="bb7eb231c96859c6082b4ce0d3b796b4329f6e8f" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="719a12bbe391db4f9a1b1f0f22d958d133e79356" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="translated">An&amp;aacute;lisis discriminante lineal ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; ) y an&amp;aacute;lisis discriminante cuadr&amp;aacute;tico ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; ) son dos clasificadores cl&amp;aacute;sicos, con, como su nombre indica, una lineal y una superficie de decisi&amp;oacute;n cuadr&amp;aacute;tica, respectivamente.</target>
        </trans-unit>
        <trans-unit id="e36f5257c349ab3d8389a5027fa29765ef7a78a4" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance &lt;em&gt;between classes&lt;/em&gt;. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.</source>
          <target state="translated">El an&amp;aacute;lisis discriminante lineal (LDA) intenta identificar los atributos que explican la mayor variaci&amp;oacute;n &lt;em&gt;entre clases&lt;/em&gt; . En particular, LDA, a diferencia de PCA, es un m&amp;eacute;todo supervisado que utiliza etiquetas de clase conocidas.</target>
        </trans-unit>
        <trans-unit id="02924b985796944d65c857ba377ee96748a5fefe" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis and Quadratic Discriminant Analysis</source>
          <target state="translated">Análisis discriminante lineal y análisis discriminante cuadrático</target>
        </trans-unit>
        <trans-unit id="37e8c1f7f3b5f52be8ccec8de8de9cd593660b24" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis, from the &lt;a href=&quot;../../modules/classes#module-sklearn.discriminant_analysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis&lt;/code&gt;&lt;/a&gt; module, and Neighborhood Components Analysis, from the &lt;a href=&quot;../../modules/classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; module, are supervised dimensionality reduction method, i.e. they make use of the provided labels, contrary to other methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe99070400d8a366d4438afb34b3817ed643e76c" translate="yes" xml:space="preserve">
          <source>Linear Model trained with L1 prior as regularizer (aka the Lasso)</source>
          <target state="translated">Modelo lineal entrenado con L1 previo como regularizador (alias el Lazo)</target>
        </trans-unit>
        <trans-unit id="b4819d272193c458d14d3c2a02b6439edb693339" translate="yes" xml:space="preserve">
          <source>Linear Regression Example</source>
          <target state="translated">Ejemplo de regresión lineal</target>
        </trans-unit>
        <trans-unit id="85494d31f5cd31cf05c6e37284f8e968283c0002" translate="yes" xml:space="preserve">
          <source>Linear SVC is not a probabilistic classifier by default but it has a built-in calibration option enabled in this example (&lt;code&gt;probability=True&lt;/code&gt;).</source>
          <target state="translated">El SVC lineal no es un clasificador probabil&amp;iacute;stico por defecto, pero tiene una opci&amp;oacute;n de calibraci&amp;oacute;n incorporada habilitada en este ejemplo ( &lt;code&gt;probability=True&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="e97d7a71e4408e1f570cb8d2ee92b68f661724af" translate="yes" xml:space="preserve">
          <source>Linear SVMs</source>
          <target state="translated">SVMs lineales</target>
        </trans-unit>
        <trans-unit id="73af0f0fe2656e7c704e9d2782f72d490054905e" translate="yes" xml:space="preserve">
          <source>Linear Sum - A n-dimensional vector holding the sum of all samples</source>
          <target state="translated">Suma lineal-Un vector n-dimensional que contiene la suma de todas las muestras</target>
        </trans-unit>
        <trans-unit id="1cd7978197df4491cb006d18687f0ce787689e06" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">La clasificaci&amp;oacute;n de vectores de soporte lineal ( &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; ) muestra una curva a&amp;uacute;n m&amp;aacute;s sigmoidea como RandomForestClassifier, que es t&amp;iacute;pica de los m&amp;eacute;todos de margen m&amp;aacute;ximo (compare Niculescu-Mizil y Caruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt; ), que se centran en muestras duras que est&amp;aacute;n cerca del l&amp;iacute;mite de decisi&amp;oacute;n ( los vectores de soporte).</target>
        </trans-unit>
        <trans-unit id="aa48807728eba9fefe9fd82435afe3ea7ac48643" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;1&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88aaad048f30298d89bc0519c1e6f4cfbb7c20ea" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification.</source>
          <target state="translated">Clasificación de Vectores de Soporte Lineal.</target>
        </trans-unit>
        <trans-unit id="4669e7bb12c975a34b6d592ccfe985850a9e31eb" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Regression.</source>
          <target state="translated">Regresión vectorial de apoyo lineal.</target>
        </trans-unit>
        <trans-unit id="299f04ebeb7ad11bec6b5498c6b639ccade4023d" translate="yes" xml:space="preserve">
          <source>Linear and Quadratic Discriminant Analysis with covariance ellipsoid</source>
          <target state="translated">Análisis discriminante lineal y cuadrático con el elipsoide de covarianza</target>
        </trans-unit>
        <trans-unit id="f5530856b3075f323ba02b6ac9992d5aae8eee6e" translate="yes" xml:space="preserve">
          <source>Linear classifiers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0463594ed874e4d015c682e8a6395a05e3fbd8b" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</source>
          <target state="translated">Clasificadores lineales (SVM,regresión logística,etc.)con entrenamiento en SGD.</target>
        </trans-unit>
        <trans-unit id="9a198808a208105876aff5b3459a1743d02b9e6c" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, etc.) with SGD training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa82faf2d530b479b3e87ec39c80fc313d729e93" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space.</source>
          <target state="translated">Reducción de la dimensionalidad lineal utilizando la Descomposición de Valor Singular de los datos centrados,manteniendo sólo los vectores singulares más significativos para proyectar los datos a un espacio dimensional inferior.</target>
        </trans-unit>
        <trans-unit id="9db7130b75e27bc47e2764b6ec7d1ce03bb7f92f" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.</source>
          <target state="translated">Reducción de la dimensionalidad lineal utilizando la Descomposición de Valor Singular de los datos para proyectarlos a un espacio dimensional inferior.</target>
        </trans-unit>
        <trans-unit id="6451e9b1aa60579e3e911ed4aef57d459ed7cbf1" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7a4096bc58f13af1ea98cce08fd73c62fb91596" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data, keeping only the most significant singular vectors to project the data to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="212b70af3cba5b501136f7c4f46821ce9f54ad31" translate="yes" xml:space="preserve">
          <source>Linear kernel (&lt;code&gt;kernel = 'linear'&lt;/code&gt;)</source>
          <target state="translated">N&amp;uacute;cleo lineal ( &lt;code&gt;kernel = 'linear'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="1196f0388e6edcd3bda2236746717385556b159a" translate="yes" xml:space="preserve">
          <source>Linear least squares with l2 regularization.</source>
          <target state="translated">Cuadrados lineales mínimos con regularización de l2.</target>
        </trans-unit>
        <trans-unit id="0663410286eb390a6a91a4885ecdb0348930bc50" translate="yes" xml:space="preserve">
          <source>Linear model fitted by minimizing a regularized empirical loss with SGD</source>
          <target state="translated">Modelo lineal ajustado minimizando una pérdida empírica regularizada con SGD</target>
        </trans-unit>
        <trans-unit id="8d6556caff9af87efd1e0ccffe2463b6a45189f7" translate="yes" xml:space="preserve">
          <source>Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure.</source>
          <target state="translated">Modelo lineal para probar el efecto individual de cada uno de los muchos regresores.Esta es una función de puntuación para ser usada en un procedimiento de selección de características,no un procedimiento de selección de características independiente.</target>
        </trans-unit>
        <trans-unit id="8f05719b5c26a33c08ae54e21caa15631a4bbbf1" translate="yes" xml:space="preserve">
          <source>Linear model: from regression to sparsity</source>
          <target state="translated">Modelo lineal:de la regresión a la escasez</target>
        </trans-unit>
        <trans-unit id="0b1d2caa3dbccbb7fc7a7a3c7fac23bacc286b81" translate="yes" xml:space="preserve">
          <source>Linear models with regularization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46d0fcb8f937066fa349aa94a3c710921424dd9b" translate="yes" xml:space="preserve">
          <source>Linear models with sparse coefficients</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c94cc16a66b49675f2acef482a0fbcd40d606ee" translate="yes" xml:space="preserve">
          <source>Linear models: \(y = X\beta + \epsilon\)</source>
          <target state="translated">Modelos lineales:\(y=X\beta+\ ~ -epsilon-)</target>
        </trans-unit>
        <trans-unit id="b501f602569674c31fc384f2cd7a29bcf6c1ce1f" translate="yes" xml:space="preserve">
          <source>Linear regression</source>
          <target state="translated">Regresión lineal</target>
        </trans-unit>
        <trans-unit id="d8f88b232d41c327138bbda59458fa5fc4086fff" translate="yes" xml:space="preserve">
          <source>Linear regression model that is robust to outliers.</source>
          <target state="translated">Un modelo de regresión lineal que es robusto a los valores atípicos.</target>
        </trans-unit>
        <trans-unit id="597ff76dcbb7bc322f194ba001977a736c193c2d" translate="yes" xml:space="preserve">
          <source>Linear regression with combined L1 and L2 priors as regularizer.</source>
          <target state="translated">Regresión lineal con los antecedentes combinados de L1 y L2 como regularizador.</target>
        </trans-unit>
        <trans-unit id="0a2d386e0774637a1788b00b4abdb8b2c6c38c74" translate="yes" xml:space="preserve">
          <source>Linear ridge regression.</source>
          <target state="translated">Regresión lineal de la cresta.</target>
        </trans-unit>
        <trans-unit id="1dc397a187bb8fae755995aa069f79a9d565d581" translate="yes" xml:space="preserve">
          <source>Linear support vector classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5959458e20a73276c12d61f1d64604d66daab52d" translate="yes" xml:space="preserve">
          <source>LinearRegression</source>
          <target state="translated">LinearRegression</target>
        </trans-unit>
        <trans-unit id="6ff9599a07718d87fb124205a5a88e8262436582" translate="yes" xml:space="preserve">
          <source>LinearRegression fits a linear model with coefficients w = (w1, &amp;hellip;, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="daf34c391f43051e2982c2bbeba34bf1a7727132" translate="yes" xml:space="preserve">
          <source>List containing the artists for the annotation boxes making up the tree.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7ed3fbb6680836b95c3db482cfaf054c37a8419" translate="yes" xml:space="preserve">
          <source>List containing train-test split of inputs.</source>
          <target state="translated">Lista que contiene la división de entradas de la prueba del tren.</target>
        </trans-unit>
        <trans-unit id="7946c78611ea79ca25491c94f60dac5182c68016" translate="yes" xml:space="preserve">
          <source>List of (name, class), where &lt;code&gt;name&lt;/code&gt; is the class name as string and &lt;code&gt;class&lt;/code&gt; is the actuall type of the class.</source>
          <target state="translated">Lista de (nombre, clase), donde &lt;code&gt;name&lt;/code&gt; es el nombre de la clase como cadena y &lt;code&gt;class&lt;/code&gt; es el tipo actual de la clase.</target>
        </trans-unit>
        <trans-unit id="01f72260e79a828ac37c6e1b27f0158a5c017639" translate="yes" xml:space="preserve">
          <source>List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.</source>
          <target state="translated">Lista de tuplas (nombre,transformación)que se encadenan,en el orden en que se encadenan,con el último objeto un estimador.</target>
        </trans-unit>
        <trans-unit id="da3552a00ac25869a883c68bd3a0b9b483a759ac" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, column(s)) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="translated">Lista de tuplas (nombre,transformador,columna(s))en las que se especifican los objetos transformadores que se aplicarán a los subconjuntos de los datos.</target>
        </trans-unit>
        <trans-unit id="d6a17bcffaea2ea2f18b9842614672499d0a26c6" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, columns) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ce9067b559ab6542ebc584f224960b4d8e01fb3" translate="yes" xml:space="preserve">
          <source>List of &lt;code&gt;n_features&lt;/code&gt;-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">Lista de &lt;code&gt;n_features&lt;/code&gt; -puntos de datos dimensionales. Cada fila corresponde a un &amp;uacute;nico punto de datos.</target>
        </trans-unit>
        <trans-unit id="5f38bb9ffb369276ed25fb7c04fb0e0e029823d8" translate="yes" xml:space="preserve">
          <source>List of all the classes that can possibly appear in the y vector.</source>
          <target state="translated">Lista de todas las clases que pueden aparecer en el vector y.</target>
        </trans-unit>
        <trans-unit id="7cd6d854280958549421b40e7d622e1782f63df4" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If &lt;code&gt;None&lt;/code&gt; alphas are set automatically</source>
          <target state="translated">Lista de alfas donde calcular los modelos. Si &lt;code&gt;None&lt;/code&gt; alfas se configuran autom&amp;aacute;ticamente</target>
        </trans-unit>
        <trans-unit id="917b5a956108e84a4893edaf20f4507c6507d0e2" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically</source>
          <target state="translated">Lista de alfas donde calcular los modelos.Si no se establece ningún alfa automáticamente</target>
        </trans-unit>
        <trans-unit id="a5422f4e0e412f7e68186f61afde0578ebd8abd7" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d057f35a68cef6d291f5ea686ce0f4438a6a2951" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If not provided, set automatically.</source>
          <target state="translated">Lista de alfas donde calcular los modelos.Si no se proporciona,se establece automáticamente.</target>
        </trans-unit>
        <trans-unit id="fb8d4641f5ca2701f801733b19cf7bb77974f371" translate="yes" xml:space="preserve">
          <source>List of arrays of terms.</source>
          <target state="translated">Lista de arreglos de términos.</target>
        </trans-unit>
        <trans-unit id="6ecedd8bbbc6137125014e8bb7a429cbcef11be8" translate="yes" xml:space="preserve">
          <source>List of built-in kernels.</source>
          <target state="translated">Lista de núcleos incorporados.</target>
        </trans-unit>
        <trans-unit id="36c7ba17f19f78b4b0b98a1a27cecbfd22dc65e4" translate="yes" xml:space="preserve">
          <source>List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept. For &lt;code&gt;multiclass='multinomial'&lt;/code&gt;, the shape is (n_classes, n_cs, n_features) or (n_classes, n_cs, n_features + 1).</source>
          <target state="translated">Lista de coeficientes del modelo de regresi&amp;oacute;n log&amp;iacute;stica. Si fit_intercept se establece en True, la segunda dimensi&amp;oacute;n ser&amp;aacute; n_features + 1, donde el &amp;uacute;ltimo elemento representa la intersecci&amp;oacute;n. Para &lt;code&gt;multiclass='multinomial'&lt;/code&gt; , la forma es (n_classes, n_cs, n_features) o (n_classes, n_cs, n_features + 1).</target>
        </trans-unit>
        <trans-unit id="d090495d2c127f32b67f3946c4bdcba721a89fcd" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If &lt;code&gt;None&lt;/code&gt; is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="568d5fc554d78a8c3f420990686843b1d52522c9" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If none is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">Lista de etiquetas para indexar la matriz. Esto se puede utilizar para reordenar o seleccionar un subconjunto de etiquetas. Si no se proporciona ninguno, los que aparecen al menos una vez en &lt;code&gt;y_true&lt;/code&gt; o &lt;code&gt;y_pred&lt;/code&gt; se utilizan en orden ordenado.</target>
        </trans-unit>
        <trans-unit id="4904457db6e3ad315971a386c35727cdd591b70f" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to select a subset of labels. If None, all labels that appear at least once in &lt;code&gt;y1&lt;/code&gt; or &lt;code&gt;y2&lt;/code&gt; are used.</source>
          <target state="translated">Lista de etiquetas para indexar la matriz. Esto se puede utilizar para seleccionar un subconjunto de etiquetas. Si es Ninguno, se utilizan todas las etiquetas que aparecen al menos una vez en &lt;code&gt;y1&lt;/code&gt; o &lt;code&gt;y2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b841f355bd90388eac15a6584a26192e6c900c97" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">Lista de n_características-puntos de datos dimensionales.Cada fila corresponde a un único punto de datos.</target>
        </trans-unit>
        <trans-unit id="af1051d092002bc2f98d27cb1ada3b5cc2dacea1" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single query.</source>
          <target state="translated">Lista de n_características-puntos de datos dimensionales.Cada fila corresponde a una única consulta.</target>
        </trans-unit>
        <trans-unit id="85e7a2833a6b5505d28e95f0c1116dee51aa01e1" translate="yes" xml:space="preserve">
          <source>List of objects to ensure sliceability.</source>
          <target state="translated">Lista de objetos para asegurar la cortabilidad.</target>
        </trans-unit>
        <trans-unit id="5538dc428bf1dd702d4666daf2c6801367c4f065" translate="yes" xml:space="preserve">
          <source>List of sample weights attached to the data X.</source>
          <target state="translated">Lista de pesos de muestra adjunta a los datos X.</target>
        </trans-unit>
        <trans-unit id="af4d88e1f955adfe14752a1cab15db410dc25046" translate="yes" xml:space="preserve">
          <source>List of samples.</source>
          <target state="translated">Lista de muestras.</target>
        </trans-unit>
        <trans-unit id="9fa149a90ccae2cfe066dfb859bf8a7c95ef01ca" translate="yes" xml:space="preserve">
          <source>List of transformer objects to be applied to the data. The first half of each tuple is the name of the transformer.</source>
          <target state="translated">Lista de objetos transformadores que se aplicarán a los datos.La primera mitad de cada tupla es el nombre del transformador.</target>
        </trans-unit>
        <trans-unit id="f2f499a9d9cf5fba3b5aa16bff4e7ad9f538a51f" translate="yes" xml:space="preserve">
          <source>List of values for the regularization parameter or integer specifying the number of regularization parameters that should be used. In this case, the parameters will be chosen in a logarithmic scale between 1e-4 and 1e4.</source>
          <target state="translated">Lista de valores del parámetro o entero de regularización que especifica el número de parámetros de regularización que deben utilizarse.En este caso,los parámetros se elegirán en una escala logarítmica entre 1e-4 y 1e4.</target>
        </trans-unit>
        <trans-unit id="d742bd356ab53d1131907c9ca41e9f89956bc677" translate="yes" xml:space="preserve">
          <source>List of weighting type to calculate the score. None means no weighted; &amp;ldquo;linear&amp;rdquo; means linear weighted; &amp;ldquo;quadratic&amp;rdquo; means quadratic weighted.</source>
          <target state="translated">Lista de tipo de ponderaci&amp;oacute;n para calcular la puntuaci&amp;oacute;n. Ninguno significa no ponderado; &quot;Lineal&quot; significa lineal ponderado; &amp;ldquo;Cuadr&amp;aacute;tico&amp;rdquo; significa cuadr&amp;aacute;tico ponderado.</target>
        </trans-unit>
        <trans-unit id="ccaadae3fd2b8d525242b8298319bebc15b1d7f7" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;lsquo;08. Eighth IEEE International Conference on.</source>
          <target state="translated">Liu, Fei Tony, Ting, Kai Ming y Zhou, Zhi-Hua. &quot;Bosque de aislamiento&quot;. Miner&amp;iacute;a de datos, 2008. ICDM'08. Octava Conferencia Internacional IEEE sobre.</target>
        </trans-unit>
        <trans-unit id="ef6455b6e1e2fee9a705bb5d5a6878e2089d032f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;rsquo;08. Eighth IEEE International Conference on.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d858831be3c025b5261ad0994fdd43e36106d2f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation-based anomaly detection.&amp;rdquo; ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 3.</source>
          <target state="translated">Liu, Fei Tony, Ting, Kai Ming y Zhou, Zhi-Hua. &quot;Detecci&amp;oacute;n de anomal&amp;iacute;as basada en aislamiento&quot;. Transacciones de ACM sobre el descubrimiento de conocimientos a partir de datos (TKDD) 6.1 (2012): 3.</target>
        </trans-unit>
        <trans-unit id="d7cbf66ae3940637cf4feeef412f2113fd5144b3" translate="yes" xml:space="preserve">
          <source>Load Data and Train a SVC</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a977facf077b843c15be4adf2b27656870bbc8b" translate="yes" xml:space="preserve">
          <source>Load Data and train model</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc1c89a3655919cbe107b23bf70fdaef2d59b7e4" translate="yes" xml:space="preserve">
          <source>Load a datasets as downloaded from &lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;</source>
          <target state="translated">Cargue un conjunto de datos descargado de &lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="10fbb828ccf5ef978744b601a27eefff85b82acd" translate="yes" xml:space="preserve">
          <source>Load and return the boston house-prices dataset (regression).</source>
          <target state="translated">Cargar y devolver el conjunto de datos de precios de la casa de Boston (regresión).</target>
        </trans-unit>
        <trans-unit id="f0b03288037dddab02ba1bf0d814f5cc8cf63088" translate="yes" xml:space="preserve">
          <source>Load and return the breast cancer wisconsin dataset (classification).</source>
          <target state="translated">Cargue y devuelva el conjunto de datos de Wisconsin sobre el cáncer de mama (clasificación).</target>
        </trans-unit>
        <trans-unit id="fb9c782009d54032f572c4b8eb05f6ff3c69b6ee" translate="yes" xml:space="preserve">
          <source>Load and return the diabetes dataset (regression).</source>
          <target state="translated">Cargar y devolver el conjunto de datos de la diabetes (regresión).</target>
        </trans-unit>
        <trans-unit id="5ee0c3f160bd1db558fab50ff07fd2d60e875939" translate="yes" xml:space="preserve">
          <source>Load and return the digits dataset (classification).</source>
          <target state="translated">Cargar y devolver el conjunto de datos de los dígitos (clasificación).</target>
        </trans-unit>
        <trans-unit id="91627f9a236f04bf8e67f696e6012e55dde096ca" translate="yes" xml:space="preserve">
          <source>Load and return the iris dataset (classification).</source>
          <target state="translated">Cargar y devolver el conjunto de datos del iris (clasificación).</target>
        </trans-unit>
        <trans-unit id="08308ecd69078eb0533ddcbcb38611925dd58ae7" translate="yes" xml:space="preserve">
          <source>Load and return the linnerud dataset (multivariate regression).</source>
          <target state="translated">Cargar y devolver el conjunto de datos de linnerud (regresión multivariante).</target>
        </trans-unit>
        <trans-unit id="24c66578782cdc888ab71f9a25a06214508e7234" translate="yes" xml:space="preserve">
          <source>Load and return the physical excercise linnerud dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a61d81b3e38cd33952ad8e4ab4da4e0afb0ac23" translate="yes" xml:space="preserve">
          <source>Load and return the wine dataset (classification).</source>
          <target state="translated">Cargar y devolver el conjunto de datos del vino (clasificación).</target>
        </trans-unit>
        <trans-unit id="34956b05e013a2f3d8d5817783733a9d69ea9a29" translate="yes" xml:space="preserve">
          <source>Load data from the training set</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="907ca9fec180a2f563a6eb0b2c208dd89483dfe5" translate="yes" xml:space="preserve">
          <source>Load dataset from multiple files in SVMlight format</source>
          <target state="translated">Cargar el conjunto de datos de múltiples archivos en formato SVMlight</target>
        </trans-unit>
        <trans-unit id="e0287d019fcfe4320ef71958ec3623d393a07d68" translate="yes" xml:space="preserve">
          <source>Load datasets in the svmlight / libsvm format into sparse CSR matrix</source>
          <target state="translated">Cargar los conjuntos de datos en el formato svmlight/libsvm en una matriz CSR dispersa</target>
        </trans-unit>
        <trans-unit id="15df99bbc404778e529956fb3833b1f8b300577d" translate="yes" xml:space="preserve">
          <source>Load sample images for image manipulation.</source>
          <target state="translated">Cargar imágenes de muestra para la manipulación de la imagen.</target>
        </trans-unit>
        <trans-unit id="93b606a5680687306536f14272c219f02caf9a74" translate="yes" xml:space="preserve">
          <source>Load text files with categories as subfolder names.</source>
          <target state="translated">Cargar archivos de texto con categorías como nombres de subcarpeta.</target>
        </trans-unit>
        <trans-unit id="ada1ba97e9c53b7b56715b1a2824f0ae676a78c6" translate="yes" xml:space="preserve">
          <source>Load the 20 newsgroups dataset and vectorize it into token counts (classification).</source>
          <target state="translated">Cargue el conjunto de datos de los 20 grupos de noticias y vectorícelo en recuentos simbólicos (clasificación).</target>
        </trans-unit>
        <trans-unit id="4f7a062fa00aaafd76d451b474abbba6455b18d1" translate="yes" xml:space="preserve">
          <source>Load the California housing dataset (regression).</source>
          <target state="translated">Cargue el conjunto de datos de viviendas de California (regresión).</target>
        </trans-unit>
        <trans-unit id="d369acbb02d6ae84bdcebcaf52c16540c4d5f177" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).</source>
          <target state="translated">Cargue el conjunto de datos de los pares de Caras Etiquetadas en la Naturaleza (LFW)(clasificación).</target>
        </trans-unit>
        <trans-unit id="fdf290fe8f8a97ef39a92df3e1f665ba9f5137b7" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) people dataset (classification).</source>
          <target state="translated">Carga el conjunto de datos de las personas etiquetadas en la selva (LFW)(clasificación).</target>
        </trans-unit>
        <trans-unit id="c1d9dfefbd2137b268a0489f71dee7b704510f30" translate="yes" xml:space="preserve">
          <source>Load the Olivetti faces data-set from AT&amp;amp;T (classification).</source>
          <target state="translated">Cargue el conjunto de datos de caras de Olivetti de AT&amp;amp;T (clasificaci&amp;oacute;n).</target>
        </trans-unit>
        <trans-unit id="2772bcfa51d7f467cdc1ff56dd2a38098daf99c8" translate="yes" xml:space="preserve">
          <source>Load the RCV1 multilabel dataset (classification).</source>
          <target state="translated">Cargar el conjunto de datos de la RCV1 (clasificación).</target>
        </trans-unit>
        <trans-unit id="b34ac8eb475e3d0c92e532ea1f16cafea2826dba" translate="yes" xml:space="preserve">
          <source>Load the covertype dataset (classification).</source>
          <target state="translated">Cargar el conjunto de datos de la cubierta (clasificación).</target>
        </trans-unit>
        <trans-unit id="ad3fd711e27424bfdcf7677e93b2ded911f0bbc0" translate="yes" xml:space="preserve">
          <source>Load the data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54322fa6d75ea033036ee5315e01f5a9e265e0ca" translate="yes" xml:space="preserve">
          <source>Load the filenames and data from the 20 newsgroups dataset (classification).</source>
          <target state="translated">Cargue los nombres de los archivos y los datos del conjunto de datos de los 20 grupos de noticias (clasificación).</target>
        </trans-unit>
        <trans-unit id="ae3c786b5593f01e176137f6a4960d769f8b9d22" translate="yes" xml:space="preserve">
          <source>Load the kddcup99 dataset (classification).</source>
          <target state="translated">Cargue el conjunto de datos kddcup99 (clasificación).</target>
        </trans-unit>
        <trans-unit id="820329ef76c355bc57213e87e726caebf3ec8e17" translate="yes" xml:space="preserve">
          <source>Load the numpy array of a single sample image</source>
          <target state="translated">Cargar la matriz numérica de una sola imagen de muestra</target>
        </trans-unit>
        <trans-unit id="6565057c8bbe701655d34466bc255155c3ea2c6e" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et.</source>
          <target state="translated">Cargador para el conjunto de datos de distribución de especies de Phillips et al.</target>
        </trans-unit>
        <trans-unit id="00912c83de18e685a34ddbd42e1354c697eb14e0" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et. al. (2006)</source>
          <target state="translated">Cargador para el conjunto de datos de distribución de especies de Phillips et.al.(2006)</target>
        </trans-unit>
        <trans-unit id="4f514b04ed6b877534da140af8e12cab5016f713" translate="yes" xml:space="preserve">
          <source>Loaders</source>
          <target state="translated">Loaders</target>
        </trans-unit>
        <trans-unit id="1d603b233f1badee343cd4d051b0c74346bf8ab5" translate="yes" xml:space="preserve">
          <source>Loading an example dataset</source>
          <target state="translated">Cargando un ejemplo de conjunto de datos</target>
        </trans-unit>
        <trans-unit id="caf6cb93de1911a37a3c4f9c173bcddd3283525a" translate="yes" xml:space="preserve">
          <source>Loading datasets, basic feature extraction and target definitions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afb9453c6f5c0750a61be0390918061037ab3605" translate="yes" xml:space="preserve">
          <source>Loading from external datasets</source>
          <target state="translated">Carga de conjuntos de datos externos</target>
        </trans-unit>
        <trans-unit id="b4240e57d982043f1f905f33f107714b1056ff0f" translate="yes" xml:space="preserve">
          <source>Loading the 20 newsgroups dataset</source>
          <target state="translated">Cargando el conjunto de datos de los 20 grupos de noticias</target>
        </trans-unit>
        <trans-unit id="bf453b7e00694519c6d048cddce89c9acdc80f61" translate="yes" xml:space="preserve">
          <source>Loads both, &lt;code&gt;china&lt;/code&gt; and &lt;code&gt;flower&lt;/code&gt;.</source>
          <target state="translated">Carga tanto de &lt;code&gt;china&lt;/code&gt; como de &lt;code&gt;flower&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5a8b86a7fef7215f7de926bc65cb224b10c3ccba" translate="yes" xml:space="preserve">
          <source>Locally Linear Embedding</source>
          <target state="translated">Encajonamiento local lineal</target>
        </trans-unit>
        <trans-unit id="f71746cee5cf3673e7e527aaea93ab0ac960ab66" translate="yes" xml:space="preserve">
          <source>Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.</source>
          <target state="translated">La incrustación lineal local (LLE)busca una proyección de datos de menor dimensión que preserve las distancias dentro de los vecindarios locales.Se puede pensar en una serie de Análisis de Componentes Principales locales que se comparan globalmente para encontrar la mejor incrustación no lineal.</target>
        </trans-unit>
        <trans-unit id="ba721026e6725be51f569c81e87377b42c664dd5" translate="yes" xml:space="preserve">
          <source>Locally linear embedding can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">La incrustaci&amp;oacute;n localmente lineal se puede realizar con la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt; o su contraparte orientada a objetos &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="120996393a2755aae459a0342f6a159574a0420b" translate="yes" xml:space="preserve">
          <source>Log likelihood of the Gaussian mixture given X.</source>
          <target state="translated">Logra la probabilidad de la mezcla gaussiana dada X.</target>
        </trans-unit>
        <trans-unit id="10dac5cbd2ef695e498b42bff8cc166d8d6c8a26" translate="yes" xml:space="preserve">
          <source>Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).</source>
          <target state="translated">La pérdida de registros no está definida para p=0 o p=1,por lo que las probabilidades se recortan a max(eps,min(1-eps,p)).</target>
        </trans-unit>
        <trans-unit id="8742f15984971d3e598576d7cde59958d4df18a1" translate="yes" xml:space="preserve">
          <source>Log loss, aka logistic loss or cross-entropy loss.</source>
          <target state="translated">Pérdida de tronco,también conocida como pérdida de logística o pérdida de entropía.</target>
        </trans-unit>
        <trans-unit id="3332ed47adb99d618a3191081bd1b56f7df887fc" translate="yes" xml:space="preserve">
          <source>Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs (&lt;code&gt;predict_proba&lt;/code&gt;) of a classifier instead of its discrete predictions.</source>
          <target state="translated">La p&amp;eacute;rdida logar&amp;iacute;tmica, tambi&amp;eacute;n llamada p&amp;eacute;rdida de regresi&amp;oacute;n log&amp;iacute;stica o p&amp;eacute;rdida de entrop&amp;iacute;a cruzada, se define en estimaciones de probabilidad. Se usa com&amp;uacute;nmente en regresiones log&amp;iacute;sticas (multinomiales) y redes neuronales, as&amp;iacute; como en algunas variantes de maximizaci&amp;oacute;n de expectativas, y se puede usar para evaluar las salidas de probabilidad ( &lt;code&gt;predict_proba&lt;/code&gt; ) de un clasificador en lugar de sus predicciones discretas.</target>
        </trans-unit>
        <trans-unit id="f8ceba0d5dd7df5e53e4d9ba0bfe4881369ef7f1" translate="yes" xml:space="preserve">
          <source>Log of probability estimates.</source>
          <target state="translated">Registro de estimaciones de probabilidad.</target>
        </trans-unit>
        <trans-unit id="b2dede1f561914a3bc83cf7a3f85dcff6bab8c76" translate="yes" xml:space="preserve">
          <source>Log probabilities of each data point in X.</source>
          <target state="translated">Registra las probabilidades de cada punto de datos en X.</target>
        </trans-unit>
        <trans-unit id="ce21bba36fd356086ab08edfbf5461d606fc0046" translate="yes" xml:space="preserve">
          <source>Log probability of each class (smoothed).</source>
          <target state="translated">Logra la probabilidad de cada clase (suavizada).</target>
        </trans-unit>
        <trans-unit id="10521a3daec9ae1f69d9eb092c8ffd785e7a6414" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model</source>
          <target state="translated">La probabilidad de registro de cada muestra en el modelo actual</target>
        </trans-unit>
        <trans-unit id="f4a66282780599e98e06ef51f0d5990ef29ec975" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c1e8dc95e554810186fcd38490e4f1fe6e53c32" translate="yes" xml:space="preserve">
          <source>Log-likelihood score on left-out data across folds.</source>
          <target state="translated">La puntuación de la probabilidad de registro en los datos de la izquierda a través de los pliegues.</target>
        </trans-unit>
        <trans-unit id="af6fc4d4c535e2fcc7787b2d2b354e641a5cdf07" translate="yes" xml:space="preserve">
          <source>Log-marginal likelihood of theta for training data.</source>
          <target state="translated">Probabilidad logarítmica de theta para los datos de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="a79f6e0f430c7ecad68ae2bba39688851de03cfd" translate="yes" xml:space="preserve">
          <source>Log: Logistic Regression.</source>
          <target state="translated">Tronco:Regresión logística.</target>
        </trans-unit>
        <trans-unit id="710d6654ab013b68e75b864460870bdb608ee833" translate="yes" xml:space="preserve">
          <source>Log: equivalent to Logistic Regression. \(L(y_i, f(x_i)) = \log(1 + \exp (-y_i f(x_i)))\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="667a374e42016ea0491009bae949bbc3eb5a98fe" translate="yes" xml:space="preserve">
          <source>Logistic Regression (aka logit, MaxEnt) classifier.</source>
          <target state="translated">Clasificador de Regresión Logística (alias logit,MaxEnt).</target>
        </trans-unit>
        <trans-unit id="7553fecbacc2ab6c754b732dd2a40625b016efa3" translate="yes" xml:space="preserve">
          <source>Logistic Regression 3-class Classifier</source>
          <target state="translated">Regresión logística Clasificador de 3 clases</target>
        </trans-unit>
        <trans-unit id="67b9d1bed8ce4778bb74ee8f32cf37a9f88e56b4" translate="yes" xml:space="preserve">
          <source>Logistic Regression CV (aka logit, MaxEnt) classifier.</source>
          <target state="translated">Clasificador CV de regresión logística (alias logit,MaxEnt).</target>
        </trans-unit>
        <trans-unit id="4c4251ffdad99c44ab6ab03dc44e767ed73a387b" translate="yes" xml:space="preserve">
          <source>Logistic function</source>
          <target state="translated">Función logística</target>
        </trans-unit>
        <trans-unit id="90723c3f54f2127002d5e8087f3fd75704debc54" translate="yes" xml:space="preserve">
          <source>Logistic regression is implemented in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional \(\ell_1\), \(\ell_2\) or Elastic-Net regularization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c0f1438d10823ae208574adad9979316ecf1f7d" translate="yes" xml:space="preserve">
          <source>Logistic regression on raw pixel values is presented for comparison. The example shows that the features extracted by the BernoulliRBM help improve the classification accuracy.</source>
          <target state="translated">La regresión logística de los valores de los píxeles en bruto se presenta para su comparación.El ejemplo muestra que las características extraídas por el BernoulliRBM ayudan a mejorar la precisión de la clasificación.</target>
        </trans-unit>
        <trans-unit id="f05fe21aed88fc82a5a0513559ae877673e205fc" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation</source>
          <target state="translated">Regresión logística con validación cruzada incorporada</target>
        </trans-unit>
        <trans-unit id="96304f46c8b5deeee00fad5a320967f13f13e69f" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3b2957f5500f497ec4678d49dfe4396dcf43781" translate="yes" xml:space="preserve">
          <source>Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;logistic function&lt;/a&gt;.</source>
          <target state="translated">La regresi&amp;oacute;n log&amp;iacute;stica, a pesar de su nombre, es un modelo lineal de clasificaci&amp;oacute;n m&amp;aacute;s que de regresi&amp;oacute;n. La regresi&amp;oacute;n log&amp;iacute;stica tambi&amp;eacute;n se conoce en la literatura como regresi&amp;oacute;n logit, clasificaci&amp;oacute;n de m&amp;aacute;xima entrop&amp;iacute;a (MaxEnt) o clasificador log-lineal. En este modelo, las probabilidades que describen los posibles resultados de un solo ensayo se modelan utilizando una &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;funci&amp;oacute;n log&amp;iacute;stica&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="8e225015f4826ba9beac040479565ae8cd20d1cd" translate="yes" xml:space="preserve">
          <source>Logistic regression.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cb16d85d5ce6d29fbfeaf0784f0b2e2c61ca591" translate="yes" xml:space="preserve">
          <source>LogisticRegression</source>
          <target state="translated">LogisticRegression</target>
        </trans-unit>
        <trans-unit id="de456a9443564fc60f026f7b3757765c6c521491" translate="yes" xml:space="preserve">
          <source>LogisticRegression returns well calibrated predictions as it directly optimizes log-loss. In contrast, the other methods return biased probabilities, with different biases per method:</source>
          <target state="translated">LogisticRegression devuelve predicciones bien calibradas ya que optimiza directamente la pérdida de registros.En contraste,los otros métodos devuelven probabilidades sesgadas,con diferentes sesgos por método:</target>
        </trans-unit>
        <trans-unit id="25965de326ab0450ae299a566621b897c09262e7" translate="yes" xml:space="preserve">
          <source>Long-awaited Generalized Linear Models with non-normal loss functions are now available. In particular, three new regressors were implemented: &lt;a href=&quot;../../modules/generated/sklearn.linear_model.poissonregressor#sklearn.linear_model.PoissonRegressor&quot;&gt;&lt;code&gt;PoissonRegressor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.linear_model.gammaregressor#sklearn.linear_model.GammaRegressor&quot;&gt;&lt;code&gt;GammaRegressor&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.tweedieregressor#sklearn.linear_model.TweedieRegressor&quot;&gt;&lt;code&gt;TweedieRegressor&lt;/code&gt;&lt;/a&gt;. The Poisson regressor can be used to model positive integer counts, or relative frequencies. Read more in the &lt;a href=&quot;../../modules/linear_model#generalized-linear-regression&quot;&gt;User Guide&lt;/a&gt;. Additionally, &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; supports a new &amp;lsquo;poisson&amp;rsquo; loss as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83ec89bbbb1925d31612bf071115de8d555d9924" translate="yes" xml:space="preserve">
          <source>Longitude house block longitude</source>
          <target state="translated">Longitud del bloque de casas longitud</target>
        </trans-unit>
        <trans-unit id="900f1aa8e30fd6bc29b8e8d4cae98f421ef462cd" translate="yes" xml:space="preserve">
          <source>Looking at the coefficient plot to gauge feature importance can be misleading as some of them vary on a small scale, while others, like AGE, varies a lot more, several decades.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65fb0d7c39bbac75b47a97a31a7e2974bcfa2753" translate="yes" xml:space="preserve">
          <source>Looking closely at the WAGE distribution reveals that it has a long tail. For this reason, we should take its logarithm to turn it approximately into a normal distribution (linear models such as ridge or lasso work best for a normal distribution of error).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9aa6c9b011905252d25083008c9809acc3f4160" translate="yes" xml:space="preserve">
          <source>Loss function used by the algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2ebf0012d7d593bf1ef0d0a316102397c08a9f0" translate="yes" xml:space="preserve">
          <source>Low-level methods</source>
          <target state="translated">Métodos de bajo nivel</target>
        </trans-unit>
        <trans-unit id="43b8e239b3dbfa96de18c459c0e716f889fbf1ff" translate="yes" xml:space="preserve">
          <source>Lower bound on the lowest predicted value (the minimum value may still be higher). If not set, defaults to -inf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea609f61be1ccbae7413cc55d9401ee01dff16e3" translate="yes" xml:space="preserve">
          <source>Lower bound value on the likelihood (of the training data with respect to the model) of the best fit of inference.</source>
          <target state="translated">Valor límite inferior de la probabilidad (de los datos de capacitación con respecto al modelo)del mejor ajuste de la inferencia.</target>
        </trans-unit>
        <trans-unit id="af301438554e0ee8815f3548a50754545e52e051" translate="yes" xml:space="preserve">
          <source>Lower bound value on the log-likelihood (of the training data with respect to the model) of the best fit of EM.</source>
          <target state="translated">Valor límite inferior de la probabilidad logarítmica (de los datos de formación con respecto al modelo)del mejor ajuste de la EM.</target>
        </trans-unit>
        <trans-unit id="4ada54abc98e483baeab7ae15def52027a7aae96" translate="yes" xml:space="preserve">
          <source>Lower-triangular Cholesky decomposition of the kernel in &lt;code&gt;X_train_&lt;/code&gt;</source>
          <target state="translated">Descomposici&amp;oacute;n de Cholesky triangular inferior del kernel en &lt;code&gt;X_train_&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b93b2eafc7fea724b9bcb08bb1fb9109b8d1d561" translate="yes" xml:space="preserve">
          <source>M. Bawa, T. Condie and P. Ganesan, &amp;ldquo;LSH Forest: Self-Tuning Indexes for Similarity Search&amp;rdquo;, WWW &amp;lsquo;05 Proceedings of the 14th international conference on World Wide Web, 651-660, 2005.</source>
          <target state="translated">M. Bawa, T. Condie y P. Ganesan, &amp;ldquo;LSH Forest: Self-Tuning Indexes for Similarity Search&amp;rdquo;, Actas WWW '05 de la 14&amp;ordf; conferencia internacional sobre World Wide Web, 651-660, 2005.</target>
        </trans-unit>
        <trans-unit id="e8445854a0cf4ad63f8ee64cb2fc2359051f4c85" translate="yes" xml:space="preserve">
          <source>M. Dumont et al, &lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;Fast multi-class image annotation with random subwindows and multiple output randomized trees&lt;/a&gt;, International Conference on Computer Vision Theory and Applications 2009</source>
          <target state="translated">M. Dumont et al, &lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;Anotaci&amp;oacute;n r&amp;aacute;pida de im&amp;aacute;genes de varias clases con subventanas aleatorias y &amp;aacute;rboles aleatorizados de salida m&amp;uacute;ltiple&lt;/a&gt; , Conferencia internacional sobre teor&amp;iacute;a y aplicaciones de la visi&amp;oacute;n por computadora 2009</target>
        </trans-unit>
        <trans-unit id="4f0f168494bf38e0f99da8c5a97b71596101a871" translate="yes" xml:space="preserve">
          <source>M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine, Journal of Machine Learning Research, Vol. 1, 2001.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2422710e8cdc4f555670a3606a875134eadd99fe" translate="yes" xml:space="preserve">
          <source>M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;The Pascal Visual Object Classes (VOC) Challenge&lt;/a&gt;, IJCV 2010.</source>
          <target state="translated">M. Everingham, L. Van Gool, CKI Williams, J. Winn, A. Zisserman, Desaf&amp;iacute;o de &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;clases de objetos visuales (VOC) de&lt;/a&gt; Pascal, IJCV 2010.</target>
        </trans-unit>
        <trans-unit id="aa09c5b3704d1cab7c2f9d80f35ab989ea05cba1" translate="yes" xml:space="preserve">
          <source>MAE output is non-negative floating point. The best value is 0.0.</source>
          <target state="translated">La salida del MAE es un punto flotante no negativo.El mejor valor es 0.0.</target>
        </trans-unit>
        <trans-unit id="dcac13e5386ab7554d99f42d2774ea6dec7dc033" translate="yes" xml:space="preserve">
          <source>MARR</source>
          <target state="translated">MARR</target>
        </trans-unit>
        <trans-unit id="203b5e4f3efe3b38e1b9f876ab3923dffadb1fa1" translate="yes" xml:space="preserve">
          <source>MARR_Unmarried</source>
          <target state="translated">MARR_Unmarried</target>
        </trans-unit>
        <trans-unit id="f4d1d18b18dbadb43dc94aafefb0919124514bb4" translate="yes" xml:space="preserve">
          <source>MEDV Median value of owner-occupied homes in $1000&amp;rsquo;s</source>
          <target state="translated">MEDV Valor medio de las viviendas ocupadas por sus propietarios en $ 1000</target>
        </trans-unit>
        <trans-unit id="33379c640ef1bcb7b4dbc3ceb61d0f9854342e44" translate="yes" xml:space="preserve">
          <source>MKL</source>
          <target state="translated">MKL</target>
        </trans-unit>
        <trans-unit id="a8e1fd8b99167af6d3e02ac86d0a101dabaf0e42" translate="yes" xml:space="preserve">
          <source>MLP can fit a non-linear model to the training data. &lt;code&gt;clf.coefs_&lt;/code&gt; contains the weight matrices that constitute the model parameters:</source>
          <target state="translated">MLP puede ajustar un modelo no lineal a los datos de entrenamiento. &lt;code&gt;clf.coefs_&lt;/code&gt; contiene las matrices de peso que constituyen los par&amp;aacute;metros del modelo:</target>
        </trans-unit>
        <trans-unit id="7fc5f2a7a15f6ccd1641b37c2fb96c6ce75018c2" translate="yes" xml:space="preserve">
          <source>MLP is sensitive to feature scaling.</source>
          <target state="translated">El MLP es sensible al escalamiento de las características.</target>
        </trans-unit>
        <trans-unit id="08431dee59de79a71b4718dbf6ee28e75fee38c3" translate="yes" xml:space="preserve">
          <source>MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.</source>
          <target state="translated">El MLP requiere la sintonización de una serie de hiperparámetros como el número de neuronas,capas e iteraciones ocultas.</target>
        </trans-unit>
        <trans-unit id="e82dbcf8b443c94c79e54d7d53faa6f5db46762a" translate="yes" xml:space="preserve">
          <source>MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples:</source>
          <target state="translated">El MLP se entrena en dos matrices:la matriz X de tamaño (n_muestras,n_características),que contiene las muestras de entrenamiento representadas como vectores de características de punto flotante;y la matriz y de tamaño (n_muestras,),que contiene los valores objetivo (etiquetas de clase)para las muestras de entrenamiento:</target>
        </trans-unit>
        <trans-unit id="f0c27305c85163e665d40daa0f2ca2e458a2e63e" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="translated">Trenes MLP usando &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Descenso de gradiente estoc&amp;aacute;stico&lt;/a&gt; , &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt; o &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt; . El descenso de gradiente estoc&amp;aacute;stico (SGD) actualiza los par&amp;aacute;metros utilizando el gradiente de la funci&amp;oacute;n de p&amp;eacute;rdida con respecto a un par&amp;aacute;metro que necesita adaptaci&amp;oacute;n, es decir</target>
        </trans-unit>
        <trans-unit id="4fc94508d8948819a05a769d08877751f90a9958" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f27922032032bfc1325082b9b33d5f3a9228ddf6" translate="yes" xml:space="preserve">
          <source>MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates \(P(y|x)\) per sample \(x\):</source>
          <target state="translated">El MLP entrena usando Backpropagation.Más precisamente,se entrena usando alguna forma de descenso de gradiente y los gradientes se calculan usando Backpropagation.Para la clasificación,minimiza la función de pérdida de la entropía cruzada,dando un vector de estimaciones de probabilidad \N (P(y|x)\N por muestra:</target>
        </trans-unit>
        <trans-unit id="c8c1d3b7c59691465cb0496f22bcb3604bca5a60" translate="yes" xml:space="preserve">
          <source>MLP uses different loss functions depending on the problem type. The loss function for classification is Cross-Entropy, which in binary case is given as,</source>
          <target state="translated">El MLP utiliza diferentes funciones de pérdida dependiendo del tipo de problema.La función de pérdida para la clasificación es la de la entropía cruzada,que en el caso binario se da como,</target>
        </trans-unit>
        <trans-unit id="d03f0b750d6ad970862b8ceab4b82a667eb1bc44" translate="yes" xml:space="preserve">
          <source>MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.</source>
          <target state="translated">Los MLP con capas ocultas tienen una función de pérdida no convexa cuando existe más de un mínimo local.Por lo tanto,las diferentes inicializaciones de peso aleatorio pueden llevar a una precisión de validación diferente.</target>
        </trans-unit>
        <trans-unit id="14160d0f2e53b28f2f5c2a7702cb0510220c29b8" translate="yes" xml:space="preserve">
          <source>MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">El MLPClassifier se entrena de forma iterativa ya que en cada paso temporal se calculan las derivadas parciales de la función de pérdida con respecto a los parámetros del modelo para actualizar los parámetros.</target>
        </trans-unit>
        <trans-unit id="a1925f1c916c80accddbe48b0d0e8da75d102083" translate="yes" xml:space="preserve">
          <source>MLPRegressor</source>
          <target state="translated">MLPRegressor</target>
        </trans-unit>
        <trans-unit id="8b27d0c0c6a8a44ae6f32f660e2bfb892d109024" translate="yes" xml:space="preserve">
          <source>MLPRegressor trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">El MLPRegressor se entrena de manera iterativa ya que en cada paso de tiempo se calculan las derivadas parciales de la función de pérdida con respecto a los parámetros del modelo para actualizar los parámetros.</target>
        </trans-unit>
        <trans-unit id="8e70290c1fc16432a8f4b616e4fd6fbaa4abfbea" translate="yes" xml:space="preserve">
          <source>MNIST classfification using multinomial logistic + L1</source>
          <target state="translated">Clasificación del MNIST usando logística multinomial+L1</target>
        </trans-unit>
        <trans-unit id="fbb70ceca40f03de1a52a87cdebacfd0c2426668" translate="yes" xml:space="preserve">
          <source>MNIST classification using multinomial logistic + L1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25845da185fe3a02cb60c18fcf84202e8f31d1e7" translate="yes" xml:space="preserve">
          <source>Machine learning algorithms need data. Go to each &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; sub-folder and run the &lt;code&gt;fetch_data.py&lt;/code&gt; script from there (after having read them first).</source>
          <target state="translated">Los algoritmos de aprendizaje autom&amp;aacute;tico necesitan datos. Vaya a cada subcarpeta &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; y ejecute el script &lt;code&gt;fetch_data.py&lt;/code&gt; desde all&amp;iacute; (despu&amp;eacute;s de haberlos le&amp;iacute;do primero).</target>
        </trans-unit>
        <trans-unit id="45f2bd27f62f0226a5b3177e6a59d79cc23fea68" translate="yes" xml:space="preserve">
          <source>Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the &lt;strong&gt;training set&lt;/strong&gt;, on which we learn some properties; we call the other set the &lt;strong&gt;testing set&lt;/strong&gt;, on which we test the learned properties.</source>
          <target state="translated">El aprendizaje autom&amp;aacute;tico consiste en aprender algunas propiedades de un conjunto de datos y luego probar esas propiedades con otro conjunto de datos. Una pr&amp;aacute;ctica com&amp;uacute;n en el aprendizaje autom&amp;aacute;tico es evaluar un algoritmo dividiendo un conjunto de datos en dos. A uno de esos conjuntos lo llamamos conjunto de &lt;strong&gt;entrenamiento&lt;/strong&gt; , en el que aprendemos algunas propiedades; llamamos al otro conjunto el conjunto de &lt;strong&gt;prueba&lt;/strong&gt; , en el que probamos las propiedades aprendidas.</target>
        </trans-unit>
        <trans-unit id="17dc705c260bdc393406dc006335656d8788b655" translate="yes" xml:space="preserve">
          <source>Machine learning: the problem setting</source>
          <target state="translated">Aprendizaje de la máquina:la configuración del problema</target>
        </trans-unit>
        <trans-unit id="e6a69273199992ddfe41f469dda4cc1f6b79ceb0" translate="yes" xml:space="preserve">
          <source>Magnesium</source>
          <target state="translated">Magnesium</target>
        </trans-unit>
        <trans-unit id="2bb08573261ae718ebb52db49951821b64f9a80c" translate="yes" xml:space="preserve">
          <source>Magnesium:</source>
          <target state="translated">Magnesium:</target>
        </trans-unit>
        <trans-unit id="ca3ae45b6eafdd0a6dfea23df60842bdb389e9fc" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;a href=&quot;#sklearn.covariance.EllipticEnvelope.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is called) observations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55e9152468183c8a16be469ac6ea3dbb0e42dd62" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;a href=&quot;#sklearn.covariance.MinCovDet.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is called) observations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91059cae8d3b76142d7879b78c0a2ccaab268e7d" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;code&gt;fit&lt;/code&gt; is called) observations.</source>
          <target state="translated">Distancias de Mahalanobis del conjunto de entrenamiento (en el que se llama &lt;code&gt;fit&lt;/code&gt; ) observaciones.</target>
        </trans-unit>
        <trans-unit id="d469f730cc4a1b58c5ef61209e446f59013c3c80" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances to centers</source>
          <target state="translated">Las distancias de Mahalanobis a los centros</target>
        </trans-unit>
        <trans-unit id="6f98cc22ed52c0a1e40fae778fadcd35627c25b5" translate="yes" xml:space="preserve">
          <source>MahalanobisDistance</source>
          <target state="translated">MahalanobisDistance</target>
        </trans-unit>
        <trans-unit id="62bce9422ff2d14f69ab80a154510232fc8a9afd" translate="yes" xml:space="preserve">
          <source>Main</source>
          <target state="translated">Main</target>
        </trans-unit>
        <trans-unit id="7a412cc8631eaef465ac98b25829da66ced3b43d" translate="yes" xml:space="preserve">
          <source>Main takeaways</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d6381188443dad8aa5d016fb4ec69dd96237200" translate="yes" xml:space="preserve">
          <source>Make a copy of input data.</source>
          <target state="translated">Haga una copia de los datos de entrada.</target>
        </trans-unit>
        <trans-unit id="f11963f5d19078a49cfab3cc41da5922accd3330" translate="yes" xml:space="preserve">
          <source>Make a large circle containing a smaller circle in 2d.</source>
          <target state="translated">Haz un círculo grande que contenga un círculo más pequeño en 2d.</target>
        </trans-unit>
        <trans-unit id="1cce5fef6c99c293eee32e22f026e768f4b5d892" translate="yes" xml:space="preserve">
          <source>Make a scorer from a performance metric or loss function.</source>
          <target state="translated">Hacer un marcador a partir de una métrica de rendimiento o de una función de pérdida.</target>
        </trans-unit>
        <trans-unit id="723bb825ac5b7b14f789cce44f7b667d39fe6543" translate="yes" xml:space="preserve">
          <source>Make and use a deep copy of X and Y (if Y exists)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bdd3abd6a5ef3ffb2c4167fd4f1b6dc60d7a55bd" translate="yes" xml:space="preserve">
          <source>Make arrays indexable for cross-validation.</source>
          <target state="translated">Hacer las matrices indexables para su validación cruzada.</target>
        </trans-unit>
        <trans-unit id="5f9f781fbc2f44502a26f1a8945369508ac6ebd5" translate="yes" xml:space="preserve">
          <source>Make pipeline to preprocess the data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d28cad808149ac4eb3c924434a0979417f24a68" translate="yes" xml:space="preserve">
          <source>Make sure that X has a minimum number of samples in its first axis (rows for a 2D array).</source>
          <target state="translated">Asegúrate de que X tenga un número mínimo de muestras en su primer eje (filas para una matriz 2D).</target>
        </trans-unit>
        <trans-unit id="b2b1ee415b35f3ec33d28dbc91a8479edeb8d71e" translate="yes" xml:space="preserve">
          <source>Make sure that array is 2D, square and symmetric.</source>
          <target state="translated">Asegúrate de que la matriz sea 2D,cuadrada y simétrica.</target>
        </trans-unit>
        <trans-unit id="6885424e5e7bfa46a7e7c7cb1bd5d6e804bbccd9" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when X has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">Aseg&amp;uacute;rese de que la matriz 2D tenga un n&amp;uacute;mero m&amp;iacute;nimo de caracter&amp;iacute;sticas (columnas). El valor predeterminado de 1 rechaza los conjuntos de datos vac&amp;iacute;os. Esta verificaci&amp;oacute;n solo se aplica cuando X tiene efectivamente 2 dimensiones o es originalmente 1D y &lt;code&gt;ensure_2d&lt;/code&gt; es True. El ajuste a 0 desactiva esta verificaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="37a276f69711964822e7fcec88111a5f6d2f84a0" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when the input data has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">Aseg&amp;uacute;rese de que la matriz 2D tenga un n&amp;uacute;mero m&amp;iacute;nimo de caracter&amp;iacute;sticas (columnas). El valor predeterminado de 1 rechaza los conjuntos de datos vac&amp;iacute;os. Esta verificaci&amp;oacute;n solo se aplica cuando los datos de entrada tienen efectivamente 2 dimensiones o son originalmente 1D y &lt;code&gt;ensure_2d&lt;/code&gt; es True. El ajuste a 0 desactiva esta verificaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="c24d8a1e4fcddcfde73957adfbe65fb40c76c786" translate="yes" xml:space="preserve">
          <source>Make sure that the array has a minimum number of samples in its first axis (rows for a 2D array). Setting to 0 disables this check.</source>
          <target state="translated">Asegúrate de que la matriz tiene un número mínimo de muestras en su primer eje (filas para una matriz 2D).Ponerlo a 0 desactiva esta comprobación.</target>
        </trans-unit>
        <trans-unit id="fdc3084b2db3fff3561874bdbe81f2954a4b0ffc" translate="yes" xml:space="preserve">
          <source>Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise. See &lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt; for convenient ways of scaling heterogeneous data.</source>
          <target state="translated">Aseg&amp;uacute;rese de que se utilice la misma escala en todas las funciones. Debido a que varios m&amp;eacute;todos de aprendizaje se basan en una b&amp;uacute;squeda del vecino m&amp;aacute;s cercano, el algoritmo puede funcionar mal de otra manera. Consulte &lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt; para conocer formas convenientes de escalar datos heterog&amp;eacute;neos.</target>
        </trans-unit>
        <trans-unit id="bd58d70c4390b61fe411e7823adf47cfb5052e9e" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration (used by default). Also, ideally, features should be standardized using e.g. &lt;code&gt;make_pipeline(StandardScaler(), SGDClassifier())&lt;/code&gt; (see &lt;a href=&quot;compose#combining-estimators&quot;&gt;Pipelines&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f7e62e13bb8885a4df4d0d5a8e1dba7c3c65c15" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration.</source>
          <target state="translated">Aseg&amp;uacute;rate de permutar (mezclar) tus datos de entrenamiento antes de ajustar el modelo o usa &lt;code&gt;shuffle=True&lt;/code&gt; para mezclar despu&amp;eacute;s de cada iteraci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="f48f3a474378f962985a41275dc98355541c63d2" translate="yes" xml:space="preserve">
          <source>Make two interleaving half circles</source>
          <target state="translated">Haz dos semicírculos intercalados</target>
        </trans-unit>
        <trans-unit id="69015b0f2ce90a52d27e40a08b160550cfb23a90" translate="yes" xml:space="preserve">
          <source>Making predictions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a0a9871e0af603535e4f6104cfca3266e203a87" translate="yes" xml:space="preserve">
          <source>Malic Acid:</source>
          <target state="translated">Ácido málico:</target>
        </trans-unit>
        <trans-unit id="245748b8f3a70aaac9759204b7d3978b9d337db8" translate="yes" xml:space="preserve">
          <source>Malic acid</source>
          <target state="translated">El ácido málico</target>
        </trans-unit>
        <trans-unit id="a81a721fb7e702ed0a37d056ec4a9d2f925e70b0" translate="yes" xml:space="preserve">
          <source>ManhattanDistance</source>
          <target state="translated">ManhattanDistance</target>
        </trans-unit>
        <trans-unit id="ccbe2127be70aaa7c3514b3155dd29902fae6143" translate="yes" xml:space="preserve">
          <source>Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.</source>
          <target state="translated">El Aprendizaje Múltiple puede ser pensado como un intento de generalizar marcos lineales como el PCA para ser sensible a la estructura no lineal en los datos.Aunque existen variantes supervisadas,el típico problema de aprendizaje múltiple no está supervisado:aprende la estructura altamente dimensional de los datos a partir de los propios datos,sin el uso de clasificaciones predeterminadas.</target>
        </trans-unit>
        <trans-unit id="7a0e60acb472080022463866637a1ea7c0251335" translate="yes" xml:space="preserve">
          <source>Manifold Learning methods on a severed sphere</source>
          <target state="translated">Múltiples métodos de aprendizaje en una esfera cortada</target>
        </trans-unit>
        <trans-unit id="5f7bca3c10846eb5854c536c3448fedf998ffbcf" translate="yes" xml:space="preserve">
          <source>Manifold learning</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aca365adba00c10f7a3cc50cff4a88afd0947dd9" translate="yes" xml:space="preserve">
          <source>Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.</source>
          <target state="translated">El aprendizaje múltiple es un enfoque para la reducción de la dimensionalidad no lineal.Los algoritmos para esta tarea se basan en la idea de que la dimensionalidad de muchos conjuntos de datos es sólo artificialmente alta.</target>
        </trans-unit>
        <trans-unit id="1e718f0bccbec4566b4c8536fdd24c184318a8a9" translate="yes" xml:space="preserve">
          <source>Manifold learning on handwritten digits: Locally Linear Embedding, Isomap&amp;hellip;</source>
          <target state="translated">Aprendizaje m&amp;uacute;ltiple en d&amp;iacute;gitos escritos a mano: incrustaci&amp;oacute;n lineal local, Isomap ...</target>
        </trans-unit>
        <trans-unit id="ad97dd09eb7e528a0b4debe1b9c745e650b7307a" translate="yes" xml:space="preserve">
          <source>Manually setting one of the environment variables (&lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt;, &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt;, or &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt;) will take precedence over what joblib tries to do. The total number of threads will be &lt;code&gt;n_jobs * &amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt;. Note that setting this limit will also impact your computations in the main process, which will only use &lt;code&gt;&amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt;. Joblib exposes a context manager for finer control over the number of threads in its workers (see joblib docs linked below).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0471386edfe0bf057e3ed471b66689189fdc892c" translate="yes" xml:space="preserve">
          <source>Manufacturing</source>
          <target state="translated">Manufacturing</target>
        </trans-unit>
        <trans-unit id="0d3695eb907329bab9f0e9752d7ff00d200420c9" translate="yes" xml:space="preserve">
          <source>Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an &lt;em&gt;inlier&lt;/em&gt;), or should be considered as different (it is an &lt;em&gt;outlier&lt;/em&gt;). Often, this ability is used to clean real data sets. Two important distinctions must be made:</source>
          <target state="translated">Muchas aplicaciones requieren poder decidir si una nueva observaci&amp;oacute;n pertenece a la misma distribuci&amp;oacute;n que las observaciones existentes (es un valor &lt;em&gt;inlier&lt;/em&gt; ), o debe considerarse diferente (es un &lt;em&gt;valor at&amp;iacute;pico&lt;/em&gt; ). A menudo, esta capacidad se utiliza para limpiar conjuntos de datos reales. Deben hacerse dos distinciones importantes:</target>
        </trans-unit>
        <trans-unit id="626f0980ad5cd6d2b6f18a99ff094a7bf141dc9a" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints</source>
          <target state="translated">Muchos grupos,posiblemente con limitaciones de conectividad</target>
        </trans-unit>
        <trans-unit id="9d1190903d42ddc70f3db2311f157c56b6260b92" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints, non Euclidean distances</source>
          <target state="translated">Muchos conglomerados,posiblemente con limitaciones de conectividad,distancias no euclidianas</target>
        </trans-unit>
        <trans-unit id="ac65e2f8a158fa7cc404d708906171f5ea9f26fd" translate="yes" xml:space="preserve">
          <source>Many clusters, uneven cluster size, non-flat geometry</source>
          <target state="translated">Muchos cúmulos,tamaño desigual de los cúmulos,geometría no plana</target>
        </trans-unit>
        <trans-unit id="241eda779a46dbe514b8b7f2a96e98aed4d935af" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="translated">Muchos conjuntos de datos contienen caracter&amp;iacute;sticas de diferentes tipos, por ejemplo, texto, flotantes y fechas, donde cada tipo de caracter&amp;iacute;stica requiere pasos separados de preprocesamiento o extracci&amp;oacute;n de caracter&amp;iacute;sticas. A menudo, es m&amp;aacute;s f&amp;aacute;cil preprocesar los datos antes de aplicar m&amp;eacute;todos de scikit-learn, por ejemplo, usando &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt; . Procesar sus datos antes de pasarlos a scikit-learn puede ser problem&amp;aacute;tico por una de las siguientes razones:</target>
        </trans-unit>
        <trans-unit id="d589144c9193e129ab4721a317a3dc3eaa909106" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="192c25a6ed904d1327958fde4c93098505cb86b8" translate="yes" xml:space="preserve">
          <source>Many metrics are not given names to be used as &lt;code&gt;scoring&lt;/code&gt; values, sometimes because they require additional parameters, such as &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;. In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;. That function converts metrics into callables that can be used for model evaluation.</source>
          <target state="translated">Muchas m&amp;eacute;tricas no reciben nombres para &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt; como valores de &lt;code&gt;scoring&lt;/code&gt; , a veces porque requieren par&amp;aacute;metros adicionales, como fbeta_score . En tales casos, debe generar un objeto de puntuaci&amp;oacute;n adecuado. La forma m&amp;aacute;s sencilla de generar un objeto invocable para puntuar es utilizando &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt; . Esa funci&amp;oacute;n convierte las m&amp;eacute;tricas en callables que se pueden usar para la evaluaci&amp;oacute;n del modelo.</target>
        </trans-unit>
        <trans-unit id="c6eff6cfb2b81378a219da9247d45ca528d18b55" translate="yes" xml:space="preserve">
          <source>Many scikit-learn estimators rely on nearest neighbors: Several classifiers and regressors such as &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.kneighborsregressor#sklearn.neighbors.KNeighborsRegressor&quot;&gt;&lt;code&gt;KNeighborsRegressor&lt;/code&gt;&lt;/a&gt;, but also some clustering methods such as &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;SpectralClustering&lt;/code&gt;&lt;/a&gt;, and some manifold embeddings such as &lt;a href=&quot;generated/sklearn.manifold.tsne#sklearn.manifold.TSNE&quot;&gt;&lt;code&gt;TSNE&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;Isomap&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a89fb6cdb80688c2fbf1190407f9abec89dd4f3" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dacd10610ea3bac36f91971d47d3019273ca964d" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;code&gt;sklearn.covariance&lt;/code&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="translated">Muchos problemas estad&amp;iacute;sticos requieren la estimaci&amp;oacute;n de la matriz de covarianza de una poblaci&amp;oacute;n, que puede verse como una estimaci&amp;oacute;n de la forma del diagrama de dispersi&amp;oacute;n del conjunto de datos. La mayor&amp;iacute;a de las veces, dicha estimaci&amp;oacute;n debe realizarse en una muestra cuyas propiedades (tama&amp;ntilde;o, estructura, homogeneidad) tienen una gran influencia en la calidad de la estimaci&amp;oacute;n. El paquete &lt;code&gt;sklearn.covariance&lt;/code&gt; proporciona herramientas para estimar con precisi&amp;oacute;n la matriz de covarianza de una poblaci&amp;oacute;n en varios entornos.</target>
        </trans-unit>
        <trans-unit id="be7bf3b7e371f4bec9a03a7522f6dcf31d112a68" translate="yes" xml:space="preserve">
          <source>Many, many more &amp;hellip;</source>
          <target state="translated">Mucho, mucho, mas &amp;hellip;</target>
        </trans-unit>
        <trans-unit id="01a4f781a04bf81d6d3609180ff5b158082bf232" translate="yes" xml:space="preserve">
          <source>Map data to a normal distribution</source>
          <target state="translated">Los datos del mapa a una distribución normal</target>
        </trans-unit>
        <trans-unit id="16409bc40b2df043ac11786860ad0f327aa511b9" translate="yes" xml:space="preserve">
          <source>Maps data to a normal distribution using a power transformation.</source>
          <target state="translated">Mapea los datos a una distribución normal usando una transformación de energía.</target>
        </trans-unit>
        <trans-unit id="05aecccd2b32722fa423ccbd7840d48763834385" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">Asigna datos a una distribuci&amp;oacute;n normal est&amp;aacute;ndar con el par&amp;aacute;metro &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="12f61571505f27a87deaf94928434363c3add704" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution='normal'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2546740d19a0cb39e3dfa40dd82138fa02a22968" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list.</source>
          <target state="translated">Mapea cada nombre de característica categórica a una lista de valores,de tal manera que el valor codificado como i es i en la lista.</target>
        </trans-unit>
        <trans-unit id="67b185421bf5d928c6a7d5bf74ee20d677f50228" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list. If &lt;code&gt;as_frame&lt;/code&gt; is True, this is None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="601b228138151f5d614818578f5a990f06465ee3" translate="yes" xml:space="preserve">
          <source>Marginal distribution for the transformed data. The choices are &amp;lsquo;uniform&amp;rsquo; (default) or &amp;lsquo;normal&amp;rsquo;.</source>
          <target state="translated">Distribuci&amp;oacute;n marginal de los datos transformados. Las opciones son 'uniforme' (predeterminado) o 'normal'.</target>
        </trans-unit>
        <trans-unit id="32cec489ab51eb304acc5d56c34e0b5894817af1" translate="yes" xml:space="preserve">
          <source>Mark Schmidt, Nicolas Le Roux, and Francis Bach: &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;Minimizing Finite Sums with the Stochastic Average Gradient.&lt;/a&gt;</source>
          <target state="translated">Mark Schmidt, Nicolas Le Roux y Francis Bach: &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;Minimizaci&amp;oacute;n de sumas finitas con el gradiente medio estoc&amp;aacute;stico.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c75a2b42a0ab364e58b54559c45fcda50d1973f4" translate="yes" xml:space="preserve">
          <source>Married</source>
          <target state="translated">Married</target>
        </trans-unit>
        <trans-unit id="b66f191d027329ba9273c4c5f9be765f9b3745f5" translate="yes" xml:space="preserve">
          <source>Mask to be used on X.</source>
          <target state="translated">Máscara para ser usada en X.</target>
        </trans-unit>
        <trans-unit id="54a21a4d94fa24c6c092fd6e4c2ec05359c76c09" translate="yes" xml:space="preserve">
          <source>MatchingDistance</source>
          <target state="translated">MatchingDistance</target>
        </trans-unit>
        <trans-unit id="38c6b835ca8294e13538ac219d64ae1244beb7fc" translate="yes" xml:space="preserve">
          <source>Matern kernel.</source>
          <target state="translated">Núcleo materno.</target>
        </trans-unit>
        <trans-unit id="c2846fd5b2a8440131137c07fea40912afc701b7" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with \(\ell_1\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">Matemáticamente,consiste en un modelo lineal entrenado con anterioridad como regularizador.La función objetiva a minimizar es:</target>
        </trans-unit>
        <trans-unit id="bf1818d1c45b1997515a16368907c8cf902bab56" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior and \(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">Matemáticamente,consiste en un modelo lineal entrenado con un prior mixto y un prior como regularizador.La función objetiva a minimizar es:</target>
        </trans-unit>
        <trans-unit id="1d9fe275a9038555cabcfe46b4a675067788d84f" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">Matemáticamente,consiste en un modelo lineal entrenado con un priorizador mixto como regularizador.La función objetiva a minimizar es:</target>
        </trans-unit>
        <trans-unit id="85bbf115a5892290f61e157be8a21f18c7185291" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\)-norm and \(\ell_2\)-norm for regularization. The objective function to minimize is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a22bf31bc3080adb481367d05d21298b6681e5ee" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\)-norm for regularization. The objective function to minimize is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47b46d2e2bec7b475ce40ef0c30d61c9f27a62f1" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="461064fec990b9f56bd78c5da4699263962dc67b" translate="yes" xml:space="preserve">
          <source>Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation : \(\Sigma_{\rm shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm Tr}\hat{\Sigma}}{p}\rm Id\).</source>
          <target state="translated">Matemáticamente,esta contracción consiste en reducir la relación entre los valores propios más pequeños y más grandes de la matriz de covarianza empírica.Se puede hacer simplemente desplazando cada valor propio según una compensación dada,lo que equivale a encontrar el estimador de máxima probabilidad penalizado en l2 de la matriz de covarianza.En la práctica,la contracción se reduce a una simple transformación convexa:\Sigma encogido=(1-alfa)que es un Sigma+alfafraccionado.</target>
        </trans-unit>
        <trans-unit id="7f2fa948973686599d9719cd22bf9c261bdbf5a5" translate="yes" xml:space="preserve">
          <source>Mathematically, truncated SVD applied to training samples \(X\) produces a low-rank approximation \(X\):</source>
          <target state="translated">Matemáticamente,la SVD truncada aplicada a las muestras de entrenamiento (X)produce una aproximación de bajo rango (X):</target>
        </trans-unit>
        <trans-unit id="b8c6141893596b10260b39727bf4a66986a56a95" translate="yes" xml:space="preserve">
          <source>Matrices:</source>
          <target state="translated">Matrices:</target>
        </trans-unit>
        <trans-unit id="878abbe8708b2c0d949ede6590fbf80f3b3ca712" translate="yes" xml:space="preserve">
          <source>Matrix \(C\) such that \(C_{i, j}\) is the number of samples in true class \(i\) and in predicted class \(j\). If &lt;code&gt;eps is None&lt;/code&gt;, the dtype of this array will be integer. If &lt;code&gt;eps&lt;/code&gt; is given, the dtype will be float. Will be a &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; if &lt;code&gt;sparse=True&lt;/code&gt;.</source>
          <target state="translated">Matriz \ (C \) tal que \ (C_ {i, j} \) es el n&amp;uacute;mero de muestras en la clase verdadera \ (i \) y en la clase predicha \ (j \). Si &lt;code&gt;eps is None&lt;/code&gt; , el dtype de esta matriz ser&amp;aacute; un n&amp;uacute;mero entero. Si se da &lt;code&gt;eps&lt;/code&gt; , el tipo d ser&amp;aacute; flotante. Ser&amp;aacute; un &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; si &lt;code&gt;sparse=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="9a9d3bf25623c95ec103c9ba9ebefafc39b31e10" translate="yes" xml:space="preserve">
          <source>Matrix of similarities between points</source>
          <target state="translated">Matriz de similitudes entre puntos</target>
        </trans-unit>
        <trans-unit id="fe09cc11ed56c787dbf583e1d3c86930e73d13b7" translate="yes" xml:space="preserve">
          <source>Matrix to be scaled.</source>
          <target state="translated">Matriz a escala.</target>
        </trans-unit>
        <trans-unit id="f581a8973d13aee9e6d301f776c7ac0aca9937df" translate="yes" xml:space="preserve">
          <source>Matrix to decompose</source>
          <target state="translated">La matriz para descomponerse</target>
        </trans-unit>
        <trans-unit id="64c58969af0dfb121c9b8582a353fed07f3ae81a" translate="yes" xml:space="preserve">
          <source>Matrix to normalize using the variance of the features.</source>
          <target state="translated">Matriz para normalizar usando la variación de las características.</target>
        </trans-unit>
        <trans-unit id="dc67599b55e21aadb81c15a2c42c0d243f238c7f" translate="yes" xml:space="preserve">
          <source>Matrix whose two columns are to be swapped.</source>
          <target state="translated">Matriz cuyas dos columnas deben ser intercambiadas.</target>
        </trans-unit>
        <trans-unit id="2ea8698954891f702cc6556af5a70f4a4777910c" translate="yes" xml:space="preserve">
          <source>Matrix whose two rows are to be swapped.</source>
          <target state="translated">Matriz cuyas dos filas deben ser intercambiadas.</target>
        </trans-unit>
        <trans-unit id="91c27cd36373d9f3e97d3e8652e4d5420038195e" translate="yes" xml:space="preserve">
          <source>Max number of iterations for updating document topic distribution in the E-step.</source>
          <target state="translated">Número máximo de iteraciones para la actualización de la distribución de temas de documentos en el E-step.</target>
        </trans-unit>
        <trans-unit id="00c71f39eb3784f568496e9b201bef34cf1fba1e" translate="yes" xml:space="preserve">
          <source>MaxAbsScaler</source>
          <target state="translated">MaxAbsScaler</target>
        </trans-unit>
        <trans-unit id="b19f6ae06ce0301b0f2f115ace4b151976f71361" translate="yes" xml:space="preserve">
          <source>Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between \(q(z,\theta,\beta)\) and the true posterior \(p(z, \theta, \beta |w, \alpha, \eta)\).</source>
          <target state="translated">Maximizar el ELBO equivale a minimizar la divergencia Kullback-Leibler(KL)entre \ ~ q(z,\N-theta,\N-beta)y el verdadero posterior (p(z,\N-theta,\N-beta,\Nw,\N-alpha,eta)).</target>
        </trans-unit>
        <trans-unit id="c7118c6c94bd33474c6bd73b2a0ef4d05bd61b9a" translate="yes" xml:space="preserve">
          <source>Maximizing the log-marginal-likelihood after subtracting the target&amp;rsquo;s mean yields the following kernel with an LML of -83.214:</source>
          <target state="translated">Maximizar la probabilidad logar&amp;iacute;tmica marginal despu&amp;eacute;s de restar la media del objetivo produce el siguiente kernel con un LML de -83,214:</target>
        </trans-unit>
        <trans-unit id="3cc68e53e734e045e272d9b61d6ce440314a7c5a" translate="yes" xml:space="preserve">
          <source>Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma. If an array is given, it will compute a safe number of components array-wise.</source>
          <target state="translated">Tasa de distorsión máxima definida por el lema de Johnson-Lindenstrauss.Si se da una matriz,se calculará un número seguro de componentes por matriz.</target>
        </trans-unit>
        <trans-unit id="2648af65469bf6064127630edb239d63fa9087cb" translate="yes" xml:space="preserve">
          <source>Maximum likelihood covariance estimator</source>
          <target state="translated">Estimador de covarianza de máxima probabilidad</target>
        </trans-unit>
        <trans-unit id="c549d82a160dc50758b33cda113fa1dc7a80727c" translate="yes" xml:space="preserve">
          <source>Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</source>
          <target state="translated">Norma máxima del residuo.Si no hay ninguno,anula los n_nonzero_coefs.</target>
        </trans-unit>
        <trans-unit id="c4d8a154588727ab7c620ef2088eb03d03fdb2cd" translate="yes" xml:space="preserve">
          <source>Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes.</source>
          <target state="translated">Número máximo de subconjuntos de FQ en cada nodo.Si una nueva muestra entra de tal manera que el número de subconjuntos excede el factor_rama,entonces ese nodo se divide en dos nodos con los subconjuntos redistribuidos en cada uno.El subclúster padre de ese nodo se elimina y se añaden dos nuevos subclústeres como padres de los 2 nodos divididos.</target>
        </trans-unit>
        <trans-unit id="ffcbfb393ae9341f5e6cf4dea80093dbb65c654d" translate="yes" xml:space="preserve">
          <source>Maximum number of epochs to not meet &lt;code&gt;tol&lt;/code&gt; improvement. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">N&amp;uacute;mero m&amp;aacute;ximo de &amp;eacute;pocas para no cumplir con la mejora de la &lt;code&gt;tol&lt;/code&gt; . Solo es efectivo cuando solver = 'sgd' o 'adam'</target>
        </trans-unit>
        <trans-unit id="7d6dde474581154e61a4c14f0f2c50aef90aa524" translate="yes" xml:space="preserve">
          <source>Maximum number of imputation rounds to perform before returning the imputations computed during the final round. A round is a single imputation of each feature with missing values. The stopping criterion is met once &lt;code&gt;abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))&lt;/code&gt; &amp;lt; tol, where &lt;code&gt;X_t&lt;/code&gt; is &lt;code&gt;X&lt;/code&gt; at iteration &lt;code&gt;t. Note that early stopping is only
applied if ``sample_posterior=False`&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d31095f21fc709b8362fbfbc8701ee49bed43f4" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations</source>
          <target state="translated">Número máximo de iteraciones</target>
        </trans-unit>
        <trans-unit id="ec0c7c7cfd1a4dd46776f2839b3e2ee30db0ceb0" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations allowed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d0f629c611a546c50fbd29c0a5c09d14523502f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations before timing out.</source>
          <target state="translated">Número máximo de iteraciones antes de la sincronización.</target>
        </trans-unit>
        <trans-unit id="e5ab15aeae2ebd19c6cc8dd9b722001d143407e7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations during fit.</source>
          <target state="translated">Número máximo de iteraciones durante el ajuste.</target>
        </trans-unit>
        <trans-unit id="4bd775e3e4801f1199d0d4b78f5390120406b7db" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack.</source>
          <target state="translated">Número máximo de iteraciones para el arpack.Si no hay ninguna,el valor óptimo será elegido por arpack.</target>
        </trans-unit>
        <trans-unit id="f374a3956c9375a530255caa54ee43ca08273ef7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; solver, the default value is 1000.</source>
          <target state="translated">N&amp;uacute;mero m&amp;aacute;ximo de iteraciones para el solucionador de gradiente conjugado. Para los solucionadores 'sparse_cg' y 'lsqr', scipy.sparse.linalg determina el valor predeterminado. Para el solucionador 'sag', el valor predeterminado es 1000.</target>
        </trans-unit>
        <trans-unit id="11f341e8f36dbf9c1af7cfa8e66dcc9686e34f6b" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For the &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; and saga solver, the default value is 1000.</source>
          <target state="translated">N&amp;uacute;mero m&amp;aacute;ximo de iteraciones para el solucionador de gradiente conjugado. Para los solucionadores 'sparse_cg' y 'lsqr', scipy.sparse.linalg determina el valor predeterminado. Para 'sag' y saga solver, el valor predeterminado es 1000.</target>
        </trans-unit>
        <trans-unit id="c5354ceb6cfff4ad460f2a35427697293dda616c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.</source>
          <target state="translated">Número máximo de iteraciones para el solucionador de gradientes conjugados.El valor por defecto está determinado por scipy.sparse.linalg.</target>
        </trans-unit>
        <trans-unit id="1e7dfd80e629f3bb34ea64892d98c69b612b79e1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for random sample selection.</source>
          <target state="translated">Número máximo de iteraciones para la selección de muestras aleatorias.</target>
        </trans-unit>
        <trans-unit id="ecd130d87b8a2f3d02f709f684a50be372cae2c9" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the arpack solver. not used if eigen_solver == &amp;lsquo;dense&amp;rsquo;.</source>
          <target state="translated">N&amp;uacute;mero m&amp;aacute;ximo de iteraciones para el solucionador de arpack. no se usa si eigen_solver == 'denso'.</target>
        </trans-unit>
        <trans-unit id="9de38c6ff2395ad8506cb6d44f0cafffcf62c788" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the calculation of spatial median.</source>
          <target state="translated">Número máximo de iteraciones para el cálculo de la mediana espacial.</target>
        </trans-unit>
        <trans-unit id="07f904d8faecfe25c803428c8e7a88d0070e3e3e" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the optimization. Should be at least 250.</source>
          <target state="translated">Número máximo de iteraciones para la optimización.Debería ser de al menos 250.</target>
        </trans-unit>
        <trans-unit id="dce17045503a4e7dfc48c40d90d1aeae843b7e1f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the solver.</source>
          <target state="translated">Número máximo de iteraciones para el solucionador.</target>
        </trans-unit>
        <trans-unit id="23888143df53d4407b7d5db42e819ad8fc40bfb6" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations in the optimization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1c1739cc631f47e83bf7484461b685fd99cca3d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the SMACOF algorithm for a single run.</source>
          <target state="translated">Número máximo de iteraciones del algoritmo SMACOF para una sola corrida.</target>
        </trans-unit>
        <trans-unit id="7f1e71a3c23990192b71291657a3bae2f490d4ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm for a single run.</source>
          <target state="translated">Número máximo de iteraciones del algoritmo k-means para una sola corrida.</target>
        </trans-unit>
        <trans-unit id="f0e6e9653318c3bc8385e39576298e438fdcd759" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm to run.</source>
          <target state="translated">Número máximo de iteraciones del algoritmo k-means a ejecutar.</target>
        </trans-unit>
        <trans-unit id="368dd40a437636dbd3f559d65e7725494d2a9fb1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the optimization algorithm.</source>
          <target state="translated">Número máximo de iteraciones del algoritmo de optimización.</target>
        </trans-unit>
        <trans-unit id="63c06831f070b2a52428ee45e49cb4b88ea5a09d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.</source>
          <target state="translated">Número máximo de iteraciones sobre el conjunto de datos completo antes de detenerse independientemente de cualquier heurística de criterio de detención temprana.</target>
        </trans-unit>
        <trans-unit id="a1da02f682252ffa124e7544bc23e0152d7ce8c8" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations performed on each seed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff75c0f1937b00a0d18d3456b10469179a13bfd5" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations run across all classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d873ede9710528d1162698e1b4121133119149c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations taken for the solvers to converge.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f2d5ee22794f6faeed2ed56167e69832301d9fb" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that &lt;code&gt;scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)&lt;/code&gt; should run for.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7899fd5b3a78738f0401cfc3b35e8ce4d2309778" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by &lt;code&gt;is_data_valid&lt;/code&gt; or invalid models defined by &lt;code&gt;is_model_valid&lt;/code&gt;.</source>
          <target state="translated">N&amp;uacute;mero m&amp;aacute;ximo de iteraciones que se pueden omitir debido a la b&amp;uacute;squeda de cero inliers o datos no v&amp;aacute;lidos definidos por &lt;code&gt;is_data_valid&lt;/code&gt; o modelos no v&amp;aacute;lidos definidos por &lt;code&gt;is_model_valid&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e7fba252520d1990cf2d4eb5716def2f32bbae99" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for.</source>
          <target state="translated">El máximo número de iteraciones que scipy.optimize.fmin_l_bfgs_b debe ejecutar.</target>
        </trans-unit>
        <trans-unit id="4e1098501827a192b62ebb4f1cab51c2d422cf00" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">N&amp;uacute;mero m&amp;aacute;ximo de iteraciones a realizar si &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f8b9c126c90c88245cf150185cad9a95bb9ec437" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm='lasso_cd'&lt;/code&gt; or &lt;code&gt;lasso_lars&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa990833ac4f020e2d41da9d6169624866ecf0ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform in the Lars algorithm.</source>
          <target state="translated">Número máximo de iteraciones a realizar en el algoritmo de Lars.</target>
        </trans-unit>
        <trans-unit id="67f387c33c051b181969f68bc7a00e313e621f7a" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform when solving the lasso problem.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0f4ce7794b613699161c8c6e0e45882e1e59e63" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform, set to infinity for no limit.</source>
          <target state="translated">Número máximo de iteraciones a realizar,fijado al infinito sin límite.</target>
        </trans-unit>
        <trans-unit id="6484f135db2cde17daa0042bcd9839216d734460" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform.</source>
          <target state="translated">Número máximo de iteraciones a realizar.</target>
        </trans-unit>
        <trans-unit id="db2ca83257c5e157920232d66349b60febf20184" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform. Can be used for early stopping.</source>
          <target state="translated">Número máximo de iteraciones a realizar.Puede ser usado para una parada temprana.</target>
        </trans-unit>
        <trans-unit id="3ae0883a212da2486dca35b829a405dbbd2b8c29" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.</source>
          <target state="translated">Número máximo de iteraciones sin progreso antes de abortar la optimización,utilizado después de 250 iteraciones iniciales con exageración temprana.Tenga en cuenta que el progreso sólo se comprueba cada 50 iteraciones,por lo que este valor se redondea al siguiente múltiplo de 50.</target>
        </trans-unit>
        <trans-unit id="5919ba2c46521b64537304f167c62803da4391df" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet.</source>
          <target state="translated">Número máximo de iteraciones,por punto de siembra antes de que termine la operación de agrupación (para ese punto de siembra),si no ha convergido todavía.</target>
        </trans-unit>
        <trans-unit id="3fcea6ff6580050eb63d7142d23e7d7896de7626" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations.</source>
          <target state="translated">Número máximo de iteraciones.</target>
        </trans-unit>
        <trans-unit id="2bd71b5c83b9f5da0d4a94baa31a035271906ce7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300</source>
          <target state="translated">Número máximo de iteraciones.Por defecto es 300</target>
        </trans-unit>
        <trans-unit id="6740f9eed55c5399b0f5fe47513d42802b2d72aa" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300.</source>
          <target state="translated">Número máximo de iteraciones.Por defecto es 300.</target>
        </trans-unit>
        <trans-unit id="78adce28aba1dcec634d8308251161f963f008c1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Should be greater than or equal to 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d3427b734648d0ef9c00f4282011f095b732bba" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. The solver iterates until convergence (determined by &amp;lsquo;tol&amp;rsquo;) or this number of iterations. For stochastic solvers (&amp;lsquo;sgd&amp;rsquo;, &amp;lsquo;adam&amp;rsquo;), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.</source>
          <target state="translated">N&amp;uacute;mero m&amp;aacute;ximo de iteraciones. El solucionador itera hasta la convergencia (determinada por 'tol') o este n&amp;uacute;mero de iteraciones. Para los solucionadores estoc&amp;aacute;sticos ('sgd', 'adam'), tenga en cuenta que esto determina el n&amp;uacute;mero de &amp;eacute;pocas (cu&amp;aacute;ntas veces se utilizar&amp;aacute; cada punto de datos), no el n&amp;uacute;mero de pasos de gradiente.</target>
        </trans-unit>
        <trans-unit id="647156dc0e4267e82660e321d855d0ab57ee6ce8" translate="yes" xml:space="preserve">
          <source>Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices.</source>
          <target state="translated">Número máximo de muestras utilizadas para estimar los cuantiles para la eficiencia de los cálculos.Obsérvese que el procedimiento de submuestreo puede diferir para matrices dispersas y densas de valor idéntico.</target>
        </trans-unit>
        <trans-unit id="415a2ec1c451656db8760ffe077b89b191d3a2b3" translate="yes" xml:space="preserve">
          <source>Maximum numbers of iterations to perform, therefore maximum features to include. 10% of &lt;code&gt;n_features&lt;/code&gt; but at least 5 if available.</source>
          <target state="translated">N&amp;uacute;mero m&amp;aacute;ximo de iteraciones para realizar, por lo tanto, caracter&amp;iacute;sticas m&amp;aacute;ximas para incluir. 10% de &lt;code&gt;n_features&lt;/code&gt; pero al menos 5 si est&amp;aacute;n disponibles.</target>
        </trans-unit>
        <trans-unit id="631012abffd401f8346d1251260aa1bdd321bf8a" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt; or the number of nodes in the path with &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">M&amp;aacute;ximo de covarianzas (en valor absoluto) en cada iteraci&amp;oacute;n. &lt;code&gt;n_alphas&lt;/code&gt; es &lt;code&gt;max_iter&lt;/code&gt; , &lt;code&gt;n_features&lt;/code&gt; o el n&amp;uacute;mero de nodos en la ruta con &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt; , el que sea menor.</target>
        </trans-unit>
        <trans-unit id="705f01a5b973480d43f7edb8b4f1d8b46afffacc" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt;, or the number of nodes in the path with correlation greater than &lt;code&gt;alpha&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">M&amp;aacute;ximo de covarianzas (en valor absoluto) en cada iteraci&amp;oacute;n. &lt;code&gt;n_alphas&lt;/code&gt; es &lt;code&gt;max_iter&lt;/code&gt; , &lt;code&gt;n_features&lt;/code&gt; o el n&amp;uacute;mero de nodos en la ruta con una correlaci&amp;oacute;n mayor que &lt;code&gt;alpha&lt;/code&gt; , el que sea menor.</target>
        </trans-unit>
        <trans-unit id="62989d4a3b259ca439d6192faa2834aa0e75a2e0" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;n_nonzero_coefs&lt;/code&gt; or &lt;code&gt;n_features&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">M&amp;aacute;ximo de covarianzas (en valor absoluto) en cada iteraci&amp;oacute;n. &lt;code&gt;n_alphas&lt;/code&gt; es &lt;code&gt;n_nonzero_coefs&lt;/code&gt; o &lt;code&gt;n_features&lt;/code&gt; , el que sea menor.</target>
        </trans-unit>
        <trans-unit id="2519f5f4a0d8bcad0dcee25fb3c2cfe0d8efda04" translate="yes" xml:space="preserve">
          <source>Maximum possible imputed value. Broadcast to shape (n_features,) if scalar. If array-like, expects shape (n_features,), one max value for each feature. &lt;code&gt;None&lt;/code&gt; (default) is converted to np.inf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e56eb85aade57d415023e1a3a8ae03f5b942a0cd" translate="yes" xml:space="preserve">
          <source>Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">M&amp;aacute;ximo residual para que una muestra de datos se clasifique como un valor interno. De forma predeterminada, el umbral se elige como la DMA (desviaci&amp;oacute;n absoluta media) de los valores objetivo &lt;code&gt;y&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6dd6334c9c1bb29cde2ace0d7ca9c039e7de14bd" translate="yes" xml:space="preserve">
          <source>Maximum size for a single training set.</source>
          <target state="translated">Tamaño máximo para un solo equipo de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="3feffec2bfb871f0142dbd2d75d410b437245309" translate="yes" xml:space="preserve">
          <source>Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.</source>
          <target state="translated">Suma cuadrada máxima de X sobre las muestras.Usado sólo en el solucionador SAG.Si no hay ninguno,se calculará,pasando por todas las muestras.El valor debe ser precalculado para acelerar la validación cruzada.</target>
        </trans-unit>
        <trans-unit id="229948f9503f6467f2a53d61f4254093e7ca3738" translate="yes" xml:space="preserve">
          <source>Maximum step size (regularization). Defaults to 1.0.</source>
          <target state="translated">Tamaño máximo del paso (regularización).Por defecto es 1.0.</target>
        </trans-unit>
        <trans-unit id="843c61e3ff0f74449c911dbb02faa43821e29850" translate="yes" xml:space="preserve">
          <source>Maximum value of a bicluster.</source>
          <target state="translated">Valor máximo de un bíceps.</target>
        </trans-unit>
        <trans-unit id="ce9a39e13a20e687ebff9fcf4496175bdfa0afbb" translate="yes" xml:space="preserve">
          <source>Maximum value of input array &lt;code&gt;X_&lt;/code&gt; for right bound.</source>
          <target state="translated">Valor m&amp;aacute;ximo de la matriz de entrada &lt;code&gt;X_&lt;/code&gt; para el l&amp;iacute;mite derecho.</target>
        </trans-unit>
        <trans-unit id="9fa80bb15d05b082522b43fcb83b05beb6f022c6" translate="yes" xml:space="preserve">
          <source>May be the string &amp;ldquo;jaccard&amp;rdquo; to use the Jaccard coefficient, or any function that takes four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows, b_columns).</source>
          <target state="translated">Puede ser la cadena &quot;jaccard&quot; para usar el coeficiente de Jaccard, o cualquier funci&amp;oacute;n que tome cuatro argumentos, cada uno de los cuales es un vector indicador 1d: (a_rows, a_columns, b_rows, b_columns).</target>
        </trans-unit>
        <trans-unit id="4fad1e9d11d435bd5f0db307b217272d94f19197" translate="yes" xml:space="preserve">
          <source>May contain any subset of (&amp;lsquo;headers&amp;rsquo;, &amp;lsquo;footers&amp;rsquo;, &amp;lsquo;quotes&amp;rsquo;). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata.</source>
          <target state="translated">Puede contener cualquier subconjunto de ('encabezados', 'pies de p&amp;aacute;gina', 'comillas'). Cada uno de estos son tipos de texto que se detectar&amp;aacute;n y eliminar&amp;aacute;n de las publicaciones del grupo de noticias, evitando que los clasificadores se ajusten demasiado a los metadatos.</target>
        </trans-unit>
        <trans-unit id="5b986034f147ad60f742da340ca1bd20b5f08ce7" translate="yes" xml:space="preserve">
          <source>McCullagh, Peter; Nelder, John (1989). Generalized Linear Models, Second Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72639b42074abb6c8ea20684337c75e653e13eb2" translate="yes" xml:space="preserve">
          <source>McSherry, F., &amp;amp; Najork, M. (2008, March). Computing information retrieval performance measures efficiently in the presence of tied scores. In European conference on information retrieval (pp. 414-421). Springer, Berlin, Heidelberg.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f0935dfe9ab3f30e90c245d2338ed727682177f" translate="yes" xml:space="preserve">
          <source>Mean Absolute Error:</source>
          <target state="translated">Error medio absoluto:</target>
        </trans-unit>
        <trans-unit id="90a417c7a65441a9ebd4508d554460a1437266a0" translate="yes" xml:space="preserve">
          <source>Mean Gamma deviance regression loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5e71e9559d855f0bb974ab566c48b140de9b95a" translate="yes" xml:space="preserve">
          <source>Mean Poisson deviance regression loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="007ffde203dd83c4c710b22da1cbe0b3700a98d1" translate="yes" xml:space="preserve">
          <source>Mean Silhouette Coefficient for all samples.</source>
          <target state="translated">Coeficiente de Silueta Media para todas las muestras.</target>
        </trans-unit>
        <trans-unit id="2762f10f75116f5e4a70c10eb14cf5f478eb498f" translate="yes" xml:space="preserve">
          <source>Mean Squared Error:</source>
          <target state="translated">Error de media cuadra:</target>
        </trans-unit>
        <trans-unit id="04bada6d1e51be3ec0e69f413da51c5f62d2f9fc" translate="yes" xml:space="preserve">
          <source>Mean Tweedie deviance regression loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43559adecf21dbddbbe17afba52b16b4a67e4402" translate="yes" xml:space="preserve">
          <source>Mean absolute error regression loss</source>
          <target state="translated">Promedio de la pérdida de regresión del error absoluto</target>
        </trans-unit>
        <trans-unit id="640f16564a014e166473dc43ba616e7e2ca59c7f" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) w.r.t. y.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec9517dd8574c2a6b45d6a307e2a503f5e6d275d" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) wrt. y.</source>
          <target state="translated">Exactitud media de la auto predicción (X)con respecto a la y.</target>
        </trans-unit>
        <trans-unit id="7493a61b1729d0e0247689ac97555930f03706df" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator</source>
          <target state="translated">Promedio de la puntuación cruzada del mejor_estimador</target>
        </trans-unit>
        <trans-unit id="140100875ffefa42eddb6a75afe4c55e05443030" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator.</source>
          <target state="translated">Promedio de la puntuación cruzada del mejor_estimador.</target>
        </trans-unit>
        <trans-unit id="905638cd4671af1b6b218d93d25fb805a7548993" translate="yes" xml:space="preserve">
          <source>Mean of feature importance over &lt;code&gt;n_repeats&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd2a669704ec2ab03a0e4ca8e2abc9c2d42f71c6" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution a query points</source>
          <target state="translated">Media de la distribución predictiva de los puntos de consulta</target>
        </trans-unit>
        <trans-unit id="b609d0f7d96a7b9c8e60baf85298ffc173cbc6f7" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution of query points.</source>
          <target state="translated">Media de la distribución predictiva de los puntos de consulta.</target>
        </trans-unit>
        <trans-unit id="e7f5a133eabd3f3a470b8b7cb54eeb045b64973a" translate="yes" xml:space="preserve">
          <source>Mean or median or quantile of the training targets or constant value given by the user.</source>
          <target state="translated">Media o mediana o cuantil de los objetivos de entrenamiento o valor constante dado por el usuario.</target>
        </trans-unit>
        <trans-unit id="9ede03e41402c8eec43dd01264f8f822af5fb92b" translate="yes" xml:space="preserve">
          <source>Mean shift clustering aims to discover &amp;ldquo;blobs&amp;rdquo; in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.</source>
          <target state="translated">La agrupaci&amp;oacute;n de cambios medios tiene como objetivo descubrir &quot;manchas&quot; en una densidad uniforme de muestras. Es un algoritmo basado en centroides, que funciona actualizando candidatos para centroides para que sean la media de los puntos dentro de una regi&amp;oacute;n determinada. Estos candidatos luego se filtran en una etapa de posprocesamiento para eliminar casi duplicados para formar el conjunto final de centroides.</target>
        </trans-unit>
        <trans-unit id="08b2e6d37eec1f5ceae1376ffba9071609b6547f" translate="yes" xml:space="preserve">
          <source>Mean shift clustering using a flat kernel.</source>
          <target state="translated">Agrupación de turnos media usando un núcleo plano.</target>
        </trans-unit>
        <trans-unit id="4484f1a9abfaeee06549ff0a6b75712b44fd35f2" translate="yes" xml:space="preserve">
          <source>Mean square error for the test set on each fold, varying l1_ratio and alpha.</source>
          <target state="translated">Error cuadrático medio del conjunto de pruebas en cada pliegue,variando l1_ratio y alfa.</target>
        </trans-unit>
        <trans-unit id="4a0031a2d59450a58aeaa638a064cb2e2c9a0da5" translate="yes" xml:space="preserve">
          <source>Mean squared error regression loss</source>
          <target state="translated">Pérdida de regresión del error cuadrado medio</target>
        </trans-unit>
        <trans-unit id="831bfb250ab69b773c25fae60f230a00bfdc7239" translate="yes" xml:space="preserve">
          <source>Mean squared logarithmic error regression loss</source>
          <target state="translated">Pérdida de regresión del error logarítmico al cuadrado medio</target>
        </trans-unit>
        <trans-unit id="2db7f6881ab1082c632822482db18fa9fc34ed90" translate="yes" xml:space="preserve">
          <source>Mean-shift</source>
          <target state="translated">Mean-shift</target>
        </trans-unit>
        <trans-unit id="896bb25ed00af769d9d9cdb21af7655bd0a22654" translate="yes" xml:space="preserve">
          <source>Measure and plot the results</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df21241945c8fd60618f888d81f315be3f7af674" translate="yes" xml:space="preserve">
          <source>Measure the similarity of two clusterings of a set of points.</source>
          <target state="translated">Mide la similitud de dos agrupaciones de un conjunto de puntos.</target>
        </trans-unit>
        <trans-unit id="e49da6a85d81735a7b28ef79fc256dec989d2443" translate="yes" xml:space="preserve">
          <source>Measurement errors in X</source>
          <target state="translated">Errores de medición en X</target>
        </trans-unit>
        <trans-unit id="471fba4dfe2d4f61d0ae5efc77acb722551abb7b" translate="yes" xml:space="preserve">
          <source>Measurement errors in y</source>
          <target state="translated">Errores de medición en y</target>
        </trans-unit>
        <trans-unit id="d59aa4a9911bb1573c0ba0a2779ea96a4989f27f" translate="yes" xml:space="preserve">
          <source>MedInc median income in block</source>
          <target state="translated">Ingreso medio en bloque</target>
        </trans-unit>
        <trans-unit id="ca6bd4b635d61f1c13fd0394e55000bb30738881" translate="yes" xml:space="preserve">
          <source>Median absolute error output is non-negative floating point. The best value is 0.0. Read more in the &lt;a href=&quot;../model_evaluation#median-absolute-error&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb82014fc42479d50d7886c5d05c20bd8db97f56" translate="yes" xml:space="preserve">
          <source>Median absolute error regression loss</source>
          <target state="translated">Pérdida de regresión del error absoluto medio</target>
        </trans-unit>
        <trans-unit id="9e7082d8eb8f2409deaa71605e5d6dbf5b190217" translate="yes" xml:space="preserve">
          <source>Medium &lt;code&gt;n_samples&lt;/code&gt;, small &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;n_samples&lt;/code&gt; medianos , &lt;code&gt;n_clusters&lt;/code&gt; peque&amp;ntilde;os</target>
        </trans-unit>
        <trans-unit id="da13fe6da16d1b0d3601ee1416010215c9da2b3d" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;coef_&lt;/code&gt; holds the weights \(w\)</source>
          <target state="translated">El miembro &lt;code&gt;coef_&lt;/code&gt; tiene los pesos \ (w \)</target>
        </trans-unit>
        <trans-unit id="b26ca7073281a8f4da3f64aafb4932ce0dc723cc" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds \(b\)</source>
          <target state="translated">La &lt;code&gt;intercept_&lt;/code&gt; tiene \ (b \)</target>
        </trans-unit>
        <trans-unit id="8e7e4ea63f467ef992e1b5515c3662fd092327fd" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds the intercept (aka offset or bias):</source>
          <target state="translated">Miembro &lt;code&gt;intercept_&lt;/code&gt; sostiene el intercepto (tambi&amp;eacute;n conocido como offset o sesgo):</target>
        </trans-unit>
        <trans-unit id="b6ef7f0fdf735583a61dbfc359227479e43bf842" translate="yes" xml:space="preserve">
          <source>Memmapping mode for numpy arrays passed to workers. See &amp;lsquo;max_nbytes&amp;rsquo; parameter documentation for more details.</source>
          <target state="translated">Modo Memmapping para matrices numpy pasadas a los trabajadores. Consulte la documentaci&amp;oacute;n del par&amp;aacute;metro 'max_nbytes' para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="424b610ebd2af23e4bb8a29dcabbc0551e9c3d87" translate="yes" xml:space="preserve">
          <source>Memory consumption for large sample sizes</source>
          <target state="translated">Consumo de memoria para muestras de gran tamaño</target>
        </trans-unit>
        <trans-unit id="5418f36b831a9c825f5d841c5ee3b1bdb2dd3e28" translate="yes" xml:space="preserve">
          <source>Meta-estimator to regress on a transformed target.</source>
          <target state="translated">Metaestimulador para retroceder en un objetivo transformado.</target>
        </trans-unit>
        <trans-unit id="79599678d3d5e2500fd2a7f727461a2dac0b6cd4" translate="yes" xml:space="preserve">
          <source>Meta-estimators for building composite models with transformers</source>
          <target state="translated">Metaestimuladores para construir modelos compuestos con transformadores</target>
        </trans-unit>
        <trans-unit id="d5a7b3579e10eeaa00ced1884380b174708caebd" translate="yes" xml:space="preserve">
          <source>Meta-transformer for selecting features based on importance weights.</source>
          <target state="translated">Meta-transformador para seleccionar características basadas en pesos de importancia.</target>
        </trans-unit>
        <trans-unit id="88306943fea7e76f9cd57cae0ea6d8b32d2e8434" translate="yes" xml:space="preserve">
          <source>Method</source>
          <target state="translated">Method</target>
        </trans-unit>
        <trans-unit id="bebb8fb7cc6768e9928f39fcd0184089c7f19d03" translate="yes" xml:space="preserve">
          <source>Method for initialization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dad9226be4bd937f8a455ee0badad4ee6cceff1" translate="yes" xml:space="preserve">
          <source>Method for initialization of k-means algorithm; defaults to &amp;lsquo;k-means++&amp;rsquo;.</source>
          <target state="translated">M&amp;eacute;todo de inicializaci&amp;oacute;n del algoritmo k-means; por defecto es 'k-means ++'.</target>
        </trans-unit>
        <trans-unit id="2c3f3dc3ba37d9b2f1af18176cea15628b89a09c" translate="yes" xml:space="preserve">
          <source>Method for initialization, default to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">M&amp;eacute;todo de inicializaci&amp;oacute;n, predeterminado en 'k-means ++':</target>
        </trans-unit>
        <trans-unit id="62b8a3dc56fc8229948d624cc5b38920d22e2365" translate="yes" xml:space="preserve">
          <source>Method for initialization, defaults to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">M&amp;eacute;todo de inicializaci&amp;oacute;n, predeterminado en 'k-means ++':</target>
        </trans-unit>
        <trans-unit id="0e3feb124243dbfe777425d1e7de652a8a95432b" translate="yes" xml:space="preserve">
          <source>Method for initialization:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b1389e0e832a05337d6ccb31e50ea1425ca91a8" translate="yes" xml:space="preserve">
          <source>Method name</source>
          <target state="translated">Nombre del método</target>
        </trans-unit>
        <trans-unit id="f2ddbb16b4269f001b2886168e6ee73674985a74" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b78ea13fd7ce3e01d82dac91ffffedca9d6a516f" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;. CAUTION: if &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt;, the data must not be sparse.</source>
          <target state="translated">M&amp;eacute;todo de normalizaci&amp;oacute;n y conversi&amp;oacute;n de vectores singulares en biclusters. Puede ser uno de 'escala', 'bistoc&amp;aacute;stico' o 'log'. Los autores recomiendan usar 'log'. Sin embargo, si los datos son escasos, la normalizaci&amp;oacute;n de registros no funcionar&amp;aacute;, por lo que el valor predeterminado es &quot;bistoc&amp;aacute;stico&quot;. PRECAUCI&amp;Oacute;N: si &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt; , los datos no deben ser escasos.</target>
        </trans-unit>
        <trans-unit id="7e7b59d1db0b41f1f7de6a768474fa98a959edfd" translate="yes" xml:space="preserve">
          <source>Method to use in finding shortest path.</source>
          <target state="translated">Método a utilizar para encontrar el camino más corto.</target>
        </trans-unit>
        <trans-unit id="46674c498c855af96974ed544b15ae6396d6f74f" translate="yes" xml:space="preserve">
          <source>Method used to encode the transformed result.</source>
          <target state="translated">Método utilizado para codificar el resultado transformado.</target>
        </trans-unit>
        <trans-unit id="155758829048f282b684f453070e5e32e8a3b098" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: &amp;lsquo;nndsvd&amp;rsquo; if n_components &amp;lt; n_features, otherwise random. Valid options:</source>
          <target state="translated">M&amp;eacute;todo utilizado para inicializar el procedimiento. Predeterminado: 'nndsvd' si n_components &amp;lt;n_features, de lo contrario aleatorio. Opciones v&amp;aacute;lidas:</target>
        </trans-unit>
        <trans-unit id="2babbe784e2e75bf6bc5c5da588a447b4ed201c9" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7506fd4c85f1f80b13ff77585a3eea447916c5af" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: None. Valid options:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e9cbcdd1d5058381751f008677bca0c5dd1dd9b" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;a href=&quot;#sklearn.decomposition.LatentDirichletAllocation.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c32a8fabfe7118c5a18a023b129a09f5774d869" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;code&gt;fit&lt;/code&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="translated">M&amp;eacute;todo utilizado para actualizar &lt;code&gt;_component&lt;/code&gt; . Solo se utiliza en el m&amp;eacute;todo de &lt;code&gt;fit&lt;/code&gt; . En general, si el tama&amp;ntilde;o de los datos es grande, la actualizaci&amp;oacute;n en l&amp;iacute;nea ser&amp;aacute; mucho m&amp;aacute;s r&amp;aacute;pida que la actualizaci&amp;oacute;n por lotes.</target>
        </trans-unit>
        <trans-unit id="7e4ac6803c9159c694f63d089cb06b2519c16aba" translate="yes" xml:space="preserve">
          <source>Methods</source>
          <target state="translated">Methods</target>
        </trans-unit>
        <trans-unit id="8de2b023f4bb12cf6f4720283ae55f1dda2214ee" translate="yes" xml:space="preserve">
          <source>Methods called for each base estimator. It can be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66a03315f429c2912a3083f4347509dcbe2b4de3" translate="yes" xml:space="preserve">
          <source>Metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a01ea489bdc9c9d6a182edc027242ff94374f848" translate="yes" xml:space="preserve">
          <source>Metric used to compute distances to neighbors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="223dc067df313c03c1e792b17b98d4c951add488" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;ldquo;precomputed&amp;rdquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted. If &amp;ldquo;precomputed&amp;rdquo;, a distance matrix (instead of a similarity matrix) is needed as input for the fit method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef01ecfb88c8d650a45a85cec9ebc18d89f4ecbc" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;lsquo;precomputed&amp;rsquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted.</source>
          <target state="translated">M&amp;eacute;trica utilizada para calcular el v&amp;iacute;nculo. Puede ser &quot;euclidiana&quot;, &quot;l1&quot;, &quot;l2&quot;, &quot;manhattan&quot;, &quot;coseno&quot; o &quot;precalculado&quot;. Si el enlace es &quot;pupilo&quot;, solo se acepta &quot;euclidiana&quot;.</target>
        </trans-unit>
        <trans-unit id="b996dbf9b464efe667f55d3c7b947b9e2ffb345f" translate="yes" xml:space="preserve">
          <source>Metrics available for various machine learning tasks are detailed in sections below.</source>
          <target state="translated">Las métricas disponibles para varias tareas de aprendizaje de la máquina se detallan en las secciones siguientes.</target>
        </trans-unit>
        <trans-unit id="6333551e93ef2383df0508df89bd1ca423b74bc9" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping, &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;, 2001.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="276b36ad13c4507935dcfa6095085df1bf048be3" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping: &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;</source>
          <target state="translated">Michael E. Tipping: &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;aprendizaje bayesiano disperso y la m&amp;aacute;quina de vectores de relevancia&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee22c86ee428b82d33b13bdebced2deed71d63a1" translate="yes" xml:space="preserve">
          <source>Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</source>
          <target state="translated">Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</target>
        </trans-unit>
        <trans-unit id="f33a348553a7d85d27424bb525b1eca4fb8a5155" translate="yes" xml:space="preserve">
          <source>MinMaxScaler</source>
          <target state="translated">MinMaxScaler</target>
        </trans-unit>
        <trans-unit id="6fad9f3e5fbaefddf87807ab8e89f2398827a6e0" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering</source>
          <target state="translated">Agrupación de Mini-Batch K-Means</target>
        </trans-unit>
        <trans-unit id="f81af70401b9fbaba1cc8f3d806f31408afbc851" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a7343b748199980306d06faf24494c5fb233c16" translate="yes" xml:space="preserve">
          <source>Mini-batch Sparse Principal Components Analysis</source>
          <target state="translated">Análisis de los componentes principales de los mini lotes dispersos</target>
        </trans-unit>
        <trans-unit id="4a04231399e807603297c55fa730ae6cac785e8b" translate="yes" xml:space="preserve">
          <source>Mini-batch dictionary learning</source>
          <target state="translated">Aprendizaje de diccionarios en miniatura</target>
        </trans-unit>
        <trans-unit id="b36d4bb746673223e9f3eddf90497433b367472a" translate="yes" xml:space="preserve">
          <source>Mini-batch sparse PCA (&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt;&lt;code&gt;MiniBatchSparsePCA&lt;/code&gt;&lt;/a&gt;) is a variant of &lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt;&lt;code&gt;SparsePCA&lt;/code&gt;&lt;/a&gt; that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.</source>
          <target state="translated">Mini-batch sparse PCA ( &lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt; &lt;code&gt;MiniBatchSparsePCA&lt;/code&gt; &lt;/a&gt; ) es una variante de &lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt; &lt;code&gt;SparsePCA&lt;/code&gt; &lt;/a&gt; que es m&amp;aacute;s r&amp;aacute;pida pero menos precisa. La velocidad aumentada se alcanza iterando sobre peque&amp;ntilde;os fragmentos del conjunto de caracter&amp;iacute;sticas, para un n&amp;uacute;mero determinado de iteraciones.</target>
        </trans-unit>
        <trans-unit id="acc629f9bc13af6fe4ccc30d47949b9a29f4708a" translate="yes" xml:space="preserve">
          <source>Minimal cost complexity pruning recursively finds the node with the &amp;ldquo;weakest link&amp;rdquo;. The weakest link is characterized by an effective alpha, where the nodes with the smallest effective alpha are pruned first. To get an idea of what values of &lt;code&gt;ccp_alpha&lt;/code&gt; could be appropriate, scikit-learn provides &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path&quot;&gt;&lt;code&gt;DecisionTreeClassifier.cost_complexity_pruning_path&lt;/code&gt;&lt;/a&gt; that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f8dae4007afe0299bfaa99f8bc74594de701fdf" translate="yes" xml:space="preserve">
          <source>Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of &lt;a href=&quot;#bre&quot; id=&quot;id2&quot;&gt;[BRE]&lt;/a&gt;. This algorithm is parameterized by \(\alpha\ge0\) known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, \(R_\alpha(T)\) of a given tree \(T\):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="338b69eb058f4b8de205ae0e6a0b364261aebe7e" translate="yes" xml:space="preserve">
          <source>Minimizes the objective function:</source>
          <target state="translated">Minimiza la función objetiva:</target>
        </trans-unit>
        <trans-unit id="84c971787220fb3e13d325cba22644ed7cfc6396" translate="yes" xml:space="preserve">
          <source>Minimizing Finite Sums with the Stochastic Average Gradient &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</source>
          <target state="translated">Minimizaci&amp;oacute;n de sumas finitas con el gradiente medio estoc&amp;aacute;stico &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="40d428add0b0bb2b15690324c7a65b1e95d21444" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant (MCD): robust estimator of covariance.</source>
          <target state="translated">Determinante de la covarianza mínima (MCD):estimador robusto de la covarianza.</target>
        </trans-unit>
        <trans-unit id="f0d923ebaec99475dba3ff68622a8b582426df2b" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant Estimator</source>
          <target state="translated">Estimador del determinante de la covarianza mínima</target>
        </trans-unit>
        <trans-unit id="acdf76216ef7494ca3a405d1a4760970f1dcb045" translate="yes" xml:space="preserve">
          <source>Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.</source>
          <target state="translated">Correlación mínima a lo largo del camino.Corresponde al parámetro de regularización alfa del Lasso.</target>
        </trans-unit>
        <trans-unit id="45261c0e2275ffe7c782578b7e04c16d232cec64" translate="yes" xml:space="preserve">
          <source>Minimum number of candidates evaluated per estimator, assuming enough items meet the &lt;code&gt;min_hash_match&lt;/code&gt; constraint.</source>
          <target state="translated">N&amp;uacute;mero m&amp;iacute;nimo de candidatos evaluados por estimador, asumiendo que suficientes elementos cumplen la restricci&amp;oacute;n &lt;code&gt;min_hash_match&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1f6032c543b0bedd4ef1a29303943c7332e81e08" translate="yes" xml:space="preserve">
          <source>Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt;, treated as a relative number &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt;) for &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt;. This is typically chosen as the minimal number of samples necessary to estimate the given &lt;code&gt;base_estimator&lt;/code&gt;. By default a &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; estimator is assumed and &lt;code&gt;min_samples&lt;/code&gt; is chosen as &lt;code&gt;X.shape[1] + 1&lt;/code&gt;.</source>
          <target state="translated">N&amp;uacute;mero m&amp;iacute;nimo de muestras elegidas al azar de los datos originales. Se trata como un n&amp;uacute;mero absoluto de muestras para &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt; , se trata como un n&amp;uacute;mero relativo &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt; ) para &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt; . Esto normalmente se elige como el n&amp;uacute;mero m&amp;iacute;nimo de muestras necesarias para estimar el dado &lt;code&gt;base_estimator&lt;/code&gt; . Por defecto, un &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; se supone estimador y &lt;code&gt;min_samples&lt;/code&gt; se elige como &lt;code&gt;X.shape[1] + 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7055b8e6fba3c22d096f09a773f8fa2a0f2c2a45" translate="yes" xml:space="preserve">
          <source>Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2). If &lt;code&gt;None&lt;/code&gt;, the value of &lt;code&gt;min_samples&lt;/code&gt; is used instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="143330e48296c9730a85d5042f3f4108bc450e1a" translate="yes" xml:space="preserve">
          <source>Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2). If &lt;code&gt;None&lt;/code&gt;, the value of &lt;code&gt;min_samples&lt;/code&gt; is used instead. Used only when &lt;code&gt;cluster_method='xi'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c28e0e254bdc137eed30061d47cd3e6720cfe75" translate="yes" xml:space="preserve">
          <source>Minimum possible imputed value. Broadcast to shape (n_features,) if scalar. If array-like, expects shape (n_features,), one min value for each feature. &lt;code&gt;None&lt;/code&gt; (default) is converted to -np.inf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a6bb55043794a0a93e8ff0524f08e79cbc35225" translate="yes" xml:space="preserve">
          <source>Minimum value of a bicluster.</source>
          <target state="translated">Valor mínimo de un bíceps.</target>
        </trans-unit>
        <trans-unit id="59e81ca4cbc76e95e029c93b9fa76bb8c2828a22" translate="yes" xml:space="preserve">
          <source>Minimum value of input array &lt;code&gt;X_&lt;/code&gt; for left bound.</source>
          <target state="translated">Valor m&amp;iacute;nimo de la matriz de entrada &lt;code&gt;X_&lt;/code&gt; para el l&amp;iacute;mite izquierdo.</target>
        </trans-unit>
        <trans-unit id="2b5d457149fe5be167ed99387c99dd4725835fe8" translate="yes" xml:space="preserve">
          <source>MinkowskiDistance</source>
          <target state="translated">MinkowskiDistance</target>
        </trans-unit>
        <trans-unit id="8984ca78ae6f645a8da0469517e3e68a7c22986d" translate="yes" xml:space="preserve">
          <source>Mirroring the example above in grid search, we can specify a continuous random variable that is log-uniformly distributed between &lt;code&gt;1e0&lt;/code&gt; and &lt;code&gt;1e3&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3547bc4550b1de5a83615a3b3bc38cc23770ce0" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;class_log_prior_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">Espejos &lt;code&gt;class_log_prior_&lt;/code&gt; para interpretar MultinomialNB como un modelo lineal.</target>
        </trans-unit>
        <trans-unit id="7814bd45bfd54f380e5f0cd3f0461e627752ef7b" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;feature_log_prob_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">Refleja &lt;code&gt;feature_log_prob_&lt;/code&gt; para interpretar MultinomialNB como un modelo lineal.</target>
        </trans-unit>
        <trans-unit id="5f2cbd107037ed23248e5058a7a64cd6bae05468" translate="yes" xml:space="preserve">
          <source>Miscellaneous</source>
          <target state="translated">Miscellaneous</target>
        </trans-unit>
        <trans-unit id="0d2ecb69e7b12979a3e40a5ab8b5183911f1a3c3" translate="yes" xml:space="preserve">
          <source>Miscellaneous and introductory examples for scikit-learn.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26655e342820eb1000893c259228858eef67a34d" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e446df504bb1a7ba9afc2f86aa7e483abdbc1937" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values:</source>
          <target state="translated">Valores de atributo faltantes:</target>
        </trans-unit>
        <trans-unit id="905705cdb93f7b29194485a892d91da6c1cdc874" translate="yes" xml:space="preserve">
          <source>Missing Value Imputation</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67cc34b1cd58b9ef03f7ff376e8541732aac180b" translate="yes" xml:space="preserve">
          <source>Missing information</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e00e76132a4d6b917901e5526c250a336a29108" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="391b82fea55e1ed8699fe6e91400e8d6055ab0df" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;. The median is a more robust estimator for data with high magnitude variables which could dominate results (otherwise known as a &amp;lsquo;long tail&amp;rsquo;).</source>
          <target state="translated">Los valores que faltan se pueden reemplazar por la media, la mediana o el valor m&amp;aacute;s frecuente usando el &lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; &lt;/a&gt; b&amp;aacute;sico . La mediana es un estimador m&amp;aacute;s robusto para datos con variables de alta magnitud que podr&amp;iacute;an dominar los resultados (tambi&amp;eacute;n conocido como 'cola larga').</target>
        </trans-unit>
        <trans-unit id="7657a2d6545adb4955b10f53c4131bc5602e90eb" translate="yes" xml:space="preserve">
          <source>Missing values in the &amp;lsquo;data&amp;rsquo; are represented as NaN&amp;rsquo;s. Missing values in &amp;lsquo;target&amp;rsquo; are represented as NaN&amp;rsquo;s (numerical target) or None (categorical target)</source>
          <target state="translated">Los valores que faltan en los 'datos' se representan como NaN. Los valores que faltan en 'objetivo' se representan como NaN (objetivo num&amp;eacute;rico) o Ninguno (objetivo categ&amp;oacute;rico)</target>
        </trans-unit>
        <trans-unit id="6a6932c856f91eed3dd44780fa6b9fe69490c4b8" translate="yes" xml:space="preserve">
          <source>Mixin class for all bicluster estimators in scikit-learn</source>
          <target state="translated">Clase mixta para todos los estimadores de bíceps en scikit-learn</target>
        </trans-unit>
        <trans-unit id="2c10e3ce37d297d342507e753914a98859330d14" translate="yes" xml:space="preserve">
          <source>Mixin class for all classifiers in scikit-learn.</source>
          <target state="translated">Clase mixta para todos los clasificadores en scikit-learn.</target>
        </trans-unit>
        <trans-unit id="5fa39e3354bc95759f1ac752182b15d93d7771cd" translate="yes" xml:space="preserve">
          <source>Mixin class for all cluster estimators in scikit-learn.</source>
          <target state="translated">Clase mixta para todos los estimadores de cúmulos en scikit-learn.</target>
        </trans-unit>
        <trans-unit id="eb8addc65b16d7fa21479da43bfb0ac745b8fd54" translate="yes" xml:space="preserve">
          <source>Mixin class for all density estimators in scikit-learn.</source>
          <target state="translated">Clase de mixina para todos los estimadores de densidad en scikit-learn.</target>
        </trans-unit>
        <trans-unit id="6ac045bb154d5d0dbd1cc9d29eba911e1ea2781a" translate="yes" xml:space="preserve">
          <source>Mixin class for all regression estimators in scikit-learn.</source>
          <target state="translated">Clase mixta para todos los estimadores de regresión en scikit-learn.</target>
        </trans-unit>
        <trans-unit id="f73bd7177b7212616f9ae4cd764e784bac6a7ce1" translate="yes" xml:space="preserve">
          <source>Mixin class for all transformers in scikit-learn.</source>
          <target state="translated">Clase de mezcla para todos los transformadores en scikit-learn.</target>
        </trans-unit>
        <trans-unit id="4d9a44acff48ccb4a2b026d4835ebe86a95495fc" translate="yes" xml:space="preserve">
          <source>Model Complexity Influence</source>
          <target state="translated">Influencia de la complejidad del modelo</target>
        </trans-unit>
        <trans-unit id="9c567347b8af7d91b331f07af5f77ec0a361f505" translate="yes" xml:space="preserve">
          <source>Model Selection</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d5e06ce8e5a0fb1e8e99aee47dbf3426de865fe" translate="yes" xml:space="preserve">
          <source>Model Selection Interface</source>
          <target state="translated">Interfaz de selección de modelos</target>
        </trans-unit>
        <trans-unit id="d9b7f2bb0f8fc0d29940e1aefd1565fcc5b449f7" translate="yes" xml:space="preserve">
          <source>Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.</source>
          <target state="translated">Mezcla de modelos:Cuando las predicciones de un estimador supervisado se utilizan para entrenar a otro estimador en métodos de conjunto.</target>
        </trans-unit>
        <trans-unit id="c3b027b1bc55171725d0853107d2cd63b70cf1b0" translate="yes" xml:space="preserve">
          <source>Model complexity</source>
          <target state="translated">Complejidad del modelo</target>
        </trans-unit>
        <trans-unit id="088cfdc97cd06f5c2647d7bc4d07170997a1804d" translate="yes" xml:space="preserve">
          <source>Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.</source>
          <target state="translated">La compresión de modelos en scikit-learn sólo concierne a los modelos lineales por el momento.En este contexto significa que queremos controlar la dispersión del modelo (es decir,el número de coordenadas no nulas en los vectores del modelo).En general,es una buena idea combinar la dispersión del modelo con la representación de los datos de entrada escasos.</target>
        </trans-unit>
        <trans-unit id="12cb4d758358636a28aab0195639a05c4c6adf09" translate="yes" xml:space="preserve">
          <source>Model persistence</source>
          <target state="translated">Model persistence</target>
        </trans-unit>
        <trans-unit id="c38101ec23202ddb2bcf9ed4925ad6d1c9181511" translate="yes" xml:space="preserve">
          <source>Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in &lt;code&gt;CSR&lt;/code&gt; format), it is generally sufficient to not generate the relevant features, leaving their columns empty.</source>
          <target state="translated">La remodelaci&amp;oacute;n del modelo consiste en seleccionar solo una parte de las funciones disponibles para adaptarse a un modelo. En otras palabras, si un modelo descarta caracter&amp;iacute;sticas durante la fase de aprendizaje, podemos eliminarlas de la entrada. Esto tiene varios beneficios. En primer lugar, reduce la sobrecarga de memoria (y, por lo tanto, de tiempo) del modelo en s&amp;iacute;. Tambi&amp;eacute;n permite descartar componentes de selecci&amp;oacute;n de caracter&amp;iacute;sticas expl&amp;iacute;citas en una canalizaci&amp;oacute;n una vez que sabemos qu&amp;eacute; caracter&amp;iacute;sticas conservar de una ejecuci&amp;oacute;n anterior. Finalmente, puede ayudar a reducir el tiempo de procesamiento y el uso de E / S en sentido ascendente en las capas de extracci&amp;oacute;n de caracter&amp;iacute;sticas y acceso a datos al no recopilar y generar caracter&amp;iacute;sticas que el modelo descarta. Por ejemplo, si los datos sin procesar provienen de una base de datos, puede hacer posible escribir consultas m&amp;aacute;s simples y r&amp;aacute;pidas o reducir el uso de E / S haciendo que las consultas devuelvan registros m&amp;aacute;s ligeros. En el momento,la remodelaci&amp;oacute;n debe realizarse manualmente en scikit-learn. En el caso de entrada escasa (particularmente en &lt;code&gt;CSR&lt;/code&gt; Formato CSR ), generalmente es suficiente no generar las caracter&amp;iacute;sticas relevantes, dejando sus columnas vac&amp;iacute;as.</target>
        </trans-unit>
        <trans-unit id="28eeecfcba5c4e3a6c993b8bf2c6736730dfbd15" translate="yes" xml:space="preserve">
          <source>Model selection</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12aba00cd9b6d07b68e1c4795de07ac9d7b738d7" translate="yes" xml:space="preserve">
          <source>Model selection and evaluation using tools, such as &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;model_selection.cross_val_score&lt;/code&gt;&lt;/a&gt;, take a &lt;code&gt;scoring&lt;/code&gt; parameter that controls what metric they apply to the estimators evaluated.</source>
          <target state="translated">La selecci&amp;oacute;n y evaluaci&amp;oacute;n de modelos mediante herramientas, como &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;model_selection.cross_val_score&lt;/code&gt; &lt;/a&gt; , toman un par&amp;aacute;metro de &lt;code&gt;scoring&lt;/code&gt; que controla qu&amp;eacute; m&amp;eacute;trica aplican a los estimadores evaluados.</target>
        </trans-unit>
        <trans-unit id="855cd5c7a76f661266d80a6648a2f964caf6a19e" translate="yes" xml:space="preserve">
          <source>Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to &amp;ldquo;train&amp;rdquo; the parameters of the grid.</source>
          <target state="translated">La selecci&amp;oacute;n del modelo mediante la evaluaci&amp;oacute;n de varios ajustes de par&amp;aacute;metros puede verse como una forma de utilizar los datos etiquetados para &quot;entrenar&quot; los par&amp;aacute;metros de la cuadr&amp;iacute;cula.</target>
        </trans-unit>
        <trans-unit id="89917070f2baaaaf3d7a4bc37b23fe9e19c05135" translate="yes" xml:space="preserve">
          <source>Model selection with Probabilistic PCA and Factor Analysis (FA)</source>
          <target state="translated">Selección de modelos con PCA Probabilístico y Análisis Factorial (FA)</target>
        </trans-unit>
        <trans-unit id="96389a0f02a3826ae6017961d0301435c315a116" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; for an analysis of these issues.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c731d5ab6adf4a98df3f1383f23cf8f7eed3338" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; for an analysis of these issues.</source>
          <target state="translated">La selecci&amp;oacute;n del modelo sin CV anidado utiliza los mismos datos para ajustar los par&amp;aacute;metros del modelo y evaluar el rendimiento del modelo. Por tanto, la informaci&amp;oacute;n puede &quot;filtrarse&quot; en el modelo y ajustarse a los datos. La magnitud de este efecto depende principalmente del tama&amp;ntilde;o del conjunto de datos y la estabilidad del modelo. Ver Cawley y Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; para un an&amp;aacute;lisis de estos temas.</target>
        </trans-unit>
        <trans-unit id="74392d3518eac75d4c193fc75b5f4945cf9be3f9" translate="yes" xml:space="preserve">
          <source>Model selection: choosing estimators and their parameters</source>
          <target state="translated">Selección del modelo:elección de los estimadores y sus parámetros</target>
        </trans-unit>
        <trans-unit id="ed1ba8eabae7e8d7de3d25705f2020f1428a1a11" translate="yes" xml:space="preserve">
          <source>Model the number of claims with a Poisson distribution, and the average claim amount per claim, also known as severity, as a Gamma distribution and multiply the predictions of both in order to get the total claim amount.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c9e82e3e8aa374bb01cb1f0583b83a3f30a27a0" translate="yes" xml:space="preserve">
          <source>Model the total claim amount per exposure directly, typically with a Tweedie distribution of Tweedie power \(p \in (1, 2)\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad79a801df8015a66d5501cf36f7ffcd2a41ddf8" translate="yes" xml:space="preserve">
          <source>Model validation</source>
          <target state="translated">Model validation</target>
        </trans-unit>
        <trans-unit id="44e8839819f969bae0c352bffb18b8d9aae37a64" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;https://matplotlib.org/basemap/&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04c7998384d3cc95ade6e27711f83c95af26806e" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;code&gt;OneClassSVM&lt;/code&gt; provided by the package &lt;code&gt;sklearn.svm&lt;/code&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">Modelar las distribuciones geogr&amp;aacute;ficas de las especies es un problema importante en la biolog&amp;iacute;a de la conservaci&amp;oacute;n. En este ejemplo modelamos la distribuci&amp;oacute;n geogr&amp;aacute;fica de dos mam&amp;iacute;feros sudamericanos dadas las observaciones anteriores y 14 variables ambientales. Dado que solo tenemos ejemplos positivos (no hay observaciones fallidas), planteamos este problema como un problema de estimaci&amp;oacute;n de densidad y usamos &lt;code&gt;OneClassSVM&lt;/code&gt; proporcionado por el paquete &lt;code&gt;sklearn.svm&lt;/code&gt; como nuestra herramienta de modelado. El conjunto de datos es proporcionado por Phillips et. Alabama. (2006). Si est&amp;aacute; disponible, el ejemplo utiliza un &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;mapa base&lt;/a&gt; para trazar las l&amp;iacute;neas costeras y los l&amp;iacute;mites nacionales de Am&amp;eacute;rica del Sur.</target>
        </trans-unit>
        <trans-unit id="3432d8d9b44052d02847746ae08d1ac381072a89" translate="yes" xml:space="preserve">
          <source>Modified Huber: \(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))^2\) if \(y_i f(x_i) &amp;gt; 1\), and \(L(y_i, f(x_i)) = -4 y_i f(x_i)\) otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41be465b762359b2fa053c297894959404d2b4b5" translate="yes" xml:space="preserve">
          <source>Module &lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt;&lt;code&gt;sklearn.kernel_ridge&lt;/code&gt;&lt;/a&gt; implements kernel ridge regression.</source>
          <target state="translated">El m&amp;oacute;dulo &lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt; &lt;code&gt;sklearn.kernel_ridge&lt;/code&gt; &lt;/a&gt; implementa la regresi&amp;oacute;n de la cresta del kernel.</target>
        </trans-unit>
        <trans-unit id="7c19bb73223842069c348f5ce2be56f6bdc47336" translate="yes" xml:space="preserve">
          <source>Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">Momento para la actualizaci&amp;oacute;n del descenso del gradiente. Debe estar entre 0 y 1. Solo se usa cuando solver = 'sgd'.</target>
        </trans-unit>
        <trans-unit id="08837633f9d15f78a0ca0401b5e29aae44a3cb25" translate="yes" xml:space="preserve">
          <source>Monotonic Constraints</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afdb29f8a2a5c8088f948d58228ab92ec4b8dbc8" translate="yes" xml:space="preserve">
          <source>Moosmann, F. and Triggs, B. and Jurie, F. &amp;ldquo;Fast discriminative visual codebooks using randomized clustering forests&amp;rdquo; NIPS 2007</source>
          <target state="translated">Moosmann, F. y Triggs, B. y Jurie, F. &amp;ldquo;Libros de c&amp;oacute;digos visuales discriminativos r&amp;aacute;pidos que utilizan bosques agrupados aleatoriamente&amp;rdquo; NIPS 2007</target>
        </trans-unit>
        <trans-unit id="3f683b2b5fe59dc7e9963e0b844a2be959abe1bd" translate="yes" xml:space="preserve">
          <source>More details about the losses formulas can be found in the &lt;a href=&quot;../sgd#sgd-mathematical-formulation&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea951c164724999b1e82491617fa7550c41c4ea4" translate="yes" xml:space="preserve">
          <source>More details can be found in the article &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Bayesian Interpolation&lt;/a&gt; by MacKay, David J. C.</source>
          <target state="translated">Se pueden encontrar m&amp;aacute;s detalles en el art&amp;iacute;culo &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Interpolaci&amp;oacute;n bayesiana&lt;/a&gt; de MacKay, David JC</target>
        </trans-unit>
        <trans-unit id="3dd8319d03052df7074a6e0643293f5dc781d510" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;http://scikit-learn.org/stable/modules/sgd.html&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5be68ba5f49b8c23c2004cef7b8f1738816fd7ff" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="translated">Se pueden encontrar m&amp;aacute;s detalles en la documentaci&amp;oacute;n de &lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eeddded239db4ba79d40ed239189aa60eea25acb" translate="yes" xml:space="preserve">
          <source>More details on tools available for model selection can be found in the sections on &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;Cross-validation: evaluating estimator performance&lt;/a&gt; and &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">Se pueden encontrar m&amp;aacute;s detalles sobre las herramientas disponibles para la selecci&amp;oacute;n de modelos en las secciones sobre &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;Validaci&amp;oacute;n cruzada: evaluaci&amp;oacute;n del rendimiento del estimador&lt;/a&gt; y &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Ajuste de los hiperpar&amp;aacute;metros de un estimador&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="d431b615f9733586de025b4f9872bf1a8badc9bd" translate="yes" xml:space="preserve">
          <source>More formally, the responsibility of a sample \(k\) to be the exemplar of sample \(i\) is given by:</source>
          <target state="translated">Más formalmente,la responsabilidad de que una muestra sea el ejemplo de la muestra está dada por:</target>
        </trans-unit>
        <trans-unit id="e5fcb8eb05185b2ae6a7561ecfd54e7e78d1825d" translate="yes" xml:space="preserve">
          <source>More formally, we define a core sample as being a sample in the dataset such that there exist &lt;code&gt;min_samples&lt;/code&gt; other samples within a distance of &lt;code&gt;eps&lt;/code&gt;, which are defined as &lt;em&gt;neighbors&lt;/em&gt; of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of &lt;em&gt;their&lt;/em&gt; neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.</source>
          <target state="translated">M&amp;aacute;s formalmente, definimos una muestra central como una muestra en el conjunto de datos, de modo que existen &lt;code&gt;min_samples&lt;/code&gt; otras muestras dentro de una distancia de &lt;code&gt;eps&lt;/code&gt; , que se definen como &lt;em&gt;vecinas&lt;/em&gt; de la muestra central. Esto nos dice que la muestra central est&amp;aacute; en un &amp;aacute;rea densa del espacio vectorial. Un cl&amp;uacute;ster es un conjunto de muestras centrales que se puede construir tomando de forma recursiva una muestra central, encontrando todos sus vecinos que son muestras centrales, encontrando todos &lt;em&gt;sus&lt;/em&gt; vecinos que son muestras centrales, y as&amp;iacute; sucesivamente. Un conglomerado tambi&amp;eacute;n tiene un conjunto de muestras no centrales, que son muestras vecinas de una muestra central del conglomerado, pero que no son en s&amp;iacute; mismas muestras centrales. Intuitivamente, estas muestras est&amp;aacute;n al margen de un grupo.</target>
        </trans-unit>
        <trans-unit id="39d3fe53d51c5218d78f036015830e2502c27f2b" translate="yes" xml:space="preserve">
          <source>More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc&amp;hellip;</source>
          <target state="translated">De manera m&amp;aacute;s general, cuando la precisi&amp;oacute;n de un clasificador es demasiado cercana a la aleatoria, probablemente significa que algo sali&amp;oacute; mal: las caracter&amp;iacute;sticas no son &amp;uacute;tiles, un hiperpar&amp;aacute;metro no est&amp;aacute; ajustado correctamente, el clasificador sufre un desequilibrio de clases, etc.</target>
        </trans-unit>
        <trans-unit id="63b4a4241c78c35e803f9a1e6a808d1813f2fb30" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="translated">Puede encontrar m&amp;aacute;s informaci&amp;oacute;n en la &lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;p&amp;aacute;gina de instalaci&amp;oacute;n de Scipy&lt;/a&gt; y en esta &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;publicaci&amp;oacute;n de blog&lt;/a&gt; de Daniel Nouri, que tiene algunas instrucciones de instalaci&amp;oacute;n paso a paso para Debian / Ubuntu.</target>
        </trans-unit>
        <trans-unit id="c3cc4720813506dbf91cef9b68d3a09728559160" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;https://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d4e98f0a8d8595ea60691c38a1427dece6499fb" translate="yes" xml:space="preserve">
          <source>More metadata from OpenML</source>
          <target state="translated">Más metadatos de OpenML</target>
        </trans-unit>
        <trans-unit id="12db8232292ca8d1cf35bc6b9168f2c8b63d47ca" translate="yes" xml:space="preserve">
          <source>More precisely its the expectation of the target response after accounting for the initial model; partial dependence plots do not include the &lt;code&gt;init&lt;/code&gt; model.</source>
          <target state="translated">M&amp;aacute;s precisamente, es la expectativa de la respuesta objetivo despu&amp;eacute;s de considerar el modelo inicial; las gr&amp;aacute;ficas de dependencia parcial no incluyen el modelo &lt;code&gt;init&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="73794f226fb348eb5da6ad63afeb16cb41727d95" translate="yes" xml:space="preserve">
          <source>More readable code, in particular since it avoids constructing list of arguments.</source>
          <target state="translated">Un código más legible,en particular porque evita la construcción de una lista de argumentos.</target>
        </trans-unit>
        <trans-unit id="a22dda2285328f04695cb396d42b64909dfc0d90" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(X|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="translated">Más específicamente,para el análisis discriminante lineal y cuadrático,\ ~ P(X|y)\ ~ se modela como una distribución gaussiana multivariante con densidad:</target>
        </trans-unit>
        <trans-unit id="2b3cb69cb34819cd8834522c11758ddc50e1f474" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(x|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79ecb6d9275fdfeccdbd69d6aa3919b92952032e" translate="yes" xml:space="preserve">
          <source>Most commonly, disparities are set to \(\hat{d}_{ij} = b S_{ij}\).</source>
          <target state="translated">Lo más común es que las disparidades se fijen en \ ~-(\ ~-que {d}_{ij}=b S_{ij}).</target>
        </trans-unit>
        <trans-unit id="242544fc56d0b5c7cafd51d576a4b175219e1770" translate="yes" xml:space="preserve">
          <source>Most estimators based on nearest neighbors graphs now accept precomputed sparse graphs as input, to reuse the same graph for multiple estimator fits. To use this feature in a pipeline, one can use the &lt;code&gt;memory&lt;/code&gt; parameter, along with one of the two new transformers, &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt;&lt;code&gt;neighbors.KNeighborsTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt;&lt;code&gt;neighbors.RadiusNeighborsTransformer&lt;/code&gt;&lt;/a&gt;. The precomputation can also be performed by custom estimators to use alternative implementations, such as approximate nearest neighbors methods. See more details in the &lt;a href=&quot;../../modules/neighbors#neighbors-transformer&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44814fa11b1099a09b484290756712e9a8679a34" translate="yes" xml:space="preserve">
          <source>Most of the parameters are unchanged from &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;. One exception is the &lt;code&gt;max_iter&lt;/code&gt; parameter that replaces &lt;code&gt;n_estimators&lt;/code&gt;, and controls the number of iterations of the boosting process:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13411f05832555677b503d8db5e64b4930c99086" translate="yes" xml:space="preserve">
          <source>Most of the variance can be explained by a bell-shaped curve of width effective_rank: the low rank part of the singular values profile is:</source>
          <target state="translated">La mayor parte de la varianza puede explicarse por una curva en forma de campana de ancho effective_rank:la parte de bajo rango del perfil de valores singulares es:</target>
        </trans-unit>
        <trans-unit id="9a311c70d6fa85e99fb6533c84253a4d2c760cf7" translate="yes" xml:space="preserve">
          <source>Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model.</source>
          <target state="translated">La mayoría de los modelos de aprendizaje de ciencias suelen ser bastante rápidos ya que se implementan con extensiones de Cython compiladas o librerías de computación optimizadas.Por otro lado,en muchas aplicaciones del mundo real el proceso de extracción de características (es decir,convertir datos en bruto como filas de bases de datos o paquetes de red en matrices numéricas)gobierna el tiempo de predicción global.Por ejemplo,en la tarea de clasificación de textos de Reuters,toda la preparación (lectura y análisis sintáctico de los archivos SGML,conversión del texto en fichas y su introducción en un espacio vectorial común)lleva entre 100 y 500 veces más tiempo que el código de predicción real,según el modelo elegido.</target>
        </trans-unit>
        <trans-unit id="1f57c7d2294fbf421c865e0ff805433c9e9164a6" translate="yes" xml:space="preserve">
          <source>Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix \(X\) so that it has shape &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt;. We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same.</source>
          <target state="translated">La mayor&amp;iacute;a de los tratamientos de LSA en la literatura sobre procesamiento del lenguaje natural (NLP) y recuperaci&amp;oacute;n de informaci&amp;oacute;n (IR) intercambian los ejes de la matriz \ (X \) para que tenga forma &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt; . Presentamos LSA de una manera diferente que coincide mejor con la API de scikit-learn, pero los valores singulares encontrados son los mismos.</target>
        </trans-unit>
        <trans-unit id="56ac69cc3d5e8e713d723baf0656a6eefef8f81b" translate="yes" xml:space="preserve">
          <source>Multi target classification</source>
          <target state="translated">Clasificación de objetivos múltiples</target>
        </trans-unit>
        <trans-unit id="b9b406b23aa7207ecf1f2aef41fc5c5ad0ba0c31" translate="yes" xml:space="preserve">
          <source>Multi target regression</source>
          <target state="translated">Regresión de objetivos múltiples</target>
        </trans-unit>
        <trans-unit id="332c064d1606c8de1a2522f9e4ee668dee56478e" translate="yes" xml:space="preserve">
          <source>Multi-class AdaBoosted Decision Trees</source>
          <target state="translated">Árboles de decisión multiclase AdaBoosted</target>
        </trans-unit>
        <trans-unit id="d384b7095ac166d1b587c1dafb0cadad28beb4c2" translate="yes" xml:space="preserve">
          <source>Multi-class targets.</source>
          <target state="translated">Objetivos multiclase.</target>
        </trans-unit>
        <trans-unit id="3243798e9c1a783043187bb0ea60ba4b8d0dfc62" translate="yes" xml:space="preserve">
          <source>Multi-class targets. An indicator matrix turns on multilabel classification.</source>
          <target state="translated">Objetivos multiclase.Una matriz de indicadores enciende la clasificación de multiples clases.</target>
        </trans-unit>
        <trans-unit id="552ba9a8fb8ef0b9cf8d9ea68e7f0c182ae9af5e" translate="yes" xml:space="preserve">
          <source>Multi-dimensional scaling</source>
          <target state="translated">Escalado multidimensional</target>
        </trans-unit>
        <trans-unit id="9815dac6e8971893d838904dc7e1cfe16372af94" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron classifier.</source>
          <target state="translated">Clasificador de Perceptrón de varias capas.</target>
        </trans-unit>
        <trans-unit id="b994a134c1a31489af71fc772bdcadb38a217ddf" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. Note that you must apply the &lt;em&gt;same&lt;/em&gt; scaling to the test set for meaningful results. You can use &lt;code&gt;StandardScaler&lt;/code&gt; for standardization.</source>
          <target state="translated">Perceptron multicapa es sensible al escalado de caracter&amp;iacute;sticas, por lo que se recomienda encarecidamente escalar sus datos. Por ejemplo, escale cada atributo en el vector de entrada X a [0, 1] o [-1, +1], o estandar&amp;iacute;celo para tener una media 0 y una varianza 1. Tenga en cuenta que debe aplicar la &lt;em&gt;misma&lt;/em&gt; escala al conjunto de prueba para resultados significativos. Puede utilizar &lt;code&gt;StandardScaler&lt;/code&gt; para la estandarizaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="8b22895cdf3840f5acfe1ac32cbc8961e8fd336a" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron regressor.</source>
          <target state="translated">Regresor Perceptrón de varias capas.</target>
        </trans-unit>
        <trans-unit id="dc72474a07afc8bb8057ac9bf6d8fbc65e56a63e" translate="yes" xml:space="preserve">
          <source>Multi-output Decision Tree Regression</source>
          <target state="translated">Regresión del árbol de decisión de múltiples salidas</target>
        </trans-unit>
        <trans-unit id="2627f8f7a5d9294ea8dcfe47a04508977edd8f7c" translate="yes" xml:space="preserve">
          <source>Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor.</source>
          <target state="translated">Objetivos de salida múltiples predichos a través de múltiples predictores.Nota:Se generan modelos separados para cada predictor.</target>
        </trans-unit>
        <trans-unit id="4da1e42d60732d934b60458ac859ed1253e6bfbd" translate="yes" xml:space="preserve">
          <source>Multi-output targets.</source>
          <target state="translated">Objetivos de salida múltiple.</target>
        </trans-unit>
        <trans-unit id="d25d7d780166f0481648cccd463a78a5e417f6f3" translate="yes" xml:space="preserve">
          <source>Multi-output targets. An indicator matrix turns on multilabel estimation.</source>
          <target state="translated">Objetivos de salida múltiple.Una matriz de indicadores activa la estimación multietapa.</target>
        </trans-unit>
        <trans-unit id="775030d60513b2f729789206b06d021b6661e16d" translate="yes" xml:space="preserve">
          <source>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">Modelo de ElasticNet multitarea entrenado con norma mixta L1/L2 como regularizador</target>
        </trans-unit>
        <trans-unit id="7259143bf01ac8062e2b5725644f1e005f808315" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 ElasticNet with built-in cross-validation.</source>
          <target state="translated">Multitarea L1/L2 ElasticNet con validación cruzada incorporada.</target>
        </trans-unit>
        <trans-unit id="5c1ad40e838b03e514631adae1736168414d6605" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation</source>
          <target state="translated">Lazo L1/L2 multitarea con validación cruzada incorporada</target>
        </trans-unit>
        <trans-unit id="a743aa48cf046a15087b0f4886f436159c359dd5" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation.</source>
          <target state="translated">Lazo L1/L2 multitarea con validación cruzada incorporada.</target>
        </trans-unit>
        <trans-unit id="6377873684d0ac47f9792cf0130c074e6b5d5c8f" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">Un modelo de lazo multitarea entrenado con una norma mixta L1/L2 como regularizador</target>
        </trans-unit>
        <trans-unit id="162889b9c309e59105e244387d111bbb75ed4cc7" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0119eef45392f9d57273a8cd6ca2fcc5f0003969" translate="yes" xml:space="preserve">
          <source>Multi-task linear regressors with variable selection</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="669e809a0e7044a9302d0da3188c44feddafe180" translate="yes" xml:space="preserve">
          <source>Multiclass and multilabel classification strategies</source>
          <target state="translated">Estrategias de clasificación multiclase y multietiqueta</target>
        </trans-unit>
        <trans-unit id="8ddfaa46f2a114c89c7cad99fd0ddf4dc7314399" translate="yes" xml:space="preserve">
          <source>Multiclass case:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="957cc5ae23e389ffa9c767fc16d7ac37036b0153" translate="yes" xml:space="preserve">
          <source>Multiclass classification</source>
          <target state="translated">Clasificación multiclase</target>
        </trans-unit>
        <trans-unit id="868117baea7dbed0e92972aac1d890c07c8ae48f" translate="yes" xml:space="preserve">
          <source>Multiclass data will be treated as if binarized under a one-vs-rest transformation. Returned confusion matrices will be in the order of sorted unique labels in the union of (y_true, y_pred).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d8780a18f5ba3e6cb5bb5a579ea67a8f2550bb3" translate="yes" xml:space="preserve">
          <source>Multiclass only. Determines the type of configuration to use. The default value raises an error, so either &lt;code&gt;'ovr'&lt;/code&gt; or &lt;code&gt;'ovo'&lt;/code&gt; must be passed explicitly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6acefc7f4185f0f4df5b1178aaed55afe1147ed" translate="yes" xml:space="preserve">
          <source>Multiclass only. List of labels that index the classes in &lt;code&gt;y_score&lt;/code&gt;. If &lt;code&gt;None&lt;/code&gt;, the numerical or lexicographical order of the labels in &lt;code&gt;y_true&lt;/code&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f43fb647f0e5eccf5a3760b6eafe5e21b79a50a6" translate="yes" xml:space="preserve">
          <source>Multiclass probability estimates are derived from binary (one-vs.-rest) estimates by simple normalization, as recommended by Zadrozny and Elkan.</source>
          <target state="translated">Las estimaciones de probabilidad de múltiples clases se derivan de las estimaciones binarias (uno-vs.-descanso)por simple normalización,como lo recomiendan Zadrozny y Elkan.</target>
        </trans-unit>
        <trans-unit id="39d0ca41499d6b7f83a17678aedf5fae33705c06" translate="yes" xml:space="preserve">
          <source>Multiclass problems are binarized and treated like the corresponding multilabel problem:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3debc5753cd55840fa65a540929e237591ad2faf" translate="yes" xml:space="preserve">
          <source>Multiclass settings</source>
          <target state="translated">Ajustes de multiclase</target>
        </trans-unit>
        <trans-unit id="e3f8736465f26b4a50bfa9739f8adbcfb24ccc56" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logisitic regression on newgroups20</source>
          <target state="translated">Regresión logística de multiclase dispersa en los nuevos grupos20</target>
        </trans-unit>
        <trans-unit id="abed0a03e9d2180975b0a68aad7dbbccddc0d2d0" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logistic regression on 20newgroups</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38a70920d0cd2001f4ef8f9ee41dfa6b122f018c" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="translated">Agrupaci&amp;oacute;n espectral multiclase, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4386a05880a13b860ce6d4571568b773373f22e3" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5d0b14c4e8dd95e44e1cd37847a8b6674049750" translate="yes" xml:space="preserve">
          <source>Multiclass vs. multilabel fitting</source>
          <target state="translated">Adaptación multiclase vs.multietiqueta</target>
        </trans-unit>
        <trans-unit id="dbc4079d7d6495ef3cfc4fdaba01141075b60d89" translate="yes" xml:space="preserve">
          <source>Multidimensional scaling</source>
          <target state="translated">Escalado multidimensional</target>
        </trans-unit>
        <trans-unit id="7c33b81ffc3ca04c62af4f5074ba33e510028ebd" translate="yes" xml:space="preserve">
          <source>Multilabel classification</source>
          <target state="translated">Clasificación de la multi-etiqueta</target>
        </trans-unit>
        <trans-unit id="c720ba81272f13af125e464e56bd5648c8146ada" translate="yes" xml:space="preserve">
          <source>Multilabel ranking metrics</source>
          <target state="translated">Métrica de clasificación multi-etiqueta</target>
        </trans-unit>
        <trans-unit id="ce79d912af81c5a374445c22e6e3cfe9ecb9a93c" translate="yes" xml:space="preserve">
          <source>Multilabel-indicator case:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6031d58e46d313eca93045d9598faea168256f9" translate="yes" xml:space="preserve">
          <source>Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See &lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;Using multiple metric evaluation&lt;/a&gt; for more details.</source>
          <target state="translated">La puntuaci&amp;oacute;n multim&amp;eacute;trica puede especificarse como una lista de cadenas de nombres de puntuaciones predefinidos o un dict que asigna el nombre del anotador a la funci&amp;oacute;n del anotador y / o el nombre (s) del anotador predefinido. Consulte &lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;Uso de la evaluaci&amp;oacute;n&lt;/a&gt; de varias m&amp;eacute;tricas para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="71e5ed6f7fb13f64a7d1e47fd6ffef12dfd5580e" translate="yes" xml:space="preserve">
          <source>Multinomial + L1 penalty</source>
          <target state="translated">Multinomio+L1 de penalización</target>
        </trans-unit>
        <trans-unit id="82d79c421161a2e0a31300a79127c9804ae62ed5" translate="yes" xml:space="preserve">
          <source>Multinomial + L2 penalty</source>
          <target state="translated">Multinomio+L2 de penalización</target>
        </trans-unit>
        <trans-unit id="efccef2252a812759badf849dd9e2acd4cd7eb95" translate="yes" xml:space="preserve">
          <source>Multinomial deviance (&lt;code&gt;'deviance'&lt;/code&gt;): The negative multinomial log-likelihood loss function for multi-class classification with &lt;code&gt;n_classes&lt;/code&gt; mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration &lt;code&gt;n_classes&lt;/code&gt; regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.</source>
          <target state="translated">Desviaci&amp;oacute;n multinomial ( &lt;code&gt;'deviance'&lt;/code&gt; ): La funci&amp;oacute;n de p&amp;eacute;rdida de probabilidad logar&amp;iacute;tmica multinomial negativa para la clasificaci&amp;oacute;n de clases m&amp;uacute;ltiples con &lt;code&gt;n_classes&lt;/code&gt; clases mutuamente excluyentes. Proporciona estimaciones de probabilidad. El modelo inicial viene dado por la probabilidad previa de cada clase. En cada iteraci&amp;oacute;n , se deben construir &lt;code&gt;n_classes&lt;/code&gt; &amp;aacute;rboles de regresi&amp;oacute;n, lo que hace que GBRT sea bastante ineficiente para conjuntos de datos con una gran cantidad de clases.</target>
        </trans-unit>
        <trans-unit id="313293589005fec34a4137f7e7a462e44753a91e" translate="yes" xml:space="preserve">
          <source>Multioutput classification support can be added to any classifier with &lt;code&gt;MultiOutputClassifier&lt;/code&gt;. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3&amp;hellip;,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3&amp;hellip;,yn).</source>
          <target state="translated">El soporte de clasificaci&amp;oacute;n de &lt;code&gt;MultiOutputClassifier&lt;/code&gt; se puede agregar a cualquier clasificador con MultiOutputClassifier . Esta estrategia consiste en ajustar un clasificador por objetivo. Esto permite m&amp;uacute;ltiples clasificaciones de variables objetivo. El prop&amp;oacute;sito de esta clase es extender los estimadores para poder estimar una serie de funciones objetivo (f1, f2, f3&amp;hellip;, fn) que se entrenan en una &amp;uacute;nica matriz de predictores X para predecir una serie de respuestas (y1, y2, y3 &amp;hellip;, Yn).</target>
        </trans-unit>
        <trans-unit id="8ec2d1e390ee85463a8b9edc1df8f6a33597454a" translate="yes" xml:space="preserve">
          <source>Multioutput methods</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e7bfb83db794fa15648cd7ab3b23c509cc8018c" translate="yes" xml:space="preserve">
          <source>Multioutput regression</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="086b68ade408f93caaac80f71e1eabb4cc44f3fd" translate="yes" xml:space="preserve">
          <source>Multioutput regression support can be added to any regressor with &lt;code&gt;MultiOutputRegressor&lt;/code&gt;. This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As &lt;code&gt;MultiOutputRegressor&lt;/code&gt; fits one regressor per target it can not take advantage of correlations between targets.</source>
          <target state="translated">Se puede agregar soporte de regresi&amp;oacute;n de &lt;code&gt;MultiOutputRegressor&lt;/code&gt; a cualquier regresor con MultiOutputRegressor . Esta estrategia consiste en ajustar un regresor por objetivo. Dado que cada objetivo est&amp;aacute; representado por exactamente un regresor, es posible obtener conocimiento sobre el objetivo inspeccionando su regresor correspondiente. Como &lt;code&gt;MultiOutputRegressor&lt;/code&gt; se ajusta a un regresor por objetivo, no puede aprovechar las correlaciones entre objetivos.</target>
        </trans-unit>
        <trans-unit id="96d87119823da5637cea208be6276fad5c922737" translate="yes" xml:space="preserve">
          <source>Multioutput- multiclass classification</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96e252b1f2ecf6cba5d585af259eddb308663e2e" translate="yes" xml:space="preserve">
          <source>Multiple metric evaluation using &lt;code&gt;cross_validate&lt;/code&gt; (please refer the &lt;code&gt;scoring&lt;/code&gt; parameter doc for more information)</source>
          <target state="translated">Evaluaci&amp;oacute;n de varias m&amp;eacute;tricas mediante &lt;code&gt;cross_validate&lt;/code&gt; (consulte el documento del par&amp;aacute;metro de &lt;code&gt;scoring&lt;/code&gt; para obtener m&amp;aacute;s informaci&amp;oacute;n)</target>
        </trans-unit>
        <trans-unit id="629b6c06ee9b92eec539c00c0d5b033d1b11a26d" translate="yes" xml:space="preserve">
          <source>Multiple metric parameter search can be done by setting the &lt;code&gt;scoring&lt;/code&gt; parameter to a list of metric scorer names or a dict mapping the scorer names to the scorer callables.</source>
          <target state="translated">Se puede realizar una b&amp;uacute;squeda de par&amp;aacute;metros de m&amp;eacute;tricas m&amp;uacute;ltiples estableciendo el par&amp;aacute;metro de &lt;code&gt;scoring&lt;/code&gt; en una lista de nombres de anotadores de m&amp;eacute;tricas o un dict que mapee los nombres de los anotadores a los anotadores callables.</target>
        </trans-unit>
        <trans-unit id="24f6a3478d65f4dead755fd18d3792f81fa6260d" translate="yes" xml:space="preserve">
          <source>Multiple stacking layers can be achieved by assigning &lt;code&gt;final_estimator&lt;/code&gt; to a &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor&quot;&gt;&lt;code&gt;StackingRegressor&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85ae0fa16a20d62aa04d6b821cea2aa9547567a2" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. Keys are transformer names, values the weights.</source>
          <target state="translated">Pesos multiplicadores para las características por transformador.Las claves son los nombres de los transformadores,los valores de los pesos.</target>
        </trans-unit>
        <trans-unit id="0634d761605b2ffdac7cc3b47cb937d2911bb7fc" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.</source>
          <target state="translated">Pesos multiplicadores para las características por transformador.La salida del transformador se multiplica por estos pesos.Las claves son los nombres de los transformadores,los valores de los pesos.</target>
        </trans-unit>
        <trans-unit id="afa1ae58a55a69631c4c27e76dfc260c619f73c7" translate="yes" xml:space="preserve">
          <source>Multipliers of parameter C for each class. Computed based on the &lt;code&gt;class_weight&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0e900a51b93880c89d198a9d719bd1f20cdb329" translate="yes" xml:space="preserve">
          <source>Multipliers of parameter C of each class. Computed based on the &lt;code&gt;class_weight&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f4f1f6c0e0110908215d6d402a5fd0376794171" translate="yes" xml:space="preserve">
          <source>Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting.</source>
          <target state="translated">Multiplica las características por el valor especificado.Si no hay ninguno,entonces los rasgos se escalan por un valor aleatorio dibujado en [1,100].Tenga en cuenta que la escalada se produce después del desplazamiento.</target>
        </trans-unit>
        <trans-unit id="d54881ba1eca5e77240b1b917c4b4af86c4398ba" translate="yes" xml:space="preserve">
          <source>Multiplying the coefficients by the standard deviation of the related feature would reduce all the coefficients to the same unit of measure. As we will see &lt;a href=&quot;#scaling-num&quot;&gt;after&lt;/a&gt; this is equivalent to normalize numerical variables to their standard deviation, as \(y = \sum{coef_i \times X_i} = \sum{(coef_i \times std_i) \times (X_i / std_i)}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="773db00cec71fc706de69e832dff6b23a68d6b97" translate="yes" xml:space="preserve">
          <source>Multithreaded BLAS libraries sometimes conflict with Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; module, which is used by e.g. &lt;code&gt;GridSearchCV&lt;/code&gt; and most other estimators that take an &lt;code&gt;n_jobs&lt;/code&gt; argument (with the exception of &lt;code&gt;SGDClassifier&lt;/code&gt;, &lt;code&gt;SGDRegressor&lt;/code&gt;, &lt;code&gt;Perceptron&lt;/code&gt;, &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; and tree-based methods such as random forests). This is true of Apple&amp;rsquo;s Accelerate and OpenBLAS when built with OpenMP support.</source>
          <target state="translated">Las bibliotecas BLAS multiproceso a veces entran en conflicto con el m&amp;oacute;dulo de &lt;code&gt;multiprocessing&lt;/code&gt; de Python , que es utilizado por, por ejemplo, &lt;code&gt;GridSearchCV&lt;/code&gt; y la mayor&amp;iacute;a de los otros estimadores que toman un argumento &lt;code&gt;n_jobs&lt;/code&gt; (con la excepci&amp;oacute;n de &lt;code&gt;SGDClassifier&lt;/code&gt; , &lt;code&gt;SGDRegressor&lt;/code&gt; , &lt;code&gt;Perceptron&lt;/code&gt; , &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; y m&amp;eacute;todos basados ​​en &amp;aacute;rboles como bosques aleatorios). Esto es cierto para Accelerate y OpenBLAS de Apple cuando se construyen con soporte OpenMP.</target>
        </trans-unit>
        <trans-unit id="3483a919f49e511ca829411c285a74281e005ef3" translate="yes" xml:space="preserve">
          <source>Multivariate imputation of missing values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e386b49678343bab915e96a29e16b2a1aa09993" translate="yes" xml:space="preserve">
          <source>Multivariate imputer that estimates each feature from all the others.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="425dc1fa519b0f6261993bae28e1ad51c131bb66" translate="yes" xml:space="preserve">
          <source>Must be provided at the first call to partial_fit, can be omitted in subsequent calls.</source>
          <target state="translated">Debe proporcionarse en la primera llamada a partial_fit,puede omitirse en llamadas posteriores.</target>
        </trans-unit>
        <trans-unit id="b6845c300d4f945b800f2e50de745703cc5cc891" translate="yes" xml:space="preserve">
          <source>Must fulfill the input assumptions of the underlying estimator.</source>
          <target state="translated">Debe cumplir con los supuestos de entrada del estimador subyacente.</target>
        </trans-unit>
        <trans-unit id="214188886e4a84a8788bdd82b6f8744f5146fead" translate="yes" xml:space="preserve">
          <source>Mutual Information (not adjusted for chance)</source>
          <target state="translated">Información mutua (no ajustada al azar)</target>
        </trans-unit>
        <trans-unit id="16b7cc0e7a5234ba809ed1e09a3a8960dff39693" translate="yes" xml:space="preserve">
          <source>Mutual Information between two clusterings.</source>
          <target state="translated">Información mutua entre dos agrupaciones.</target>
        </trans-unit>
        <trans-unit id="81e08bee8a8968c08bd07aed8cef44d9fb7a13f3" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">La informaci&amp;oacute;n mutua (MI) &lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; entre dos variables aleatorias es un valor no negativo, que mide la dependencia entre las variables. Es igual a cero si y solo si dos variables aleatorias son independientes y los valores m&amp;aacute;s altos significan una mayor dependencia.</target>
        </trans-unit>
        <trans-unit id="92d0e5dc6672a19ad8c5b9523b6a6d1a9b82c89e" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">La informaci&amp;oacute;n mutua (MI) &lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; entre dos variables aleatorias es un valor no negativo, que mide la dependencia entre las variables. Es igual a cero si y solo si dos variables aleatorias son independientes y los valores m&amp;aacute;s altos significan una mayor dependencia.</target>
        </trans-unit>
        <trans-unit id="4276bd70be44db9c6fb9548906a97ab826aba297" translate="yes" xml:space="preserve">
          <source>Mutual information between features and the target.</source>
          <target state="translated">Información mutua entre las características y el objetivo.</target>
        </trans-unit>
        <trans-unit id="33ca9360bf5453bcb4bf9aed03e658f161f23932" translate="yes" xml:space="preserve">
          <source>Mutual information for a continuous target.</source>
          <target state="translated">Información mutua para un objetivo continuo.</target>
        </trans-unit>
        <trans-unit id="aa199ad103c044c23c4e2e0edbe572bad088827c" translate="yes" xml:space="preserve">
          <source>Mutual information for a contnuous target.</source>
          <target state="translated">Información mutua para un objetivo contnuo.</target>
        </trans-unit>
        <trans-unit id="ef9610a089a978dd0d661be292e2bde712e413d1" translate="yes" xml:space="preserve">
          <source>Mutual information for a discrete target.</source>
          <target state="translated">Información mutua para un objetivo discreto.</target>
        </trans-unit>
        <trans-unit id="2555b04ef28112b324874c1cc2f3bf2b5b5c384e" translate="yes" xml:space="preserve">
          <source>Mutual information, a non-negative value</source>
          <target state="translated">La información mutua,un valor no negativo</target>
        </trans-unit>
        <trans-unit id="b51a60734da64be0e618bacbea2865a8a7dcd669" translate="yes" xml:space="preserve">
          <source>N</source>
          <target state="translated">N</target>
        </trans-unit>
        <trans-unit id="4e1221dedd7ee34eb6931a44dc15d9a84ca69a81" translate="yes" xml:space="preserve">
          <source>N : number of dimensions</source>
          <target state="translated">N:número de dimensiones</target>
        </trans-unit>
        <trans-unit id="8daf5ce04352d841160e980446540ca50cce58e4" translate="yes" xml:space="preserve">
          <source>N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.</source>
          <target state="translated">¡N-grams al rescate! En lugar de construir una simple colección de unigramas (n=1),uno podría preferir una colección de bigrams (n=2),donde se cuentan las ocurrencias de pares de palabras consecutivas.</target>
        </trans-unit>
        <trans-unit id="0d5c48bb908535393359e08969f6193dc43fff44" translate="yes" xml:space="preserve">
          <source>NCA can be seen as learning a (squared) Mahalanobis distance metric:</source>
          <target state="new"/>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
