<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="73890f2d8730349939758bb99469c8dd6b7e6151" translate="yes" xml:space="preserve">
          <source>The maximum number of patches per image to extract. If max_patches is a float in (0, 1), it is taken to mean a proportion of the total number of patches.</source>
          <target state="translated">El número máximo de parches por imagen a extraer.Si max_patches es un flotador en (0,1),se considera que es una proporción del número total de parches.</target>
        </trans-unit>
        <trans-unit id="9914613b7b3d16fc7caab060022c123f66024339" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="translated">El número máximo de parches a extraer.Si max_patches es un flotador entre 0 y 1,se considera que es una proporción del número total de parches.</target>
        </trans-unit>
        <trans-unit id="353b6da890fd9a7a3db82e5fc166d25d61607aa3" translate="yes" xml:space="preserve">
          <source>The maximum number of points on the path used to compute the residuals in the cross-validation</source>
          <target state="translated">El número máximo de puntos en el camino utilizado para calcular los residuos en la validación cruzada</target>
        </trans-unit>
        <trans-unit id="c53181fb6c1656f1967e529a2fe72fd9bc29ff60" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the full dataset, which break down as soon as there are outliers in the data set</source>
          <target state="translated">La media y la covarianza empírica del conjunto de datos completo,que se descomponen tan pronto como hay valores atípicos en el conjunto de datos</target>
        </trans-unit>
        <trans-unit id="f962fad68fff332002b42ee3dcc1e06f41fcfe6c" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the observations that are known to be good ones. This can be considered as a &amp;ldquo;perfect&amp;rdquo; MCD estimation, so one can trust our implementation by comparing to this case.</source>
          <target state="translated">La media y la covarianza emp&amp;iacute;rica de las observaciones que se sabe que son buenas. Esto puede considerarse como una estimaci&amp;oacute;n MCD &amp;ldquo;perfecta&amp;rdquo;, por lo que uno puede confiar en nuestra implementaci&amp;oacute;n compar&amp;aacute;ndola con este caso.</target>
        </trans-unit>
        <trans-unit id="f98043dc9f229ba1c70dcb8021abddb5d793d8af" translate="yes" xml:space="preserve">
          <source>The mean of each mixture component.</source>
          <target state="translated">La media de cada componente de la mezcla.</target>
        </trans-unit>
        <trans-unit id="05329e8678ffdabb505c75140f9a49322a5129f1" translate="yes" xml:space="preserve">
          <source>The mean of the multi-dimensional normal distribution. If None then use the origin (0, 0, &amp;hellip;).</source>
          <target state="translated">La media de la distribuci&amp;oacute;n normal multidimensional. Si es Ninguno, utilice el origen (0, 0,&amp;hellip;).</target>
        </trans-unit>
        <trans-unit id="3598a929d1a64528ce62560f1aa02f9fb9981b30" translate="yes" xml:space="preserve">
          <source>The mean predicted probability in each bin.</source>
          <target state="translated">La media de la probabilidad predicha en cada recipiente.</target>
        </trans-unit>
        <trans-unit id="8aaf5602eb2242bddc9912d47746326b31b0a1d5" translate="yes" xml:space="preserve">
          <source>The mean score and the 95% confidence interval of the score estimate are hence given by:</source>
          <target state="translated">La puntuación media y el intervalo de confianza del 95% de la estimación de la puntuación vienen dados por:</target>
        </trans-unit>
        <trans-unit id="94d27e1df24bf9a05c82ae625a1197b415bc3fdc" translate="yes" xml:space="preserve">
          <source>The mean value for each feature in the training set. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_mean=False&lt;/code&gt;.</source>
          <target state="translated">El valor medio de cada caracter&amp;iacute;stica del conjunto de entrenamiento. Igual a &lt;code&gt;None&lt;/code&gt; cuando &lt;code&gt;with_mean=False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b5e7dcb7d882c8689297cb339a998a6e74140e5f" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.</source>
          <target state="translated">La media, el error est&amp;aacute;ndar y el &quot;peor&quot; o el m&amp;aacute;s grande (la media de los tres valores m&amp;aacute;s grandes) de estas caracter&amp;iacute;sticas se calcularon para cada imagen, lo que result&amp;oacute; en 30 caracter&amp;iacute;sticas. Por ejemplo, el campo 3 es Radio medio, el campo 13 es Radio SE, el campo 23 es Peor radio.</target>
        </trans-unit>
        <trans-unit id="9952f9f2a02ad0002415a0bd8b6c9bae196d7ee8" translate="yes" xml:space="preserve">
          <source>The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.</source>
          <target state="translated">La medida de la normalidad de una observación dada en un árbol es la profundidad de la hoja que contiene esta observación,que equivale al número de divisiones necesarias para aislar este punto.En el caso de varias observaciones n_left en la hoja,se añade la longitud media del trayecto de un árbol de aislamiento de n_left muestras.</target>
        </trans-unit>
        <trans-unit id="3ff66cd9c701474c3d9f795cee9d82f5e1659bc8" translate="yes" xml:space="preserve">
          <source>The median absolute deviation to non corrupt new data is used to judge the quality of the prediction.</source>
          <target state="translated">La media de la desviación absoluta de los nuevos datos no corruptos se utiliza para juzgar la calidad de la predicción.</target>
        </trans-unit>
        <trans-unit id="970571bcac487854f4caea3e516e12da8ac34c22" translate="yes" xml:space="preserve">
          <source>The median value for each feature in the training set.</source>
          <target state="translated">El valor medio de cada característica del conjunto de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="c8bf90a15bbbe280812d1ade2a9f9077adb9d41d" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments.</source>
          <target state="translated">El modo de memmapping utilizado cuando se carga desde la caché de matrices numéricas.Ver numpy.load para el significado de los argumentos.</target>
        </trans-unit>
        <trans-unit id="4971ea4c8c64c5bc8a4bdd1e4563c9705f8166bf" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments. By default that of the memory object is used.</source>
          <target state="translated">El modo de memmapping utilizado cuando se carga desde la caché de matrices numéricas.Ver numpy.load para el significado de los argumentos.Por defecto se utiliza el del objeto de memoria.</target>
        </trans-unit>
        <trans-unit id="85874f620128fa58a2e359ad1c9c828fcfd537bb" translate="yes" xml:space="preserve">
          <source>The memory footprint of randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is also proportional to \(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\) instead of \(n_{\max} \cdot n_{\min}\) for the exact method.</source>
          <target state="translated">La huella de memoria del &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; aleatorizado tambi&amp;eacute;n es proporcional a \ (2 \ cdot n _ {\ max} \ cdot n _ {\ mathrm {components}} \) en lugar de \ (n _ {\ max} \ cdot n _ {\ min} \) para el m&amp;eacute;todo exacto.</target>
        </trans-unit>
        <trans-unit id="4fa2b59a5a9d6863d86f91b2a847ac2c39bc204e" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier.</source>
          <target state="translated">El método asume que las entradas provienen de un clasificador binario.</target>
        </trans-unit>
        <trans-unit id="ff8ec8205e374ddf520735804ecbfa39550f37bd" translate="yes" xml:space="preserve">
          <source>The method fits the model &lt;code&gt;n_init&lt;/code&gt; times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. If &lt;code&gt;warm_start&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then &lt;code&gt;n_init&lt;/code&gt; is ignored and a single initialization is performed upon the first call. Upon consecutive calls, training starts where it left off.</source>
          <target state="translated">El m&amp;eacute;todo se ajusta al modelo &lt;code&gt;n_init&lt;/code&gt; veces y establece los par&amp;aacute;metros con los que el modelo tiene la mayor probabilidad o l&amp;iacute;mite inferior. Dentro de cada ensayo, el m&amp;eacute;todo itera entre el paso E y el paso M para tiempos de &lt;code&gt;max_iter&lt;/code&gt; hasta que el cambio de probabilidad o l&amp;iacute;mite inferior sea menor que &lt;code&gt;tol&lt;/code&gt; ; de lo contrario, se genera una advertencia de &lt;code&gt;ConvergenceWarning&lt;/code&gt; . Si &lt;code&gt;warm_start&lt;/code&gt; es &lt;code&gt;True&lt;/code&gt; , entonces &lt;code&gt;n_init&lt;/code&gt; se ignora y se realiza una &amp;uacute;nica inicializaci&amp;oacute;n en la primera llamada. Tras llamadas consecutivas, el entrenamiento comienza donde lo dej&amp;oacute;.</target>
        </trans-unit>
        <trans-unit id="5aeb4fb178bdd7f679ad7ab31606d57c02d18da8" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="translated">El m&amp;eacute;todo se ajusta al modelo n_init veces y establece los par&amp;aacute;metros con los que el modelo tiene la mayor probabilidad o l&amp;iacute;mite inferior. Dentro de cada prueba, el m&amp;eacute;todo itera entre el paso E y el paso M para tiempos de &lt;code&gt;max_iter&lt;/code&gt; hasta que el cambio de probabilidad o l&amp;iacute;mite inferior sea menor que &lt;code&gt;tol&lt;/code&gt; ; de lo contrario, se genera una advertencia de &lt;code&gt;ConvergenceWarning&lt;/code&gt; . Despu&amp;eacute;s de ajustar, predice la etiqueta m&amp;aacute;s probable para los puntos de datos de entrada.</target>
        </trans-unit>
        <trans-unit id="02e13e01ea0f52a6e082ed53d9636e409ba7f22e" translate="yes" xml:space="preserve">
          <source>The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.</source>
          <target state="translated">El método ganó popularidad para inicializar las redes neuronales profundas con los pesos de los RBM independientes.Este método se conoce como pre-entrenamiento no supervisado.</target>
        </trans-unit>
        <trans-unit id="867b88e44ce337abe62bbd2070ac4235fc6ec53b" translate="yes" xml:space="preserve">
          <source>The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.</source>
          <target state="translated">El método de Clasificación de Vectores de Apoyo puede extenderse para resolver problemas de regresión.Este método se llama Regresión del Vector de Apoyo.</target>
        </trans-unit>
        <trans-unit id="0b9b2478ee76aa8f8a62d64db00bfa346a8108cd" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit. Use sigmoids (Platt&amp;rsquo;s calibration) in this case.</source>
          <target state="translated">El m&amp;eacute;todo a utilizar para la calibraci&amp;oacute;n. Puede ser &quot;sigmoide&quot;, que corresponde al m&amp;eacute;todo de Platt, o &quot;isot&amp;oacute;nico&quot;, que es un enfoque no param&amp;eacute;trico. No se recomienda utilizar la calibraci&amp;oacute;n isot&amp;oacute;nica con muy pocas muestras de calibraci&amp;oacute;n &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; ya que tiende a sobreajustarse. Utilice sigmoides (calibraci&amp;oacute;n de Platt) en este caso.</target>
        </trans-unit>
        <trans-unit id="21e55011234a7eafaab581faf9cf56cabba5a5c2" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the covariances. Must be one of:</source>
          <target state="translated">El método utilizado para inicializar los pesos,los medios y las covarianzas.Debe ser uno de:</target>
        </trans-unit>
        <trans-unit id="963cb60032e3efabe5bef9b8ad76de7b8282f830" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the precisions. Must be one of:</source>
          <target state="translated">El método utilizado para inicializar los pesos,los medios y las precisiones.Debe ser uno de:</target>
        </trans-unit>
        <trans-unit id="8015d2d76c1dfbbe3de68f3d3f8aa78fbf89dd67" translate="yes" xml:space="preserve">
          <source>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">El m&amp;eacute;todo funciona tanto en estimadores simples como en objetos anidados (como tuber&amp;iacute;as). Estos &amp;uacute;ltimos tienen par&amp;aacute;metros de la forma &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; para que sea posible actualizar cada componente de un objeto anidado.</target>
        </trans-unit>
        <trans-unit id="5d1673cba254e117532409bf5f2a151ed75e6f39" translate="yes" xml:space="preserve">
          <source>The method works on simple kernels as well as on nested kernels. The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">El m&amp;eacute;todo funciona tanto en kernels simples como en kernels anidados. Estos &amp;uacute;ltimos tienen par&amp;aacute;metros de la forma &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; para que sea posible actualizar cada componente de un objeto anidado.</target>
        </trans-unit>
        <trans-unit id="653c073b67dfbefaf963d2a9e7b682aaf13d10c5" translate="yes" xml:space="preserve">
          <source>The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.</source>
          <target state="translated">Los métodos basados en la prueba F estiman el grado de dependencia lineal entre dos variables aleatorias.Por otra parte,los métodos de información mutua pueden captar cualquier tipo de dependencia estadística,pero al no ser paramétricos,requieren más muestras para una estimación precisa.</target>
        </trans-unit>
        <trans-unit id="42e3e79f7ef752ca5a9bd963af37b6abd9f4c61f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena o invocable, debe ser una de las opciones permitidas por &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt; &lt;/a&gt; para su par&amp;aacute;metro de m&amp;eacute;trica. Si la m&amp;eacute;trica est&amp;aacute; &amp;ldquo;calculada previamente&amp;rdquo;, se supone que X es una matriz de distancia y debe ser cuadrada. X puede ser una matriz dispersa, en cuyo caso solo los elementos &quot;distintos de cero&quot; pueden considerarse vecinos para DBSCAN.</target>
        </trans-unit>
        <trans-unit id="379ba7f47f97569c7bd4b20c4c77667f05344897" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples corresponding to each class is the point from which the sum of the distances (according to the metric) of all samples that belong to that particular class are minimized. If the &amp;ldquo;manhattan&amp;rdquo; metric is provided, this centroid is the median and for all other metrics, the centroid is now set to be the mean.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena o se puede llamar, debe ser una de las opciones permitidas por metrics.pairwise.pairwise_distances para su par&amp;aacute;metro de m&amp;eacute;trica. Los centroides de las muestras correspondientes a cada clase es el punto desde el cual se minimiza la suma de las distancias (seg&amp;uacute;n la m&amp;eacute;trica) de todas las muestras que pertenecen a esa clase en particular. Si se proporciona la m&amp;eacute;trica &quot;manhattan&quot;, este centroide es la mediana y para todas las dem&amp;aacute;s m&amp;eacute;tricas, el centroide ahora se establece como la media.</target>
        </trans-unit>
        <trans-unit id="c83cb22e1c81dc697b4c0637e95b8aeb91bdccce" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt;.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si metric es una cadena, debe ser una de las opciones permitidas por &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt; . Si X es la matriz de distancias en s&amp;iacute;, use &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="76b4b37055a409fe2e4e93cf3ad5227beac3187f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si metric es una cadena, debe ser una de las opciones permitidas por &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt; . Si X es la matriz de distancia en s&amp;iacute;, use &quot;precalculado&quot; como m&amp;eacute;trica.</target>
        </trans-unit>
        <trans-unit id="f0f209bf70493187675786c136d5f63b20f1b34d" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena, debe ser una de las opciones permitidas por scipy.spatial.distance.pdist para su par&amp;aacute;metro de m&amp;eacute;trica, o una m&amp;eacute;trica listada en pares.PAIRWISE_DISTANCE_FUNCTIONS. Si la m&amp;eacute;trica est&amp;aacute; &amp;ldquo;precalculada&amp;rdquo;, se supone que X es una matriz de distancia. Alternativamente, si m&amp;eacute;trica es una funci&amp;oacute;n invocable, se llama en cada par de instancias (filas) y se registra el valor resultante. El invocable debe tomar dos matrices de X como entrada y devolver un valor que indique la distancia entre ellas.</target>
        </trans-unit>
        <trans-unit id="5d6bfb4b6dd59ce295c63dba458c016e3505d6e2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is &amp;ldquo;euclidean&amp;rdquo; which is interpreted as squared euclidean distance.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena, debe ser una de las opciones permitidas por scipy.spatial.distance.pdist para su par&amp;aacute;metro de m&amp;eacute;trica, o una m&amp;eacute;trica listada en pares.PAIRWISE_DISTANCE_FUNCTIONS. Si la m&amp;eacute;trica est&amp;aacute; &amp;ldquo;precalculada&amp;rdquo;, se supone que X es una matriz de distancia. Alternativamente, si m&amp;eacute;trica es una funci&amp;oacute;n invocable, se llama en cada par de instancias (filas) y se registra el valor resultante. El invocable debe tomar dos matrices de X como entrada y devolver un valor que indique la distancia entre ellas. El valor predeterminado es &quot;euclidiana&quot;, que se interpreta como distancia euclidiana al cuadrado.</target>
        </trans-unit>
        <trans-unit id="1d21e688cc862f7d70c6767af76fb5452c5fdcd0" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options specified in PAIRED_DISTANCES, including &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, or &amp;ldquo;cosine&amp;rdquo;. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena, debe ser una de las opciones especificadas en PAIRED_DISTANCES, incluidas &amp;ldquo;euclidiana&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo; o &amp;ldquo;coseno&amp;rdquo;. Alternativamente, si m&amp;eacute;trica es una funci&amp;oacute;n invocable, se llama en cada par de instancias (filas) y se registra el valor resultante. El invocable debe tomar dos matrices de X como entrada y devolver un valor que indique la distancia entre ellas.</target>
        </trans-unit>
        <trans-unit id="f36b3ebd6c1e0314a19882dfd7c792465ced8cb2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular el kernel entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena, debe ser una de las m&amp;eacute;tricas en pares.PAIRWISE_KERNEL_FUNCTIONS. Si la m&amp;eacute;trica est&amp;aacute; &quot;precalculada&quot;, se supone que X es una matriz del n&amp;uacute;cleo. Alternativamente, si m&amp;eacute;trica es una funci&amp;oacute;n invocable, se llama en cada par de instancias (filas) y se registra el valor resultante. El invocable debe tomar dos matrices de X como entrada y devolver un valor que indique la distancia entre ellas.</target>
        </trans-unit>
        <trans-unit id="f4d18e9d9a4e3ba27fc7d43cfc9aa960df294a66" translate="yes" xml:space="preserve">
          <source>The minimal number of components to guarantee with good probability an eps-embedding with n_samples.</source>
          <target state="translated">El número mínimo de componentes para garantizar con buena probabilidad un acoplamiento de eps con n_muestras.</target>
        </trans-unit>
        <trans-unit id="dcfd1fcfc3ab5a126126fe7777d3f592dbce3714" translate="yes" xml:space="preserve">
          <source>The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.</source>
          <target state="translated">La puntuación mínima de consenso,0,se produce cuando todos los pares de bíceps son totalmente diferentes.La puntuación máxima,1,se produce cuando ambos conjuntos son idénticos.</target>
        </trans-unit>
        <trans-unit id="546685a327b5ecf09c20bafa81b23b9f31e36223" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantee the eps-embedding is given by:</source>
          <target state="translated">El número mínimo de componentes para garantizar la incorporación de la eps viene dado por:</target>
        </trans-unit>
        <trans-unit id="d171bb6d353676c30b749e2ca85c8811f4027e78" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantees the eps-embedding is given by:</source>
          <target state="translated">El número mínimo de componentes para garantizar la incorporación de la eps viene dado por:</target>
        </trans-unit>
        <trans-unit id="78f0aa92767e958a159a7201fe662ff62e3924a6" translate="yes" xml:space="preserve">
          <source>The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and &lt;code&gt;min_features_to_select&lt;/code&gt; isn&amp;rsquo;t divisible by &lt;code&gt;step&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero m&amp;iacute;nimo de funciones que se seleccionar&amp;aacute;n. Este n&amp;uacute;mero de funciones siempre se &lt;code&gt;min_features_to_select&lt;/code&gt; , incluso si la diferencia entre el recuento de funciones original y min_features_to_select no es divisible por &lt;code&gt;step&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1db45f422759f4529cb388eaa249fdf3a381a4d3" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least &lt;code&gt;min_samples_leaf&lt;/code&gt; training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</source>
          <target state="translated">El n&amp;uacute;mero m&amp;iacute;nimo de muestras necesarias para estar en un nodo hoja. Un punto de divisi&amp;oacute;n a cualquier profundidad solo se considerar&amp;aacute; si deja al menos muestras de entrenamiento &lt;code&gt;min_samples_leaf&lt;/code&gt; en cada una de las ramas izquierda y derecha. Esto puede tener el efecto de suavizar el modelo, especialmente en regresi&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="754bd23e1994f02a9f11fc7f2013baebc017ca4d" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to split an internal node:</source>
          <target state="translated">El número mínimo de muestras necesarias para dividir un nodo interno:</target>
        </trans-unit>
        <trans-unit id="34dec0ced4163b0b0c5f6b2dfda2c2ae915251bf" translate="yes" xml:space="preserve">
          <source>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</source>
          <target state="translated">La fracción ponderada mínima de la suma total de pesos (de todas las muestras de entrada)debe estar en un nodo de hoja.Las muestras tienen igual peso cuando no se proporciona el peso_de_la_muestra.</target>
        </trans-unit>
        <trans-unit id="adbfb8d83e84eef2c959ef548acb01276646831a" translate="yes" xml:space="preserve">
          <source>The missing indicator for input data. The data type of &lt;code&gt;Xt&lt;/code&gt; will be boolean.</source>
          <target state="translated">El indicador que falta para los datos de entrada. El tipo de datos de &lt;code&gt;Xt&lt;/code&gt; ser&amp;aacute; booleano.</target>
        </trans-unit>
        <trans-unit id="2b5249d3ed9eb260ab7aedd15c61af2c7df4b9f1" translate="yes" xml:space="preserve">
          <source>The mixing matrix to be used to initialize the algorithm.</source>
          <target state="translated">La matriz de mezcla que se utilizará para inicializar el algoritmo.</target>
        </trans-unit>
        <trans-unit id="bc7bce3b2af118e0efad437caf7d72239aa18a90" translate="yes" xml:space="preserve">
          <source>The mixing matrix.</source>
          <target state="translated">La matriz de mezcla.</target>
        </trans-unit>
        <trans-unit id="d9852ae9396dc8e73ffef0a95fb9e0d330c7fbbc" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.</source>
          <target state="translated">El modelo ajusta una densidad gaussiana a cada clase,asumiendo que todas las clases comparten la misma matriz de covarianza.</target>
        </trans-unit>
        <trans-unit id="c1fbc40d4a13f1c2e80dae47c2199d2a0ef37a24" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class.</source>
          <target state="translated">El modelo ajusta una densidad gaussiana a cada clase.</target>
        </trans-unit>
        <trans-unit id="744fe4c01d7aac1fa2b93515c5b761f9f450f5fc" translate="yes" xml:space="preserve">
          <source>The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.</source>
          <target state="translated">El modelo hace supuestos sobre la distribuci&amp;oacute;n de insumos. Por el momento, scikit-learn solo proporciona &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; , que asume que las entradas son valores binarios o valores entre 0 y 1, y cada uno codifica la probabilidad de que la funci&amp;oacute;n espec&amp;iacute;fica se active.</target>
        </trans-unit>
        <trans-unit id="3e7d91224c848a3199f89b8ed290703400860de0" translate="yes" xml:space="preserve">
          <source>The model need to have probability information computed at training time: fit with attribute &lt;code&gt;probability&lt;/code&gt; set to True.</source>
          <target state="translated">El modelo debe tener informaci&amp;oacute;n de probabilidad calculada en el momento del entrenamiento: ajuste con &lt;code&gt;probability&lt;/code&gt; atributo establecida en Verdadero.</target>
        </trans-unit>
        <trans-unit id="532fcfc5fc4303622a7e9b0379fb5dd6d278b658" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the members &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;:</source>
          <target state="translated">Se puede acceder a los par&amp;aacute;metros del modelo a trav&amp;eacute;s de los miembros &lt;code&gt;coef_&lt;/code&gt; e &lt;code&gt;intercept_&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="87fcd2dcd4a9683677aa4a82e264db34c1778c7c" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.</source>
          <target state="translated">El modelo producido por la clasificación de vectores de apoyo (como se ha descrito anteriormente)depende sólo de un subconjunto de los datos de capacitación,porque la función de costos para la construcción del modelo no se preocupa por los puntos de capacitación que se encuentran más allá del margen.Análogamente,el modelo producido por la regresión del vector de apoyo depende sólo de un subconjunto de los datos de capacitación,porque la función de costo para construir el modelo ignora cualquier dato de capacitación cercano a la predicción del modelo.</target>
        </trans-unit>
        <trans-unit id="ffaf4f38449ad493c6f07021545ef1cc0f916269" translate="yes" xml:space="preserve">
          <source>The models are ordered from strongest regularized to least regularized. The 4 coefficients of the models are collected and plotted as a &amp;ldquo;regularization path&amp;rdquo;: on the left-hand side of the figure (strong regularizers), all the coefficients are exactly 0. When regularization gets progressively looser, coefficients can get non-zero values one after the other.</source>
          <target state="translated">Los modelos est&amp;aacute;n ordenados desde el m&amp;aacute;s fuerte al menos regularizado. Los 4 coeficientes de los modelos se recopilan y trazan como una &quot;ruta de regularizaci&amp;oacute;n&quot;: en el lado izquierdo de la figura (regularizadores fuertes), todos los coeficientes son exactamente 0. Cuando la regularizaci&amp;oacute;n se vuelve progresivamente m&amp;aacute;s flexible, los coeficientes pueden ser distintos de cero valores uno tras otro.</target>
        </trans-unit>
        <trans-unit id="41864e959b3c3945768dfd483a7ad2160aff5a56" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;.</source>
          <target state="translated">El m&amp;oacute;dulo &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; incluye el popular algoritmo de refuerzo AdaBoost, introducido en 1995 por Freund y Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="06014352d795d21d24d63a43012b2cdda688123a" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted regression trees.</source>
          <target state="translated">El m&amp;oacute;dulo &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; proporciona m&amp;eacute;todos para clasificaci&amp;oacute;n y regresi&amp;oacute;n a trav&amp;eacute;s de &amp;aacute;rboles de regresi&amp;oacute;n potenciados por gradientes.</target>
        </trans-unit>
        <trans-unit id="c3b04b84598eb19b700a6f5a28a6ebcc8d4034a0" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; also exposes a set of simple functions measuring a prediction error given ground truth and prediction:</source>
          <target state="translated">El m&amp;oacute;dulo &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt; tambi&amp;eacute;n expone un conjunto de funciones simples que miden un error de predicci&amp;oacute;n dada la verdad y la predicci&amp;oacute;n del terreno:</target>
        </trans-unit>
        <trans-unit id="a4ba63f9b8e0d9fa0a182e0bb4dc5760121e3e0e" translate="yes" xml:space="preserve">
          <source>The module &lt;code&gt;partial_dependence&lt;/code&gt; provides a convenience function &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; and a two-way PDP between the two features:</source>
          <target state="translated">El m&amp;oacute;dulo &lt;code&gt;partial_dependence&lt;/code&gt; proporciona una funci&amp;oacute;n de conveniencia &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt; para crear gr&amp;aacute;ficos de dependencia parcial unidireccionales y bidireccionales. En el siguiente ejemplo, mostramos c&amp;oacute;mo crear una cuadr&amp;iacute;cula de gr&amp;aacute;ficos de dependencia parcial: dos PDP unidireccionales para las caracter&amp;iacute;sticas &lt;code&gt;0&lt;/code&gt; y &lt;code&gt;1&lt;/code&gt; y un PDP bidireccional entre las dos caracter&amp;iacute;sticas:</target>
        </trans-unit>
        <trans-unit id="5b2518b0fdafd75a6658a4d2a018bccb79345ce3" translate="yes" xml:space="preserve">
          <source>The module contains the public attributes &lt;code&gt;coefs_&lt;/code&gt; and &lt;code&gt;intercepts_&lt;/code&gt;. &lt;code&gt;coefs_&lt;/code&gt; is a list of weight matrices, where weight matrix at index \(i\) represents the weights between layer \(i\) and layer \(i+1\). &lt;code&gt;intercepts_&lt;/code&gt; is a list of bias vectors, where the vector at index \(i\) represents the bias values added to layer \(i+1\).</source>
          <target state="translated">El m&amp;oacute;dulo contiene los atributos p&amp;uacute;blicos &lt;code&gt;coefs_&lt;/code&gt; e &lt;code&gt;intercepts_&lt;/code&gt; . &lt;code&gt;coefs_&lt;/code&gt; es una lista de matrices de peso, donde la matriz de peso en el &amp;iacute;ndice \ (i \) representa los pesos entre la capa \ (i \) y la capa \ (i + 1 \). &lt;code&gt;intercepts_&lt;/code&gt; es una lista de vectores de sesgo, donde el vector en el &amp;iacute;ndice \ (i \) representa los valores de sesgo agregados a la capa \ (i + 1 \).</target>
        </trans-unit>
        <trans-unit id="b31fa7fd3ac8f2a64f0dd29549e8dd9abb96ee19" translate="yes" xml:space="preserve">
          <source>The module: &lt;code&gt;random_projection&lt;/code&gt; provides several tools for data reduction by random projections. See the relevant section of the documentation: &lt;a href=&quot;random_projection#random-projection&quot;&gt;Random Projection&lt;/a&gt;.</source>
          <target state="translated">El m&amp;oacute;dulo: &lt;code&gt;random_projection&lt;/code&gt; proporciona varias herramientas para la reducci&amp;oacute;n de datos mediante proyecciones aleatorias. Consulte la secci&amp;oacute;n correspondiente de la documentaci&amp;oacute;n: &lt;a href=&quot;random_projection#random-projection&quot;&gt;Proyecci&amp;oacute;n aleatoria&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="81f532ac90d157aeffd690ee0c3b7aa6b7bbd149" translate="yes" xml:space="preserve">
          <source>The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of &lt;code&gt;_fit_stages&lt;/code&gt; as keyword arguments &lt;code&gt;callable(i, self,
locals())&lt;/code&gt;. If the callable returns &lt;code&gt;True&lt;/code&gt; the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.</source>
          <target state="translated">Se llama al monitor despu&amp;eacute;s de cada iteraci&amp;oacute;n con la iteraci&amp;oacute;n actual, una referencia al estimador y las variables locales de &lt;code&gt;_fit_stages&lt;/code&gt; como argumentos de palabra clave &lt;code&gt;callable(i, self, locals())&lt;/code&gt; . Si el invocable devuelve &lt;code&gt;True&lt;/code&gt; , el procedimiento de ajuste se detiene. El monitor se puede utilizar para varias cosas, como calcular estimaciones retenidas, detenerse antes de tiempo, introspecci&amp;oacute;n del modelo e instant&amp;aacute;neas.</target>
        </trans-unit>
        <trans-unit id="52aa15c9df5da45110f63c8c26b8cfa477557d62" translate="yes" xml:space="preserve">
          <source>The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the &lt;strong&gt;regularization path&lt;/strong&gt; of the estimator.</source>
          <target state="translated">El par&amp;aacute;metro m&amp;aacute;s com&amp;uacute;n susceptible de esta estrategia es el par&amp;aacute;metro que codifica la fuerza del regularizador. En este caso decimos que calculamos la &lt;strong&gt;ruta&lt;/strong&gt; de &lt;strong&gt;regularizaci&amp;oacute;n&lt;/strong&gt; del estimador.</target>
        </trans-unit>
        <trans-unit id="3156312e704bdb91fdff12aa2e110d4f21e21302" translate="yes" xml:space="preserve">
          <source>The most intuitive way to do so is to use a bags of words representation:</source>
          <target state="translated">La forma más intuitiva de hacerlo es usar una representación de bolsas de palabras:</target>
        </trans-unit>
        <trans-unit id="399fde41b63b2ea676c5145583a3950879f6c9fa" translate="yes" xml:space="preserve">
          <source>The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.</source>
          <target state="translated">La motivación para usar este escalamiento incluye la robustez hasta desviaciones estándar muy pequeñas de las características y la preservación de entradas cero en datos escasos.</target>
        </trans-unit>
        <trans-unit id="3ec6281766fab2b8f9e9f9f6e92da7d4704e6316" translate="yes" xml:space="preserve">
          <source>The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable.</source>
          <target state="translated">El lazo multitarea permite ajustar múltiples problemas de regresión conjuntamente haciendo que las características seleccionadas sean las mismas en todas las tareas.Este ejemplo simula mediciones secuenciales,cada tarea es un instante de tiempo,y las características relevantes varían en amplitud a lo largo del tiempo mientras son las mismas.El lazo de tareas múltiples impone que los rasgos que se seleccionan en un punto de tiempo se seleccionen para todo el punto de tiempo.Esto hace que la selección de características por el lazo sea más estable.</target>
        </trans-unit>
        <trans-unit id="7b55b776834f44186e095c0df9ee2b704ce3630c" translate="yes" xml:space="preserve">
          <source>The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature:</source>
          <target state="translated">La definición de multiclase aquí parece la extensión más razonable de la métrica utilizada en la clasificación binaria,aunque no hay un consenso seguro en la literatura:</target>
        </trans-unit>
        <trans-unit id="5fdb408d7bc477f0762da630aa0f3aea39db3f13" translate="yes" xml:space="preserve">
          <source>The multiclass support is handled according to a one-vs-one scheme.</source>
          <target state="translated">El apoyo multiclase se maneja según un esquema de uno contra uno.</target>
        </trans-unit>
        <trans-unit id="66359e593d021aa5ae2d568f4acc056ec0bf4a32" translate="yes" xml:space="preserve">
          <source>The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.</source>
          <target state="translated">El clasificador multinomial Naive Bayes es adecuado para la clasificación con características discretas (por ejemplo,el recuento de palabras para la clasificación del texto).La distribución multinomial normalmente requiere un recuento de características enteras.Sin embargo,en la práctica,los recuentos fraccionarios como el tf-idf también pueden funcionar.</target>
        </trans-unit>
        <trans-unit id="9fcb660998dc7edb2d34f5dc5854df445bad9a92" translate="yes" xml:space="preserve">
          <source>The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:</source>
          <target state="translated">Las métricas múltiples pueden especificarse como una lista,una tupla o un conjunto de nombres de anotadores predefinidos:</target>
        </trans-unit>
        <trans-unit id="a09e250651300d47ed9cd74b128d0a12a5aa8e3e" translate="yes" xml:space="preserve">
          <source>The name of the sample image loaded</source>
          <target state="translated">El nombre de la imagen de muestra cargada</target>
        </trans-unit>
        <trans-unit id="b853190860f83bcb94f9f4edb130e6f04adf4ae5" translate="yes" xml:space="preserve">
          <source>The names &lt;code&gt;vect&lt;/code&gt;, &lt;code&gt;tfidf&lt;/code&gt; and &lt;code&gt;clf&lt;/code&gt; (classifier) are arbitrary. We will use them to perform grid search for suitable hyperparameters below. We can now train the model with a single command:</source>
          <target state="translated">Los nombres &lt;code&gt;vect&lt;/code&gt; , &lt;code&gt;tfidf&lt;/code&gt; y &lt;code&gt;clf&lt;/code&gt; (clasificador) son arbitrarios. Los usaremos para realizar una b&amp;uacute;squeda de cuadr&amp;iacute;cula de hiperpar&amp;aacute;metros adecuados a continuaci&amp;oacute;n. Ahora podemos entrenar el modelo con un solo comando:</target>
        </trans-unit>
        <trans-unit id="9146e93a1405c3d2cac65e0084d1b1c7a951b3b3" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns</source>
          <target state="translated">Los nombres de las columnas del conjunto de datos</target>
        </trans-unit>
        <trans-unit id="a0fb0e8bf0410fa55523d36fa2a7a49ac4415117" translate="yes" xml:space="preserve">
          <source>The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.</source>
          <target state="translated">El nuevo tipo será np.float32 o np.float64,dependiendo del tipo original.La función puede crear una copia o modificar el argumento dependiendo de la copia del argumento.</target>
        </trans-unit>
        <trans-unit id="6d7b951e7afa9271dd7834467cb67e175d7e626f" translate="yes" xml:space="preserve">
          <source>The new entry \(d(u,v)\) is computed as follows,</source>
          <target state="translated">La nueva entrada (d(u,v)\Nse calcula de la siguiente manera,</target>
        </trans-unit>
        <trans-unit id="c771668f5d4b4d7aa145f31455a67e2ba21af3ce" translate="yes" xml:space="preserve">
          <source>The next figure compares the results obtained for the different type of the weight concentration prior (parameter &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;) for different values of &lt;code&gt;weight_concentration_prior&lt;/code&gt;. Here, we can see the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is &amp;lsquo;dirichlet_distribution&amp;rsquo; while this is not necessarily the case for the &amp;lsquo;dirichlet_process&amp;rsquo; type (used by default).</source>
          <target state="translated">La siguiente figura compara los resultados obtenidos para los diferentes tipos de concentraci&amp;oacute;n de peso anterior (par&amp;aacute;metro &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; ) para diferentes valores de &lt;code&gt;weight_concentration_prior&lt;/code&gt; . Aqu&amp;iacute;, podemos ver que el valor del par&amp;aacute;metro &lt;code&gt;weight_concentration_prior&lt;/code&gt; tiene un fuerte impacto en el n&amp;uacute;mero efectivo de componentes activos obtenidos. Tambi&amp;eacute;n podemos notar que los valores grandes para el peso de concentraci&amp;oacute;n anterior conducen a pesos m&amp;aacute;s uniformes cuando el tipo de prior es 'dirichlet_distribution', mientras que este no es necesariamente el caso del tipo 'dirichlet_process' (usado por defecto).</target>
        </trans-unit>
        <trans-unit id="b6d38fd34e131451ba8b974153991556ff8b9398" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;code&gt;SVR&lt;/code&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;code&gt;SVR&lt;/code&gt; scales better. With regard to prediction time, &lt;code&gt;SVR&lt;/code&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;code&gt;SVR&lt;/code&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="translated">La siguiente figura compara el tiempo de ajuste y predicci&amp;oacute;n de &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; y &lt;code&gt;SVR&lt;/code&gt; para diferentes tama&amp;ntilde;os del conjunto de entrenamiento. El ajuste de &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; es m&amp;aacute;s r&amp;aacute;pido que el &lt;code&gt;SVR&lt;/code&gt; para conjuntos de entrenamiento de tama&amp;ntilde;o mediano (menos de 1000 muestras); sin embargo, para un entrenamiento m&amp;aacute;s grande, las escalas de &lt;code&gt;SVR&lt;/code&gt; mejores. Con respecto al tiempo de predicci&amp;oacute;n, &lt;code&gt;SVR&lt;/code&gt; es m&amp;aacute;s r&amp;aacute;pido que &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; para todos los tama&amp;ntilde;os del conjunto de entrenamiento debido a la soluci&amp;oacute;n escasa aprendida. Tenga en cuenta que el grado de escasez y, por tanto, el tiempo de predicci&amp;oacute;n depende de los par&amp;aacute;metros \ (\ epsilon \) y \ (C \) del &lt;code&gt;SVR&lt;/code&gt; ; \ (\ epsilon = 0 \) corresponder&amp;iacute;a a un modelo denso.</target>
        </trans-unit>
        <trans-unit id="7fb8a1fd588ff5df76899e15821355218c8c70df" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of KRR and SVR for different sizes of the training set. Fitting KRR is faster than SVR for medium- sized training sets (less than 1000 samples); however, for larger training sets SVR scales better. With regard to prediction time, SVR is faster than KRR for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters epsilon and C of the SVR.</source>
          <target state="translated">La siguiente figura compara el tiempo de ajuste y predicción de la KRR y SVR para diferentes tamaños del conjunto de entrenamiento.El ajuste de la KRR es más rápido que la SVR para los conjuntos de entrenamiento de tamaño medio (menos de 1000 muestras);sin embargo,para los conjuntos de entrenamiento más grandes la SVR escala mejor.En cuanto al tiempo de predicción,la RVS es más rápida que la RCE para todos los tamaños del conjunto de entrenamiento debido a la solución de dispersión aprendida.Obsérvese que el grado de dispersión y,por lo tanto,el tiempo de predicción depende de los parámetros épsilon y C de la SVR.</target>
        </trans-unit>
        <trans-unit id="b8b8c72913234ec998c925f50d9fa1a29ef7082f" translate="yes" xml:space="preserve">
          <source>The next image illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">La siguiente imagen ilustra cómo la calibración sigmoide cambia las probabilidades previstas para un problema de clasificación de 3 clases.Se ilustra el estándar 2-simplex,donde las tres esquinas corresponden a las tres clases.Las flechas apuntan desde los vectores de probabilidad predichos por un clasificador no calibrado hasta los vectores de probabilidad predichos por el mismo clasificador después de la calibración del sigmoide en un conjunto de validación de retención.Los colores indican la verdadera clase de una instancia (rojo:clase 1,verde:clase 2,azul:clase 3).</target>
        </trans-unit>
        <trans-unit id="d16a780143f0785e9248e298dec17c9fe8de9d1e" translate="yes" xml:space="preserve">
          <source>The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity.</source>
          <target state="translated">Los nodos son variables aleatorias cuyos estados dependen del estado de los otros nodos a los que están conectados.Por lo tanto,el modelo se parametriza por los pesos de las conexiones,así como un término de intercepción (sesgo)para cada unidad visible y oculta,omitido de la imagen por simplicidad.</target>
        </trans-unit>
        <trans-unit id="55f688a90e745dd5020f6b5c567bbe8f6396fa6e" translate="yes" xml:space="preserve">
          <source>The noise level in the targets can be specified by passing it via the parameter &lt;code&gt;alpha&lt;/code&gt;, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below).</source>
          <target state="translated">El nivel de ruido en los objetivos se puede especificar pas&amp;aacute;ndolo a trav&amp;eacute;s del par&amp;aacute;metro &lt;code&gt;alpha&lt;/code&gt; , ya sea globalmente como escalar o por punto de datos. Tenga en cuenta que un nivel de ruido moderado tambi&amp;eacute;n puede ser &amp;uacute;til para tratar los problemas num&amp;eacute;ricos durante el ajuste, ya que se implementa de manera efectiva como regularizaci&amp;oacute;n de Tikhonov, es decir, agreg&amp;aacute;ndolo a la diagonal de la matriz del n&amp;uacute;cleo. Una alternativa a especificar el nivel de ruido expl&amp;iacute;citamente es incluir un componente WhiteKernel en el kernel, que puede estimar el nivel de ruido global a partir de los datos (ver ejemplo a continuaci&amp;oacute;n).</target>
        </trans-unit>
        <trans-unit id="99b58f648d173c7793dc0e913318a8835572f0c2" translate="yes" xml:space="preserve">
          <source>The non-fixed, log-transformed hyperparameters of the kernel</source>
          <target state="translated">Los hiperparámetros no fijos y transformados en logaritmo del núcleo</target>
        </trans-unit>
        <trans-unit id="8ea3cf8563db967f688cb46c0883a9d020905a15" translate="yes" xml:space="preserve">
          <source>The nonmetric algorithm adds a monotonic regression step before computing the stress.</source>
          <target state="translated">El algoritmo no métrico añade un paso de regresión monótona antes de calcular el estrés.</target>
        </trans-unit>
        <trans-unit id="8cecdfcc92ccd83125ecaa6c4beb7447e43f92cb" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).</source>
          <target state="translated">La norma a utilizar para normalizar cada muestra no nula (o cada característica no nula si el eje es 0).</target>
        </trans-unit>
        <trans-unit id="39e8a7c3ff72c23082d4ee16a84d74279c8584ba" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample.</source>
          <target state="translated">La norma a utilizar para normalizar cada muestra no nula.</target>
        </trans-unit>
        <trans-unit id="b4510db24d586496f5cd27e9cd8024ffe16a368e" translate="yes" xml:space="preserve">
          <source>The normalized mutual information is defined as</source>
          <target state="translated">La información mutua normalizada se define como</target>
        </trans-unit>
        <trans-unit id="ad22e0beb8a3a9f5574d757b3678d39d78057ba3" translate="yes" xml:space="preserve">
          <source>The normalizer instance can then be used on sample vectors as any transformer:</source>
          <target state="translated">La instancia normalizadora puede entonces utilizarse en vectores de muestra como cualquier transformador:</target>
        </trans-unit>
        <trans-unit id="82d79be22b3b3fa23f79d393f8a5b628a27ddb08" translate="yes" xml:space="preserve">
          <source>The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below).</source>
          <target state="translated">El número k de vecinos considerados,(alias parámetro n_vecinos)se elige típicamente 1)mayor que el número mínimo de objetos que un cúmulo tiene que contener,para que otros objetos puedan ser valores atípicos locales en relación con este cúmulo,y 2)menor que el número máximo de objetos cercanos que pueden ser potencialmente valores atípicos locales.En la práctica,esa información no suele estar disponible,y tomar n_vecinos=20 parece funcionar bien en general.Cuando la proporción de valores atípicos es alta (es decir,superior al 10 %,como en el ejemplo siguiente),n_vecinos debe ser mayor (n_vecinos=35 en el ejemplo siguiente).</target>
        </trans-unit>
        <trans-unit id="4a35a9d90f2abd9af864433913570f3dbe7f2765" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de CPU que se utilizar&amp;aacute;n para realizar el c&amp;aacute;lculo OVA (One Versus All, para problemas de varias clases). &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="36ae328d3607f79ec631b3fe327da7ed8cf8098b" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de CPU que se utilizar&amp;aacute;n para realizar el c&amp;aacute;lculo. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="720879c36f3df22b5ec2b112039efcd8b5ba8b1b" translate="yes" xml:space="preserve">
          <source>The number of EM iterations to perform.</source>
          <target state="translated">El número de iteraciones EM a realizar.</target>
        </trans-unit>
        <trans-unit id="40648db4f1524a67ed66976c14cc75bac6eed386" translate="yes" xml:space="preserve">
          <source>The number of atomic tasks to dispatch at once to each worker. When individual evaluations are very fast, dispatching calls to workers can be slower than sequential computation because of the overhead. Batching fast computations together can mitigate this. The &lt;code&gt;'auto'&lt;/code&gt; strategy keeps track of the time it takes for a batch to complete, and dynamically adjusts the batch size to keep the time on the order of half a second, using a heuristic. The initial batch size is 1. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; with &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; will dispatch batches of a single task at a time as the threading backend has very little overhead and using larger batch size has not proved to bring any gain in that case.</source>
          <target state="translated">El n&amp;uacute;mero de tareas at&amp;oacute;micas para enviar a la vez a cada trabajador. Cuando las evaluaciones individuales son muy r&amp;aacute;pidas, el env&amp;iacute;o de llamadas a los trabajadores puede ser m&amp;aacute;s lento que el c&amp;aacute;lculo secuencial debido a la sobrecarga. La agrupaci&amp;oacute;n de c&amp;aacute;lculos r&amp;aacute;pidos juntos puede mitigar esto. La estrategia &lt;code&gt;'auto'&lt;/code&gt; realiza un seguimiento del tiempo que tarda un lote en completarse y ajusta din&amp;aacute;micamente el tama&amp;ntilde;o del lote para mantener el tiempo en el orden de medio segundo, utilizando una heur&amp;iacute;stica. El tama&amp;ntilde;o del lote inicial es 1. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; con &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; enviar&amp;aacute; lotes de una sola tarea a la vez, ya que el backend de subprocesos tiene muy poca sobrecarga y el uso de un tama&amp;ntilde;o de lote m&amp;aacute;s grande no ha demostrado generar ninguna ganancia en eso. caso.</target>
        </trans-unit>
        <trans-unit id="3ae39cb6a942b138204dbb141781bfb4c75e3760" translate="yes" xml:space="preserve">
          <source>The number of base estimators in the ensemble.</source>
          <target state="translated">El número de estimadores de base en el conjunto.</target>
        </trans-unit>
        <trans-unit id="9fe1c6517ea4c36af295e91a2e2410361ff8432c" translate="yes" xml:space="preserve">
          <source>The number of batches (of tasks) to be pre-dispatched. Default is &amp;lsquo;2*n_jobs&amp;rsquo;. When batch_size=&amp;rdquo;auto&amp;rdquo; this is reasonable default and the workers should never starve.</source>
          <target state="translated">El n&amp;uacute;mero de lotes (de tareas) que se enviar&amp;aacute;n previamente. El valor predeterminado es '2 * n_jobs'. Cuando batch_size = &amp;rdquo;auto&amp;rdquo; es un valor predeterminado razonable y los trabajadores nunca deben morir de hambre.</target>
        </trans-unit>
        <trans-unit id="f9d0a981566d58378881b2cabe57053da9d34fb6" translate="yes" xml:space="preserve">
          <source>The number of biclusters to find.</source>
          <target state="translated">El número de bíceps que hay que encontrar.</target>
        </trans-unit>
        <trans-unit id="b9c52ec2125ed0e2339a6eae69fb246b4dc5348e" translate="yes" xml:space="preserve">
          <source>The number of biclusters.</source>
          <target state="translated">El número de bíceps.</target>
        </trans-unit>
        <trans-unit id="3424019fa75a2f96f62b30f8336ea4cebfa5c4fe" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. The intervals for the bins are determined by the minimum and maximum of the input data. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de contenedores a producir. Los intervalos para los contenedores est&amp;aacute;n determinados por el m&amp;iacute;nimo y el m&amp;aacute;ximo de los datos de entrada. Aumenta ValueError si &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="016567b86fa6f6a4b4c043763b4e841bd445fc32" translate="yes" xml:space="preserve">
          <source>The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.</source>
          <target state="translated">El número de etapas de impulso a realizar.El refuerzo gradual es bastante robusto para sobreajustar,por lo que un gran número suele dar lugar a un mejor rendimiento.</target>
        </trans-unit>
        <trans-unit id="d31e8a7065eae23aeb71500302a31ebb485560ec" translate="yes" xml:space="preserve">
          <source>The number of classes</source>
          <target state="translated">El número de clases</target>
        </trans-unit>
        <trans-unit id="77a0204799f583fccb414a4215d0dc841510a2f7" translate="yes" xml:space="preserve">
          <source>The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).</source>
          <target state="translated">El número de clases (para los problemas de salida única),o una lista que contenga el número de clases para cada salida (para los problemas de salida múltiple).</target>
        </trans-unit>
        <trans-unit id="221daa93ea83049b71eff667b2da7d0ce3fa89ae" translate="yes" xml:space="preserve">
          <source>The number of classes (or labels) of the classification problem.</source>
          <target state="translated">El número de clases (o etiquetas)del problema de clasificación.</target>
        </trans-unit>
        <trans-unit id="7b75bf8cc583f50195e96e51d244ce57f23360a0" translate="yes" xml:space="preserve">
          <source>The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).</source>
          <target state="translated">El número de clases (problema de salida única),o una lista que contenga el número de clases para cada salida (problema de salida múltiple).</target>
        </trans-unit>
        <trans-unit id="8b5fc0d622c5abb76da13c6a8f50c84a211eca46" translate="yes" xml:space="preserve">
          <source>The number of classes in the training data</source>
          <target state="translated">El número de clases en los datos de entrenamiento</target>
        </trans-unit>
        <trans-unit id="a096e01744ef01b9d139303dfd4a94a83569986b" translate="yes" xml:space="preserve">
          <source>The number of classes of the classification problem.</source>
          <target state="translated">El número de clases del problema de clasificación.</target>
        </trans-unit>
        <trans-unit id="21583bce2023d80fa6dedc801ccace76e8a2445f" translate="yes" xml:space="preserve">
          <source>The number of classes to return.</source>
          <target state="translated">El número de clases a devolver.</target>
        </trans-unit>
        <trans-unit id="7defdc95bb1ddc0580ac0be76fc251278b72fb84" translate="yes" xml:space="preserve">
          <source>The number of classes.</source>
          <target state="translated">El número de clases.</target>
        </trans-unit>
        <trans-unit id="062fe5ee9cf896a5c74f9ba057f89b8b6de5fa8a" translate="yes" xml:space="preserve">
          <source>The number of clusters per class.</source>
          <target state="translated">El número de grupos por clase.</target>
        </trans-unit>
        <trans-unit id="21f23a23e06d5c295fade98ab37ab55d16e621ca" translate="yes" xml:space="preserve">
          <source>The number of clusters to find.</source>
          <target state="translated">El número de grupos a encontrar.</target>
        </trans-unit>
        <trans-unit id="b4184f0399110f5f8e690a26513dd6f78511c0a5" translate="yes" xml:space="preserve">
          <source>The number of clusters to form as well as the number of centroids to generate.</source>
          <target state="translated">El número de cúmulos a formar así como el número de centroides a generar.</target>
        </trans-unit>
        <trans-unit id="35d84f4a157603ef94bd9baafa0d524311104205" translate="yes" xml:space="preserve">
          <source>The number of columns in the grid plot (default: 3).</source>
          <target state="translated">El número de columnas en el gráfico de la cuadrícula (por defecto:3).</target>
        </trans-unit>
        <trans-unit id="25441c707266953707d0c752071de32d0051d235" translate="yes" xml:space="preserve">
          <source>The number of connected components in the graph.</source>
          <target state="translated">El número de componentes conectados en el gráfico.</target>
        </trans-unit>
        <trans-unit id="5d17146f816382e55c9752b437d1e0a90ca8e256" translate="yes" xml:space="preserve">
          <source>The number of cross-validation splits (folds/iterations).</source>
          <target state="translated">El número de divisiones de validación cruzada (pliegues/iteraciones).</target>
        </trans-unit>
        <trans-unit id="aa32f92ae0454e6e9ee52207fe93d7eb6600c995" translate="yes" xml:space="preserve">
          <source>The number of degrees of freedom of each components in the model.</source>
          <target state="translated">El número de grados de libertad de cada componente del modelo.</target>
        </trans-unit>
        <trans-unit id="b2ef77453518fcb650d3ae2f90fcfda15611a36f" translate="yes" xml:space="preserve">
          <source>The number of duplicated features, drawn randomly from the informative and the redundant features.</source>
          <target state="translated">El número de características duplicadas,extraídas aleatoriamente de las características informativas y redundantes.</target>
        </trans-unit>
        <trans-unit id="f3756e321fd8c5762ae8ac31516f2ecb21110717" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de puntos igualmente espaciados en la &lt;code&gt;grid&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b29bfd2ea8e3e1682dd4a79d1706f446dfb38a05" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes.</source>
          <target state="translated">El número de puntos igualmente espaciados en los ejes.</target>
        </trans-unit>
        <trans-unit id="a2df871c8f4e92cb59ed15324ccda38fad42ea75" translate="yes" xml:space="preserve">
          <source>The number of estimators as selected by early stopping (if &lt;code&gt;n_iter_no_change&lt;/code&gt; is specified). Otherwise it is set to &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de estimadores seleccionados por la detenci&amp;oacute;n anticipada (si se especifica &lt;code&gt;n_iter_no_change&lt;/code&gt; ). De lo contrario, se establece en &lt;code&gt;n_estimators&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1c41aa32f1f0f6cf009b98065236eda5fafde67a" translate="yes" xml:space="preserve">
          <source>The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.</source>
          <target state="translated">El número de características (columnas)en las matrices de salida.Es probable que un pequeño número de características provoque colisiones de hash,pero un gran número provocará mayores dimensiones de coeficientes en los aprendices lineales.</target>
        </trans-unit>
        <trans-unit id="586a175a91db906a71d554d2394ac768c54a011d" translate="yes" xml:space="preserve">
          <source>The number of features for each sample.</source>
          <target state="translated">El número de características de cada muestra.</target>
        </trans-unit>
        <trans-unit id="7aa2f8ce84af9869f7a766d49383cc26f37d08c4" translate="yes" xml:space="preserve">
          <source>The number of features has to be &amp;gt;= 5.</source>
          <target state="translated">El n&amp;uacute;mero de funciones debe ser&amp;gt; = 5.</target>
        </trans-unit>
        <trans-unit id="129d21ac97bd672e4633231039dccb80b794a16c" translate="yes" xml:space="preserve">
          <source>The number of features to consider when looking for the best split:</source>
          <target state="translated">El número de características a considerar cuando se busca la mejor división:</target>
        </trans-unit>
        <trans-unit id="7bfa1c66757d06ac8d59fc4bf744ebce5057ba13" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator.</source>
          <target state="translated">El número de características a extraer de X para entrenar a cada estimador base.</target>
        </trans-unit>
        <trans-unit id="d51bb9401f1843316f34a353f5592cb098582ce3" translate="yes" xml:space="preserve">
          <source>The number of features to select. If &lt;code&gt;None&lt;/code&gt;, half of the features are selected.</source>
          <target state="translated">El n&amp;uacute;mero de funciones para seleccionar. Si es &lt;code&gt;None&lt;/code&gt; , se selecciona la mitad de las funciones.</target>
        </trans-unit>
        <trans-unit id="9a2e8b8c9f83e59315eadeb30286733009f73579" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred from the maximum column index occurring in any of the files.</source>
          <target state="translated">El número de características a utilizar.Si no hay ninguna,se deducirá del índice máximo de la columna que aparece en cualquiera de los archivos.</target>
        </trans-unit>
        <trans-unit id="98598b839c3ae52a8bef761a8c292e4971f87fa1" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. n_features is only required if &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed a non-default value.</source>
          <target state="translated">La cantidad de funciones que se utilizar&amp;aacute;n. Si es Ninguno, se deducir&amp;aacute;. Este argumento es &amp;uacute;til para cargar varios archivos que son subconjuntos de un conjunto de datos dividido m&amp;aacute;s grande: es posible que cada subconjunto no tenga ejemplos de todas las caracter&amp;iacute;sticas, por lo que la forma inferida puede variar de un segmento a otro. n_features solo es necesario si el &lt;code&gt;offset&lt;/code&gt; o la &lt;code&gt;length&lt;/code&gt; se pasan a un valor no predeterminado.</target>
        </trans-unit>
        <trans-unit id="deca4fdfd1ca37b7e04bbcf3985c1b98fb8a3a95" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">El n&amp;uacute;mero de funciones cuando se realiza el &lt;code&gt;fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="000ec70bd77b0bf4abd3e711fc085eb14c1166a9" translate="yes" xml:space="preserve">
          <source>The number of features.</source>
          <target state="translated">El número de características.</target>
        </trans-unit>
        <trans-unit id="9910eba7c229f9255e2c9649c38205ffc855802c" translate="yes" xml:space="preserve">
          <source>The number of features. Should be at least 5.</source>
          <target state="translated">El número de características.Debería ser al menos 5.</target>
        </trans-unit>
        <trans-unit id="6b858bf37b4f474c4599d32f8587786621c82cda" translate="yes" xml:space="preserve">
          <source>The number of informative features, i.e., the number of features used to build the linear model used to generate the output.</source>
          <target state="translated">El número de características informativas,es decir,el número de características utilizadas para construir el modelo lineal utilizado para generar la salida.</target>
        </trans-unit>
        <trans-unit id="1a77292033054c608772a804dab5a2e1fdf27c5c" translate="yes" xml:space="preserve">
          <source>The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension &lt;code&gt;n_informative&lt;/code&gt;. For each cluster, informative features are drawn independently from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube.</source>
          <target state="translated">El n&amp;uacute;mero de caracter&amp;iacute;sticas informativas. Cada clase est&amp;aacute; compuesta por varios grupos gaussianos, cada uno ubicado alrededor de los v&amp;eacute;rtices de un hipercubo en un subespacio de dimensi&amp;oacute;n &lt;code&gt;n_informative&lt;/code&gt; . Para cada grupo, las caracter&amp;iacute;sticas informativas se extraen independientemente de N (0, 1) y luego se combinan linealmente al azar dentro de cada grupo para agregar covarianza. Luego, los grupos se colocan en los v&amp;eacute;rtices del hipercubo.</target>
        </trans-unit>
        <trans-unit id="8860e037fbe039fe78d5e3c48f8e1848feb12312" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The best results are kept.</source>
          <target state="translated">El número de inicializaciones a realizar.Los mejores resultados se mantienen.</target>
        </trans-unit>
        <trans-unit id="51a26d62cae82295bdfa00f11021a221252f4d93" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The result with the highest lower bound value on the likelihood is kept.</source>
          <target state="translated">El número de inicializaciones a realizar.Se mantiene el resultado con el valor límite inferior más alto de la probabilidad.</target>
        </trans-unit>
        <trans-unit id="38ff309a985e30ab2e29b9afc192f79c8b9a0c6a" translate="yes" xml:space="preserve">
          <source>The number of integer to sample.</source>
          <target state="translated">El número de entero a muestrear.</target>
        </trans-unit>
        <trans-unit id="a52a9529854ceea9a883fc2df829925e52c70f47" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before this call to partial_fit. This is optional: if no number is passed, the memory of the object is used.</source>
          <target state="translated">El número de iteración en los lotes de datos que se ha realizado antes de esta llamada a partial_fit.Esto es opcional:si no se pasa ningún número,se utiliza la memoria del objeto.</target>
        </trans-unit>
        <trans-unit id="a3bc7b28196b3922c89415e845096f6cf78b8faf" translate="yes" xml:space="preserve">
          <source>The number of iterations corresponding to the best stress. Returned only if &lt;code&gt;return_n_iter&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de iteraciones correspondientes a la mejor tensi&amp;oacute;n. Se devuelve solo si &lt;code&gt;return_n_iter&lt;/code&gt; se establece en &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="09e334ba47a4a0e3959de08638739eb9eaaab635" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by lars_path to find the grid of alphas for each target.</source>
          <target state="translated">El número de iteraciones tomadas por lars_path para encontrar la cuadrícula de alfas para cada objetivo.</target>
        </trans-unit>
        <trans-unit id="b1098a1922ae37c291de7345b0094c64c3a02834" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha.</source>
          <target state="translated">El número de iteraciones tomadas por el optimizador de descenso de coordenadas para alcanzar la tolerancia especificada para cada alfa.</target>
        </trans-unit>
        <trans-unit id="cb53eef7959113120386cfa7066166d0b269478f" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when &lt;code&gt;return_n_iter&lt;/code&gt; is set to True).</source>
          <target state="translated">El n&amp;uacute;mero de iteraciones tomadas por el optimizador de descenso de coordenadas para alcanzar la tolerancia especificada para cada alfa. (Se devuelve cuando &lt;code&gt;return_n_iter&lt;/code&gt; se establece en True).</target>
        </trans-unit>
        <trans-unit id="33de6e55bb60c2bc5ac457a31b105deefc3f996b" translate="yes" xml:space="preserve">
          <source>The number of iterations the solver has ran.</source>
          <target state="translated">El número de iteraciones que ha realizado el solucionador.</target>
        </trans-unit>
        <trans-unit id="db80deec4964c14c90bbb2ba0d38dab8d92c99f4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se ejecutar&amp;aacute;n en paralelo para &lt;code&gt;fit&lt;/code&gt; . &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="828e81cc3e32985aa18f25c0aa8cb224a18c0ff7" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se ejecutar&amp;aacute;n en paralelo tanto para &lt;code&gt;fit&lt;/code&gt; como para &lt;code&gt;predict&lt;/code&gt; . &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="c43ba6eb14ab669b0479493b6caf6c135c124b5c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None`&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se ejecutar&amp;aacute;n en paralelo tanto para &lt;code&gt;fit&lt;/code&gt; como para &lt;code&gt;predict&lt;/code&gt; . &lt;code&gt;None`&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="1c706846846b90bc0cd14952c05d5f24ee8d014f" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n para el c&amp;aacute;lculo. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="6833984bce80d83c32632eedfcb38fdab54d1b8b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. If multiple initializations are used (&lt;code&gt;n_init&lt;/code&gt;), each run of the algorithm is computed in parallel.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n para el c&amp;aacute;lculo. Si se utilizan m&amp;uacute;ltiples inicializaciones ( &lt;code&gt;n_init&lt;/code&gt; ), cada ejecuci&amp;oacute;n del algoritmo se calcula en paralelo.</target>
        </trans-unit>
        <trans-unit id="6691b8acedaea4810fafdb230140a5bee73c0f13" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n para el c&amp;aacute;lculo. Hace cada variable de destino en y en paralelo. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="e3d27815195706836eb73a8c598c8129b01e46ca" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n para el c&amp;aacute;lculo. Esto solo proporcionar&amp;aacute; una aceleraci&amp;oacute;n para n_targets&amp;gt; 1 y suficientes problemas grandes. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="aea0fe9213de8cdb23ff0f2fe182f38ade8bf1a8" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.</source>
          <target state="translated">El número de trabajos a utilizar para el cómputo.Esto funciona descomponiendo la matriz por pares en n_trabajos incluso rebanadas y calculándolas en paralelo.</target>
        </trans-unit>
        <trans-unit id="355b7ccfb662137f73492d9f4ce45fbb16afbbbc" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.</source>
          <target state="translated">El número de trabajos a utilizar para el cómputo.Esto funciona computando cada uno de los n_init que se ejecutan en paralelo.</target>
        </trans-unit>
        <trans-unit id="a5a46de70d97dcbce1cd57e62c5d0b48ff30934b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n en el paso E. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="27ef8dc8b90e4adb5cf60c4b5861107cfc3fcd66" translate="yes" xml:space="preserve">
          <source>The number of leaves in the tree</source>
          <target state="translated">El número de hojas del árbol</target>
        </trans-unit>
        <trans-unit id="aa280d8fa86c4668dcae50e829dc6fe4b82c5c09" translate="yes" xml:space="preserve">
          <source>The number of longitudes (x) and latitudes (y) in the grid</source>
          <target state="translated">El número de longitudes (x)y latitudes (y)en la cuadrícula</target>
        </trans-unit>
        <trans-unit id="c1079f8f5554d74d12065ae30d0d0d5ad6da869f" translate="yes" xml:space="preserve">
          <source>The number of mixture components.</source>
          <target state="translated">El número de componentes de la mezcla.</target>
        </trans-unit>
        <trans-unit id="1b5111fc1060a675cec3a1f3743c5b7235e4d74d" translate="yes" xml:space="preserve">
          <source>The number of mixture components. Depending on the data and the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; the model can decide to not use all the components by setting some component &lt;code&gt;weights_&lt;/code&gt; to values very close to zero. The number of effective components is therefore smaller than n_components.</source>
          <target state="translated">El n&amp;uacute;mero de componentes de la mezcla. Dependiendo de los datos y del valor de &lt;code&gt;weight_concentration_prior&lt;/code&gt; , el modelo puede decidir no usar todos los componentes estableciendo algunos de los componentes &lt;code&gt;weights_&lt;/code&gt; en valores muy cercanos a cero. Por tanto, el n&amp;uacute;mero de componentes efectivos es menor que n_components.</target>
        </trans-unit>
        <trans-unit id="add0998b97eff8ce13b181824a749694c3eae657" translate="yes" xml:space="preserve">
          <source>The number of nearest neighbors to return</source>
          <target state="translated">El número de vecinos más cercanos a regresar</target>
        </trans-unit>
        <trans-unit id="15fc684195ef8537dde92583b88b14e2ce9227a5" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">El número de vecinos considerados (parámetro n_vecinos)se establece típicamente 1)mayor que el número mínimo de muestras que un cúmulo tiene que contener,de modo que otras muestras pueden ser valores atípicos locales en relación con este cúmulo,y 2)menor que el número máximo de muestras cercanas que pueden ser potencialmente valores atípicos locales.En la práctica,por lo general no se dispone de esa información,y la toma de n_vecinos=20 parece funcionar bien en general.</target>
        </trans-unit>
        <trans-unit id="acccc5b01db683b034e704a8a9a779e161564526" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered, (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">El número de vecinos considerados,(parámetro n_vecinos)se establece típicamente 1)mayor que el número mínimo de muestras que un cúmulo tiene que contener,de modo que otras muestras pueden ser valores atípicos locales en relación con este cúmulo,y 2)menor que el número máximo de muestras cercanas que pueden ser potencialmente valores atípicos locales.En la práctica,por lo general no se dispone de esa información,y la toma de n_vecinos=20 parece funcionar bien en general.</target>
        </trans-unit>
        <trans-unit id="877763fdbf37d108bc07acba2c3c7a43a6b1f5d0" translate="yes" xml:space="preserve">
          <source>The number of occurrences of each label in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de apariciones de cada etiqueta en &lt;code&gt;y_true&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d77e127c483a951faddd4898060a91624b32c7e2" translate="yes" xml:space="preserve">
          <source>The number of outlying points matters, but also how much they are outliers.</source>
          <target state="translated">El número de puntos periféricos es importante,pero también lo mucho que son periféricos.</target>
        </trans-unit>
        <trans-unit id="28c74611a0fbfe9d7c9bc15166aba9285108f840" translate="yes" xml:space="preserve">
          <source>The number of outputs when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">El n&amp;uacute;mero de salidas cuando se realiza el &lt;code&gt;fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="bb79176b795c17c39b28af054f09df09e008175b" translate="yes" xml:space="preserve">
          <source>The number of outputs.</source>
          <target state="translated">El número de salidas.</target>
        </trans-unit>
        <trans-unit id="b09418506b739dbda708239c423400be69d20b7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search.</source>
          <target state="translated">El número de trabajos paralelos para la búsqueda de vecinos.</target>
        </trans-unit>
        <trans-unit id="f8c9b6b47de0b026f28768b5a0afa5c012cbf5fc" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n para la b&amp;uacute;squeda de vecinos. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="7704c4671581e69d735cac67f4c8350e13fe7ef2" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Affects only &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; methods.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n para la b&amp;uacute;squeda de vecinos. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles. Afecta solo a los m&amp;eacute;todos &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;kneighbors_graph&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="361059c7cf2c1088b100ce86f03251d9b9b3606a" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n para la b&amp;uacute;squeda de vecinos. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles. No afecta el m&amp;eacute;todo de &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0d88911bb44ab8ec6ef4a44ece33bbbd3a1ce8ce" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n para la b&amp;uacute;squeda de vecinos. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles. No afecta el m&amp;eacute;todo de &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="92152098131975c56771bce4e909262b46d5eb7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="99143dd67a79337f4ad9e3dbea2cd1e515a938df" translate="yes" xml:space="preserve">
          <source>The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21.</source>
          <target state="translated">El número de pasadas sobre los datos de entrenamiento (alias épocas).Por defecto,ninguno.Depreciado,se eliminará en 0.21.</target>
        </trans-unit>
        <trans-unit id="ec5cdfb884714451da411c4ff8cb1db87b4e7636" translate="yes" xml:space="preserve">
          <source>The number of redundant features. These features are generated as random linear combinations of the informative features.</source>
          <target state="translated">El número de características redundantes.Estos rasgos se generan como combinaciones lineales aleatorias de los rasgos informativos.</target>
        </trans-unit>
        <trans-unit id="47b1c5caf88dbd07fabbf6968de5281c23c7d24f" translate="yes" xml:space="preserve">
          <source>The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar.</source>
          <target state="translated">El número de objetivos de regresión,es decir,la dimensión del vector de salida y asociado a una muestra.Por defecto,la salida es un escalar.</target>
        </trans-unit>
        <trans-unit id="338ec305a58ac58159822d262713a3e274e16037" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed.</source>
          <target state="translated">El n&amp;uacute;mero de reinicios del optimizador para encontrar los par&amp;aacute;metros del kernel que maximizan la probabilidad log-marginal. La primera ejecuci&amp;oacute;n del optimizador se realiza a partir de los par&amp;aacute;metros iniciales del kernel, los restantes (si los hay) de thetas se muestrean log-uniform aleatoriamente desde el espacio de valores theta permitidos. Si es mayor que 0, todos los l&amp;iacute;mites deben ser finitos. Tenga en cuenta que n_restarts_optimizer == 0 implica que se realiza una ejecuci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="a4b3a5a4d2704e96fc904a7b55b3f1c5b2c8d507" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed.</source>
          <target state="translated">El n&amp;uacute;mero de reinicios del optimizador para encontrar los par&amp;aacute;metros del kernel que maximizan la probabilidad log-marginal. La primera ejecuci&amp;oacute;n del optimizador se realiza a partir de los par&amp;aacute;metros iniciales del kernel, los restantes (si los hay) de thetas se muestrean log-uniform aleatoriamente desde el espacio de valores theta permitidos. Si es mayor que 0, todos los l&amp;iacute;mites deben ser finitos. Tenga en cuenta que n_restarts_optimizer = 0 implica que se realiza una ejecuci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="8f64e7c256c29c0afec3f69dafbe77018e516c64" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters in the checkerboard structure.</source>
          <target state="translated">El número de grupos de filas y columnas en la estructura del tablero.</target>
        </trans-unit>
        <trans-unit id="1d47b9ac186fe69411b80e5cb5c0bc35de2b1552" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters.</source>
          <target state="translated">El número de grupos de filas y columnas.</target>
        </trans-unit>
        <trans-unit id="a93f4e954bb39f85a0cd6723eb5913eb20116e8e" translate="yes" xml:space="preserve">
          <source>The number of sample points on the S curve.</source>
          <target state="translated">El número de puntos de muestra en la curva S.</target>
        </trans-unit>
        <trans-unit id="e771006aa290a36177ef8a2fd15f85ed45a9f7ce" translate="yes" xml:space="preserve">
          <source>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.</source>
          <target state="translated">El número de muestras (o el peso total)en un barrio para que un punto sea considerado como un punto central.Esto incluye el punto en sí mismo.</target>
        </trans-unit>
        <trans-unit id="d8023265c3023688c589a292c9dfac5ffedb5a44" translate="yes" xml:space="preserve">
          <source>The number of samples drawn from the Gaussian process</source>
          <target state="translated">El número de muestras extraídas del proceso Gaussiano</target>
        </trans-unit>
        <trans-unit id="1589955effed900fd5766b276491de7c2d2b9bf4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator for each feature. If there are not missing samples, the &lt;code&gt;n_samples_seen&lt;/code&gt; will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">El n&amp;uacute;mero de muestras procesadas por el estimador para cada caracter&amp;iacute;stica. Si no faltan muestras, &lt;code&gt;n_samples_seen&lt;/code&gt; ser&amp;aacute; un n&amp;uacute;mero entero, de lo contrario ser&amp;aacute; una matriz. Se restablecer&amp;aacute; en nuevas llamadas para adaptarse, pero se incrementar&amp;aacute; en las llamadas de ajuste &lt;code&gt;partial_fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5282c1e1c469ae21abb6fc766981198bf00b68c4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">El n&amp;uacute;mero de muestras procesadas por el estimador. Se restablecer&amp;aacute; en nuevas llamadas para adaptarse, pero se incrementar&amp;aacute; en las llamadas de ajuste &lt;code&gt;partial_fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1793fd9997ff1f0552981d710a775a55fe6d48a0" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator.</source>
          <target state="translated">El número de muestras a extraer de X para entrenar a cada estimador de base.</target>
        </trans-unit>
        <trans-unit id="bb3a932b7568c921848c0d1cfe6540980845aa8f" translate="yes" xml:space="preserve">
          <source>The number of samples to take in each batch.</source>
          <target state="translated">El número de muestras a tomar en cada lote.</target>
        </trans-unit>
        <trans-unit id="45c4c8ec08a2e1d782bf77a47114ec6ebd4dca5e" translate="yes" xml:space="preserve">
          <source>The number of samples to use for each batch. Only used when calling &lt;code&gt;fit&lt;/code&gt;. If &lt;code&gt;batch_size&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, then &lt;code&gt;batch_size&lt;/code&gt; is inferred from the data and set to &lt;code&gt;5 * n_features&lt;/code&gt;, to provide a balance between approximation accuracy and memory consumption.</source>
          <target state="translated">El n&amp;uacute;mero de muestras que se utilizar&amp;aacute;n para cada lote. Solo se usa cuando se llama a &lt;code&gt;fit&lt;/code&gt; . Si &lt;code&gt;batch_size&lt;/code&gt; es &lt;code&gt;None&lt;/code&gt; , entonces &lt;code&gt;batch_size&lt;/code&gt; se infiere de los datos y se establece en &lt;code&gt;5 * n_features&lt;/code&gt; , para proporcionar un equilibrio entre la precisi&amp;oacute;n de aproximaci&amp;oacute;n y el consumo de memoria.</target>
        </trans-unit>
        <trans-unit id="cc9506bec678834c1e3b7d03354e1ff3a3f08ddc" translate="yes" xml:space="preserve">
          <source>The number of samples to use. If not given, all samples are used.</source>
          <target state="translated">El número de muestras a utilizar.Si no se da,se utilizan todas las muestras.</target>
        </trans-unit>
        <trans-unit id="06d71b7513bf6cfbd6babc125146f20e54cecd08" translate="yes" xml:space="preserve">
          <source>The number of samples.</source>
          <target state="translated">El número de muestras.</target>
        </trans-unit>
        <trans-unit id="8ed3bb3b4a6145be5f5af9755587163fa8bebbaa" translate="yes" xml:space="preserve">
          <source>The number of seconds contained in delta</source>
          <target state="translated">El número de segundos contenidos en el delta</target>
        </trans-unit>
        <trans-unit id="fd2b02981da97aea3b937c6f113a2aa5413af4a0" translate="yes" xml:space="preserve">
          <source>The number of selected features with cross-validation.</source>
          <target state="translated">El número de características seleccionadas con validación cruzada.</target>
        </trans-unit>
        <trans-unit id="f83194da71427b41aa36e9d801ef17814b93c795" translate="yes" xml:space="preserve">
          <source>The number of selected features.</source>
          <target state="translated">El número de características seleccionadas.</target>
        </trans-unit>
        <trans-unit id="ecf3b2d99c2ce29bc7f1b5f51d42517d75e68e85" translate="yes" xml:space="preserve">
          <source>The number of stages of the final model is available at the attribute &lt;code&gt;n_estimators_&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de etapas del modelo final est&amp;aacute; disponible en el atributo &lt;code&gt;n_estimators_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="40db2965d8f58e28ab3da3567d512db3201d5fa4" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed.</source>
          <target state="translated">El número de veces que se refina la red.No se usa si se pasan valores explícitos de alfas.</target>
        </trans-unit>
        <trans-unit id="9b5e4a652e0296cb387fe06bf82a965799e670b1" translate="yes" xml:space="preserve">
          <source>The number of trees in the forest.</source>
          <target state="translated">El número de árboles en el bosque.</target>
        </trans-unit>
        <trans-unit id="b609a37dc2d7220a5b1e0ac4ad20a19f7617a90e" translate="yes" xml:space="preserve">
          <source>The number of weak learners (i.e. regression trees) is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;; &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;The size of each tree&lt;/a&gt; can be controlled either by setting the tree depth via &lt;code&gt;max_depth&lt;/code&gt; or by setting the number of leaf nodes via &lt;code&gt;max_leaf_nodes&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;shrinkage&lt;/a&gt; .</source>
          <target state="translated">El n&amp;uacute;mero de alumnos d&amp;eacute;biles (es decir, &amp;aacute;rboles de regresi&amp;oacute;n) se controla mediante el par&amp;aacute;metro &lt;code&gt;n_estimators&lt;/code&gt; ; &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;El tama&amp;ntilde;o de cada &amp;aacute;rbol&lt;/a&gt; se puede controlar configurando la profundidad del &amp;aacute;rbol a trav&amp;eacute;s de &lt;code&gt;max_depth&lt;/code&gt; o configurando el n&amp;uacute;mero de nodos de hojas a trav&amp;eacute;s de &lt;code&gt;max_leaf_nodes&lt;/code&gt; . El &lt;code&gt;learning_rate&lt;/code&gt; es un par&amp;aacute;metro hiper en el intervalo (0,0, 1,0] que los controles overfitting a trav&amp;eacute;s de &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;la contracci&amp;oacute;n&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="68e68bbcb31faf3cd13be5ec90f20eecf860f6d7" translate="yes" xml:space="preserve">
          <source>The number of weak learners is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the &lt;code&gt;base_estimator&lt;/code&gt; parameter. The main parameters to tune to obtain good results are &lt;code&gt;n_estimators&lt;/code&gt; and the complexity of the base estimators (e.g., its depth &lt;code&gt;max_depth&lt;/code&gt; or minimum required number of samples to consider a split &lt;code&gt;min_samples_split&lt;/code&gt;).</source>
          <target state="translated">El n&amp;uacute;mero de estudiantes d&amp;eacute;biles se controla mediante el par&amp;aacute;metro &lt;code&gt;n_estimators&lt;/code&gt; . El par&amp;aacute;metro &lt;code&gt;learning_rate&lt;/code&gt; controla la contribuci&amp;oacute;n de los estudiantes d&amp;eacute;biles en la combinaci&amp;oacute;n final. De forma predeterminada, los estudiantes d&amp;eacute;biles son tocones de decisi&amp;oacute;n. Se pueden especificar diferentes alumnos d&amp;eacute;biles mediante el par&amp;aacute;metro &lt;code&gt;base_estimator&lt;/code&gt; . Los principales par&amp;aacute;metros a ajustar para obtener buenos resultados son &lt;code&gt;n_estimators&lt;/code&gt; y la complejidad de los estimadores base (por ejemplo, su profundidad &lt;code&gt;max_depth&lt;/code&gt; o el n&amp;uacute;mero m&amp;iacute;nimo requerido de muestras para considerar una divisi&amp;oacute;n &lt;code&gt;min_samples_split&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="a6d1422bf72f5a61faa255a9a1207169e4a394c6" translate="yes" xml:space="preserve">
          <source>The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets.</source>
          <target state="translated">El objeto resuelve el mismo problema que el objeto LassoCV.Sin embargo,a diferencia del LassoCV,encuentra los valores alfa relevantes por sí mismo.En general,debido a esta propiedad,será más estable.Sin embargo,es más frágil a los conjuntos de datos fuertemente multicolineales.</target>
        </trans-unit>
        <trans-unit id="909e016dde1f323cbfe3daf2401dd2bee2829e0c" translate="yes" xml:space="preserve">
          <source>The object to use to fit the data.</source>
          <target state="translated">El objeto a utilizar para ajustar los datos.</target>
        </trans-unit>
        <trans-unit id="d5c8a5f63b0e142ed66ba0f0aa5318c460719fdf" translate="yes" xml:space="preserve">
          <source>The object&amp;rsquo;s &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; attributes store the best mean score and the parameters setting corresponding to that score:</source>
          <target state="translated">Los atributos &lt;code&gt;best_score_&lt;/code&gt; y &lt;code&gt;best_params_&lt;/code&gt; del objeto almacenan la mejor puntuaci&amp;oacute;n media y la configuraci&amp;oacute;n de los par&amp;aacute;metros correspondientes a esa puntuaci&amp;oacute;n:</target>
        </trans-unit>
        <trans-unit id="512dd720938772db73af76fb4221b6596459608c" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H.</source>
          <target state="translated">La función objetivo se minimiza con una minimización alterna de W y H.</target>
        </trans-unit>
        <trans-unit id="a139874cf1d9e2c13462d6567d8563d658f0907b" translate="yes" xml:space="preserve">
          <source>The objective function is:</source>
          <target state="translated">La función del objetivo es:</target>
        </trans-unit>
        <trans-unit id="bf56422fecab64934517c1fdfbcaed66ab27df08" translate="yes" xml:space="preserve">
          <source>The objective function to minimize is in this case</source>
          <target state="translated">La función objetiva a minimizar es en este caso</target>
        </trans-unit>
        <trans-unit id="e546a5e9110cb4077ff5ab03d80b93cb6429070c" translate="yes" xml:space="preserve">
          <source>The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.</source>
          <target state="translated">Se supone que las observaciones están causadas por una transformación lineal de factores latentes de menor dimensión y ruido gaussiano añadido.Sin pérdida de generalidad los factores se distribuyen según un Gaussiano con media cero y covarianza unitaria.El ruido también es de media cero y tiene una matriz de covarianza diagonal arbitraria.</target>
        </trans-unit>
        <trans-unit id="f8b5be5464139b25f15f97e4d9d483501d55af35" translate="yes" xml:space="preserve">
          <source>The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="translated">Las observaciones a agrupar.Hay que señalar que los datos se convertirán en orden C,lo que provocará una copia en memoria si los datos dados no son C-contiguos.</target>
        </trans-unit>
        <trans-unit id="5af240b8e218ff237309779b56f83d5e52d1af7b" translate="yes" xml:space="preserve">
          <source>The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.</source>
          <target state="translated">Las observaciones,las distancias de Mahalanobis de las que calculamos.Se supone que las observaciones se extraen de la misma distribución que los datos utilizados en el ajuste.</target>
        </trans-unit>
        <trans-unit id="eb71736c8c9acd5b1d75a4e7144f466f3b859a53" translate="yes" xml:space="preserve">
          <source>The obtained score is always strictly greater than 0 and the best value is 1.</source>
          <target state="translated">La puntuación obtenida es siempre estrictamente superior a 0 y el mejor valor es 1.</target>
        </trans-unit>
        <trans-unit id="b7dea734e0307eec19c3b5341e6f81839af6082a" translate="yes" xml:space="preserve">
          <source>The one-vs-the-rest meta-classifier also implements a &lt;code&gt;predict_proba&lt;/code&gt; method, so long as such a method is implemented by the base classifier. This method returns probabilities of class membership in both the single label and multilabel case. Note that in the multilabel case, probabilities are the marginal probability that a given sample falls in the given class. As such, in the multilabel case the sum of these probabilities over all possible labels for a given sample &lt;em&gt;will not&lt;/em&gt; sum to unity, as they do in the single label case.</source>
          <target state="translated">El &lt;code&gt;predict_proba&lt;/code&gt; uno contra el resto tambi&amp;eacute;n implementa un m&amp;eacute;todo predict_proba , siempre que dicho m&amp;eacute;todo sea implementado por el clasificador base. Este m&amp;eacute;todo devuelve probabilidades de pertenencia a una clase tanto en el caso de etiqueta &amp;uacute;nica como en el de m&amp;uacute;ltiples etiquetas. Tenga en cuenta que en el caso de varias etiquetas, las probabilidades son la probabilidad marginal de que una muestra dada caiga en la clase dada. Como tal, en el caso de m&amp;uacute;ltiples etiquetas, la suma de estas probabilidades sobre todas las etiquetas posibles para una muestra dada &lt;em&gt;no&lt;/em&gt; sumar&amp;aacute; la unidad, como ocurre en el caso de una sola etiqueta.</target>
        </trans-unit>
        <trans-unit id="f36ee9570dbac199195d25256879fa51a751bbaa" translate="yes" xml:space="preserve">
          <source>The opposite LOF of the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (&lt;code&gt;negative_outlier_factor_&lt;/code&gt; close to -1), while outliers tend to have a larger LOF score.</source>
          <target state="translated">El LOF opuesto de las muestras de entrenamiento. Cuanto m&amp;aacute;s alto, m&amp;aacute;s normal. Inliers tienden a tener una puntuaci&amp;oacute;n de LOF cerca de 1 ( &lt;code&gt;negative_outlier_factor_&lt;/code&gt; cerca de -1), mientras que los valores at&amp;iacute;picos tienden a tener una puntuaci&amp;oacute;n LOF m&amp;aacute;s grande.</target>
        </trans-unit>
        <trans-unit id="df3e454f1090a47509e70dbea510891595829b28" translate="yes" xml:space="preserve">
          <source>The opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal.</source>
          <target state="translated">Lo opuesto al Factor Local Atípico de cada muestra de entrada.Cuanto más bajo,más anormal.</target>
        </trans-unit>
        <trans-unit id="4312ae849a138f7db034f6fd7f5a1ea0b817b171" translate="yes" xml:space="preserve">
          <source>The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:</source>
          <target state="translated">El algoritmo óptimo para un determinado conjunto de datos es una elección complicada,y depende de una serie de factores:</target>
        </trans-unit>
        <trans-unit id="e994897b226c2495f8cea3f1e46f2c7029c61954" translate="yes" xml:space="preserve">
          <source>The optimal lambda parameter for minimizing skewness is estimated on each feature independently using maximum likelihood.</source>
          <target state="translated">El parámetro lambda óptimo para reducir al mínimo la asimetría se estima en cada característica de forma independiente utilizando la máxima probabilidad.</target>
        </trans-unit>
        <trans-unit id="033713ef73311880bab79fd88513749d67a56824" translate="yes" xml:space="preserve">
          <source>The optimization objective for Lasso is:</source>
          <target state="translated">El objetivo de optimización para Lasso es:</target>
        </trans-unit>
        <trans-unit id="64214421bf615c105d2891f743437d06253a6ade" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskElasticNet is:</source>
          <target state="translated">El objetivo de optimización de la MultiTaskElasticNet es:</target>
        </trans-unit>
        <trans-unit id="f43132c3f7097ed8020d37840c051e421e41e300" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskLasso is:</source>
          <target state="translated">El objetivo de optimización de la MultiTaskLasso es:</target>
        </trans-unit>
        <trans-unit id="3bee4ea6ed3cad366ecf4d67dfc05469de43ad46" translate="yes" xml:space="preserve">
          <source>The optimization objective for the case method=&amp;rsquo;lasso&amp;rsquo; is:</source>
          <target state="translated">El objetivo de optimizaci&amp;oacute;n para el m&amp;eacute;todo de caso = 'lazo' es:</target>
        </trans-unit>
        <trans-unit id="8d89fe15a9f2092d4f8a7ef569d775bf0279d26d" translate="yes" xml:space="preserve">
          <source>The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:</source>
          <target state="translated">El argumento extra opcional se añadirá al mensaje de depreciación y a la cadena de documentación.Nota:para usar esto con el valor por defecto de extra,pon un vacío de paréntesis:</target>
        </trans-unit>
        <trans-unit id="0c86ba504c90ec1412c0ccc3d7f0b2f074294dc1" translate="yes" xml:space="preserve">
          <source>The optional parameter &lt;code&gt;whiten=True&lt;/code&gt; makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.</source>
          <target state="translated">El par&amp;aacute;metro opcional &lt;code&gt;whiten=True&lt;/code&gt; hace posible proyectar los datos en el espacio singular mientras escala cada componente a la varianza de la unidad. Esto suele ser &amp;uacute;til si los modelos posteriores hacen suposiciones s&amp;oacute;lidas sobre la isotrop&amp;iacute;a de la se&amp;ntilde;al: este es, por ejemplo, el caso de las m&amp;aacute;quinas de vectores de soporte con el kernel RBF y el algoritmo de agrupaci&amp;oacute;n de K-Means.</target>
        </trans-unit>
        <trans-unit id="846ba284e005e0c78a1fd443a812172705bbc8f3" translate="yes" xml:space="preserve">
          <source>The order of labels in the classifier chain.</source>
          <target state="translated">El orden de las etiquetas en la cadena de clasificación.</target>
        </trans-unit>
        <trans-unit id="7b082acdda498538bd71b7e32b0b230c2144c284" translate="yes" xml:space="preserve">
          <source>The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.:</source>
          <target state="translated">El orden de la cadena puede establecerse explícitamente proporcionando una lista de números enteros.Por ejemplo,para una cadena de longitud 5.:</target>
        </trans-unit>
        <trans-unit id="f9d11a930ecaf4c38c05b12592342da86a8c77ff" translate="yes" xml:space="preserve">
          <source>The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the &lt;code&gt;transformers&lt;/code&gt; list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the &lt;code&gt;passthrough&lt;/code&gt; keyword. Those columns specified with &lt;code&gt;passthrough&lt;/code&gt; are added at the right to the output of the transformers.</source>
          <target state="translated">El orden de las columnas en la matriz de caracter&amp;iacute;sticas transformadas sigue el orden de c&amp;oacute;mo se especifican las columnas en la lista de &lt;code&gt;transformers&lt;/code&gt; . Las columnas de la matriz de caracter&amp;iacute;sticas original que no se especifican se eliminan de la matriz de caracter&amp;iacute;sticas transformada resultante, a menos que se especifiquen en la palabra clave de &lt;code&gt;passthrough&lt;/code&gt; . Las columnas especificadas con &lt;code&gt;passthrough&lt;/code&gt; se agregan a la derecha de la salida de los transformadores.</target>
        </trans-unit>
        <trans-unit id="0b5c0a29228cf2f99af9ecdff2f5b2a0dc08ed2d" translate="yes" xml:space="preserve">
          <source>The original data</source>
          <target state="translated">Los datos originales</target>
        </trans-unit>
        <trans-unit id="77422b337aacebf704cab947c2765e7ef78426db" translate="yes" xml:space="preserve">
          <source>The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.</source>
          <target state="translated">El conjunto de datos original consistía en 92 x 112,mientras que la versión disponible aquí consiste en imágenes de 64x64.</target>
        </trans-unit>
        <trans-unit id="11407c8a67a7b9eefbd2b65794ca1ecbfa48a97d" translate="yes" xml:space="preserve">
          <source>The original formulation of the hashing trick by Weinberger et al. used two separate hash functions \(h\) and \(\xi\) to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits.</source>
          <target state="translated">La formulación original del truco del hash de Weinberger y otros,usaba dos funciones de hash separadas \ ~-h y x para determinar el índice de la columna y el signo de una característica,respectivamente.La presente implementación funciona bajo el supuesto de que el bit de signo de MurmurHash3 es independiente de sus otros bits.</target>
        </trans-unit>
        <trans-unit id="8eec938f1b6ddec4315f3fac08b8dc8e8e67d5d5" translate="yes" xml:space="preserve">
          <source>The original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 47.</source>
          <target state="translated">Las imágenes originales son de 250 x 250 píxeles,pero los argumentos de rebanar y redimensionar por defecto las reducen a 62 x 47.</target>
        </trans-unit>
        <trans-unit id="a555f40ab758aba55bcba7bb9ac36af67b065d6a" translate="yes" xml:space="preserve">
          <source>The other kernels</source>
          <target state="translated">Los otros núcleos</target>
        </trans-unit>
        <trans-unit id="5978e5bbc3d0aa56b4c3321d1d9ccbd8b149a1a9" translate="yes" xml:space="preserve">
          <source>The outer product of the row and column label vectors shows a representation of the checkerboard structure.</source>
          <target state="translated">El producto exterior de los vectores de las etiquetas de filas y columnas muestra una representación de la estructura del tablero de damas.</target>
        </trans-unit>
        <trans-unit id="4e7d9f946f101134a65fd7d38a82dce953516bd7" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">La salida &lt;code&gt;y&lt;/code&gt; se crea de acuerdo con la f&amp;oacute;rmula:</target>
        </trans-unit>
        <trans-unit id="7348f35a4e7d1a3450461b231ee28ef95517ba57" translate="yes" xml:space="preserve">
          <source>The output is generated by applying a (potentially biased) random linear regression model with &lt;code&gt;n_informative&lt;/code&gt; nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.</source>
          <target state="translated">La salida se genera aplicando un modelo de regresi&amp;oacute;n lineal aleatoria (potencialmente sesgado) con &lt;code&gt;n_informative&lt;/code&gt; regresores informativos distintos de cero a la entrada generada previamente y algo de ruido centrado en gauss con alguna escala ajustable.</target>
        </trans-unit>
        <trans-unit id="0c62eccd54e33fb989ef31175162303c11637d7c" translate="yes" xml:space="preserve">
          <source>The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If &lt;code&gt;flip_sign&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.</source>
          <target state="translated">La salida de una descomposici&amp;oacute;n de valor singular es &amp;uacute;nica hasta una permutaci&amp;oacute;n de los signos de los vectores singulares. Si &lt;code&gt;flip_sign&lt;/code&gt; se establece en &lt;code&gt;True&lt;/code&gt; , la ambig&amp;uuml;edad del signo se resuelve al hacer que las cargas m&amp;aacute;s grandes para cada componente en los vectores singulares de la izquierda sean positivas.</target>
        </trans-unit>
        <trans-unit id="ab5b2ab18fdef999b51de35c5e9c33e7d3ac7cc7" translate="yes" xml:space="preserve">
          <source>The output of the 3 models are combined in a 2D graph where nodes represents the stocks and edges the:</source>
          <target state="translated">La salida de los 3 modelos se combinan en un gráfico 2D donde los nodos representan las existencias y los bordes:</target>
        </trans-unit>
        <trans-unit id="c9b52f3c910ded5afcb915db265ed0583f446660" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to as the 1-of-K coding scheme.</source>
          <target state="translated">La salida de la transformación se conoce a veces como el esquema de codificación 1-de-K.</target>
        </trans-unit>
        <trans-unit id="f172299244e465e38a0859a0349f26df08f75701" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to by some authors as the 1-of-K coding scheme.</source>
          <target state="translated">Algunos autores se refieren a veces a la salida de la transformación como el esquema de codificación 1-de-K.</target>
        </trans-unit>
        <trans-unit id="58983c4b722f7fa5c156e992e47b9ca634d33156" translate="yes" xml:space="preserve">
          <source>The output values.</source>
          <target state="translated">Los valores de salida.</target>
        </trans-unit>
        <trans-unit id="f348d2a86dcf3bc20a82250a4d5068aa50c67aad" translate="yes" xml:space="preserve">
          <source>The overall complexity of Isomap is \(O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]\).</source>
          <target state="translated">La complejidad general del Isomap es \ ~ (O[D \log(k)N \log(N)]+O[N^2(k+\log(N))]+O[d N^2]\).</target>
        </trans-unit>
        <trans-unit id="9cec4ae56de9cf1cd9cf2f8a0ff2e2e24864f054" translate="yes" xml:space="preserve">
          <source>The overall complexity of MLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\).</source>
          <target state="translated">La complejidad general de MLLE es \N \N \N-O[D \N-N \N-N \N-N \N-O[D N k^3]+O[N (k-D)k^2]+O[d N^2]\N).</target>
        </trans-unit>
        <trans-unit id="7fa546000c6a79308906d0c040bf81047f6b149e" translate="yes" xml:space="preserve">
          <source>The overall complexity of spectral embedding is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">La complejidad general de la incrustación espectral es \ ~ (O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]\).</target>
        </trans-unit>
        <trans-unit id="808f28c633edaf7537ca8395b6bc52f0086ed496" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard HLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\).</source>
          <target state="translated">La complejidad general de la norma HLLE es \N(O[D \log(k)N \log(N)]+O[D N k^3]+O[N d^6]+O[d N^2]\N).</target>
        </trans-unit>
        <trans-unit id="63617858af3d41882021a1ab37e78e76cce39983" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">La complejidad general de la norma LLE es \ ~ (O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]\).</target>
        </trans-unit>
        <trans-unit id="b88c53c3806a592f7e00e19aef010a599cfaf4dc" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LTSA is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]\).</source>
          <target state="translated">La complejidad general del estándar LTSA es \ ~ (O[D \log(k)N \log(N)]+O[D N k^3]+O[k^2 d]+O[d N^2]\).</target>
        </trans-unit>
        <trans-unit id="fc12a97c8a559f9672542a072947ef2eae0adcac" translate="yes" xml:space="preserve">
          <source>The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:</source>
          <target state="translated">El valor p,que se aproxima a la probabilidad de que la puntuación se obtenga por casualidad.Se calcula como:</target>
        </trans-unit>
        <trans-unit id="7f9aaf22278cea2b541fbd8b88b09de54aad3e99" translate="yes" xml:space="preserve">
          <source>The parallel version of K-Means is broken on OS X when &lt;code&gt;numpy&lt;/code&gt; uses the &lt;code&gt;Accelerate&lt;/code&gt; Framework. This is expected behavior: &lt;code&gt;Accelerate&lt;/code&gt; can be called after a fork but you need to execv the subprocess with the Python binary (which multiprocessing does not do under posix).</source>
          <target state="translated">La versi&amp;oacute;n paralela de K-Means se rompe en OS X cuando &lt;code&gt;numpy&lt;/code&gt; usa &lt;code&gt;Accelerate&lt;/code&gt; Framework. Este es el comportamiento esperado: se puede llamar a &lt;code&gt;Accelerate&lt;/code&gt; despu&amp;eacute;s de una bifurcaci&amp;oacute;n, pero es necesario ejecutar el subproceso con el binario de Python (lo que no ocurre con el multiprocesamiento en posix).</target>
        </trans-unit>
        <trans-unit id="26cab4ba2ce5f7f1aa5cff1bedcda88264af63ea" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="translated">El par&amp;aacute;metro &lt;code&gt;learning_rate&lt;/code&gt; interact&amp;uacute;a fuertemente con el par&amp;aacute;metro &lt;code&gt;n_estimators&lt;/code&gt; , el n&amp;uacute;mero de estudiantes d&amp;eacute;biles que encajar. Los valores m&amp;aacute;s peque&amp;ntilde;os de &lt;code&gt;learning_rate&lt;/code&gt; requieren un mayor n&amp;uacute;mero de estudiantes d&amp;eacute;biles para mantener un error de entrenamiento constante. La evidencia emp&amp;iacute;rica sugiere que valores peque&amp;ntilde;os de tasa de &lt;code&gt;learning_rate&lt;/code&gt; favorecen un mejor error de prueba. &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; recomienda establecer la tasa de aprendizaje en una peque&amp;ntilde;a constante (por ejemplo, &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt; ) y elegir &lt;code&gt;n_estimators&lt;/code&gt; mediante una parada temprana. Para una discusi&amp;oacute;n m&amp;aacute;s detallada de la interacci&amp;oacute;n entre &lt;code&gt;learning_rate&lt;/code&gt; y &lt;code&gt;n_estimators&lt;/code&gt; , consulte &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0e6ab9c66156a9d2feae55ed2060a2c6f7d6986c" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;memory&lt;/code&gt; is needed in order to cache the transformers. &lt;code&gt;memory&lt;/code&gt; can be either a string containing the directory where to cache the transformers or a &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; object:</source>
          <target state="translated">La &lt;code&gt;memory&lt;/code&gt; par&amp;aacute;metros es necesaria para almacenar en cach&amp;eacute; los transformadores. &lt;code&gt;memory&lt;/code&gt; puede ser una cadena que contenga el directorio donde almacenar en cach&amp;eacute; los transformadores o un objeto &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="a19846a3c7376460662acabedd23579aba492f87" translate="yes" xml:space="preserve">
          <source>The parameter \(\nu\) is also called the &lt;strong&gt;learning rate&lt;/strong&gt; because it scales the step length the gradient descent procedure; it can be set via the &lt;code&gt;learning_rate&lt;/code&gt; parameter.</source>
          <target state="translated">El par&amp;aacute;metro \ (\ nu \) tambi&amp;eacute;n se denomina &lt;strong&gt;tasa de aprendizaje&lt;/strong&gt; porque escala la longitud del paso del procedimiento de descenso de gradiente; se puede configurar mediante el par&amp;aacute;metro &lt;code&gt;learning_rate&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="76b14eb14f0e0be16176b9f0182e58f523e33b2d" translate="yes" xml:space="preserve">
          <source>The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers.</source>
          <target state="translated">El parámetro épsilon controla el número de muestras que deben clasificarse como valores atípicos.Cuanto más pequeña es la épsilon,más robusta es para los valores atípicos.</target>
        </trans-unit>
        <trans-unit id="33a23a95b6f6e50b11cce3982dee22dc3afbeaef" translate="yes" xml:space="preserve">
          <source>The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.</source>
          <target state="translated">La cuadrícula de parámetros a explorar,como un diccionario que mapea los parámetros del estimador a secuencias de valores permitidos.</target>
        </trans-unit>
        <trans-unit id="95ad4f92539995843fbdd8a8c8263fdd9652cae5" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. More specifically, the optimization objective is:</source>
          <target state="translated">El parámetro l1_ratio corresponde a alpha en el paquete glmnet R,mientras que alpha corresponde al parámetro lambda en glmnet.Más específicamente,el objetivo de optimización es:</target>
        </trans-unit>
        <trans-unit id="444f64a3bffea2bee3d3973fdc03c3dbbbbf54d9" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio &amp;lt;= 0.01 is not reliable, unless you supply your own sequence of alpha.</source>
          <target state="translated">El par&amp;aacute;metro l1_ratio corresponde a alpha en el paquete glmnet R mientras que alpha corresponde al par&amp;aacute;metro lambda en glmnet. Espec&amp;iacute;ficamente, l1_ratio = 1 es la penalizaci&amp;oacute;n del lazo. Actualmente, l1_ratio &amp;lt;= 0.01 no es confiable, a menos que proporcione su propia secuencia de alfa.</target>
        </trans-unit>
        <trans-unit id="3d75707f4ee017e7f35982b539c7d0e6825a3d30" translate="yes" xml:space="preserve">
          <source>The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized.</source>
          <target state="translated">El parámetro que controla la suavidad de la función aprendida.Cuanto más pequeño es nu,menos suave es la función aproximada.Para nu=inf,el núcleo se convierte en equivalente al núcleo RBF y para nu=0,5 al núcleo exponencial absoluto.Los valores intermedios importantes son nu=1,5 (funciones una vez diferenciables)y nu=2,5 (funciones dos veces diferenciables).Obsérvese que los valores de nu que no están en [0,5,1,5,2,5,inf]incurren en un coste computacional considerablemente mayor (aprox.10 veces más alto)ya que requieren evaluar la función de Bessel modificada.Además,a diferencia de l,nu se mantiene fijo en su valor inicial y no se optimiza.</target>
        </trans-unit>
        <trans-unit id="fa3c09390eafe53f0b58881f0d7db646d5796fe2" translate="yes" xml:space="preserve">
          <source>The parameters \(\sigma_y\) and \(\mu_y\) are estimated using maximum likelihood.</source>
          <target state="translated">Los parámetros \N -sigma_y\N-y \N -más_menos_y\N-se estiman usando la máxima probabilidad.</target>
        </trans-unit>
        <trans-unit id="d8334dfb20de589f58500a915aef89a4fab7377d" translate="yes" xml:space="preserve">
          <source>The parameters \(\theta_y\) is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:</source>
          <target state="translated">Los parámetros se estiman mediante una versión suavizada de la máxima probabilidad,es decir,el recuento de la frecuencia relativa:</target>
        </trans-unit>
        <trans-unit id="162d2ed9f70c881f87134a87c0c741cb23832c22" translate="yes" xml:space="preserve">
          <source>The parameters implementation of the &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">La implementaci&amp;oacute;n de par&amp;aacute;metros de la clase &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; &lt;/a&gt; propone dos tipos de prior para la distribuci&amp;oacute;n de pesos: un modelo de mezcla finita con distribuci&amp;oacute;n de Dirichlet y un modelo de mezcla infinita con el Proceso de Dirichlet. En la pr&amp;aacute;ctica, el algoritmo de inferencia del proceso de Dirichlet es aproximado y utiliza una distribuci&amp;oacute;n truncada con un n&amp;uacute;mero m&amp;aacute;ximo fijo de componentes (denominada representaci&amp;oacute;n de ruptura de palos). El n&amp;uacute;mero de componentes realmente utilizados casi siempre depende de los datos.</target>
        </trans-unit>
        <trans-unit id="957eda9a79d6f1f87ea7b634983b57c94627c96b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.</source>
          <target state="translated">Los parámetros del estimador utilizado para aplicar estos métodos se optimizan mediante una búsqueda cruzada de cuadrículas validadas sobre una cuadrícula de parámetros.</target>
        </trans-unit>
        <trans-unit id="6f20d8d61d83d6376fa9caf869f91f4640e0cc5b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.</source>
          <target state="translated">Los parámetros del estimador utilizado para aplicar estos métodos se optimizan mediante una búsqueda validada cruzada sobre los ajustes de los parámetros.</target>
        </trans-unit>
        <trans-unit id="1d4a34fa151b21edbbd9fa634476deabc1cb44cf" translate="yes" xml:space="preserve">
          <source>The parameters of the power transformation for the selected features.</source>
          <target state="translated">Los parámetros de la transformación de la energía para las características seleccionadas.</target>
        </trans-unit>
        <trans-unit id="62f294aa209942b08fddf4e74d29c13f78a9e67d" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter.</source>
          <target state="translated">Los parámetros seleccionados son los que maximizan la puntuación de los datos retenidos,según el parámetro de puntuación.</target>
        </trans-unit>
        <trans-unit id="b6d7555a36c15ae2ffe9751024a0eebcf2421d57" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead.</source>
          <target state="translated">Los parámetros seleccionados son los que maximizan el puntaje de los datos omitidos,a menos que se pase un puntaje explícito,en cuyo caso se utiliza en su lugar.</target>
        </trans-unit>
        <trans-unit id="34d594dbc00b197d06cf788b61a5dfe36db335bf" translate="yes" xml:space="preserve">
          <source>The parameters that have been evaluated.</source>
          <target state="translated">Los parámetros que han sido evaluados.</target>
        </trans-unit>
        <trans-unit id="f139c5090bf57fe74c58422c7266093265927a67" translate="yes" xml:space="preserve">
          <source>The parent of each node. Only returned when a connectivity matrix is specified, elsewhere &amp;lsquo;None&amp;rsquo; is returned.</source>
          <target state="translated">El padre de cada nodo. Solo se devuelve cuando se especifica una matriz de conectividad, en otros lugares se devuelve 'Ninguno'.</target>
        </trans-unit>
        <trans-unit id="89bfd729c83f5c14808628511ada19e6e2b220e5" translate="yes" xml:space="preserve">
          <source>The partial dependence function evaluated on the &lt;code&gt;grid&lt;/code&gt;. For regression and binary classification &lt;code&gt;n_classes==1&lt;/code&gt;.</source>
          <target state="translated">La funci&amp;oacute;n de dependencia parcial evaluada en la &lt;code&gt;grid&lt;/code&gt; . Para regresi&amp;oacute;n y clasificaci&amp;oacute;n binaria &lt;code&gt;n_classes==1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="053dbcb3edc2e230ef9a4eff0313b65bae4689ca" translate="yes" xml:space="preserve">
          <source>The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">Los algoritmos pasivo-agresivos son una familia de algoritmos para el aprendizaje a gran escala. Son similares al Perceptron en que no requieren una tasa de aprendizaje. Sin embargo, contrariamente a la Perceptron, incluyen un par&amp;aacute;metro de regularizaci&amp;oacute;n &lt;code&gt;C&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cf6ba8c5f2e782ae102fc993ff0f91af87853f26" translate="yes" xml:space="preserve">
          <source>The path of the base directory to use as a data store or None. If None is given, no caching is done and the Memory object is completely transparent. This option replaces cachedir since version 0.12.</source>
          <target state="translated">La ruta del directorio base para usar como almacén de datos o ninguno.Si se da None,no se hace ningún caching y el objeto de la memoria es completamente transparente.Esta opción reemplaza al cacheo desde la versión 0.12.</target>
        </trans-unit>
        <trans-unit id="e60c1638cc188f171e30720c51f9233ac8e0591d" translate="yes" xml:space="preserve">
          <source>The path to scikit-learn data dir.</source>
          <target state="translated">El camino a la dirección de datos de aprendizaje científico.</target>
        </trans-unit>
        <trans-unit id="780a6be1d44fa6c4bc32e0e6a573d3b4ad24b942" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to &amp;lsquo;l2&amp;rsquo; which is the standard regularizer for linear SVM models. &amp;lsquo;l1&amp;rsquo; and &amp;lsquo;elasticnet&amp;rsquo; might bring sparsity to the model (feature selection) not achievable with &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="translated">La penalizaci&amp;oacute;n (tambi&amp;eacute;n conocida como t&amp;eacute;rmino de regularizaci&amp;oacute;n) que se utilizar&amp;aacute;. El valor predeterminado es 'l2', que es el regularizador est&amp;aacute;ndar para los modelos SVM lineales. 'l1' y 'elasticnet' pueden traer escasez al modelo (selecci&amp;oacute;n de caracter&amp;iacute;sticas) que no se puede lograr con 'l2'.</target>
        </trans-unit>
        <trans-unit id="5ec62ab0e251a48390ce125b16975a3fa4c7ca91" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to None.</source>
          <target state="translated">La pena (también conocida como término de regularización)que se utilizará.Por defecto,ninguna.</target>
        </trans-unit>
        <trans-unit id="195880b4735384ca5f4a3196f2d3109be0fc5155" translate="yes" xml:space="preserve">
          <source>The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.</source>
          <target state="translated">El rendimiento es ligeramente peor para la búsqueda aleatoria,aunque lo más probable es que se trate de un efecto de ruido y no se trasladaría a un conjunto de pruebas de retención.</target>
        </trans-unit>
        <trans-unit id="66758ccad6c0faa66ee36e2739cca75d7cb80119" translate="yes" xml:space="preserve">
          <source>The performance measure reported by &lt;em&gt;k&lt;/em&gt;-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</source>
          <target state="translated">La medida de rendimiento informada por la validaci&amp;oacute;n cruzada de &lt;em&gt;k&lt;/em&gt; veces es el promedio de los valores calculados en el ciclo. Este enfoque puede ser costoso desde el punto de vista computacional, pero no desperdicia demasiados datos (como es el caso cuando se arregla un conjunto de validaci&amp;oacute;n arbitrario), lo cual es una gran ventaja en problemas como la inferencia inversa donde el n&amp;uacute;mero de muestras es muy peque&amp;ntilde;o.</target>
        </trans-unit>
        <trans-unit id="3b50299f6e3a476dff9e3b98d46cee16ed70fcdb" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="translated">Se compara el rendimiento de los algoritmos SAMME y SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; . SAMME.R usa las estimaciones de probabilidad para actualizar el modelo aditivo, mientras que SAMME usa solo las clasificaciones. Como ilustra el ejemplo, el algoritmo SAMME.R generalmente converge m&amp;aacute;s r&amp;aacute;pido que SAMME, logrando un error de prueba menor con menos iteraciones de impulso. El error de cada algoritmo en el conjunto de prueba despu&amp;eacute;s de cada iteraci&amp;oacute;n de refuerzo se muestra a la izquierda, el error de clasificaci&amp;oacute;n en el conjunto de prueba de cada &amp;aacute;rbol se muestra en el medio y el peso de refuerzo de cada &amp;aacute;rbol se muestra a la derecha. Todos los &amp;aacute;rboles tienen un peso de uno en el algoritmo SAMME.R y, por lo tanto, no se muestran.</target>
        </trans-unit>
        <trans-unit id="2f1cd52ca3b14d7125644c3724ffbf78c03b2ab3" translate="yes" xml:space="preserve">
          <source>The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step.</source>
          <target state="translated">El rendimiento de los hiperparámetros seleccionados y del modelo entrenado se mide entonces en un conjunto de evaluación dedicado que no se utilizó durante el paso de selección del modelo.</target>
        </trans-unit>
        <trans-unit id="dd106efb0f7012bff421a0ad8f3697f8283515ed" translate="yes" xml:space="preserve">
          <source>The periodicity of the kernel.</source>
          <target state="translated">La periodicidad del núcleo.</target>
        </trans-unit>
        <trans-unit id="5dce1e0dbd7277c2f73689bcac70f69df5d2fc02" translate="yes" xml:space="preserve">
          <source>The perplexity is defined as \(k=2^{(S)}\) where \(S\) is the Shannon entropy of the conditional probability distribution. The perplexity of a \(k\)-sided die is \(k\), so that \(k\) is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.</source>
          <target state="translated">La perplejidad se define como \(k=2^{(S)}\)donde \N(S)es la entropía Shannon de la distribución de probabilidad condicional.La perplejidad de un dado con un lado es de tal manera que es efectivamente el número de vecinos más cercanos que t-SNE considera al generar las probabilidades condicionales.Las perplejidades más grandes conducen a más vecinos cercanos y menos sensibles a la estructura pequeña.Por el contrario,una perplejidad menor considera un número menor de vecinos,y por lo tanto ignora más información global a favor del vecindario local.A medida que los tamaños de los conjuntos de datos sean mayores,se requerirán más puntos para obtener una muestra razonable del vecindario local y,por lo tanto,es posible que se requieran perplejidades mayores.De manera similar,los conjuntos de datos más ruidosos requerirán mayores valores de perplejidad para abarcar suficientes vecinos locales para ver más allá del ruido de fondo.</target>
        </trans-unit>
        <trans-unit id="08299376158415917838ffdf1e208cdfe76446d4" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter.</source>
          <target state="translated">La perplejidad está relacionada con el número de vecinos más cercanos que se utiliza en otros múltiples algoritmos de aprendizaje.Los conjuntos de datos más grandes generalmente requieren una mayor perplejidad.Considere la posibilidad de seleccionar un valor entre 5 y 50.La elección no es extremadamente crítica ya que el t-SNE es bastante insensible a este parámetro.</target>
        </trans-unit>
        <trans-unit id="3d1257b149fa9d88b1966c8dbdc55bf21ea6a0db" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed.</source>
          <target state="translated">El marcador de posici&amp;oacute;n de los valores faltantes. Se &lt;code&gt;missing_values&lt;/code&gt; todas las apariciones de missing_values .</target>
        </trans-unit>
        <trans-unit id="9c95ae315d785709e445ae217c5567fb1ce65f07" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For missing values encoded as np.nan, use the string value &amp;ldquo;NaN&amp;rdquo;.</source>
          <target state="translated">El marcador de posici&amp;oacute;n de los valores faltantes. Se &lt;code&gt;missing_values&lt;/code&gt; todas las apariciones de missing_values . Para los valores faltantes codificados como np.nan, utilice el valor de cadena &quot;NaN&quot;.</target>
        </trans-unit>
        <trans-unit id="50d8f581357d70f0923b3a8bf25734f11fbbdc88" translate="yes" xml:space="preserve">
          <source>The plot represents the learning curve of the classifier: the evolution of classification accuracy over the course of the mini-batches. Accuracy is measured on the first 1000 samples, held out as a validation set.</source>
          <target state="translated">El gráfico representa la curva de aprendizaje del clasificador:la evolución de la precisión de la clasificación a lo largo de los mini-lotes.La precisión se mide en las primeras 1000 muestras,que se mantienen como un conjunto de validación.</target>
        </trans-unit>
        <trans-unit id="2108780ce5e305bd6eefeccab3f1f12e712db7ba" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.</source>
          <target state="translated">La trama muestra los límites de decisión para el análisis discriminante lineal y el análisis discriminante cuadrático.La fila inferior demuestra que el Análisis Discriminante Lineal sólo puede aprender límites lineales,mientras que el Análisis Discriminante Cuadrático puede aprender límites cuadráticos y por lo tanto es más flexible.</target>
        </trans-unit>
        <trans-unit id="17f022b1b904616a8feb650bfceadf60a7908c45" translate="yes" xml:space="preserve">
          <source>The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), avg. occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and avg. rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="translated">La gr&amp;aacute;fica muestra cuatro gr&amp;aacute;ficas de dependencia parcial unidireccionales y una bidireccional. Las variables objetivo para el PDP unidireccional son: ingreso medio ( &lt;code&gt;MedInc&lt;/code&gt; ), avg. ocupantes por hogar ( &lt;code&gt;AvgOccup&lt;/code&gt; ), edad promedio del hogar ( &lt;code&gt;HouseAge&lt;/code&gt; ) y promedio. habitaciones por hogar ( &lt;code&gt;AveRooms&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="d1f9d8f4302df476eee3089e3160330e3191c729" translate="yes" xml:space="preserve">
          <source>The plot shows the regions where the discretized encoding is constant.</source>
          <target state="translated">La trama muestra las regiones donde la codificación discretizada es constante.</target>
        </trans-unit>
        <trans-unit id="a1523290796b6a8ba4024eaf4b48817249c66e13" translate="yes" xml:space="preserve">
          <source>The plots below illustrate the effect the parameter &lt;code&gt;C&lt;/code&gt; has on the separation line. A large value of &lt;code&gt;C&lt;/code&gt; basically tells our model that we do not have that much faith in our data&amp;rsquo;s distribution, and will only consider points close to line of separation.</source>
          <target state="translated">Las gr&amp;aacute;ficas siguientes ilustran el efecto que tiene el par&amp;aacute;metro &lt;code&gt;C&lt;/code&gt; en la l&amp;iacute;nea de separaci&amp;oacute;n. Un gran valor de &lt;code&gt;C&lt;/code&gt; b&amp;aacute;sicamente le dice a nuestro modelo que no tenemos tanta fe en la distribuci&amp;oacute;n de nuestros datos y que solo consideraremos puntos cercanos a la l&amp;iacute;nea de separaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="faaabc25a5f9dd9badc9ef9f18d9770e507882de" translate="yes" xml:space="preserve">
          <source>The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.</source>
          <target state="translated">Los gráficos muestran en primer lugar lo que un algoritmo K-means produciría usando tres grupos.Luego se muestra cuál es el efecto de una mala inicialización en el proceso de clasificación:Al establecer n_init a sólo 1 (el valor por defecto es 10),se reduce la cantidad de veces que el algoritmo se ejecutará con diferentes semillas centroides.El siguiente gráfico muestra lo que el uso de ocho cúmulos podría entregar y finalmente la verdad del terreno.</target>
        </trans-unit>
        <trans-unit id="3ad47df403d87eb821e2edd090b9e74720bc0be8" translate="yes" xml:space="preserve">
          <source>The plots represent the distribution of the prediction latency as a boxplot.</source>
          <target state="translated">Las gráficas representan la distribución de la latencia de la predicción como una gráfica de caja.</target>
        </trans-unit>
        <trans-unit id="ee583f2ef106ae04159c6d135c18b8bf01049699" translate="yes" xml:space="preserve">
          <source>The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.</source>
          <target state="translated">Las tramas muestran puntos de entrenamiento en colores sólidos y puntos de prueba semitransparentes.La parte inferior derecha muestra la precisión de la clasificación en el conjunto de pruebas.</target>
        </trans-unit>
        <trans-unit id="a10b5c6300ecb650c0782e5a6bff1ffc576100bb" translate="yes" xml:space="preserve">
          <source>The point cloud spanned by the observations above is very flat in one direction: one of the three univariate features can almost be exactly computed using the other two. PCA finds the directions in which the data is not &lt;em&gt;flat&lt;/em&gt;</source>
          <target state="translated">La nube de puntos abarcada por las observaciones anteriores es muy plana en una direcci&amp;oacute;n: una de las tres caracter&amp;iacute;sticas univariadas se puede calcular casi exactamente con las otras dos. PCA encuentra las direcciones en las que los datos no son &lt;em&gt;planos&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="9874b880b8ee39ca50e864dab01006b5213a2351" translate="yes" xml:space="preserve">
          <source>The points.</source>
          <target state="translated">Los puntos.</target>
        </trans-unit>
        <trans-unit id="0cdbbfd6a3924811880d5b83f6e26dafaaff0147" translate="yes" xml:space="preserve">
          <source>The polynomial kernel is defined as:</source>
          <target state="translated">El núcleo polinómico se define como:</target>
        </trans-unit>
        <trans-unit id="8b34b53d18875cd269c1341b177c3bcbb9230141" translate="yes" xml:space="preserve">
          <source>The pooled values for each feature cluster.</source>
          <target state="translated">Los valores agrupados para cada grupo de características.</target>
        </trans-unit>
        <trans-unit id="6285f4c6cbbb9a8676e04ca09291d710e5d4992f" translate="yes" xml:space="preserve">
          <source>The possible options are &amp;lsquo;hinge&amp;rsquo;, &amp;lsquo;log&amp;rsquo;, &amp;lsquo;modified_huber&amp;rsquo;, &amp;lsquo;squared_hinge&amp;rsquo;, &amp;lsquo;perceptron&amp;rsquo;, or a regression loss: &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;.</source>
          <target state="translated">Las opciones posibles son 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron' o una p&amp;eacute;rdida de regresi&amp;oacute;n: 'squared_loss', 'huber', 'epsilon_insensitive' o 'squared_epsilon_insensitive'.</target>
        </trans-unit>
        <trans-unit id="5a756ae9d1d42224e6a27c29b135c093bd570bbe" translate="yes" xml:space="preserve">
          <source>The power of the Minkowski metric to be used to calculate distance between points.</source>
          <target state="translated">El poder de la métrica de Minkowski para ser usada para calcular la distancia entre puntos.</target>
        </trans-unit>
        <trans-unit id="581738f7dfe64a35233afe8207a1e82379bfb8cd" translate="yes" xml:space="preserve">
          <source>The power transform is useful as a transformation in modeling problems where homoscedasticity and normality are desired. Below are examples of Box-Cox and Yeo-Johnwon applied to six different probability distributions: Lognormal, Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.</source>
          <target state="translated">La transformación de potencia es útil como una transformación en los problemas de modelado donde se desea la homosexualidad y la normalidad.A continuación se presentan ejemplos de Box-Cox y Yeo-Johnwon aplicados a seis distribuciones de probabilidad diferentes:Lognormal,Chi-cuadrado,Weibull,Gaussiana,Uniforme y Bimodal.</target>
        </trans-unit>
        <trans-unit id="837e2e1ea652ea6696a91670a638bda69523a446" translate="yes" xml:space="preserve">
          <source>The power transform method. Available methods are:</source>
          <target state="translated">El método de transformación de energía.Los métodos disponibles son:</target>
        </trans-unit>
        <trans-unit id="1605256f2d1d73782f41770777e3757d50960cf1" translate="yes" xml:space="preserve">
          <source>The power transform method. Currently, &amp;lsquo;box-cox&amp;rsquo; (Box-Cox transform) is the only option available.</source>
          <target state="translated">El m&amp;eacute;todo de transformaci&amp;oacute;n de energ&amp;iacute;a. Actualmente, 'box-cox' (transformaci&amp;oacute;n de Box-Cox) es la &amp;uacute;nica opci&amp;oacute;n disponible.</target>
        </trans-unit>
        <trans-unit id="9e3ef4e072a07501cd2ec598f0a1011b6178f209" translate="yes" xml:space="preserve">
          <source>The precision is the ratio &lt;code&gt;tp / (tp + fp)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fp&lt;/code&gt; the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.</source>
          <target state="translated">La precisi&amp;oacute;n es la relaci&amp;oacute;n &lt;code&gt;tp / (tp + fp)&lt;/code&gt; donde &lt;code&gt;tp&lt;/code&gt; es el n&amp;uacute;mero de verdaderos positivos y &lt;code&gt;fp&lt;/code&gt; el n&amp;uacute;mero de falsos positivos. La precisi&amp;oacute;n es intuitivamente la capacidad del clasificador de no etiquetar como positiva una muestra que es negativa.</target>
        </trans-unit>
        <trans-unit id="5e318fd9419ebb0c7685cd2fcf271eb0864ec5bb" translate="yes" xml:space="preserve">
          <source>The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">Las matrices de precisi&amp;oacute;n para cada componente de la mezcla. Una matriz de precisi&amp;oacute;n es la inversa de una matriz de covarianza. Una matriz de covarianza es sim&amp;eacute;trica positiva definida, por lo que la mezcla de gaussiano puede parametrizarse de manera equivalente mediante las matrices de precisi&amp;oacute;n. El almacenamiento de las matrices de precisi&amp;oacute;n en lugar de las matrices de covarianza hace que sea m&amp;aacute;s eficiente calcular la probabilidad logar&amp;iacute;tmica de nuevas muestras en el momento de la prueba. La forma depende de &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="027e42693446de348f6d01928068eb7453ef8a97" translate="yes" xml:space="preserve">
          <source>The precision matrix associated to the current covariance object.</source>
          <target state="translated">La matriz de precisión asociada al objeto de covarianza actual.</target>
        </trans-unit>
        <trans-unit id="d78de957c617714f761992022534f1c5cbc37dc0" translate="yes" xml:space="preserve">
          <source>The precision of each components on the mean distribution (Gaussian).</source>
          <target state="translated">La precisión de cada componente en la distribución media (Gaussiana).</target>
        </trans-unit>
        <trans-unit id="208aaa08d6ec7d945a950063ea09933d0bd5ac66" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;.</source>
          <target state="translated">La precisi&amp;oacute;n antes de la distribuci&amp;oacute;n media (gaussiana). Controla la extensi&amp;oacute;n hasta donde se pueden colocar los medios. Los valores m&amp;aacute;s peque&amp;ntilde;os concentran las medias de cada grupo alrededor de &lt;code&gt;mean_prior&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b72296dd4015be0343e22e7dc45bdf0f7e18f582" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to 1.</source>
          <target state="translated">La precisi&amp;oacute;n antes de la distribuci&amp;oacute;n media (gaussiana). Controla la extensi&amp;oacute;n hasta donde se pueden colocar los medios. Los valores m&amp;aacute;s peque&amp;ntilde;os concentran las medias de cada grupo alrededor de &lt;code&gt;mean_prior&lt;/code&gt; . El valor del par&amp;aacute;metro debe ser mayor que 0. Si es Ninguno, se establece en 1.</target>
        </trans-unit>
        <trans-unit id="03d920e00078b8bd9b4faa0b1aa14a8c65b257bb" translate="yes" xml:space="preserve">
          <source>The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</source>
          <target state="translated">La curva de precisión-recordatorio muestra el equilibrio entre la precisión y el recordatorio para diferentes umbrales.Un área alta bajo la curva representa tanto una alta memoria como una alta precisión,donde la alta precisión se relaciona con una baja tasa de falsos positivos,y la alta memoria se relaciona con una baja tasa de falsos negativos.Las puntuaciones altas de ambas muestran que el clasificador está produciendo resultados precisos (alta precisión),así como la mayoría de los resultados positivos (alta recuperación).</target>
        </trans-unit>
        <trans-unit id="b03d17dd4f0a50b42b8760dd1442678e73495423" translate="yes" xml:space="preserve">
          <source>The predicted class C for each sample in X is returned.</source>
          <target state="translated">Se devuelve la clase C prevista para cada muestra en X.</target>
        </trans-unit>
        <trans-unit id="bfeb54e7bff0bb15dd098a7327cddb86a1801899" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the base estimators in the ensemble.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como el registro de la media de las probabilidades de clase pronosticadas de los estimadores de base en el conjunto.</target>
        </trans-unit>
        <trans-unit id="cd67555dc49cf41a5e5df91097299e3752772d2d" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como el tronco de la media de las probabilidades de clase pronosticadas de los árboles del bosque.</target>
        </trans-unit>
        <trans-unit id="eab6e37cbb9069ff9afad75097cea6fc5d34926b" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.</source>
          <target state="translated">Las probabilidades de registro de clase pronosticadas de una muestra de entrada se calculan como la media ponderada de las probabilidades de registro de clase pronosticadas de los clasificadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="47d729548b0a8e226d9e4c530c0297fbe0f5665f" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.</source>
          <target state="translated">La clase prevista de una muestra de entrada es un voto de los árboles del bosque,ponderado por sus estimaciones de probabilidad.Es decir,la clase pronosticada es la que tiene la estimación de probabilidad media más alta entre los árboles.</target>
        </trans-unit>
        <trans-unit id="228bf14c189aaaaad2dc0e65c7c1dff58773904b" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting.</source>
          <target state="translated">La clase predicha de una muestra de entrada se calcula como la clase con la probabilidad media predicha m&amp;aacute;s alta. Si los estimadores de base no implementan un m&amp;eacute;todo &lt;code&gt;predict_proba&lt;/code&gt; , entonces recurre a la votaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="8652ca51db9ef5a2b15410ff134f217292f6ef55" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.</source>
          <target state="translated">La clase prevista de una muestra de entrada se calcula como la predicción media ponderada de los clasificadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="46c6bfcd5571fcbc8ac1e94c5ec5ef7b9ca3bcfc" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como la media de las probabilidades de clase pronosticadas de los árboles del bosque.La probabilidad de clase de un solo árbol es la fracción de muestras de la misma clase en una hoja.</target>
        </trans-unit>
        <trans-unit id="90e3cd15b2277da446a86ed04d9b97d9385dbcbd" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the base estimators in the ensemble. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting and the predicted class probabilities of an input sample represents the proportion of estimators predicting each class.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como las probabilidades de clase pronosticadas medias de los estimadores base en el conjunto. Si los estimadores base no implementan un m&amp;eacute;todo &lt;code&gt;predict_proba&lt;/code&gt; , entonces se recurre a la votaci&amp;oacute;n y las probabilidades de clase predichas de una muestra de entrada representan la proporci&amp;oacute;n de estimadores que predicen cada clase.</target>
        </trans-unit>
        <trans-unit id="f43014d849d28fe556560f6e74f971c02171e223" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como la media ponderada de las probabilidades de clase pronosticadas de los clasificadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="232fb255d370f3424586a7b0c3f74d049bd6d607" translate="yes" xml:space="preserve">
          <source>The predicted class probability is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">La probabilidad de clase pronosticada es la fracción de muestras de la misma clase en una hoja.</target>
        </trans-unit>
        <trans-unit id="45d2cef0bd7682bfccc693d1957f2c12cdb9bae8" translate="yes" xml:space="preserve">
          <source>The predicted class.</source>
          <target state="translated">La clase prevista.</target>
        </trans-unit>
        <trans-unit id="d01b8d940e0e34c3c7942798bc90fe03e260970d" translate="yes" xml:space="preserve">
          <source>The predicted classes, or the predict values.</source>
          <target state="translated">Las clases predichas,o los valores predichos.</target>
        </trans-unit>
        <trans-unit id="b68f3b27cbad35418e1a35aab9bbe1837a195e37" translate="yes" xml:space="preserve">
          <source>The predicted classes.</source>
          <target state="translated">Las clases previstas.</target>
        </trans-unit>
        <trans-unit id="f9c849804a5f2e4c77a6bd9f1a72aeb60a174b11" translate="yes" xml:space="preserve">
          <source>The predicted log-probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;. Equivalent to log(predict_proba(X))</source>
          <target state="translated">El logaritmo de probabilidad predicho de la muestra para cada clase en el modelo, donde las clases se ordenan como en &lt;code&gt;self.classes_&lt;/code&gt; . Equivalente a log (predict_proba (X))</target>
        </trans-unit>
        <trans-unit id="b73f981e520b253c83fbe81831bba280a3db569a" translate="yes" xml:space="preserve">
          <source>The predicted probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;.</source>
          <target state="translated">La probabilidad predicha de la muestra para cada clase en el modelo, donde las clases se ordenan como en &lt;code&gt;self.classes_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a286cd68d524dde2fbaba23c990f981ec35b78f3" translate="yes" xml:space="preserve">
          <source>The predicted probas.</source>
          <target state="translated">Las probas predichas.</target>
        </trans-unit>
        <trans-unit id="53b10e885ebd4a72b834dc0bd6649d47d09469b6" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.</source>
          <target state="translated">El objetivo de regresión previsto de una muestra de entrada se calcula como la media de los objetivos de regresión previstos de los estimadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="d575d8b045eb46db33f9cbaec364a954b218830a" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.</source>
          <target state="translated">El objetivo de regresión previsto de una muestra de entrada se calcula como la media de los objetivos de regresión previstos de los árboles del bosque.</target>
        </trans-unit>
        <trans-unit id="21875214f30fbe829fb7e391b984beb01fcc0748" translate="yes" xml:space="preserve">
          <source>The predicted regression value of an input sample is computed as the weighted median prediction of the classifiers in the ensemble.</source>
          <target state="translated">El valor de regresión previsto de una muestra de entrada se calcula como la predicción de la mediana ponderada de los clasificadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="d728cdaebb39fe6a42651804a843b92d06b095fc" translate="yes" xml:space="preserve">
          <source>The predicted regression values.</source>
          <target state="translated">Los valores de regresión predichos.</target>
        </trans-unit>
        <trans-unit id="eb2e0fb384bae49f48977b71692091d27df9ee48" translate="yes" xml:space="preserve">
          <source>The predicted target values.</source>
          <target state="translated">Los valores previstos del objetivo.</target>
        </trans-unit>
        <trans-unit id="16c2ec05bdd825f5e946bffd1661391a4efc1866" translate="yes" xml:space="preserve">
          <source>The predicted value of the input samples.</source>
          <target state="translated">El valor predicho de las muestras de entrada.</target>
        </trans-unit>
        <trans-unit id="71f7a8826bad533ae16d312cbce730009c206751" translate="yes" xml:space="preserve">
          <source>The predicted values.</source>
          <target state="translated">Los valores predichos.</target>
        </trans-unit>
        <trans-unit id="d3f70f498146a996702d8957eebe13ff93b2e60c" translate="yes" xml:space="preserve">
          <source>The prediction interpolates the observations (at least for regular kernels).</source>
          <target state="translated">La predicción interpola las observaciones (al menos para los núcleos regulares).</target>
        </trans-unit>
        <trans-unit id="0a5a460e70a5f18f6e563f520727c4e907b64c8a" translate="yes" xml:space="preserve">
          <source>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.</source>
          <target state="translated">La predicción es probabilística (gaussiana),de modo que se pueden calcular intervalos de confianza empíricos y decidir sobre la base de éstos si se debe reajustar (ajuste en línea,ajuste adaptativo)la predicción en alguna región de interés.</target>
        </trans-unit>
        <trans-unit id="22369282bb204be005e939800f6b06d8c430453c" translate="yes" xml:space="preserve">
          <source>The previously introduced metrics are &lt;strong&gt;not normalized with regards to random labeling&lt;/strong&gt;: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular &lt;strong&gt;random labeling won&amp;rsquo;t yield zero scores especially when the number of clusters is large&lt;/strong&gt;.</source>
          <target state="translated">Las m&amp;eacute;tricas introducidas anteriormente &lt;strong&gt;no&lt;/strong&gt; est&amp;aacute;n &lt;strong&gt;normalizadas con respecto al etiquetado aleatorio&lt;/strong&gt; : esto significa que, dependiendo del n&amp;uacute;mero de muestras, grupos y clases de verdad del terreno, un etiquetado completamente aleatorio no siempre producir&amp;aacute; los mismos valores de homogeneidad, integridad y, por lo tanto, medida v. En particular, &lt;strong&gt;el etiquetado aleatorio no arrojar&amp;aacute; puntajes cero, especialmente cuando el n&amp;uacute;mero de grupos es grande&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="cd0e1bbfb34ee27f92c4a3d8c327812d7edee1f8" translate="yes" xml:space="preserve">
          <source>The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as &lt;em&gt;non-generalizing&lt;/em&gt; machine learning methods, since they simply &amp;ldquo;remember&amp;rdquo; all of its training data (possibly transformed into a fast indexing structure such as a &lt;a href=&quot;#ball-tree&quot;&gt;Ball Tree&lt;/a&gt; or &lt;a href=&quot;#kd-tree&quot;&gt;KD Tree&lt;/a&gt;).</source>
          <target state="translated">El principio detr&amp;aacute;s de los m&amp;eacute;todos de vecino m&amp;aacute;s cercano es encontrar un n&amp;uacute;mero predefinido de muestras de entrenamiento m&amp;aacute;s cercanas en distancia al nuevo punto y predecir la etiqueta a partir de ellas. El n&amp;uacute;mero de muestras puede ser una constante definida por el usuario (aprendizaje del vecino m&amp;aacute;s cercano k) o variar seg&amp;uacute;n la densidad local de puntos (aprendizaje del vecino basado en el radio). En general, la distancia puede ser cualquier medida m&amp;eacute;trica: la distancia euclidiana est&amp;aacute;ndar es la opci&amp;oacute;n m&amp;aacute;s com&amp;uacute;n. Los m&amp;eacute;todos basados ​​en vecinos se conocen como m&amp;eacute;todos de aprendizaje autom&amp;aacute;tico &lt;em&gt;no generalizantes&lt;/em&gt; , ya que simplemente &quot;recuerdan&quot; todos sus datos de entrenamiento (posiblemente transformados en una estructura de indexaci&amp;oacute;n r&amp;aacute;pida como un &lt;a href=&quot;#ball-tree&quot;&gt;&amp;aacute;rbol de bolas&lt;/a&gt; o un &amp;aacute;rbol &lt;a href=&quot;#kd-tree&quot;&gt;KD&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="947ce8b8522668844673470c4e6efaad3bb4bafb" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel are shown in the following figure:</source>
          <target state="translated">El anterior y posterior de un GP resultante de un kernel &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt; se muestran en la siguiente figura:</target>
        </trans-unit>
        <trans-unit id="43f31a8e2502a5518bdd46434b1800e86463052e" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:</source>
          <target state="translated">El anterior y posterior de un GP resultante de un núcleo ExpSineSquared se muestran en la siguiente figura:</target>
        </trans-unit>
        <trans-unit id="f92d48b9507eee6a0b35b438f1fb3d6ff89e93fd" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart).</source>
          <target state="translated">El anterior del número de grados de libertad en las distribuciones de covarianza (Wishart).</target>
        </trans-unit>
        <trans-unit id="f920ca641ad93ad7a821ae4139b67430b9eddb8f" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart). If it is None, it&amp;rsquo;s set to &lt;code&gt;n_features&lt;/code&gt;.</source>
          <target state="translated">El anterior del n&amp;uacute;mero de grados de libertad en las distribuciones de covarianza (Wishart). Si es None, se establece en &lt;code&gt;n_features&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cf6f1da4c11f5b6aa97c72a194e67d10417600f3" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). If it is None, the emiprical covariance prior is initialized using the covariance of X. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">El prior sobre la distribuci&amp;oacute;n de covarianza (Wishart). Si es None, la covarianza emiprica a priori se inicializa usando la covarianza de X. La forma depende de &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="449bf6ea1f50ae651db1aabfda8187878d697851" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">El prior sobre la distribuci&amp;oacute;n de covarianza (Wishart). La forma depende de &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="a7d0d50e2fe2007735b69660533329d841055128" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian).</source>
          <target state="translated">El anterior sobre la distribución media (Gaussiano).</target>
        </trans-unit>
        <trans-unit id="70a81456bc5946b8879a5b610e7810c1053668bd" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it&amp;rsquo;s set to the mean of X.</source>
          <target state="translated">El anterior sobre la distribuci&amp;oacute;n media (gaussiana). Si es Ninguno, se establece en la media de X.</target>
        </trans-unit>
        <trans-unit id="0c84abbb5dade5fc9d4b47758b34408cb97bc08f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian.</source>
          <target state="translated">Los priors sobre \ (\ alpha \) y \ (\ lambda \) se eligen para ser &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;distribuciones gamma&lt;/a&gt; , el conjugado previo para la precisi&amp;oacute;n del gaussiano.</target>
        </trans-unit>
        <trans-unit id="69c3bd38e0a5cd7a1c6a9d7a1b4382f7891ca245" translate="yes" xml:space="preserve">
          <source>The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.</source>
          <target state="translated">El modelo de probabilidad se crea utilizando la validación cruzada,de modo que los resultados pueden ser ligeramente diferentes de los obtenidos por la predicción.Además,producirá resultados sin sentido en conjuntos de datos muy pequeños.</target>
        </trans-unit>
        <trans-unit id="5a547056b7368b638d698d05d3f24b68fc3e86df" translate="yes" xml:space="preserve">
          <source>The probability of each class being drawn. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">La probabilidad de que se extraiga cada clase. Solo se devuelve si &lt;code&gt;return_distributions=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="91c6b31a68e2490c2f85bd0b221deab07bc16086" translate="yes" xml:space="preserve">
          <source>The probability of each feature being drawn given each class. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">La probabilidad de que se dibuje cada caracter&amp;iacute;stica dada cada clase. Solo se devuelve si &lt;code&gt;return_distributions=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e0b4e863c306f0bcfef6f345210b44e3fb52ffa9" translate="yes" xml:space="preserve">
          <source>The probability that a coefficient is zero (see notes). Larger values enforce more sparsity.</source>
          <target state="translated">La probabilidad de que un coeficiente sea cero (ver notas).Los valores más grandes obligan a una mayor dispersión.</target>
        </trans-unit>
        <trans-unit id="4678dc803cddccad6a08944794f2a1c194908b2a" translate="yes" xml:space="preserve">
          <source>The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.</source>
          <target state="translated">Se sabe que el problema de aprender un árbol de decisiones óptimas es NP-completo bajo varios aspectos de la optimización e incluso para conceptos simples.Por consiguiente,los prácticos algoritmos de aprendizaje de árboles de decisión se basan en algoritmos heurísticos como el algoritmo de la codicia,en el que se toman decisiones óptimas a nivel local en cada nodo.Esos algoritmos no pueden garantizar la devolución del árbol de decisión globalmente óptimo.Esto puede mitigarse mediante la formación de múltiples árboles en un conjunto de aprendizaje,en el que las características y muestras se muestrean al azar con sustitución.</target>
        </trans-unit>
        <trans-unit id="6c819ed9fb97cd33099d0eff9842389e345641fd" translate="yes" xml:space="preserve">
          <source>The problem solved in clustering</source>
          <target state="translated">El problema resuelto en la agrupación</target>
        </trans-unit>
        <trans-unit id="2c5b556d82e8f03aa8505b7b204354187717fb43" translate="yes" xml:space="preserve">
          <source>The problem solved in supervised learning</source>
          <target state="translated">El problema resuelto en el aprendizaje supervisado</target>
        </trans-unit>
        <trans-unit id="ab484ff296e26d980b5f93762f848e40818065e8" translate="yes" xml:space="preserve">
          <source>The progress meter: the higher the value of &lt;code&gt;verbose&lt;/code&gt;, the more messages:</source>
          <target state="translated">El medidor de progreso: cuanto mayor sea el valor de &lt;code&gt;verbose&lt;/code&gt; , m&amp;aacute;s mensajes:</target>
        </trans-unit>
        <trans-unit id="4163ad4952f27df98667c245d9d8133cfc5f4bae" translate="yes" xml:space="preserve">
          <source>The project mailing list</source>
          <target state="translated">La lista de correo del proyecto</target>
        </trans-unit>
        <trans-unit id="c7689a2c8d3ddf3814b537c25de9417bcceff1dc" translate="yes" xml:space="preserve">
          <source>The projected data.</source>
          <target state="translated">Los datos proyectados.</target>
        </trans-unit>
        <trans-unit id="1179732ddee68f2a030de922cca4a7f3acc9e816" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: [n_sample + n_features + 1] / 2</source>
          <target state="translated">La proporción de puntos que se incluirán en el apoyo de la estimación bruta de MCD.El valor por defecto es Ninguno,lo que implica que se utilizará el valor mínimo de support_fraction dentro del algoritmo:n_muestra+n_características+1]/2</target>
        </trans-unit>
        <trans-unit id="5f706b185c5fa578abf1be5586afd62f084c2356" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;.</source>
          <target state="translated">La proporci&amp;oacute;n de puntos que se incluir&amp;aacute;n en el apoyo de la estimaci&amp;oacute;n bruta de MCD. Si es None, se utilizar&amp;aacute; el valor m&amp;iacute;nimo de support_fraction dentro del algoritmo: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c5ef332af0dc668a636cc813bd9c0d699622c033" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;n_iter_no_change&lt;/code&gt; is set to an integer.</source>
          <target state="translated">La proporci&amp;oacute;n de datos de entrenamiento que se deben reservar como conjunto de validaci&amp;oacute;n para la detenci&amp;oacute;n anticipada. Debe estar entre 0 y 1. Solo se usa si &lt;code&gt;n_iter_no_change&lt;/code&gt; se establece en un n&amp;uacute;mero entero.</target>
        </trans-unit>
        <trans-unit id="e7040634736e41a179ebec81405480b75c6b1afc" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True</source>
          <target state="translated">La proporción de los datos de entrenamiento que se reservan como validación para la detención temprana.Debe estar entre 0 y 1.Sólo se utiliza si la parada temprana es verdadera</target>
        </trans-unit>
        <trans-unit id="22eea34be82cd504d8c4cf88134dfff15444db48" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.</source>
          <target state="translated">La proporción de los datos de entrenamiento que se reservan como validación para la detención temprana.Debe estar entre 0 y 1.Sólo se usa si la parada temprana es verdadera.</target>
        </trans-unit>
        <trans-unit id="3b21c544ac1d6b2edc3be7d45619235a28b43e4c" translate="yes" xml:space="preserve">
          <source>The proportions of samples assigned to each class. If None, then classes are balanced. Note that if &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt;, then the last class weight is automatically inferred. More than &lt;code&gt;n_samples&lt;/code&gt; samples may be returned if the sum of &lt;code&gt;weights&lt;/code&gt; exceeds 1.</source>
          <target state="translated">Las proporciones de muestras asignadas a cada clase. Si es Ninguno, las clases est&amp;aacute;n equilibradas. Tenga en cuenta que si &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt; , el &amp;uacute;ltimo peso de la clase se infiere autom&amp;aacute;ticamente. Se pueden devolver m&amp;aacute;s de &lt;code&gt;n_samples&lt;/code&gt; si la suma de &lt;code&gt;weights&lt;/code&gt; excede 1.</target>
        </trans-unit>
        <trans-unit id="824b6a86b9524b4fdb9e2919749fc7db8e534162" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.</source>
          <target state="translated">El prop&amp;oacute;sito de la canalizaci&amp;oacute;n es reunir varios pasos que se pueden validar juntos mientras se establecen diferentes par&amp;aacute;metros. Para ello, permite configurar los par&amp;aacute;metros de los distintos pasos utilizando sus nombres y el nombre del par&amp;aacute;metro separados por un '__', como en el ejemplo siguiente. El estimador de un paso se puede reemplazar por completo configurando el par&amp;aacute;metro con su nombre en otro estimador, o un transformador eliminado estableciendo en Ninguno.</target>
        </trans-unit>
        <trans-unit id="cd8f7005fb1918c1319c0b334e68b25127992a8d" translate="yes" xml:space="preserve">
          <source>The python source code used to generate the model</source>
          <target state="translated">El código fuente de la pitón utilizado para generar el modelo</target>
        </trans-unit>
        <trans-unit id="f7347085b6a9f16c7df450dca9dbc878bc14f5cc" translate="yes" xml:space="preserve">
          <source>The quantile to predict using the &amp;ldquo;quantile&amp;rdquo; strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.</source>
          <target state="translated">El cuantil a predecir utilizando la estrategia de &quot;cuantiles&quot;. Un cuantil de 0.5 corresponde a la mediana, mientras que 0.0 al m&amp;iacute;nimo y 1.0 al m&amp;aacute;ximo.</target>
        </trans-unit>
        <trans-unit id="38be089aaf53753add8a6e12d9d6e104537de3bf" translate="yes" xml:space="preserve">
          <source>The quantity that we use is the daily variation in quote price: quotes that are linked tend to cofluctuate during a day.</source>
          <target state="translated">La cantidad que utilizamos es la variación diaria del precio de la cotización:las cotizaciones que están vinculadas tienden a cofluir durante un día.</target>
        </trans-unit>
        <trans-unit id="96cf03fa14346bfc08bd6f97110fef23b56f72d1" translate="yes" xml:space="preserve">
          <source>The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.</source>
          <target state="translated">El punto o puntos de consulta.Si no se proporciona,se devuelven los vecinos de cada punto indexado.En este caso,el punto de consulta no se considera su propio vecino.</target>
        </trans-unit>
        <trans-unit id="30a7cdee7bb9697635b16e9d03b7cdd4b400cabd" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. the training samples.</source>
          <target state="translated">La muestra o muestras de consulta para calcular el Factor Atípico Local w.r.t.las muestras de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="e9edc4411fbea590103c5860156a749b07db92a2" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. to the training samples.</source>
          <target state="translated">La muestra o muestras de consulta para calcular el factor atípico local w.r.t.a las muestras de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="2f63e59d1f59e9a5989ccb3ba903c403d9c311f4" translate="yes" xml:space="preserve">
          <source>The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.</source>
          <target state="translated">El radio del subconjunto obtenido mediante la fusión de una nueva muestra y del subconjunto más cercano debe ser menor que el umbral.De lo contrario,se inicia un nuevo subclúster.Fijar este valor en muy bajo promueve la división y viceversa.</target>
        </trans-unit>
        <trans-unit id="c1fb03bbf68de1d40f0cd10af0722ed019408483" translate="yes" xml:space="preserve">
          <source>The random forest regressor will only ever predict values within the range of observations or closer to zero for each of the targets. As a result the predictions are biased towards the centre of the circle.</source>
          <target state="translated">El regresor forestal aleatorio sólo podrá predecir valores dentro del rango de observaciones o más cercanos a cero para cada uno de los objetivos.Como resultado,las predicciones están sesgadas hacia el centro del círculo.</target>
        </trans-unit>
        <trans-unit id="ff0bc6a79a727353502babbe6e55a993a0f80508" translate="yes" xml:space="preserve">
          <source>The random number generator is used to generate random chain orders.</source>
          <target state="translated">El generador de números aleatorios se utiliza para generar órdenes en cadena aleatorias.</target>
        </trans-unit>
        <trans-unit id="31617e37a4673ca35baf50a5963330e598289ccc" translate="yes" xml:space="preserve">
          <source>The random symmetric, positive-definite matrix.</source>
          <target state="translated">La matriz simétrica aleatoria,positiva-definida.</target>
        </trans-unit>
        <trans-unit id="0ae670050b82bcbccc27a159ae39c91afa76e218" translate="yes" xml:space="preserve">
          <source>The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.</source>
          <target state="translated">La búsqueda aleatoria y la búsqueda por cuadrículas exploran exactamente el mismo espacio de parámetros.El resultado en los ajustes de los parámetros es bastante similar,mientras que el tiempo de ejecución de la búsqueda aleatoria es drásticamente menor.</target>
        </trans-unit>
        <trans-unit id="6f118fa702c125f64290c65f38abc4f6dddff279" translate="yes" xml:space="preserve">
          <source>The raw (unadjusted) Rand index is then given by:</source>
          <target state="translated">El índice bruto (no ajustado)de Rand viene dado por:</target>
        </trans-unit>
        <trans-unit id="6b2fe9bd420d4a6948923a1a40e37c6224028547" translate="yes" xml:space="preserve">
          <source>The raw RI score is then &amp;ldquo;adjusted for chance&amp;rdquo; into the ARI score using the following scheme:</source>
          <target state="translated">Luego, la puntuaci&amp;oacute;n RI bruta se &quot;ajusta por azar&quot; en la puntuaci&amp;oacute;n ARI mediante el siguiente esquema:</target>
        </trans-unit>
        <trans-unit id="884b3f9d013732b636db9c9b452d7d3559d50f2d" translate="yes" xml:space="preserve">
          <source>The raw robust estimated covariance before correction and re-weighting.</source>
          <target state="translated">La robusta covarianza bruta estimada antes de la corrección y la reponderación.</target>
        </trans-unit>
        <trans-unit id="70b59f0ad9598471a02599d6a34b12e103ef557b" translate="yes" xml:space="preserve">
          <source>The raw robust estimated location before correction and re-weighting.</source>
          <target state="translated">La localización robusta bruta estimada antes de la corrección y la reponderación.</target>
        </trans-unit>
        <trans-unit id="246acb046d6b9bd62c3702633fac1169a8d836d0" translate="yes" xml:space="preserve">
          <source>The real data lies in the &lt;code&gt;filenames&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; attributes. The target attribute is the integer index of the category:</source>
          <target state="translated">Los datos reales se encuentran en los &lt;code&gt;filenames&lt;/code&gt; y &lt;code&gt;target&lt;/code&gt; atributos de destino . El atributo de destino es el &amp;iacute;ndice entero de la categor&amp;iacute;a:</target>
        </trans-unit>
        <trans-unit id="480e842bb5c6e42d98f06aa4f6c096c0c7aba356" translate="yes" xml:space="preserve">
          <source>The recall is the ratio &lt;code&gt;tp / (tp + fn)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fn&lt;/code&gt; the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</source>
          <target state="translated">La recuperaci&amp;oacute;n es la relaci&amp;oacute;n &lt;code&gt;tp / (tp + fn)&lt;/code&gt; donde &lt;code&gt;tp&lt;/code&gt; es el n&amp;uacute;mero de verdaderos positivos y &lt;code&gt;fn&lt;/code&gt; el n&amp;uacute;mero de falsos negativos. El retiro es intuitivamente la capacidad del clasificador de encontrar todas las muestras positivas.</target>
        </trans-unit>
        <trans-unit id="65f6a86aee18eed07b01f195c73d746d5f2ca909" translate="yes" xml:space="preserve">
          <source>The reconstructed points using the metric MDS and non metric MDS are slightly shifted to avoid overlapping.</source>
          <target state="translated">Los puntos reconstruidos usando el MDS métrico y el MDS no métrico están ligeramente desplazados para evitar superposiciones.</target>
        </trans-unit>
        <trans-unit id="cefc093c3489c62b159a3ce090f6d8b10ecaa3b1" translate="yes" xml:space="preserve">
          <source>The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a \(d\)-dimensional manifold embedded in a \(D\)-dimensional parameter space, the reconstruction error will decrease as &lt;code&gt;n_components&lt;/code&gt; is increased until &lt;code&gt;n_components == d&lt;/code&gt;.</source>
          <target state="translated">El error de reconstrucci&amp;oacute;n calculado por cada rutina se puede utilizar para elegir la dimensi&amp;oacute;n de salida &amp;oacute;ptima. Para una variedad dimensional \ (d \) incrustada en un espacio de par&amp;aacute;metros dimensionales \ (D \), el error de reconstrucci&amp;oacute;n disminuir&amp;aacute; a medida que &lt;code&gt;n_components&lt;/code&gt; aumente hasta &lt;code&gt;n_components == d&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cd5de02ec688b7a05331073a7d6e7032a5c96c24" translate="yes" xml:space="preserve">
          <source>The reconstruction with L1 penalization gives a result with zero error (all pixels are successfully labeled with 0 or 1), even if noise was added to the projections. In comparison, an L2 penalization (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt;&lt;/a&gt;) produces a large number of labeling errors for the pixels. Important artifacts are observed on the reconstructed image, contrary to the L1 penalization. Note in particular the circular artifact separating the pixels in the corners, that have contributed to fewer projections than the central disk.</source>
          <target state="translated">La reconstrucci&amp;oacute;n con penalizaci&amp;oacute;n L1 da un resultado con cero error (todos los p&amp;iacute;xeles se etiquetan correctamente con 0 o 1), incluso si se agreg&amp;oacute; ruido a las proyecciones. En comparaci&amp;oacute;n, una penalizaci&amp;oacute;n L2 ( &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; &lt;/a&gt; ) produce una gran cantidad de errores de etiquetado para los p&amp;iacute;xeles. Se observan artefactos importantes en la imagen reconstruida, contrario a la penalizaci&amp;oacute;n L1. N&amp;oacute;tese en particular el artefacto circular que separa los p&amp;iacute;xeles en las esquinas, que han contribuido a menos proyecciones que el disco central.</target>
        </trans-unit>
        <trans-unit id="cc89055f829a16401789a0501b3aaaa652548713" translate="yes" xml:space="preserve">
          <source>The reduced distance, defined for some metrics, is a computationally more efficient measure which preserves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is the squared-euclidean distance.</source>
          <target state="translated">La distancia reducida,definida para algunas métricas,es una medida computacionalmente más eficiente que preserva el rango de la distancia verdadera.Por ejemplo,en la métrica de la distancia euclidiana,la distancia reducida es la distancia euclidiana al cuadrado.</target>
        </trans-unit>
        <trans-unit id="d3411437239f8f2dc8ee2706670c4e4a91db1791" translate="yes" xml:space="preserve">
          <source>The reduced samples.</source>
          <target state="translated">Las muestras reducidas.</target>
        </trans-unit>
        <trans-unit id="67c2217cec61f41257c896a393db0ce694f3f635" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;GridSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">El estimador reajustado est&amp;aacute; disponible en el atributo &lt;code&gt;best_estimator_&lt;/code&gt; y permite usar &lt;code&gt;predict&lt;/code&gt; directamente en esta instancia de &lt;code&gt;GridSearchCV&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1178e040e21448dfc3d87635a64add70ee2fc71f" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;RandomizedSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">El estimador reajustado est&amp;aacute; disponible en el atributo &lt;code&gt;best_estimator_&lt;/code&gt; y permite usar &lt;code&gt;predict&lt;/code&gt; directamente en esta instancia de &lt;code&gt;RandomizedSearchCV&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="eb2b5c40285ce2a0d935c8174b2b1df487e2e554" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical.</source>
          <target state="translated">El objetivo de regresión o las etiquetas de clasificación,si procede.El tipo D es flotante si es numérico,y el objeto si es categórico.</target>
        </trans-unit>
        <trans-unit id="444eaf2365e5ec07e25a0ed19fde31ede0366773" translate="yes" xml:space="preserve">
          <source>The regressor is used to predict and the &lt;code&gt;inverse_func&lt;/code&gt; or &lt;code&gt;inverse_transform&lt;/code&gt; is applied before returning the prediction.</source>
          <target state="translated">El regresor se usa para predecir y se aplica &lt;code&gt;inverse_func&lt;/code&gt; o &lt;code&gt;inverse_transform&lt;/code&gt; antes de devolver la predicci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="4f20e2edcb6e4e7d6bdbe8de69b9b24c7725d7e1" translate="yes" xml:space="preserve">
          <source>The regularised covariance is:</source>
          <target state="translated">La covarianza regularizada es:</target>
        </trans-unit>
        <trans-unit id="4dfecc4cc3d2dd0e68757701d8e0aef7bde77c4d" translate="yes" xml:space="preserve">
          <source>The regularization mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 &amp;lt; l1_ratio &amp;lt; 1, the penalty is a combination of L1 and L2.</source>
          <target state="translated">El par&amp;aacute;metro de mezcla de regularizaci&amp;oacute;n, con 0 &amp;lt;= l1_ratio &amp;lt;= 1. Para l1_ratio = 0, la penalizaci&amp;oacute;n es una penalizaci&amp;oacute;n L2 por elementos (tambi&amp;eacute;n conocida como Norma Frobenius). Para l1_ratio = 1 es una penalizaci&amp;oacute;n L1 por elementos. Para 0 &amp;lt;l1_ratio &amp;lt;1, la penalizaci&amp;oacute;n es una combinaci&amp;oacute;n de L1 y L2.</target>
        </trans-unit>
        <trans-unit id="f1711bca786b8ddb98e81cd17b706bc6925eee44" translate="yes" xml:space="preserve">
          <source>The regularization parameter C in the LogisticRegression. When C is an array, fit will take each regularization parameter in C one by one for LogisticRegression and store results for each one in &lt;code&gt;all_scores_&lt;/code&gt;, where columns and rows represent corresponding reg_parameters and features.</source>
          <target state="translated">El par&amp;aacute;metro de regularizaci&amp;oacute;n C en LogisticRegression. Cuando C es una matriz, fit tomar&amp;aacute; cada par&amp;aacute;metro de regularizaci&amp;oacute;n en C uno por uno para LogisticRegression y almacenar&amp;aacute; los resultados de cada uno en &lt;code&gt;all_scores_&lt;/code&gt; , donde las columnas y filas representan los par&amp;aacute;metros reg_parameters y caracter&amp;iacute;sticas correspondientes.</target>
        </trans-unit>
        <trans-unit id="7865779166b38ce9dcfe2e41f3eba5295cbd3874" translate="yes" xml:space="preserve">
          <source>The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling.</source>
          <target state="translated">El parámetro de regularización alfa en el Lasso.Advertencia:este no es el parámetro alfa del artículo de selección de estabilidad que está escalando.</target>
        </trans-unit>
        <trans-unit id="0ed37f5cbd4d43ac0f2b6fe6eac86f942c91256a" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance.</source>
          <target state="translated">El parámetro de regularización:cuanto más alto el alfa,más regularización,más escasa la covarianza inversa.</target>
        </trans-unit>
        <trans-unit id="7a1b8c0c02e4613adc42b58f49402bda71930b51" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is given by:</source>
          <target state="translated">La covarianza regularizada (encogida)viene dada por:</target>
        </trans-unit>
        <trans-unit id="0a5a9291b6a07681a32efc9a88d6f2d7eab96435" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is:</source>
          <target state="translated">La covarianza regularizada (encogida)es:</target>
        </trans-unit>
        <trans-unit id="5061099398d935daccb5dfd0421b959c5f17fce1" translate="yes" xml:space="preserve">
          <source>The regularized covariance is given by:</source>
          <target state="translated">La covarianza regularizada está dada por:</target>
        </trans-unit>
        <trans-unit id="bba73ee876f52c89494b029f9c7d24e1239abc01" translate="yes" xml:space="preserve">
          <source>The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.</source>
          <target state="translated">El regularizador es una penalización añadida a la función de pérdida que encoge los parámetros del modelo hacia el vector cero utilizando la norma euclidiana cuadrada L2 o la norma absoluta L1 o una combinación de ambas (Red Elástica).Si la actualización del parámetro cruza el valor de 0,0 debido al regularizador,la actualización se trunca a 0,0 para permitir el aprendizaje de modelos escasos y lograr la selección de características en línea.</target>
        </trans-unit>
        <trans-unit id="dccfd8ff5de1af25843574f310f33b70fd7f2fcc" translate="yes" xml:space="preserve">
          <source>The relationship between recall and precision can be observed in the stairstep area of the plot - at the edges of these steps a small change in the threshold considerably reduces precision, with only a minor gain in recall.</source>
          <target state="translated">La relación entre el recuerdo y la precisión puede observarse en el área de los escalones de la trama-en los bordes de estos escalones un pequeño cambio en el umbral reduce considerablemente la precisión,con sólo una pequeña ganancia en el recuerdo.</target>
        </trans-unit>
        <trans-unit id="e4d930448408dd13aba80cfbcc12949bce572769" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile if &lt;code&gt;effective_rank&lt;/code&gt; is not None.</source>
          <target state="translated">La importancia relativa de la cola gruesa y ruidosa del perfil de valores singulares si &lt;code&gt;effective_rank&lt;/code&gt; no es Ninguno.</target>
        </trans-unit>
        <trans-unit id="5b4ee2ad411363b437ce1738486767191903cc20" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile.</source>
          <target state="translated">La importancia relativa de la cola gorda y ruidosa del perfil de valores singulares.</target>
        </trans-unit>
        <trans-unit id="1bf8ab4629789502195bd390130a1b2aa7e78e4e" translate="yes" xml:space="preserve">
          <source>The relative increment in the results before declaring convergence.</source>
          <target state="translated">El incremento relativo de los resultados antes de declarar la convergencia.</target>
        </trans-unit>
        <trans-unit id="ae073568b2b2b27a72266212b55fc2cb2d4cd169" translate="yes" xml:space="preserve">
          <source>The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The &lt;strong&gt;expected fraction of the samples&lt;/strong&gt; they contribute to can thus be used as an estimate of the &lt;strong&gt;relative importance of the features&lt;/strong&gt;. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.</source>
          <target state="translated">El rango relativo (es decir, la profundidad) de una caracter&amp;iacute;stica utilizada como nodo de decisi&amp;oacute;n en un &amp;aacute;rbol puede usarse para evaluar la importancia relativa de esa caracter&amp;iacute;stica con respecto a la predictibilidad de la variable objetivo. Las caracter&amp;iacute;sticas utilizadas en la parte superior del &amp;aacute;rbol contribuyen a la decisi&amp;oacute;n de predicci&amp;oacute;n final de una fracci&amp;oacute;n mayor de las muestras de entrada. Por tanto, la &lt;strong&gt;fracci&amp;oacute;n esperada de las muestras a las&lt;/strong&gt; que contribuyen puede utilizarse como una estimaci&amp;oacute;n de la &lt;strong&gt;importancia relativa de las caracter&amp;iacute;sticas&lt;/strong&gt; . En scikit-learn, la fracci&amp;oacute;n de muestras a las que contribuye una caracter&amp;iacute;stica se combina con la disminuci&amp;oacute;n de la impureza al dividirlas para crear una estimaci&amp;oacute;n normalizada del poder predictivo de esa caracter&amp;iacute;stica.</target>
        </trans-unit>
        <trans-unit id="0fc6d12d2c584bef7c5a793374c798219700e42f" translate="yes" xml:space="preserve">
          <source>The remaining singular values&amp;rsquo; tail is fat, decreasing as:</source>
          <target state="translated">La cola de los valores singulares restantes es gruesa, disminuyendo como:</target>
        </trans-unit>
        <trans-unit id="006806051caab55f94063393a09eaea2afaeb022" translate="yes" xml:space="preserve">
          <source>The reported averages include micro average (averaging the total true positives, false negatives and false positives), macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label) and sample average (only for multilabel classification). See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="translated">Los promedios informados incluyen micropromedio (promediando el total de verdaderos positivos, falsos negativos y falsos positivos), macropromedio (promediando la media no ponderada por etiqueta), promedio ponderado (promediando la media ponderada de soporte por etiqueta) y promedio de la muestra (solo para etiquetas m&amp;uacute;ltiples clasificaci&amp;oacute;n). Consulte tambi&amp;eacute;n &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt; para obtener m&amp;aacute;s detalles sobre los promedios.</target>
        </trans-unit>
        <trans-unit id="8f7b2580f38f68e2972536327271d77d45324cd3" translate="yes" xml:space="preserve">
          <source>The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.</source>
          <target state="translated">La matriz residual del bloque X (Xk+1)se obtiene por la deflación en la puntuación actual de X:x_puntuación.</target>
        </trans-unit>
        <trans-unit id="18a2377c2d81e0ebb00644fcb1d33204e8d98249" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented.</source>
          <target state="translated">La matriz residual del bloque Y (Yk+1)se obtiene por deflación en la puntuación X actual.Esto realiza la regresión PLS conocida como PLS2.Este modo está orientado a la predicción.</target>
        </trans-unit>
        <trans-unit id="953a5ab9533846fbfb5f76815143b2efa25824fa" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score.</source>
          <target state="translated">La matriz residual del bloque Y (Yk+1)se obtiene por deflación en la puntuación actual de Y.</target>
        </trans-unit>
        <trans-unit id="4786c768ceb1dac6ca78b9b9b4bd8e26183b7ef9" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.</source>
          <target state="translated">La matriz residual del bloque Y (Yk+1)se obtiene por deflación en la puntuación actual de Y.Esto realiza una versión simétrica canónica de la regresión PLS.Pero ligeramente diferente a la CCA.Esto se utiliza principalmente para el modelado.</target>
        </trans-unit>
        <trans-unit id="3050c2e87895e094a17bdbd4ce41119b71fa1e6b" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; is much more strongly biased: the difference is reminiscent of the local intensity value of the original image.</source>
          <target state="translated">El resultado de la &lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;regresi&amp;oacute;n de &amp;aacute;ngulo m&amp;iacute;nimo&lt;/a&gt; est&amp;aacute; mucho m&amp;aacute;s sesgado: la diferencia recuerda el valor de intensidad local de la imagen original.</target>
        </trans-unit>
        <trans-unit id="2844a2c76b7226f0533d494b8e60503f59b9c4e8" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; may be different from those obtained using &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; as the elements are grouped in different ways. The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; takes an average over cross-validation folds, whereas &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; is not an appropriate measure of generalisation error.</source>
          <target state="translated">El resultado de &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; puede ser diferente de los obtenidos con &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; ya que los elementos se agrupan de diferentes formas. La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; toma un promedio de los pliegues de validaci&amp;oacute;n cruzada, mientras que &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; simplemente devuelve las etiquetas (o probabilidades) de varios modelos distintos sin distinci&amp;oacute;n. Por tanto, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; no es una medida apropiada de error de generalizaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="a51791cdd2f13d5121648ef37a39961811acb402" translate="yes" xml:space="preserve">
          <source>The result of calling &lt;code&gt;fit&lt;/code&gt; on a &lt;code&gt;GridSearchCV&lt;/code&gt; object is a classifier that we can use to &lt;code&gt;predict&lt;/code&gt;:</source>
          <target state="translated">El resultado de llamar a &lt;code&gt;fit&lt;/code&gt; en un objeto &lt;code&gt;GridSearchCV&lt;/code&gt; es un clasificador que podemos usar para &lt;code&gt;predict&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="b0c66fbb583b621a3ed191db16f52e8d9fa96072" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="translated">El resultado de este método es idéntico al de np.diag(self(X));sin embargo,puede evaluarse de manera más eficiente ya que sólo se evalúa la diagonal.</target>
        </trans-unit>
        <trans-unit id="629d23e2107cca0c4a5d2061641bb34723082f2c" translate="yes" xml:space="preserve">
          <source>The result points are &lt;em&gt;not&lt;/em&gt; necessarily sorted by distance to their query point.</source>
          <target state="translated">Los puntos de resultado &lt;em&gt;no se&lt;/em&gt; ordenan necesariamente por distancia al punto de consulta.</target>
        </trans-unit>
        <trans-unit id="dc861dceeba49e88611e2b04a0b3ec8cf37af89c" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabaz score.</source>
          <target state="translated">La puntuación resultante de Calinski-Harabaz.</target>
        </trans-unit>
        <trans-unit id="91d97654f948931244cdd794d37fd9ac58fe871c" translate="yes" xml:space="preserve">
          <source>The resulting Davies-Bouldin score.</source>
          <target state="translated">El resultado de la puntuación de Davies-Bouldin.</target>
        </trans-unit>
        <trans-unit id="cfd8b506a89d0c11900fefa11e6003ff64d7a508" translate="yes" xml:space="preserve">
          <source>The resulting Fowlkes-Mallows score.</source>
          <target state="translated">La puntuación resultante de Fowlkes-Mallows.</target>
        </trans-unit>
        <trans-unit id="82343548ff27f39a450415f81d355404a35e8f50" translate="yes" xml:space="preserve">
          <source>The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.</source>
          <target state="translated">La estructura resultante de los bíceps es bloque-diagonal,ya que cada fila y cada columna pertenece exactamente a un bíceps.</target>
        </trans-unit>
        <trans-unit id="af7844c7ade28bb6e966130c36de65d0b613a4b9" translate="yes" xml:space="preserve">
          <source>The resulting dataset contains ordinal attributes which can be further used in a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">El conjunto de datos resultante contiene atributos ordinales que se pueden usar m&amp;aacute;s en un &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2b9ee0d5d80ffdad0cbeed5368095411cec46727" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_exp(X, Y) = k(X, Y) ** exponent</source>
          <target state="translated">El núcleo resultante se define como k_exp(X,Y)=k(X,Y)**exponente</target>
        </trans-unit>
        <trans-unit id="5f2e205585c93945d55d6e2cae73c2d9f60e4b99" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_prod(X, Y) = k1(X, Y) * k2(X, Y)</source>
          <target state="translated">El núcleo resultante se define como k_prod(X,Y)=k1(X,Y)*k2(X,Y)</target>
        </trans-unit>
        <trans-unit id="65c4e9abf2f65b5e239efef34833d44fb903de78" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_sum(X, Y) = k1(X, Y) + k2(X, Y)</source>
          <target state="translated">El núcleo resultante se define como k_sum(X,Y)=k1(X,Y)+k2(X,Y)</target>
        </trans-unit>
        <trans-unit id="fb9ca9651a528caa2f6deb96ee39233de0fa48d4" translate="yes" xml:space="preserve">
          <source>The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;. The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. The parameters are estimated by maximizing the &lt;em&gt;marginal log likelihood&lt;/em&gt;.</source>
          <target state="translated">El modelo resultante se llama &lt;em&gt;Regresi&amp;oacute;n de la cresta bayesiana&lt;/em&gt; y es similar a la &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; cl&amp;aacute;sica . Los par&amp;aacute;metros \ (w \), \ (\ alpha \) y \ (\ lambda \) se estiman conjuntamente durante el ajuste del modelo. Los hiperpar&amp;aacute;metros restantes son los par&amp;aacute;metros de los gamma previos sobre \ (\ alpha \) y \ (\ lambda \). Por lo general, se eligen para que &lt;em&gt;no sean informativos&lt;/em&gt; . Los par&amp;aacute;metros se estiman maximizando la &lt;em&gt;probabilidad logar&amp;iacute;tmica marginal&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="2ff5ad65586245c919e0a8e1ef11c4948b5606a5" translate="yes" xml:space="preserve">
          <source>The resulting patches are allocated in a dedicated array.</source>
          <target state="translated">Los parches resultantes se asignan en una matriz dedicada.</target>
        </trans-unit>
        <trans-unit id="64dfac2f5dc0c167f2eb731c8522fdbbf80022e7" translate="yes" xml:space="preserve">
          <source>The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data.</source>
          <target state="translated">El transformador resultante ha aprendido entonces una incrustación categórica supervisada,dispersa y de alta dimensión de los datos.</target>
        </trans-unit>
        <trans-unit id="3f9dd224b05fada5edc12dcb7a4c8d5421d61c2d" translate="yes" xml:space="preserve">
          <source>The return value is a cross-validator which generates the train/test splits via the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">El valor de retorno es un validador cruzado que genera las divisiones de tren / prueba a trav&amp;eacute;s del m&amp;eacute;todo de &lt;code&gt;split&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="63a7df0fd8542a7b486adfe1466de16955c3587a" translate="yes" xml:space="preserve">
          <source>The returned dataset is a &lt;code&gt;scikit-learn&lt;/code&gt; &amp;ldquo;bunch&amp;rdquo;: a simple holder object with fields that can be both accessed as python &lt;code&gt;dict&lt;/code&gt; keys or &lt;code&gt;object&lt;/code&gt; attributes for convenience, for instance the &lt;code&gt;target_names&lt;/code&gt; holds the list of the requested category names:</source>
          <target state="translated">El conjunto de datos devuelto es un &quot;grupo&quot; de &lt;code&gt;scikit-learn&lt;/code&gt; : un objeto contenedor simple con campos a los que se puede acceder como claves de &lt;code&gt;dict&lt;/code&gt; ado de Python o atributos de &lt;code&gt;object&lt;/code&gt; por conveniencia, por ejemplo, &lt;code&gt;target_names&lt;/code&gt; contiene la lista de los nombres de categor&amp;iacute;a solicitados:</target>
        </trans-unit>
        <trans-unit id="c5d262a3b65ccb350b8a1d10b0eaf6eba41fc62d" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by label of classes.</source>
          <target state="translated">Las estimaciones devueltas para todas las clases están ordenadas por etiqueta de clases.</target>
        </trans-unit>
        <trans-unit id="8520c10c8edb7f58383ef36900c902f5398bf785" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by the label of classes.</source>
          <target state="translated">Las estimaciones devueltas para todas las clases están ordenadas por la etiqueta de clases.</target>
        </trans-unit>
        <trans-unit id="9fa1a308376d0d44aa58fe9b54fd102c1f0b956a" translate="yes" xml:space="preserve">
          <source>The returned object is a MemorizedFunc object, that is callable (behaves like a function), but offers extra methods for cache lookup and management. See the documentation for &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt;&lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">El objeto devuelto es un objeto MemorizedFunc, que se puede llamar (se comporta como una funci&amp;oacute;n), pero ofrece m&amp;eacute;todos adicionales para la b&amp;uacute;squeda y administraci&amp;oacute;n de cach&amp;eacute;. Consulte la documentaci&amp;oacute;n de &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt; &lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="4a1e44716730f4f771067bf0b441674e2034e883" translate="yes" xml:space="preserve">
          <source>The richer dictionary on the right is not larger in size, heavier subsampling is performed in order to stay on the same order of magnitude.</source>
          <target state="translated">El diccionario más rico de la derecha no es de mayor tamaño,se realiza un submuestreo más pesado para mantenerse en el mismo orden de magnitud.</target>
        </trans-unit>
        <trans-unit id="6f396634dd18eef11c3d60404319f2831b13040b" translate="yes" xml:space="preserve">
          <source>The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around &lt;code&gt;x=2&lt;/code&gt;). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance.</source>
          <target state="translated">Las cifras de la derecha corresponden a las mismas parcelas pero utilizando en su lugar un conjunto de &amp;aacute;rboles de decisi&amp;oacute;n de ensacado. En ambas figuras podemos observar que el t&amp;eacute;rmino de sesgo es mayor que en el caso anterior. En la figura superior derecha, la diferencia entre la predicci&amp;oacute;n promedio (en cian) y el mejor modelo posible es mayor (por ejemplo, observe el desplazamiento alrededor de &lt;code&gt;x=2&lt;/code&gt; ). En la figura inferior derecha, la curva de sesgo tambi&amp;eacute;n es ligeramente m&amp;aacute;s alta que en la figura inferior izquierda. Sin embargo, en t&amp;eacute;rminos de varianza, el haz de predicciones es m&amp;aacute;s estrecho, lo que sugiere que la varianza es menor. De hecho, como confirma la figura inferior derecha, el t&amp;eacute;rmino de varianza (en verde) es m&amp;aacute;s bajo que para &amp;aacute;rboles de decisi&amp;oacute;n individuales. En general, la descomposici&amp;oacute;n de sesgo-varianza ya no es la misma. La compensaci&amp;oacute;n es mejor para el ensacado: promediar varios &amp;aacute;rboles de decisi&amp;oacute;n que se ajustan a las copias de arranque del conjunto de datos aumenta ligeramente el t&amp;eacute;rmino de sesgo, pero permite una mayor reducci&amp;oacute;n de la varianza, lo que da como resultado un error cuadr&amp;aacute;tico medio general m&amp;aacute;s bajo (compare las curvas rojas en las cifras). La salida del script tambi&amp;eacute;n confirma esta intuici&amp;oacute;n. El error total del conjunto de ensacado es menor que el error total de un solo &amp;aacute;rbol de decisi&amp;oacute;n,y esta diferencia, de hecho, se debe principalmente a una variaci&amp;oacute;n reducida.</target>
        </trans-unit>
        <trans-unit id="4281560832d700a912bb9768ad9253748f3986db" translate="yes" xml:space="preserve">
          <source>The right plot shows the mean squared error between the coefficients found by the model and the chosen vector w. Less regularised models retrieve the exact coefficients (error is equal to 0), stronger regularised models increase the error.</source>
          <target state="translated">El gráfico de la derecha muestra el error cuadrático medio entre los coeficientes encontrados por el modelo y el vector w elegido.Los modelos menos regularizados recuperan los coeficientes exactos (el error es igual a 0),los modelos regularizados más fuertes aumentan el error.</target>
        </trans-unit>
        <trans-unit id="e91440f2fdf83c048c6ec145aa4ba8b2408d7a77" translate="yes" xml:space="preserve">
          <source>The robust MCD, that has a low error provided \(n_\text{samples} &amp;gt; 5n_\text{features}\)</source>
          <target state="translated">El MCD robusto, que tiene un error bajo proporcionado \ (n_ \ text {samples}&amp;gt; 5n_ \ text {features} \)</target>
        </trans-unit>
        <trans-unit id="17f09e161fdb50b4bf88ea2669cf0485f026fd48" translate="yes" xml:space="preserve">
          <source>The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.</source>
          <target state="translated">Las filas son las muestras y las columnas son:Longitud del Sépalo,Ancho del Sépalo,Longitud del Pétalo y Ancho del Pétalo.</target>
        </trans-unit>
        <trans-unit id="075c19426795ecca36fadcd5142db0c818eae0c2" translate="yes" xml:space="preserve">
          <source>The s parameter used to randomly scale the penalty of different features. Should be between 0 and 1.</source>
          <target state="translated">El parámetro s utilizado para escalar aleatoriamente la pena de diferentes características.Debería estar entre 0 y 1.</target>
        </trans-unit>
        <trans-unit id="8e103a284fd35ca6afde93378ca71e413fedf538" translate="yes" xml:space="preserve">
          <source>The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).</source>
          <target state="translated">El mismo grupo no aparecerá en dos pliegues diferentes (el número de grupos distintos tiene que ser al menos igual al número de pliegues).</target>
        </trans-unit>
        <trans-unit id="0bd48a9cd63a739602e7de633cedbe34c0721a4f" translate="yes" xml:space="preserve">
          <source>The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:</source>
          <target state="translated">La misma instancia del transformador puede aplicarse entonces a algunos nuevos datos de prueba no vistos durante la llamada de ajuste:se aplicarán las mismas operaciones de escalado y desplazamiento para ser consistentes con la transformación realizada en los datos del tren:</target>
        </trans-unit>
        <trans-unit id="c16a50cac2b1dc1101733e88917cf565ad0ad55a" translate="yes" xml:space="preserve">
          <source>The sample counts that are shown are weighted with any sample_weights that might be present.</source>
          <target state="translated">Los recuentos de muestras que se muestran se ponderan con los pesos_de_muestras que puedan estar presentes.</target>
        </trans-unit>
        <trans-unit id="da56cc1e3278be97e394d14d7afaac125d975593" translate="yes" xml:space="preserve">
          <source>The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible.</source>
          <target state="translated">La ponderación de la muestra reajusta el parámetro C,lo que significa que el clasificador pone más énfasis en acertar estos puntos.El efecto puede ser a menudo sutil.Para enfatizar el efecto aquí,particularmente ponderamos los valores atípicos,haciendo muy visible la deformación del límite de decisión.</target>
        </trans-unit>
        <trans-unit id="67f0f1888d07b20765c25e213bac379b2147854b" translate="yes" xml:space="preserve">
          <source>The sampled subsets of integer. The subset of selected integer might not be randomized, see the method argument.</source>
          <target state="translated">Los subconjuntos muestreados de números enteros.El subconjunto de números enteros seleccionados podría no ser aleatorio,véase el argumento del método.</target>
        </trans-unit>
        <trans-unit id="582c77a6e6e223c6a451ff779c949c2d01a1e4b1" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="translated">Las muestras en este conjunto de datos corresponden a parches de bosque de 30 &amp;times; 30 m en los EE. UU., Recolectados para la tarea de predecir el tipo de cobertura de cada parche, es decir, la especie de &amp;aacute;rbol dominante. Hay siete tipos encubiertos, lo que lo convierte en un problema de clasificaci&amp;oacute;n multiclase. Cada muestra tiene 54 caracter&amp;iacute;sticas, descritas en la &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;p&amp;aacute;gina de inicio del conjunto de datos&lt;/a&gt; . Algunas de las caracter&amp;iacute;sticas son indicadores booleanos, mientras que otras son medidas discretas o continuas.</target>
        </trans-unit>
        <trans-unit id="3faf112740a094590f64233617c65a5fa097ee8f" translate="yes" xml:space="preserve">
          <source>The samples.</source>
          <target state="translated">Las muestras.</target>
        </trans-unit>
        <trans-unit id="1aa98782517735bf609d139ad8030ba79e53e38f" translate="yes" xml:space="preserve">
          <source>The scaler instance can then be used on new data to transform it the same way it did on the training set:</source>
          <target state="translated">La instancia del escalador puede entonces utilizarse en nuevos datos para transformarlos de la misma manera que lo hizo en el conjunto de entrenamiento:</target>
        </trans-unit>
        <trans-unit id="fae25cf01c011421f9b169e383c16cc5b0e7dc5d" translate="yes" xml:space="preserve">
          <source>The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data:</source>
          <target state="translated">El proyecto scikit-learn proporciona un conjunto de herramientas de aprendizaje automático que pueden utilizarse tanto para la detección de novedades como de anomalías.Esta estrategia se implementa con objetos que aprenden de forma no supervisada a partir de los datos:</target>
        </trans-unit>
        <trans-unit id="b2bcbd53d39d84e38eaccf61f40a5b7e3d9f1072" translate="yes" xml:space="preserve">
          <source>The scikit-learn provides an object &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt; that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.</source>
          <target state="translated">Scikit-learn proporciona una &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt; &lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt; &lt;/a&gt; objeto. ElipticEnvelope que se ajusta a una estimaci&amp;oacute;n de covarianza robusta a los datos y, por lo tanto, ajusta una elipse a los puntos de datos centrales, ignorando los puntos fuera del modo central.</target>
        </trans-unit>
        <trans-unit id="c6f20ecec2f178819b742529ff67a9012c4e74b9" translate="yes" xml:space="preserve">
          <source>The score above which features should be selected.</source>
          <target state="translated">La puntuación por encima de la cual se deben seleccionar los rasgos.</target>
        </trans-unit>
        <trans-unit id="fbc2c7bdd77bdb62a30109845f32d883a40f3289" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split.</source>
          <target state="translated">La matriz de puntajes para los resultados de los exámenes en cada división de cv.</target>
        </trans-unit>
        <trans-unit id="21374122c0da50ea660a65ea3274fa15ca9abbe5" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">La matriz de puntuaci&amp;oacute;n para las puntuaciones de los trenes en cada divisi&amp;oacute;n de cv. Esto solo est&amp;aacute; disponible si el par&amp;aacute;metro &lt;code&gt;return_train_score&lt;/code&gt; es &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f9f0e8fbf15571f500f19345d44e98db663f1949" translate="yes" xml:space="preserve">
          <source>The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.</source>
          <target state="translated">El puntaje está limitado entre -1 por agrupación incorrecta y +1 por agrupación altamente densa.Las puntuaciones alrededor de cero indican agrupaciones superpuestas.</target>
        </trans-unit>
        <trans-unit id="10419dac5247bd799d768931a57ece7edc36ff14" translate="yes" xml:space="preserve">
          <source>The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.</source>
          <target state="translated">La puntuación se define como la relación entre la dispersión dentro de un grupo y la dispersión entre grupos.</target>
        </trans-unit>
        <trans-unit id="a5fc2e46656049d91b95dd6363e2ef13687eb710" translate="yes" xml:space="preserve">
          <source>The score is defined as the ratio of within-cluster distances to between-cluster distances.</source>
          <target state="translated">La puntuación se define como la relación entre las distancias dentro del grupo y las distancias entre los grupos.</target>
        </trans-unit>
        <trans-unit id="a9ccc42adfd31440d43d06c5c4aab96f5e8222f0" translate="yes" xml:space="preserve">
          <source>The score is fast to compute</source>
          <target state="translated">La puntuación es rápida de calcular</target>
        </trans-unit>
        <trans-unit id="298450881e1d83a617f17a47cba5b823081f8332" translate="yes" xml:space="preserve">
          <source>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.</source>
          <target state="translated">El puntaje es más alto cuando los cúmulos son densos y están bien separados,lo que se relaciona con un concepto estándar de un cúmulo.</target>
        </trans-unit>
        <trans-unit id="8ebec04d9506729ea096f793265427e501ac6359" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - \text{n\_classes}}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="translated">El puntaje var&amp;iacute;a de 0 a 1, o cuando se usa &lt;code&gt;adjusted=True&lt;/code&gt; , se vuelve a escalar al rango \ (\ frac {1} {1 - \ text {n \ _classes}} \) a 1, inclusive, con rendimiento al azar puntuaci&amp;oacute;n 0.</target>
        </trans-unit>
        <trans-unit id="99f201771620c880cb8fcf0a4290517d17eb61e7" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.</source>
          <target state="translated">La puntuación va de 0 a 1.Un valor alto indica una buena similitud entre dos grupos.</target>
        </trans-unit>
        <trans-unit id="67703aed8c056d27307c0c36c3685d8f65de746b" translate="yes" xml:space="preserve">
          <source>The scorer callable object / function must have its signature as &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">El objeto / funci&amp;oacute;n invocable anotador debe tener su firma como &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5e77d6fe812bc8f046068c5fa078dda3a1146e44" translate="yes" xml:space="preserve">
          <source>The scorer.</source>
          <target state="translated">El anotador.</target>
        </trans-unit>
        <trans-unit id="dd1c18a79b397962bdc5e0312d35f12a484f084a" translate="yes" xml:space="preserve">
          <source>The scores for each feature along the path.</source>
          <target state="translated">Las puntuaciones de cada característica a lo largo del camino.</target>
        </trans-unit>
        <trans-unit id="81a1b1406082100f034b3df6c2ec24562a123854" translate="yes" xml:space="preserve">
          <source>The scores obtained for each permutations.</source>
          <target state="translated">Las puntuaciones obtenidas para cada permutación.</target>
        </trans-unit>
        <trans-unit id="12753b21f50efe6accfab886a283f46c3614c6fe" translate="yes" xml:space="preserve">
          <source>The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does not attempt to completely filter the outliers but lessen their effect.</source>
          <target state="translated">Las puntuaciones de HuberRegressor no pueden compararse directamente con las de TheilSen y RANSAC porque no intenta filtrar completamente los valores atípicos sino disminuir su efecto.</target>
        </trans-unit>
        <trans-unit id="48d352a6767e745c328aec9195da7218a62bd613" translate="yes" xml:space="preserve">
          <source>The scores of all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at keys ending in &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; (&lt;code&gt;'mean_test_precision'&lt;/code&gt;, &lt;code&gt;'rank_test_precision'&lt;/code&gt;, etc&amp;hellip;)</source>
          <target state="translated">Las puntuaciones de todos los anotadores est&amp;aacute;n disponibles en el &lt;code&gt;cv_results_&lt;/code&gt; en las claves que terminan en &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; ( &lt;code&gt;'mean_test_precision'&lt;/code&gt; , &lt;code&gt;'rank_test_precision'&lt;/code&gt; , etc.)</target>
        </trans-unit>
        <trans-unit id="1676248dcc2b8fd0688c9d8da4bc193152185788" translate="yes" xml:space="preserve">
          <source>The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on.</source>
          <target state="translated">La búsqueda del parámetro de penalización óptimo (alfa)se realiza en una cuadrícula refinada iterativamente:primero se calculan las puntuaciones cruzadas validadas en una cuadrícula,luego una nueva cuadrícula refinada se centra en el máximo,y así sucesivamente.</target>
        </trans-unit>
        <trans-unit id="929b69ae47a09aabafb756d8ff0633d79e90c985" translate="yes" xml:space="preserve">
          <source>The searched parameter.</source>
          <target state="translated">El parámetro buscado.</target>
        </trans-unit>
        <trans-unit id="0d9a4b24aef1596fe1105c7b9e28ffdd6e589d45" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the product-kernel</source>
          <target state="translated">El segundo núcleo de base del núcleo del producto</target>
        </trans-unit>
        <trans-unit id="2d0afedea31e6e04d617f4edd60a150835c5916d" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the sum-kernel</source>
          <target state="translated">El segundo núcleo de base del núcleo de la suma</target>
        </trans-unit>
        <trans-unit id="544eb81771c23c2f620b00755dbcd12f00cacc66" translate="yes" xml:space="preserve">
          <source>The second example shows the ability of the Minimum Covariance Determinant robust estimator of covariance to concentrate on the main mode of the data distribution: the location seems to be well estimated, although the covariance is hard to estimate due to the banana-shaped distribution. Anyway, we can get rid of some outlying observations. The One-Class SVM is able to capture the real data structure, but the difficulty is to adjust its kernel bandwidth parameter so as to obtain a good compromise between the shape of the data scatter matrix and the risk of over-fitting the data.</source>
          <target state="translated">El segundo ejemplo muestra la capacidad del estimador robusto del Determinante de Covarianza Mínima de la covarianza para concentrarse en el modo principal de la distribución de los datos:la ubicación parece estar bien estimada,aunque la covarianza es difícil de estimar debido a la distribución en forma de plátano.De todos modos,podemos deshacernos de algunas observaciones periféricas.El SVM de una clase es capaz de capturar la estructura real de los datos,pero la dificultad es ajustar su parámetro de ancho de banda del núcleo para obtener un buen compromiso entre la forma de la matriz de dispersión de datos y el riesgo de sobreajustar los datos.</target>
        </trans-unit>
        <trans-unit id="5a46c67db9268b64cb76670e9e1b845e906b608f" translate="yes" xml:space="preserve">
          <source>The second figure shows the calibration curve of a linear support-vector classifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian naive Bayes: the calibration curve has a sigmoid curve, which is typical for an under-confident classifier. In the case of LinearSVC, this is caused by the margin property of the hinge loss, which lets the model focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">La segunda figura muestra la curva de calibración de un clasificador vectorial de soporte lineal (LinearSVC).LinearSVC muestra el comportamiento opuesto al de los Bayes gausianos ingenuos:la curva de calibración tiene una curva sigmoide,que es típica de un clasificador poco seguro.En el caso de LinearSVC,esto se debe a la propiedad de margen de la pérdida de la bisagra,que permite al modelo centrarse en muestras duras que están cerca del límite de decisión (los vectores de apoyo).</target>
        </trans-unit>
        <trans-unit id="e56142dda4889d67d8f5448567e56cb678c48e3c" translate="yes" xml:space="preserve">
          <source>The second figure shows the log-marginal-likelihood for different choices of the kernel&amp;rsquo;s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.</source>
          <target state="translated">La segunda figura muestra la probabilidad log-marginal para diferentes elecciones de los hiperpar&amp;aacute;metros del kernel, destacando las dos opciones de los hiperpar&amp;aacute;metros utilizados en la primera figura mediante puntos negros.</target>
        </trans-unit>
        <trans-unit id="69a6b8988bb4a9da22ae7a8129c60d4bfa19ab9d" translate="yes" xml:space="preserve">
          <source>The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:</source>
          <target state="translated">El segundo cargador suele utilizarse para la tarea de verificación de la cara:cada muestra es un par de dos cuadros pertenecientes o no a la misma persona:</target>
        </trans-unit>
        <trans-unit id="80a536b57ba05b05dbab01bd549517eccd6caab7" translate="yes" xml:space="preserve">
          <source>The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process prior fit with variational inference. The low value of the concentration prior makes the model favor a lower number of active components. This models &amp;ldquo;decides&amp;rdquo; to focus its modeling power on the big picture of the structure of the dataset: groups of points with alternating directions modeled by non-diagonal covariance matrices. Those alternating directions roughly capture the alternating nature of the original sine signal.</source>
          <target state="translated">El segundo modelo es un modelo de mezcla gaussiana bayesiana con un ajuste previo del proceso de Dirichlet con inferencia variacional. El bajo valor de la concentraci&amp;oacute;n previa hace que el modelo favorezca un menor n&amp;uacute;mero de componentes activos. Este modelo &quot;decide&quot; enfocar su poder de modelado en el panorama general de la estructura del conjunto de datos: grupos de puntos con direcciones alternas modelados por matrices de covarianza no diagonales. Esas direcciones alternas capturan aproximadamente la naturaleza alterna de la se&amp;ntilde;al sinusoidal original.</target>
        </trans-unit>
        <trans-unit id="ce63f012cd3f9d5ba6717aef4dc1ee5e80e67c9d" translate="yes" xml:space="preserve">
          <source>The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">El segundo tiene un nivel de ruido más bajo y una escala de longitud más corta,lo que explica la mayor parte de la variación por la relación funcional libre de ruido.El segundo modelo tiene una mayor probabilidad;sin embargo,dependiendo del valor inicial de los hiperparámetros,la optimización basada en el gradiente podría también converger hacia la solución de alto ruido.Por consiguiente,es importante repetir la optimización varias veces para diferentes inicializaciones.</target>
        </trans-unit>
        <trans-unit id="d5dcf1b11e556ea9365934aae9c750b0508ecb89" translate="yes" xml:space="preserve">
          <source>The second plot demonstrate one single run of the &lt;code&gt;MiniBatchKMeans&lt;/code&gt; estimator using a &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; and &lt;code&gt;n_init=1&lt;/code&gt;. This run leads to a bad convergence (local optimum) with estimated centers stuck between ground truth clusters.</source>
          <target state="translated">El segundo gr&amp;aacute;fico demuestra una sola ejecuci&amp;oacute;n del estimador &lt;code&gt;MiniBatchKMeans&lt;/code&gt; usando un &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; y &lt;code&gt;n_init=1&lt;/code&gt; . Esta ejecuci&amp;oacute;n conduce a una mala convergencia (&amp;oacute;ptimo local) con centros estimados atrapados entre grupos de verdad del suelo.</target>
        </trans-unit>
        <trans-unit id="41338f596d871499307d96f69f697b91afdfa1c4" translate="yes" xml:space="preserve">
          <source>The second plot is a heatmap of the classifier&amp;rsquo;s cross-validation accuracy as a function of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from \(10^{-3}\) to \(10^3\) is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.</source>
          <target state="translated">El segundo gr&amp;aacute;fico es un mapa de calor de la precisi&amp;oacute;n de la validaci&amp;oacute;n cruzada del clasificador en funci&amp;oacute;n de &lt;code&gt;C&lt;/code&gt; y &lt;code&gt;gamma&lt;/code&gt; . Para este ejemplo, exploramos una cuadr&amp;iacute;cula relativamente grande con fines ilustrativos. En la pr&amp;aacute;ctica, una cuadr&amp;iacute;cula logar&amp;iacute;tmica de \ (10 ​​^ {- 3} \) a \ (10 ​​^ 3 \) suele ser suficiente. Si los mejores par&amp;aacute;metros se encuentran en los l&amp;iacute;mites de la cuadr&amp;iacute;cula, se puede extender en esa direcci&amp;oacute;n en una b&amp;uacute;squeda posterior.</target>
        </trans-unit>
        <trans-unit id="57fb8e025f7ec94b6d1e30748cbff5561f1e9901" translate="yes" xml:space="preserve">
          <source>The second plot shows that an increase of the admissible distortion &lt;code&gt;eps&lt;/code&gt; allows to reduce drastically the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; for a given number of samples &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">El segundo gr&amp;aacute;fico muestra que un aumento de la distorsi&amp;oacute;n admisible &lt;code&gt;eps&lt;/code&gt; permite reducir dr&amp;aacute;sticamente el n&amp;uacute;mero m&amp;iacute;nimo de dimensiones &lt;code&gt;n_components&lt;/code&gt; para un n&amp;uacute;mero dado de muestras &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9c6c0ec72e2e2da2def0a20a979b162b66b4f25f" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span.</source>
          <target state="translated">El segundo gráfico visualizó las superficies de decisión del SVM del núcleo RBF y del SVM lineal con mapas aproximados del núcleo.El gráfico muestra las superficies de decisión de los clasificadores proyectados en los dos primeros componentes principales de los datos.Esta visualización debe tomarse con un grano de sal ya que es sólo un interesante corte a través de la superficie de decisión en 64 dimensiones.Nótese en particular que un punto de datos (representado como un punto)no se clasifica necesariamente en la región en la que se encuentra,ya que no se encontrará en el plano que abarcan los dos primeros componentes principales.</target>
        </trans-unit>
        <trans-unit id="a7aa029b23a556153d8e76aebf5393705fc5a931" translate="yes" xml:space="preserve">
          <source>The second use case is to build a completely custom scorer object from a simple python function using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;, which can take several parameters:</source>
          <target state="translated">El segundo caso de uso es construir un objeto de marcador completamente personalizado a partir de una funci&amp;oacute;n de Python simple usando &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt; , que puede tomar varios par&amp;aacute;metros:</target>
        </trans-unit>
        <trans-unit id="c00cf63770db0bb2225569e421e3076426129a55" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios para agregar un peque&amp;ntilde;o ruido a las variables continuas para eliminar los valores repetidos. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="06cdf003d2aac9179d5700a91f249ff0a94d6f8e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que selecciona una caracter&amp;iacute;stica aleatoria para actualizar. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; . Se usa cuando la &lt;code&gt;selection&lt;/code&gt; == 'aleatoria'</target>
        </trans-unit>
        <trans-unit id="0bc2f2e46810cbdc913e04d68694f60b2b83e0d8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que selecciona una caracter&amp;iacute;stica aleatoria para actualizar. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; . Se usa cuando la &lt;code&gt;selection&lt;/code&gt; == 'aleatoria'.</target>
        </trans-unit>
        <trans-unit id="c9b5b4205f687d2590f4c80ac04919e87b2e36db" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data for the dual coordinate descent (if &lt;code&gt;dual=True&lt;/code&gt;). When &lt;code&gt;dual=False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se usar&amp;aacute; al mezclar los datos para el descenso de coordenadas dual (si &lt;code&gt;dual=True&lt;/code&gt; ). Cuando &lt;code&gt;dual=False&lt;/code&gt; , la implementaci&amp;oacute;n subyacente de &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; no es aleatoria y &lt;code&gt;random_state&lt;/code&gt; no tiene ning&amp;uacute;n efecto sobre los resultados. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f24e839d9f0c07b405eb078c6f50d58ca84b7fe6" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se utilizar&amp;aacute; al mezclar los datos. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="58318e33f8140e9303ba15931c5e93b11c5df63e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo; or &amp;lsquo;liblinear&amp;rsquo;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se utilizar&amp;aacute; al mezclar los datos. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; . Se usa cuando &lt;code&gt;solver&lt;/code&gt; == 'sag' o 'liblinear'.</target>
        </trans-unit>
        <trans-unit id="1e64edd7c479eb06717df1495b4d044bfaa961d0" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se utilizar&amp;aacute; al mezclar los datos. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; . Se usa cuando &lt;code&gt;solver&lt;/code&gt; == 'sag'.</target>
        </trans-unit>
        <trans-unit id="db7139d208134a3c43811feaafd4124e4ee369a8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator used when shuffling the data for probability estimates. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se usa al mezclar los datos para estimaciones de probabilidad. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f1665069fc9c4c9d9dabb36f1931e17db4945528" translate="yes" xml:space="preserve">
          <source>The set of F values.</source>
          <target state="translated">El conjunto de valores F.</target>
        </trans-unit>
        <trans-unit id="d03a062cac2973dfcc9173b5fc93f7520db21987" translate="yes" xml:space="preserve">
          <source>The set of labels can be different for each output variable. For instance, a sample could be assigned &amp;ldquo;pear&amp;rdquo; for an output variable that takes possible values in a finite set of species such as &amp;ldquo;pear&amp;rdquo;, &amp;ldquo;apple&amp;rdquo;; and &amp;ldquo;blue&amp;rdquo; or &amp;ldquo;green&amp;rdquo; for a second output variable that takes possible values in a finite set of colors such as &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;blue&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo;&amp;hellip;</source>
          <target state="translated">El conjunto de etiquetas puede ser diferente para cada variable de salida. Por ejemplo, a una muestra se le podr&amp;iacute;a asignar &quot;pera&quot; para una variable de salida que toma valores posibles en un conjunto finito de especies como &quot;pera&quot;, &quot;manzana&quot;; y &quot;azul&quot; o &quot;verde&quot; para una segunda variable de salida que toma valores posibles en un conjunto finito de colores como &quot;verde&quot;, &quot;rojo&quot;, &quot;azul&quot;, &quot;amarillo&quot; ...</target>
        </trans-unit>
        <trans-unit id="143c46009e14705cc3449ca19bc2f349662d5283" translate="yes" xml:space="preserve">
          <source>The set of labels for each sample such that &lt;code&gt;y[i]&lt;/code&gt; consists of &lt;code&gt;classes_[j]&lt;/code&gt; for each &lt;code&gt;yt[i, j] == 1&lt;/code&gt;.</source>
          <target state="translated">El conjunto de etiquetas para cada muestra de modo que &lt;code&gt;y[i]&lt;/code&gt; consta de &lt;code&gt;classes_[j]&lt;/code&gt; para cada &lt;code&gt;yt[i, j] == 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2d10d69a1c09af3eb9fb75910b7cd4ebd942ac2e" translate="yes" xml:space="preserve">
          <source>The set of labels to include when &lt;code&gt;average != 'binary'&lt;/code&gt;, and their order if &lt;code&gt;average is None&lt;/code&gt;. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">El conjunto de etiquetas que se incluir&amp;aacute;n cuando &lt;code&gt;average != 'binary'&lt;/code&gt; , y su orden si &lt;code&gt;average is None&lt;/code&gt; . Las etiquetas presentes en los datos se pueden excluir, por ejemplo, para calcular un promedio multiclase ignorando una clase negativa mayoritaria, mientras que las etiquetas que no est&amp;aacute;n presentes en los datos dar&amp;aacute;n como resultado 0 componentes en un promedio macro. Para objetivos de etiquetas m&amp;uacute;ltiples, las etiquetas son &amp;iacute;ndices de columna. De forma predeterminada, todas las etiquetas en &lt;code&gt;y_true&lt;/code&gt; e &lt;code&gt;y_pred&lt;/code&gt; se utilizan en orden ordenado.</target>
        </trans-unit>
        <trans-unit id="fc018ce1a1ad3dcbb813a9b943d602142a80bfe5" translate="yes" xml:space="preserve">
          <source>The set of p-values.</source>
          <target state="translated">El conjunto de valores p.</target>
        </trans-unit>
        <trans-unit id="ae935a83c454932c18aabaf5527bf9d40a05884a" translate="yes" xml:space="preserve">
          <source>The set of regressors that will be tested sequentially.</source>
          <target state="translated">El conjunto de regresores que serán probados secuencialmente.</target>
        </trans-unit>
        <trans-unit id="418cd7fd5c32b01bfeb33989472034a267c4e833" translate="yes" xml:space="preserve">
          <source>The shape (Nx, Ny) array of pairwise distances between points in X and Y.</source>
          <target state="translated">La forma (Nx,Ny)de la matriz de distancias en pares entre los puntos en X y Y.</target>
        </trans-unit>
        <trans-unit id="be8e19650b3e56af8959e9d1bdb9305ca252ca97" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_class - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_class - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="translated">La forma de &lt;code&gt;dual_coef_&lt;/code&gt; es &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; con un dise&amp;ntilde;o algo dif&amp;iacute;cil de entender. Las columnas corresponden a los vectores de soporte involucrados en cualquiera de los &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;uno contra uno&amp;rdquo;. Cada uno de los vectores de soporte se utiliza en clasificadores &lt;code&gt;n_class - 1&lt;/code&gt; . Las entradas &lt;code&gt;n_class - 1&lt;/code&gt; en cada fila corresponden a los coeficientes duales para estos clasificadores.</target>
        </trans-unit>
        <trans-unit id="79705c107a83df8d45ef47d82375b7c707f4d5ae" translate="yes" xml:space="preserve">
          <source>The shape of the result.</source>
          <target state="translated">La forma del resultado.</target>
        </trans-unit>
        <trans-unit id="770a88634d7b4928209cc52a1f8aea4bce68a8e8" translate="yes" xml:space="preserve">
          <source>The shift offset allows a zero threshold for being an outlier. Only available for novelty detection (when novelty is set to True). The argument X is supposed to contain &lt;em&gt;new data&lt;/em&gt;: if X contains a point from training, it considers the later in its own neighborhood. Also, the samples in X are not considered in the neighborhood of any point.</source>
          <target state="translated">El desplazamiento de cambio permite un umbral cero por ser un valor at&amp;iacute;pico. Solo disponible para la detecci&amp;oacute;n de novedades (cuando la novedad se establece en Verdadero). Se supone que el argumento X contiene &lt;em&gt;nuevos datos&lt;/em&gt; : si X contiene un punto del entrenamiento, considera el &amp;uacute;ltimo en su propio vecindario. Adem&amp;aacute;s, las muestras en X no se consideran pr&amp;oacute;ximas a ning&amp;uacute;n punto.</target>
        </trans-unit>
        <trans-unit id="fd58d8b5ba5725b529e01c853ef09676528984ae" translate="yes" xml:space="preserve">
          <source>The shifted opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">El desplazamiento opuesto al Factor Local Atípico de cada muestra de entrada.Cuanto más bajo,más anormal.Las puntuaciones negativas representan los valores atípicos,las positivas representan los valores atípicos.</target>
        </trans-unit>
        <trans-unit id="13c37fa5739748416ca3f9521f51dec2de533ef6" translate="yes" xml:space="preserve">
          <source>The similarity of two sets of biclusters.</source>
          <target state="translated">La similitud de dos conjuntos de bíceps.</target>
        </trans-unit>
        <trans-unit id="7becf18fe4f5e5f9bf982cb2425cc3a834e0c404" translate="yes" xml:space="preserve">
          <source>The simplest metric &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; model, called &lt;em&gt;absolute MDS&lt;/em&gt;, disparities are defined by \(\hat{d}_{ij} = S_{ij}\). With absolute MDS, the value \(S_{ij}\) should then correspond exactly to the distance between point \(i\) and \(j\) in the embedding point.</source>
          <target state="translated">El modelo de &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt; m&amp;eacute;trico m&amp;aacute;s simple , llamado &lt;em&gt;MDS absoluto&lt;/em&gt; , las disparidades se definen mediante \ (\ hat {d} _ {ij} = S_ {ij} \). Con MDS absoluto, el valor \ (S_ {ij} \) deber&amp;iacute;a corresponder exactamente a la distancia entre el punto \ (i \) y \ (j \) en el punto de inserci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="1d805d3b9d3166d024fed65dbe7dc1ddecd0fb40" translate="yes" xml:space="preserve">
          <source>The simplest possible classifier is the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;nearest neighbor&lt;/a&gt;: given a new observation &lt;code&gt;X_test&lt;/code&gt;, find in the training set (i.e. the data used to train the estimator) the observation with the closest feature vector. (Please see the &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors section&lt;/a&gt; of the online Scikit-learn documentation for more information about this type of classifier.)</source>
          <target state="translated">El clasificador m&amp;aacute;s simple posible es el &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;vecino m&amp;aacute;s cercano&lt;/a&gt; : dada una nueva observaci&amp;oacute;n &lt;code&gt;X_test&lt;/code&gt; , encuentre en el conjunto de entrenamiento (es decir, los datos usados ​​para entrenar al estimador) la observaci&amp;oacute;n con el vector de caracter&amp;iacute;sticas m&amp;aacute;s cercano. (Consulte la &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;secci&amp;oacute;n Vecinos m&amp;aacute;s cercanos&lt;/a&gt; de la documentaci&amp;oacute;n en l&amp;iacute;nea de Scikit-learn para obtener m&amp;aacute;s informaci&amp;oacute;n sobre este tipo de clasificador).</target>
        </trans-unit>
        <trans-unit id="1fac5c5ae1360f0509b6fb6c9bee7a89fd16eea5" translate="yes" xml:space="preserve">
          <source>The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost.</source>
          <target state="translated">La forma más simple de lograr esta reducción de la dimensionalidad es tomando una proyección aleatoria de los datos.Aunque esto permite cierto grado de visualización de la estructura de los datos,la aleatoriedad de la elección deja mucho que desear.En una proyección aleatoria,es probable que se pierda la estructura más interesante dentro de los datos.</target>
        </trans-unit>
        <trans-unit id="a8c7e68caf35057fba3785bb16a14da344816784" translate="yes" xml:space="preserve">
          <source>The simplest way to use cross-validation is to call the &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper function on the estimator and the dataset.</source>
          <target state="translated">La forma m&amp;aacute;s sencilla de utilizar la validaci&amp;oacute;n cruzada es llamar a la funci&amp;oacute;n auxiliar &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; en el estimador y el conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="6b61610397fd4bcbb9699a5ba6221c6a9dfd8212" translate="yes" xml:space="preserve">
          <source>The singular value decomposition, \(A_n = U \Sigma V^\top\), provides the partitions of the rows and columns of \(A\). A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.</source>
          <target state="translated">La descomposición del valor singular,\ ~-A_n=U \Sigma V^top\),proporciona las particiones de las filas y columnas de \ ~-A.Un subconjunto de los vectores singulares de la izquierda da las particiones de las filas,y un subconjunto de los vectores singulares de la derecha da las particiones de las columnas.</target>
        </trans-unit>
        <trans-unit id="f785df3fd21ed481c16151f3b91e613616eec382" translate="yes" xml:space="preserve">
          <source>The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the &lt;code&gt;n_components&lt;/code&gt; variables in the lower-dimensional space.</source>
          <target state="translated">Los valores singulares correspondientes a cada uno de los componentes seleccionados. Los valores singulares son iguales a las 2 normas de las &lt;code&gt;n_components&lt;/code&gt; variables en el espacio de menor dimensi&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="fda37f3c2ceb4ddf1d93860ac5de12a5ffc950e8" translate="yes" xml:space="preserve">
          <source>The size of &lt;code&gt;grid_scores_&lt;/code&gt; is equal to &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt;, where step is the number of features removed at each iteration.</source>
          <target state="translated">El tama&amp;ntilde;o de &lt;code&gt;grid_scores_&lt;/code&gt; es igual a &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt; , donde step es el n&amp;uacute;mero de caracter&amp;iacute;sticas eliminadas en cada iteraci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="9867bd15789365ba5d58a7dbdcd5815e87ddbf26" translate="yes" xml:space="preserve">
          <source>The size of the model with the default parameters is \(O( M * N * log (N) )\), where \(M\) is the number of trees and \(N\) is the number of samples. In order to reduce the size of the model, you can change these parameters: &lt;code&gt;min_samples_split&lt;/code&gt;, &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt; and &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="translated">El tama&amp;ntilde;o del modelo con los par&amp;aacute;metros predeterminados es \ (O (M * N * log (N)) \), donde \ (M \) es el n&amp;uacute;mero de &amp;aacute;rboles y \ (N \) es el n&amp;uacute;mero de muestras. Para reducir el tama&amp;ntilde;o del modelo, puede cambiar estos par&amp;aacute;metros: &lt;code&gt;min_samples_split&lt;/code&gt; , &lt;code&gt;max_leaf_nodes&lt;/code&gt; , &lt;code&gt;max_depth&lt;/code&gt; y &lt;code&gt;min_samples_leaf&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1f17f4e69474d346e8934f03ff8be8d8add88c9f" translate="yes" xml:space="preserve">
          <source>The size of the random matrix to generate.</source>
          <target state="translated">El tamaño de la matriz aleatoria a generar.</target>
        </trans-unit>
        <trans-unit id="b31452681b2b7098990045ccc11256312f76473c" translate="yes" xml:space="preserve">
          <source>The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth &lt;code&gt;h&lt;/code&gt; can capture interactions of order &lt;code&gt;h&lt;/code&gt; . There are two ways in which the size of the individual regression trees can be controlled.</source>
          <target state="translated">El tama&amp;ntilde;o de los alumnos de la base del &amp;aacute;rbol de regresi&amp;oacute;n define el nivel de interacciones variables que puede capturar el modelo de aumento de gradiente. En general, un &amp;aacute;rbol de profundidad &lt;code&gt;h&lt;/code&gt; puede capturar interacciones de orden &lt;code&gt;h&lt;/code&gt; . Hay dos formas de controlar el tama&amp;ntilde;o de los &amp;aacute;rboles de regresi&amp;oacute;n individuales.</target>
        </trans-unit>
        <trans-unit id="eb0fc3f910e5dcbfcfe6af4540bbfd0a452530ef" translate="yes" xml:space="preserve">
          <source>The size of the sample to use when computing the Silhouette Coefficient on a random subset of the data. If &lt;code&gt;sample_size is None&lt;/code&gt;, no sampling is used.</source>
          <target state="translated">El tama&amp;ntilde;o de la muestra que se utilizar&amp;aacute; al calcular el coeficiente de silueta en un subconjunto aleatorio de datos. Si &lt;code&gt;sample_size is None&lt;/code&gt; , no se usa muestreo.</target>
        </trans-unit>
        <trans-unit id="b3605bfba06cab15c1207c4102bf5bbb9eee0a53" translate="yes" xml:space="preserve">
          <source>The size of the set to sample from.</source>
          <target state="translated">El tamaño del conjunto del que se va a tomar la muestra.</target>
        </trans-unit>
        <trans-unit id="d5051ed3b169b87ed97be7e20bda0cdf9d82ecae" translate="yes" xml:space="preserve">
          <source>The size, the distance and the shape of clusters may vary upon initialization, perplexity values and does not always convey a meaning.</source>
          <target state="translated">El tamaño,la distancia y la forma de los racimos pueden variar en el momento de la inicialización,lo que genera perplejidad y no siempre transmite un significado.</target>
        </trans-unit>
        <trans-unit id="6830208b16eb851287384293d35fab1e64fb8f9a" translate="yes" xml:space="preserve">
          <source>The skewed chi squared kernel is given by:</source>
          <target state="translated">El núcleo cuadrado del chi sesgado viene dado por:</target>
        </trans-unit>
        <trans-unit id="bdf0bd9101d435249fe017ac2196fd5049ca1688" translate="yes" xml:space="preserve">
          <source>The smoothing priors \(\alpha \ge 0\) accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting \(\alpha = 1\) is called Laplace smoothing, while \(\alpha &amp;lt; 1\) is called Lidstone smoothing.</source>
          <target state="translated">El suavizado a priors \ (\ alpha \ ge 0 \) tiene en cuenta las caracter&amp;iacute;sticas que no est&amp;aacute;n presentes en las muestras de aprendizaje y evita probabilidades cero en c&amp;aacute;lculos posteriores. La configuraci&amp;oacute;n de \ (\ alpha = 1 \) se denomina suavizado de Laplace, mientras que \ (\ alpha &amp;lt;1 \) se denomina suavizado de Lidstone.</target>
        </trans-unit>
        <trans-unit id="4842d9fce84143997f4f4321a8717acf3b670822" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For L1 penalization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="translated">El solucionador &quot;liblinear&quot; utiliza un algoritmo de descenso de coordenadas (CD) y se basa en la excelente &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;biblioteca&lt;/a&gt; C ++ LIBLINEAR , que se env&amp;iacute;a con scikit-learn. Sin embargo, el algoritmo de CD implementado en liblinear no puede aprender un verdadero modelo multinomial (multiclase); en cambio, el problema de optimizaci&amp;oacute;n se descompone en una forma de &quot;uno frente al resto&quot;, por lo que se entrenan clasificadores binarios separados para todas las clases. Esto sucede bajo el cap&amp;oacute;, por lo que las instancias de &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; que&lt;/a&gt; usan este solucionador se comportan como clasificadores multiclase. Para la penalizaci&amp;oacute;n de L1, &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt; &lt;/a&gt; permite calcular el l&amp;iacute;mite inferior de C para obtener un modelo no &quot;nulo&quot; (todos los pesos de caracter&amp;iacute;sticas a cero).</target>
        </trans-unit>
        <trans-unit id="ed7150245f4b6e455142137cba3f8af716071c7a" translate="yes" xml:space="preserve">
          <source>The solver for weight optimization.</source>
          <target state="translated">El solucionador para la optimización del peso.</target>
        </trans-unit>
        <trans-unit id="7be0c6e2677e69b218f8a95f391cfbcac99c36a1" translate="yes" xml:space="preserve">
          <source>The solvers implemented in the class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; are &amp;ldquo;liblinear&amp;rdquo;, &amp;ldquo;newton-cg&amp;rdquo;, &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;saga&amp;rdquo;:</source>
          <target state="translated">Los solucionadores implementados en la clase &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; son &quot;liblinear&quot;, &quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;sag&quot; y &quot;saga&quot;:</target>
        </trans-unit>
        <trans-unit id="2562620e10231c5093b1e84475f4d077e3e41917" translate="yes" xml:space="preserve">
          <source>The sought maximum memory for temporary distance matrix chunks. When None (default), the value of &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; is used.</source>
          <target state="translated">La memoria m&amp;aacute;xima buscada para fragmentos de matriz de distancia temporal. Cuando Ninguno (predeterminado), se &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; el valor de sklearn.get_config () ['working_memory'] .</target>
        </trans-unit>
        <trans-unit id="d35f1b775d7d16bbabc524099db10f7048897e6d" translate="yes" xml:space="preserve">
          <source>The source can also be found &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;on Github&lt;/a&gt;.</source>
          <target state="translated">La fuente tambi&amp;eacute;n se puede encontrar &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;en Github&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="351456aed7b7d0fc1d0034839135c70dde192f05" translate="yes" xml:space="preserve">
          <source>The source of this tutorial can be found within your scikit-learn folder:</source>
          <target state="translated">La fuente de este tutorial se encuentra en su carpeta de aprendizaje de ciencias:</target>
        </trans-unit>
        <trans-unit id="aa3459bc05f5ce02810c5dff6ad6cfbeb1ca9a04" translate="yes" xml:space="preserve">
          <source>The spacing between points of the grid, in degrees</source>
          <target state="translated">El espacio entre los puntos de la cuadrícula,en grados</target>
        </trans-unit>
        <trans-unit id="464028094b69433f3db44d7a263916deccf86cb6" translate="yes" xml:space="preserve">
          <source>The sparse code factor in the matrix factorization.</source>
          <target state="translated">El factor de código escaso en la factorización de la matriz.</target>
        </trans-unit>
        <trans-unit id="f9d9c0a7d6ff3b6246478d922234360e15ed6ab9" translate="yes" xml:space="preserve">
          <source>The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-zero items (X).</source>
          <target state="translated">El código es tan escaso que cada columna de esta matriz tiene exactamente n_nonzero_coefs elementos que no son cero (X).</target>
        </trans-unit>
        <trans-unit id="ee7d500960f94d10a937d21e436dcc2adbbc9a2a" translate="yes" xml:space="preserve">
          <source>The sparse codes</source>
          <target state="translated">Los códigos dispersos</target>
        </trans-unit>
        <trans-unit id="0e1ce62165b4e0dbd0b6b1199e83c88e1c88959d" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results than the dense implementation due to a shrunk learning rate for the intercept.</source>
          <target state="translated">La implementación dispersa produce resultados ligeramente diferentes que la implementación densa,debido a la disminución de la tasa de aprendizaje de la intercepción.</target>
        </trans-unit>
        <trans-unit id="e4890de0c56a0e7645deea9088355f84adac629f" translate="yes" xml:space="preserve">
          <source>The sparse vector</source>
          <target state="translated">El vector disperso</target>
        </trans-unit>
        <trans-unit id="df0fd56f5b70016c4466d03cfc8859f7c3ea7663" translate="yes" xml:space="preserve">
          <source>The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself.</source>
          <target state="translated">La escasez se impone en realidad sobre el factor coloso de la matriz.Por lo tanto,el alfa no se traduce directamente en la fracción de relleno de la propia matriz.</target>
        </trans-unit>
        <trans-unit id="56d99fb198a4da207bde97c2ceeeb3653451f496" translate="yes" xml:space="preserve">
          <source>The sparsity-inducing \(\ell_1\) norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter &lt;code&gt;alpha&lt;/code&gt;. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero.</source>
          <target state="translated">La norma \ (\ ell_1 \) que induce a la dispersi&amp;oacute;n tambi&amp;eacute;n evita el ruido de los componentes de aprendizaje cuando hay pocas muestras de entrenamiento disponibles. El grado de penalizaci&amp;oacute;n (y por lo tanto la escasez) se puede ajustar a trav&amp;eacute;s del hiperpar&amp;aacute;metro &lt;code&gt;alpha&lt;/code&gt; . Los valores peque&amp;ntilde;os conducen a una factorizaci&amp;oacute;n suavemente regularizada, mientras que los valores m&amp;aacute;s grandes reducen muchos coeficientes a cero.</target>
        </trans-unit>
        <trans-unit id="1e3f3420100e85d456b50524681a1e1c7bbc338a" translate="yes" xml:space="preserve">
          <source>The split code for a single sample has length &lt;code&gt;2 * n_components&lt;/code&gt; and is constructed using the following rule: First, the regular code of length &lt;code&gt;n_components&lt;/code&gt; is computed. Then, the first &lt;code&gt;n_components&lt;/code&gt; entries of the &lt;code&gt;split_code&lt;/code&gt; are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative.</source>
          <target state="translated">El c&amp;oacute;digo dividido para una sola muestra tiene una longitud de &lt;code&gt;2 * n_components&lt;/code&gt; y se construye usando la siguiente regla: Primero, se calcula el c&amp;oacute;digo regular de longitud &lt;code&gt;n_components&lt;/code&gt; . Luego, las primeras &lt;code&gt;n_components&lt;/code&gt; entradas del &lt;code&gt;split_code&lt;/code&gt; se rellenan con la parte positiva del vector de c&amp;oacute;digo regular. La segunda mitad del c&amp;oacute;digo dividido se llena con la parte negativa del vector de c&amp;oacute;digo, solo con un signo positivo. Por lo tanto, split_code no es negativo.</target>
        </trans-unit>
        <trans-unit id="619ea10ad996c929a97e389d99c020b2211157d4" translate="yes" xml:space="preserve">
          <source>The standard LLE algorithm comprises three stages:</source>
          <target state="translated">El algoritmo estándar del LLE comprende tres etapas:</target>
        </trans-unit>
        <trans-unit id="a5bdc246aecc7e3bec9121428d3f9c450814299f" translate="yes" xml:space="preserve">
          <source>The standard deviation of the clusters.</source>
          <target state="translated">La desviación estándar de los grupos.</target>
        </trans-unit>
        <trans-unit id="b61516300476f785958503bfbc63e8e96de11d18" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise applied to the output.</source>
          <target state="translated">La desviación estándar del ruido gaussiano aplicada a la salida.</target>
        </trans-unit>
        <trans-unit id="e5c05e9131e22b6718043e99a2bae1b92b538981" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise.</source>
          <target state="translated">La desviación estándar del ruido gaussiano.</target>
        </trans-unit>
        <trans-unit id="3689d32cff3e62dded279fb6b657e94942cdc7dc" translate="yes" xml:space="preserve">
          <source>The stepwise interpolating function that covers the input domain &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">La funci&amp;oacute;n de paso a paso de interpolaci&amp;oacute;n que cubre el dominio de entrada &lt;code&gt;X&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4d8cf8d82bcb0bc25727f81a7a8d626fa0a70639" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.</source>
          <target state="translated">El criterio de parada. Si no es None, las iteraciones se detendr&amp;aacute;n cuando (loss&amp;gt; previous_loss - tol). Por defecto es Ninguno. El valor predeterminado es 1e-3 desde 0,21.</target>
        </trans-unit>
        <trans-unit id="ba1aa4a75fb51f2479b447aa001f5e6386a1f799" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization.</source>
          <target state="translated">La estrategia a utilizar para asignar etiquetas en el espacio de incrustación.Hay dos maneras de asignar etiquetas después de la incrustación laplaciana.Se pueden aplicar los medios k y es una opción popular.Pero también puede ser sensible a la inicialización.La discretización es otro enfoque que es menos sensible a la inicialización aleatoria.</target>
        </trans-unit>
        <trans-unit id="0a02a9e0399d776c0fdc23972d432af0d079552e" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the &amp;lsquo;Multiclass spectral clustering&amp;rsquo; paper referenced below for more details on the discretization approach.</source>
          <target state="translated">La estrategia a utilizar para asignar etiquetas en el espacio de incrustaci&amp;oacute;n. Hay dos formas de asignar etiquetas despu&amp;eacute;s de la incrustaci&amp;oacute;n laplaciana. k-means se puede aplicar y es una opci&amp;oacute;n popular. Pero tambi&amp;eacute;n puede ser sensible a la inicializaci&amp;oacute;n. La discretizaci&amp;oacute;n es otro enfoque que es menos sensible a la inicializaci&amp;oacute;n aleatoria. Consulte el documento 'Agrupaci&amp;oacute;n espectral multiclase' al que se hace referencia a continuaci&amp;oacute;n para obtener m&amp;aacute;s detalles sobre el enfoque de discretizaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="067b32fbfa4aafe8139a50a2cd0dc62d48f6ce7c" translate="yes" xml:space="preserve">
          <source>The strategy used to choose the split at each node. Supported strategies are &amp;ldquo;best&amp;rdquo; to choose the best split and &amp;ldquo;random&amp;rdquo; to choose the best random split.</source>
          <target state="translated">La estrategia utilizada para elegir la divisi&amp;oacute;n en cada nodo. Las estrategias admitidas son &quot;mejores&quot; para elegir la mejor divisi&amp;oacute;n y &quot;aleatorias&quot; para elegir la mejor divisi&amp;oacute;n aleatoria.</target>
        </trans-unit>
        <trans-unit id="e4b87188ae01dfb2ea23e8aa9287a121677cbc35" translate="yes" xml:space="preserve">
          <source>The strength of recall versus precision in the F-score.</source>
          <target state="translated">La fuerza de la memoria versus la precisión en el puntaje F.</target>
        </trans-unit>
        <trans-unit id="9380c762fbcadf9826438d5ff503ef39938c2642" translate="yes" xml:space="preserve">
          <source>The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.</source>
          <target state="translated">La fuerza del algoritmo de la LOF es que tiene en cuenta las propiedades locales y globales de los conjuntos de datos:puede funcionar bien incluso en conjuntos de datos en los que las muestras anormales tienen diferentes densidades subyacentes.La pregunta no es cuán aislada está la muestra,sino cuán aislada está con respecto al vecindario circundante.</target>
        </trans-unit>
        <trans-unit id="ef559a266ab9339add9416970528df4288b9fe07" translate="yes" xml:space="preserve">
          <source>The string to decode</source>
          <target state="translated">La cadena a decodificar</target>
        </trans-unit>
        <trans-unit id="71af025de90c113777cd083adeb4a7dc41c643ae" translate="yes" xml:space="preserve">
          <source>The string value &amp;ldquo;auto&amp;rdquo; determines whether y should increase or decrease based on the Spearman correlation estimate&amp;rsquo;s sign.</source>
          <target state="translated">El valor de la cadena &quot;auto&quot; determina si y debe aumentar o disminuir seg&amp;uacute;n el signo de la estimaci&amp;oacute;n de correlaci&amp;oacute;n de Spearman.</target>
        </trans-unit>
        <trans-unit id="68913ceeaf791c0f89f5da0e6ea267a6c64604e5" translate="yes" xml:space="preserve">
          <source>The submatrix corresponding to bicluster i.</source>
          <target state="translated">La submatriz correspondiente al bicluster i.</target>
        </trans-unit>
        <trans-unit id="a10436d8ac7a1230da5ccfb4c02351e0bfbb4701" translate="yes" xml:space="preserve">
          <source>The subset of drawn features for each base estimator.</source>
          <target state="translated">El subconjunto de características dibujadas para cada estimador base.</target>
        </trans-unit>
        <trans-unit id="57675bd8615ef838a99f713d239feb9810e82664" translate="yes" xml:space="preserve">
          <source>The subset of drawn samples for each base estimator.</source>
          <target state="translated">El subconjunto de muestras extraídas para cada estimador de base.</target>
        </trans-unit>
        <trans-unit id="58b06737b1ee81d6af2156c3790929db0bc0c464" translate="yes" xml:space="preserve">
          <source>The sum of the features (number of words if documents) is drawn from a Poisson distribution with this expected value.</source>
          <target state="translated">La suma de las características (número de palabras si se trata de documentos)se extrae de una distribución de Poisson con este valor esperado.</target>
        </trans-unit>
        <trans-unit id="690aa5752a21a529c84a7d2bed72d6956dfbf2f1" translate="yes" xml:space="preserve">
          <source>The support is the number of occurrences of each class in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">El soporte es el n&amp;uacute;mero de ocurrencias de cada clase en &lt;code&gt;y_true&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b8f29f24bf343b8a8c6a9702bbb3373c30b3886a" translate="yes" xml:space="preserve">
          <source>The support vector machines in scikit-learn support both dense (&lt;code&gt;numpy.ndarray&lt;/code&gt; and convertible to that by &lt;code&gt;numpy.asarray&lt;/code&gt;) and sparse (any &lt;code&gt;scipy.sparse&lt;/code&gt;) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered &lt;code&gt;numpy.ndarray&lt;/code&gt; (dense) or &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (sparse) with &lt;code&gt;dtype=float64&lt;/code&gt;.</source>
          <target state="translated">Las m&amp;aacute;quinas de vectores de soporte en scikit-learn admiten vectores de muestra densos ( &lt;code&gt;numpy.ndarray&lt;/code&gt; y convertibles a eso por &lt;code&gt;numpy.asarray&lt;/code&gt; ) y dispersos (cualquier &lt;code&gt;scipy.sparse&lt;/code&gt; ) como entrada. Sin embargo, para usar una SVM para hacer predicciones para datos escasos, debe haber encajado en dichos datos. Para un rendimiento &amp;oacute;ptimo, use &lt;code&gt;numpy.ndarray&lt;/code&gt; (denso) ordenado en C o &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (disperso) con &lt;code&gt;dtype=float64&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b8e93494175fc764f9336519319d2c72af2d3469" translate="yes" xml:space="preserve">
          <source>The target features for which the partial dependecy should be computed (size should be smaller than 3 for visual renderings).</source>
          <target state="translated">Las características del objetivo para las que se debe computar la dependencia parcial (el tamaño debe ser inferior a 3 para las representaciones visuales).</target>
        </trans-unit>
        <trans-unit id="c6694d34ee3bb1ae73f71f475a9064ea6bb5aa61" translate="yes" xml:space="preserve">
          <source>The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.</source>
          <target state="translated">El objetivo se predice mediante la interpolación local de los objetivos asociados de los vecinos más cercanos en el conjunto de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="836251639a1d2dd67d806e7910ad6656af016095" translate="yes" xml:space="preserve">
          <source>The target values (class labels in classification, real numbers in regression).</source>
          <target state="translated">Los valores objetivo (etiquetas de clase en la clasificación,números reales en la regresión).</target>
        </trans-unit>
        <trans-unit id="581ab16d01401b52de3a8a770522ae3215b0d2a3" translate="yes" xml:space="preserve">
          <source>The target values (class labels) as integers or strings.</source>
          <target state="translated">Los valores objetivo (etiquetas de clase)como números enteros o cadenas.</target>
        </trans-unit>
        <trans-unit id="c42151a6b9abff52d7ec8ba07a5200409a48a3c6" translate="yes" xml:space="preserve">
          <source>The target values (class labels).</source>
          <target state="translated">Los valores objetivo (etiquetas de clase).</target>
        </trans-unit>
        <trans-unit id="af111c076ceef9b210aa6e236368271feda238ba" translate="yes" xml:space="preserve">
          <source>The target values (integers that correspond to classes in classification, real numbers in regression).</source>
          <target state="translated">Los valores objetivo (números enteros que corresponden a clases en la clasificación,números reales en la regresión).</target>
        </trans-unit>
        <trans-unit id="94e263fd180ba7303394b0318de33e42ec7bd528" translate="yes" xml:space="preserve">
          <source>The target values (real numbers).</source>
          <target state="translated">Los valores objetivo (números reales).</target>
        </trans-unit>
        <trans-unit id="4f52bd7c58bfce68aafb35b3e2f5dd531ddc572f" translate="yes" xml:space="preserve">
          <source>The target values (real numbers). Use &lt;code&gt;dtype=np.float64&lt;/code&gt; and &lt;code&gt;order='C'&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">Los valores objetivo (n&amp;uacute;meros reales). Utilice &lt;code&gt;dtype=np.float64&lt;/code&gt; y &lt;code&gt;order='C'&lt;/code&gt; para obtener la m&amp;aacute;xima eficiencia.</target>
        </trans-unit>
        <trans-unit id="53a447e32fbe96637e9d7be27a641b24353eba6e" translate="yes" xml:space="preserve">
          <source>The target values.</source>
          <target state="translated">Los valores del objetivo.</target>
        </trans-unit>
        <trans-unit id="863f0df40c91ba7090e8eb769163050f217d7bc5" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems.</source>
          <target state="translated">La variable objetivo para los problemas de aprendizaje supervisado.</target>
        </trans-unit>
        <trans-unit id="f60efda845afa3f0dc7adc040cd9b2450fbcc2aa" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems. Stratification is done based on the y labels.</source>
          <target state="translated">La variable objetivo para los problemas de aprendizaje supervisado.La estratificación se hace en base a las etiquetas y.</target>
        </trans-unit>
        <trans-unit id="aa20ad6bc67663b59dda9b76a4a065ca1f86af8f" translate="yes" xml:space="preserve">
          <source>The target variable is the median house value for California districts.</source>
          <target state="translated">La variable objetivo es el valor medio de la vivienda para los distritos de California.</target>
        </trans-unit>
        <trans-unit id="40a696d29fc2e13fd302babe7b309a6f769a208a" translate="yes" xml:space="preserve">
          <source>The target variable to try to predict in the case of supervised learning.</source>
          <target state="translated">La variable objetivo a tratar de predecir en el caso del aprendizaje supervisado.</target>
        </trans-unit>
        <trans-unit id="a34872d2673c11b79098f51c73d69310c6a49da3" translate="yes" xml:space="preserve">
          <source>The task at hand is to predict disease progression from physiological variables.</source>
          <target state="translated">La tarea que tenemos entre manos es predecir la progresión de la enfermedad a partir de variables fisiológicas.</target>
        </trans-unit>
        <trans-unit id="609ac3008273ee503e4809fb5a54a64b3f8be85d" translate="yes" xml:space="preserve">
          <source>The ten features are standard independent Gaussian and the target &lt;code&gt;y&lt;/code&gt; is defined by:</source>
          <target state="translated">Las diez caracter&amp;iacute;sticas son gaussianas independientes est&amp;aacute;ndar y el objetivo &lt;code&gt;y&lt;/code&gt; est&amp;aacute; definido por:</target>
        </trans-unit>
        <trans-unit id="b7581ec7a4d469cfac096612cf94e3958274d6aa" translate="yes" xml:space="preserve">
          <source>The term &amp;ldquo;discrete features&amp;rdquo; is used instead of naming them &amp;ldquo;categorical&amp;rdquo;, because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that.</source>
          <target state="translated">El t&amp;eacute;rmino &quot;caracter&amp;iacute;sticas discretas&quot; se utiliza en lugar de denominarlas &quot;categ&amp;oacute;ricas&quot;, porque describe la esencia con mayor precisi&amp;oacute;n. Por ejemplo, las intensidades de p&amp;iacute;xeles de una imagen son caracter&amp;iacute;sticas discretas (pero dif&amp;iacute;cilmente categ&amp;oacute;ricas) y obtendr&amp;aacute; mejores resultados si las marca como tales. Tambi&amp;eacute;n tenga en cuenta que tratar una variable continua como discreta y viceversa generalmente dar&amp;aacute; resultados incorrectos, as&amp;iacute; que est&amp;eacute; atento a eso.</target>
        </trans-unit>
        <trans-unit id="1748689c159b7c280435a293a84c60a8fa11ddf6" translate="yes" xml:space="preserve">
          <source>The test points for the data. Same format as the training data.</source>
          <target state="translated">Los puntos de prueba para los datos.El mismo formato que los datos de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="c4412981e13c1e09172f9e595c07a27a82a32abb" translate="yes" xml:space="preserve">
          <source>The testing set indices for that split.</source>
          <target state="translated">Las pruebas establecieron índices para esa división.</target>
        </trans-unit>
        <trans-unit id="c079148b8f468ab34a1c911c5b93e7fb1e4fbf43" translate="yes" xml:space="preserve">
          <source>The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; takes an &lt;code&gt;encoding&lt;/code&gt; parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (&lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt;).</source>
          <target state="translated">Los extractores de funciones de texto en scikit-learn saben c&amp;oacute;mo decodificar archivos de texto, pero solo si usted les dice en qu&amp;eacute; codificaci&amp;oacute;n est&amp;aacute;n los archivos. &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; toma un par&amp;aacute;metro de &lt;code&gt;encoding&lt;/code&gt; para este prop&amp;oacute;sito. Para archivos de texto modernos, la codificaci&amp;oacute;n correcta es probablemente UTF-8, que por lo tanto es la predeterminada ( &lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="5da204cc914d9d1b370d2a7e3d3f9fed2399ad38" translate="yes" xml:space="preserve">
          <source>The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow.</source>
          <target state="translated">La teoría dice que para lograr la consistencia de la predicción,el parámetro de penalización debe mantenerse constante a medida que el número de muestras crece.</target>
        </trans-unit>
        <trans-unit id="ed8c4048d538066672a56cef623687584e4d51a1" translate="yes" xml:space="preserve">
          <source>The third figure compares kernel density estimates for a distribution of 100 samples in 1 dimension. Though this example uses 1D distributions, kernel density estimation is easily and efficiently extensible to higher dimensions as well.</source>
          <target state="translated">La tercera figura compara las estimaciones de la densidad del núcleo para una distribución de 100 muestras en una dimensión.Aunque este ejemplo utiliza distribuciones 1D,la estimación de la densidad del núcleo es fácil y eficiente de extender a dimensiones mayores también.</target>
        </trans-unit>
        <trans-unit id="0742a25a1bc73abd7670f1b33a5e8f933c99aa55" translate="yes" xml:space="preserve">
          <source>The third model is also a Bayesian Gaussian mixture model with a Dirichlet process prior but this time the value of the concentration prior is higher giving the model more liberty to model the fine-grained structure of the data. The result is a mixture with a larger number of active components that is similar to the first model where we arbitrarily decided to fix the number of components to 10.</source>
          <target state="translated">El tercer modelo es también un modelo de mezcla bayesiana gaussiana con un proceso Dirichlet previo,pero esta vez el valor de la concentración previa es mayor,lo que da al modelo más libertad para modelar la estructura de grano fino de los datos.El resultado es una mezcla con un mayor número de componentes activos que es similar al primer modelo en el que arbitrariamente decidimos fijar el número de componentes en 10.</target>
        </trans-unit>
        <trans-unit id="f1f7cd9ed7ac82273a71a8430edb1e09f61b8cab" translate="yes" xml:space="preserve">
          <source>The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If &amp;ldquo;median&amp;rdquo; (resp. &amp;ldquo;mean&amp;rdquo;), then the &lt;code&gt;threshold&lt;/code&gt; value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., &amp;ldquo;1.25*mean&amp;rdquo;) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, &amp;ldquo;mean&amp;rdquo; is used by default.</source>
          <target state="translated">El valor de umbral que se utilizar&amp;aacute; para la selecci&amp;oacute;n de funciones. Los rasgos cuya importancia es mayor o igual se mantienen mientras que los dem&amp;aacute;s se descartan. Si es &quot;mediana&quot; (resp. &quot;Media&quot;), entonces el valor &lt;code&gt;threshold&lt;/code&gt; es la mediana (resp. La media) de las caracter&amp;iacute;sticas importantes. Tambi&amp;eacute;n se puede utilizar un factor de escala (por ejemplo, &quot;1,25 * media&quot;). Si Ninguno y si el estimador tiene una penalizaci&amp;oacute;n de par&amp;aacute;metro establecida en l1, ya sea expl&amp;iacute;cita o impl&amp;iacute;citamente (por ejemplo, Lasso), el umbral utilizado es 1e-5. De lo contrario, se utiliza &quot;mean&quot; de forma predeterminada.</target>
        </trans-unit>
        <trans-unit id="91a1a785c1bc7a11d4e20e6e119c885bf4142111" translate="yes" xml:space="preserve">
          <source>The threshold value used for feature selection.</source>
          <target state="translated">El valor umbral utilizado para la selección de características.</target>
        </trans-unit>
        <trans-unit id="5a48126d50b83afd18e220a4fcf0cc7ffc7180d9" translate="yes" xml:space="preserve">
          <source>The time complexity of this implementation is &lt;code&gt;O(d ** 2)&lt;/code&gt; assuming d ~ n_features ~ n_components.</source>
          <target state="translated">La complejidad temporal de esta implementaci&amp;oacute;n es &lt;code&gt;O(d ** 2)&lt;/code&gt; asumiendo d ~ n_features ~ n_components.</target>
        </trans-unit>
        <trans-unit id="ff8097e0ec52614ae168c4cd444f04e3f78b14e2" translate="yes" xml:space="preserve">
          <source>The time for fitting the estimator on the train set for each cv split.</source>
          <target state="translated">El tiempo para ajustar el estimador en el tren establecido para cada división de cv.</target>
        </trans-unit>
        <trans-unit id="dc0535c66e14595ad26a449dc475b0e83c5091ba" translate="yes" xml:space="preserve">
          <source>The time for scoring the estimator on the test set for each cv split. (Note time for scoring on the train set is not included even if &lt;code&gt;return_train_score&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">El tiempo para calificar el estimador en el conjunto de prueba para cada divisi&amp;oacute;n de cv. (Tenga en cuenta que el tiempo de puntuaci&amp;oacute;n en el conjunto de trenes no se incluye incluso si &lt;code&gt;return_train_score&lt;/code&gt; se establece en &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1cd8d7a025bee860396ab665c8f7316a009f2f5a" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;.</source>
          <target state="translated">La tolerancia para el solucionador de red el&amp;aacute;stica utilizada para calcular la direcci&amp;oacute;n de descenso. Este par&amp;aacute;metro controla la precisi&amp;oacute;n de la direcci&amp;oacute;n de b&amp;uacute;squeda para una actualizaci&amp;oacute;n de columna determinada, no de la estimaci&amp;oacute;n general del par&amp;aacute;metro. Solo se usa para mode = 'cd'.</target>
        </trans-unit>
        <trans-unit id="54800ee92ded7e21b226653650e94d2e245f5dc0" translate="yes" xml:space="preserve">
          <source>The tolerance for the optimization: if the updates are smaller than &lt;code&gt;tol&lt;/code&gt;, the optimization code checks the dual gap for optimality and continues until it is smaller than &lt;code&gt;tol&lt;/code&gt;.</source>
          <target state="translated">La tolerancia para la optimizaci&amp;oacute;n: si las actualizaciones son m&amp;aacute;s peque&amp;ntilde;as que &lt;code&gt;tol&lt;/code&gt; , el c&amp;oacute;digo de optimizaci&amp;oacute;n verifica la optimizaci&amp;oacute;n del espacio dual y contin&amp;uacute;a hasta que es m&amp;aacute;s peque&amp;ntilde;o que &lt;code&gt;tol&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7abdf76511114d1a589dafaf8c3b9fea94410d4e" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped.</source>
          <target state="translated">La tolerancia para declarar la convergencia:si la brecha dual baja de este valor,las iteraciones se detienen.</target>
        </trans-unit>
        <trans-unit id="1174cbfc6c5cd8bb9e51e269de526360c060c73a" translate="yes" xml:space="preserve">
          <source>The tomography projection operation is a linear transformation. In addition to the data-fidelity term corresponding to a linear regression, we penalize the L1 norm of the image to account for its sparsity. The resulting optimization problem is called the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;. We use the class &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt;&lt;/a&gt;, that uses the coordinate descent algorithm. Importantly, this implementation is more computationally efficient on a sparse matrix, than the projection operator used here.</source>
          <target state="translated">La operaci&amp;oacute;n de proyecci&amp;oacute;n de tomograf&amp;iacute;a es una transformaci&amp;oacute;n lineal. Adem&amp;aacute;s del t&amp;eacute;rmino de fidelidad de datos correspondiente a una regresi&amp;oacute;n lineal, penalizamos la norma L1 de la imagen para dar cuenta de su escasez. El problema de optimizaci&amp;oacute;n resultante se llama &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lazo&lt;/a&gt; . Usamos la clase &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt; &lt;/a&gt; , que usa el algoritmo de descenso de coordenadas. Es importante destacar que esta implementaci&amp;oacute;n es m&amp;aacute;s eficiente computacionalmente en una matriz dispersa que el operador de proyecci&amp;oacute;n que se usa aqu&amp;iacute;.</target>
        </trans-unit>
        <trans-unit id="6067d7bbaca20238e188da5c4c1f4f9b915cbe46" translate="yes" xml:space="preserve">
          <source>The total number of features.</source>
          <target state="translated">El número total de características.</target>
        </trans-unit>
        <trans-unit id="52b059ac9ef2b76cd7b57a438578db960faf18a1" translate="yes" xml:space="preserve">
          <source>The total number of features. These comprise &lt;code&gt;n_informative&lt;/code&gt; informative features, &lt;code&gt;n_redundant&lt;/code&gt; redundant features, &lt;code&gt;n_repeated&lt;/code&gt; duplicated features and &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; useless features drawn at random.</source>
          <target state="translated">El n&amp;uacute;mero total de funciones. Estos comprenden &lt;code&gt;n_informative&lt;/code&gt; caracter&amp;iacute;sticas informativas informativas, &lt;code&gt;n_redundant&lt;/code&gt; caracter&amp;iacute;sticas redundantes redundantes, &lt;code&gt;n_repeated&lt;/code&gt; caracter&amp;iacute;sticas duplicadas &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; y n_features-n_informative-n_redundant-n_repeated caracter&amp;iacute;sticas in&amp;uacute;tiles extra&amp;iacute;das al azar.</target>
        </trans-unit>
        <trans-unit id="1b95194d002b1fa90370f5ac460e11ae527d46c4" translate="yes" xml:space="preserve">
          <source>The total number of input features.</source>
          <target state="translated">El número total de características de entrada.</target>
        </trans-unit>
        <trans-unit id="75f4c14304ea0320f6751402bd1abbcf764fb2d4" translate="yes" xml:space="preserve">
          <source>The total number of points equally divided among classes.</source>
          <target state="translated">El número total de puntos dividido equitativamente entre las clases.</target>
        </trans-unit>
        <trans-unit id="6fd1a66b2c352f2c105828a73ec9201a20677da3" translate="yes" xml:space="preserve">
          <source>The total number of points generated.</source>
          <target state="translated">El número total de puntos generados.</target>
        </trans-unit>
        <trans-unit id="b463685bc7194f4e1bd9c9e9566855d8af2442c3" translate="yes" xml:space="preserve">
          <source>The total number of points generated. If odd, the inner circle will have one point more than the outer circle.</source>
          <target state="translated">El número total de puntos generados.Si es impar,el círculo interior tendrá un punto más que el círculo exterior.</target>
        </trans-unit>
        <trans-unit id="30df1d74b9688e22831b35a50354a3af5ea3a9b9" translate="yes" xml:space="preserve">
          <source>The total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features.</source>
          <target state="translated">El número total de características de salida de polinomios.El número de características de salida se calcula por iteración sobre todas las combinaciones de tamaño adecuado de las características de entrada.</target>
        </trans-unit>
        <trans-unit id="2341cd15b4f720c4e4ca39627124f453602ba043" translate="yes" xml:space="preserve">
          <source>The traditional way to compute the principal eigenvector is to use the power iteration method:</source>
          <target state="translated">La forma tradicional de calcular el vector principal es usar el método de iteración de energía:</target>
        </trans-unit>
        <trans-unit id="486bc8b3be25b906a521777296790c0e7db3392b" translate="yes" xml:space="preserve">
          <source>The training algorithm implemented in &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt; is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:</source>
          <target state="translated">El algoritmo de entrenamiento implementado en &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; se conoce como m&amp;aacute;xima verosimilitud estoc&amp;aacute;stica (SML) o divergencia contrastante persistente (PCD). La optimizaci&amp;oacute;n de la probabilidad m&amp;aacute;xima directamente no es factible debido a la forma de la probabilidad de los datos:</target>
        </trans-unit>
        <trans-unit id="03936b9df8c7a04402a186159fb53bde128b3907" translate="yes" xml:space="preserve">
          <source>The training data</source>
          <target state="translated">Los datos de entrenamiento</target>
        </trans-unit>
        <trans-unit id="a7dac9ee1d012d3a04e96a076bd4bb0255f4fa3a" translate="yes" xml:space="preserve">
          <source>The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.</source>
          <target state="translated">Los datos de entrenamiento contienen valores atípicos que se definen como observaciones que están lejos de las demás.Los estimadores de detección de valores atípicos intentan así ajustarse a las regiones en las que los datos de formación están más concentrados,ignorando las observaciones desviadas.</target>
        </trans-unit>
        <trans-unit id="58b640148d7ae1e7f191ee9d800aaef902a6aef0" translate="yes" xml:space="preserve">
          <source>The training data is not polluted by outliers and we are interested in detecting whether a &lt;strong&gt;new&lt;/strong&gt; observation is an outlier. In this context an outlier is also called a novelty.</source>
          <target state="translated">Los datos de entrenamiento no est&amp;aacute;n contaminados por valores at&amp;iacute;picos y estamos interesados ​​en detectar si una &lt;strong&gt;nueva&lt;/strong&gt; observaci&amp;oacute;n es un valor at&amp;iacute;pico. En este contexto, un valor at&amp;iacute;pico tambi&amp;eacute;n se denomina novedad.</target>
        </trans-unit>
        <trans-unit id="431a9d8e158a3b53d56cef19b322f4a781297a84" translate="yes" xml:space="preserve">
          <source>The training data, e.g. a reference to an immutable snapshot</source>
          <target state="translated">Los datos de entrenamiento,por ejemplo,una referencia a una instantánea inmutable</target>
        </trans-unit>
        <trans-unit id="44e706b0239b6ac2fad3df8ff059d735dbfbf296" translate="yes" xml:space="preserve">
          <source>The training input samples.</source>
          <target state="translated">Las muestras de entrada del entrenamiento.</target>
        </trans-unit>
        <trans-unit id="92c6c3d94fd9608db44f0ede45f9bbe4de664d1d" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">Las muestras de entrada de formaci&amp;oacute;n. Internamente, se convertir&amp;aacute; a &lt;code&gt;dtype=np.float32&lt;/code&gt; y si se proporciona una matriz dispersa a una &lt;code&gt;csc_matrix&lt;/code&gt; dispersa .</target>
        </trans-unit>
        <trans-unit id="fdce812a7f2c7f82334342af14e6e75d01b61ef1" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">Las muestras de entrada de formaci&amp;oacute;n. Internamente, su dtype se convertir&amp;aacute; a &lt;code&gt;dtype=np.float32&lt;/code&gt; . Si se proporciona una matriz dispersa, se convertir&amp;aacute; en una &lt;code&gt;csc_matrix&lt;/code&gt; dispersa .</target>
        </trans-unit>
        <trans-unit id="549582a79c03c6cf3a88a3f5ae8ab3477fe6f4f0" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.</source>
          <target state="translated">Las muestras de entrada del entrenamiento.Las matrices dispersas son aceptadas sólo si son apoyadas por el estimador de base.</target>
        </trans-unit>
        <trans-unit id="ed9fac6749e01b46bdd0e4eebbecd7ed42fc3bac" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.</source>
          <target state="translated">Las muestras de entrada del entrenamiento.La matriz dispersa puede ser CSC,CSR,COO,DOK,o LIL.DOK y LIL se convierten en CSR.</target>
        </trans-unit>
        <trans-unit id="dba95905d10a3a6d61cb924560b16f35840a95de" translate="yes" xml:space="preserve">
          <source>The training points for the data. Each point has three fields:</source>
          <target state="translated">Los puntos de entrenamiento para los datos.Cada punto tiene tres campos:</target>
        </trans-unit>
        <trans-unit id="49afe52f19d67077f586b0d86a3a80bb7b9051b3" translate="yes" xml:space="preserve">
          <source>The training set has size &lt;code&gt;i * n_samples // (n_splits + 1)
+ n_samples % (n_splits + 1)&lt;/code&gt; in the &lt;code&gt;i``th split,
with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">El conjunto de entrenamiento tiene un tama&amp;ntilde;o &lt;code&gt;i * n_samples // (n_splits + 1) + n_samples % (n_splits + 1)&lt;/code&gt; en la &lt;code&gt;i``th split, with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt; , donde &lt;code&gt;n_samples&lt;/code&gt; es el n&amp;uacute;mero de muestras.</target>
        </trans-unit>
        <trans-unit id="7d0d65f5b4df90d39c92efe2c541dfa70477a3f3" translate="yes" xml:space="preserve">
          <source>The training set indices for that split.</source>
          <target state="translated">El entrenamiento estableció índices para esa división.</target>
        </trans-unit>
        <trans-unit id="1fe1c9ebc66c3c60358a4b7d7ce6958b71f5be6a" translate="yes" xml:space="preserve">
          <source>The transformation can be triggered by setting either &lt;code&gt;transformer&lt;/code&gt; or the pair of functions &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt;. However, setting both options will raise an error.</source>
          <target state="translated">La transformaci&amp;oacute;n se puede activar configurando &lt;code&gt;transformer&lt;/code&gt; o el par de funciones &lt;code&gt;func&lt;/code&gt; e &lt;code&gt;inverse_func&lt;/code&gt; . Sin embargo, configurar ambas opciones generar&amp;aacute; un error.</target>
        </trans-unit>
        <trans-unit id="c1ce2cd56f04ea729576a873f5ef18af794ea62c" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. The cumulative density function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="translated">La transformación se aplica a cada característica de forma independiente.La función de densidad acumulada de un rasgo se utiliza para proyectar los valores originales.Los valores de las características de los datos nuevos/no vistos que caen por debajo o por encima del rango ajustado se asignarán a los límites de la distribución de salida.Nótese que esta transformación es no lineal.Puede distorsionar las correlaciones lineales entre las variables medidas en la misma escala,pero hace que las variables medidas a diferentes escalas sean más directamente comparables.</target>
        </trans-unit>
        <trans-unit id="ddd72b82186574150d900e6dfd8721345fa62073" translate="yes" xml:space="preserve">
          <source>The transformation is given by:</source>
          <target state="translated">La transformación viene dada por:</target>
        </trans-unit>
        <trans-unit id="1db1e441acce63afd6d696933447c0b2d453bb8c" translate="yes" xml:space="preserve">
          <source>The transformed data</source>
          <target state="translated">Los datos transformados</target>
        </trans-unit>
        <trans-unit id="0a52b9359f518099e81b711eeaccc5e0c7c17eb0" translate="yes" xml:space="preserve">
          <source>The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.</source>
          <target state="translated">Los datos transformados se utilizan luego para entrenar un clasificador Bayes ingenuo,y se observa una clara diferencia en la precisión de las predicciones en que el conjunto de datos que se escalan antes de la ACP supera ampliamente a la versión no escalada.</target>
        </trans-unit>
        <trans-unit id="80e852f03673351dd5ee00932a57dc4a209cdf2d" translate="yes" xml:space="preserve">
          <source>The transformed data.</source>
          <target state="translated">Los datos transformados.</target>
        </trans-unit>
        <trans-unit id="831c10597508e086776cd00760f56c708f8d2bba" translate="yes" xml:space="preserve">
          <source>The tree algorithm to use. Valid options are [&amp;lsquo;kd_tree&amp;rsquo;|&amp;rsquo;ball_tree&amp;rsquo;|&amp;rsquo;auto&amp;rsquo;]. Default is &amp;lsquo;auto&amp;rsquo;.</source>
          <target state="translated">El algoritmo de &amp;aacute;rbol a utilizar. Las opciones v&amp;aacute;lidas son ['kd_tree' | 'ball_tree' | 'auto']. El valor predeterminado es &quot;autom&amp;aacute;tico&quot;.</target>
        </trans-unit>
        <trans-unit id="d745501ed4c4af3a67688bd307560f44fb29c904" translate="yes" xml:space="preserve">
          <source>The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node.</source>
          <target state="translated">La estructura de datos del árbol consiste en nodos,y cada nodo consta de varios subconjuntos.El número máximo de subconjuntos en un nodo está determinado por el factor de ramificación.Cada subclúster mantiene una suma lineal,una suma cuadrada y el número de muestras en ese subclúster.Además,cada subclúster también puede tener un nodo como hijo,si el subclúster no es miembro de un nodo de hoja.</target>
        </trans-unit>
        <trans-unit id="fbc6ef1abc08faa2debda525a78475d34850618e" translate="yes" xml:space="preserve">
          <source>The true probability in each bin (fraction of positives).</source>
          <target state="translated">La verdadera probabilidad en cada recipiente (fracción de positivos).</target>
        </trans-unit>
        <trans-unit id="89aec1004fa9767eced66561c3ed3161000c45a1" translate="yes" xml:space="preserve">
          <source>The true score without permuting targets.</source>
          <target state="translated">La verdadera puntuación sin objetivos permanentes.</target>
        </trans-unit>
        <trans-unit id="311da69a1a719737450cc586668ba277c4bde011" translate="yes" xml:space="preserve">
          <source>The tutorial folder should contain the following sub-folders:</source>
          <target state="translated">La carpeta del tutorial debe contener las siguientes subcarpetas:</target>
        </trans-unit>
        <trans-unit id="2e8b6623a8185bbb598cfdf6b3f71f1ee2c2a837" translate="yes" xml:space="preserve">
          <source>The two figures below plot the values of &lt;code&gt;C&lt;/code&gt; on the &lt;code&gt;x-axis&lt;/code&gt; and the corresponding cross-validation scores on the &lt;code&gt;y-axis&lt;/code&gt;, for several different fractions of a generated data-set.</source>
          <target state="translated">Las dos figuras siguientes trazan los valores de &lt;code&gt;C&lt;/code&gt; en el &lt;code&gt;x-axis&lt;/code&gt; las puntuaciones de validaci&amp;oacute;n cruzada correspondientes en el &lt;code&gt;y-axis&lt;/code&gt; , para varias fracciones diferentes de un conjunto de datos generado.</target>
        </trans-unit>
        <trans-unit id="3ed5804ea6261422d9ec471fffa8f8a41a307d64" translate="yes" xml:space="preserve">
          <source>The two species are:</source>
          <target state="translated">Las dos especies son:</target>
        </trans-unit>
        <trans-unit id="4aa36c5d8992165f7895075f7b16a37f9864b092" translate="yes" xml:space="preserve">
          <source>The type of criterion to use.</source>
          <target state="translated">El tipo de criterio a utilizar.</target>
        </trans-unit>
        <trans-unit id="6bc8093d847369045cfca5abe49cd94f035f902d" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument.</source>
          <target state="translated">El tipo de valores de las características.Pasado a los constructores de matrices Numpy array/scipy.sparse como el argumento dtype.</target>
        </trans-unit>
        <trans-unit id="51cef5c60b8823e16a67a0821a4b5311abdcd5ed" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.</source>
          <target state="translated">El tipo de valores de las características.Pasado a los constructores de matrices scipy.sparse como el argumento dtype.No lo configure como bool,np.boolean o cualquier tipo de entero sin signo.</target>
        </trans-unit>
        <trans-unit id="25d83e9ee3b7a100418b9b6d2bb7936470b09825" translate="yes" xml:space="preserve">
          <source>The type of norm used to compute the error. Available error types: - &amp;lsquo;frobenius&amp;rsquo; (default): sqrt(tr(A^t.A)) - &amp;lsquo;spectral&amp;rsquo;: sqrt(max(eigenvalues(A^t.A)) where A is the error &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt;.</source>
          <target state="translated">El tipo de norma utilizada para calcular el error. Tipos de error disponibles: - 'frobenius' (predeterminado): sqrt (tr (A ^ tA)) - 'spectral': sqrt (max (valores propios (A ^ tA)) donde A es el error &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f0b5e941b7e074477ab9a734aa8fbc101aeb347c" translate="yes" xml:space="preserve">
          <source>The unchanged dictionary atoms</source>
          <target state="translated">Los átomos del diccionario sin cambios</target>
        </trans-unit>
        <trans-unit id="776a52daee9321c0497d40c79109e87f34af6b7e" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="translated">La implementaci&amp;oacute;n subyacente de &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; utiliza un generador de n&amp;uacute;meros aleatorios para seleccionar caracter&amp;iacute;sticas cuando se ajusta el modelo con un descenso de coordenadas dual (es decir, cuando &lt;code&gt;dual&lt;/code&gt; se establece en &lt;code&gt;True&lt;/code&gt; ). Por lo tanto, no es raro tener resultados ligeramente diferentes para los mismos datos de entrada. Si eso sucede, intente con un par&amp;aacute;metro tol m&amp;aacute;s peque&amp;ntilde;o. Esta aleatoriedad tambi&amp;eacute;n se puede controlar con el par&amp;aacute;metro &lt;code&gt;random_state&lt;/code&gt; . Cuando &lt;code&gt;dual&lt;/code&gt; se establece en &lt;code&gt;False&lt;/code&gt; , la implementaci&amp;oacute;n subyacente de &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; no es aleatoria y &lt;code&gt;random_state&lt;/code&gt; no tiene ning&amp;uacute;n efecto en los resultados.</target>
        </trans-unit>
        <trans-unit id="68c40938fee4ae1976f2ebe09bc5c5a404f12d37" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter.</source>
          <target state="translated">La implementaci&amp;oacute;n de C subyacente utiliza un generador de n&amp;uacute;meros aleatorios para seleccionar caracter&amp;iacute;sticas al ajustar el modelo. Por lo tanto, no es raro tener resultados ligeramente diferentes para los mismos datos de entrada. Si eso sucede, intente con un par&amp;aacute;metro &lt;code&gt;tol&lt;/code&gt; m&amp;aacute;s peque&amp;ntilde;o .</target>
        </trans-unit>
        <trans-unit id="2f30e1e8a65635cf877334fb55d57ec3cedb0460" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.</source>
          <target state="translated">La implementación C subyacente utiliza un generador de números aleatorios para seleccionar las características cuando se ajusta el modelo.Por lo tanto,no es raro tener resultados ligeramente diferentes para los mismos datos de entrada.Si eso ocurre,inténtelo con un parámetro de tolerancia más pequeño.</target>
        </trans-unit>
        <trans-unit id="4ed849766bc74a6b6038e401c289ce378db99dcc" translate="yes" xml:space="preserve">
          <source>The underlying Tree object. Please refer to &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; for attributes of Tree object and &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;Understanding the decision tree structure&lt;/a&gt; for basic usage of these attributes.</source>
          <target state="translated">El objeto Tree subyacente. Consulte la &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; para conocer los atributos del objeto &amp;Aacute;rbol y &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;Comprender la estructura del &amp;aacute;rbol de decisiones&lt;/a&gt; para el uso b&amp;aacute;sico de estos atributos.</target>
        </trans-unit>
        <trans-unit id="eaa9f70240ca573ce446579a8e3077ce3fd3b2de" translate="yes" xml:space="preserve">
          <source>The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.</source>
          <target state="translated">La implementación subyacente es MurmurHash3_x86_32 generando un hash de 32 bits de baja latencia adecuado para la implementación de tablas de búsqueda,filtros Bloom,sketch de conteo min o hash de características.</target>
        </trans-unit>
        <trans-unit id="8b27403493b3e1cf67d088cabd49f20f60beabb5" translate="yes" xml:space="preserve">
          <source>The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy.</source>
          <target state="translated">La implementación subyacente,liblinear,utiliza una escasa representación interna para los datos que incurrirán en una copia de memoria.</target>
        </trans-unit>
        <trans-unit id="22bad6cd0a0fca07f198954a6d755d20a0363412" translate="yes" xml:space="preserve">
          <source>The univariate position of the sample according to the main dimension of the points in the manifold.</source>
          <target state="translated">La posición univariante de la muestra según la dimensión principal de los puntos del colector.</target>
        </trans-unit>
        <trans-unit id="e1bff9cf5ae34029868daf8dfe6afee04fc2666d" translate="yes" xml:space="preserve">
          <source>The unmixing matrix.</source>
          <target state="translated">La matriz de desmiembración.</target>
        </trans-unit>
        <trans-unit id="4c5425dc309e3cab0153df4bbf4dcfe21bae1aa0" translate="yes" xml:space="preserve">
          <source>The unsupervised data reduction and the supervised estimator can be chained in one step. See &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline: chaining estimators&lt;/a&gt;.</source>
          <target state="translated">La reducci&amp;oacute;n de datos no supervisada y el estimador supervisado se pueden encadenar en un solo paso. Consulte &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline: encadenamiento de estimadores&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="fca2372915cd9dbf5bde8febd27812d65a387223" translate="yes" xml:space="preserve">
          <source>The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for &lt;code&gt;x&lt;/code&gt; to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide).</source>
          <target state="translated">La figura superior izquierda ilustra las predicciones (en rojo oscuro) de un &amp;uacute;nico &amp;aacute;rbol de decisi&amp;oacute;n entrenado sobre un conjunto de datos aleatorio LS (los puntos azules) de un problema de regresi&amp;oacute;n 1d de juguete. Tambi&amp;eacute;n ilustra las predicciones (en rojo claro) de otros &amp;aacute;rboles de decisi&amp;oacute;n individuales entrenados sobre otras (y diferentes) instancias LS del problema dibujadas al azar. Intuitivamente, el t&amp;eacute;rmino de varianza aqu&amp;iacute; corresponde al ancho del haz de predicciones (en rojo claro) de los estimadores individuales. Cuanto mayor es la varianza, m&amp;aacute;s sensibles son las predicciones para &lt;code&gt;x&lt;/code&gt; a peque&amp;ntilde;os cambios en el conjunto de entrenamiento. El t&amp;eacute;rmino de sesgo corresponde a la diferencia entre la predicci&amp;oacute;n media del estimador (en cian) y el mejor modelo posible (en azul oscuro). En este problema, podemos observar que el sesgo es bastante bajo (las curvas cian y azul est&amp;aacute;n cerca una de la otra) mientras que la varianza es grande (el haz rojo es bastante ancho).</target>
        </trans-unit>
        <trans-unit id="1125be6cb70e3f587339b27b5931d4f81c0d0d9b" translate="yes" xml:space="preserve">
          <source>The usage of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="translated">El uso de &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; se describe en detalle en &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7012369902c9ebdfcf4a40c832356073db31ae9a" translate="yes" xml:space="preserve">
          <source>The usage of centroid distance limits the distance metric to Euclidean space.</source>
          <target state="translated">El uso de la distancia centroide limita la distancia métrica al espacio euclidiano.</target>
        </trans-unit>
        <trans-unit id="4f2ed911398b7e07d993b089b964fc574c420c21" translate="yes" xml:space="preserve">
          <source>The usage of the &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt;&lt;code&gt;SkewedChi2Sampler&lt;/code&gt;&lt;/a&gt; is the same as the usage described above for the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;. The only difference is in the free parameter, that is called \(c\). For a motivation for this mapping and the mathematical details see &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt;.</source>
          <target state="translated">El uso de &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt; &lt;code&gt;SkewedChi2Sampler&lt;/code&gt; &lt;/a&gt; es el mismo que el descrito anteriormente para &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; . La &amp;uacute;nica diferencia est&amp;aacute; en el par&amp;aacute;metro libre, que se llama \ (c \). Para una motivaci&amp;oacute;n para este mapeo y los detalles matem&amp;aacute;ticos, consulte &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="fabe9be99adc410c1907ea1daa7576972e4b19e8" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">El uso de vecinos m&amp;aacute;s cercanos de m&amp;uacute;ltiples salidas para la regresi&amp;oacute;n se demuestra en &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Compleci&amp;oacute;n&lt;/a&gt; de caras con estimadores de m&amp;uacute;ltiples salidas . En este ejemplo, las entradas X son los p&amp;iacute;xeles de la mitad superior de las caras y las salidas Y son los p&amp;iacute;xeles de la mitad inferior de esas caras.</target>
        </trans-unit>
        <trans-unit id="adaab44ae672254ed2b0f1474d8d83b9d0ff364a" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">El uso de &amp;aacute;rboles de m&amp;uacute;ltiples salidas para la clasificaci&amp;oacute;n se demuestra en la &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;finalizaci&amp;oacute;n de Face con estimadores de m&amp;uacute;ltiples salidas&lt;/a&gt; . En este ejemplo, las entradas X son los p&amp;iacute;xeles de la mitad superior de las caras y las salidas Y son los p&amp;iacute;xeles de la mitad inferior de esas caras.</target>
        </trans-unit>
        <trans-unit id="e18a96fd5f8412fd8540943bfa9bb84448ef6eef" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for regression is demonstrated in &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;Multi-output Decision Tree Regression&lt;/a&gt;. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.</source>
          <target state="translated">El uso de &amp;aacute;rboles de m&amp;uacute;ltiples salidas para la regresi&amp;oacute;n se demuestra en &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;Regresi&amp;oacute;n de &amp;aacute;rboles de decisi&amp;oacute;n de m&amp;uacute;ltiples salidas&lt;/a&gt; . En este ejemplo, la entrada X es un valor real &amp;uacute;nico y las salidas Y son el seno y el coseno de X.</target>
        </trans-unit>
        <trans-unit id="6d9188616eccce81f2896ac591395f1642a23655" translate="yes" xml:space="preserve">
          <source>The used categories can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute.</source>
          <target state="translated">Las categor&amp;iacute;as utilizadas se pueden encontrar en el atributo &lt;code&gt;categories_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ada4ba90a5864aca46a2fd4279e91da24ee2ef32" translate="yes" xml:space="preserve">
          <source>The user-provided initial means, defaults to None, If it None, means are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">Los medios iniciales proporcionados por el usuario, por defecto es Ninguno, si es Ninguno, los medios se inicializan utilizando el m&amp;eacute;todo &lt;code&gt;init_params&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a1cbbec0505b4f4802950c392df8579b38a3220d" translate="yes" xml:space="preserve">
          <source>The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the &amp;lsquo;init_params&amp;rsquo; method. The shape depends on &amp;lsquo;covariance_type&amp;rsquo;:</source>
          <target state="translated">Las precisiones iniciales proporcionadas por el usuario (inversas de las matrices de covarianza), por defecto son Ninguna. Si es None, las precisiones se inicializan usando el m&amp;eacute;todo 'init_params'. La forma depende de 'covariance_type':</target>
        </trans-unit>
        <trans-unit id="930a8f090bbd3f5ab357e6a55436cb80968a37a5" translate="yes" xml:space="preserve">
          <source>The user-provided initial weights, defaults to None. If it None, weights are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">Los pesos iniciales proporcionados por el usuario, predeterminados son Ninguno. Si es None, los pesos se inicializan usando el m&amp;eacute;todo &lt;code&gt;init_params&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="845cdaf2c42f719deb3141928cabec5996b4eedb" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.</source>
          <target state="translated">La estimación habitual de la máxima probabilidad de covarianza puede regularizarse utilizando la contracción.Ledoit y Wolf propusieron una fórmula cercana para calcular el parámetro de contracción asintóticamente óptimo (minimizando un criterio de MSE),lo que dio como resultado la estimación de covarianza de Ledoit-Wolf.</target>
        </trans-unit>
        <trans-unit id="5d9f8abe9cd1fbb176b951540baae1f3defa7962" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set.</source>
          <target state="translated">La estimaci&amp;oacute;n de m&amp;aacute;xima verosimilitud de la covarianza habitual es muy sensible a la presencia de valores at&amp;iacute;picos en el conjunto de datos. En tal caso, ser&amp;iacute;a mejor utilizar un estimador robusto de covarianza para garantizar que la estimaci&amp;oacute;n sea resistente a observaciones &quot;err&amp;oacute;neas&quot; en el conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="bf61b06542d3e7e313e8463ad6cb36679683eb16" translate="yes" xml:space="preserve">
          <source>The utility function &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt;&lt;code&gt;make_pipeline&lt;/code&gt;&lt;/a&gt; is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:</source>
          <target state="translated">La funci&amp;oacute;n de utilidad &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt; &lt;code&gt;make_pipeline&lt;/code&gt; &lt;/a&gt; es una forma abreviada de construir tuber&amp;iacute;as; toma un n&amp;uacute;mero variable de estimadores y devuelve una canalizaci&amp;oacute;n, completando los nombres autom&amp;aacute;ticamente:</target>
        </trans-unit>
        <trans-unit id="e6fa170ec90d321cecf4d3db13ca13c99c71342b" translate="yes" xml:space="preserve">
          <source>The valid distance metrics, and the function they map to, are:</source>
          <target state="translated">Las métricas de distancia válidas,y la función a la que se asignan,son:</target>
        </trans-unit>
        <trans-unit id="7aae4519886038a4fe27266a449c263068305fb0" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.</source>
          <target state="translated">El valor 2 tiene la puntuación más alta:aparece dos veces con pesos de 1,5 y 2:la suma de estos es 3.</target>
        </trans-unit>
        <trans-unit id="cf2aaa6a69c13c7a1da598bbc487a099c24264e9" translate="yes" xml:space="preserve">
          <source>The value 4 appears three times: with uniform weights, the result is simply the mode of the distribution.</source>
          <target state="translated">El valor 4 aparece tres veces:con pesos uniformes,el resultado es simplemente el modo de la distribución.</target>
        </trans-unit>
        <trans-unit id="ee4d7bc4aea75382ac7c13c2d3ccdb7c85b05695" translate="yes" xml:space="preserve">
          <source>The value by which &lt;code&gt;|y - X'w - c|&lt;/code&gt; is scaled down.</source>
          <target state="translated">El valor por el cual &lt;code&gt;|y - X'w - c|&lt;/code&gt; se reduce.</target>
        </trans-unit>
        <trans-unit id="208f4b20d150446e32489b5141e41ca0c231e069" translate="yes" xml:space="preserve">
          <source>The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor.</source>
          <target state="translated">El valor del criterio de inercia asociado a la partición elegida (si compute_labels se establece en True).La inercia se define como la suma de las distancias cuadradas de las muestras a su vecino más cercano.</target>
        </trans-unit>
        <trans-unit id="605e5e84334ac11a6b1bcf93811952a1f4c93232" translate="yes" xml:space="preserve">
          <source>The value of the information criteria (&amp;lsquo;aic&amp;rsquo;, &amp;lsquo;bic&amp;rsquo;) across all alphas. The alpha which has the smallest information criterion is chosen. This value is larger by a factor of &lt;code&gt;n_samples&lt;/code&gt; compared to Eqns. 2.15 and 2.16 in (Zou et al, 2007).</source>
          <target state="translated">El valor de los criterios de informaci&amp;oacute;n ('aic', 'bic') en todos los alfa. Se elige el alfa que tiene el criterio de informaci&amp;oacute;n m&amp;aacute;s peque&amp;ntilde;o. Este valor es mayor en un factor de &lt;code&gt;n_samples&lt;/code&gt; comparaci&amp;oacute;n con las ecuaciones. 2,15 y 2,16 en (Zou et al, 2007).</target>
        </trans-unit>
        <trans-unit id="31a3294fc25a32d76657f5de9f45e5deb67968f8" translate="yes" xml:space="preserve">
          <source>The value of the largest coefficient.</source>
          <target state="translated">El valor del mayor coeficiente.</target>
        </trans-unit>
        <trans-unit id="35a39b75c4cf1051868175c0eb7c4d08a5d8c4c6" translate="yes" xml:space="preserve">
          <source>The value of the smallest coefficient.</source>
          <target state="translated">El valor del coeficiente más pequeño.</target>
        </trans-unit>
        <trans-unit id="d764819e9c8551a1ae19a24b5d4cd3ac77d8b2aa" translate="yes" xml:space="preserve">
          <source>The values corresponding the quantiles of reference.</source>
          <target state="translated">Los valores correspondientes a los cuantiles de referencia.</target>
        </trans-unit>
        <trans-unit id="e64c591fdc6bfea9ce8a9d182cdb9d3354d8a90b" translate="yes" xml:space="preserve">
          <source>The values listed by the ValueError exception correspond to the functions measuring prediction accuracy described in the following sections. The scorer objects for those functions are stored in the dictionary &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt;.</source>
          <target state="translated">Los valores enumerados por la excepci&amp;oacute;n ValueError corresponden a las funciones que miden la precisi&amp;oacute;n de la predicci&amp;oacute;n descritas en las siguientes secciones. Los objetos de &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt; para esas funciones se almacenan en el diccionario sklearn.metrics.SCORERS .</target>
        </trans-unit>
        <trans-unit id="ad8b80eff2782593fcb8941c5fc504eacc683633" translate="yes" xml:space="preserve">
          <source>The values of the parameter that will be evaluated.</source>
          <target state="translated">Los valores del parámetro que será evaluado.</target>
        </trans-unit>
        <trans-unit id="896e61b11bd1cccb5014a25cacfbd49baded2da1" translate="yes" xml:space="preserve">
          <source>The values to be assigned to each cluster of samples</source>
          <target state="translated">Los valores que se asignarán a cada grupo de muestras</target>
        </trans-unit>
        <trans-unit id="de1379c1aa59c6a0f9d415d70cec6205d70e58b1" translate="yes" xml:space="preserve">
          <source>The variance for each feature in the training set. Used to compute &lt;code&gt;scale_&lt;/code&gt;. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_std=False&lt;/code&gt;.</source>
          <target state="translated">La varianza de cada caracter&amp;iacute;stica en el conjunto de entrenamiento. Se usa para calcular &lt;code&gt;scale_&lt;/code&gt; . Igual a &lt;code&gt;None&lt;/code&gt; cuando &lt;code&gt;with_std=False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6909f327165fbbe5fe9d7a24773b9839e3b76030" translate="yes" xml:space="preserve">
          <source>The variance of the training samples transformed by a projection to each component.</source>
          <target state="translated">La variación de las muestras de entrenamiento transformadas por una proyección a cada componente.</target>
        </trans-unit>
        <trans-unit id="73778e0f44de95a7d306fdc5c4bc68ceb4bf8f71" translate="yes" xml:space="preserve">
          <source>The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">Los valores variables de los coeficientes a lo largo del camino. No est&amp;aacute; presente si el par&amp;aacute;metro &lt;code&gt;fit_path&lt;/code&gt; es &lt;code&gt;False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f193fe45abdd49c251c120544e6bc124439f7f88" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;lsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;lsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="translated">El vector \ (h_i \) se llama &quot;latente&quot; porque no se observa. \ (\ epsilon \) se considera un t&amp;eacute;rmino de ruido distribuido seg&amp;uacute;n un gaussiano con media 0 y covarianza \ (\ Psi \) (es decir, \ (\ epsilon \ sim \ mathcal {N} (0, \ Psi) \)), \ (\ mu \) es un vector de desplazamiento arbitrario. Este modelo se llama &quot;generativo&quot; porque describe c&amp;oacute;mo se genera \ (x_i \) a partir de \ (h_i \). Si usamos todas las \ (x_i \) como columnas para formar una matriz \ (\ mathbf {X} \) y todas las \ (h_i \) como columnas de una matriz \ (\ mathbf {H} \ ) entonces podemos escribir (con \ (\ mathbf {M} \) y \ (\ mathbf {E} \) adecuadamente definidos):</target>
        </trans-unit>
        <trans-unit id="4545cfee48c8d7d2034c72caee40a8d82cfcf4ce" translate="yes" xml:space="preserve">
          <source>The verbose setting on the MiniBatchKMeans enables us to see that some clusters are reassigned during the successive calls to partial-fit. This is because the number of patches that they represent has become too low, and it is better to choose a random new cluster.</source>
          <target state="translated">El verborreico ajuste en el MiniBatchKMeans nos permite ver que algunos grupos son reasignados durante las sucesivas llamadas a ajuste parcial.Esto se debe a que el número de parches que representan se ha vuelto demasiado bajo,y es mejor elegir un nuevo clúster al azar.</target>
        </trans-unit>
        <trans-unit id="e4c09c360935c392206a8639f0a0b8f4c48b519d" translate="yes" xml:space="preserve">
          <source>The verbosity level</source>
          <target state="translated">El nivel de verbosidad</target>
        </trans-unit>
        <trans-unit id="4b4be8a44f0a986b95a00a99e1db7cc81cee43d8" translate="yes" xml:space="preserve">
          <source>The verbosity level.</source>
          <target state="translated">El nivel de verborrea.</target>
        </trans-unit>
        <trans-unit id="bca220a25d0b85cbc18bcc37bc0c66babd623c39" translate="yes" xml:space="preserve">
          <source>The verbosity level. The default, zero, means silent mode.</source>
          <target state="translated">El nivel de verborrea.El valor predeterminado,cero,significa modo silencioso.</target>
        </trans-unit>
        <trans-unit id="b790263c39903969cb477edb620406690c93cb40" translate="yes" xml:space="preserve">
          <source>The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported.</source>
          <target state="translated">El nivel de verbosidad:si no es cero,se imprimen mensajes de progreso.Por encima de 50,la salida se envía a stdout.La frecuencia de los mensajes aumenta con el nivel de verbosidad.Si es superior a 10,se informa de todas las iteraciones.</target>
        </trans-unit>
        <trans-unit id="ac67f0eccad920e701e068f47d28e090c55e23b1" translate="yes" xml:space="preserve">
          <source>The verbosity mode of the function. By default that of the memory object is used.</source>
          <target state="translated">El modo de verbosidad de la función.Por defecto se utiliza el del objeto de memoria.</target>
        </trans-unit>
        <trans-unit id="31fbaba32a3bec24c49d9ccad5d6e7edcbc904dc" translate="yes" xml:space="preserve">
          <source>The versions of scikit-learn and its dependencies</source>
          <target state="translated">Las versiones de scikit-learn y sus dependencias</target>
        </trans-unit>
        <trans-unit id="6a1686cc9631522f215a91033f5d185a9b750f85" translate="yes" xml:space="preserve">
          <source>The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns:</source>
          <target state="translated">El vocabulario extraído por este vectorizador es,por lo tanto,mucho más grande y ahora puede resolver las ambigüedades codificadas en los patrones de posicionamiento local:</target>
        </trans-unit>
        <trans-unit id="16d6455593e440b1ece6b5d9b609b990b16728ff" translate="yes" xml:space="preserve">
          <source>The weighted average probabilities for a sample would then be calculated as follows:</source>
          <target state="translated">Las probabilidades medias ponderadas de una muestra se calcularían entonces de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="cf3727918257ef88c9160136d7b98b7188c886ca" translate="yes" xml:space="preserve">
          <source>The weighted impurity decrease equation is the following:</source>
          <target state="translated">La ecuación de disminución de impurezas ponderada es la siguiente:</target>
        </trans-unit>
        <trans-unit id="b71363fa16752021c27a44ccc8c03e740b0054e3" translate="yes" xml:space="preserve">
          <source>The weights \(w\) of the model can be access:</source>
          <target state="translated">Se puede acceder a los pesos del modelo:</target>
        </trans-unit>
        <trans-unit id="fd37cf293f530a725eccba4fc386300500a89671" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None)</source>
          <target state="translated">Los pesos de cada observación en X.Si no hay ninguna,todas las observaciones tienen el mismo peso (por defecto:ninguna)</target>
        </trans-unit>
        <trans-unit id="0fd9c75eb2401f4a1ca174b965b1602b9d82941e" translate="yes" xml:space="preserve">
          <source>The weights of each feature computed by the &lt;code&gt;fit&lt;/code&gt; method call are stored in a model attribute:</source>
          <target state="translated">Los pesos de cada caracter&amp;iacute;stica calculados por la llamada al m&amp;eacute;todo de &lt;code&gt;fit&lt;/code&gt; se almacenan en un atributo de modelo:</target>
        </trans-unit>
        <trans-unit id="44235eb4c2d368f2a7e225131bb791a1dc59a457" translate="yes" xml:space="preserve">
          <source>The weights of each mixture components.</source>
          <target state="translated">Los pesos de cada componente de la mezcla.</target>
        </trans-unit>
        <trans-unit id="dd426f25816730ede96a98b838186f9fc1553a50" translate="yes" xml:space="preserve">
          <source>The wine dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">El conjunto de datos del vino es un clásico y muy fácil conjunto de datos de clasificación multiclase.</target>
        </trans-unit>
        <trans-unit id="9f15574c628488edf549f3131c3e1a8bfce37b9c" translate="yes" xml:space="preserve">
          <source>The word &amp;ldquo;article&amp;rdquo; is a significant feature, based on how often people quote previous posts like this: &amp;ldquo;In article [article ID], [name] &amp;lt;[e-mail address]&amp;gt; wrote:&amp;rdquo;</source>
          <target state="translated">La palabra &quot;art&amp;iacute;culo&quot; es una caracter&amp;iacute;stica importante, seg&amp;uacute;n la frecuencia con la que las personas citan publicaciones anteriores como esta: &quot;En el art&amp;iacute;culo [ID del art&amp;iacute;culo], [nombre] &amp;lt;[direcci&amp;oacute;n de correo electr&amp;oacute;nico]&amp;gt; escribi&amp;oacute;:&quot;</target>
        </trans-unit>
        <trans-unit id="364df8b6c4c1dfcb405abf5ac32289b3f8338a10" translate="yes" xml:space="preserve">
          <source>The word &lt;em&gt;restricted&lt;/em&gt; refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:</source>
          <target state="translated">La palabra &lt;em&gt;restringida se&lt;/em&gt; refiere a la estructura bipartita del modelo, que proh&amp;iacute;be la interacci&amp;oacute;n directa entre unidades ocultas o entre unidades visibles. Esto significa que se asumen las siguientes independientes condicionales:</target>
        </trans-unit>
        <trans-unit id="92b782ef9bfc8a9813d7ea0d8efb9106d8f1896e" translate="yes" xml:space="preserve">
          <source>The word boundaries-aware variant &lt;code&gt;char_wb&lt;/code&gt; is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw &lt;code&gt;char&lt;/code&gt; variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations.</source>
          <target state="translated">La variante &lt;code&gt;char_wb&lt;/code&gt; que reconoce los l&amp;iacute;mites de las palabras es especialmente interesante para los idiomas que utilizan espacios en blanco para la separaci&amp;oacute;n de palabras, ya que genera caracter&amp;iacute;sticas significativamente menos ruidosas que la variante &lt;code&gt;char&lt;/code&gt; sin procesar en ese caso. Para tales lenguajes, puede aumentar tanto la precisi&amp;oacute;n predictiva como la velocidad de convergencia de los clasificadores entrenados con dichas caracter&amp;iacute;sticas, al tiempo que conserva la solidez con respecto a errores ortogr&amp;aacute;ficos y derivaciones de palabras.</target>
        </trans-unit>
        <trans-unit id="8e8bd120180a9cd30000d6c7bc4b945e5f585fc6" translate="yes" xml:space="preserve">
          <source>The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, &amp;lsquo;How slow is the k-means method?&amp;rsquo; SoCG2006)</source>
          <target state="translated">La complejidad del peor caso viene dada por O (n ^ (k + 2 / p)) con n = n_samples, p = n_features. (D. Arthur y S. Vassilvitskii, '&amp;iquest;Qu&amp;eacute; tan lento es el m&amp;eacute;todo k-medias?' SoCG2006)</target>
        </trans-unit>
        <trans-unit id="08b4bc51642b52ec53b43a5b2dca7586d296a859" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimator: robust multivariate regression model.</source>
          <target state="translated">Estimador Theil-Sen:modelo de regresión multivariante robusto.</target>
        </trans-unit>
        <trans-unit id="26357ed7a886288385aec0522bda7ee91031a5a9" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</source>
          <target state="translated">Estimadores de Theil-Sen en un modelo de regresi&amp;oacute;n lineal m&amp;uacute;ltiple, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang y Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bfbe9912c49db3d4af2258ea17cbacd11611139b" translate="yes" xml:space="preserve">
          <source>Theil-Sen Regression</source>
          <target state="translated">La regresión de Theil-Sen</target>
        </trans-unit>
        <trans-unit id="43407a9ed591c227c94e72cd0fbe8ae10a8a282d" translate="yes" xml:space="preserve">
          <source>TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs worse than OLS.</source>
          <target state="translated">El TheilSen es bueno para los pequeños valores atípicos,tanto en la dirección X como en la Y,pero tiene un punto de ruptura por encima del cual se desempeña peor que el OLS.</target>
        </trans-unit>
        <trans-unit id="bae71614b2af83560543a8e9bca47722a78d80c8" translate="yes" xml:space="preserve">
          <source>Their harmonic mean called &lt;strong&gt;V-measure&lt;/strong&gt; is computed by &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt;&lt;code&gt;v_measure_score&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">Su media arm&amp;oacute;nica llamada &lt;strong&gt;medida V&lt;/strong&gt; se calcula mediante &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt; &lt;code&gt;v_measure_score&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="de0b6ab722b9ec4be95cb7086fea273ea373e1de" translate="yes" xml:space="preserve">
          <source>Then fire an ipython shell and run the work-in-progress script with:</source>
          <target state="translated">Entonces dispara una concha de ipython y ejecuta el guión de trabajo en curso con:</target>
        </trans-unit>
        <trans-unit id="8f78bb6cc31961e9507c2d8e418f53fe822a9ecd" translate="yes" xml:space="preserve">
          <source>Then one can show that to classify a data point after scaling is equivalent to finding the estimated class mean \(\mu^*_k\) which is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the \(K-1\) affine subspace \(H_K\) generated by all the \(\mu^*_k\) for all classes. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a \(K-1\) dimensional space.</source>
          <target state="translated">Entonces se puede mostrar que clasificar un punto de datos después de escalar es equivalente a encontrar la media estimada de la clase \(\mu^*_k\)que está más cerca del punto de datos en la distancia euclidiana.Pero esto se puede hacer igual de bien después de proyectar en el subespacio afín (K-1)generado por todas las clases.Esto muestra que,implícito en el clasificador LDA,hay una reducción de la dimensionalidad por proyección lineal en un espacio dimensional (K-1).</target>
        </trans-unit>
        <trans-unit id="37e493a483dcc9fefbfec81c47f8c835f7340e3f" translate="yes" xml:space="preserve">
          <source>Then the Davies-Bouldin index is defined as:</source>
          <target state="translated">Entonces el índice Davies-Bouldin se define como:</target>
        </trans-unit>
        <trans-unit id="54059dffef3b64a1ba00cd6fbe5ba85d85229195" translate="yes" xml:space="preserve">
          <source>Then the metrics are defined as:</source>
          <target state="translated">Entonces las métricas se definen como:</target>
        </trans-unit>
        <trans-unit id="8da3ae858e8aee4768b456c38c7b9a98b999a912" translate="yes" xml:space="preserve">
          <source>Then the multiclass MCC is defined as:</source>
          <target state="translated">Entonces el MCC multiclase se define como:</target>
        </trans-unit>
        <trans-unit id="826d143749061228ef1caa51bd344b5a543a3ad8" translate="yes" xml:space="preserve">
          <source>Then the rows of \(Z\) are clustered using &lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;. The first &lt;code&gt;n_rows&lt;/code&gt; labels provide the row partitioning, and the remaining &lt;code&gt;n_columns&lt;/code&gt; labels provide the column partitioning.</source>
          <target state="translated">Luego, las filas de \ (Z \) se agrupan usando &lt;a href=&quot;clustering#k-means&quot;&gt;k-medias&lt;/a&gt; . Las primeras etiquetas &lt;code&gt;n_rows&lt;/code&gt; proporcionan la partici&amp;oacute;n de filas y las etiquetas &lt;code&gt;n_columns&lt;/code&gt; restantes proporcionan la partici&amp;oacute;n de columnas.</target>
        </trans-unit>
        <trans-unit id="a7548beecedd6ae525f17db272cd47fef5592b2e" translate="yes" xml:space="preserve">
          <source>Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:</source>
          <target state="translated">Luego,aplicando la norma euclidiana (L2),obtenemos los siguientes tf-idfs para el documento 1:</target>
        </trans-unit>
        <trans-unit id="743e0b74d42270bf2752b1121cf66d48c177b1df" translate="yes" xml:space="preserve">
          <source>Then, the &lt;code&gt;raw_X&lt;/code&gt; to be fed to &lt;code&gt;FeatureHasher.transform&lt;/code&gt; can be constructed using:</source>
          <target state="translated">Luego, el &lt;code&gt;raw_X&lt;/code&gt; que se alimentar&amp;aacute; a &lt;code&gt;FeatureHasher.transform&lt;/code&gt; se puede construir usando:</target>
        </trans-unit>
        <trans-unit id="4665e6f004c55fa84e60f1409a5b03c15037dd8c" translate="yes" xml:space="preserve">
          <source>Theoretical bounds</source>
          <target state="translated">Límites teóricos</target>
        </trans-unit>
        <trans-unit id="3ebe4a41a0b35192395aac8a80293b4ff462a23b" translate="yes" xml:space="preserve">
          <source>There are 3 different APIs for evaluating the quality of a model&amp;rsquo;s predictions:</source>
          <target state="translated">Hay 3 API diferentes para evaluar la calidad de las predicciones de un modelo:</target>
        </trans-unit>
        <trans-unit id="aa910de5e10f2b04644e63996efaedb310779ee6" translate="yes" xml:space="preserve">
          <source>There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let &lt;code&gt;D&lt;/code&gt; be the distance, and &lt;code&gt;S&lt;/code&gt; be the kernel:</source>
          <target state="translated">Hay varias formas de convertir entre una m&amp;eacute;trica de distancia y una medida de similitud, como un kernel. Sea &lt;code&gt;D&lt;/code&gt; la distancia y &lt;code&gt;S&lt;/code&gt; el n&amp;uacute;cleo:</target>
        </trans-unit>
        <trans-unit id="daf4e822cec5d40489080bfb83cea014bb21271c" translate="yes" xml:space="preserve">
          <source>There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):</source>
          <target state="translated">También hay un par de contras (frente al uso de un CountVectorizer con un vocabulario en memoria):</target>
        </trans-unit>
        <trans-unit id="17151dd0dcece68b8abecb55a500335bd2f90b73" translate="yes" xml:space="preserve">
          <source>There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.</source>
          <target state="translated">Hay conceptos que son difíciles de aprender porque los árboles de decisión no los expresan fácilmente,como los problemas de XOR,paridad o multiplexores.</target>
        </trans-unit>
        <trans-unit id="fbd19b439fe6ba80531cd23629fc7a9a883a8dcf" translate="yes" xml:space="preserve">
          <source>There are different things to keep in mind when dealing with data corrupted by outliers:</source>
          <target state="translated">Hay diferentes cosas que hay que tener en cuenta cuando se trata de datos corrompidos por valores atípicos:</target>
        </trans-unit>
        <trans-unit id="0fcc62fa36cf231416a60ac162100095676c743b" translate="yes" xml:space="preserve">
          <source>There are many learning routines which rely on nearest neighbors at their core. One example is &lt;a href=&quot;density#kernel-density&quot;&gt;kernel density estimation&lt;/a&gt;, discussed in the &lt;a href=&quot;density#density-estimation&quot;&gt;density estimation&lt;/a&gt; section.</source>
          <target state="translated">Hay muchas rutinas de aprendizaje que dependen fundamentalmente de los vecinos m&amp;aacute;s cercanos. Un ejemplo es la &lt;a href=&quot;density#kernel-density&quot;&gt;estimaci&amp;oacute;n de la densidad del n&amp;uacute;cleo&lt;/a&gt; , que se analiza en la secci&amp;oacute;n de &lt;a href=&quot;density#density-estimation&quot;&gt;estimaci&amp;oacute;n de&lt;/a&gt; la densidad .</target>
        </trans-unit>
        <trans-unit id="84513801465c2346e62e2a9ecaecce1cb0c3994b" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt;.</source>
          <target state="translated">Hay varios problemas conocidos en nuestra lista de palabras vac&amp;iacute;as en 'ingl&amp;eacute;s' proporcionada. Consulte &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="87d60889b216a9f6a8543438325d8902e749b92a" translate="yes" xml:space="preserve">
          <source>There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).</source>
          <target state="translated">Hay diez imágenes diferentes de cada uno de los 40 sujetos distintos.Para algunos sujetos,las imágenes se tomaron en momentos diferentes,variando la iluminación,las expresiones faciales (ojos abiertos/cerrados,sonriendo/no sonriendo)y los detalles faciales (gafas/sin gafas).Todas las imágenes se tomaron contra un fondo oscuro y homogéneo con los sujetos en posición vertical,frontal (con tolerancia para algún movimiento lateral).</target>
        </trans-unit>
        <trans-unit id="840b342f8bab010ed9a33830388b79c022d751bb" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers linear kernels, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="translated">Hay tres implementaciones diferentes de Regresi&amp;oacute;n de vectores de soporte: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt; proporciona una implementaci&amp;oacute;n m&amp;aacute;s r&amp;aacute;pida que &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; pero solo considera kernels lineales, mientras que &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt; implementa una formulaci&amp;oacute;n ligeramente diferente a &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt; . Consulte &lt;a href=&quot;#svm-implementation-details&quot;&gt;Detalles de implementaci&amp;oacute;n&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="6325cb8ad2d750db2de1f50457999e3ed60c95ad" translate="yes" xml:space="preserve">
          <source>There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.</source>
          <target state="translated">Hay tres tipos principales de interfaces de conjuntos de datos que pueden utilizarse para obtener conjuntos de datos dependiendo del tipo de conjunto de datos deseado.</target>
        </trans-unit>
        <trans-unit id="8d867308d6cfc04930e6d66867b250110233d07e" translate="yes" xml:space="preserve">
          <source>There are two options to assign labels:</source>
          <target state="translated">Hay dos opciones para asignar etiquetas:</target>
        </trans-unit>
        <trans-unit id="60a8f9c96bc7f93958ce08005db84d1ac5e8a2b4" translate="yes" xml:space="preserve">
          <source>There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.</source>
          <target state="translated">Hay dos maneras de evaluar un resultado de biclustering:interno y externo.Las medidas internas,como la estabilidad de los conglomerados,dependen sólo de los datos y del propio resultado.Actualmente no hay medidas internas de biclustering en scikit-learn.Las medidas externas se refieren a una fuente externa de información,como la verdadera solución.Cuando se trabaja con datos reales,la solución verdadera suele ser desconocida,pero los datos artificiales de bicluster pueden ser útiles para evaluar los algoritmos precisamente porque la solución verdadera es conocida.</target>
        </trans-unit>
        <trans-unit id="21ade7db23177cbafdee1241d820ab218814e247" translate="yes" xml:space="preserve">
          <source>There are two ways to specify multiple scoring metrics for the &lt;code&gt;scoring&lt;/code&gt; parameter:</source>
          <target state="translated">Hay dos formas de especificar varias m&amp;eacute;tricas de puntuaci&amp;oacute;n para el par&amp;aacute;metro de &lt;code&gt;scoring&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="cdf4947857438b0c51097ba679415b8309e576f2" translate="yes" xml:space="preserve">
          <source>There exists two types of MDS algorithm: metric and non metric. In the scikit-learn, the class &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.</source>
          <target state="translated">Existen dos tipos de algoritmo MDS: m&amp;eacute;trico y no m&amp;eacute;trico. En scikit-learn, la clase &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt; implementa ambos. En Metric MDS, la matriz de similitud de entrada surge de una m&amp;eacute;trica (y, por lo tanto, respeta la desigualdad triangular), las distancias entre dos puntos de salida se establecen para que est&amp;eacute;n lo m&amp;aacute;s cerca posible de los datos de similitud o disimilitud. En la versi&amp;oacute;n no m&amp;eacute;trica, los algoritmos intentar&amp;aacute;n preservar el orden de las distancias y, por lo tanto, buscar&amp;aacute;n una relaci&amp;oacute;n mon&amp;oacute;tona entre las distancias en el espacio incrustado y las similitudes / disimilitudes.</target>
        </trans-unit>
        <trans-unit id="9e945ec56932f8495741411aac1ef0391f41ea70" translate="yes" xml:space="preserve">
          <source>There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue.</source>
          <target state="translated">No hay absolutamente ninguna garantía de recuperar una verdad fundamental.Primero,elegir el número correcto de grupos es difícil.Segundo,el algoritmo es sensible a la inicialización,y puede caer en los mínimos locales,aunque scikit-learn emplea varios trucos para mitigar este problema.</target>
        </trans-unit>
        <trans-unit id="a5fa0b690cccf03ea1d32deadc1c15c3039ee96d" translate="yes" xml:space="preserve">
          <source>There is built-in support for sparse data given in any matrix in a format supported by &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparse&lt;/a&gt;. For maximum efficiency, however, use the CSR matrix format as defined in &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrix&lt;/a&gt;.</source>
          <target state="translated">Hay soporte incorporado para datos dispersos proporcionados en cualquier matriz en un formato compatible con &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparse&lt;/a&gt; . Sin embargo, para una m&amp;aacute;xima eficiencia, use el formato de matriz CSR como se define en &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrix&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7bf90eed7a42877d9e674dd4cae64f5ef3af07a9" translate="yes" xml:space="preserve">
          <source>There is no general rule to select an alpha parameter for recovery of non-zero coefficients. It can by set by cross-validation (&lt;code&gt;LassoCV&lt;/code&gt; or &lt;code&gt;LassoLarsCV&lt;/code&gt;), though this may lead to under-penalized models: including a small number of non-relevant variables is not detrimental to prediction score. BIC (&lt;code&gt;LassoLarsIC&lt;/code&gt;) tends, on the opposite, to set high values of alpha.</source>
          <target state="translated">No existe una regla general para seleccionar un par&amp;aacute;metro alfa para la recuperaci&amp;oacute;n de coeficientes distintos de cero. Puede establecerse mediante validaci&amp;oacute;n cruzada ( &lt;code&gt;LassoCV&lt;/code&gt; o &lt;code&gt;LassoLarsCV&lt;/code&gt; ), aunque esto puede conducir a modelos poco penalizados: incluir una peque&amp;ntilde;a cantidad de variables no relevantes no es perjudicial para la puntuaci&amp;oacute;n de predicci&amp;oacute;n. BIC ( &lt;code&gt;LassoLarsIC&lt;/code&gt; ) tiende, por el contrario, a establecer valores altos de alfa.</target>
        </trans-unit>
        <trans-unit id="ff0ab54a4c7a8a1cf35484dbae4d9dce95d5c026" translate="yes" xml:space="preserve">
          <source>There might be a difference in the scores obtained between &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;solver=liblinear&lt;/code&gt; or &lt;code&gt;LinearSVC&lt;/code&gt; and the external liblinear library directly, when &lt;code&gt;fit_intercept=False&lt;/code&gt; and the fit &lt;code&gt;coef_&lt;/code&gt; (or) the data to be predicted are zeroes. This is because for the sample(s) with &lt;code&gt;decision_function&lt;/code&gt; zero, &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;LinearSVC&lt;/code&gt; predict the negative class, while liblinear predicts the positive class. Note that a model with &lt;code&gt;fit_intercept=False&lt;/code&gt; and having many samples with &lt;code&gt;decision_function&lt;/code&gt; zero, is likely to be a underfit, bad model and you are advised to set &lt;code&gt;fit_intercept=True&lt;/code&gt; and increase the intercept_scaling.</source>
          <target state="translated">Puede haber una diferencia en las puntuaciones obtenidas entre &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; con &lt;code&gt;solver=liblinear&lt;/code&gt; o &lt;code&gt;LinearSVC&lt;/code&gt; y la biblioteca liblinear externa directamente, cuando &lt;code&gt;fit_intercept=False&lt;/code&gt; y el &lt;code&gt;coef_&lt;/code&gt; (o) los datos a predecir son ceros. Esto se debe a que para las muestras con &lt;code&gt;decision_function&lt;/code&gt; cero, &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; y &lt;code&gt;LinearSVC&lt;/code&gt; predicen la clase negativa, mientras que liblinear predice la clase positiva. Tenga en cuenta que un modelo con &lt;code&gt;fit_intercept=False&lt;/code&gt; y que tiene muchas muestras con &lt;code&gt;decision_function&lt;/code&gt; cero, es probable que sea un modelo incorrecto y no apto, y se recomienda configurar &lt;code&gt;fit_intercept=True&lt;/code&gt; y aumente el intercept_scaling.</target>
        </trans-unit>
        <trans-unit id="4a4216d2986ca1c413a45927f7f8dc07ca811204" translate="yes" xml:space="preserve">
          <source>Therefore, a logarithmic (&lt;code&gt;np.log1p&lt;/code&gt;) and an exponential function (&lt;code&gt;np.expm1&lt;/code&gt;) will be used to transform the targets before training a linear regression model and using it for prediction.</source>
          <target state="translated">Por lo tanto, se &lt;code&gt;np.log1p&lt;/code&gt; una funci&amp;oacute;n logar&amp;iacute;tmica ( np.log1p ) y una exponencial ( &lt;code&gt;np.expm1&lt;/code&gt; ) para transformar los objetivos antes de entrenar un modelo de regresi&amp;oacute;n lineal y usarlo para la predicci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="6912f2dbecf0d873a7ad1021b00fc015a0232427" translate="yes" xml:space="preserve">
          <source>These are transformers that are not intended to be used on features, only on supervised learning targets. See also &lt;a href=&quot;compose#transformed-target-regressor&quot;&gt;Transforming target in regression&lt;/a&gt; if you want to transform the prediction target for learning, but evaluate the model in the original (untransformed) space.</source>
          <target state="translated">Estos son transformadores que no est&amp;aacute;n dise&amp;ntilde;ados para usarse en funciones, solo en objetivos de aprendizaje supervisado. Consulte tambi&amp;eacute;n &lt;a href=&quot;compose#transformed-target-regressor&quot;&gt;Transformaci&amp;oacute;n del objetivo en regresi&amp;oacute;n&lt;/a&gt; si desea transformar el objetivo de predicci&amp;oacute;n para el aprendizaje, pero eval&amp;uacute;e el modelo en el espacio original (sin transformar).</target>
        </trans-unit>
        <trans-unit id="9c615abfdf912d61f49e70314e0bacb6d3789d48" translate="yes" xml:space="preserve">
          <source>These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune.</source>
          <target state="translated">Estos clasificadores son atractivos porque tienen soluciones de forma cerrada que se pueden computar fácilmente,son inherentemente multiclases,han demostrado funcionar bien en la práctica y no tienen hiperparámetros que afinar.</target>
        </trans-unit>
        <trans-unit id="fde55c65b8197fd0cbaa58300d107768af9ed389" translate="yes" xml:space="preserve">
          <source>These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high.</source>
          <target state="translated">Estas limitaciones son útiles para imponer una cierta estructura local,pero también hacen que el algoritmo sea más rápido,especialmente cuando el número de muestras es elevado.</target>
        </trans-unit>
        <trans-unit id="ebd210a6b7ae21f6cafc3c41203edaaa46e25728" translate="yes" xml:space="preserve">
          <source>These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.</source>
          <target state="translated">Estos conjuntos de datos son útiles para ilustrar rápidamente el comportamiento de los diversos algoritmos implementados en scikit-learn.Sin embargo,a menudo son demasiado pequeños para ser representativos de las tareas de aprendizaje de la máquina en el mundo real.</target>
        </trans-unit>
        <trans-unit id="8d2ab26191aa60fb366e6a03a6fce9c7d01208ba" translate="yes" xml:space="preserve">
          <source>These environment variables should be set before importing scikit-learn.</source>
          <target state="translated">Estas variables de entorno deben fijarse antes de importar Scikit-learn.</target>
        </trans-unit>
        <trans-unit id="b821932825baa77bb11f8b1f95c8cc38b1d75bf5" translate="yes" xml:space="preserve">
          <source>These estimators are called similarly to their counterparts, with &amp;lsquo;CV&amp;rsquo; appended to their name.</source>
          <target state="translated">Estos estimadores se denominan de manera similar a sus contrapartes, con 'CV' adjunto a su nombre.</target>
        </trans-unit>
        <trans-unit id="8bef6d4f1ba3e716c219b98d63998eb428a7438d" translate="yes" xml:space="preserve">
          <source>These families of algorithms are useful to find linear relations between two multivariate datasets: the &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; arguments of the &lt;code&gt;fit&lt;/code&gt; method are 2D arrays.</source>
          <target state="translated">Estas familias de algoritmos son &amp;uacute;tiles para encontrar relaciones lineales entre dos conjuntos de datos multivariados: los argumentos &lt;code&gt;X&lt;/code&gt; e &lt;code&gt;Y&lt;/code&gt; del m&amp;eacute;todo de &lt;code&gt;fit&lt;/code&gt; son matrices 2D.</target>
        </trans-unit>
        <trans-unit id="e90c7c20b495d718d6ee1761b52661a07d31f2ac" translate="yes" xml:space="preserve">
          <source>These figures aid in illustrating how a point cloud can be very flat in one direction&amp;ndash;which is where PCA comes in to choose a direction that is not flat.</source>
          <target state="translated">Estas cifras ayudan a ilustrar c&amp;oacute;mo una nube de puntos puede ser muy plana en una direcci&amp;oacute;n, que es donde entra PCA para elegir una direcci&amp;oacute;n que no sea plana.</target>
        </trans-unit>
        <trans-unit id="5b3c06102d71e9aa21ce3635f214fb7544bfe93e" translate="yes" xml:space="preserve">
          <source>These functions have an &lt;code&gt;multioutput&lt;/code&gt; keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is &lt;code&gt;'uniform_average'&lt;/code&gt;, which specifies a uniformly weighted mean over outputs. If an &lt;code&gt;ndarray&lt;/code&gt; of shape &lt;code&gt;(n_outputs,)&lt;/code&gt; is passed, then its entries are interpreted as weights and an according weighted average is returned. If &lt;code&gt;multioutput&lt;/code&gt; is &lt;code&gt;'raw_values'&lt;/code&gt; is specified, then all unaltered individual scores or losses will be returned in an array of shape &lt;code&gt;(n_outputs,)&lt;/code&gt;.</source>
          <target state="translated">Estas funciones tienen un argumento de palabra clave de &lt;code&gt;multioutput&lt;/code&gt; que especifica la forma en que se deben promediar las puntuaciones o p&amp;eacute;rdidas para cada objetivo individual. El valor predeterminado es &lt;code&gt;'uniform_average'&lt;/code&gt; , que especifica una media ponderada uniformemente sobre las salidas. Si se &lt;code&gt;ndarray&lt;/code&gt; un ndarray de forma &lt;code&gt;(n_outputs,)&lt;/code&gt; , sus entradas se interpretan como pesos y se devuelve un promedio ponderado correspondiente. Si se especifica &lt;code&gt;multioutput&lt;/code&gt; es &lt;code&gt;'raw_values'&lt;/code&gt; , entonces todos los puntajes o p&amp;eacute;rdidas individuales inalterados se devolver&amp;aacute;n en una matriz de forma &lt;code&gt;(n_outputs,)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8f744234c4fef479ca52103694dddbbb39569d67" translate="yes" xml:space="preserve">
          <source>These functions return a tuple &lt;code&gt;(X, y)&lt;/code&gt; consisting of a &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; numpy array &lt;code&gt;X&lt;/code&gt; and an array of length &lt;code&gt;n_samples&lt;/code&gt; containing the targets &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">Estas funciones devuelven una tupla &lt;code&gt;(X, y)&lt;/code&gt; consta de &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; numpy array &lt;code&gt;X&lt;/code&gt; y una matriz de longitud &lt;code&gt;n_samples&lt;/code&gt; que contiene los objetivos &lt;code&gt;y&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7ea4b087a47ece8eb67eda1c13d069b3e5f40d70" translate="yes" xml:space="preserve">
          <source>These generators produce a matrix of features and corresponding discrete targets.</source>
          <target state="translated">Estos generadores producen una matriz de características y los correspondientes objetivos discretos.</target>
        </trans-unit>
        <trans-unit id="7870342f67a34d74383fe781d3e91593569275ef" translate="yes" xml:space="preserve">
          <source>These images how similar features are merged together using feature agglomeration.</source>
          <target state="translated">Estas imágenes muestran cómo se fusionan características similares utilizando la aglomeración de características.</target>
        </trans-unit>
        <trans-unit id="2f6550a6d877a222d311c7f7d2e4efbab68b15f3" translate="yes" xml:space="preserve">
          <source>These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering (&lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;), but also to build precomputed kernels, or similarity matrices.</source>
          <target state="translated">Estas matrices se pueden utilizar para imponer conectividad en estimadores que utilizan informaci&amp;oacute;n de conectividad, como el agrupamiento de Ward ( &lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;agrupamiento jer&amp;aacute;rquico&lt;/a&gt; ), pero tambi&amp;eacute;n para construir n&amp;uacute;cleos precalculados o matrices de similitud.</target>
        </trans-unit>
        <trans-unit id="f1476efa19922a5a7b34be362fc1e06a3ae81118" translate="yes" xml:space="preserve">
          <source>These metrics &lt;strong&gt;require the knowledge of the ground truth classes&lt;/strong&gt; while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).</source>
          <target state="translated">Estas m&amp;eacute;tricas &lt;strong&gt;requieren el conocimiento de las clases de verdad b&amp;aacute;sica,&lt;/strong&gt; mientras que casi nunca est&amp;aacute;n disponibles en la pr&amp;aacute;ctica o requieren una asignaci&amp;oacute;n manual por parte de anotadores humanos (como en el entorno de aprendizaje supervisado).</target>
        </trans-unit>
        <trans-unit id="1b78634dcd7b938ef7f64bb3d2756751845f06a1" translate="yes" xml:space="preserve">
          <source>These objects take as input a scoring function that returns univariate scores and p-values (or only scores for &lt;a href=&quot;generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest&quot;&gt;&lt;code&gt;SelectKBest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile&quot;&gt;&lt;code&gt;SelectPercentile&lt;/code&gt;&lt;/a&gt;):</source>
          <target state="translated">Estos objetos toman como entrada una funci&amp;oacute;n de puntuaci&amp;oacute;n que devuelve puntuaciones univariadas y valores p (o solo puntuaciones para &lt;a href=&quot;generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest&quot;&gt; &lt;code&gt;SelectKBest&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile&quot;&gt; &lt;code&gt;SelectPercentile&lt;/code&gt; &lt;/a&gt; ):</target>
        </trans-unit>
        <trans-unit id="3403571e1d1dd77b054e08b8e749a4d9b5c4d316" translate="yes" xml:space="preserve">
          <source>These parameters can be accessed through the members &lt;code&gt;dual_coef_&lt;/code&gt; which holds the difference \(\alpha_i - \alpha_i^*\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(\rho\)</source>
          <target state="translated">Se puede acceder a estos par&amp;aacute;metros a trav&amp;eacute;s de los miembros &lt;code&gt;dual_coef_&lt;/code&gt; que contiene la diferencia \ (\ alpha_i - \ alpha_i ^ * \), &lt;code&gt;support_vectors_&lt;/code&gt; que contiene los vectores de soporte e &lt;code&gt;intercept_&lt;/code&gt; que contiene el t&amp;eacute;rmino independiente \ (\ rho \)</target>
        </trans-unit>
        <trans-unit id="9254aef96f1c8727db185406da2727e74822dda9" translate="yes" xml:space="preserve">
          <source>These quantities are also related to the (\(F_1\)) score, which is defined as the harmonic mean of precision and recall.</source>
          <target state="translated">Estas cantidades también están relacionadas con la puntuación (\(F_1\)),que se define como la media armónica de precisión y recuerdo.</target>
        </trans-unit>
        <trans-unit id="8ec6209edcb97e57d934d74900289c4f7467ca16" translate="yes" xml:space="preserve">
          <source>These represent the 14 features measured at each point of the map grid. The latitude/longitude values for the grid are discussed below. Missing data is represented by the value -9999.</source>
          <target state="translated">Estos representan los 14 rasgos medidos en cada punto de la cuadrícula del mapa.Los valores de latitud y longitud de la cuadrícula se examinan a continuación.Los datos que faltan están representados por el valor -9999.</target>
        </trans-unit>
        <trans-unit id="e11ef00d62661e429b4b798a345a9c8d62a348e6" translate="yes" xml:space="preserve">
          <source>These steps are performed either a maximum number of times (&lt;code&gt;max_trials&lt;/code&gt;) or until one of the special stop criteria are met (see &lt;code&gt;stop_n_inliers&lt;/code&gt; and &lt;code&gt;stop_score&lt;/code&gt;). The final model is estimated using all inlier samples (consensus set) of the previously determined best model.</source>
          <target state="translated">Estos pasos se llevan a cabo un n&amp;uacute;mero m&amp;aacute;ximo de veces ( &lt;code&gt;max_trials&lt;/code&gt; ) o hasta que se cumpla uno de los criterios especiales de parada (consulte &lt;code&gt;stop_n_inliers&lt;/code&gt; y &lt;code&gt;stop_score&lt;/code&gt; ). El modelo final se estima utilizando todas las muestras internas (conjunto de consenso) del mejor modelo previamente determinado.</target>
        </trans-unit>
        <trans-unit id="eff89650d9905b662c6df80228e926f2f144c91b" translate="yes" xml:space="preserve">
          <source>These three distances are special cases of the beta-divergence family, with \(\beta = 2, 1, 0\) respectively &lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;[6]&lt;/a&gt;. The beta-divergence are defined by :</source>
          <target state="translated">Estas tres distancias son casos especiales de la familia de divergencia beta, con \ (\ beta = 2, 1, 0 \) respectivamente &lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;[6]&lt;/a&gt; . La beta-divergencia se define por:</target>
        </trans-unit>
        <trans-unit id="100dafc268c3e9d0b628da1715aed2440b14ba32" translate="yes" xml:space="preserve">
          <source>These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GIL&lt;/a&gt;) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though.</source>
          <target state="translated">Estos rendimientos se logran en un solo proceso. Una forma obvia de aumentar el rendimiento de su aplicaci&amp;oacute;n es generar instancias adicionales (generalmente procesos en Python debido a &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GIL&lt;/a&gt; ) que comparten el mismo modelo. Tambi&amp;eacute;n se pueden agregar m&amp;aacute;quinas para distribuir la carga. Sin embargo, una explicaci&amp;oacute;n detallada sobre c&amp;oacute;mo lograr esto est&amp;aacute; m&amp;aacute;s all&amp;aacute; del alcance de esta documentaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="e63971597c4df5f2dc150f90b566e1219b6a632a" translate="yes" xml:space="preserve">
          <source>They are not sparse, i.e., they use the whole samples/features information to perform the prediction.</source>
          <target state="translated">No son escasos,es decir,utilizan toda la información de las muestras/funciones para realizar la predicción.</target>
        </trans-unit>
        <trans-unit id="26f84bd6fe103542912ebb1fb588d29515a2fd6d" translate="yes" xml:space="preserve">
          <source>They can be loaded using the following functions:</source>
          <target state="translated">Pueden ser cargados usando las siguientes funciones:</target>
        </trans-unit>
        <trans-unit id="f7ddbd9f45ee3f137b5eb656135dce7584351da0" translate="yes" xml:space="preserve">
          <source>They expose a &lt;code&gt;split&lt;/code&gt; method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy.</source>
          <target state="translated">Exponen un m&amp;eacute;todo &lt;code&gt;split&lt;/code&gt; que acepta que el conjunto de datos de entrada se divida y produce los &amp;iacute;ndices del conjunto de tren / prueba para cada iteraci&amp;oacute;n de la estrategia de validaci&amp;oacute;n cruzada elegida.</target>
        </trans-unit>
        <trans-unit id="03bbcc98e1d567dd0afc112fe378a99851c98226" translate="yes" xml:space="preserve">
          <source>They lose efficiency in high dimensional spaces &amp;ndash; namely when the number of features exceeds a few dozens.</source>
          <target state="translated">Pierden eficacia en espacios de gran dimensi&amp;oacute;n, es decir, cuando el n&amp;uacute;mero de funciones supera unas pocas docenas.</target>
        </trans-unit>
        <trans-unit id="a673690dbc33eee17ee6f3995f817097918876ff" translate="yes" xml:space="preserve">
          <source>This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).</source>
          <target state="translated">Este escalador elimina la mediana y escala los datos de acuerdo con el rango de cuantiles (por defecto es IQR:Interquartile Range).El IQR es el rango entre el 1er cuartil (25º cuantil)y el 3er cuartil (75º cuantil).</target>
        </trans-unit>
        <trans-unit id="d9994b645c162ae9e80a2c6bc1c81390f2e92325" translate="yes" xml:space="preserve">
          <source>This Warning is used in meta estimators GridSearchCV and RandomizedSearchCV and the cross-validation helper function cross_val_score to warn when there is an error while fitting the estimator.</source>
          <target state="translated">Esta advertencia se utiliza en los metaestimuladores GridSearchCV y RandomizedSearchCV y en la función de ayuda de validación cruzada cross_val_score para advertir cuando hay un error al ajustar el estimador.</target>
        </trans-unit>
        <trans-unit id="7f5e3d23312509826c13f1663d34b7e7912ac4af" translate="yes" xml:space="preserve">
          <source>This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by &lt;code&gt;n_clusters&lt;/code&gt;. If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.</source>
          <target state="translated">Este algoritmo puede verse como una instancia o un m&amp;eacute;todo de reducci&amp;oacute;n de datos, ya que reduce los datos de entrada a un conjunto de subclusters que se obtienen directamente de las hojas del CFT. Estos datos reducidos pueden procesarse a&amp;uacute;n m&amp;aacute;s introduci&amp;eacute;ndolos en un agrupador global. Este agrupador global se puede configurar mediante &lt;code&gt;n_clusters&lt;/code&gt; . Si &lt;code&gt;n_clusters&lt;/code&gt; se establece en None, los subclusters de las hojas se leen directamente; de ​​lo contrario, un paso de agrupaci&amp;oacute;n global etiqueta estos subclusters en agrupaciones globales (etiquetas) y las muestras se asignan a la etiqueta global del subcluster m&amp;aacute;s cercano.</target>
        </trans-unit>
        <trans-unit id="895ec6bb1e64f58ea59b9ae781fa620e703125a9" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#r4d113ba76fc0-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#r4d113ba76fc0-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#r4d113ba76fc0-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#r4d113ba76fc0-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">Este algoritmo engloba varios trabajos de la literatura. Cuando se extraen subconjuntos aleatorios del conjunto de datos como subconjuntos aleatorios de las muestras, este algoritmo se conoce como Pegado &lt;a href=&quot;#r4d113ba76fc0-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; . Si las muestras se extraen con reemplazo, el m&amp;eacute;todo se conoce como ensacado &lt;a href=&quot;#r4d113ba76fc0-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; . Cuando se extraen subconjuntos aleatorios del conjunto de datos como subconjuntos aleatorios de las caracter&amp;iacute;sticas, el m&amp;eacute;todo se conoce como subespacios aleatorios &lt;a href=&quot;#r4d113ba76fc0-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt; . Finalmente, cuando los estimadores base se construyen sobre subconjuntos de muestras y caracter&amp;iacute;sticas, el m&amp;eacute;todo se conoce como Parches aleatorios &lt;a href=&quot;#r4d113ba76fc0-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="870122c945d57891e89c2ba931542331f1b4afdb" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#rb1846455d0e5-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#rb1846455d0e5-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#rb1846455d0e5-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#rb1846455d0e5-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">Este algoritmo engloba varios trabajos de la literatura. Cuando se extraen subconjuntos aleatorios del conjunto de datos como subconjuntos aleatorios de las muestras, este algoritmo se conoce como Pegado &lt;a href=&quot;#rb1846455d0e5-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; . Si las muestras se extraen con reemplazo, el m&amp;eacute;todo se conoce como ensacado &lt;a href=&quot;#rb1846455d0e5-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; . Cuando se extraen subconjuntos aleatorios del conjunto de datos como subconjuntos aleatorios de las caracter&amp;iacute;sticas, el m&amp;eacute;todo se conoce como subespacios aleatorios &lt;a href=&quot;#rb1846455d0e5-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt; . Finalmente, cuando los estimadores base se construyen sobre subconjuntos de muestras y caracter&amp;iacute;sticas, el m&amp;eacute;todo se conoce como Parches aleatorios &lt;a href=&quot;#rb1846455d0e5-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="866e774dfaaba96a720c3090067c49c13cac935f" translate="yes" xml:space="preserve">
          <source>This algorithm finds a (usually very good) approximate truncated singular value decomposition using randomization to speed up the computations. It is particularly fast on large matrices on which you wish to extract only a small number of components. In order to obtain further speed up, &lt;code&gt;n_iter&lt;/code&gt; can be set &amp;lt;=2 (at the cost of loss of precision).</source>
          <target state="translated">Este algoritmo encuentra una descomposici&amp;oacute;n de valor singular truncado aproximado (generalmente muy buena) utilizando la aleatorizaci&amp;oacute;n para acelerar los c&amp;aacute;lculos. Es particularmente r&amp;aacute;pido en matrices grandes en las que desea extraer solo una peque&amp;ntilde;a cantidad de componentes. Para obtener una mayor velocidad, &lt;code&gt;n_iter&lt;/code&gt; se puede establecer &amp;lt;= 2 (a costa de la p&amp;eacute;rdida de precisi&amp;oacute;n).</target>
        </trans-unit>
        <trans-unit id="77977d31f5357c834112cbb27bbc98b5c64c10d7" translate="yes" xml:space="preserve">
          <source>This algorithm has constant memory complexity, on the order of &lt;code&gt;batch_size&lt;/code&gt;, enabling use of np.memmap files without loading the entire file into memory.</source>
          <target state="translated">Este algoritmo tiene una complejidad de memoria constante, del orden del &lt;code&gt;batch_size&lt;/code&gt; de lote , lo que permite el uso de archivos np.memmap sin cargar el archivo completo en la memoria.</target>
        </trans-unit>
        <trans-unit id="9586aed3b1a5b6a2c44b32af5cc0558b6ad496a6" translate="yes" xml:space="preserve">
          <source>This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering.</source>
          <target state="translated">Este algoritmo resuelve el corte normalizado para k=2:es una agrupación espectral normalizada.</target>
        </trans-unit>
        <trans-unit id="01c65a021c885e4e00baaaa5c6652a314a97daa3" translate="yes" xml:space="preserve">
          <source>This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues.</source>
          <target state="translated">Este algoritmo utilizará siempre todos los componentes a los que tenga acceso,necesitando datos retenidos o criterios teóricos de información para decidir cuántos componentes utilizar en ausencia de pistas externas.</target>
        </trans-unit>
        <trans-unit id="ddf04fe856314e7dd4ddddf49f4086029e04d842" translate="yes" xml:space="preserve">
          <source>This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise:</source>
          <target state="translated">Esto permite una mejor selección del modelo que el PCA probabilístico en presencia de ruido heteroscedástico:</target>
        </trans-unit>
        <trans-unit id="3307a2458ebbefda8ea7fe8b898077ec4cadcf2c" translate="yes" xml:space="preserve">
          <source>This also works where final estimator is &lt;code&gt;None&lt;/code&gt;: all prior transformations are applied.</source>
          <target state="translated">Esto tambi&amp;eacute;n funciona cuando el estimador final es &lt;code&gt;None&lt;/code&gt; : se aplican todas las transformaciones anteriores.</target>
        </trans-unit>
        <trans-unit id="c63b80512d8853cd76b24210ee07543da5c60fc4" translate="yes" xml:space="preserve">
          <source>This assumption is the base of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_Space_Model&quot;&gt;Vector Space Model&lt;/a&gt; often used in text classification and clustering contexts.</source>
          <target state="translated">Esta suposici&amp;oacute;n es la base del &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_Space_Model&quot;&gt;modelo de espacio vectorial que&lt;/a&gt; se utiliza a menudo en contextos de clasificaci&amp;oacute;n y agrupaci&amp;oacute;n de texto.</target>
        </trans-unit>
        <trans-unit id="8142b653ecd3322ad782e8a8807635d6c4c0325e" translate="yes" xml:space="preserve">
          <source>This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">Esta calibración da como resultado una menor pérdida de registros.Nótese que una alternativa habría sido aumentar el número de estimadores de base,lo que habría resultado en una disminución similar de la pérdida de logaritmos.</target>
        </trans-unit>
        <trans-unit id="3ba422e075fa10216e381bf46530abda4e719fab" translate="yes" xml:space="preserve">
          <source>This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.</source>
          <target state="translated">Esta llamada requiere la estimación de una matriz p x q,que puede ser un problema en el espacio de altas dimensiones.</target>
        </trans-unit>
        <trans-unit id="345a3bd30c8553d3251f728b344d1bd100958670" translate="yes" xml:space="preserve">
          <source>This can be confirmed on a independent testing set with similar remarks:</source>
          <target state="translated">Esto puede confirmarse en un conjunto de pruebas independientes con observaciones similares:</target>
        </trans-unit>
        <trans-unit id="e60927eda9d0f07135f56fa39eaa1a8f1f9c2813" translate="yes" xml:space="preserve">
          <source>This can be done by introducing &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;uninformative priors&lt;/a&gt; over the hyper parameters of the model. The \(\ell_{2}\) regularization used in &lt;a href=&quot;#id2&quot;&gt;Ridge Regression&lt;/a&gt; is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the parameters \(w\) with precision \(\lambda^{-1}\). Instead of setting &lt;code&gt;lambda&lt;/code&gt; manually, it is possible to treat it as a random variable to be estimated from the data.</source>
          <target state="translated">Esto se puede hacer introduciendo a &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;priori no informativos&lt;/a&gt; sobre los hiperpar&amp;aacute;metros del modelo. La regularizaci&amp;oacute;n \ (\ ell_ {2} \) usada en &lt;a href=&quot;#id2&quot;&gt;Ridge Regression&lt;/a&gt; es equivalente a encontrar una estimaci&amp;oacute;n m&amp;aacute;xima a posteriori bajo un gaussiano anterior sobre los par&amp;aacute;metros \ (w \) con precisi&amp;oacute;n \ (\ lambda ^ {- 1} \). En lugar de configurar &lt;code&gt;lambda&lt;/code&gt; manualmente, es posible tratarlo como una variable aleatoria para estimar a partir de los datos.</target>
        </trans-unit>
        <trans-unit id="aa181018cfeb3219e1e67073f9c58ca90a0c4faa" translate="yes" xml:space="preserve">
          <source>This can be done by using the &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; utility function.</source>
          <target state="translated">Esto se puede hacer usando la funci&amp;oacute;n de utilidad &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="821e5435b62e56a883478da64d6731f6479103d5" translate="yes" xml:space="preserve">
          <source>This can be set to a higher value than the actual number of features in any of the input files, but setting it to a lower value will cause an exception to be raised.</source>
          <target state="translated">Se puede ajustar a un valor más alto que el número real de características en cualquiera de los archivos de entrada,pero ajustarlo a un valor más bajo hará que aumente la excepción.</target>
        </trans-unit>
        <trans-unit id="4a0bd36c1ccd6d51b38a900236d58695657d5aff" translate="yes" xml:space="preserve">
          <source>This class allows to infer an approximate posterior distribution over the parameters of a Gaussian mixture distribution. The effective number of components can be inferred from the data.</source>
          <target state="translated">Esta clase permite inferir una distribución posterior aproximada sobre los parámetros de una distribución de mezcla gaussiana.A partir de los datos se puede inferir el número efectivo de componentes.</target>
        </trans-unit>
        <trans-unit id="6130cf2c7564234b715447525f16ff596ebe842e" translate="yes" xml:space="preserve">
          <source>This class can be used to cross-validate time series data samples that are observed at fixed time intervals.</source>
          <target state="translated">Esta clase puede utilizarse para validar de forma cruzada las muestras de datos de series temporales que se observan en intervalos de tiempo fijos.</target>
        </trans-unit>
        <trans-unit id="88afb4091eb2a25b2e2017ee8508b1ff5c5ae125" translate="yes" xml:space="preserve">
          <source>This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.</source>
          <target state="translated">Esta clase implementa un metaestimulador que se ajusta a un número de árboles de decisión aleatorios (también conocidos como árboles extra)en varias submuestras del conjunto de datos y utiliza el promediado para mejorar la precisión de la predicción y controlar el exceso de ajuste.</target>
        </trans-unit>
        <trans-unit id="dd60f6580be8e1908408c4fbfd8d3915f46e55b1" translate="yes" xml:space="preserve">
          <source>This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</source>
          <target state="translated">Esta clase implementa la regresión logística usando el optimizador liblinear,newton-cg,sag de lbfgs.Los solucionadores newton-cg,sag y lbfgs sólo admiten la regularización L2 con la formulación primaria.El solucionador liblineal soporta tanto la regularización L1 como la L2,con una formulación dual sólo para la penalización L2.</target>
        </trans-unit>
        <trans-unit id="350693d0493245dcbd676a8f10e001d5020f745e" translate="yes" xml:space="preserve">
          <source>This class implements regularized logistic regression using the &amp;lsquo;liblinear&amp;rsquo; library, &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).</source>
          <target state="translated">Esta clase implementa regresi&amp;oacute;n log&amp;iacute;stica regularizada usando la biblioteca 'liblinear', los solucionadores 'newton-cg', 'sag' y 'lbfgs'. Puede manejar entradas densas y dispersas. Utilice matrices ordenadas en C o matrices CSR que contengan flotantes de 64 bits para un rendimiento &amp;oacute;ptimo; cualquier otro formato de entrada ser&amp;aacute; convertido (y copiado).</target>
        </trans-unit>
        <trans-unit id="2c107aea4a3526193efe1f31d97ccab92891d87f" translate="yes" xml:space="preserve">
          <source>This class implements the Graphical Lasso algorithm.</source>
          <target state="translated">Esta clase implementa el algoritmo del Lazo Gráfico.</target>
        </trans-unit>
        <trans-unit id="456e7e6f7be68de425401d6f66fe28d8a1090538" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost-SAMME [2].</source>
          <target state="translated">Esta clase implementa el algoritmo conocido como AdaBoost-SAMME [2].</target>
        </trans-unit>
        <trans-unit id="003b6bf538c58eeda3c6fd28b21967bfd8b6c40e" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost.R2 [2].</source>
          <target state="translated">Esta clase implementa el algoritmo conocido como AdaBoost.R2 [2].</target>
        </trans-unit>
        <trans-unit id="63c4b52b10df12140782204ea7cf58fe0edab9de" translate="yes" xml:space="preserve">
          <source>This class implements two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">Esta clase implementa dos tipos de previos para la distribución de pesos:un modelo de mezcla finita con la distribución Dirichlet y un modelo de mezcla infinita con el Proceso Dirichlet.En la práctica,el algoritmo de inferencia del Proceso de Dirichlet es aproximado y utiliza una distribución truncada con un número máximo fijo de componentes (llamada la representación de la ruptura del palo).El número de componentes realmente utilizado casi siempre depende de los datos.</target>
        </trans-unit>
        <trans-unit id="e15e7e7d8d04d41fdd2a4f34183baa0366e86dea" translate="yes" xml:space="preserve">
          <source>This class inherits from PLS with mode=&amp;rdquo;A&amp;rdquo; and deflation_mode=&amp;rdquo;canonical&amp;rdquo;, norm_y_weights=True and algorithm=&amp;rdquo;nipals&amp;rdquo;, but svd should provide similar results up to numerical errors.</source>
          <target state="translated">Esta clase hereda de PLS con mode = &amp;rdquo;A&amp;rdquo; y deflation_mode = &amp;rdquo;canonical&amp;rdquo;, norm_y_weights = True y algor&amp;iacute;tm = &amp;rdquo;nipals&amp;rdquo;, pero svd deber&amp;iacute;a proporcionar resultados similares hasta errores num&amp;eacute;ricos.</target>
        </trans-unit>
        <trans-unit id="a0a1d1daa83e6ad727cb13fd589f28f37b44df20" translate="yes" xml:space="preserve">
          <source>This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.</source>
          <target state="translated">Esta clase hereda tanto de ValueError como de AttributeError para ayudar con el manejo de excepciones y la compatibilidad retroactiva.</target>
        </trans-unit>
        <trans-unit id="d34b4e0a14d3a494edeba399329a47fb49b3ee6c" translate="yes" xml:space="preserve">
          <source>This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.</source>
          <target state="translated">Esta clase es una alternativa de baja memoria a DictVectorizer y CountVectorizer,destinada al aprendizaje a gran escala (en línea)y a situaciones en las que la memoria es escasa,por ejemplo,cuando se ejecuta el código de predicción en dispositivos incorporados.</target>
        </trans-unit>
        <trans-unit id="8e34d865883b535f68990f01a240eaab9f87f080" translate="yes" xml:space="preserve">
          <source>This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">Por lo tanto, esta clase es adecuada para su uso en los primeros pasos de un &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="eef0760dd6d25fd731b9abce61656453bb690cee" translate="yes" xml:space="preserve">
          <source>This class is useful when the behavior of &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt; is desired, but the number of groups is large enough that generating all possible partitions with \(P\) groups withheld would be prohibitively expensive. In such a scenario, &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt;&lt;code&gt;GroupShuffleSplit&lt;/code&gt;&lt;/a&gt; provides a random sample (with replacement) of the train / test splits generated by &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Esta clase es &amp;uacute;til cuando se desea el comportamiento de &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt; , pero el n&amp;uacute;mero de grupos es lo suficientemente grande como para generar todas las particiones posibles con grupos \ (P \) retenidos ser&amp;iacute;a prohibitivamente costoso. En tal escenario, &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt; &lt;code&gt;GroupShuffleSplit&lt;/code&gt; &lt;/a&gt; proporciona una muestra aleatoria (con reemplazo) de las divisiones de tren / prueba generadas por &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="edf4c29bfa2afe43016dc0b6660ad50132bd51ec" translate="yes" xml:space="preserve">
          <source>This class provides a uniform interface to fast distance metric functions. The various metrics can be accessed via the &lt;code&gt;get_metric&lt;/code&gt; class method and the metric string identifier (see below). For example, to use the Euclidean distance:</source>
          <target state="translated">Esta clase proporciona una interfaz uniforme para funciones m&amp;eacute;tricas de distancia r&amp;aacute;pida. Se puede acceder a las diversas m&amp;eacute;tricas a trav&amp;eacute;s del m&amp;eacute;todo de clase &lt;code&gt;get_metric&lt;/code&gt; y el identificador de cadena de m&amp;eacute;tricas (ver m&amp;aacute;s abajo). Por ejemplo, para usar la distancia euclidiana:</target>
        </trans-unit>
        <trans-unit id="336f533fd7cf96bc18f597ff7211d31f0cd62386" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.</source>
          <target state="translated">Esta clase soporta tanto la entrada densa como la dispersa y el soporte multiclase se maneja según un esquema de uno contra uno.</target>
        </trans-unit>
        <trans-unit id="aebcf6792861578bf4b4a69a0be71898d4023b6a" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input.</source>
          <target state="translated">Esta clase soporta tanto la entrada densa como la escasa.</target>
        </trans-unit>
        <trans-unit id="4042c6697e9df3310aa51f4f2bbe289c3d222c6e" translate="yes" xml:space="preserve">
          <source>This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">Esta clase convierte secuencias de nombres de rasgos simbólicos (cuerdas)en matrices scipy.sparse,utilizando una función hash para calcular la columna de la matriz correspondiente a un nombre.La función hash empleada es la versión firmada de 32 bits de Murmurhash3.</target>
        </trans-unit>
        <trans-unit id="afb2ca6e635ea8a6abf9d5cec38428c8ecdc147c" translate="yes" xml:space="preserve">
          <source>This classification dataset is constructed by taking a multi-dimensional standard normal distribution and defining classes separated by nested concentric multi-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="translated">Este conjunto de datos de clasificación se construye tomando una distribución normal estándar multidimensional y definiendo clases separadas por esferas multidimensionales concéntricas anidadas,de tal manera que en cada clase hay un número aproximadamente igual de muestras (cuantiles de la distribución \(\chi^2\)).</target>
        </trans-unit>
        <trans-unit id="a35e593acabc67ebdf8fae8d0484d5f85056d4a7" translate="yes" xml:space="preserve">
          <source>This classifier is useful as a simple baseline to compare with other (real) classifiers. Do not use it for real problems.</source>
          <target state="translated">Este clasificador es útil como una simple línea de base para comparar con otros clasificadores (reales).No lo use para problemas reales.</target>
        </trans-unit>
        <trans-unit id="4ceac36d1efc9bb429dd84350330a84101bb5c8c" translate="yes" xml:space="preserve">
          <source>This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:</source>
          <target state="translated">Este clasificador perdió gran parte de su puntuación F,sólo porque eliminamos metadatos que tienen poco que ver con la clasificación de los temas.Pierde aún más si también quitamos estos metadatos de los datos de formación:</target>
        </trans-unit>
        <trans-unit id="064e5da463cfecd3ca166c4023a79d0a6500160d" translate="yes" xml:space="preserve">
          <source>This combination is implementing in &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt;, a transformer class that is mostly API compatible with &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; is stateless, meaning that you don&amp;rsquo;t have to call &lt;code&gt;fit&lt;/code&gt; on it:</source>
          <target state="translated">Esta combinaci&amp;oacute;n se implementa en &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt; , una clase de transformador que es principalmente compatible con API con &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; no&lt;/a&gt; tiene estado, lo que significa que no tiene que llamar a &lt;code&gt;fit&lt;/code&gt; en &amp;eacute;l:</target>
        </trans-unit>
        <trans-unit id="1d56591f4aa7f0ebb864c2d8835af7dfb7f8f26b" translate="yes" xml:space="preserve">
          <source>This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument &lt;code&gt;axis=1&lt;/code&gt;, and reduce it to an array of size [M].</source>
          <target state="translated">Esto combina los valores de las caracter&amp;iacute;sticas aglomeradas en un solo valor y debe aceptar una matriz de forma [M, N] y el argumento de palabra clave &lt;code&gt;axis=1&lt;/code&gt; , y reducirlo a una matriz de tama&amp;ntilde;o [M].</target>
        </trans-unit>
        <trans-unit id="26c788fe31e79bd1bbf24fbba8a1b8342ff15ce1" translate="yes" xml:space="preserve">
          <source>This consumes less memory than shuffling the data directly.</source>
          <target state="translated">Esto consume menos memoria que barajar los datos directamente.</target>
        </trans-unit>
        <trans-unit id="9c8cf431c4f1299ae41612f84a50a597aa4a1059" translate="yes" xml:space="preserve">
          <source>This creates binary hashes of input data points by getting the dot product of input points and hash_function then transforming the projection into a binary string array based on the sign (positive/negative) of the projection. A sorted array of binary hashes is created.</source>
          <target state="translated">Esto crea hashes binarios de puntos de datos de entrada obteniendo el producto puntual de los puntos de entrada y la función hash_función y luego transformando la proyección en una matriz de cadenas binarias basada en el signo (positivo/negativo)de la proyección.Se crea una matriz ordenada de hashes binarios.</target>
        </trans-unit>
        <trans-unit id="75f340063df2a6996986c297d7d4423dc4417e05" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="translated">Este objeto de validación cruzada es una fusión de StratifiedKFold y ShuffleSplit,que devuelve pliegues aleatorios estratificados.Los pliegues se hacen preservando el porcentaje de muestras de cada clase.</target>
        </trans-unit>
        <trans-unit id="ab90d891a9b7f61040a0bd2fc78eae2488d53a3a" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt;. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.</source>
          <target state="translated">Este objeto de validaci&amp;oacute;n cruzada es una variaci&amp;oacute;n de &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; . En la divisi&amp;oacute;n k, devuelve los primeros k pliegues como conjunto de tren y el (k + 1) pliegue como conjunto de prueba.</target>
        </trans-unit>
        <trans-unit id="c3d6a6171342c06e134b7087a7e83e3f80ba8a79" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="translated">Este objeto de validación cruzada es una variación de KFold que devuelve pliegues estratificados.Los pliegues se hacen preservando el porcentaje de muestras de cada clase.</target>
        </trans-unit>
        <trans-unit id="2c34ca372157ddad0d3d18ffb66a8717775276f6" translate="yes" xml:space="preserve">
          <source>This data sets consists of 3 different types of irises&amp;rsquo; (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray</source>
          <target state="translated">Este conjunto de datos consta de 3 tipos diferentes de iris (Setosa, Versicolour y Virginica) de longitud de p&amp;eacute;talos y s&amp;eacute;palos, almacenados en un numpy.ndarray de 150x4.</target>
        </trans-unit>
        <trans-unit id="158d23b76a72fb850a277110200a4b319b51d7f5" translate="yes" xml:space="preserve">
          <source>This database is also available through the UW CS ftp server:</source>
          <target state="translated">Esta base de datos también está disponible a través del servidor ftp UW CS:</target>
        </trans-unit>
        <trans-unit id="8f0ed5f875aa21e65ca9d6116dc0e918ff34c0ff" translate="yes" xml:space="preserve">
          <source>This dataset consists of 20,640 samples and 9 features.</source>
          <target state="translated">Este conjunto de datos consta de 20.640 muestras y 9 características.</target>
        </trans-unit>
        <trans-unit id="2e15d3adbea56bdf31e76cb4ee55fcf6dfb7c05f" translate="yes" xml:space="preserve">
          <source>This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:</source>
          <target state="translated">Este conjunto de datos es una colección de fotos JPEG de personajes famosos recogidas en Internet,todos los detalles están disponibles en la página web oficial:</target>
        </trans-unit>
        <trans-unit id="1c0b9129c637e05601004735cb7bfdbdba431796" translate="yes" xml:space="preserve">
          <source>This dataset is described in Celeux et al [1]. as:</source>
          <target state="translated">Este conjunto de datos se describe en Celeux et al [1].como:</target>
        </trans-unit>
        <trans-unit id="e92704f97c6e77c413a0405e7cf7dd2e32191660" translate="yes" xml:space="preserve">
          <source>This dataset is described in Friedman [1] and Breiman [2].</source>
          <target state="translated">Este conjunto de datos se describe en Friedman [1]y Breiman [2].</target>
        </trans-unit>
        <trans-unit id="473d3b557a1c23b381f633c2c2acf0c38c2a328f" translate="yes" xml:space="preserve">
          <source>This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we&amp;rsquo;d have to first transform it into a feature vector with length 64.</source>
          <target state="translated">Este conjunto de datos se compone de 1797 im&amp;aacute;genes de 8x8. Cada imagen, como la que se muestra a continuaci&amp;oacute;n, es de un d&amp;iacute;gito escrito a mano. Para utilizar una figura de 8x8 como esta, primero tendr&amp;iacute;amos que transformarla en un vector de caracter&amp;iacute;sticas con una longitud de 64.</target>
        </trans-unit>
        <trans-unit id="73cdbbbbdb25af126933f658f4b065e86b9c1a55" translate="yes" xml:space="preserve">
          <source>This dataset represents the geographic distribution of species. The dataset is provided by Phillips et. al. (2006).</source>
          <target state="translated">Este conjunto de datos representa la distribución geográfica de las especies.El conjunto de datos es proporcionado por Phillips et.al.(2006).</target>
        </trans-unit>
        <trans-unit id="e72397d5f7691c5c60df49442bb007cee4717786" translate="yes" xml:space="preserve">
          <source>This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).</source>
          <target state="translated">Este conjunto de datos se derivó del censo de los EE.UU.de 1990,utilizando una fila por grupo de bloques censales.Un grupo de bloques es la unidad geográfica más pequeña para la que la Oficina del Censo de los Estados Unidos publica datos de muestra (un grupo de bloques suele tener una población de 600 a 3.000 personas).</target>
        </trans-unit>
        <trans-unit id="47bf559cf1abc9fb33a96a120f583ad0bcd0f9f8" translate="yes" xml:space="preserve">
          <source>This dataset was obtained from the StatLib repository. &lt;a href=&quot;http://lib.stat.cmu.edu/datasets/&quot;&gt;http://lib.stat.cmu.edu/datasets/&lt;/a&gt;</source>
          <target state="translated">Este conjunto de datos se obtuvo del repositorio StatLib. &lt;a href=&quot;http://lib.stat.cmu.edu/datasets/&quot;&gt;http://lib.stat.cmu.edu/datasets/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4668c093fae272aec3196fc6d39e4e5a4eede980" translate="yes" xml:space="preserve">
          <source>This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.</source>
          <target state="translated">Este conjunto de datos fue tomado de la biblioteca StatLib que se mantiene en la Universidad Carnegie Mellon.</target>
        </trans-unit>
        <trans-unit id="9b1b90be23e0adb200134bcc2570822a79ae6e88" translate="yes" xml:space="preserve">
          <source>This demonstrates Label Propagation learning a good boundary even with a small amount of labeled data.</source>
          <target state="translated">Esto demuestra que la Propagación de Etiquetas aprende un buen límite incluso con una pequeña cantidad de datos etiquetados.</target>
        </trans-unit>
        <trans-unit id="33d36bf521beb70b7b2f4b7e5d24d58f96f46c86" translate="yes" xml:space="preserve">
          <source>This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; for normalization):</source>
          <target state="translated">Esta descripci&amp;oacute;n se puede vectorizar en una matriz bidimensional dispersa adecuada para alimentar a un clasificador (tal vez despu&amp;eacute;s de ser canalizada en un &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;text.TfidfTransformer&lt;/code&gt; &lt;/a&gt; para normalizaci&amp;oacute;n):</target>
        </trans-unit>
        <trans-unit id="12f7e332bc936dbb460a17349dda07780cab7782" translate="yes" xml:space="preserve">
          <source>This determines which warnings will be made in the case that this function is being used to return only one of its metrics.</source>
          <target state="translated">Esto determina qué advertencias se harán en caso de que esta función se utilice para devolver sólo una de sus métricas.</target>
        </trans-unit>
        <trans-unit id="a1a66d0ad9255f63c11b93170b95da4e6eeaea4f" translate="yes" xml:space="preserve">
          <source>This downscaling is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;tf&amp;ndash;idf&lt;/a&gt; for &amp;ldquo;Term Frequency times Inverse Document Frequency&amp;rdquo;.</source>
          <target state="translated">Esta reducci&amp;oacute;n de escala se denomina &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;tf-idf&lt;/a&gt; para &quot;Frecuencia de t&amp;eacute;rminos multiplicada por frecuencia de documento inversa&quot;.</target>
        </trans-unit>
        <trans-unit id="edeebc499fdf886cf2b1fe82f9cc25a148384f70" translate="yes" xml:space="preserve">
          <source>This early stopping strategy is activated if &lt;code&gt;early_stopping=True&lt;/code&gt;; otherwise the stopping criterion only uses the training loss on the entire input data. To better control the early stopping strategy, we can specify a parameter &lt;code&gt;validation_fraction&lt;/code&gt; which set the fraction of the input dataset that we keep aside to compute the validation score. The optimization will continue until the validation score did not improve by at least &lt;code&gt;tol&lt;/code&gt; during the last &lt;code&gt;n_iter_no_change&lt;/code&gt; iterations. The actual number of iterations is available at the attribute &lt;code&gt;n_iter_&lt;/code&gt;.</source>
          <target state="translated">Esta estrategia de parada anticipada se activa si &lt;code&gt;early_stopping=True&lt;/code&gt; ; de lo contrario, el criterio de detenci&amp;oacute;n solo utiliza la p&amp;eacute;rdida de entrenamiento en todos los datos de entrada. Para controlar mejor la estrategia de detenci&amp;oacute;n temprana, podemos especificar un par&amp;aacute;metro &lt;code&gt;validation_fraction&lt;/code&gt; que establece la fracci&amp;oacute;n del conjunto de datos de entrada que mantenemos a un lado para calcular la puntuaci&amp;oacute;n de validaci&amp;oacute;n. La optimizaci&amp;oacute;n continuar&amp;aacute; hasta que la puntuaci&amp;oacute;n de validaci&amp;oacute;n no mejore en al menos &lt;code&gt;tol&lt;/code&gt; durante las &amp;uacute;ltimas &lt;code&gt;n_iter_no_change&lt;/code&gt; iteraciones. El n&amp;uacute;mero real de iteraciones est&amp;aacute; disponible en el atributo &lt;code&gt;n_iter_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6528dcf2523991bd357ca56474b42d537ada09b8" translate="yes" xml:space="preserve">
          <source>This embedding can also &amp;lsquo;work&amp;rsquo; even if the &lt;code&gt;adjacency&lt;/code&gt; variable is not strictly the adjacency matrix of a graph but more generally an affinity or similarity matrix between samples (for instance the heat kernel of a euclidean distance matrix or a k-NN matrix).</source>
          <target state="translated">Esta incrustaci&amp;oacute;n tambi&amp;eacute;n puede 'funcionar' incluso si la variable de &lt;code&gt;adjacency&lt;/code&gt; no es estrictamente la matriz de adyacencia de un gr&amp;aacute;fico, sino m&amp;aacute;s generalmente una matriz de afinidad o similitud entre muestras (por ejemplo, el n&amp;uacute;cleo de calor de una matriz de distancia euclidiana o una matriz k-NN).</target>
        </trans-unit>
        <trans-unit id="662188aeeffeee289aab2f0d97150266d90c022d" translate="yes" xml:space="preserve">
          <source>This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.</source>
          <target state="translated">Esta codificación es necesaria para alimentar con datos categóricos a muchos estimadores de aprendizaje científico,especialmente modelos lineales y SVM con los núcleos estándar.</target>
        </trans-unit>
        <trans-unit id="752036d9bd5ae374e975c046f504e0b38de39538" translate="yes" xml:space="preserve">
          <source>This estimator</source>
          <target state="translated">Este estimador</target>
        </trans-unit>
        <trans-unit id="36ceeb9f387752a577aeb048b3f21cd319ecfb48" translate="yes" xml:space="preserve">
          <source>This estimator allows different columns or column subsets of the input to be transformed separately and the results combined into a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.</source>
          <target state="translated">Este estimador permite transformar por separado diferentes columnas o subconjuntos de columnas de la entrada y combinar los resultados en un único espacio de características.Esto es útil para datos heterogéneos o columnares,para combinar varios mecanismos de extracción de características o transformaciones en un solo transformador.</target>
        </trans-unit>
        <trans-unit id="02199e2b9b2bd941c7464261eedf68a6fd2d82e2" translate="yes" xml:space="preserve">
          <source>This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.</source>
          <target state="translated">Este estimador aplica una lista de objetos transformadores en paralelo a los datos de entrada,y luego concatena los resultados.Esto es útil para combinar varios mecanismos de extracción de características en un solo transformador.</target>
        </trans-unit>
        <trans-unit id="a93ab3d6ae360c030a56bd4fabca46c42abdaf54" translate="yes" xml:space="preserve">
          <source>This estimator approximates a slightly different version of the additive chi squared kernel then &lt;code&gt;metric.additive_chi2&lt;/code&gt; computes.</source>
          <target state="translated">Este estimador se aproxima a una versi&amp;oacute;n ligeramente diferente del n&amp;uacute;cleo de chi cuadrado aditivo, luego &lt;code&gt;metric.additive_chi2&lt;/code&gt; calcula.</target>
        </trans-unit>
        <trans-unit id="57f1dab8dd3e838e06f9128461ff865667eb2891" translate="yes" xml:space="preserve">
          <source>This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).</source>
          <target state="translated">Este estimador tiene incorporado el soporte para la regresión multivariante (es decir,cuando y es un conjunto de dos formas [n_muestras,n_objetivos]).</target>
        </trans-unit>
        <trans-unit id="ab45bcef3fd31ebffe9c4724334c2d64921dae44" translate="yes" xml:space="preserve">
          <source>This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.</source>
          <target state="translated">Este estimador implementa modelos lineales regularizados con aprendizaje de gradiente de descenso estocástico (SGD):el gradiente de la pérdida se estima en cada muestra a la vez y el modelo se actualiza a lo largo del camino con un programa de fuerza decreciente (también conocido como tasa de aprendizaje).El SGD permite el aprendizaje minibatch (en línea/fuera del núcleo),ver el método partial_fit.Para obtener los mejores resultados utilizando el esquema de tasa de aprendizaje predeterminado,los datos deben tener una media cero y una varianza unitaria.</target>
        </trans-unit>
        <trans-unit id="5fdb17bfdb14f498d9377f8ca9b7cc2199a41d7f" translate="yes" xml:space="preserve">
          <source>This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline.</source>
          <target state="translated">Este estimador es apátrida (además de los parámetros de construcción),el método de ajuste no hace nada pero es útil cuando se utiliza en una tubería.</target>
        </trans-unit>
        <trans-unit id="42d08f109b32ce107e9f058e7449521b0b6eab28" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.</source>
          <target state="translated">Este estimador escala y traduce cada característica individualmente de tal manera que está en el rango dado en el conjunto de entrenamiento,es decir,entre cero y uno.</target>
        </trans-unit>
        <trans-unit id="ce7850baf5a7a3e7ef75716db2d2e4af71c92137" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.</source>
          <target state="translated">Este estimador escala y traduce cada característica individualmente de tal manera que el valor absoluto máximo de cada característica en el conjunto de entrenamiento será 1.0.No cambia/centro los datos,y por lo tanto no destruye ninguna escasez.</target>
        </trans-unit>
        <trans-unit id="0ff92b3f8e56701f386ccbc5279ec5fdb2974bc0" translate="yes" xml:space="preserve">
          <source>This estimator scales each feature individually such that the maximal absolute value of each feature in the training set will be 1.0.</source>
          <target state="translated">Este estimador escala cada característica individualmente de tal manera que el valor absoluto máximo de cada característica en el conjunto de entrenamiento será 1.0.</target>
        </trans-unit>
        <trans-unit id="354214ed410106228bbb593cd82a49f32a2508f8" translate="yes" xml:space="preserve">
          <source>This estimator supports two algorithms: a fast randomized SVD solver, and a &amp;ldquo;naive&amp;rdquo; algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.</source>
          <target state="translated">Este estimador admite dos algoritmos: un solucionador de SVD aleatorio r&amp;aacute;pido y un algoritmo &amp;ldquo;ingenuo&amp;rdquo; que usa ARPACK como un solucionador propio en (X * XT) o (XT * X), ​​el que sea m&amp;aacute;s eficiente.</target>
        </trans-unit>
        <trans-unit id="3ed5861f671a7b7a1c102cd9c37c8bf2e273de13" translate="yes" xml:space="preserve">
          <source>This estimator will run an extensive test-suite for input validation, shapes, etc. Additional tests for classifiers, regressors, clustering or transformers will be run if the Estimator class inherits from the corresponding mixin from sklearn.base.</source>
          <target state="translated">Este estimador ejecutará un extenso conjunto de pruebas para la validación de entradas,formas,etc.Se realizarán pruebas adicionales para clasificadores,regresores,agrupaciones o transformadores si la clase del Estimador hereda de la correspondiente mezcla de sklearn.base.</target>
        </trans-unit>
        <trans-unit id="7011f5e2b484f266188acd0aba4f3d3175f11ed0" translate="yes" xml:space="preserve">
          <source>This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise).</source>
          <target state="translated">Este ejemplo también muestra la utilidad de aplicar la regresión de Ridge a matrices muy mal acondicionadas.Para tales matrices,un ligero cambio en la variable objetivo puede causar enormes variaciones en los pesos calculados.En tales casos,es útil establecer una cierta regularización (alfa)para reducir esta variación (ruido).</target>
        </trans-unit>
        <trans-unit id="7f82721c4674480adff4241dbcac8e543f16c868" translate="yes" xml:space="preserve">
          <source>This example applies to olivetti_faces different unsupervised matrix decomposition (dimension reduction) methods from the module &lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; (see the documentation chapter &lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;Decomposing signals in components (matrix factorization problems)&lt;/a&gt;) .</source>
          <target state="translated">Este ejemplo se aplica a olivetti_faces diferentes m&amp;eacute;todos de descomposici&amp;oacute;n matricial no supervisada (reducci&amp;oacute;n de dimensi&amp;oacute;n) del m&amp;oacute;dulo &lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt; &lt;code&gt;sklearn.decomposition&lt;/code&gt; &lt;/a&gt; (consulte el cap&amp;iacute;tulo de documentaci&amp;oacute;n &lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;Descomposici&amp;oacute;n de se&amp;ntilde;ales en componentes (problemas de factorizaci&amp;oacute;n matricial)&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="33924f5409489cd3edd1b22f28ee011b17a585da" translate="yes" xml:space="preserve">
          <source>This example compares 2 dimensionality reduction strategies:</source>
          <target state="translated">En este ejemplo se comparan dos estrategias de reducción de la dimensionalidad:</target>
        </trans-unit>
        <trans-unit id="1ba8c26b14d0dc5555ed6b18d75cbed15c385668" translate="yes" xml:space="preserve">
          <source>This example compares non-nested and nested cross-validation strategies on a classifier of the iris data set. Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.</source>
          <target state="translated">En este ejemplo se comparan las estrategias de validación cruzada no anidadas y anidadas en un clasificador del conjunto de datos del iris.La validación cruzada anidada (CV)se utiliza a menudo para entrenar un modelo en el que también hay que optimizar los hiperparámetros.La CV anidada estima el error de generalización del modelo subyacente y su búsqueda de (hiper)parámetros.La elección de los parámetros que maximizan la CV no anidada predispone el modelo al conjunto de datos,lo que produce una puntuación demasiado optimista.</target>
        </trans-unit>
        <trans-unit id="7ef1cb506f6769f9ea8f57cc850c6090d62898eb" translate="yes" xml:space="preserve">
          <source>This example compares the timing of Birch (with and without the global clustering step) and MiniBatchKMeans on a synthetic dataset having 100,000 samples and 2 features generated using make_blobs.</source>
          <target state="translated">Este ejemplo compara el tiempo de Birch (con y sin el paso de agrupación global)y MiniBatchKMeans en un conjunto de datos sintéticos que tiene 100.000 muestras y 2 características generadas usando make_blobs.</target>
        </trans-unit>
        <trans-unit id="8901e1f5225dc1b7e06d2fabb8f06f19a3753c45" translate="yes" xml:space="preserve">
          <source>This example constructs a pipeline that does dimensionality reduction followed by prediction with a support vector classifier. It demonstrates the use of &lt;code&gt;GridSearchCV&lt;/code&gt; and &lt;code&gt;Pipeline&lt;/code&gt; to optimize over different classes of estimators in a single CV run &amp;ndash; unsupervised &lt;code&gt;PCA&lt;/code&gt; and &lt;code&gt;NMF&lt;/code&gt; dimensionality reductions are compared to univariate feature selection during the grid search.</source>
          <target state="translated">Este ejemplo construye una canalizaci&amp;oacute;n que realiza una reducci&amp;oacute;n de dimensionalidad seguida de una predicci&amp;oacute;n con un clasificador de vectores de soporte. Demuestra el uso de &lt;code&gt;GridSearchCV&lt;/code&gt; y &lt;code&gt;Pipeline&lt;/code&gt; para optimizar diferentes clases de estimadores en una sola ejecuci&amp;oacute;n de CV: las reducciones de dimensionalidad de &lt;code&gt;PCA&lt;/code&gt; y &lt;code&gt;NMF&lt;/code&gt; no supervisadas se comparan con la selecci&amp;oacute;n de caracter&amp;iacute;sticas univariadas durante la b&amp;uacute;squeda de la cuadr&amp;iacute;cula.</target>
        </trans-unit>
        <trans-unit id="b084d11db0bc218128b35a7afe55ac9fa7c4daf1" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge regression. Concretely, from n_samples 1d points, it suffices to build the Vandermonde matrix, which is n_samples x n_degree+1 and has the following form:</source>
          <target state="translated">Este ejemplo demuestra cómo aproximar una función con un polinomio de grado n_grado utilizando la regresión de cresta.Concretamente,a partir de n_muestras 1d puntos,basta con construir la matriz de Vandermonde,que es n_muestras x n_grado+1 y tiene la siguiente forma:</target>
        </trans-unit>
        <trans-unit id="8509d7895b98e9b3d2d07ed03eae90baa68f1c72" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a checkerboard dataset and bicluster it using the Spectral Biclustering algorithm.</source>
          <target state="translated">Este ejemplo demuestra cómo generar un conjunto de datos de tablero de ajedrez y ponerlo en bicluster usando el algoritmo de Biclustering Espectral.</target>
        </trans-unit>
        <trans-unit id="fb9a20a0b6ffdb624c38c357324f211d180af27f" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a dataset and bicluster it using the Spectral Co-Clustering algorithm.</source>
          <target state="translated">Este ejemplo demuestra cómo generar un conjunto de datos y hacer un bicluster con el algoritmo de Co-Clustering Espectral.</target>
        </trans-unit>
        <trans-unit id="4e024e96a6e8109f327c2d890a3a85ee374e03bd" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to use &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; on a dataset containing different types of features. We use the 20-newsgroups dataset and compute standard bag-of-words features for the subject line and body in separate pipelines as well as ad hoc features on the body. We combine them (with weights) using a ColumnTransformer and finally train a classifier on the combined set of features.</source>
          <target state="translated">Este ejemplo demuestra c&amp;oacute;mo usar &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt; en un conjunto de datos que contiene diferentes tipos de caracter&amp;iacute;sticas. Usamos el conjunto de datos de 20 grupos de noticias y calculamos funciones est&amp;aacute;ndar de bolsa de palabras para la l&amp;iacute;nea de asunto y el cuerpo en canales separados, as&amp;iacute; como funciones ad hoc en el cuerpo. Los combinamos (con pesos) usando un ColumnTransformer y finalmente entrenamos un clasificador en el conjunto combinado de caracter&amp;iacute;sticas.</target>
        </trans-unit>
        <trans-unit id="76d491fec0fed042e6d7927f2dc124b698f9bded" translate="yes" xml:space="preserve">
          <source>This example demonstrates the Spectral Co-clustering algorithm on the twenty newsgroups dataset. The &amp;lsquo;comp.os.ms-windows.misc&amp;rsquo; category is excluded because it contains many posts containing nothing but data.</source>
          <target state="translated">Este ejemplo demuestra el algoritmo de agrupaci&amp;oacute;n conjunta espectral en el conjunto de datos de veinte grupos de noticias. La categor&amp;iacute;a 'comp.os.ms-windows.misc' est&amp;aacute; excluida porque contiene muchas publicaciones que solo contienen datos.</target>
        </trans-unit>
        <trans-unit id="6186f51b55d8cba756254a15fe651e5e8504791a" translate="yes" xml:space="preserve">
          <source>This example demonstrates the behavior of Gaussian mixture models fit on data that was not sampled from a mixture of Gaussian random variables. The dataset is formed by 100 points loosely spaced following a noisy sine curve. There is therefore no ground truth value for the number of Gaussian components.</source>
          <target state="translated">Este ejemplo demuestra el comportamiento de los modelos de mezcla gaussiana ajustados a datos que no fueron muestreados a partir de una mezcla de variables aleatorias gaussianas.El conjunto de datos está formado por 100 puntos vagamente espaciados siguiendo una ruidosa curva sinusoidal.Por lo tanto,no hay un valor de verdad de base para el número de componentes gausianos.</target>
        </trans-unit>
        <trans-unit id="a1949f51dde2d60d7d4d1e707d145c1301537434" translate="yes" xml:space="preserve">
          <source>This example demonstrates the power of semisupervised learning by training a Label Spreading model to classify handwritten digits with sets of very few labels.</source>
          <target state="translated">Este ejemplo demuestra el poder del aprendizaje semisupervisado al entrenar un modelo de difusión de etiquetas para clasificar los dígitos escritos a mano con conjuntos de muy pocas etiquetas.</target>
        </trans-unit>
        <trans-unit id="c6020c6a7334e89ced3b2c5f4b02f4ed9989e37f" translate="yes" xml:space="preserve">
          <source>This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called &lt;strong&gt;underfitting&lt;/strong&gt;. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will &lt;strong&gt;overfit&lt;/strong&gt; the training data, i.e. it learns the noise of the training data. We evaluate quantitatively &lt;strong&gt;overfitting&lt;/strong&gt; / &lt;strong&gt;underfitting&lt;/strong&gt; by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.</source>
          <target state="translated">Este ejemplo demuestra los problemas de desajuste y sobreajuste y c&amp;oacute;mo podemos usar la regresi&amp;oacute;n lineal con caracter&amp;iacute;sticas polinomiales para aproximar funciones no lineales. La gr&amp;aacute;fica muestra la funci&amp;oacute;n que queremos aproximar, que es parte de la funci&amp;oacute;n coseno. Adem&amp;aacute;s, se muestran las muestras de la funci&amp;oacute;n real y las aproximaciones de diferentes modelos. Los modelos tienen caracter&amp;iacute;sticas polinomiales de diferentes grados. Podemos ver que una funci&amp;oacute;n lineal (polinomio con grado 1) no es suficiente para ajustar las muestras de entrenamiento. Esto se llama &lt;strong&gt;desajuste&lt;/strong&gt; . Un polinomio de grado 4 se aproxima a la funci&amp;oacute;n verdadera casi a la perfecci&amp;oacute;n. Sin embargo, para grados m&amp;aacute;s altos, el modelo &lt;strong&gt;sobreajustar&amp;aacute;&lt;/strong&gt; los datos de entrenamiento, es decir, aprende el ruido de los datos de entrenamiento. Evaluamos cuantitativamente&lt;strong&gt;overfitting&lt;/strong&gt; / &lt;strong&gt;underfitting&lt;/strong&gt; mediante el uso de validaci&amp;oacute;n cruzada. Calculamos el error cuadr&amp;aacute;tico medio (MSE) en el conjunto de validaci&amp;oacute;n, cuanto m&amp;aacute;s alto, menos probable es que el modelo se generalice correctamente a partir de los datos de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="ab8d51c9ac9762c2930c84a5a03c1c12345a6627" translate="yes" xml:space="preserve">
          <source>This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms through &lt;code&gt;preprocessing.PowerTransformer&lt;/code&gt; to map data from various distributions to a normal distribution.</source>
          <target state="translated">Este ejemplo demuestra el uso de las transformaciones de Box-Cox y Yeo-Johnson a trav&amp;eacute;s del &lt;code&gt;preprocessing.PowerTransformer&lt;/code&gt; para mapear datos de varias distribuciones a una distribuci&amp;oacute;n normal.</target>
        </trans-unit>
        <trans-unit id="9b62ef0be0bf7ce49acab00a23e04164c0becf9d" translate="yes" xml:space="preserve">
          <source>This example does not perform any learning over the data (see &lt;a href=&quot;../applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;Species distribution modeling&lt;/a&gt; for an example of classification based on the attributes in this dataset). It simply shows the kernel density estimate of observed data points in geospatial coordinates.</source>
          <target state="translated">Este ejemplo no realiza ning&amp;uacute;n aprendizaje sobre los datos (consulte &lt;a href=&quot;../applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;Modelado de distribuci&amp;oacute;n de especies&lt;/a&gt; para ver un ejemplo de clasificaci&amp;oacute;n basada en los atributos de este conjunto de datos). Simplemente muestra la estimaci&amp;oacute;n de la densidad del n&amp;uacute;cleo de los puntos de datos observados en coordenadas geoespaciales.</target>
        </trans-unit>
        <trans-unit id="03eea75ae69c05ca0f9dc1a13d89bbd210d48145" translate="yes" xml:space="preserve">
          <source>This example doesn&amp;rsquo;t show it, as we&amp;rsquo;re in a low-dimensional space, but another advantage of the Dirichlet process model is that it can fit full covariance matrices effectively even when there are less examples per cluster than there are dimensions in the data, due to regularization properties of the inference algorithm.</source>
          <target state="translated">Este ejemplo no lo muestra, ya que estamos en un espacio de baja dimensi&amp;oacute;n, pero otra ventaja del modelo de proceso de Dirichlet es que puede ajustarse a matrices de covarianza total de manera efectiva incluso cuando hay menos ejemplos por grupo que dimensiones en el datos, debido a las propiedades de regularizaci&amp;oacute;n del algoritmo de inferencia.</target>
        </trans-unit>
        <trans-unit id="7b398c0b1dbb0f3edb9cc17097a9c9f8696a24cc" translate="yes" xml:space="preserve">
          <source>This example employs several unsupervised learning techniques to extract the stock market structure from variations in historical quotes.</source>
          <target state="translated">Este ejemplo emplea varias técnicas de aprendizaje no supervisadas para extraer la estructura del mercado de valores de las variaciones de las cotizaciones históricas.</target>
        </trans-unit>
        <trans-unit id="41937f256baaea1198c4d043d4bb81b61df0d50b" translate="yes" xml:space="preserve">
          <source>This example fits a Gradient Boosting model with least squares loss and 500 regression trees of depth 4.</source>
          <target state="translated">Este ejemplo se ajusta a un modelo de Gradient Boosting con la menor pérdida de cuadrados y 500 árboles de regresión de profundidad 4.</target>
        </trans-unit>
        <trans-unit id="e8121408498cf6fb8c886d50eef2580465ff3307" translate="yes" xml:space="preserve">
          <source>This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two &amp;ldquo;Gaussian quantiles&amp;rdquo; clusters (see &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;sklearn.datasets.make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value.</source>
          <target state="translated">Este ejemplo se ajusta a un mu&amp;ntilde;&amp;oacute;n de decisi&amp;oacute;n AdaBoosted en un conjunto de datos de clasificaci&amp;oacute;n no linealmente separables compuesto por dos grupos de &quot;cuantiles gaussianos&quot; (ver &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt; &lt;code&gt;sklearn.datasets.make_gaussian_quantiles&lt;/code&gt; &lt;/a&gt; ) y traza el l&amp;iacute;mite de decisi&amp;oacute;n y las puntuaciones de decisi&amp;oacute;n. Las distribuciones de las puntuaciones de decisi&amp;oacute;n se muestran por separado para las muestras de clase A y B. La etiqueta de clase predicha para cada muestra est&amp;aacute; determinada por el signo de la puntuaci&amp;oacute;n de decisi&amp;oacute;n. Las muestras con puntuaciones de decisi&amp;oacute;n superiores a cero se clasifican como B y, de lo contrario, se clasifican como A. La magnitud de una puntuaci&amp;oacute;n de decisi&amp;oacute;n determina el grado de semejanza con la etiqueta de clase predicha. Adem&amp;aacute;s, se podr&amp;iacute;a construir un nuevo conjunto de datos que contenga una pureza deseada de clase B, por ejemplo, seleccionando solo muestras con una puntuaci&amp;oacute;n de decisi&amp;oacute;n por encima de alg&amp;uacute;n valor.</target>
        </trans-unit>
        <trans-unit id="02c230a18f98a72777c5f2652062014b16511fe8" translate="yes" xml:space="preserve">
          <source>This example has a fair amount of visualization-related code, as visualization is crucial here to display the graph. One of the challenge is to position the labels minimizing overlap. For this we use an heuristic based on the direction of the nearest neighbor along each axis.</source>
          <target state="translated">Este ejemplo tiene una buena cantidad de código relacionado con la visualización,ya que la visualización es crucial aquí para mostrar el gráfico.Uno de los retos es posicionar las etiquetas minimizando la superposición.Para ello utilizamos una heurística basada en la dirección del vecino más cercano a lo largo de cada eje.</target>
        </trans-unit>
        <trans-unit id="259139974bd9ca7304e763dff02af979bb7908e6" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;) and a non-stationary kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt;). On this particular dataset, the &lt;code&gt;DotProduct&lt;/code&gt; kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; often obtain better results.</source>
          <target state="translated">Este ejemplo ilustra GPC en datos XOR. Se comparan un kernel isotr&amp;oacute;pico estacionario ( &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt; ) y un kernel no estacionario ( &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt; ). En este conjunto de datos en particular, el n&amp;uacute;cleo &lt;code&gt;DotProduct&lt;/code&gt; obtiene resultados considerablemente mejores porque los l&amp;iacute;mites de clase son lineales y coinciden con los ejes de coordenadas. En la pr&amp;aacute;ctica, sin embargo, los granos estacionarios como el &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt; suelen obtener mejores resultados.</target>
        </trans-unit>
        <trans-unit id="a467b781e30c272f4f5e4645f1261bd726b14e8d" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In general, stationary kernels often obtain better results.</source>
          <target state="translated">Este ejemplo ilustra el GPC en los datos de XOR.Se comparan un núcleo estacionario e isotrópico (RBF)y un núcleo no estacionario (DotProduct).En este conjunto de datos en particular,el núcleo DotProduct obtiene resultados considerablemente mejores porque los límites de la clase son lineales y coinciden con los ejes de coordenadas.En general,los núcleos estacionarios suelen obtener mejores resultados.</target>
        </trans-unit>
        <trans-unit id="b43f5231cb55a1a95a64bd8afe5472e7955fc98b" translate="yes" xml:space="preserve">
          <source>This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble.</source>
          <target state="translated">Este ejemplo ilustra y compara la descomposición del sesgo-varianza del error cuadrático medio esperado de un solo estimador contra un conjunto de empaquetamiento.</target>
        </trans-unit>
        <trans-unit id="0a3450a632d656139c95a4f138b569cec22ba6ef" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The first figure compares the learned model of KRR and SVR when both complexity/regularization and bandwidth of the RBF kernel are optimized using grid-search. The learned functions are very similar; however, fitting KRR is approx. seven times faster than fitting SVR (both with grid-search). However, prediction of 100000 target values is more than tree times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">Este ejemplo ilustra ambos métodos en un conjunto de datos artificial,que consiste en una función de objetivo sinusoidal y un fuerte ruido añadido a cada quinto punto de datos.La primera figura compara el modelo aprendido de la RBC y la RVS cuando tanto la complejidad/regularización como el ancho de banda del núcleo de la RBC se optimizan mediante la búsqueda en cuadrículas.Las funciones aprendidas son muy similares;sin embargo,ajustar la RBC es aproximadamente siete veces más rápido que ajustar la SVR (ambas con la búsqueda en la cuadrícula).Sin embargo,la predicción de 100000 valores objetivo es más de tres veces más rápida con la SVR,ya que ha aprendido un modelo escaso utilizando sólo aproximadamente 1/3 de los 100 puntos de datos de entrenamiento como vectores de apoyo.</target>
        </trans-unit>
        <trans-unit id="bdbaaa805869c3c13d157f267aeffaf332ff1284" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (l) and periodicity of the kernel (p). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">Este ejemplo ilustra ambos m&amp;eacute;todos en un conjunto de datos artificial, que consta de una funci&amp;oacute;n objetivo sinusoidal y ruido fuerte. La figura compara el modelo aprendido de KRR y GPR basado en un kernel ExpSineSquared, que es adecuado para aprender funciones peri&amp;oacute;dicas. Los hiperpar&amp;aacute;metros del kernel controlan la suavidad (l) y la periodicidad del kernel (p). Adem&amp;aacute;s, el nivel de ruido de los datos se aprende expl&amp;iacute;citamente mediante GPR mediante un componente adicional de WhiteKernel en el kernel y mediante el par&amp;aacute;metro de regularizaci&amp;oacute;n alfa de KRR.</target>
        </trans-unit>
        <trans-unit id="745a420beb7d4cfe3dcb516bc28a36f2576e5395" translate="yes" xml:space="preserve">
          <source>This example illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">Este ejemplo ilustra cómo los cambios en la calibración del sigmoide predijeron las probabilidades de un problema de clasificación de 3 clases.Se ilustra el estándar 2-simplex,donde las tres esquinas corresponden a las tres clases.Las flechas apuntan desde los vectores de probabilidad predichos por un clasificador no calibrado hasta los vectores de probabilidad predichos por el mismo clasificador después de la calibración del sigmoide en un conjunto de validación de retención.Los colores indican la verdadera clase de una instancia (rojo:clase 1,verde:clase 2,azul:clase 3).</target>
        </trans-unit>
        <trans-unit id="12c733d8527ccd2c1862324a52d9d453fa3717b9" translate="yes" xml:space="preserve">
          <source>This example illustrates how the Mahalanobis distances are affected by outlying data: observations drawn from a contaminating distribution are not distinguishable from the observations coming from the real, Gaussian distribution that one may want to work with. Using MCD-based Mahalanobis distances, the two populations become distinguishable. Associated applications are outliers detection, observations ranking, clustering, &amp;hellip; For visualization purpose, the cubic root of the Mahalanobis distances are represented in the boxplot, as Wilson and Hilferty suggest [2]</source>
          <target state="translated">Este ejemplo ilustra c&amp;oacute;mo las distancias de Mahalanobis se ven afectadas por los datos perif&amp;eacute;ricos: las observaciones extra&amp;iacute;das de una distribuci&amp;oacute;n contaminante no se pueden distinguir de las observaciones que provienen de la distribuci&amp;oacute;n gaussiana real con la que uno puede querer trabajar. Usando distancias de Mahalanobis basadas en MCD, las dos poblaciones se vuelven distinguibles. Las aplicaciones asociadas son detecci&amp;oacute;n de valores at&amp;iacute;picos, clasificaci&amp;oacute;n de observaciones, agrupamiento,&amp;hellip; Para fines de visualizaci&amp;oacute;n, la ra&amp;iacute;z c&amp;uacute;bica de las distancias de Mahalanobis se representa en el diagrama de caja, como sugieren Wilson y Hilferty [2]</target>
        </trans-unit>
        <trans-unit id="e6c2656adbcd3c5e59b375bc18300061f72c931d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;sklearn.ensemble.GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping using many fewer estimators. This can significantly reduce training time, memory usage and prediction latency.</source>
          <target state="translated">Este ejemplo ilustra c&amp;oacute;mo se puede utilizar la detenci&amp;oacute;n anticipada en el modelo &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;sklearn.ensemble.GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; para lograr casi la misma precisi&amp;oacute;n en comparaci&amp;oacute;n con un modelo creado sin la detenci&amp;oacute;n anticipada utilizando muchos menos estimadores. Esto puede reducir significativamente el tiempo de entrenamiento, el uso de memoria y la latencia de predicci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="c9534999032b13d85edbba90f8420279af14435d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping. This can significantly reduce training time. Note that scores differ between the stopping criteria even from early iterations because some of the training data is held out with the validation stopping criterion.</source>
          <target state="translated">Este ejemplo ilustra c&amp;oacute;mo se puede utilizar la detenci&amp;oacute;n anticipada en el modelo &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt; para lograr casi la misma precisi&amp;oacute;n en comparaci&amp;oacute;n con un modelo creado sin detenci&amp;oacute;n anticipada. Esto puede reducir significativamente el tiempo de entrenamiento. Tenga en cuenta que las puntuaciones difieren entre los criterios de detenci&amp;oacute;n incluso desde las primeras iteraciones porque algunos de los datos de entrenamiento se mantienen con el criterio de detenci&amp;oacute;n de validaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="c861b8fa50d5165e1e9cdc8044114c0ba76be7f1" translate="yes" xml:space="preserve">
          <source>This example illustrates how to apply different preprocessing and feature extraction pipelines to different subsets of features, using &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt;. This is particularly handy for the case of datasets that contain heterogeneous data types, since we may want to scale the numeric features and one-hot encode the categorical ones.</source>
          <target state="translated">Este ejemplo ilustra c&amp;oacute;mo aplicar diferentes canalizaciones de extracci&amp;oacute;n de caracter&amp;iacute;sticas y preprocesamiento a diferentes subconjuntos de caracter&amp;iacute;sticas, utilizando &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt; . Esto es particularmente &amp;uacute;til para el caso de conjuntos de datos que contienen tipos de datos heterog&amp;eacute;neos, ya que es posible que deseemos escalar las caracter&amp;iacute;sticas num&amp;eacute;ricas y codificar de forma directa las categ&amp;oacute;ricas.</target>
        </trans-unit>
        <trans-unit id="7688b615fac5133028d275260a50b4a9e7d6a213" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML.</source>
          <target state="translated">Este ejemplo ilustra que el GPR con un núcleo de suma que incluye un núcleo blanco puede estimar el nivel de ruido de los datos.Una ilustración del paisaje de probabilidad logarítmica-marginal (LML)muestra que existen dos máximos locales de LML.</target>
        </trans-unit>
        <trans-unit id="aab3b4258aabcd6bfe55c7c5adf04622c59229dc" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML. The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise. The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">Este ejemplo ilustra que el GPR con un núcleo de suma que incluye un núcleo blanco puede estimar el nivel de ruido de los datos.Una ilustración del paisaje de probabilidad logarítmica-marginal (LML)muestra que existen dos máximos locales de LML.El primero corresponde a un modelo con un alto nivel de ruido y una gran escala de longitud,que explica todas las variaciones de los datos por el ruido.El segundo tiene un nivel de ruido más bajo y una escala de longitud menor,lo que explica la mayor parte de la variación por la relación funcional libre de ruido.El segundo modelo tiene una mayor probabilidad;sin embargo,dependiendo del valor inicial de los hiperparámetros,la optimización basada en el gradiente podría también converger hacia la solución de alto ruido.Por consiguiente,es importante repetir la optimización varias veces para diferentes inicializaciones.</target>
        </trans-unit>
        <trans-unit id="d19a1528c92e37a37fb4fd9cad2cad1ac3d91123" translate="yes" xml:space="preserve">
          <source>This example illustrates the differences between univariate F-test statistics and mutual information.</source>
          <target state="translated">Este ejemplo ilustra las diferencias entre las estadísticas de pruebas F univariadas y la información mutua.</target>
        </trans-unit>
        <trans-unit id="b411e019157b9b0953b37eb36e27bc6a8ffb3f5f" translate="yes" xml:space="preserve">
          <source>This example illustrates the effect of the parameters &lt;code&gt;gamma&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; of the Radial Basis Function (RBF) kernel SVM.</source>
          <target state="translated">Este ejemplo ilustra el efecto de los par&amp;aacute;metros &lt;code&gt;gamma&lt;/code&gt; y &lt;code&gt;C&lt;/code&gt; del kernel SVM de la funci&amp;oacute;n de base radial (RBF).</target>
        </trans-unit>
        <trans-unit id="0f49634dcf1598fde3c403bd7ddd702e3816c634" translate="yes" xml:space="preserve">
          <source>This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.</source>
          <target state="translated">Este ejemplo ilustra la necesidad de una estimación robusta de covarianza en un conjunto de datos reales.Es útil tanto para la detección de valores atípicos como para una mejor comprensión de la estructura de los datos.</target>
        </trans-unit>
        <trans-unit id="dc09ff23a3cae32be328d47d0a0a7e64185cd75a" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).</source>
          <target state="translated">Este ejemplo ilustra la probabilidad prevista de GPC para un núcleo RBF con diferentes opciones de los hiperparámetros.La primera figura muestra la probabilidad prevista de GPC con hiperparámetros elegidos arbitrariamente y con los hiperparámetros correspondientes a la máxima probabilidad logarítmica-marginal (LML).</target>
        </trans-unit>
        <trans-unit id="d49b62c58d1a1ad4b52cfdb4df97043fcb25dfc7" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="translated">Este ejemplo ilustra la probabilidad prevista de GPC para un núcleo RBF isotrópico y anisotrópico en una versión bidimensional para el conjunto de datos del iris.El núcleo RBF anisotrópico obtiene una probabilidad logarítmica-marginal ligeramente más alta asignando diferentes escalas de longitud a las dos dimensiones de los rasgos.</target>
        </trans-unit>
        <trans-unit id="f8e8b4dfa6bc8bbc237c34d26328609c3561c0d3" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="translated">Este ejemplo ilustra la probabilidad prevista de GPC para un núcleo RBF isotrópico y anisotrópico en una versión bidimensional para el conjunto de datos del iris.Esto ilustra la aplicabilidad de la GPC a la clasificación no binaria.El núcleo RBF anisotrópico obtiene una probabilidad logarítmica-marginal ligeramente más alta asignando diferentes escalas de longitud a las dos dimensiones de los rasgos.</target>
        </trans-unit>
        <trans-unit id="d60d503f722a9b87495f756d071794c2e2c52164" translate="yes" xml:space="preserve">
          <source>This example illustrates the prior and posterior of a GPR with different kernels. Mean, standard deviation, and 10 samples are shown for both prior and posterior.</source>
          <target state="translated">Este ejemplo ilustra el anterior y posterior de un GPR con diferentes núcleos.Se muestra la media,la desviación estándar y 10 muestras tanto para el anterior como para el posterior.</target>
        </trans-unit>
        <trans-unit id="7e4f09a45ae58596e6f6f82a5f372f88442c0679" translate="yes" xml:space="preserve">
          <source>This example illustrates the use of the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt; meta-estimator to perform multi-output regression. A random forest regressor is used, which supports multi-output regression natively, so the results can be compared.</source>
          <target state="translated">Este ejemplo ilustra el uso del &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;metaestimador multioutput.MultiOutputRegressor&lt;/a&gt; para realizar una regresi&amp;oacute;n de m&amp;uacute;ltiples salidas. Se utiliza un regresor de bosque aleatorio, que admite la regresi&amp;oacute;n de m&amp;uacute;ltiples salidas de forma nativa, por lo que los resultados se pueden comparar.</target>
        </trans-unit>
        <trans-unit id="8a4413b8994df2fb3a9be31d12e5c3eff453a657" translate="yes" xml:space="preserve">
          <source>This example illustrates visually in the feature space a comparison by results using two different component analysis techniques.</source>
          <target state="translated">Este ejemplo ilustra visualmente en el espacio de las características una comparación por resultados utilizando dos técnicas diferentes de análisis de componentes.</target>
        </trans-unit>
        <trans-unit id="2cd2616385e5968100e838cea35843639b5d4649" translate="yes" xml:space="preserve">
          <source>This example is based on Figure 10.2 from Hastie et al 2009 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and illustrates the difference in performance between the discrete SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features.</source>
          <target state="translated">Este ejemplo se basa en la Figura 10.2 de Hastie et al 2009 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; e ilustra la diferencia en el rendimiento entre el algoritmo de impulso discreto SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; y el algoritmo real de impulso SAMME.R. Ambos algoritmos se eval&amp;uacute;an en una tarea de clasificaci&amp;oacute;n binaria donde el objetivo Y es una funci&amp;oacute;n no lineal de 10 caracter&amp;iacute;sticas de entrada.</target>
        </trans-unit>
        <trans-unit id="f74f72b73b7f5fc12f50525ee3a073668f796ed6" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &amp;ldquo;Gaussian Processes for Machine Learning&amp;rdquo; [RW2006]. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="translated">Este ejemplo se basa en la Secci&amp;oacute;n 5.4.3 de &amp;ldquo;Procesos gaussianos para el aprendizaje autom&amp;aacute;tico&amp;rdquo; [RW2006]. Ilustra un ejemplo de ingenier&amp;iacute;a de kernel compleja y optimizaci&amp;oacute;n de hiperpar&amp;aacute;metros usando el ascenso de gradiente en la probabilidad log-marginal. Los datos consisten en las concentraciones de CO2 atmosf&amp;eacute;rico promedio mensuales (en partes por mill&amp;oacute;n por volumen (ppmv)) recolectadas en el Observatorio Mauna Loa en Hawai, entre 1958 y 2001. El objetivo es modelar la concentraci&amp;oacute;n de CO2 en funci&amp;oacute;n del tiempo t .</target>
        </trans-unit>
        <trans-unit id="b3b5741f90375b259727a574f2a0488e7637e5bc" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id2&quot;&gt;[RW2006]&lt;/a&gt;. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="translated">Este ejemplo se basa en la Secci&amp;oacute;n 5.4.3 de &lt;a href=&quot;#rw2006&quot; id=&quot;id2&quot;&gt;[RW2006]&lt;/a&gt; . Ilustra un ejemplo de ingenier&amp;iacute;a de kernel compleja y optimizaci&amp;oacute;n de hiperpar&amp;aacute;metros usando el ascenso de gradiente en la probabilidad log-marginal. Los datos consisten en las concentraciones de CO2 atmosf&amp;eacute;rico promedio mensuales (en partes por mill&amp;oacute;n por volumen (ppmv)) recolectadas en el Observatorio Mauna Loa en Hawai, entre 1958 y 1997. El objetivo es modelar la concentraci&amp;oacute;n de CO2 en funci&amp;oacute;n del tiempo t .</target>
        </trans-unit>
        <trans-unit id="9ecc1c02a68f38d70271669af08df8c1497b1d98" translate="yes" xml:space="preserve">
          <source>This example is commented in the &lt;a href=&quot;../../tutorial/basic/tutorial#introduction&quot;&gt;tutorial section of the user manual&lt;/a&gt;.</source>
          <target state="translated">Este ejemplo se comenta en la &lt;a href=&quot;../../tutorial/basic/tutorial#introduction&quot;&gt;secci&amp;oacute;n de tutoriales del manual de usuario&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="9143eb314f05280f157b4081755ef4be5d0d1857" translate="yes" xml:space="preserve">
          <source>This example is meant to illustrate situations where k-means will produce unintuitive and possibly unexpected clusters. In the first three plots, the input data does not conform to some implicit assumption that k-means makes and undesirable clusters are produced as a result. In the last plot, k-means returns intuitive clusters despite unevenly sized blobs.</source>
          <target state="translated">Este ejemplo tiene por objeto ilustrar situaciones en las que los medios &quot;k&quot; producirán agrupaciones poco intuitivas y posiblemente inesperadas.En los tres primeros gráficos,los datos de entrada no se ajustan a ninguna suposición implícita de que k-means hace y se producen clusters no deseados como resultado.En la última parcela,k-means devuelve cúmulos intuitivos a pesar de las manchas de tamaño desigual.</target>
        </trans-unit>
        <trans-unit id="fb84e7488212d58818869cb1a848aafb46dc6573" translate="yes" xml:space="preserve">
          <source>This example plots the covariance ellipsoids of each class and decision boundary learned by LDA and QDA. The ellipsoids display the double standard deviation for each class. With LDA, the standard deviation is the same for all the classes, while each class has its own standard deviation with QDA.</source>
          <target state="translated">En este ejemplo se trazan los elipsoides de covarianza de cada clase y límite de decisión aprendidos por LDA y QDA.Los elipsoides muestran la doble desviación estándar de cada clase.Con LDA,la desviación estándar es la misma para todas las clases,mientras que cada clase tiene su propia desviación estándar con QDA.</target>
        </trans-unit>
        <trans-unit id="61cf8846c08926de131cab630666b0d7a1ff4033" translate="yes" xml:space="preserve">
          <source>This example plots the ellipsoids obtained from a toy dataset (mixture of three Gaussians) fitted by the &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; class models with a Dirichlet distribution prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_distribution'&lt;/code&gt;) and a Dirichlet process prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_process'&lt;/code&gt;). On each figure, we plot the results for three different values of the weight concentration prior.</source>
          <target state="translated">Este ejemplo traza los elipsoides obtenidos de un conjunto de datos de juguete (mezcla de tres gaussianos) ajustados por los modelos de clase &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; con una distribuci&amp;oacute;n de Dirichlet anterior ( &lt;code&gt;weight_concentration_prior_type='dirichlet_distribution'&lt;/code&gt; ) y un proceso de Dirichlet anterior ( &lt;code&gt;weight_concentration_prior_type='dirichlet_process'&lt;/code&gt; ). En cada figura, graficamos los resultados para tres valores diferentes de la concentraci&amp;oacute;n de peso antes.</target>
        </trans-unit>
        <trans-unit id="b0d46d4faf4d4d45c7ba844c05b8ff931d890d6c" translate="yes" xml:space="preserve">
          <source>This example presents the different strategies implemented in KBinsDiscretizer:</source>
          <target state="translated">Este ejemplo presenta las diferentes estrategias implementadas en KBinsDiscretizer:</target>
        </trans-unit>
        <trans-unit id="aff95617011fc0f39a1d4573107679df90fa3c83" translate="yes" xml:space="preserve">
          <source>This example reproduces Figure 1 of Zhu et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="translated">Este ejemplo reproduce la Figura 1 de Zhu et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; y muestra c&amp;oacute;mo el impulso puede mejorar la precisi&amp;oacute;n de la predicci&amp;oacute;n en un problema de varias clases. El conjunto de datos de clasificaci&amp;oacute;n se construye tomando una distribuci&amp;oacute;n normal est&amp;aacute;ndar de diez dimensiones y definiendo tres clases separadas por esferas conc&amp;eacute;ntricas de diez dimensiones anidadas, de modo que haya aproximadamente el mismo n&amp;uacute;mero de muestras en cada clase (cuantiles de la distribuci&amp;oacute;n \ (\ chi ^ 2 \) ).</target>
        </trans-unit>
        <trans-unit id="f0193d5eae04ea4c45d1272eb8f24ceb76514e1e" translate="yes" xml:space="preserve">
          <source>This example serves as a visual check that IPCA is able to find a similar projection of the data to PCA (to a sign flip), while only processing a few samples at a time. This can be considered a &amp;ldquo;toy example&amp;rdquo;, as IPCA is intended for large datasets which do not fit in main memory, requiring incremental approaches.</source>
          <target state="translated">Este ejemplo sirve como una verificaci&amp;oacute;n visual de que IPCA puede encontrar una proyecci&amp;oacute;n similar de los datos a PCA (a un cambio de signo), mientras que solo procesa algunas muestras a la vez. Esto puede considerarse un &quot;ejemplo de juguete&quot;, ya que IPCA est&amp;aacute; dise&amp;ntilde;ado para grandes conjuntos de datos que no caben en la memoria principal, lo que requiere enfoques incrementales.</target>
        </trans-unit>
        <trans-unit id="3dfa9a56451c107730b94ff094ad060fe6660b05" translate="yes" xml:space="preserve">
          <source>This example should be taken with a grain of salt, as the intuition conveyed does not necessarily carry over to real datasets. Particularly in high-dimensional spaces, data can more easily be separated linearly. Moreover, using feature discretization and one-hot encoding increases the number of features, which easily lead to overfitting when the number of samples is small.</source>
          <target state="translated">Este ejemplo debe tomarse con un grano de sal,ya que la intuición que se transmite no se transmite necesariamente a los conjuntos de datos reales.Particularmente en espacios de altas dimensiones,los datos pueden ser separados más fácilmente de forma lineal.Además,la utilización de la discretización de las características y la codificación de una sola vez aumenta el número de características,lo que fácilmente conduce a la sobrecarga cuando el número de muestras es pequeño.</target>
        </trans-unit>
        <trans-unit id="ad21e470ff2bffe875ca12e50c6f8a883eaa0515" translate="yes" xml:space="preserve">
          <source>This example shows an example usage of the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">Este ejemplo muestra un ejemplo de uso del m&amp;eacute;todo &lt;code&gt;split&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="05321a1b8964aa5eaada463be8f8b6ab44686817" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different anomaly detection algorithms on 2D datasets. Datasets contain one or two modes (regions of high density) to illustrate the ability of algorithms to cope with multimodal data.</source>
          <target state="translated">Este ejemplo muestra las características de los diferentes algoritmos de detección de anomalías en conjuntos de datos 2D.Los conjuntos de datos contienen uno o dos modos (regiones de alta densidad)para ilustrar la capacidad de los algoritmos de hacer frente a los datos multimodales.</target>
        </trans-unit>
        <trans-unit id="9671740bdc9e0f010272719df08d61d30b070724" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different clustering algorithms on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D. With the exception of the last dataset, the parameters of each of these dataset-algorithm pairs has been tuned to produce good clustering results. Some algorithms are more sensitive to parameter values than others.</source>
          <target state="translated">Este ejemplo muestra caracter&amp;iacute;sticas de diferentes algoritmos de agrupamiento en conjuntos de datos que son &quot;interesantes&quot; pero a&amp;uacute;n en 2D. Con la excepci&amp;oacute;n del &amp;uacute;ltimo conjunto de datos, los par&amp;aacute;metros de cada uno de estos pares de algoritmo y conjunto de datos se han ajustado para producir buenos resultados de agrupaci&amp;oacute;n. Algunos algoritmos son m&amp;aacute;s sensibles a los valores de los par&amp;aacute;metros que otros.</target>
        </trans-unit>
        <trans-unit id="408c25df8162bc85c75adf89aefb6c4283aab313" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different linkage methods for hierarchical clustering on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D.</source>
          <target state="translated">Este ejemplo muestra las caracter&amp;iacute;sticas de diferentes m&amp;eacute;todos de vinculaci&amp;oacute;n para la agrupaci&amp;oacute;n jer&amp;aacute;rquica en conjuntos de datos que son &quot;interesantes&quot; pero a&amp;uacute;n en 2D.</target>
        </trans-unit>
        <trans-unit id="ee904b77cbf769dbe7d1093eb852f864329f4bb5" translate="yes" xml:space="preserve">
          <source>This example shows how kernel density estimation (KDE), a powerful non-parametric density estimation technique, can be used to learn a generative model for a dataset. With this generative model in place, new samples can be drawn. These new samples reflect the underlying model of the data.</source>
          <target state="translated">Este ejemplo muestra cómo la estimación de la densidad del núcleo (KDE),una potente técnica de estimación de la densidad no paramétrica,puede utilizarse para aprender un modelo generativo para un conjunto de datos.Con este modelo generativo en su lugar,se pueden dibujar nuevas muestras.Estas nuevas muestras reflejan el modelo subyacente de los datos.</target>
        </trans-unit>
        <trans-unit id="54102d8f78c42d496181e5bcdf5a40bdaee3e42d" translate="yes" xml:space="preserve">
          <source>This example shows how quantile regression can be used to create prediction intervals.</source>
          <target state="translated">Este ejemplo muestra cómo la regresión de cuantiles puede utilizarse para crear intervalos de predicción.</target>
        </trans-unit>
        <trans-unit id="682ea376dc5fd413204f119c7903367cc1b71149" translate="yes" xml:space="preserve">
          <source>This example shows how to build a classification pipeline with a BernoulliRBM feature extractor and a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; classifier. The hyperparameters of the entire model (learning rate, hidden layer size, regularization) were optimized by grid search, but the search is not reproduced here because of runtime constraints.</source>
          <target state="translated">Este ejemplo muestra c&amp;oacute;mo crear una canalizaci&amp;oacute;n de clasificaci&amp;oacute;n con un extractor de caracter&amp;iacute;sticas BernoulliRBM y un clasificador &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; . Los hiperpar&amp;aacute;metros de todo el modelo (tasa de aprendizaje, tama&amp;ntilde;o de capa oculta, regularizaci&amp;oacute;n) se optimizaron mediante la b&amp;uacute;squeda de cuadr&amp;iacute;cula, pero la b&amp;uacute;squeda no se reproduce aqu&amp;iacute; debido a restricciones de tiempo de ejecuci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="e6287a37f5ab2f7e58b3aa65bfc9b371f8d2e434" translate="yes" xml:space="preserve">
          <source>This example shows how to obtain partial dependence plots from a &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; trained on the California housing dataset. The example is taken from &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">Este ejemplo muestra c&amp;oacute;mo obtener parcelas de dependencia parcial de un &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; capacitado en el conjunto de datos de vivienda de California. El ejemplo se toma de &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="dd4b844c3488b502b915a6b11d22b13911beaa74" translate="yes" xml:space="preserve">
          <source>This example shows how to perform univariate feature selection before running a SVC (support vector classifier) to improve the classification scores.</source>
          <target state="translated">Este ejemplo muestra cómo realizar una selección univariante de características antes de ejecutar un SVC (clasificador de vectores de apoyo)para mejorar los resultados de la clasificación.</target>
        </trans-unit>
        <trans-unit id="47a26e628df959c3e5ed3ffe4f4f3490e8927a8d" translate="yes" xml:space="preserve">
          <source>This example shows how to plot some of the first layer weights in a MLPClassifier trained on the MNIST dataset.</source>
          <target state="translated">Este ejemplo muestra cómo trazar algunos de los primeros pesos de las capas en un clasificador MLPC entrenado en el conjunto de datos del MNIST.</target>
        </trans-unit>
        <trans-unit id="dd07bd7e7afed03f3c2a3c74458c42dc14028dfe" translate="yes" xml:space="preserve">
          <source>This example shows how to plot the decision surface for four SVM classifiers with different kernels.</source>
          <target state="translated">Este ejemplo muestra cómo trazar la superficie de decisión para cuatro clasificadores SVM con diferentes núcleos.</target>
        </trans-unit>
        <trans-unit id="52dcf2b8bf47a153b3ba3beca30c9af85b850fad" translate="yes" xml:space="preserve">
          <source>This example shows how to use &lt;code&gt;cross_val_predict&lt;/code&gt; to visualize prediction errors.</source>
          <target state="translated">Este ejemplo muestra c&amp;oacute;mo usar &lt;code&gt;cross_val_predict&lt;/code&gt; para visualizar errores de predicci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="351c02b1031f149a08df70cbeed39cd2a6bb9ec7" translate="yes" xml:space="preserve">
          <source>This example shows that Kernel PCA is able to find a projection of the data that makes data linearly separable.</source>
          <target state="translated">Este ejemplo muestra que el núcleo PCA es capaz de encontrar una proyección de los datos que hace que los datos sean linealmente separables.</target>
        </trans-unit>
        <trans-unit id="607fdb6fda0285694fd1dd982f83082f2f9a6687" translate="yes" xml:space="preserve">
          <source>This example shows that imputing the missing values can give better results than discarding the samples containing any missing value. Imputing does not always improve the predictions, so please check via cross-validation. Sometimes dropping rows or using marker values is more effective.</source>
          <target state="translated">Este ejemplo muestra que la imputación de los valores perdidos puede dar mejores resultados que el descarte de las muestras que contienen cualquier valor perdido.La imputación no siempre mejora las predicciones,así que por favor compruébelo mediante una validación cruzada.A veces,el descarte de filas o el uso de valores de marcadores es más efectivo.</target>
        </trans-unit>
        <trans-unit id="b6045a3110197ccfd64402c3c879acac73bdaadb" translate="yes" xml:space="preserve">
          <source>This example shows that model selection can be performed with Gaussian Mixture Models using information-theoretic criteria (BIC). Model selection concerns both the covariance type and the number of components in the model. In that case, AIC also provides the right result (not shown to save time), but BIC is better suited if the problem is to identify the right model. Unlike Bayesian procedures, such inferences are prior-free.</source>
          <target state="translated">Este ejemplo muestra que la selección de modelos puede realizarse con los modelos de mezcla gaussiana utilizando criterios de información-teórica (BIC).La selección del modelo se refiere tanto al tipo de covarianza como al número de componentes del modelo.En ese caso,el AIC también proporciona el resultado correcto (no se muestra para ahorrar tiempo),pero el BIC es más adecuado si el problema es identificar el modelo correcto.A diferencia de los procedimientos bayesianos,esas inferencias no tienen antecedentes.</target>
        </trans-unit>
        <trans-unit id="7ed4db944a196b1cb5ab23d8834c95cd5421c757" translate="yes" xml:space="preserve">
          <source>This example shows that you can do non-linear regression with a linear model, using a pipeline to add non-linear features. Kernel methods extend this idea and can induce very high (even infinite) dimensional feature spaces.</source>
          <target state="translated">Este ejemplo muestra que se puede hacer una regresión no lineal con un modelo lineal,usando una tubería para añadir características no lineales.Los métodos del núcleo extienden esta idea y pueden inducir espacios de características dimensionales muy altos (incluso infinitos).</target>
        </trans-unit>
        <trans-unit id="48dcc848c2d6e1561f6c336d60b0fb518f9ab59a" translate="yes" xml:space="preserve">
          <source>This example shows the ROC response of different datasets, created from K-fold cross-validation. Taking all of these curves, it is possible to calculate the mean area under curve, and see the variance of the curve when the training set is split into different subsets. This roughly shows how the classifier output is affected by changes in the training data, and how different the splits generated by K-fold cross-validation are from one another.</source>
          <target state="translated">Este ejemplo muestra la respuesta ROC de diferentes conjuntos de datos,creados a partir de la validación cruzada de la carpeta K.Tomando todas estas curvas,es posible calcular el área media bajo la curva,y ver la variación de la curva cuando el conjunto de entrenamiento se divide en diferentes subconjuntos.Esto muestra a grandes rasgos cómo la salida del clasificador se ve afectada por los cambios en los datos de entrenamiento,y cuán diferentes son las divisiones generadas por la validación cruzada de la curva K entre sí.</target>
        </trans-unit>
        <trans-unit id="7b92e840bf44fca60c7edc394c5ccf3da0546857" translate="yes" xml:space="preserve">
          <source>This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is simply the graph of 20 nearest neighbors.</source>
          <target state="translated">Este ejemplo muestra el efecto de imponer un gráfico de conectividad para captar la estructura local en los datos.El gráfico es simplemente el gráfico de los 20 vecinos más cercanos.</target>
        </trans-unit>
        <trans-unit id="a122bd5b47879a72d10715b2e2741901d74ebd5f" translate="yes" xml:space="preserve">
          <source>This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles. Such a dataset is acquired in &lt;strong&gt;computed tomography&lt;/strong&gt; (CT).</source>
          <target state="translated">Este ejemplo muestra la reconstrucci&amp;oacute;n de una imagen a partir de un conjunto de proyecciones paralelas, adquiridas a lo largo de diferentes &amp;aacute;ngulos. Dicho conjunto de datos se adquiere en &lt;strong&gt;tomograf&amp;iacute;a computarizada&lt;/strong&gt; (TC).</target>
        </trans-unit>
        <trans-unit id="277c7e399c7f521a9367c3279ba6605ffa32bb5b" translate="yes" xml:space="preserve">
          <source>This example shows the use of forests of trees to evaluate the importance of the pixels in an image classification task (faces). The hotter the pixel, the more important.</source>
          <target state="translated">Este ejemplo muestra el uso de bosques de árboles para evaluar la importancia de los píxeles en una tarea de clasificación de imágenes (caras).Cuanto más caliente el píxel,más importante.</target>
        </trans-unit>
        <trans-unit id="181df9c721e88b620fbcd3038af372d2bc17958d" translate="yes" xml:space="preserve">
          <source>This example shows the use of multi-output estimator to complete images. The goal is to predict the lower half of a face given its upper half.</source>
          <target state="translated">Este ejemplo muestra el uso de un estimador multi-salida para completar las imágenes.El objetivo es predecir la mitad inferior de una cara dada su mitad superior.</target>
        </trans-unit>
        <trans-unit id="c194d9ad3fd820ff97a5b60a54a77ad5e096ac46" translate="yes" xml:space="preserve">
          <source>This example simulates a multi-label document classification problem. The dataset is generated randomly based on the following process:</source>
          <target state="translated">Este ejemplo simula un problema de clasificación de documentos de múltiples etiquetas.El conjunto de datos se genera aleatoriamente en base al siguiente proceso:</target>
        </trans-unit>
        <trans-unit id="beca62f6aeebe1ac71c16bdf04b277689fc51da6" translate="yes" xml:space="preserve">
          <source>This example uses &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;Spectral clustering&lt;/a&gt; on a graph created from voxel-to-voxel difference on an image to break this image into multiple partly-homogeneous regions.</source>
          <target state="translated">Este ejemplo utiliza &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;agrupaci&amp;oacute;n espectral&lt;/a&gt; en un gr&amp;aacute;fico creado a partir de la diferencia de v&amp;oacute;xeles a v&amp;oacute;xeles en una imagen para dividir esta imagen en varias regiones parcialmente homog&amp;eacute;neas.</target>
        </trans-unit>
        <trans-unit id="b1b3f16a0bb367262a72b24da0008ca16f86a096" translate="yes" xml:space="preserve">
          <source>This example uses a large dataset of faces to learn a set of 20 x 20 images patches that constitute faces.</source>
          <target state="translated">Este ejemplo utiliza un gran conjunto de datos de rostros para aprender un conjunto de parches de imágenes de 20 x 20 que constituyen los rostros.</target>
        </trans-unit>
        <trans-unit id="0ad2a4281006cf2ce3833b3619a37abf58d678e0" translate="yes" xml:space="preserve">
          <source>This example uses different scalers, transformers, and normalizers to bring the data within a pre-defined range.</source>
          <target state="translated">Este ejemplo utiliza diferentes escaladores,transformadores y normalizadores para llevar los datos dentro de un rango predefinido.</target>
        </trans-unit>
        <trans-unit id="e1bfbae38c6acd2b8b362eacb6ca0227cf12cdc6" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; class to demonstrate the principles of Kernel Density Estimation in one dimension.</source>
          <target state="translated">Este ejemplo utiliza la clase &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt; para demostrar los principios de la estimaci&amp;oacute;n de la densidad del n&amp;uacute;cleo en una dimensi&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="1c91a9c343e9d52fecff06520d2432c55d78e2a0" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;code&gt;scipy.stats&lt;/code&gt; module, which contains many useful distributions for sampling parameters, such as &lt;code&gt;expon&lt;/code&gt;, &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;uniform&lt;/code&gt; or &lt;code&gt;randint&lt;/code&gt;. In principle, any function can be passed that provides a &lt;code&gt;rvs&lt;/code&gt; (random variate sample) method to sample a value. A call to the &lt;code&gt;rvs&lt;/code&gt; function should provide independent random samples from possible parameter values on consecutive calls.</source>
          <target state="translated">Este ejemplo utiliza el m&amp;oacute;dulo &lt;code&gt;scipy.stats&lt;/code&gt; , que contiene muchas distribuciones &amp;uacute;tiles para los par&amp;aacute;metros de muestreo, como &lt;code&gt;expon&lt;/code&gt; , &lt;code&gt;gamma&lt;/code&gt; , &lt;code&gt;uniform&lt;/code&gt; o &lt;code&gt;randint&lt;/code&gt; . En principio, se puede pasar cualquier funci&amp;oacute;n que proporcione un m&amp;eacute;todo &lt;code&gt;rvs&lt;/code&gt; (muestra variable aleatoria) para muestrear un valor. Una llamada a la funci&amp;oacute;n &lt;code&gt;rvs&lt;/code&gt; deber&amp;iacute;a proporcionar muestras aleatorias independientes de posibles valores de par&amp;aacute;metros en llamadas consecutivas.</target>
        </trans-unit>
        <trans-unit id="d9381762b80079a0275cf5384c50652951b2c3b8" translate="yes" xml:space="preserve">
          <source>This example uses the only the first feature of the &lt;code&gt;diabetes&lt;/code&gt; dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.</source>
          <target state="translated">Este ejemplo utiliza la &amp;uacute;nica primera caracter&amp;iacute;stica del conjunto de datos de &lt;code&gt;diabetes&lt;/code&gt; , con el fin de ilustrar un gr&amp;aacute;fico bidimensional de esta t&amp;eacute;cnica de regresi&amp;oacute;n. La l&amp;iacute;nea recta se puede ver en el gr&amp;aacute;fico, mostrando c&amp;oacute;mo la regresi&amp;oacute;n lineal intenta dibujar una l&amp;iacute;nea recta que minimizar&amp;aacute; mejor la suma de cuadrados residual entre las respuestas observadas en el conjunto de datos y las respuestas predichas por la aproximaci&amp;oacute;n lineal.</target>
        </trans-unit>
        <trans-unit id="c6bb5d76743a81844f0fc5afc16345d399cae103" translate="yes" xml:space="preserve">
          <source>This example visualizes some training loss curves for different stochastic learning strategies, including SGD and Adam. Because of time-constraints, we use several small datasets, for which L-BFGS might be more suitable. The general trend shown in these examples seems to carry over to larger datasets, however.</source>
          <target state="translated">Este ejemplo visualiza algunas curvas de pérdida de entrenamiento para diferentes estrategias de aprendizaje estocástico,incluyendo SGD y Adam.Debido a las limitaciones de tiempo,usamos varios pequeños conjuntos de datos,para los cuales L-BFGS podría ser más adecuado.Sin embargo,la tendencia general que se muestra en estos ejemplos parece trasladarse a conjuntos de datos más grandes.</target>
        </trans-unit>
        <trans-unit id="65646a35859e04e16667e59a0e282463725f9c9e" translate="yes" xml:space="preserve">
          <source>This example visualizes the behavior of several common scikit-learn objects for comparison.</source>
          <target state="translated">Este ejemplo visualiza el comportamiento de varios objetos comunes de aprendizaje de ciencias para compararlos.</target>
        </trans-unit>
        <trans-unit id="49dcb9492cd2c3de6ca468ca869fddbd4adf1109" translate="yes" xml:space="preserve">
          <source>This example visualizes the partitions given by several trees and shows how the transformation can also be used for non-linear dimensionality reduction or non-linear classification.</source>
          <target state="translated">En este ejemplo se visualizan las particiones dadas por varios árboles y se muestra cómo la transformación también puede utilizarse para la reducción de la dimensionalidad no lineal o la clasificación no lineal.</target>
        </trans-unit>
        <trans-unit id="6888e16176d5ba984d30e5b2aecac9e36e3202eb" translate="yes" xml:space="preserve">
          <source>This example will also work by replacing &lt;code&gt;SVC(kernel=&quot;linear&quot;)&lt;/code&gt; with &lt;code&gt;SGDClassifier(loss=&quot;hinge&quot;)&lt;/code&gt;. Setting the &lt;code&gt;loss&lt;/code&gt; parameter of the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; equal to &lt;code&gt;hinge&lt;/code&gt; will yield behaviour such as that of a SVC with a linear kernel.</source>
          <target state="translated">Este ejemplo tambi&amp;eacute;n funcionar&amp;aacute; reemplazando &lt;code&gt;SVC(kernel=&quot;linear&quot;)&lt;/code&gt; con &lt;code&gt;SGDClassifier(loss=&quot;hinge&quot;)&lt;/code&gt; . Establecer el par&amp;aacute;metro de &lt;code&gt;loss&lt;/code&gt; del &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; igual a la &lt;code&gt;hinge&lt;/code&gt; producir&amp;aacute; un comportamiento como el de un SVC con un kernel lineal.</target>
        </trans-unit>
        <trans-unit id="08633b59361c5b4332dffd09b9ac681bbe920080" translate="yes" xml:space="preserve">
          <source>This example, inspired from Chen&amp;rsquo;s publication [1], shows a comparison of the estimated MSE of the LW and OAS methods, using Gaussian distributed data.</source>
          <target state="translated">Este ejemplo, inspirado en la publicaci&amp;oacute;n de Chen [1], muestra una comparaci&amp;oacute;n del MSE estimado de los m&amp;eacute;todos LW y OAS, utilizando datos distribuidos en Gauss.</target>
        </trans-unit>
        <trans-unit id="f9260a90e6c35e520398765702d07497fe04f1a8" translate="yes" xml:space="preserve">
          <source>This examples shows how a classifier is optimized by cross-validation, which is done using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; object on a development set that comprises only half of the available labeled data.</source>
          <target state="translated">Este ejemplo muestra c&amp;oacute;mo se optimiza un clasificador mediante la validaci&amp;oacute;n cruzada, que se realiza utilizando el objeto &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt; en un conjunto de desarrollo que comprende solo la mitad de los datos etiquetados disponibles.</target>
        </trans-unit>
        <trans-unit id="e16048c7f7cf75798d53fe7e675cc81cd1d6af8b" translate="yes" xml:space="preserve">
          <source>This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability.</source>
          <target state="translated">Este ejemplo muestra el uso de los bosques de árboles para evaluar la importancia de las características en una tarea de clasificación artificial.Las barras rojas son la importancia de los rasgos del bosque,junto con su variabilidad entre los árboles.</target>
        </trans-unit>
        <trans-unit id="4fb4f14900902539137295018be0a0c7a07b1094" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-estimators-tut&quot;&gt;Cross-validated estimators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">Este ejercicio se utiliza en la parte de &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-estimators-tut&quot;&gt;Estimadores con validaci&amp;oacute;n cruzada&lt;/a&gt; de la secci&amp;oacute;n &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Selecci&amp;oacute;n&lt;/a&gt; del modelo: elecci&amp;oacute;n de estimadores y sus par&amp;aacute;metros del &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;tutorial sobre aprendizaje estad&amp;iacute;stico para el procesamiento de datos cient&amp;iacute;ficos&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="621b0c8349abb129c7bc150bd9745f46f49af3ab" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-generators-tut&quot;&gt;Cross-validation generators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">Este ejercicio se utiliza en la secci&amp;oacute;n &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-generators-tut&quot;&gt;Generadores de validaci&amp;oacute;n cruzada&lt;/a&gt; de la secci&amp;oacute;n &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Selecci&amp;oacute;n de modelos: elecci&amp;oacute;n de estimadores y sus par&amp;aacute;metros&lt;/a&gt; del &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;tutorial sobre aprendizaje estad&amp;iacute;stico para el procesamiento de datos cient&amp;iacute;ficos&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="6915ba6643fea2f9e387885428a6e6189c7df3bf" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#clf-tut&quot;&gt;Classification&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">Este ejercicio se utiliza en la parte de &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#clf-tut&quot;&gt;Clasificaci&amp;oacute;n&lt;/a&gt; del &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;aprendizaje supervisado: predicci&amp;oacute;n de una variable de salida a partir de la&lt;/a&gt; secci&amp;oacute;n de observaciones de alta dimensi&amp;oacute;n del &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;tutorial A sobre aprendizaje estad&amp;iacute;stico para el procesamiento de datos cient&amp;iacute;ficos&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="421684adc1a996556d28fe46ff4c101c2f3063ef" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#using-kernels-tut&quot;&gt;Using kernels&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">Este ejercicio se utiliza en la secci&amp;oacute;n &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#using-kernels-tut&quot;&gt;Uso de n&amp;uacute;cleos&lt;/a&gt; de &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Aprendizaje supervisado: predicci&amp;oacute;n de una variable de salida a partir de observaciones de alta dimensi&amp;oacute;n&lt;/a&gt; del &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;tutorial A sobre aprendizaje estad&amp;iacute;stico para el procesamiento de datos cient&amp;iacute;ficos&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="dadb46eeaf7a2bf2f8f61dc107ba2d3f5d55d33a" translate="yes" xml:space="preserve">
          <source>This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix \(Y\), i.e., \(y_{i,k} = 1\) if sample \(i\) has label \(k\) taken from a set of \(K\) labels. Let \(P\) be a matrix of probability estimates, with \(p_{i,k} = \operatorname{Pr}(t_{i,k} = 1)\). Then the log loss of the whole set is</source>
          <target state="translated">Esto se extiende al caso de las multiclases como sigue.Dejemos que las etiquetas verdaderas de un conjunto de muestras se codifiquen como una matriz indicadora binaria 1-de-K \N-es decir,\N si la muestra tiene una etiqueta \N tomada de un conjunto de etiquetas \N-K.Sea una matriz de estimaciones de probabilidad,con &quot;p_{i,k}=nombre del operador&quot; (Pr}(t_{i,k}=1)).Entonces la pérdida de registro de todo el conjunto es</target>
        </trans-unit>
        <trans-unit id="826a67cf49f96f56e23921af61e52712fab61d33" translate="yes" xml:space="preserve">
          <source>This factory function wraps scoring functions for use in GridSearchCV and cross_val_score. It takes a score function, such as &lt;code&gt;accuracy_score&lt;/code&gt;, &lt;code&gt;mean_squared_error&lt;/code&gt;, &lt;code&gt;adjusted_rand_index&lt;/code&gt; or &lt;code&gt;average_precision&lt;/code&gt; and returns a callable that scores an estimator&amp;rsquo;s output.</source>
          <target state="translated">Esta funci&amp;oacute;n de f&amp;aacute;brica envuelve las funciones de puntuaci&amp;oacute;n para su uso en GridSearchCV y cross_val_score. Se necesita una funci&amp;oacute;n de puntuaci&amp;oacute;n, como &lt;code&gt;accuracy_score&lt;/code&gt; , &lt;code&gt;mean_squared_error&lt;/code&gt; , &lt;code&gt;adjusted_rand_index&lt;/code&gt; o &lt;code&gt;average_precision&lt;/code&gt; y devuelve una salida que las puntuaciones de un estimador exigible.</target>
        </trans-unit>
        <trans-unit id="5eb7a4eb6e2161d3d9f2a7b4c7010506ccf35ad1" translate="yes" xml:space="preserve">
          <source>This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined:</source>
          <target state="translated">Esta característica corresponde a la longitud del sépalo en cm.Una vez aplicada la transformación del cuantil,esos puntos de referencia se acercan mucho a los percentiles previamente definidos:</target>
        </trans-unit>
        <trans-unit id="18a1d2c5a41fd4d57af7a6bb802060cade230322" translate="yes" xml:space="preserve">
          <source>This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.</source>
          <target state="translated">Este algoritmo de selección de características sólo mira las características (X),no las salidas deseadas (y),por lo que puede utilizarse para el aprendizaje no supervisado.</target>
        </trans-unit>
        <trans-unit id="677c582ff4a458e9dc8e636909bbbb985fe5cce6" translate="yes" xml:space="preserve">
          <source>This figure is created using the &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; preprocessor. This preprocessor transforms an input data matrix into a new data matrix of a given degree. It can be used as follows:</source>
          <target state="translated">Esta figura se crea utilizando el preprocesador &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; &lt;/a&gt; . Este preprocesador transforma una matriz de datos de entrada en una nueva matriz de datos de un grado determinado. Se puede utilizar de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="adbd1df9acbf84e51fe5dc83e34aca6a9423eabf" translate="yes" xml:space="preserve">
          <source>This figure shows an example of such an ROC curve:</source>
          <target state="translated">Esta figura muestra un ejemplo de tal curva ROC:</target>
        </trans-unit>
        <trans-unit id="e1207da0df5038f5f29891db83b7d5022ead8471" translate="yes" xml:space="preserve">
          <source>This folder is used by some large dataset loaders to avoid downloading the data several times.</source>
          <target state="translated">Esta carpeta es utilizada por algunos grandes cargadores de datos para evitar descargar los datos varias veces.</target>
        </trans-unit>
        <trans-unit id="697a12fdadac01e298b3e16a8634659c2b054014" translate="yes" xml:space="preserve">
          <source>This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.</source>
          <target state="translated">Este formato es un formato basado en texto,con una muestra por línea.No almacena características de valor cero,por lo que es adecuado para conjuntos de datos escasos.</target>
        </trans-unit>
        <trans-unit id="a2fe6f0ee6734c2a60bcbb1d0d95dc9dfd886002" translate="yes" xml:space="preserve">
          <source>This format is used as the default format for both svmlight and the libsvm command line programs.</source>
          <target state="translated">Este formato se utiliza como el formato por defecto tanto para svmlight como para los programas de línea de comandos libsvm.</target>
        </trans-unit>
        <trans-unit id="a2e505f490185afab8e1242d5832b6872eb9a667" translate="yes" xml:space="preserve">
          <source>This formulation has two advantages over other ways of computing distances. First, it is computationally efficient when dealing with sparse data. Second, if one argument varies but the other remains unchanged, then &lt;code&gt;dot(x, x)&lt;/code&gt; and/or &lt;code&gt;dot(y, y)&lt;/code&gt; can be pre-computed.</source>
          <target state="translated">Esta formulaci&amp;oacute;n tiene dos ventajas sobre otras formas de calcular distancias. Primero, es computacionalmente eficiente cuando se trata de datos escasos. En segundo lugar, si un argumento var&amp;iacute;a pero el otro permanece sin cambios, entonces el &lt;code&gt;dot(x, x)&lt;/code&gt; y / o el &lt;code&gt;dot(y, y)&lt;/code&gt; pueden calcularse previamente.</target>
        </trans-unit>
        <trans-unit id="fd76e45c139161a6c2340aa524dcf0429afb583e" translate="yes" xml:space="preserve">
          <source>This function computes Cohen&amp;rsquo;s kappa &lt;a href=&quot;#r219a3b9132e1-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;, a score that expresses the level of agreement between two annotators on a classification problem. It is defined as</source>
          <target state="translated">Esta funci&amp;oacute;n calcula el kappa de Cohen &lt;a href=&quot;#r219a3b9132e1-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; , una puntuaci&amp;oacute;n que expresa el nivel de acuerdo entre dos anotadores en un problema de clasificaci&amp;oacute;n. Se define como</target>
        </trans-unit>
        <trans-unit id="1a26f30c64bc915159ba37349551edb033f8db68" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance).</source>
          <target state="translated">Esta función calcula para cada fila de X,el índice de la fila de Y que está más cerca (según la distancia especificada).</target>
        </trans-unit>
        <trans-unit id="40ca8ec3788866f2480b1619115207e644d05cac" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance). The minimal distances are also returned.</source>
          <target state="translated">Esta función calcula para cada fila de X,el índice de la fila de Y que está más cerca (según la distancia especificada).También se devuelven las distancias mínimas.</target>
        </trans-unit>
        <trans-unit id="6e14f241e1c03d6b67a6c0f22515d375ae432487" translate="yes" xml:space="preserve">
          <source>This function crawls the module and gets all classes that inherit from BaseEstimator. Classes that are defined in test-modules are not included. By default meta_estimators such as GridSearchCV are also not included.</source>
          <target state="translated">Esta función rastrea el módulo y obtiene todas las clases que hereda del BaseEstimator.Las clases que se definen en los módulos de prueba no están incluidas.Por defecto,los meta_estimadores como GridSearchCV tampoco están incluidos.</target>
        </trans-unit>
        <trans-unit id="311d27372cf5315019acfac7480657777c623446" translate="yes" xml:space="preserve">
          <source>This function does not try to extract features into a numpy array or scipy sparse matrix. In addition, if load_content is false it does not try to load the files in memory.</source>
          <target state="translated">Esta función no trata de extraer características en una matriz numérica o una matriz de scipy sparse.Además,si load_content es falso no intenta cargar los archivos en la memoria.</target>
        </trans-unit>
        <trans-unit id="28042729acf4bf75d343c82f013b112dad91f4c8" translate="yes" xml:space="preserve">
          <source>This function generates a GraphViz representation of the decision tree, which is then written into &lt;code&gt;out_file&lt;/code&gt;. Once exported, graphical renderings can be generated using, for example:</source>
          <target state="translated">Esta funci&amp;oacute;n genera una representaci&amp;oacute;n GraphViz del &amp;aacute;rbol de decisiones, que luego se escribe en &lt;code&gt;out_file&lt;/code&gt; . Una vez exportados, se pueden generar representaciones gr&amp;aacute;ficas utilizando, por ejemplo:</target>
        </trans-unit>
        <trans-unit id="90af5dc07a0d6b1b987fbe6284966c35b8f7dbed" translate="yes" xml:space="preserve">
          <source>This function implements Test 1 in:</source>
          <target state="translated">Esta función implementa la prueba 1 en:</target>
        </trans-unit>
        <trans-unit id="51510777ab8c25d02b45243629e172250d646e40" translate="yes" xml:space="preserve">
          <source>This function is called with the estimated model and the randomly selected data: &lt;code&gt;is_model_valid(model, X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with &lt;code&gt;is_data_valid&lt;/code&gt;. &lt;code&gt;is_model_valid&lt;/code&gt; should therefore only be used if the estimated model is needed for making the rejection decision.</source>
          <target state="translated">Esta funci&amp;oacute;n se llama con el modelo estimado y los datos seleccionados aleatoriamente: &lt;code&gt;is_model_valid(model, X, y)&lt;/code&gt; . Si su valor de retorno es Falso, se omite la submuestra seleccionada al azar actual. Rechazar muestras con esta funci&amp;oacute;n es computacionalmente m&amp;aacute;s costoso que con &lt;code&gt;is_data_valid&lt;/code&gt; . &lt;code&gt;is_model_valid&lt;/code&gt; lo tanto, is_model_valid solo debe usarse si el modelo estimado es necesario para tomar la decisi&amp;oacute;n de rechazo.</target>
        </trans-unit>
        <trans-unit id="a4283f593950d3e9c84617d07a78fd011e78bfa4" translate="yes" xml:space="preserve">
          <source>This function is called with the randomly selected data before the model is fitted to it: &lt;code&gt;is_data_valid(X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped.</source>
          <target state="translated">Esta funci&amp;oacute;n se llama con los datos seleccionados al azar antes de que el modelo se &lt;code&gt;is_data_valid(X, y)&lt;/code&gt; a ella: is_data_valid (X, y) . Si su valor de retorno es Falso, se omite la submuestra seleccionada al azar actual.</target>
        </trans-unit>
        <trans-unit id="7289fd594a0de96a89a572bf0a7bd6e9501fda52" translate="yes" xml:space="preserve">
          <source>This function is equivalent to mapping load_svmlight_file over a list of files, except that the results are concatenated into a single, flat list and the samples vectors are constrained to all have the same number of features.</source>
          <target state="translated">Esta función equivale a mapear load_svmlight_file sobre una lista de archivos,excepto que los resultados se concatenan en una única lista plana y los vectores de las muestras se limitan a que todos tengan el mismo número de características.</target>
        </trans-unit>
        <trans-unit id="828fa7414b6e6676bd49f6624bf5ec1232d13777" translate="yes" xml:space="preserve">
          <source>This function makes it possible to compute this transformation for a fixed set of class labels known ahead of time.</source>
          <target state="translated">Esta función permite calcular esta transformación para un conjunto fijo de etiquetas de clase conocidas de antemano.</target>
        </trans-unit>
        <trans-unit id="910452d7cd5e91358a13a185cadee882cea17632" translate="yes" xml:space="preserve">
          <source>This function modifies the estimator in-place.</source>
          <target state="translated">Esta función modifica el estimador en el lugar.</target>
        </trans-unit>
        <trans-unit id="3e0bd9f3948e27380d0112f277598d80d17269d2" translate="yes" xml:space="preserve">
          <source>This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions. Here is a small example of how to use the &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">Esta funci&amp;oacute;n requiere el valor binario verdadero y las puntuaciones objetivo, que pueden ser estimaciones de probabilidad de la clase positiva, valores de confianza o decisiones binarias. Aqu&amp;iacute; hay un peque&amp;ntilde;o ejemplo de c&amp;oacute;mo usar la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="2ddc8c678c75b432a0f0fafa6490a9a69a784bf8" translate="yes" xml:space="preserve">
          <source>This function returns a score of the mean square difference between the actual outcome and the predicted probability of the possible outcome. The actual outcome has to be 1 or 0 (true or false), while the predicted probability of the actual outcome can be a value between 0 and 1.</source>
          <target state="translated">Esta función devuelve una puntuación de la diferencia cuadrada media entre el resultado real y la probabilidad prevista del posible resultado.El resultado real tiene que ser 1 ó 0 (verdadero o falso),mientras que la probabilidad prevista del resultado real puede ser un valor entre 0 y 1.</target>
        </trans-unit>
        <trans-unit id="fbdeef434a34fee928d2d9974f81cfc54768558d" translate="yes" xml:space="preserve">
          <source>This function returns posterior probabilities of classification according to each class on an array of test vectors X.</source>
          <target state="translated">Esta función devuelve probabilidades posteriores de clasificación según cada clase en un conjunto de vectores de prueba X.</target>
        </trans-unit>
        <trans-unit id="f7adc46ef6325367984cfe5d6cabca879706d70d" translate="yes" xml:space="preserve">
          <source>This function returns the Silhouette Coefficient for each sample.</source>
          <target state="translated">Esta función devuelve el Coeficiente de Silueta para cada muestra.</target>
        </trans-unit>
        <trans-unit id="30221178098fdd3682a8c91454092d6226b25e4f" translate="yes" xml:space="preserve">
          <source>This function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use &lt;a href=&quot;sklearn.metrics.silhouette_samples#sklearn.metrics.silhouette_samples&quot;&gt;&lt;code&gt;silhouette_samples&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Esta funci&amp;oacute;n devuelve el coeficiente de silueta medio de todas las muestras. Para obtener los valores de cada muestra, use &lt;a href=&quot;sklearn.metrics.silhouette_samples#sklearn.metrics.silhouette_samples&quot;&gt; &lt;code&gt;silhouette_samples&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="bd812dc35d867d7152375e5009e2698c8db08fc0" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists to allow for a description of the mapping for each of the valid strings.</source>
          <target state="translated">Esta función simplemente devuelve las métricas de distancia válidas de los pares.Existe para permitir una descripción del mapeo de cada una de las cadenas válidas.</target>
        </trans-unit>
        <trans-unit id="fa4705a70e55596dcf2ace89a6d2a8d09a9fcccf" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists, however, to allow for a verbose description of the mapping for each of the valid strings.</source>
          <target state="translated">Esta función simplemente devuelve las métricas de distancia válidas de los pares.Existe,sin embargo,para permitir una descripción verbosa del mapeo para cada una de las cadenas válidas.</target>
        </trans-unit>
        <trans-unit id="d1a7a45215b31f0644d6e686c87b329e59419299" translate="yes" xml:space="preserve">
          <source>This function won&amp;rsquo;t compute the intercept.</source>
          <target state="translated">Esta funci&amp;oacute;n no calcular&amp;aacute; la intersecci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="a808622a520f852134a2d8734b9e29ce0a669efe" translate="yes" xml:space="preserve">
          <source>This function works with dense 2D arrays only.</source>
          <target state="translated">Esta función funciona sólo con matrices 2D densas.</target>
        </trans-unit>
        <trans-unit id="7541ac358f5bb3bc5dcdfc1193ff72d6ac233a66" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost.</source>
          <target state="translated">Este método generador produce el conjunto de probabilidades de clase pronosticadas después de cada iteración de impulso y,por lo tanto,permite la supervisión,como para determinar las probabilidades de clase pronosticadas en un conjunto de pruebas después de cada impulso.</target>
        </trans-unit>
        <trans-unit id="7c1ad29f5d19940cf714626cd821b0934b5bc400" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.</source>
          <target state="translated">Este método generador produce la predicción de conjunto después de cada iteración de impulso y,por lo tanto,permite el seguimiento,como para determinar la predicción en un conjunto de pruebas después de cada impulso.</target>
        </trans-unit>
        <trans-unit id="acc74c06c673308a3e484c230b5ff2c8348cfe79" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.</source>
          <target state="translated">Este método generador produce la puntuación del conjunto después de cada iteración de refuerzo y,por lo tanto,permite el seguimiento,como para determinar la puntuación en un conjunto de pruebas después de cada refuerzo.</target>
        </trans-unit>
        <trans-unit id="cb7462acd1f1e763247c87d170997bea5c436272" translate="yes" xml:space="preserve">
          <source>This illustrates the &lt;code&gt;datasets.make_multilabel_classification&lt;/code&gt; dataset generator. Each sample consists of counts of two features (up to 50 in total), which are differently distributed in each of two classes.</source>
          <target state="translated">Esto ilustra el generador de conjuntos de &lt;code&gt;datasets.make_multilabel_classification&lt;/code&gt; datasets.make_multilabel_classification. Cada muestra consta de recuentos de dos caracter&amp;iacute;sticas (hasta 50 en total), que se distribuyen de manera diferente en cada una de las dos clases.</target>
        </trans-unit>
        <trans-unit id="b80113eed9b4b9668cc4e8b638bede5d1f2cf638" translate="yes" xml:space="preserve">
          <source>This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). It may attract a higher memory complexity when querying these nearest neighborhoods, depending on the &lt;code&gt;algorithm&lt;/code&gt;.</source>
          <target state="translated">Esta implementaci&amp;oacute;n calcula de forma masiva todas las consultas de vecindad, lo que aumenta la complejidad de la memoria a O (nd) donde d es el n&amp;uacute;mero promedio de vecinos, mientras que el DBSCAN original ten&amp;iacute;a una complejidad de memoria O (n). Puede atraer una mayor complejidad de memoria al consultar estos vecindarios m&amp;aacute;s cercanos, seg&amp;uacute;n el &lt;code&gt;algorithm&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="92eb61902d4dfd6571a464d87feff72fe7b32901" translate="yes" xml:space="preserve">
          <source>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. &lt;a href=&quot;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&lt;/a&gt;</source>
          <target state="translated">Esta implementaci&amp;oacute;n se basa en Rubinstein, R., Zibulevsky, M. y Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, abril de 2008. &lt;a href=&quot;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt;http: //www.cs. technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6fb112845601277f8931b295b857e73c1428c8fb" translate="yes" xml:space="preserve">
          <source>This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g. with sparse matrices). This matrix will consume n^2 floats. A couple of mechanisms for getting around this are:</source>
          <target state="translated">Esta implementación no es por defecto eficiente en cuanto a la memoria porque construye una matriz de similitud completa por pares en el caso de que los árboles kd o los árboles bola no puedan utilizarse (por ejemplo,con matrices escasas).Esta matriz consumirá n^2 flotadores.Un par de mecanismos para evitar esto son:</target>
        </trans-unit>
        <trans-unit id="cc51c30dcd01f51cada4be15777f17eb95eb7cbd" translate="yes" xml:space="preserve">
          <source>This implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support. For much faster, GPU-based implementations, as well as frameworks offering much more flexibility to build deep learning architectures, see &lt;a href=&quot;http://scikit-learn.org/stable/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;.</source>
          <target state="translated">Esta implementaci&amp;oacute;n no est&amp;aacute; destinada a aplicaciones a gran escala. En particular, scikit-learn no ofrece soporte para GPU. Para implementaciones mucho m&amp;aacute;s r&amp;aacute;pidas basadas en GPU, as&amp;iacute; como marcos que ofrecen mucha m&amp;aacute;s flexibilidad para construir arquitecturas de aprendizaje profundo, consulte &lt;a href=&quot;http://scikit-learn.org/stable/related_projects.html#related-projects&quot;&gt;Proyectos relacionados&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0e30a130e6449e6025aca5fcf59ecb737e97cb91" translate="yes" xml:space="preserve">
          <source>This implementation is written in Cython and is reasonably fast. However, a faster API-compatible loader is also available at:</source>
          <target state="translated">Esta implementación está escrita en Cython y es razonablemente rápida.Sin embargo,un cargador más rápido compatible con la API también está disponible en:</target>
        </trans-unit>
        <trans-unit id="b488dd9d3cb1238d47d93805595214963db6dd0c" translate="yes" xml:space="preserve">
          <source>This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.</source>
          <target state="translated">Esta implementación produce una representación dispersa de los recuentos utilizando scipy.sparse.csr_matrix.</target>
        </trans-unit>
        <trans-unit id="42bc7bbc3f5bd0df8efbfbad62976f6ec6db583b" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that 3 PLS packages provided in the R language (R-project):</source>
          <target state="translated">Esta implementación proporciona los mismos resultados que los 3 paquetes de PLS proporcionados en el lenguaje R (R-proyecto):</target>
        </trans-unit>
        <trans-unit id="09c013dbb84b7e3d406ea0732bc3a8236ed37cbd" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that the &amp;ldquo;plspm&amp;rdquo; package provided in the R language (R-project), using the function plsca(X, Y). Results are equal or collinear with the function &lt;code&gt;pls(..., mode = &quot;canonical&quot;)&lt;/code&gt; of the &amp;ldquo;mixOmics&amp;rdquo; package. The difference relies in the fact that mixOmics implementation does not exactly implement the Wold algorithm since it does not normalize y_weights to one.</source>
          <target state="translated">Esta implementaci&amp;oacute;n proporciona los mismos resultados que el paquete &amp;ldquo;plspm&amp;rdquo; proporcionado en el lenguaje R (proyecto R), utilizando la funci&amp;oacute;n plsca (X, Y). Los resultados son iguales o colineales con la funci&amp;oacute;n &lt;code&gt;pls(..., mode = &quot;canonical&quot;)&lt;/code&gt; del paquete &quot;mixOmics&quot;. La diferencia radica en el hecho de que la implementaci&amp;oacute;n de mixOmics no implementa exactamente el algoritmo de Wold ya que no normaliza y_weights a uno.</target>
        </trans-unit>
        <trans-unit id="36e4a374c505873717456a086d5c9ed44e5157f6" translate="yes" xml:space="preserve">
          <source>This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems.</source>
          <target state="translated">Esta implementación se negará a centrar las matrices scipy.sparse ya que las haría no dispersas y potencialmente bloquearía el programa con problemas de agotamiento de memoria.</target>
        </trans-unit>
        <trans-unit id="6684c1532df5f751b6b61c242ea952621dc3f4e8" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense and sparse numpy arrays of floating point values.</source>
          <target state="translated">Esta implementación funciona con datos representados como matrices numéricas densas y dispersas de valores de punto flotante.</target>
        </trans-unit>
        <trans-unit id="b0df3cb22108e4cd0ed0fcd534ed03e427412c64" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays of floating point values for the features.</source>
          <target state="translated">Esta implementación funciona con datos representados como matrices numéricas densas de valores de punto flotante para las características.</target>
        </trans-unit>
        <trans-unit id="4abb3ee00da8e0ef45c7b10f884f8d272712ca81" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays or sparse scipy arrays of floating point values.</source>
          <target state="translated">Esta implementación funciona con datos representados como matrices numéricas densas o matrices de scipy dispersas de valores de punto flotante.</target>
        </trans-unit>
        <trans-unit id="c3c22c958df17cff584f8c572beeb762a9a0290e" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).</source>
          <target state="translated">Esta implementación funciona con datos representados como conjuntos densos o escasos de valores de punto flotante para las características.El modelo al que se ajusta puede ser controlado con el parámetro de pérdida;por defecto,se ajusta a una máquina vectorial de soporte lineal (SVM).</target>
        </trans-unit>
        <trans-unit id="c43a7d8bb7931a79100804db2f074a29d45e4b6b" translate="yes" xml:space="preserve">
          <source>This improvement is not visible in the Silhouette Coefficient which is small for both as this measure seem to suffer from the phenomenon called &amp;ldquo;Concentration of Measure&amp;rdquo; or &amp;ldquo;Curse of Dimensionality&amp;rdquo; for high dimensional datasets such as text data. Other measures such as V-measure and Adjusted Rand Index are information theoretic based evaluation scores: as they are only based on cluster assignments rather than distances, hence not affected by the curse of dimensionality.</source>
          <target state="translated">Esta mejora no es visible en el coeficiente de silueta, que es peque&amp;ntilde;o para ambos, ya que esta medida parece sufrir el fen&amp;oacute;meno llamado &quot;concentraci&amp;oacute;n de medida&quot; o &quot;maldici&amp;oacute;n de dimensionalidad&quot; para conjuntos de datos de alta dimensi&amp;oacute;n, como datos de texto. Otras medidas, como la medida V y el &amp;iacute;ndice Rand ajustado, son puntuaciones de evaluaci&amp;oacute;n basadas en la teor&amp;iacute;a de la informaci&amp;oacute;n: ya que solo se basan en asignaciones de grupos en lugar de distancias, por lo que no se ven afectadas por la maldici&amp;oacute;n de la dimensionalidad.</target>
        </trans-unit>
        <trans-unit id="a0d4ffe805942e66e32866a4eb458e728074d78e" translate="yes" xml:space="preserve">
          <source>This initially creates clusters of points normally distributed (std=1) about vertices of an &lt;code&gt;n_informative&lt;/code&gt;-dimensional hypercube with sides of length &lt;code&gt;2*class_sep&lt;/code&gt; and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.</source>
          <target state="translated">Esto crea inicialmente grupos de puntos distribuidos normalmente (std = 1) sobre los v&amp;eacute;rtices de un hipercubo &lt;code&gt;n_informative&lt;/code&gt; dimensional con lados de longitud &lt;code&gt;2*class_sep&lt;/code&gt; y asigna un n&amp;uacute;mero igual de grupos a cada clase. Introduce la interdependencia entre estas caracter&amp;iacute;sticas y agrega varios tipos de ruido adicional a los datos.</target>
        </trans-unit>
        <trans-unit id="5b14c6be212126cf2e3bdc1fe1c6fe8f3c7dc46d" translate="yes" xml:space="preserve">
          <source>This interface is &lt;strong&gt;experimental&lt;/strong&gt; as at version 0.20 and subsequent releases may change attributes without notice (although there should only be minor changes to &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;).</source>
          <target state="translated">Esta interfaz es &lt;strong&gt;experimental&lt;/strong&gt; en la versi&amp;oacute;n 0.20 y las versiones posteriores pueden cambiar los atributos sin previo aviso (aunque solo debe haber cambios menores en los &lt;code&gt;data&lt;/code&gt; y el &lt;code&gt;target&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="7f6be37b4617684744b3ccc169d2c583b6e3ddc1" translate="yes" xml:space="preserve">
          <source>This is a convenience alias to &lt;code&gt;resample(*arrays, replace=False)&lt;/code&gt; to do random permutations of the collections.</source>
          <target state="translated">Este es un alias conveniente para &lt;code&gt;resample(*arrays, replace=False)&lt;/code&gt; a muestrear (* matrices, replace = False) para realizar permutaciones aleatorias de las colecciones.</target>
        </trans-unit>
        <trans-unit id="4632bc2ee98a17257db1d248b06f38b79a53d4ef" translate="yes" xml:space="preserve">
          <source>This is a convenience function; the transformation is done using the default settings for &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;. For more advanced usage (stopword filtering, n-gram extraction, etc.), combine fetch_20newsgroups with a custom &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.HashingVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Esta es una funci&amp;oacute;n de conveniencia; la transformaci&amp;oacute;n se realiza utilizando la configuraci&amp;oacute;n predeterminada para &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; . Para un uso m&amp;aacute;s avanzado (filtrado de palabras vac&amp;iacute;as, extracci&amp;oacute;n de n-gramas, etc.), combine fetch_20newsgroups con &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.HashingVectorizer&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.TfidfTransformer&lt;/code&gt; &lt;/a&gt; o &lt;a href=&quot;sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2b3dbf5e1c5e08d66c77786f2bcbc632afc0312a" translate="yes" xml:space="preserve">
          <source>This is a convenience routine for the sake of testing. For many metrics, the utilities in scipy.spatial.distance.cdist and scipy.spatial.distance.pdist will be faster.</source>
          <target state="translated">Esta es una rutina de conveniencia por el bien de la prueba.Para muchas métricas,las utilidades en cdist de distancia espacial de scipy y pdist de distancia espacial de scipy serán más rápidas.</target>
        </trans-unit>
        <trans-unit id="7a67e0a846a2ab3c88cb06fc2b7950cd30914abe" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. &lt;a href=&quot;https://goo.gl/U2Uwz2&quot;&gt;https://goo.gl/U2Uwz2&lt;/a&gt;</source>
          <target state="translated">&amp;Eacute;sta es una copia de los conjuntos de datos de UCI ML Breast Cancer Wisconsin (Diagnostic). &lt;a href=&quot;https://goo.gl/U2Uwz2&quot;&gt;https://goo.gl/U2Uwz2&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22bae61d9be3213577df5087cb01b12cfdf8dff4" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Wine recognition datasets. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/a&gt;</source>
          <target state="translated">Esta es una copia de los conjuntos de datos de reconocimiento de UCI ML Wine. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c52f45448ee0e84b694b7c38bdbed7fd0e586461" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML housing dataset. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&lt;/a&gt;</source>
          <target state="translated">Esta es una copia del conjunto de datos de viviendas de UCI ML. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3dee2146876159a5a0b048cd24a612fae4a810ca" translate="yes" xml:space="preserve">
          <source>This is a copy of the test set of the UCI ML hand-written digits datasets &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&lt;/a&gt;</source>
          <target state="translated">Esta es una copia del conjunto de prueba de los conjuntos de datos de d&amp;iacute;gitos escritos a mano de UCI ML &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fd4a60c08b29c6d6af237f8cfa14740252c7d04a" translate="yes" xml:space="preserve">
          <source>This is a general function, given points on a curve. For computing the area under the ROC-curve, see &lt;a href=&quot;sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt;. For an alternative way to summarize a precision-recall curve, see &lt;a href=&quot;sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Esta es una funci&amp;oacute;n general, dados puntos en una curva. Para calcular el &amp;aacute;rea bajo la curva ROC, consulte &lt;a href=&quot;sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt; . Para obtener una forma alternativa de resumir una curva de recuperaci&amp;oacute;n de precisi&amp;oacute;n, consulte &lt;a href=&quot;sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="37510c6c60985c0ea76b5bcc4364db965e5a12fd" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="translated">Esta es una abreviatura para el constructor del Transformador de Columna;no requiere,ni permite,nombrar a los transformadores.En su lugar,se les dará un nombre automáticamente basado en sus tipos.Tampoco permite la ponderación.</target>
        </trans-unit>
        <trans-unit id="022d95ed0540e35ea0bdce867e9a502603bc5f51" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the FeatureUnion constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="translated">Esta es una abreviatura para el constructor de FeatureUnion;no requiere,ni permite,nombrar a los transformadores.En su lugar,se les dará un nombre automáticamente basado en sus tipos.Tampoco permite la ponderación.</target>
        </trans-unit>
        <trans-unit id="52a890ca0cc5d284d366294db21e8c380349733e" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.</source>
          <target state="translated">Esta es una abreviatura para el constructor del oleoducto;no requiere,ni permite,nombrar a los estimadores.En su lugar,sus nombres serán puestos en minúsculas de sus tipos automáticamente.</target>
        </trans-unit>
        <trans-unit id="bcbf4cb6d3eb7ea12d02241a9a60f7a4e6044f4d" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.predict(X)&lt;/code&gt;.</source>
          <target state="translated">Este es un contenedor para &lt;code&gt;estimator_.predict(X)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="17ebe8027dbbfba976bb150f0e9172e06d0c02ec" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.score(X, y)&lt;/code&gt;.</source>
          <target state="translated">Este es un contenedor para &lt;code&gt;estimator_.score(X, y)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="602675ab661ad893c615b29b13c1d56146fcfa0b" translate="yes" xml:space="preserve">
          <source>This is an alternative to passing a &lt;code&gt;backend='backend_name'&lt;/code&gt; argument to the &lt;code&gt;Parallel&lt;/code&gt; class constructor. It is particularly useful when calling into library code that uses joblib internally but does not expose the backend argument in its own API.</source>
          <target state="translated">Esta es una alternativa a pasar un argumento &lt;code&gt;backend='backend_name'&lt;/code&gt; al constructor de la clase &lt;code&gt;Parallel&lt;/code&gt; . Es particularmente &amp;uacute;til cuando se llama al c&amp;oacute;digo de la biblioteca que usa joblib internamente pero no expone el argumento de backend en su propia API.</target>
        </trans-unit>
        <trans-unit id="47f9c3947e84bb8a914aa6f2f19cb2c5e42e970f" translate="yes" xml:space="preserve">
          <source>This is an example of &lt;strong&gt;bias/variance tradeoff&lt;/strong&gt;: the larger the ridge &lt;code&gt;alpha&lt;/code&gt; parameter, the higher the bias and the lower the variance.</source>
          <target state="translated">Este es un ejemplo de &lt;strong&gt;compensaci&amp;oacute;n&lt;/strong&gt; de &lt;strong&gt;sesgo / varianza&lt;/strong&gt; : cuanto mayor sea el par&amp;aacute;metro &lt;code&gt;alpha&lt;/code&gt; la cresta , mayor ser&amp;aacute; el sesgo y menor la varianza.</target>
        </trans-unit>
        <trans-unit id="d75c7c933c17fefbabe7c2e292b885d0ecac3a21" translate="yes" xml:space="preserve">
          <source>This is an example of applying &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt;&lt;code&gt;sklearn.decomposition.LatentDirichletAllocation&lt;/code&gt;&lt;/a&gt; on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).</source>
          <target state="translated">Este es un ejemplo de &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt; &lt;/a&gt; aplicar sklearn.decomposition.NMF y &lt;a href=&quot;../../modules/generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt; &lt;code&gt;sklearn.decomposition.LatentDirichletAllocation&lt;/code&gt; &lt;/a&gt; en un corpus de documentos y extraer modelos aditivos de la estructura tem&amp;aacute;tica del corpus. El resultado es una lista de temas, cada uno representado como una lista de t&amp;eacute;rminos (no se muestran los pesos).</target>
        </trans-unit>
        <trans-unit id="53176f2993974522405fa17c9a80a84e38a4969c" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used for classification using an out-of-core approach: learning from data that doesn&amp;rsquo;t fit into main memory. We make use of an online classifier, i.e., one that supports the partial_fit method, that will be fed with batches of examples. To guarantee that the features space remains the same over time we leverage a HashingVectorizer that will project each example into the same feature space. This is especially useful in the case of text classification where new features (words) may appear in each batch.</source>
          <target state="translated">Este es un ejemplo que muestra c&amp;oacute;mo se puede usar scikit-learn para la clasificaci&amp;oacute;n usando un enfoque fuera del n&amp;uacute;cleo: aprender de datos que no caben en la memoria principal. Hacemos uso de un clasificador en l&amp;iacute;nea, es decir, uno que admita el m&amp;eacute;todo de ajuste parcial, que ser&amp;aacute; alimentado con lotes de ejemplos. Para garantizar que el espacio de funciones siga siendo el mismo a lo largo del tiempo, aprovechamos un HashingVectorizer que proyectar&amp;aacute; cada ejemplo en el mismo espacio de funciones. Esto es especialmente &amp;uacute;til en el caso de la clasificaci&amp;oacute;n de texto donde pueden aparecer nuevas caracter&amp;iacute;sticas (palabras) en cada lote.</target>
        </trans-unit>
        <trans-unit id="1620bf9fc1f7795235eabc8e2a67657eca16390d" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.</source>
          <target state="translated">Se trata de un ejemplo que muestra cómo se puede utilizar scikit-learn para clasificar documentos por temas utilizando un enfoque de bolsa de palabras.Este ejemplo utiliza una matriz scipy.sparse para almacenar las características y demuestra varios clasificadores que pueden manejar eficientemente las matrices sparse.</target>
        </trans-unit>
        <trans-unit id="687fdb042e4ef171de769c4722977550577ec678" translate="yes" xml:space="preserve">
          <source>This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.</source>
          <target state="translated">Se trata de un ejemplo que muestra cómo se puede utilizar el scikit-learn para agrupar documentos por temas utilizando un enfoque de bolsa de palabras.Este ejemplo utiliza una matriz scipy.sparse para almacenar las características en lugar de las matrices numéricas estándar.</target>
        </trans-unit>
        <trans-unit id="c505c2f7b70a5aa0d5582bdc56a7d9627b32a4d8" translate="yes" xml:space="preserve">
          <source>This is an example showing the prediction latency of various scikit-learn estimators.</source>
          <target state="translated">Este es un ejemplo que muestra la latencia de la predicción de varios estimadores de la ciencia.</target>
        </trans-unit>
        <trans-unit id="9e4f7a05490ee1267f03d9980bace7147baa0b76" translate="yes" xml:space="preserve">
          <source>This is an extension of the algorithm in scipy.stats.mode.</source>
          <target state="translated">Esta es una extensión del algoritmo en modo scipy.stats.</target>
        </trans-unit>
        <trans-unit id="375819c22c211b4c7fc97205acd724c3a575f620" translate="yes" xml:space="preserve">
          <source>This is an implementation that uses the result of the previous model to speed up computations along the set of solutions, making it faster than sequentially calling LogisticRegression for the different parameters. Note that there will be no speedup with liblinear solver, since it does not handle warm-starting.</source>
          <target state="translated">Se trata de una implementación que utiliza el resultado del modelo anterior para acelerar los cómputos a lo largo del conjunto de soluciones,haciéndolo más rápido que llamando secuencialmente a LogisticRegression para los diferentes parámetros.Tenga en cuenta que no habrá aceleración con el solucionador liblineal,ya que no maneja el arranque en caliente.</target>
        </trans-unit>
        <trans-unit id="89098058da4c55a1db96b87aadaa162a0a15baba" translate="yes" xml:space="preserve">
          <source>This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a &lt;code&gt;score&lt;/code&gt; function, or &lt;code&gt;scoring&lt;/code&gt; must be passed.</source>
          <target state="translated">Se supone que esto implementa la interfaz del estimador scikit-learn. Cualquiera de los estimadores debe proporcionar una funci&amp;oacute;n de &lt;code&gt;score&lt;/code&gt; o se debe aprobar la &lt;code&gt;scoring&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a9f1a5b0fa7d00ad69170d1ab81bf1031dee11a2" translate="yes" xml:space="preserve">
          <source>This is called a &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; cross-validation.</source>
          <target state="translated">Esto se denomina &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; cruzada de KFold .</target>
        </trans-unit>
        <trans-unit id="2e974743bc0fdffbf7238debaf0ee76bb5a5d9b2" translate="yes" xml:space="preserve">
          <source>This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.</source>
          <target state="translated">Esto se denomina similitud de coseno,porque la normalización euclidiana (L2)proyecta los vectores sobre la esfera unitaria,y su producto de puntos es entonces el coseno del ángulo entre los puntos denotados por los vectores.</target>
        </trans-unit>
        <trans-unit id="9b01365512b47448f649e450ddd111360a73cfc3" translate="yes" xml:space="preserve">
          <source>This is called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curse of dimensionality&lt;/a&gt; and is a core problem that machine learning addresses.</source>
          <target state="translated">Esto se denomina la &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;maldici&amp;oacute;n de la dimensionalidad&lt;/a&gt; y es un problema central que aborda el aprendizaje autom&amp;aacute;tico.</target>
        </trans-unit>
        <trans-unit id="25e5e11d6a0e13a60841f1cb72db59989d03472f" translate="yes" xml:space="preserve">
          <source>This is currently implemented in the following classes:</source>
          <target state="translated">Esto se aplica actualmente en las siguientes clases:</target>
        </trans-unit>
        <trans-unit id="a774a1be5070f83615f896d6d2ec16ebfbe92e4e" translate="yes" xml:space="preserve">
          <source>This is done in 2 steps:</source>
          <target state="translated">Esto se hace en dos pasos:</target>
        </trans-unit>
        <trans-unit id="0c564a0d4cfad247ff47792f5a12558130a84f0c" translate="yes" xml:space="preserve">
          <source>This is equivalent to fit followed by transform, but more efficiently implemented.</source>
          <target state="translated">Esto equivale a un ajuste seguido de una transformación,pero con una aplicación más eficiente.</target>
        </trans-unit>
        <trans-unit id="831022bba18e9ed70a7a762cd8243e7523afeddb" translate="yes" xml:space="preserve">
          <source>This is especially useful when the whole dataset is too big to fit in memory at once.</source>
          <target state="translated">Esto es especialmente útil cuando el conjunto de datos es demasiado grande para caber en la memoria a la vez.</target>
        </trans-unit>
        <trans-unit id="75c0bef753e28aecf63e47221c52fe362a027981" translate="yes" xml:space="preserve">
          <source>This is implemented as &lt;code&gt;argmax(decision_function(X), axis=1)&lt;/code&gt; which will return the label of the class with most votes by estimators predicting the outcome of a decision for each possible class pair.</source>
          <target state="translated">Esto se implementa como &lt;code&gt;argmax(decision_function(X), axis=1)&lt;/code&gt; que devolver&amp;aacute; la etiqueta de la clase con m&amp;aacute;s votos mediante estimadores que predicen el resultado de una decisi&amp;oacute;n para cada posible par de clases.</target>
        </trans-unit>
        <trans-unit id="9b2a6723fed7b2d139e18e341020f963dc7f8450" translate="yes" xml:space="preserve">
          <source>This is implemented by linking the points X into the graph of geodesic distances of the training data. First the &lt;code&gt;n_neighbors&lt;/code&gt; nearest neighbors of X are found in the training data, and from these the shortest geodesic distances from each point in X to each point in the training data are computed in order to construct the kernel. The embedding of X is the projection of this kernel onto the embedding vectors of the training set.</source>
          <target state="translated">Esto se implementa vinculando los puntos X en el gr&amp;aacute;fico de distancias geod&amp;eacute;sicas de los datos de entrenamiento. Primero, los &lt;code&gt;n_neighbors&lt;/code&gt; vecinos m&amp;aacute;s cercanos de X se encuentran en los datos de entrenamiento, y a partir de estos, se calculan las distancias geod&amp;eacute;sicas m&amp;aacute;s cortas desde cada punto en X a cada punto en los datos de entrenamiento para construir el n&amp;uacute;cleo. La incrustaci&amp;oacute;n de X es la proyecci&amp;oacute;n de este n&amp;uacute;cleo en los vectores de incrustaci&amp;oacute;n del conjunto de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="51ae8a82b3eff6fb295f70aa28171d41c73ac3db" translate="yes" xml:space="preserve">
          <source>This is implemented in &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt;&lt;/a&gt;. The desired dimensionality can be set using the &lt;code&gt;n_components&lt;/code&gt; constructor parameter. This parameter has no influence on &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.fit&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.predict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Esto se implementa en &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt; &lt;/a&gt; . La dimensionalidad deseada se puede establecer utilizando el par&amp;aacute;metro constructor &lt;code&gt;n_components&lt;/code&gt; . Este par&amp;aacute;metro no tiene influencia en &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.fit&lt;/code&gt; &lt;/a&gt; o &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.predict&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="de8220f3c95931fc4241bdd5334e66fca04da2f0" translate="yes" xml:space="preserve">
          <source>This is known as &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Esto se conoce como &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="1d1ef16c8ffe6df8c7a1a81b132f67cdda1b92ea" translate="yes" xml:space="preserve">
          <source>This is more efficient than calling fit followed by transform.</source>
          <target state="translated">Esto es más eficiente que el llamado ajuste seguido de la transformación.</target>
        </trans-unit>
        <trans-unit id="1dfb3afc660617ced3002fefa24847b5bb1a14dd" translate="yes" xml:space="preserve">
          <source>This is mostly equivalent to calling:</source>
          <target state="translated">Esto es mayormente equivalente a llamar:</target>
        </trans-unit>
        <trans-unit id="bbd51f304b678a157464ff7ab03c52123d04218d" translate="yes" xml:space="preserve">
          <source>This is not a symmetric function.</source>
          <target state="translated">Esta no es una función simétrica.</target>
        </trans-unit>
        <trans-unit id="0b5c42967b0e34c52656dd80a4e659c6f0fa2181" translate="yes" xml:space="preserve">
          <source>This is not exactly the same as &lt;code&gt;sklearn.metrics.additive_chi2_kernel&lt;/code&gt;. The authors of &lt;a href=&quot;#vz2010&quot; id=&quot;id4&quot;&gt;[VZ2010]&lt;/a&gt; prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components \(x_i\) separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling.</source>
          <target state="translated">Esto no es exactamente lo mismo que &lt;code&gt;sklearn.metrics.additive_chi2_kernel&lt;/code&gt; . Los autores de &lt;a href=&quot;#vz2010&quot; id=&quot;id4&quot;&gt;[VZ2010]&lt;/a&gt; prefieren la versi&amp;oacute;n anterior ya que siempre es positiva definida. Dado que el n&amp;uacute;cleo es aditivo, es posible tratar todos los componentes \ (x_i \) por separado para la incrustaci&amp;oacute;n. Esto hace posible muestrear la transformada de Fourier en intervalos regulares, en lugar de realizar una aproximaci&amp;oacute;n utilizando el muestreo de Monte Carlo.</target>
        </trans-unit>
        <trans-unit id="f7f802fcac19c1c8ed1c55f673312d761a2c94a7" translate="yes" xml:space="preserve">
          <source>This is not the case for &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt;: both are bound by the relationship:</source>
          <target state="translated">Este no es el caso de &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; &lt;/a&gt; : ambos est&amp;aacute;n vinculados por la relaci&amp;oacute;n:</target>
        </trans-unit>
        <trans-unit id="5d73f5b087f2ecb2dadb8778d3d18cfc19507034" translate="yes" xml:space="preserve">
          <source>This is not true for &lt;code&gt;mutual_info_score&lt;/code&gt;, which is therefore harder to judge:</source>
          <target state="translated">Esto no es cierto para &lt;code&gt;mutual_info_score&lt;/code&gt; , que por lo tanto es m&amp;aacute;s dif&amp;iacute;cil de juzgar:</target>
        </trans-unit>
        <trans-unit id="982101a3d677907e48e034c807cde26531a0468b" translate="yes" xml:space="preserve">
          <source>This is only available if no vocabulary was given.</source>
          <target state="translated">Esto sólo está disponible si no se ha dado ningún vocabulario.</target>
        </trans-unit>
        <trans-unit id="0ca1a333516e3b52907835d092958662636ea528" translate="yes" xml:space="preserve">
          <source>This is particularly important for doing grid searches:</source>
          <target state="translated">Esto es particularmente importante para hacer búsquedas de cuadrículas:</target>
        </trans-unit>
        <trans-unit id="2413be66af5e312ff97e484aff33c56868971fbe" translate="yes" xml:space="preserve">
          <source>This is perhaps the best known database to be found in the pattern recognition literature. Fisher&amp;rsquo;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp;amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.</source>
          <target state="translated">Esta es quiz&amp;aacute;s la base de datos m&amp;aacute;s conocida que se puede encontrar en la literatura sobre reconocimiento de patrones. El art&amp;iacute;culo de Fisher es un cl&amp;aacute;sico en el campo y se hace referencia con frecuencia hasta el d&amp;iacute;a de hoy. (Ver Duda &amp;amp; Hart, por ejemplo.) El conjunto de datos contiene 3 clases de 50 instancias cada una, donde cada clase se refiere a un tipo de planta de iris. Una clase es linealmente separable de las otras 2; estos &amp;uacute;ltimos NO son linealmente separables entre s&amp;iacute;.</target>
        </trans-unit>
        <trans-unit id="32ef6d8e689e0cbccf726fb7a94339c2864c487f" translate="yes" xml:space="preserve">
          <source>This is present only if &lt;code&gt;refit&lt;/code&gt; is not False.</source>
          <target state="translated">Esto est&amp;aacute; presente solo si el &lt;code&gt;refit&lt;/code&gt; no es Falso.</target>
        </trans-unit>
        <trans-unit id="717414c2af196799a3d1dc1aec1269a995318377" translate="yes" xml:space="preserve">
          <source>This is similar to the error set size, but weighted by the number of relevant and irrelevant labels. The best performance is achieved with a ranking loss of zero.</source>
          <target state="translated">Esto es similar al tamaño del conjunto de errores,pero ponderado por el número de etiquetas relevantes e irrelevantes.El mejor rendimiento se logra con una pérdida de clasificación de cero.</target>
        </trans-unit>
        <trans-unit id="00034266cafd87d919959c8134650d981f2c4466" translate="yes" xml:space="preserve">
          <source>This is the class and function reference of scikit-learn. Please refer to the &lt;a href=&quot;http://scikit-learn.org/stable/user_guide.html#user-guide&quot;&gt;full user guide&lt;/a&gt; for further details, as the class and function raw specifications may not be enough to give full guidelines on their uses. For reference on concepts repeated across the API, see &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt;.</source>
          <target state="translated">Esta es la referencia de clase y funci&amp;oacute;n de scikit-learn. Consulte la &lt;a href=&quot;http://scikit-learn.org/stable/user_guide.html#user-guide&quot;&gt;gu&amp;iacute;a del usuario completa&lt;/a&gt; para obtener m&amp;aacute;s detalles, ya que las especificaciones en bruto de la clase y la funci&amp;oacute;n pueden no ser suficientes para brindar pautas completas sobre sus usos. Para obtener referencia sobre conceptos repetidos en la API, consulte &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glosario de t&amp;eacute;rminos comunes y elementos de API&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="9c0b38fb17b983574b86706f2172c4c4bac1c6a5" translate="yes" xml:space="preserve">
          <source>This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier&amp;rsquo;s predictions. The log loss is only defined for two or more labels. For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is</source>
          <target state="translated">Esta es la funci&amp;oacute;n de p&amp;eacute;rdida utilizada en la regresi&amp;oacute;n log&amp;iacute;stica (multinomial) y sus extensiones, como las redes neuronales, definida como la probabilidad logar&amp;iacute;tmica negativa de las etiquetas verdaderas dadas las predicciones de un clasificador probabil&amp;iacute;stico. La p&amp;eacute;rdida de registros solo se define para dos o m&amp;aacute;s etiquetas. Para una sola muestra con etiqueta verdadera yt en {0,1} y probabilidad estimada yp de que yt = 1, la p&amp;eacute;rdida logar&amp;iacute;tmica es</target>
        </trans-unit>
        <trans-unit id="a9111de5c7f4d9db0e2dc4faf926c5c47ae0eb12" translate="yes" xml:space="preserve">
          <source>This is the result of calling &lt;code&gt;method&lt;/code&gt;</source>
          <target state="translated">Este es el resultado de llamar al &lt;code&gt;method&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="611492c50f944d397ce592f12eabee4801e28de1" translate="yes" xml:space="preserve">
          <source>This is the structured version, that takes into account some topological structure between samples.</source>
          <target state="translated">Esta es la versión estructurada,que tiene en cuenta cierta estructura topológica entre las muestras.</target>
        </trans-unit>
        <trans-unit id="eb0a6c68cdbb70ef7265210b8b8760243faa6912" translate="yes" xml:space="preserve">
          <source>This is useful for fitting an intercept term with implementations which cannot otherwise fit it directly.</source>
          <target state="translated">Esto es útil para ajustar un término de intercepción con implementaciones que de otra manera no pueden ajustarlo directamente.</target>
        </trans-unit>
        <trans-unit id="1c420e62ce6697ac415dbca5798c0153080b025c" translate="yes" xml:space="preserve">
          <source>This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">Esto es &amp;uacute;til si los atributos almacenados de un modelo usado previamente deben reutilizarse. Si se establece en False, los coeficientes se reescribir&amp;aacute;n para que cada llamada se ajuste. Consulte &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;el glosario&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="42f2c0052d88e51f5df6c26752c192f81040ec01" translate="yes" xml:space="preserve">
          <source>This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; accepts &lt;code&gt;scipy.sparse&lt;/code&gt; matrices. (Note that the tf-idf functionality in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; can produce normalized vectors, in which case &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; is equivalent to &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt;, only slower.)</source>
          <target state="translated">Este kernel es una opci&amp;oacute;n popular para calcular la similitud de documentos representados como vectores tf-idf. &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt; &lt;code&gt;cosine_similarity&lt;/code&gt; &lt;/a&gt; acepta matrices &lt;code&gt;scipy.sparse&lt;/code&gt; . (Tenga en cuenta que la funcionalidad tf-idf en &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; puede producir vectores normalizados, en cuyo caso &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt; &lt;code&gt;cosine_similarity&lt;/code&gt; &lt;/a&gt; es equivalente a &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt; , solo que m&amp;aacute;s lento).</target>
        </trans-unit>
        <trans-unit id="f1fbcea40cf4aba903be6eddf36c859a1718e3b8" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth.</source>
          <target state="translated">Este núcleo es infinitamente diferenciable,lo que implica que los GP con este núcleo como función de covarianza tienen derivados cuadrados medios de todos los órdenes,y por lo tanto son muy suaves.</target>
        </trans-unit>
        <trans-unit id="6ad60f9f3ef06c9a3898414b0afc593eaff20c1d" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:</source>
          <target state="translated">Este núcleo es infinitamente diferenciable,lo que implica que los GP con este núcleo como función de covarianza tienen derivados cuadrados medios de todos los órdenes,y por lo tanto son muy suaves.El anterior y posterior de un GP resultante de un núcleo RBF se muestran en la siguiente figura:</target>
        </trans-unit>
        <trans-unit id="6cbded70a18b870dfff7fda8d59e204ebd77900f" translate="yes" xml:space="preserve">
          <source>This kind of singular profiles is often seen in practice, for instance:</source>
          <target state="translated">Este tipo de perfiles singulares se ve a menudo en la práctica,por ejemplo:</target>
        </trans-unit>
        <trans-unit id="1df4414d1e47e9ea41e98b8c6e13b5eeb49e06ac" translate="yes" xml:space="preserve">
          <source>This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes &amp;ldquo;for free&amp;rdquo; as no additional data is needed and can be used for model selection.</source>
          <target state="translated">Esta parte omitida se puede utilizar para estimar el error de generalizaci&amp;oacute;n sin tener que depender de un conjunto de validaci&amp;oacute;n independiente. Esta estimaci&amp;oacute;n viene &quot;gratis&quot; ya que no se necesitan datos adicionales y se puede utilizar para la selecci&amp;oacute;n del modelo.</target>
        </trans-unit>
        <trans-unit id="4f52c7809ab985057ba93af9b88459b0d10b33a2" translate="yes" xml:space="preserve">
          <source>This makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect.</source>
          <target state="translated">Esto asegura que la función de pérdida no esté fuertemente influenciada por los valores atípicos,sin ignorar completamente su efecto.</target>
        </trans-unit>
        <trans-unit id="4826ff4b754b03a246d73c38aa2c971ffdf335e1" translate="yes" xml:space="preserve">
          <source>This means each weight \(w_{i}\) is drawn from a Gaussian distribution, centered on zero and with a precision \(\lambda_{i}\):</source>
          <target state="translated">Esto significa que cada peso se extrae de una distribución gaussiana,centrada en el cero y con una precisión:</target>
        </trans-unit>
        <trans-unit id="de2308f740624c092eea038ac13deb5af2b1fa80" translate="yes" xml:space="preserve">
          <source>This means that any classifiers handling multi-output multiclass or multi-task classification tasks, support the multi-label classification task as a special case. Multi-task classification is similar to the multi-output classification task with different model formulations. For more information, see the relevant estimator documentation.</source>
          <target state="translated">Esto significa que cualquier clasificador que se ocupe de tareas de clasificación de salida múltiple o de tareas múltiples,apoyará la tarea de clasificación de etiquetas múltiples como un caso especial.La clasificación multitarea es similar a la tarea de clasificación multi-salida con diferentes formulaciones de modelos.Para obtener más información,véase la documentación pertinente del estimador.</target>
        </trans-unit>
        <trans-unit id="6157bc0b8c2f67c8a593bf2d12852c000be43e72" translate="yes" xml:space="preserve">
          <source>This measure is not adjusted for chance. Therefore &lt;a href=&quot;sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt;&lt;code&gt;adjusted_mutual_info_score&lt;/code&gt;&lt;/a&gt; might be preferred.</source>
          <target state="translated">Esta medida no se ajusta al azar. Por lo tanto, se podr&amp;iacute;a preferir la puntuaci&amp;oacute;n_informaci&amp;oacute;n_mutual &lt;a href=&quot;sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt; &lt;code&gt;adjusted_mutual_info_score&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="cca48ea1404a91d2df7b18cac656df30067cb12b" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each boosting iteration.</source>
          <target state="translated">Este método permite la supervisión (es decir,determinar el error en el conjunto de pruebas)después de cada iteración de impulso.</target>
        </trans-unit>
        <trans-unit id="778da8cdceb825f45e49260c13130972c415ba18" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each stage.</source>
          <target state="translated">Este método permite la supervisión (es decir,determinar el error en el conjunto de pruebas)después de cada etapa.</target>
        </trans-unit>
        <trans-unit id="893e20b8eaafff147aad0ee519c22b2be0783798" translate="yes" xml:space="preserve">
          <source>This method allows to generalize prediction to &lt;em&gt;new observations&lt;/em&gt; (not in the training set). Only available for novelty detection (when novelty is set to True).</source>
          <target state="translated">Este m&amp;eacute;todo permite generalizar la predicci&amp;oacute;n a &lt;em&gt;nuevas observaciones&lt;/em&gt; (no en el conjunto de entrenamiento). Solo disponible para la detecci&amp;oacute;n de novedades (cuando la novedad se establece en Verdadero).</target>
        </trans-unit>
        <trans-unit id="e13bbab31202d773dbd5b155d7e0b7799ca54f84" translate="yes" xml:space="preserve">
          <source>This method computes the least squares solution using a singular value decomposition of X. If X is a matrix of size (n, p) this method has a cost of \(O(n p^2)\), assuming that \(n \geq p\).</source>
          <target state="translated">Este método calcula la solución de mínimos cuadrados usando un valor de descomposición singular de X.Si X es una matriz de tamaño (n,p)este método tiene un costo de \(O(n p^2)\),asumiendo que \ ~ (n \ ~ geq p).</target>
        </trans-unit>
        <trans-unit id="dc0919b0c811a79ed295c00492df459fa5ab93e8" translate="yes" xml:space="preserve">
          <source>This method doesn&amp;rsquo;t do anything. It exists purely for compatibility with the scikit-learn transformer API.</source>
          <target state="translated">Este m&amp;eacute;todo no hace nada. Existe &amp;uacute;nicamente por compatibilidad con la API del transformador de scikit-learn.</target>
        </trans-unit>
        <trans-unit id="89035c0aee87e0125ffcf7bf5f0dbe56fcfb74a9" translate="yes" xml:space="preserve">
          <source>This method has some performance and numerical stability overhead, hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.</source>
          <target state="translated">Este método tiene cierto rendimiento y estabilidad numérica en la sobrecarga,por lo que es mejor llamar a partial_fit en trozos de datos que sean lo más grandes posible (siempre y cuando se ajusten al presupuesto de memoria)para ocultar la sobrecarga.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
