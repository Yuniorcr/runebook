<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="aaa6add3e7cf1ec084105f029a9e3050a4d18bde" translate="yes" xml:space="preserve">
          <source>The loss function to use in the boosting process. &amp;lsquo;binary_crossentropy&amp;rsquo; (also known as logistic loss) is used for binary classification and generalizes to &amp;lsquo;categorical_crossentropy&amp;rsquo; for multiclass classification. &amp;lsquo;auto&amp;rsquo; will automatically choose either loss depending on the nature of the problem.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d058a2ad82a18d6b6a9ed5c1f1cacf9ddbfdb363" translate="yes" xml:space="preserve">
          <source>The loss function to use in the boosting process. Note that the &amp;ldquo;least squares&amp;rdquo; and &amp;ldquo;poisson&amp;rdquo; losses actually implement &amp;ldquo;half least squares loss&amp;rdquo; and &amp;ldquo;half poisson deviance&amp;rdquo; to simplify the computation of the gradient. Furthermore, &amp;ldquo;poisson&amp;rdquo; loss internally uses a log-link and requires &lt;code&gt;y &amp;gt;= 0&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="efb9498c0013e0aa10110f406847617e3f7359e7" translate="yes" xml:space="preserve">
          <source>The loss function to use when updating the weights after each boosting iteration.</source>
          <target state="translated">La función de pérdida que se utiliza cuando se actualizan los pesos después de cada iteración de refuerzo.</target>
        </trans-unit>
        <trans-unit id="76e41cb6fdfbaf660e2380953e681777b0e6378c" translate="yes" xml:space="preserve">
          <source>The loss function used is binomial deviance. Regularization via shrinkage (&lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt;) improves performance considerably. In combination with shrinkage, stochastic gradient boosting (&lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;) can produce more accurate models by reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in Random Forests (via the &lt;code&gt;max_features&lt;/code&gt; parameter).</source>
          <target state="translated">La funci&amp;oacute;n de p&amp;eacute;rdida utilizada es la desviaci&amp;oacute;n binomial. La regularizaci&amp;oacute;n a trav&amp;eacute;s de la contracci&amp;oacute;n (tasa de &lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt; ) mejora considerablemente el rendimiento. En combinaci&amp;oacute;n con la contracci&amp;oacute;n, el aumento del gradiente estoc&amp;aacute;stico ( &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; ) puede producir modelos m&amp;aacute;s precisos al reducir la varianza mediante el ensacado. El submuestreo sin contracci&amp;oacute;n generalmente no funciona bien. Otra estrategia para reducir la varianza es submuestrear las caracter&amp;iacute;sticas an&amp;aacute;logas a las divisiones aleatorias en Random Forests (a trav&amp;eacute;s del par&amp;aacute;metro &lt;code&gt;max_features&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="b776e738f1fc829ad5cd97ce27a8c60ba5e6ea09" translate="yes" xml:space="preserve">
          <source>The low rank part of the profile can be considered the structured signal part of the data while the tail can be considered the noisy part of the data that cannot be summarized by a low number of linear components (singular vectors).</source>
          <target state="translated">La parte de rango bajo del perfil puede considerarse la parte de la señal estructurada de los datos,mientras que la cola puede considerarse la parte ruidosa de los datos que no puede ser resumida por un número bajo de componentes lineales (vectores singulares).</target>
        </trans-unit>
        <trans-unit id="798401f91692e498e70c4cd9edead7945caa2484" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;alpha&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;alpha&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d36bb18704838afeb9c91e3cf29995f34fe370d6" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;gamma&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;gamma&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05c76bb31dceb610d87492d4b267252a46764fae" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;length_scale&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;length_scale&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="971e4cf6ca3d178a182a1d45d26bbec9249f02e4" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;noise_level&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;noise_level&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f18d5554a9b373b29b2cfd43681990a3c136384e" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;periodicity&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;periodicity&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c4988467c8a93656299039dfe058cd67baf3941" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;sigma_0&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;sigma_0&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb2363bfa3c4e13746c6b4233da08b1f95f83117" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &lt;code&gt;constant_value&lt;/code&gt;. If set to &amp;ldquo;fixed&amp;rdquo;, &lt;code&gt;constant_value&lt;/code&gt; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eab337b7facddd1f9b82eece11282a55e0039c48" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on alpha</source>
          <target state="translated">El límite inferior y superior del alfa</target>
        </trans-unit>
        <trans-unit id="59acfd562a002dfced1122413eab7f832a55dd1c" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on constant_value</source>
          <target state="translated">El límite inferior y superior en valor_constante</target>
        </trans-unit>
        <trans-unit id="3a719a94ceaa10893529e6c7cb2929f1cad3fed1" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on gamma</source>
          <target state="translated">El límite inferior y superior de los rayos gamma</target>
        </trans-unit>
        <trans-unit id="1ed15fdef2e92d07e56ad7fd40f1816e9ce5774a" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on l</source>
          <target state="translated">El límite inferior y superior de l</target>
        </trans-unit>
        <trans-unit id="5a4d3b39beb7cd8132b2abe44c4adfccffd03b85" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on length_scale</source>
          <target state="translated">El límite inferior y superior de la escala_de_longitud</target>
        </trans-unit>
        <trans-unit id="23389af6ff1a664526029d031ba30d8bce5de030" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on noise_level</source>
          <target state="translated">El límite inferior y superior del nivel de ruido</target>
        </trans-unit>
        <trans-unit id="5d396b2f8516f35fa726d028e889486242bc7cef" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on periodicity</source>
          <target state="translated">El límite inferior y superior de la periodicidad</target>
        </trans-unit>
        <trans-unit id="ca7705b5107aa6db64b47bb244d2b0d4033a135a" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on the parameter. If n_elements&amp;gt;1, a pair of 1d array with n_elements each may be given alternatively. If the string &amp;ldquo;fixed&amp;rdquo; is passed as bounds, the hyperparameter&amp;rsquo;s value cannot be changed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67f0743ba9ff76a5a36032511baa5e62618c04ec" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &amp;lt;= n &amp;lt;= max_n will be used.</source>
          <target state="translated">L&amp;iacute;mite inferior y superior del rango de valores n para los diferentes n-gramos que se van a extraer. Se utilizar&amp;aacute;n todos los valores de n tales que min_n &amp;lt;= n &amp;lt;= max_n.</target>
        </trans-unit>
        <trans-unit id="1a06e2593892f38e2c7f98d2aaf16f88ee8f121a" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &amp;lt;= n &amp;lt;= max_n will be used. For example an &lt;code&gt;ngram_range&lt;/code&gt; of &lt;code&gt;(1, 1)&lt;/code&gt; means only unigrams, &lt;code&gt;(1, 2)&lt;/code&gt; means unigrams and bigrams, and &lt;code&gt;(2, 2)&lt;/code&gt; means only bigrams. Only applies if &lt;code&gt;analyzer is not callable&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfc6e4d4438c9aec5ef09350cb9a99015ebb32f0" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n &amp;lt;= n &amp;lt;= max_n will be used. For example an &lt;code&gt;ngram_range&lt;/code&gt; of &lt;code&gt;(1, 1)&lt;/code&gt; means only unigrams, &lt;code&gt;(1, 2)&lt;/code&gt; means unigrams and bigrams, and &lt;code&gt;(2, 2)&lt;/code&gt; means only bigrams. Only applies if &lt;code&gt;analyzer is not callable&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cf3abbfa7f1a3041db626cf750d4ae3ee4a5f71" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used create the extreme values for the &lt;code&gt;grid&lt;/code&gt;. Only if &lt;code&gt;X&lt;/code&gt; is not None.</source>
          <target state="translated">Los percentiles inferior y superior utilizados crean los valores extremos de la &lt;code&gt;grid&lt;/code&gt; . Solo si &lt;code&gt;X&lt;/code&gt; no es Ninguno.</target>
        </trans-unit>
        <trans-unit id="ff88012ee3d2491153687a1e07b6eefe04629c89" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the PDP axes.</source>
          <target state="translated">El percentil inferior y superior utilizado para crear los valores extremos de los ejes PDP.</target>
        </trans-unit>
        <trans-unit id="8e5bebe8622375d11e17f863f40a564811e9afd3" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the PDP axes. Must be in [0, 1].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9186aa58b7ce14e687d5b2e114470d0ce75e6d07" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca509231087e3737e2d90c6b71a37c419327789b" translate="yes" xml:space="preserve">
          <source>The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around &lt;code&gt;0.01&lt;/code&gt;.</source>
          <target state="translated">La figura inferior izquierda traza la descomposici&amp;oacute;n puntual del error cuadr&amp;aacute;tico medio esperado de un solo &amp;aacute;rbol de decisi&amp;oacute;n. Confirma que el t&amp;eacute;rmino de sesgo (en azul) es bajo mientras que la varianza es grande (en verde). Tambi&amp;eacute;n ilustra la parte de ruido del error que, como se esperaba, parece ser constante y alrededor de &lt;code&gt;0.01&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="caf1b50ddc9e3b679e122c659dff7510d4ed2966" translate="yes" xml:space="preserve">
          <source>The lower the better.</source>
          <target state="translated">Cuanto más bajo,mejor.</target>
        </trans-unit>
        <trans-unit id="71c561a637b01dde1b3c6166ce3bd23db956f72f" translate="yes" xml:space="preserve">
          <source>The machine-learning pipeline</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfd900c27bac872165454194e9ba0284baa1f3aa" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.</source>
          <target state="translated">La regularización de la precisión de la máquina en el cálculo de los factores diagonales de Cholesky.Aumenta esto para sistemas muy mal acondicionados.</target>
        </trans-unit>
        <trans-unit id="d9100545597c68bda47e361ae3fa5acd8ebd846b" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. By default, &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48832baaa50854fed8a3f45f6240f92c4370d802" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Default is &lt;code&gt;np.finfo(np.float64).eps&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69abd1713b39f884bea4ffeed0d502e131117e74" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &amp;lsquo;tol&amp;rsquo; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">La regularizaci&amp;oacute;n de precisi&amp;oacute;n de la m&amp;aacute;quina en el c&amp;aacute;lculo de los factores diagonales de Cholesky. Aumente esto para sistemas muy mal acondicionados. A diferencia del par&amp;aacute;metro 'tol' en algunos algoritmos basados ​​en optimizaci&amp;oacute;n iterativa, este par&amp;aacute;metro no controla la tolerancia de la optimizaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="03f1e7333f5d863384674ba69d2200c51c214771" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">La regularizaci&amp;oacute;n de precisi&amp;oacute;n de la m&amp;aacute;quina en el c&amp;aacute;lculo de los factores diagonales de Cholesky. Aumente esto para sistemas muy mal acondicionados. A diferencia del par&amp;aacute;metro &lt;code&gt;tol&lt;/code&gt; en algunos algoritmos iterativos basados ​​en optimizaci&amp;oacute;n, este par&amp;aacute;metro no controla la tolerancia de la optimizaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="638a11bb66c553576b68621fdefa92568dded5ac" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; is used</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a355ae348914d14284e933a2a39b838c82d13102" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="686cc4307ca584641912f9ca8288a53bb10ce6de" translate="yes" xml:space="preserve">
          <source>The main advantage for Factor Analysis over &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is that it can model the variance in every direction of the input space independently (heteroscedastic noise):</source>
          <target state="translated">La principal ventaja del an&amp;aacute;lisis factorial sobre el &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; es que puede modelar la varianza en todas las direcciones del espacio de entrada de forma independiente (ruido heteroced&amp;aacute;stico):</target>
        </trans-unit>
        <trans-unit id="c6108525766abe52c974030991b5e8a7ddcb6e28" translate="yes" xml:space="preserve">
          <source>The main difficulty in learning Gaussian mixture models from unlabeled data is that it is one usually doesn&amp;rsquo;t know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;Expectation-maximization&lt;/a&gt; is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum.</source>
          <target state="translated">La principal dificultad para aprender modelos de mezcla gaussiana a partir de datos no etiquetados es que normalmente uno no sabe qu&amp;eacute; puntos provienen de qu&amp;eacute; componente latente (si uno tiene acceso a esta informaci&amp;oacute;n, es muy f&amp;aacute;cil ajustar una distribuci&amp;oacute;n gaussiana separada a cada conjunto de datos). puntos). &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;La maximizaci&amp;oacute;n de expectativas&lt;/a&gt; es un algoritmo estad&amp;iacute;stico bien fundado para solucionar este problema mediante un proceso iterativo. Primero, uno asume componentes aleatorios (centrados aleatoriamente en puntos de datos, aprendidos de k-medias, o incluso distribuidos normalmente alrededor del origen) y calcula para cada punto una probabilidad de ser generado por cada componente del modelo. Luego, uno ajusta los par&amp;aacute;metros para maximizar la probabilidad de los datos dadas esas asignaciones. La repetici&amp;oacute;n de este proceso garantiza la convergencia siempre hacia un &amp;oacute;ptimo local.</target>
        </trans-unit>
        <trans-unit id="d2139f3f89c20ed8a9cf480b63f0750e12fef92b" translate="yes" xml:space="preserve">
          <source>The main documentation. This contains an in-depth description of all algorithms and how to apply them.</source>
          <target state="translated">La documentación principal.Contiene una descripción detallada de todos los algoritmos y cómo aplicarlos.</target>
        </trans-unit>
        <trans-unit id="5f94da6f46e5f5e5d800fbb67f1c2ae40caf9c89" translate="yes" xml:space="preserve">
          <source>The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\) is the number of samples and \(T\) is the number of iterations until convergence. Further, the memory complexity is of the order \(O(N^2)\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets.</source>
          <target state="translated">El principal inconveniente de la Propagación por Afinidad es su complejidad.El algoritmo tiene una complejidad temporal del orden \ ~ (O (N ^ 2 T)),donde \ ~ N es el número de muestras y \ ~ es el número de iteraciones hasta la convergencia.Además,la complejidad de la memoria es del orden \ ~ (O (N^2)\ ~ si se utiliza una matriz de similitud densa,pero reducible si se utiliza una matriz de similitud escasa.Esto hace que la propagación de afinidad sea más apropiada para conjuntos de datos de tamaño pequeño a mediano.</target>
        </trans-unit>
        <trans-unit id="f23633268affa3b7ac5da4b687edb4096985bac5" translate="yes" xml:space="preserve">
          <source>The main factors that influence the prediction latency are</source>
          <target state="translated">Los principales factores que influyen en la latencia de la predicción son</target>
        </trans-unit>
        <trans-unit id="26167f635fda3c2035f2a73e215b9329a59f7a43" translate="yes" xml:space="preserve">
          <source>The main observations to make are:</source>
          <target state="translated">Las principales observaciones que hay que hacer son:</target>
        </trans-unit>
        <trans-unit id="36a91e6ee1a9ee310e3b9c8a52ba666f014296cb" translate="yes" xml:space="preserve">
          <source>The main parameters to adjust when using these methods is &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt;. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are &lt;code&gt;max_features=None&lt;/code&gt; (always considering all features instead of a random subset) for regression problems, and &lt;code&gt;max_features=&quot;sqrt&quot;&lt;/code&gt; (using a random subset of size &lt;code&gt;sqrt(n_features)&lt;/code&gt;) for classification tasks (where &lt;code&gt;n_features&lt;/code&gt; is the number of features in the data). Good results are often achieved when setting &lt;code&gt;max_depth=None&lt;/code&gt; in combination with &lt;code&gt;min_samples_split=2&lt;/code&gt; (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (&lt;code&gt;bootstrap=True&lt;/code&gt;) while the default strategy for extra-trees is to use the whole dataset (&lt;code&gt;bootstrap=False&lt;/code&gt;). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting &lt;code&gt;oob_score=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5b8962f0fe90f98d375c2c689eb4ed0da37d3b3" translate="yes" xml:space="preserve">
          <source>The main parameters to adjust when using these methods is &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt;. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are &lt;code&gt;max_features=n_features&lt;/code&gt; for regression problems, and &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; for classification tasks (where &lt;code&gt;n_features&lt;/code&gt; is the number of features in the data). Good results are often achieved when setting &lt;code&gt;max_depth=None&lt;/code&gt; in combination with &lt;code&gt;min_samples_split=2&lt;/code&gt; (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (&lt;code&gt;bootstrap=True&lt;/code&gt;) while the default strategy for extra-trees is to use the whole dataset (&lt;code&gt;bootstrap=False&lt;/code&gt;). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting &lt;code&gt;oob_score=True&lt;/code&gt;.</source>
          <target state="translated">Los principales par&amp;aacute;metros a ajustar cuando se utilizan estos m&amp;eacute;todos son &lt;code&gt;n_estimators&lt;/code&gt; y &lt;code&gt;max_features&lt;/code&gt; . El primero es el n&amp;uacute;mero de &amp;aacute;rboles en el bosque. Cuanto m&amp;aacute;s grande, mejor, pero tambi&amp;eacute;n m&amp;aacute;s tiempo llevar&amp;aacute; calcularlo. Adem&amp;aacute;s, tenga en cuenta que los resultados dejar&amp;aacute;n de mejorar significativamente m&amp;aacute;s all&amp;aacute; de un n&amp;uacute;mero cr&amp;iacute;tico de &amp;aacute;rboles. Este &amp;uacute;ltimo es el tama&amp;ntilde;o de los subconjuntos aleatorios de caracter&amp;iacute;sticas que se deben considerar al dividir un nodo. Cuanto menor, mayor ser&amp;aacute; la reducci&amp;oacute;n de la varianza, pero tambi&amp;eacute;n mayor ser&amp;aacute; el aumento del sesgo. Los buenos valores emp&amp;iacute;ricos predeterminados son &lt;code&gt;max_features=n_features&lt;/code&gt; para problemas de regresi&amp;oacute;n y &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; para tareas de clasificaci&amp;oacute;n (donde &lt;code&gt;n_features&lt;/code&gt; es el n&amp;uacute;mero de caracter&amp;iacute;sticas en los datos). A menudo se obtienen buenos resultados cuando se configura &lt;code&gt;max_depth=None&lt;/code&gt; en combinaci&amp;oacute;n con &lt;code&gt;min_samples_split=2&lt;/code&gt; (es decir, cuando se desarrollan completamente los &amp;aacute;rboles). Sin embargo, tenga en cuenta que estos valores no suelen ser &amp;oacute;ptimos y pueden dar lugar a modelos que consuman mucha RAM. Los mejores valores de par&amp;aacute;metros siempre deben tener una validaci&amp;oacute;n cruzada. Adem&amp;aacute;s, tenga en cuenta que en los bosques aleatorios, las muestras de bootstrap se utilizan de forma predeterminada ( &lt;code&gt;bootstrap=True&lt;/code&gt; ) mientras que la estrategia predeterminada para &amp;aacute;rboles adicionales es usar el conjunto de datos completo ( &lt;code&gt;bootstrap=False&lt;/code&gt; ). Cuando se utiliza el muestreo bootstrap, la precisi&amp;oacute;n de la generalizaci&amp;oacute;n se puede estimar en las muestras excluidas o fuera de la bolsa. Esto se puede habilitar configurando &lt;code&gt;oob_score=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="842b51b29e297ee475304c463fec6849d760da97" translate="yes" xml:space="preserve">
          <source>The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions.</source>
          <target state="translated">El principal propósito del t-SNE es la visualización de datos de alta dimensión.Por lo tanto,funciona mejor cuando los datos se incrustan en dos o tres dimensiones.</target>
        </trans-unit>
        <trans-unit id="85533a7e662fe3db2975f7f26edf978cd22f568d" translate="yes" xml:space="preserve">
          <source>The main theoretical result behind the efficiency of random projection is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma (quoting Wikipedia)&lt;/a&gt;:</source>
          <target state="translated">El principal resultado te&amp;oacute;rico detr&amp;aacute;s de la eficiencia de la proyecci&amp;oacute;n aleatoria es el &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;lema de Johnson-Lindenstrauss (citando Wikipedia)&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="c6fec5e2fce31198cf0394ae7c6624fc74feba7a" translate="yes" xml:space="preserve">
          <source>The main usage of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; is to compute the GP&amp;rsquo;s covariance between datapoints. For this, the method &lt;code&gt;__call__&lt;/code&gt; of the kernel can be called. This method can either be used to compute the &amp;ldquo;auto-covariance&amp;rdquo; of all pairs of datapoints in a 2d array X, or the &amp;ldquo;cross-covariance&amp;rdquo; of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt;): &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</source>
          <target state="translated">El uso principal de un &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt; es calcular la covarianza de GP entre puntos de datos. Para ello, se puede llamar al m&amp;eacute;todo &lt;code&gt;__call__&lt;/code&gt; del kernel. Este m&amp;eacute;todo se puede utilizar para calcular la &quot;covarianza autom&amp;aacute;tica&quot; de todos los pares de puntos de datos en una matriz 2d X, o la &quot;covarianza cruzada&quot; de todas las combinaciones de puntos de datos de una matriz X 2d con puntos de datos en una matriz 2d Y. La siguiente identidad es v&amp;aacute;lida para todos los n&amp;uacute;cleos k (excepto para &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt; ): &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4418da68cc8e1590c7a6e820d42fa84b7f1fff49" translate="yes" xml:space="preserve">
          <source>The main use-case of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt; kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54123ac391766adafa62d20ad77d558f0edc6a11" translate="yes" xml:space="preserve">
          <source>The main use-case of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt; kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:e</source>
          <target state="translated">El caso de uso principal del kernel &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt; es como parte de un kernel de suma donde explica el componente de ruido de la se&amp;ntilde;al. Ajustar su par&amp;aacute;metro \ (ruido \ _level \) corresponde a estimar el nivel de ruido. Se define como: e</target>
        </trans-unit>
        <trans-unit id="9e4c246aa3bcb240193dfa5bc5ce95df373e50e0" translate="yes" xml:space="preserve">
          <source>The main use-case of this kernel is as part of a sum-kernel where it explains the noise of the signal as independently and identically normally-distributed. The parameter noise_level equals the variance of this noise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b785a852cf47604fe35c8c001dbd6b426026b75" translate="yes" xml:space="preserve">
          <source>The main use-case of this kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter corresponds to estimating the noise-level.</source>
          <target state="translated">El principal caso de uso de este núcleo es como parte de un núcleo de suma donde explica el componente de ruido de la señal.Afinar su parámetro corresponde a la estimación del nivel de ruido.</target>
        </trans-unit>
        <trans-unit id="b8d24bce3b0ca4f2f608d76c9973e497dd5a4966" translate="yes" xml:space="preserve">
          <source>The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of \(O(k n \bar p)\), where k is the number of iterations (epochs) and \(\bar p\) is the average number of non-zero attributes per sample.</source>
          <target state="translated">La principal ventaja del SGD es su eficiencia,que es básicamente lineal en el número de ejemplos de entrenamiento.Si X es una matriz de tamaño (n,p)la formación tiene un coste de \N(O(k n \bar p)\N),donde k es el número de iteraciones (épocas)y \N(\Nbar p)es el número medio de atributos no nulos por muestra.</target>
        </trans-unit>
        <trans-unit id="c142aa304b117907c4ec82e7c8f2e0e1debc825c" translate="yes" xml:space="preserve">
          <source>The manifold learning implementations available in scikit-learn are summarized below</source>
          <target state="translated">A continuación se resumen las múltiples aplicaciones de aprendizaje disponibles en scikit-learn</target>
        </trans-unit>
        <trans-unit id="97fe69cbbfc59d33068b7f1700f458b4fb09dc06" translate="yes" xml:space="preserve">
          <source>The mapping from the value \(F_M(x_i)\) to a class or a probability is loss-dependent. For the deviance (or log-loss), the probability that \(x_i\) belongs to the positive class is modeled as \(p(y_i = 1 | x_i) = \sigma(F_M(x_i))\) where \(\sigma\) is the sigmoid function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d450abcdfe5ff938dd90f9482f51850eeea5e6fe" translate="yes" xml:space="preserve">
          <source>The mapping relies on a Monte Carlo approximation to the kernel values. The &lt;code&gt;fit&lt;/code&gt; function performs the Monte Carlo sampling, whereas the &lt;code&gt;transform&lt;/code&gt; method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the &lt;code&gt;fit&lt;/code&gt; function.</source>
          <target state="translated">El mapeo se basa en una aproximaci&amp;oacute;n de Monte Carlo a los valores del kernel. La funci&amp;oacute;n de &lt;code&gt;fit&lt;/code&gt; realiza el muestreo de Monte Carlo, mientras que el m&amp;eacute;todo de &lt;code&gt;transform&lt;/code&gt; aci&amp;oacute;n realiza el mapeo de los datos. Debido a la aleatoriedad inherente del proceso, los resultados pueden variar entre diferentes llamadas a la funci&amp;oacute;n de &lt;code&gt;fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1292a72996c3834d41499b1c7abdc4ace6de6204" translate="yes" xml:space="preserve">
          <source>The mask of selected features.</source>
          <target state="translated">La máscara de los rasgos seleccionados.</target>
        </trans-unit>
        <trans-unit id="c45c8ea0546b1c79f5dc6ae49b8b9eba3b94ab9e" translate="yes" xml:space="preserve">
          <source>The mathematical formulation is the following:</source>
          <target state="translated">La formulación matemática es la siguiente:</target>
        </trans-unit>
        <trans-unit id="d7ffadc6e201c4cc579f6014a4ba6a553d3e6d97" translate="yes" xml:space="preserve">
          <source>The matrix</source>
          <target state="translated">La matriz</target>
        </trans-unit>
        <trans-unit id="2cec1c9e3ed6a74b7dbd8e5442d20aacdeb523d2" translate="yes" xml:space="preserve">
          <source>The matrix dimension.</source>
          <target state="translated">La dimensión de la matriz.</target>
        </trans-unit>
        <trans-unit id="e194b944fcebb1be4c3463284dee201bd8108680" translate="yes" xml:space="preserve">
          <source>The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as &lt;em&gt;covariance selection&lt;/em&gt;.</source>
          <target state="translated">La matriz inversa de la matriz de covarianza, a menudo llamada matriz de precisi&amp;oacute;n, es proporcional a la matriz de correlaci&amp;oacute;n parcial. Da la relaci&amp;oacute;n de independencia parcial. En otras palabras, si dos caracter&amp;iacute;sticas son independientes condicionalmente de las dem&amp;aacute;s, el coeficiente correspondiente en la matriz de precisi&amp;oacute;n ser&amp;aacute; cero. Por eso tiene sentido estimar una matriz de precisi&amp;oacute;n escasa: la estimaci&amp;oacute;n de la matriz de covarianza est&amp;aacute; mejor condicionada por el aprendizaje de relaciones de independencia a partir de los datos. Esto se conoce como &lt;em&gt;selecci&amp;oacute;n de covarianza&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="e578a80d83c508825e0e59f2ea21cfd3dab91a77" translate="yes" xml:space="preserve">
          <source>The matrix of features, where NP is the number of polynomial features generated from the combination of inputs.</source>
          <target state="translated">La matriz de características,donde NP es el número de características polinómicas generadas a partir de la combinación de entradas.</target>
        </trans-unit>
        <trans-unit id="64f7c7c3f44e0d1f1fa6c715782355e1f1d2054c" translate="yes" xml:space="preserve">
          <source>The matrix.</source>
          <target state="translated">La matriz.</target>
        </trans-unit>
        <trans-unit id="9e68c52b3ee0713ce10cfb911b2833fc7e9a1df1" translate="yes" xml:space="preserve">
          <source>The maximal number of iterations for the solver.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3c27fa93df86412f11493765849dd6aeb05082d" translate="yes" xml:space="preserve">
          <source>The maximum depth of each tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">La profundidad máxima de cada árbol.Si no hay ninguno,entonces los nodos se expanden hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de min_muestras_split muestras.</target>
        </trans-unit>
        <trans-unit id="826fe580f0140d0a1b7e9876f557d09be8bbd041" translate="yes" xml:space="preserve">
          <source>The maximum depth of each tree. The depth of a tree is the number of edges to go from the root to the deepest leaf. Depth isn&amp;rsquo;t constrained by default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52e0ef4a9206a897434b4d55c59a4824cb11d988" translate="yes" xml:space="preserve">
          <source>The maximum depth of the representation. If None, the tree is fully generated.</source>
          <target state="translated">La máxima profundidad de la representación.Si no hay ninguna,el árbol se genera completamente.</target>
        </trans-unit>
        <trans-unit id="f5406d9d68ce630037091594fec01e2950e41c42" translate="yes" xml:space="preserve">
          <source>The maximum depth of the tree.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0ae7481b9fb370fd4191031ea9f70e91832a43a" translate="yes" xml:space="preserve">
          <source>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">La máxima profundidad del árbol.Si no hay ninguna,entonces los nodos se expanden hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de min_muestras_split muestras.</target>
        </trans-unit>
        <trans-unit id="a55adc01e4f423a3103fea57c0a2cfa41d8471f9" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for one to be considered as in the neighborhood of the other. By default it assumes the same value as &lt;code&gt;max_eps&lt;/code&gt;. Used only when &lt;code&gt;cluster_method='dbscan'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2785d22ac4386c9273ddac709630eaca73ae74f0" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for one to be considered as in the neighborhood of the other. Default value of &lt;code&gt;np.inf&lt;/code&gt; will identify clusters across all scales; reducing &lt;code&gt;max_eps&lt;/code&gt; will result in shorter run times.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86a132d622f603d09919856c8916a271668d069a" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cec94a1183be1885b8ca965cd0a310a0d6ef03e" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for them to be considered as in the same neighborhood.</source>
          <target state="translated">La distancia máxima entre dos muestras para que se consideren como en el mismo vecindario.</target>
        </trans-unit>
        <trans-unit id="2d03a3cd1f8dea91e35b793f66d9300ac80fe58d" translate="yes" xml:space="preserve">
          <source>The maximum number of bins to use for non-missing values. Before training, each feature of the input array &lt;code&gt;X&lt;/code&gt; is binned into integer-valued bins, which allows for a much faster training stage. Features with a small number of unique values may use less than &lt;code&gt;max_bins&lt;/code&gt; bins. In addition to the &lt;code&gt;max_bins&lt;/code&gt; bins, one more bin is always reserved for missing values. Must be no larger than 255.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78f8fd023fb60eb04dc691f81a2e17712a336e66" translate="yes" xml:space="preserve">
          <source>The maximum number of columns in the grid plot. Only active when &lt;code&gt;ax&lt;/code&gt; is a single axes or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78e415aabcf40a1b66551486961973263113520d" translate="yes" xml:space="preserve">
          <source>The maximum number of columns in the grid plot. Only active when &lt;code&gt;ax&lt;/code&gt; is a single axis or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72469eac4bccf834885ed51dab58c805d8cdc971" translate="yes" xml:space="preserve">
          <source>The maximum number of concurrently running jobs, such as the number of Python worker processes when backend=&amp;rdquo;multiprocessing&amp;rdquo; or the size of the thread-pool when backend=&amp;rdquo;threading&amp;rdquo;. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. None is a marker for &amp;lsquo;unset&amp;rsquo; that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a parallel_backend context manager that sets another value for n_jobs.</source>
          <target state="translated">El n&amp;uacute;mero m&amp;aacute;ximo de trabajos que se ejecutan simult&amp;aacute;neamente, como el n&amp;uacute;mero de procesos de trabajo de Python cuando backend = &quot;multiprocesamiento&quot; o el tama&amp;ntilde;o del grupo de subprocesos cuando backend = &quot;subprocesos&quot;. Si es -1, se utilizan todas las CPU. Si se da 1, no se utiliza ning&amp;uacute;n c&amp;oacute;digo de computaci&amp;oacute;n paralelo, lo cual es &amp;uacute;til para depurar. Para n_jobs por debajo de -1, se utilizan (n_cpus + 1 + n_jobs). Por tanto, para n_jobs = -2, se utilizan todas las CPU menos una. Ninguno es un marcador de 'no establecido' que se interpretar&amp;aacute; como n_jobs = 1 (ejecuci&amp;oacute;n secuencial) a menos que la llamada se realice bajo un administrador de contexto paralelo_backend que establezca otro valor para n_jobs.</target>
        </trans-unit>
        <trans-unit id="d68dbcc29192b28b1c0e16950a4955da0229e9e9" translate="yes" xml:space="preserve">
          <source>The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.</source>
          <target state="translated">El número máximo de estimadores a los que se termina el impulso.En caso de un ajuste perfecto,el procedimiento de aprendizaje se detiene antes.</target>
        </trans-unit>
        <trans-unit id="0ace226674121a7119418c464f4452694b61318d" translate="yes" xml:space="preserve">
          <source>The maximum number of features selected scoring above &lt;code&gt;threshold&lt;/code&gt;. To disable &lt;code&gt;threshold&lt;/code&gt; and only select based on &lt;code&gt;max_features&lt;/code&gt;, set &lt;code&gt;threshold=-np.inf&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero m&amp;aacute;ximo de caracter&amp;iacute;sticas seleccionadas con una puntuaci&amp;oacute;n por encima del &lt;code&gt;threshold&lt;/code&gt; . Para deshabilitar el &lt;code&gt;threshold&lt;/code&gt; y solo seleccionar en funci&amp;oacute;n de &lt;code&gt;max_features&lt;/code&gt; , configure el &lt;code&gt;threshold=-np.inf&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="305b61ba0832c48af4c31e220047663d92a85155" translate="yes" xml:space="preserve">
          <source>The maximum number of features to select. To only select based on &lt;code&gt;max_features&lt;/code&gt;, set &lt;code&gt;threshold=-np.inf&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1360fbfbf92ec231e131ca0c0a387fb9e5b6a69f" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations</source>
          <target state="translated">El número máximo de iteraciones</target>
        </trans-unit>
        <trans-unit id="07cf3627f06a0c97fd8b5fd02bbb00e02fc4d154" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations in Newton&amp;rsquo;s method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results.</source>
          <target state="translated">El n&amp;uacute;mero m&amp;aacute;ximo de iteraciones en el m&amp;eacute;todo de Newton para aproximar el posterior durante la predicci&amp;oacute;n. Los valores m&amp;aacute;s peque&amp;ntilde;os reducir&amp;aacute;n el tiempo de c&amp;aacute;lculo a costa de peores resultados.</target>
        </trans-unit>
        <trans-unit id="5ecb214f49e2d4dbf3814aaa5686d1fc9fb18e01" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten&amp;rsquo;s FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results.</source>
          <target state="translated">El n&amp;uacute;mero m&amp;aacute;ximo de iteraciones suele ser lo suficientemente alto y no necesita ning&amp;uacute;n ajuste. La optimizaci&amp;oacute;n consta de dos fases: la fase de exageraci&amp;oacute;n inicial y la optimizaci&amp;oacute;n final. Durante la exageraci&amp;oacute;n inicial, las probabilidades conjuntas en el espacio original se incrementar&amp;aacute;n artificialmente mediante la multiplicaci&amp;oacute;n con un factor dado. Los factores m&amp;aacute;s grandes dan como resultado brechas m&amp;aacute;s grandes entre los grupos naturales en los datos. Si el factor es demasiado alto, la divergencia KL podr&amp;iacute;a aumentar durante esta fase. Por lo general, no es necesario ajustarlo. Un par&amp;aacute;metro cr&amp;iacute;tico es la tasa de aprendizaje. Si la pendiente es demasiado baja, el descenso se atascar&amp;aacute; en un m&amp;iacute;nimo local malo. Si es demasiado alto, la divergencia de KL aumentar&amp;aacute; durante la optimizaci&amp;oacute;n. Se pueden encontrar m&amp;aacute;s consejos en las preguntas frecuentes de Laurens van der Maaten (ver referencias). El &amp;uacute;ltimo par&amp;aacute;metro, el &amp;aacute;ngulo, es una compensaci&amp;oacute;n entre rendimiento y precisi&amp;oacute;n.Los &amp;aacute;ngulos m&amp;aacute;s grandes implican que podemos aproximar regiones m&amp;aacute;s grandes por un solo punto, lo que conduce a una mejor velocidad pero resultados menos precisos.</target>
        </trans-unit>
        <trans-unit id="ae7ecafb1fbb8659823714d61be48b0de277df78" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations of the boosting process, i.e. the maximum number of trees for binary classification. For multiclass classification, &lt;code&gt;n_classes&lt;/code&gt; trees per iteration are built.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d7badc19d2ae32c74e6847ba0481766700bc0d4" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations of the boosting process, i.e. the maximum number of trees.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a263d39eeb80bbc61890473f9e4f7de61b380fb" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations to be run.</source>
          <target state="translated">El número máximo de iteraciones a ejecutar.</target>
        </trans-unit>
        <trans-unit id="42a16ed4baf475d1973848504fece7b7ddcdacc6" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations.</source>
          <target state="translated">El número máximo de iteraciones.</target>
        </trans-unit>
        <trans-unit id="828c8485a088b2e8fca34b650a5b350287a8d14c" translate="yes" xml:space="preserve">
          <source>The maximum number of leaves for each tree. Must be strictly greater than 1. If None, there is no maximum limit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7623dada54053128bc10f5932064e15f34e6e533" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.PassiveAggressiveClassifier.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3d89c5f2bce98870adcd2b6143612a20116ca64" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.Perceptron.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcd0f20b2b6a22d8d1fd2812b58e7d3f318eed78" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.SGDClassifier.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="728ec87f7057b93a5bbf338c5362cf41c1855cde" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.SGDRegressor.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd38afe4aeee3f58ba57bcb27ec36aa15fe40a58" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;code&gt;partial_fit&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06bd569f5fc7509f2f4591004e3b0ffe2a685cf9" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;code&gt;partial_fit&lt;/code&gt;. Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</source>
          <target state="translated">El n&amp;uacute;mero m&amp;aacute;ximo de pasadas sobre los datos de entrenamiento (tambi&amp;eacute;n conocido como &amp;eacute;pocas). Solo afecta el comportamiento en el m&amp;eacute;todo de &lt;code&gt;fit&lt;/code&gt; , y no el ajuste &lt;code&gt;partial_fit&lt;/code&gt; . El valor predeterminado es 5. El valor predeterminado es 1000 desde 0,21, o si tol no es Ninguno.</target>
        </trans-unit>
        <trans-unit id="73890f2d8730349939758bb99469c8dd6b7e6151" translate="yes" xml:space="preserve">
          <source>The maximum number of patches per image to extract. If max_patches is a float in (0, 1), it is taken to mean a proportion of the total number of patches.</source>
          <target state="translated">El número máximo de parches por imagen a extraer.Si max_patches es un flotador en (0,1),se considera que es una proporción del número total de parches.</target>
        </trans-unit>
        <trans-unit id="52d2f42a039085572a1dcc0b5530d4df0b8b6859" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If &lt;code&gt;max_patches&lt;/code&gt; is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9914613b7b3d16fc7caab060022c123f66024339" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="translated">El número máximo de parches a extraer.Si max_patches es un flotador entre 0 y 1,se considera que es una proporción del número total de parches.</target>
        </trans-unit>
        <trans-unit id="353b6da890fd9a7a3db82e5fc166d25d61607aa3" translate="yes" xml:space="preserve">
          <source>The maximum number of points on the path used to compute the residuals in the cross-validation</source>
          <target state="translated">El número máximo de puntos en el camino utilizado para calcular los residuos en la validación cruzada</target>
        </trans-unit>
        <trans-unit id="36c1ffc6cd6501f0a66093a4a420f28ce4311341" translate="yes" xml:space="preserve">
          <source>The maximum valid value the parameter can take. If None (default) it is implied that the parameter does not have an upper bound.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c53181fb6c1656f1967e529a2fe72fd9bc29ff60" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the full dataset, which break down as soon as there are outliers in the data set</source>
          <target state="translated">La media y la covarianza empírica del conjunto de datos completo,que se descomponen tan pronto como hay valores atípicos en el conjunto de datos</target>
        </trans-unit>
        <trans-unit id="f962fad68fff332002b42ee3dcc1e06f41fcfe6c" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the observations that are known to be good ones. This can be considered as a &amp;ldquo;perfect&amp;rdquo; MCD estimation, so one can trust our implementation by comparing to this case.</source>
          <target state="translated">La media y la covarianza emp&amp;iacute;rica de las observaciones que se sabe que son buenas. Esto puede considerarse como una estimaci&amp;oacute;n MCD &amp;ldquo;perfecta&amp;rdquo;, por lo que uno puede confiar en nuestra implementaci&amp;oacute;n compar&amp;aacute;ndola con este caso.</target>
        </trans-unit>
        <trans-unit id="24b29efa5c5ad7917d67d95c19c0b6440c20e3d8" translate="yes" xml:space="preserve">
          <source>The mean claim amount or severity (&lt;code&gt;AvgClaimAmount&lt;/code&gt;) can be empirically shown to follow approximately a Gamma distribution. We fit a GLM model for the severity with the same features as the frequency model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f98043dc9f229ba1c70dcb8021abddb5d793d8af" translate="yes" xml:space="preserve">
          <source>The mean of each mixture component.</source>
          <target state="translated">La media de cada componente de la mezcla.</target>
        </trans-unit>
        <trans-unit id="05329e8678ffdabb505c75140f9a49322a5129f1" translate="yes" xml:space="preserve">
          <source>The mean of the multi-dimensional normal distribution. If None then use the origin (0, 0, &amp;hellip;).</source>
          <target state="translated">La media de la distribuci&amp;oacute;n normal multidimensional. Si es Ninguno, utilice el origen (0, 0,&amp;hellip;).</target>
        </trans-unit>
        <trans-unit id="bde1358a0967a15325e4e7cc6fcf95bc7ef0f700" translate="yes" xml:space="preserve">
          <source>The mean over features. Only set if &lt;code&gt;self.whiten&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3598a929d1a64528ce62560f1aa02f9fb9981b30" translate="yes" xml:space="preserve">
          <source>The mean predicted probability in each bin.</source>
          <target state="translated">La media de la probabilidad predicha en cada recipiente.</target>
        </trans-unit>
        <trans-unit id="8aaf5602eb2242bddc9912d47746326b31b0a1d5" translate="yes" xml:space="preserve">
          <source>The mean score and the 95% confidence interval of the score estimate are hence given by:</source>
          <target state="translated">La puntuación media y el intervalo de confianza del 95% de la estimación de la puntuación vienen dados por:</target>
        </trans-unit>
        <trans-unit id="fb645170ce6174deae19b9d9f752a59168ce3c91" translate="yes" xml:space="preserve">
          <source>The mean squared error (&lt;code&gt;power=0&lt;/code&gt;) is very sensitive to the prediction difference of the second point,:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94d27e1df24bf9a05c82ae625a1197b415bc3fdc" translate="yes" xml:space="preserve">
          <source>The mean value for each feature in the training set. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_mean=False&lt;/code&gt;.</source>
          <target state="translated">El valor medio de cada caracter&amp;iacute;stica del conjunto de entrenamiento. Igual a &lt;code&gt;None&lt;/code&gt; cuando &lt;code&gt;with_mean=False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b5e7dcb7d882c8689297cb339a998a6e74140e5f" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.</source>
          <target state="translated">La media, el error est&amp;aacute;ndar y el &quot;peor&quot; o el m&amp;aacute;s grande (la media de los tres valores m&amp;aacute;s grandes) de estas caracter&amp;iacute;sticas se calcularon para cada imagen, lo que result&amp;oacute; en 30 caracter&amp;iacute;sticas. Por ejemplo, el campo 3 es Radio medio, el campo 13 es Radio SE, el campo 23 es Peor radio.</target>
        </trans-unit>
        <trans-unit id="8d0066f714eed86dea1c0b26b34c2da2cae60854" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9952f9f2a02ad0002415a0bd8b6c9bae196d7ee8" translate="yes" xml:space="preserve">
          <source>The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.</source>
          <target state="translated">La medida de la normalidad de una observación dada en un árbol es la profundidad de la hoja que contiene esta observación,que equivale al número de divisiones necesarias para aislar este punto.En el caso de varias observaciones n_left en la hoja,se añade la longitud media del trayecto de un árbol de aislamiento de n_left muestras.</target>
        </trans-unit>
        <trans-unit id="3ff66cd9c701474c3d9f795cee9d82f5e1659bc8" translate="yes" xml:space="preserve">
          <source>The median absolute deviation to non corrupt new data is used to judge the quality of the prediction.</source>
          <target state="translated">La media de la desviación absoluta de los nuevos datos no corruptos se utiliza para juzgar la calidad de la predicción.</target>
        </trans-unit>
        <trans-unit id="970571bcac487854f4caea3e516e12da8ac34c22" translate="yes" xml:space="preserve">
          <source>The median value for each feature in the training set.</source>
          <target state="translated">El valor medio de cada característica del conjunto de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="c8bf90a15bbbe280812d1ade2a9f9077adb9d41d" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments.</source>
          <target state="translated">El modo de memmapping utilizado cuando se carga desde la caché de matrices numéricas.Ver numpy.load para el significado de los argumentos.</target>
        </trans-unit>
        <trans-unit id="4971ea4c8c64c5bc8a4bdd1e4563c9705f8166bf" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments. By default that of the memory object is used.</source>
          <target state="translated">El modo de memmapping utilizado cuando se carga desde la caché de matrices numéricas.Ver numpy.load para el significado de los argumentos.Por defecto se utiliza el del objeto de memoria.</target>
        </trans-unit>
        <trans-unit id="85874f620128fa58a2e359ad1c9c828fcfd537bb" translate="yes" xml:space="preserve">
          <source>The memory footprint of randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is also proportional to \(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\) instead of \(n_{\max} \cdot n_{\min}\) for the exact method.</source>
          <target state="translated">La huella de memoria del &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; aleatorizado tambi&amp;eacute;n es proporcional a \ (2 \ cdot n _ {\ max} \ cdot n _ {\ mathrm {components}} \) en lugar de \ (n _ {\ max} \ cdot n _ {\ min} \) para el m&amp;eacute;todo exacto.</target>
        </trans-unit>
        <trans-unit id="ad92e195504975d239e8efd05966b747d9d3adc0" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier, and discretize the [0, 1] interval into bins.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fa2b59a5a9d6863d86f91b2a847ac2c39bc204e" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier.</source>
          <target state="translated">El método asume que las entradas provienen de un clasificador binario.</target>
        </trans-unit>
        <trans-unit id="ff8ec8205e374ddf520735804ecbfa39550f37bd" translate="yes" xml:space="preserve">
          <source>The method fits the model &lt;code&gt;n_init&lt;/code&gt; times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. If &lt;code&gt;warm_start&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then &lt;code&gt;n_init&lt;/code&gt; is ignored and a single initialization is performed upon the first call. Upon consecutive calls, training starts where it left off.</source>
          <target state="translated">El m&amp;eacute;todo se ajusta al modelo &lt;code&gt;n_init&lt;/code&gt; veces y establece los par&amp;aacute;metros con los que el modelo tiene la mayor probabilidad o l&amp;iacute;mite inferior. Dentro de cada ensayo, el m&amp;eacute;todo itera entre el paso E y el paso M para tiempos de &lt;code&gt;max_iter&lt;/code&gt; hasta que el cambio de probabilidad o l&amp;iacute;mite inferior sea menor que &lt;code&gt;tol&lt;/code&gt; ; de lo contrario, se genera una advertencia de &lt;code&gt;ConvergenceWarning&lt;/code&gt; . Si &lt;code&gt;warm_start&lt;/code&gt; es &lt;code&gt;True&lt;/code&gt; , entonces &lt;code&gt;n_init&lt;/code&gt; se ignora y se realiza una &amp;uacute;nica inicializaci&amp;oacute;n en la primera llamada. Tras llamadas consecutivas, el entrenamiento comienza donde lo dej&amp;oacute;.</target>
        </trans-unit>
        <trans-unit id="64dcb6bda2581c1c56dbef7b8447b61db8e804dd" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;a href=&quot;sklearn.exceptions.convergencewarning#sklearn.exceptions.ConvergenceWarning&quot;&gt;&lt;code&gt;ConvergenceWarning&lt;/code&gt;&lt;/a&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aeb4fb178bdd7f679ad7ab31606d57c02d18da8" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="translated">El m&amp;eacute;todo se ajusta al modelo n_init veces y establece los par&amp;aacute;metros con los que el modelo tiene la mayor probabilidad o l&amp;iacute;mite inferior. Dentro de cada prueba, el m&amp;eacute;todo itera entre el paso E y el paso M para tiempos de &lt;code&gt;max_iter&lt;/code&gt; hasta que el cambio de probabilidad o l&amp;iacute;mite inferior sea menor que &lt;code&gt;tol&lt;/code&gt; ; de lo contrario, se genera una advertencia de &lt;code&gt;ConvergenceWarning&lt;/code&gt; . Despu&amp;eacute;s de ajustar, predice la etiqueta m&amp;aacute;s probable para los puntos de datos de entrada.</target>
        </trans-unit>
        <trans-unit id="02e13e01ea0f52a6e082ed53d9636e409ba7f22e" translate="yes" xml:space="preserve">
          <source>The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.</source>
          <target state="translated">El método ganó popularidad para inicializar las redes neuronales profundas con los pesos de los RBM independientes.Este método se conoce como pre-entrenamiento no supervisado.</target>
        </trans-unit>
        <trans-unit id="867b88e44ce337abe62bbd2070ac4235fc6ec53b" translate="yes" xml:space="preserve">
          <source>The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.</source>
          <target state="translated">El método de Clasificación de Vectores de Apoyo puede extenderse para resolver problemas de regresión.Este método se llama Regresión del Vector de Apoyo.</target>
        </trans-unit>
        <trans-unit id="0ea5095ab7a2f7c4087eef18fc7667a5380ac2de" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method (i.e. a logistic regression model) or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b9b2478ee76aa8f8a62d64db00bfa346a8108cd" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit. Use sigmoids (Platt&amp;rsquo;s calibration) in this case.</source>
          <target state="translated">El m&amp;eacute;todo a utilizar para la calibraci&amp;oacute;n. Puede ser &quot;sigmoide&quot;, que corresponde al m&amp;eacute;todo de Platt, o &quot;isot&amp;oacute;nico&quot;, que es un enfoque no param&amp;eacute;trico. No se recomienda utilizar la calibraci&amp;oacute;n isot&amp;oacute;nica con muy pocas muestras de calibraci&amp;oacute;n &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; ya que tiende a sobreajustarse. Utilice sigmoides (calibraci&amp;oacute;n de Platt) en este caso.</target>
        </trans-unit>
        <trans-unit id="1d0e566bfd264bfe0a9882ca94efb92a9385b867" translate="yes" xml:space="preserve">
          <source>The method used by each base estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87ca64fde3cf11f4163bb009ef5ad4abbcdc8e98" translate="yes" xml:space="preserve">
          <source>The method used to calculate the averaged predictions:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21e55011234a7eafaab581faf9cf56cabba5a5c2" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the covariances. Must be one of:</source>
          <target state="translated">El método utilizado para inicializar los pesos,los medios y las covarianzas.Debe ser uno de:</target>
        </trans-unit>
        <trans-unit id="963cb60032e3efabe5bef9b8ad76de7b8282f830" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the precisions. Must be one of:</source>
          <target state="translated">El método utilizado para inicializar los pesos,los medios y las precisiones.Debe ser uno de:</target>
        </trans-unit>
        <trans-unit id="8015d2d76c1dfbbe3de68f3d3f8aa78fbf89dd67" translate="yes" xml:space="preserve">
          <source>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">El m&amp;eacute;todo funciona tanto en estimadores simples como en objetos anidados (como tuber&amp;iacute;as). Estos &amp;uacute;ltimos tienen par&amp;aacute;metros de la forma &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; para que sea posible actualizar cada componente de un objeto anidado.</target>
        </trans-unit>
        <trans-unit id="5d1673cba254e117532409bf5f2a151ed75e6f39" translate="yes" xml:space="preserve">
          <source>The method works on simple kernels as well as on nested kernels. The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">El m&amp;eacute;todo funciona tanto en kernels simples como en kernels anidados. Estos &amp;uacute;ltimos tienen par&amp;aacute;metros de la forma &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; para que sea posible actualizar cada componente de un objeto anidado.</target>
        </trans-unit>
        <trans-unit id="653c073b67dfbefaf963d2a9e7b682aaf13d10c5" translate="yes" xml:space="preserve">
          <source>The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.</source>
          <target state="translated">Los métodos basados en la prueba F estiman el grado de dependencia lineal entre dos variables aleatorias.Por otra parte,los métodos de información mutua pueden captar cualquier tipo de dependencia estadística,pero al no ser paramétricos,requieren más muestras para una estimación precisa.</target>
        </trans-unit>
        <trans-unit id="86d449abad7d30ccc370e439f66e761a4c276339" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;Glossary&lt;/a&gt;, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9346206e9d06ed06060a101f1e51a8e80f2445f7" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42e3e79f7ef752ca5a9bd963af37b6abd9f4c61f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena o invocable, debe ser una de las opciones permitidas por &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt; &lt;/a&gt; para su par&amp;aacute;metro de m&amp;eacute;trica. Si la m&amp;eacute;trica est&amp;aacute; &amp;ldquo;calculada previamente&amp;rdquo;, se supone que X es una matriz de distancia y debe ser cuadrada. X puede ser una matriz dispersa, en cuyo caso solo los elementos &quot;distintos de cero&quot; pueden considerarse vecinos para DBSCAN.</target>
        </trans-unit>
        <trans-unit id="379ba7f47f97569c7bd4b20c4c77667f05344897" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples corresponding to each class is the point from which the sum of the distances (according to the metric) of all samples that belong to that particular class are minimized. If the &amp;ldquo;manhattan&amp;rdquo; metric is provided, this centroid is the median and for all other metrics, the centroid is now set to be the mean.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena o se puede llamar, debe ser una de las opciones permitidas por metrics.pairwise.pairwise_distances para su par&amp;aacute;metro de m&amp;eacute;trica. Los centroides de las muestras correspondientes a cada clase es el punto desde el cual se minimiza la suma de las distancias (seg&amp;uacute;n la m&amp;eacute;trica) de todas las muestras que pertenecen a esa clase en particular. Si se proporciona la m&amp;eacute;trica &quot;manhattan&quot;, este centroide es la mediana y para todas las dem&amp;aacute;s m&amp;eacute;tricas, el centroide ahora se establece como la media.</target>
        </trans-unit>
        <trans-unit id="c83cb22e1c81dc697b4c0637e95b8aeb91bdccce" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt;.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si metric es una cadena, debe ser una de las opciones permitidas por &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt; . Si X es la matriz de distancias en s&amp;iacute;, use &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="76b4b37055a409fe2e4e93cf3ad5227beac3187f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si metric es una cadena, debe ser una de las opciones permitidas por &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt; . Si X es la matriz de distancia en s&amp;iacute;, use &quot;precalculado&quot; como m&amp;eacute;trica.</target>
        </trans-unit>
        <trans-unit id="74dc2788261a34d78d8ba4595625c4b4c3a263d6" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric. Precomputed distance matrices must have 0 along the diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0f209bf70493187675786c136d5f63b20f1b34d" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena, debe ser una de las opciones permitidas por scipy.spatial.distance.pdist para su par&amp;aacute;metro de m&amp;eacute;trica, o una m&amp;eacute;trica listada en pares.PAIRWISE_DISTANCE_FUNCTIONS. Si la m&amp;eacute;trica est&amp;aacute; &amp;ldquo;precalculada&amp;rdquo;, se supone que X es una matriz de distancia. Alternativamente, si m&amp;eacute;trica es una funci&amp;oacute;n invocable, se llama en cada par de instancias (filas) y se registra el valor resultante. El invocable debe tomar dos matrices de X como entrada y devolver un valor que indique la distancia entre ellas.</target>
        </trans-unit>
        <trans-unit id="5d6bfb4b6dd59ce295c63dba458c016e3505d6e2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is &amp;ldquo;euclidean&amp;rdquo; which is interpreted as squared euclidean distance.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena, debe ser una de las opciones permitidas por scipy.spatial.distance.pdist para su par&amp;aacute;metro de m&amp;eacute;trica, o una m&amp;eacute;trica listada en pares.PAIRWISE_DISTANCE_FUNCTIONS. Si la m&amp;eacute;trica est&amp;aacute; &amp;ldquo;precalculada&amp;rdquo;, se supone que X es una matriz de distancia. Alternativamente, si m&amp;eacute;trica es una funci&amp;oacute;n invocable, se llama en cada par de instancias (filas) y se registra el valor resultante. El invocable debe tomar dos matrices de X como entrada y devolver un valor que indique la distancia entre ellas. El valor predeterminado es &quot;euclidiana&quot;, que se interpreta como distancia euclidiana al cuadrado.</target>
        </trans-unit>
        <trans-unit id="1d21e688cc862f7d70c6767af76fb5452c5fdcd0" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options specified in PAIRED_DISTANCES, including &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, or &amp;ldquo;cosine&amp;rdquo;. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular la distancia entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena, debe ser una de las opciones especificadas en PAIRED_DISTANCES, incluidas &amp;ldquo;euclidiana&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo; o &amp;ldquo;coseno&amp;rdquo;. Alternativamente, si m&amp;eacute;trica es una funci&amp;oacute;n invocable, se llama en cada par de instancias (filas) y se registra el valor resultante. El invocable debe tomar dos matrices de X como entrada y devolver un valor que indique la distancia entre ellas.</target>
        </trans-unit>
        <trans-unit id="f36b3ebd6c1e0314a19882dfd7c792465ced8cb2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">La m&amp;eacute;trica que se utilizar&amp;aacute; al calcular el kernel entre instancias en una matriz de caracter&amp;iacute;sticas. Si la m&amp;eacute;trica es una cadena, debe ser una de las m&amp;eacute;tricas en pares.PAIRWISE_KERNEL_FUNCTIONS. Si la m&amp;eacute;trica est&amp;aacute; &quot;precalculada&quot;, se supone que X es una matriz del n&amp;uacute;cleo. Alternativamente, si m&amp;eacute;trica es una funci&amp;oacute;n invocable, se llama en cada par de instancias (filas) y se registra el valor resultante. El invocable debe tomar dos matrices de X como entrada y devolver un valor que indique la distancia entre ellas.</target>
        </trans-unit>
        <trans-unit id="fc294d94430f6d524884c3e3bbe7bf2fd080ba19" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from &lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4d18e9d9a4e3ba27fc7d43cfc9aa960df294a66" translate="yes" xml:space="preserve">
          <source>The minimal number of components to guarantee with good probability an eps-embedding with n_samples.</source>
          <target state="translated">El número mínimo de componentes para garantizar con buena probabilidad un acoplamiento de eps con n_muestras.</target>
        </trans-unit>
        <trans-unit id="bffda997c9866bdeb3072a563a59e0dfd40b41cb" translate="yes" xml:space="preserve">
          <source>The minimization problem becomes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcfd1fcfc3ab5a126126fe7777d3f592dbce3714" translate="yes" xml:space="preserve">
          <source>The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.</source>
          <target state="translated">La puntuación mínima de consenso,0,se produce cuando todos los pares de bíceps son totalmente diferentes.La puntuación máxima,1,se produce cuando ambos conjuntos son idénticos.</target>
        </trans-unit>
        <trans-unit id="546685a327b5ecf09c20bafa81b23b9f31e36223" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantee the eps-embedding is given by:</source>
          <target state="translated">El número mínimo de componentes para garantizar la incorporación de la eps viene dado por:</target>
        </trans-unit>
        <trans-unit id="d171bb6d353676c30b749e2ca85c8811f4027e78" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantees the eps-embedding is given by:</source>
          <target state="translated">El número mínimo de componentes para garantizar la incorporación de la eps viene dado por:</target>
        </trans-unit>
        <trans-unit id="78f0aa92767e958a159a7201fe662ff62e3924a6" translate="yes" xml:space="preserve">
          <source>The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and &lt;code&gt;min_features_to_select&lt;/code&gt; isn&amp;rsquo;t divisible by &lt;code&gt;step&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero m&amp;iacute;nimo de funciones que se seleccionar&amp;aacute;n. Este n&amp;uacute;mero de funciones siempre se &lt;code&gt;min_features_to_select&lt;/code&gt; , incluso si la diferencia entre el recuento de funciones original y min_features_to_select no es divisible por &lt;code&gt;step&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="dc2c2ff1c0458b789a8dbc35522b80adbfc0fde6" translate="yes" xml:space="preserve">
          <source>The minimum number of samples per leaf. For small datasets with less than a few hundred samples, it is recommended to lower this value since only very shallow trees would be built.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1db45f422759f4529cb388eaa249fdf3a381a4d3" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least &lt;code&gt;min_samples_leaf&lt;/code&gt; training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</source>
          <target state="translated">El n&amp;uacute;mero m&amp;iacute;nimo de muestras necesarias para estar en un nodo hoja. Un punto de divisi&amp;oacute;n a cualquier profundidad solo se considerar&amp;aacute; si deja al menos muestras de entrenamiento &lt;code&gt;min_samples_leaf&lt;/code&gt; en cada una de las ramas izquierda y derecha. Esto puede tener el efecto de suavizar el modelo, especialmente en regresi&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="754bd23e1994f02a9f11fc7f2013baebc017ca4d" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to split an internal node:</source>
          <target state="translated">El número mínimo de muestras necesarias para dividir un nodo interno:</target>
        </trans-unit>
        <trans-unit id="28930c1108db91ea501be86c776a39ad33671bd9" translate="yes" xml:space="preserve">
          <source>The minimum score is zero, with lower values indicating better clustering.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cad4521e1577b76d3fbf78851e9af149127ba4db" translate="yes" xml:space="preserve">
          <source>The minimum valid value the parameter can take. If None (default) it is implied that the parameter does not have a lower bound.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34dec0ced4163b0b0c5f6b2dfda2c2ae915251bf" translate="yes" xml:space="preserve">
          <source>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</source>
          <target state="translated">La fracción ponderada mínima de la suma total de pesos (de todas las muestras de entrada)debe estar en un nodo de hoja.Las muestras tienen igual peso cuando no se proporciona el peso_de_la_muestra.</target>
        </trans-unit>
        <trans-unit id="adbfb8d83e84eef2c959ef548acb01276646831a" translate="yes" xml:space="preserve">
          <source>The missing indicator for input data. The data type of &lt;code&gt;Xt&lt;/code&gt; will be boolean.</source>
          <target state="translated">El indicador que falta para los datos de entrada. El tipo de datos de &lt;code&gt;Xt&lt;/code&gt; ser&amp;aacute; booleano.</target>
        </trans-unit>
        <trans-unit id="2b5249d3ed9eb260ab7aedd15c61af2c7df4b9f1" translate="yes" xml:space="preserve">
          <source>The mixing matrix to be used to initialize the algorithm.</source>
          <target state="translated">La matriz de mezcla que se utilizará para inicializar el algoritmo.</target>
        </trans-unit>
        <trans-unit id="bc7bce3b2af118e0efad437caf7d72239aa18a90" translate="yes" xml:space="preserve">
          <source>The mixing matrix.</source>
          <target state="translated">La matriz de mezcla.</target>
        </trans-unit>
        <trans-unit id="d9852ae9396dc8e73ffef0a95fb9e0d330c7fbbc" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.</source>
          <target state="translated">El modelo ajusta una densidad gaussiana a cada clase,asumiendo que todas las clases comparten la misma matriz de covarianza.</target>
        </trans-unit>
        <trans-unit id="c1fbc40d4a13f1c2e80dae47c2199d2a0ef37a24" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class.</source>
          <target state="translated">El modelo ajusta una densidad gaussiana a cada clase.</target>
        </trans-unit>
        <trans-unit id="c383334c8db0249de2e6de6266d715cf757d0e4f" translate="yes" xml:space="preserve">
          <source>The model learnt is far from being a good model making accurate predictions: this is obvious when looking at the plot above, where good predictions should lie on the red line.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="744fe4c01d7aac1fa2b93515c5b761f9f450f5fc" translate="yes" xml:space="preserve">
          <source>The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.</source>
          <target state="translated">El modelo hace supuestos sobre la distribuci&amp;oacute;n de insumos. Por el momento, scikit-learn solo proporciona &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; , que asume que las entradas son valores binarios o valores entre 0 y 1, y cada uno codifica la probabilidad de que la funci&amp;oacute;n espec&amp;iacute;fica se active.</target>
        </trans-unit>
        <trans-unit id="3e7d91224c848a3199f89b8ed290703400860de0" translate="yes" xml:space="preserve">
          <source>The model need to have probability information computed at training time: fit with attribute &lt;code&gt;probability&lt;/code&gt; set to True.</source>
          <target state="translated">El modelo debe tener informaci&amp;oacute;n de probabilidad calculada en el momento del entrenamiento: ajuste con &lt;code&gt;probability&lt;/code&gt; atributo establecida en Verdadero.</target>
        </trans-unit>
        <trans-unit id="f6f2922bb9e8bafadd9354168cde2a4c9535aa4f" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; attributes: &lt;code&gt;coef_&lt;/code&gt; holds the weights \(w\) and &lt;code&gt;intercept_&lt;/code&gt; holds \(b\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="532fcfc5fc4303622a7e9b0379fb5dd6d278b658" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the members &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;:</source>
          <target state="translated">Se puede acceder a los par&amp;aacute;metros del modelo a trav&amp;eacute;s de los miembros &lt;code&gt;coef_&lt;/code&gt; e &lt;code&gt;intercept_&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="87fcd2dcd4a9683677aa4a82e264db34c1778c7c" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.</source>
          <target state="translated">El modelo producido por la clasificación de vectores de apoyo (como se ha descrito anteriormente)depende sólo de un subconjunto de los datos de capacitación,porque la función de costos para la construcción del modelo no se preocupa por los puntos de capacitación que se encuentran más allá del margen.Análogamente,el modelo producido por la regresión del vector de apoyo depende sólo de un subconjunto de los datos de capacitación,porque la función de costo para construir el modelo ignora cualquier dato de capacitación cercano a la predicción del modelo.</target>
        </trans-unit>
        <trans-unit id="14cc80df7b1b8b9f6ebfb922c7e5dfe3e7692df9" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4911f4659c5f4e900454be23c5d2d4b6dba06b2b" translate="yes" xml:space="preserve">
          <source>The model will stay unchanged.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ffaf4f38449ad493c6f07021545ef1cc0f916269" translate="yes" xml:space="preserve">
          <source>The models are ordered from strongest regularized to least regularized. The 4 coefficients of the models are collected and plotted as a &amp;ldquo;regularization path&amp;rdquo;: on the left-hand side of the figure (strong regularizers), all the coefficients are exactly 0. When regularization gets progressively looser, coefficients can get non-zero values one after the other.</source>
          <target state="translated">Los modelos est&amp;aacute;n ordenados desde el m&amp;aacute;s fuerte al menos regularizado. Los 4 coeficientes de los modelos se recopilan y trazan como una &quot;ruta de regularizaci&amp;oacute;n&quot;: en el lado izquierdo de la figura (regularizadores fuertes), todos los coeficientes son exactamente 0. Cuando la regularizaci&amp;oacute;n se vuelve progresivamente m&amp;aacute;s flexible, los coeficientes pueden ser distintos de cero valores uno tras otro.</target>
        </trans-unit>
        <trans-unit id="41864e959b3c3945768dfd483a7ad2160aff5a56" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;.</source>
          <target state="translated">El m&amp;oacute;dulo &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; incluye el popular algoritmo de refuerzo AdaBoost, introducido en 1995 por Freund y Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="1ad1575f52119d57fdeed35bbf3eb9cde06a6188" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted decision trees.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06014352d795d21d24d63a43012b2cdda688123a" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted regression trees.</source>
          <target state="translated">El m&amp;oacute;dulo &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; proporciona m&amp;eacute;todos para clasificaci&amp;oacute;n y regresi&amp;oacute;n a trav&amp;eacute;s de &amp;aacute;rboles de regresi&amp;oacute;n potenciados por gradientes.</target>
        </trans-unit>
        <trans-unit id="c3b04b84598eb19b700a6f5a28a6ebcc8d4034a0" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; also exposes a set of simple functions measuring a prediction error given ground truth and prediction:</source>
          <target state="translated">El m&amp;oacute;dulo &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt; tambi&amp;eacute;n expone un conjunto de funciones simples que miden un error de predicci&amp;oacute;n dada la verdad y la predicci&amp;oacute;n del terreno:</target>
        </trans-unit>
        <trans-unit id="a4ba63f9b8e0d9fa0a182e0bb4dc5760121e3e0e" translate="yes" xml:space="preserve">
          <source>The module &lt;code&gt;partial_dependence&lt;/code&gt; provides a convenience function &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; and a two-way PDP between the two features:</source>
          <target state="translated">El m&amp;oacute;dulo &lt;code&gt;partial_dependence&lt;/code&gt; proporciona una funci&amp;oacute;n de conveniencia &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt; para crear gr&amp;aacute;ficos de dependencia parcial unidireccionales y bidireccionales. En el siguiente ejemplo, mostramos c&amp;oacute;mo crear una cuadr&amp;iacute;cula de gr&amp;aacute;ficos de dependencia parcial: dos PDP unidireccionales para las caracter&amp;iacute;sticas &lt;code&gt;0&lt;/code&gt; y &lt;code&gt;1&lt;/code&gt; y un PDP bidireccional entre las dos caracter&amp;iacute;sticas:</target>
        </trans-unit>
        <trans-unit id="5b2518b0fdafd75a6658a4d2a018bccb79345ce3" translate="yes" xml:space="preserve">
          <source>The module contains the public attributes &lt;code&gt;coefs_&lt;/code&gt; and &lt;code&gt;intercepts_&lt;/code&gt;. &lt;code&gt;coefs_&lt;/code&gt; is a list of weight matrices, where weight matrix at index \(i\) represents the weights between layer \(i\) and layer \(i+1\). &lt;code&gt;intercepts_&lt;/code&gt; is a list of bias vectors, where the vector at index \(i\) represents the bias values added to layer \(i+1\).</source>
          <target state="translated">El m&amp;oacute;dulo contiene los atributos p&amp;uacute;blicos &lt;code&gt;coefs_&lt;/code&gt; e &lt;code&gt;intercepts_&lt;/code&gt; . &lt;code&gt;coefs_&lt;/code&gt; es una lista de matrices de peso, donde la matriz de peso en el &amp;iacute;ndice \ (i \) representa los pesos entre la capa \ (i \) y la capa \ (i + 1 \). &lt;code&gt;intercepts_&lt;/code&gt; es una lista de vectores de sesgo, donde el vector en el &amp;iacute;ndice \ (i \) representa los valores de sesgo agregados a la capa \ (i + 1 \).</target>
        </trans-unit>
        <trans-unit id="b31fa7fd3ac8f2a64f0dd29549e8dd9abb96ee19" translate="yes" xml:space="preserve">
          <source>The module: &lt;code&gt;random_projection&lt;/code&gt; provides several tools for data reduction by random projections. See the relevant section of the documentation: &lt;a href=&quot;random_projection#random-projection&quot;&gt;Random Projection&lt;/a&gt;.</source>
          <target state="translated">El m&amp;oacute;dulo: &lt;code&gt;random_projection&lt;/code&gt; proporciona varias herramientas para la reducci&amp;oacute;n de datos mediante proyecciones aleatorias. Consulte la secci&amp;oacute;n correspondiente de la documentaci&amp;oacute;n: &lt;a href=&quot;random_projection#random-projection&quot;&gt;Proyecci&amp;oacute;n aleatoria&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="81f532ac90d157aeffd690ee0c3b7aa6b7bbd149" translate="yes" xml:space="preserve">
          <source>The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of &lt;code&gt;_fit_stages&lt;/code&gt; as keyword arguments &lt;code&gt;callable(i, self,
locals())&lt;/code&gt;. If the callable returns &lt;code&gt;True&lt;/code&gt; the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.</source>
          <target state="translated">Se llama al monitor despu&amp;eacute;s de cada iteraci&amp;oacute;n con la iteraci&amp;oacute;n actual, una referencia al estimador y las variables locales de &lt;code&gt;_fit_stages&lt;/code&gt; como argumentos de palabra clave &lt;code&gt;callable(i, self, locals())&lt;/code&gt; . Si el invocable devuelve &lt;code&gt;True&lt;/code&gt; , el procedimiento de ajuste se detiene. El monitor se puede utilizar para varias cosas, como calcular estimaciones retenidas, detenerse antes de tiempo, introspecci&amp;oacute;n del modelo e instant&amp;aacute;neas.</target>
        </trans-unit>
        <trans-unit id="52aa15c9df5da45110f63c8c26b8cfa477557d62" translate="yes" xml:space="preserve">
          <source>The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the &lt;strong&gt;regularization path&lt;/strong&gt; of the estimator.</source>
          <target state="translated">El par&amp;aacute;metro m&amp;aacute;s com&amp;uacute;n susceptible de esta estrategia es el par&amp;aacute;metro que codifica la fuerza del regularizador. En este caso decimos que calculamos la &lt;strong&gt;ruta&lt;/strong&gt; de &lt;strong&gt;regularizaci&amp;oacute;n&lt;/strong&gt; del estimador.</target>
        </trans-unit>
        <trans-unit id="3156312e704bdb91fdff12aa2e110d4f21e21302" translate="yes" xml:space="preserve">
          <source>The most intuitive way to do so is to use a bags of words representation:</source>
          <target state="translated">La forma más intuitiva de hacerlo es usar una representación de bolsas de palabras:</target>
        </trans-unit>
        <trans-unit id="399fde41b63b2ea676c5145583a3950879f6c9fa" translate="yes" xml:space="preserve">
          <source>The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.</source>
          <target state="translated">La motivación para usar este escalamiento incluye la robustez hasta desviaciones estándar muy pequeñas de las características y la preservación de entradas cero en datos escasos.</target>
        </trans-unit>
        <trans-unit id="3ec6281766fab2b8f9e9f9f6e92da7d4704e6316" translate="yes" xml:space="preserve">
          <source>The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable.</source>
          <target state="translated">El lazo multitarea permite ajustar múltiples problemas de regresión conjuntamente haciendo que las características seleccionadas sean las mismas en todas las tareas.Este ejemplo simula mediciones secuenciales,cada tarea es un instante de tiempo,y las características relevantes varían en amplitud a lo largo del tiempo mientras son las mismas.El lazo de tareas múltiples impone que los rasgos que se seleccionan en un punto de tiempo se seleccionen para todo el punto de tiempo.Esto hace que la selección de características por el lazo sea más estable.</target>
        </trans-unit>
        <trans-unit id="7b55b776834f44186e095c0df9ee2b704ce3630c" translate="yes" xml:space="preserve">
          <source>The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature:</source>
          <target state="translated">La definición de multiclase aquí parece la extensión más razonable de la métrica utilizada en la clasificación binaria,aunque no hay un consenso seguro en la literatura:</target>
        </trans-unit>
        <trans-unit id="5fdb408d7bc477f0762da630aa0f3aea39db3f13" translate="yes" xml:space="preserve">
          <source>The multiclass support is handled according to a one-vs-one scheme.</source>
          <target state="translated">El apoyo multiclase se maneja según un esquema de uno contra uno.</target>
        </trans-unit>
        <trans-unit id="559112a07c580b9f5163f4eaab25d31b6f252bc0" translate="yes" xml:space="preserve">
          <source>The multilabel_confusion_matrix calculates class-wise or sample-wise multilabel confusion matrices, and in multiclass tasks, labels are binarized under a one-vs-rest way; while confusion_matrix calculates one confusion matrix for confusion between every two classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66359e593d021aa5ae2d568f4acc056ec0bf4a32" translate="yes" xml:space="preserve">
          <source>The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.</source>
          <target state="translated">El clasificador multinomial Naive Bayes es adecuado para la clasificación con características discretas (por ejemplo,el recuento de palabras para la clasificación del texto).La distribución multinomial normalmente requiere un recuento de características enteras.Sin embargo,en la práctica,los recuentos fraccionarios como el tf-idf también pueden funcionar.</target>
        </trans-unit>
        <trans-unit id="9fcb660998dc7edb2d34f5dc5854df445bad9a92" translate="yes" xml:space="preserve">
          <source>The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:</source>
          <target state="translated">Las métricas múltiples pueden especificarse como una lista,una tupla o un conjunto de nombres de anotadores predefinidos:</target>
        </trans-unit>
        <trans-unit id="0f55ab2d94390a312af5800dfb617d9ab1f868e3" translate="yes" xml:space="preserve">
          <source>The name of the hyperparameter. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66cdcb45ef3965759b13ccc7a57841383638bd06" translate="yes" xml:space="preserve">
          <source>The name of the parameter to be printed in error messages.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a09e250651300d47ed9cd74b128d0a12a5aa8e3e" translate="yes" xml:space="preserve">
          <source>The name of the sample image loaded</source>
          <target state="translated">El nombre de la imagen de muestra cargada</target>
        </trans-unit>
        <trans-unit id="b853190860f83bcb94f9f4edb130e6f04adf4ae5" translate="yes" xml:space="preserve">
          <source>The names &lt;code&gt;vect&lt;/code&gt;, &lt;code&gt;tfidf&lt;/code&gt; and &lt;code&gt;clf&lt;/code&gt; (classifier) are arbitrary. We will use them to perform grid search for suitable hyperparameters below. We can now train the model with a single command:</source>
          <target state="translated">Los nombres &lt;code&gt;vect&lt;/code&gt; , &lt;code&gt;tfidf&lt;/code&gt; y &lt;code&gt;clf&lt;/code&gt; (clasificador) son arbitrarios. Los usaremos para realizar una b&amp;uacute;squeda de cuadr&amp;iacute;cula de hiperpar&amp;aacute;metros adecuados a continuaci&amp;oacute;n. Ahora podemos entrenar el modelo con un solo comando:</target>
        </trans-unit>
        <trans-unit id="37788c74066a15702915a906173e5747b775560c" translate="yes" xml:space="preserve">
          <source>The names of features</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a6d82313da18df77f363e5439a6b3ce08c6680a" translate="yes" xml:space="preserve">
          <source>The names of target classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9146e93a1405c3d2cac65e0084d1b1c7a951b3b3" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns</source>
          <target state="translated">Los nombres de las columnas del conjunto de datos</target>
        </trans-unit>
        <trans-unit id="9c71b3cc12d889ddb9194bd05e135e431b48c18f" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e5fcf3b8aec8853bca895bccf214ac28ac4d27e" translate="yes" xml:space="preserve">
          <source>The names of the target columns</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40ffca0689344b78c3e10b40b262e0c45ea9b50f" translate="yes" xml:space="preserve">
          <source>The names of the target columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1942d0e2fca00caa96a5a7573d350784bbfcadf1" translate="yes" xml:space="preserve">
          <source>The new backend can then be selected by passing its name as the backend argument to the Parallel class. Moreover, the default backend can be overwritten globally by setting make_default=True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0fb0e8bf0410fa55523d36fa2a7a49ac4415117" translate="yes" xml:space="preserve">
          <source>The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.</source>
          <target state="translated">El nuevo tipo será np.float32 o np.float64,dependiendo del tipo original.La función puede crear una copia o modificar el argumento dependiendo de la copia del argumento.</target>
        </trans-unit>
        <trans-unit id="6d7b951e7afa9271dd7834467cb67e175d7e626f" translate="yes" xml:space="preserve">
          <source>The new entry \(d(u,v)\) is computed as follows,</source>
          <target state="translated">La nueva entrada (d(u,v)\Nse calcula de la siguiente manera,</target>
        </trans-unit>
        <trans-unit id="c771668f5d4b4d7aa145f31455a67e2ba21af3ce" translate="yes" xml:space="preserve">
          <source>The next figure compares the results obtained for the different type of the weight concentration prior (parameter &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;) for different values of &lt;code&gt;weight_concentration_prior&lt;/code&gt;. Here, we can see the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is &amp;lsquo;dirichlet_distribution&amp;rsquo; while this is not necessarily the case for the &amp;lsquo;dirichlet_process&amp;rsquo; type (used by default).</source>
          <target state="translated">La siguiente figura compara los resultados obtenidos para los diferentes tipos de concentraci&amp;oacute;n de peso anterior (par&amp;aacute;metro &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; ) para diferentes valores de &lt;code&gt;weight_concentration_prior&lt;/code&gt; . Aqu&amp;iacute;, podemos ver que el valor del par&amp;aacute;metro &lt;code&gt;weight_concentration_prior&lt;/code&gt; tiene un fuerte impacto en el n&amp;uacute;mero efectivo de componentes activos obtenidos. Tambi&amp;eacute;n podemos notar que los valores grandes para el peso de concentraci&amp;oacute;n anterior conducen a pesos m&amp;aacute;s uniformes cuando el tipo de prior es 'dirichlet_distribution', mientras que este no es necesariamente el caso del tipo 'dirichlet_process' (usado por defecto).</target>
        </trans-unit>
        <trans-unit id="c25f5050f6fd3011382e4c50ae76e75315ba1a26" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; scales better. With regard to prediction time, &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6d38fd34e131451ba8b974153991556ff8b9398" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;code&gt;SVR&lt;/code&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;code&gt;SVR&lt;/code&gt; scales better. With regard to prediction time, &lt;code&gt;SVR&lt;/code&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;code&gt;SVR&lt;/code&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="translated">La siguiente figura compara el tiempo de ajuste y predicci&amp;oacute;n de &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; y &lt;code&gt;SVR&lt;/code&gt; para diferentes tama&amp;ntilde;os del conjunto de entrenamiento. El ajuste de &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; es m&amp;aacute;s r&amp;aacute;pido que el &lt;code&gt;SVR&lt;/code&gt; para conjuntos de entrenamiento de tama&amp;ntilde;o mediano (menos de 1000 muestras); sin embargo, para un entrenamiento m&amp;aacute;s grande, las escalas de &lt;code&gt;SVR&lt;/code&gt; mejores. Con respecto al tiempo de predicci&amp;oacute;n, &lt;code&gt;SVR&lt;/code&gt; es m&amp;aacute;s r&amp;aacute;pido que &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; para todos los tama&amp;ntilde;os del conjunto de entrenamiento debido a la soluci&amp;oacute;n escasa aprendida. Tenga en cuenta que el grado de escasez y, por tanto, el tiempo de predicci&amp;oacute;n depende de los par&amp;aacute;metros \ (\ epsilon \) y \ (C \) del &lt;code&gt;SVR&lt;/code&gt; ; \ (\ epsilon = 0 \) corresponder&amp;iacute;a a un modelo denso.</target>
        </trans-unit>
        <trans-unit id="7fb8a1fd588ff5df76899e15821355218c8c70df" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of KRR and SVR for different sizes of the training set. Fitting KRR is faster than SVR for medium- sized training sets (less than 1000 samples); however, for larger training sets SVR scales better. With regard to prediction time, SVR is faster than KRR for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters epsilon and C of the SVR.</source>
          <target state="translated">La siguiente figura compara el tiempo de ajuste y predicción de la KRR y SVR para diferentes tamaños del conjunto de entrenamiento.El ajuste de la KRR es más rápido que la SVR para los conjuntos de entrenamiento de tamaño medio (menos de 1000 muestras);sin embargo,para los conjuntos de entrenamiento más grandes la SVR escala mejor.En cuanto al tiempo de predicción,la RVS es más rápida que la RCE para todos los tamaños del conjunto de entrenamiento debido a la solución de dispersión aprendida.Obsérvese que el grado de dispersión y,por lo tanto,el tiempo de predicción depende de los parámetros épsilon y C de la SVR.</target>
        </trans-unit>
        <trans-unit id="b8b8c72913234ec998c925f50d9fa1a29ef7082f" translate="yes" xml:space="preserve">
          <source>The next image illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">La siguiente imagen ilustra cómo la calibración sigmoide cambia las probabilidades previstas para un problema de clasificación de 3 clases.Se ilustra el estándar 2-simplex,donde las tres esquinas corresponden a las tres clases.Las flechas apuntan desde los vectores de probabilidad predichos por un clasificador no calibrado hasta los vectores de probabilidad predichos por el mismo clasificador después de la calibración del sigmoide en un conjunto de validación de retención.Los colores indican la verdadera clase de una instancia (rojo:clase 1,verde:clase 2,azul:clase 3).</target>
        </trans-unit>
        <trans-unit id="d16a780143f0785e9248e298dec17c9fe8de9d1e" translate="yes" xml:space="preserve">
          <source>The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity.</source>
          <target state="translated">Los nodos son variables aleatorias cuyos estados dependen del estado de los otros nodos a los que están conectados.Por lo tanto,el modelo se parametriza por los pesos de las conexiones,así como un término de intercepción (sesgo)para cada unidad visible y oculta,omitido de la imagen por simplicidad.</target>
        </trans-unit>
        <trans-unit id="55f688a90e745dd5020f6b5c567bbe8f6396fa6e" translate="yes" xml:space="preserve">
          <source>The noise level in the targets can be specified by passing it via the parameter &lt;code&gt;alpha&lt;/code&gt;, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below).</source>
          <target state="translated">El nivel de ruido en los objetivos se puede especificar pas&amp;aacute;ndolo a trav&amp;eacute;s del par&amp;aacute;metro &lt;code&gt;alpha&lt;/code&gt; , ya sea globalmente como escalar o por punto de datos. Tenga en cuenta que un nivel de ruido moderado tambi&amp;eacute;n puede ser &amp;uacute;til para tratar los problemas num&amp;eacute;ricos durante el ajuste, ya que se implementa de manera efectiva como regularizaci&amp;oacute;n de Tikhonov, es decir, agreg&amp;aacute;ndolo a la diagonal de la matriz del n&amp;uacute;cleo. Una alternativa a especificar el nivel de ruido expl&amp;iacute;citamente es incluir un componente WhiteKernel en el kernel, que puede estimar el nivel de ruido global a partir de los datos (ver ejemplo a continuaci&amp;oacute;n).</target>
        </trans-unit>
        <trans-unit id="99b58f648d173c7793dc0e913318a8835572f0c2" translate="yes" xml:space="preserve">
          <source>The non-fixed, log-transformed hyperparameters of the kernel</source>
          <target state="translated">Los hiperparámetros no fijos y transformados en logaritmo del núcleo</target>
        </trans-unit>
        <trans-unit id="8ea3cf8563db967f688cb46c0883a9d020905a15" translate="yes" xml:space="preserve">
          <source>The nonmetric algorithm adds a monotonic regression step before computing the stress.</source>
          <target state="translated">El algoritmo no métrico añade un paso de regresión monótona antes de calcular el estrés.</target>
        </trans-unit>
        <trans-unit id="8cecdfcc92ccd83125ecaa6c4beb7447e43f92cb" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).</source>
          <target state="translated">La norma a utilizar para normalizar cada muestra no nula (o cada característica no nula si el eje es 0).</target>
        </trans-unit>
        <trans-unit id="39e8a7c3ff72c23082d4ee16a84d74279c8584ba" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample.</source>
          <target state="translated">La norma a utilizar para normalizar cada muestra no nula.</target>
        </trans-unit>
        <trans-unit id="6d3c61aac7334b6f6f54f73be53f19b464b34cbe" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample. If norm=&amp;rsquo;max&amp;rsquo; is used, values will be rescaled by the maximum of the absolute values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4510db24d586496f5cd27e9cd8024ffe16a368e" translate="yes" xml:space="preserve">
          <source>The normalized mutual information is defined as</source>
          <target state="translated">La información mutua normalizada se define como</target>
        </trans-unit>
        <trans-unit id="ad22e0beb8a3a9f5574d757b3678d39d78057ba3" translate="yes" xml:space="preserve">
          <source>The normalizer instance can then be used on sample vectors as any transformer:</source>
          <target state="translated">La instancia normalizadora puede entonces utilizarse en vectores de muestra como cualquier transformador:</target>
        </trans-unit>
        <trans-unit id="82d79be22b3b3fa23f79d393f8a5b628a27ddb08" translate="yes" xml:space="preserve">
          <source>The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below).</source>
          <target state="translated">El número k de vecinos considerados,(alias parámetro n_vecinos)se elige típicamente 1)mayor que el número mínimo de objetos que un cúmulo tiene que contener,para que otros objetos puedan ser valores atípicos locales en relación con este cúmulo,y 2)menor que el número máximo de objetos cercanos que pueden ser potencialmente valores atípicos locales.En la práctica,esa información no suele estar disponible,y tomar n_vecinos=20 parece funcionar bien en general.Cuando la proporción de valores atípicos es alta (es decir,superior al 10 %,como en el ejemplo siguiente),n_vecinos debe ser mayor (n_vecinos=35 en el ejemplo siguiente).</target>
        </trans-unit>
        <trans-unit id="9650bdc07aaa5281bfc3be8ecd4292573a8743c6" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to compute the partial dependences. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a35a9d90f2abd9af864433913570f3dbe7f2765" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de CPU que se utilizar&amp;aacute;n para realizar el c&amp;aacute;lculo OVA (One Versus All, para problemas de varias clases). &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="985a6706e191013209fa45542581d0ffa377ae50" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36ae328d3607f79ec631b3fe327da7ed8cf8098b" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de CPU que se utilizar&amp;aacute;n para realizar el c&amp;aacute;lculo. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="2e29d4d145cd20eb08e9ea5571074f934df7e0d8" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="720879c36f3df22b5ec2b112039efcd8b5ba8b1b" translate="yes" xml:space="preserve">
          <source>The number of EM iterations to perform.</source>
          <target state="translated">El número de iteraciones EM a realizar.</target>
        </trans-unit>
        <trans-unit id="df46ce41d5f6eae9d9abc12693e64e5ffe0f042c" translate="yes" xml:space="preserve">
          <source>The number of OpenMP threads to use for the computation. Parallelism is sample-wise on the main cython loop which assigns each sample to its closest center.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40648db4f1524a67ed66976c14cc75bac6eed386" translate="yes" xml:space="preserve">
          <source>The number of atomic tasks to dispatch at once to each worker. When individual evaluations are very fast, dispatching calls to workers can be slower than sequential computation because of the overhead. Batching fast computations together can mitigate this. The &lt;code&gt;'auto'&lt;/code&gt; strategy keeps track of the time it takes for a batch to complete, and dynamically adjusts the batch size to keep the time on the order of half a second, using a heuristic. The initial batch size is 1. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; with &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; will dispatch batches of a single task at a time as the threading backend has very little overhead and using larger batch size has not proved to bring any gain in that case.</source>
          <target state="translated">El n&amp;uacute;mero de tareas at&amp;oacute;micas para enviar a la vez a cada trabajador. Cuando las evaluaciones individuales son muy r&amp;aacute;pidas, el env&amp;iacute;o de llamadas a los trabajadores puede ser m&amp;aacute;s lento que el c&amp;aacute;lculo secuencial debido a la sobrecarga. La agrupaci&amp;oacute;n de c&amp;aacute;lculos r&amp;aacute;pidos juntos puede mitigar esto. La estrategia &lt;code&gt;'auto'&lt;/code&gt; realiza un seguimiento del tiempo que tarda un lote en completarse y ajusta din&amp;aacute;micamente el tama&amp;ntilde;o del lote para mantener el tiempo en el orden de medio segundo, utilizando una heur&amp;iacute;stica. El tama&amp;ntilde;o del lote inicial es 1. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; con &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; enviar&amp;aacute; lotes de una sola tarea a la vez, ya que el backend de subprocesos tiene muy poca sobrecarga y el uso de un tama&amp;ntilde;o de lote m&amp;aacute;s grande no ha demostrado generar ninguna ganancia en eso. caso.</target>
        </trans-unit>
        <trans-unit id="3ae39cb6a942b138204dbb141781bfb4c75e3760" translate="yes" xml:space="preserve">
          <source>The number of base estimators in the ensemble.</source>
          <target state="translated">El número de estimadores de base en el conjunto.</target>
        </trans-unit>
        <trans-unit id="9fe1c6517ea4c36af295e91a2e2410361ff8432c" translate="yes" xml:space="preserve">
          <source>The number of batches (of tasks) to be pre-dispatched. Default is &amp;lsquo;2*n_jobs&amp;rsquo;. When batch_size=&amp;rdquo;auto&amp;rdquo; this is reasonable default and the workers should never starve.</source>
          <target state="translated">El n&amp;uacute;mero de lotes (de tareas) que se enviar&amp;aacute;n previamente. El valor predeterminado es '2 * n_jobs'. Cuando batch_size = &amp;rdquo;auto&amp;rdquo; es un valor predeterminado razonable y los trabajadores nunca deben morir de hambre.</target>
        </trans-unit>
        <trans-unit id="f9d0a981566d58378881b2cabe57053da9d34fb6" translate="yes" xml:space="preserve">
          <source>The number of biclusters to find.</source>
          <target state="translated">El número de bíceps que hay que encontrar.</target>
        </trans-unit>
        <trans-unit id="b9c52ec2125ed0e2339a6eae69fb246b4dc5348e" translate="yes" xml:space="preserve">
          <source>The number of biclusters.</source>
          <target state="translated">El número de bíceps.</target>
        </trans-unit>
        <trans-unit id="d76125e41f0deb521acf9ed71af5267a484f2d30" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3424019fa75a2f96f62b30f8336ea4cebfa5c4fe" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. The intervals for the bins are determined by the minimum and maximum of the input data. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de contenedores a producir. Los intervalos para los contenedores est&amp;aacute;n determinados por el m&amp;iacute;nimo y el m&amp;aacute;ximo de los datos de entrada. Aumenta ValueError si &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b2f078045a64e3b870ff56d7d5abb73dda7d8a43" translate="yes" xml:space="preserve">
          <source>The number of bins used to bin the data is controlled with the &lt;code&gt;max_bins&lt;/code&gt; parameter. Using less bins acts as a form of regularization. It is generally recommended to use as many bins as possible, which is the default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="016567b86fa6f6a4b4c043763b4e841bd445fc32" translate="yes" xml:space="preserve">
          <source>The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.</source>
          <target state="translated">El número de etapas de impulso a realizar.El refuerzo gradual es bastante robusto para sobreajustar,por lo que un gran número suele dar lugar a un mejor rendimiento.</target>
        </trans-unit>
        <trans-unit id="d5d9cefb2fff3dbdd417050b8ce7c297d42aaa9f" translate="yes" xml:space="preserve">
          <source>The number of claims (&lt;code&gt;ClaimNb&lt;/code&gt;) is a positive integer (0 included). Thus, this target can be modelled by a Poisson distribution. It is then assumed to be the number of discrete events occuring with a constant rate in a given time interval (&lt;code&gt;Exposure&lt;/code&gt;, in units of years). Here we model the frequency &lt;code&gt;y = ClaimNb / Exposure&lt;/code&gt;, which is still a (scaled) Poisson distribution, and use &lt;code&gt;Exposure&lt;/code&gt; as &lt;code&gt;sample_weight&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a186b2a8600030ec87eb172dc6f657942fddfcfb" translate="yes" xml:space="preserve">
          <source>The number of claims (&lt;code&gt;ClaimNb&lt;/code&gt;) is a positive integer that can be modeled as a Poisson distribution. It is then assumed to be the number of discrete events occurring with a constant rate in a given time interval (&lt;code&gt;Exposure&lt;/code&gt;, in units of years).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d31e8a7065eae23aeb71500302a31ebb485560ec" translate="yes" xml:space="preserve">
          <source>The number of classes</source>
          <target state="translated">El número de clases</target>
        </trans-unit>
        <trans-unit id="77a0204799f583fccb414a4215d0dc841510a2f7" translate="yes" xml:space="preserve">
          <source>The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).</source>
          <target state="translated">El número de clases (para los problemas de salida única),o una lista que contenga el número de clases para cada salida (para los problemas de salida múltiple).</target>
        </trans-unit>
        <trans-unit id="221daa93ea83049b71eff667b2da7d0ce3fa89ae" translate="yes" xml:space="preserve">
          <source>The number of classes (or labels) of the classification problem.</source>
          <target state="translated">El número de clases (o etiquetas)del problema de clasificación.</target>
        </trans-unit>
        <trans-unit id="7b75bf8cc583f50195e96e51d244ce57f23360a0" translate="yes" xml:space="preserve">
          <source>The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).</source>
          <target state="translated">El número de clases (problema de salida única),o una lista que contenga el número de clases para cada salida (problema de salida múltiple).</target>
        </trans-unit>
        <trans-unit id="8b5fc0d622c5abb76da13c6a8f50c84a211eca46" translate="yes" xml:space="preserve">
          <source>The number of classes in the training data</source>
          <target state="translated">El número de clases en los datos de entrenamiento</target>
        </trans-unit>
        <trans-unit id="a096e01744ef01b9d139303dfd4a94a83569986b" translate="yes" xml:space="preserve">
          <source>The number of classes of the classification problem.</source>
          <target state="translated">El número de clases del problema de clasificación.</target>
        </trans-unit>
        <trans-unit id="21583bce2023d80fa6dedc801ccace76e8a2445f" translate="yes" xml:space="preserve">
          <source>The number of classes to return.</source>
          <target state="translated">El número de clases a devolver.</target>
        </trans-unit>
        <trans-unit id="7defdc95bb1ddc0580ac0be76fc251278b72fb84" translate="yes" xml:space="preserve">
          <source>The number of classes.</source>
          <target state="translated">El número de clases.</target>
        </trans-unit>
        <trans-unit id="9298de4c7bbca93c3d80fc1d9af6567cb0b29b0a" translate="yes" xml:space="preserve">
          <source>The number of clusters found by the algorithm. If &lt;code&gt;distance_threshold=None&lt;/code&gt;, it will be equal to the given &lt;code&gt;n_clusters&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="062fe5ee9cf896a5c74f9ba057f89b8b6de5fa8a" translate="yes" xml:space="preserve">
          <source>The number of clusters per class.</source>
          <target state="translated">El número de grupos por clase.</target>
        </trans-unit>
        <trans-unit id="21f23a23e06d5c295fade98ab37ab55d16e621ca" translate="yes" xml:space="preserve">
          <source>The number of clusters to find.</source>
          <target state="translated">El número de grupos a encontrar.</target>
        </trans-unit>
        <trans-unit id="703c8a00edcb110a0abc7bd2f2de73653cb3b8bc" translate="yes" xml:space="preserve">
          <source>The number of clusters to find. It must be &lt;code&gt;None&lt;/code&gt; if &lt;code&gt;distance_threshold&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4184f0399110f5f8e690a26513dd6f78511c0a5" translate="yes" xml:space="preserve">
          <source>The number of clusters to form as well as the number of centroids to generate.</source>
          <target state="translated">El número de cúmulos a formar así como el número de centroides a generar.</target>
        </trans-unit>
        <trans-unit id="35d84f4a157603ef94bd9baafa0d524311104205" translate="yes" xml:space="preserve">
          <source>The number of columns in the grid plot (default: 3).</source>
          <target state="translated">El número de columnas en el gráfico de la cuadrícula (por defecto:3).</target>
        </trans-unit>
        <trans-unit id="1ab9cd0e34ad15344a0623bebcdec3c277a5d196" translate="yes" xml:space="preserve">
          <source>The number of components. It is same as the &lt;code&gt;n_components&lt;/code&gt; parameter if it was given. Otherwise, it will be same as the number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25441c707266953707d0c752071de32d0051d235" translate="yes" xml:space="preserve">
          <source>The number of connected components in the graph.</source>
          <target state="translated">El número de componentes conectados en el gráfico.</target>
        </trans-unit>
        <trans-unit id="5d17146f816382e55c9752b437d1e0a90ca8e256" translate="yes" xml:space="preserve">
          <source>The number of cross-validation splits (folds/iterations).</source>
          <target state="translated">El número de divisiones de validación cruzada (pliegues/iteraciones).</target>
        </trans-unit>
        <trans-unit id="4e96a6f0802b675664ce18cac028e5815cdfcfc0" translate="yes" xml:space="preserve">
          <source>The number of data features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa32f92ae0454e6e9ee52207fe93d7eb6600c995" translate="yes" xml:space="preserve">
          <source>The number of degrees of freedom of each components in the model.</source>
          <target state="translated">El número de grados de libertad de cada componente del modelo.</target>
        </trans-unit>
        <trans-unit id="b2ef77453518fcb650d3ae2f90fcfda15611a36f" translate="yes" xml:space="preserve">
          <source>The number of duplicated features, drawn randomly from the informative and the redundant features.</source>
          <target state="translated">El número de características duplicadas,extraídas aleatoriamente de las características informativas y redundantes.</target>
        </trans-unit>
        <trans-unit id="81a86627bf928c8685cc6768d089212f83e7a3d6" translate="yes" xml:space="preserve">
          <source>The number of elements of the hyperparameter value. Defaults to 1, which corresponds to a scalar hyperparameter. n_elements &amp;gt; 1 corresponds to a hyperparameter which is vector-valued, such as, e.g., anisotropic length-scales.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3756e321fd8c5762ae8ac31516f2ecb21110717" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de puntos igualmente espaciados en la &lt;code&gt;grid&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1ea7fab9bca4ccb2b6b305c06e165e2e51c05101" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes of the plots, for each target feature.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b29bfd2ea8e3e1682dd4a79d1706f446dfb38a05" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes.</source>
          <target state="translated">El número de puntos igualmente espaciados en los ejes.</target>
        </trans-unit>
        <trans-unit id="6843d07b6a565b2c9c3cc44410aeae6ef1353131" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the grid, for each target feature.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2df871c8f4e92cb59ed15324ccda38fad42ea75" translate="yes" xml:space="preserve">
          <source>The number of estimators as selected by early stopping (if &lt;code&gt;n_iter_no_change&lt;/code&gt; is specified). Otherwise it is set to &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de estimadores seleccionados por la detenci&amp;oacute;n anticipada (si se especifica &lt;code&gt;n_iter_no_change&lt;/code&gt; ). De lo contrario, se establece en &lt;code&gt;n_estimators&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1c41aa32f1f0f6cf009b98065236eda5fafde67a" translate="yes" xml:space="preserve">
          <source>The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.</source>
          <target state="translated">El número de características (columnas)en las matrices de salida.Es probable que un pequeño número de características provoque colisiones de hash,pero un gran número provocará mayores dimensiones de coeficientes en los aprendices lineales.</target>
        </trans-unit>
        <trans-unit id="586a175a91db906a71d554d2394ac768c54a011d" translate="yes" xml:space="preserve">
          <source>The number of features for each sample.</source>
          <target state="translated">El número de características de cada muestra.</target>
        </trans-unit>
        <trans-unit id="7aa2f8ce84af9869f7a766d49383cc26f37d08c4" translate="yes" xml:space="preserve">
          <source>The number of features has to be &amp;gt;= 5.</source>
          <target state="translated">El n&amp;uacute;mero de funciones debe ser&amp;gt; = 5.</target>
        </trans-unit>
        <trans-unit id="129d21ac97bd672e4633231039dccb80b794a16c" translate="yes" xml:space="preserve">
          <source>The number of features to consider when looking for the best split:</source>
          <target state="translated">El número de características a considerar cuando se busca la mejor división:</target>
        </trans-unit>
        <trans-unit id="c3e18b2792aee2f260136dbfb16e08ac8ac5d9d0" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator ( without replacement by default, see &lt;code&gt;bootstrap_features&lt;/code&gt; for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bfa1c66757d06ac8d59fc4bf744ebce5057ba13" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator.</source>
          <target state="translated">El número de características a extraer de X para entrenar a cada estimador base.</target>
        </trans-unit>
        <trans-unit id="d51bb9401f1843316f34a353f5592cb098582ce3" translate="yes" xml:space="preserve">
          <source>The number of features to select. If &lt;code&gt;None&lt;/code&gt;, half of the features are selected.</source>
          <target state="translated">El n&amp;uacute;mero de funciones para seleccionar. Si es &lt;code&gt;None&lt;/code&gt; , se selecciona la mitad de las funciones.</target>
        </trans-unit>
        <trans-unit id="9a2e8b8c9f83e59315eadeb30286733009f73579" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred from the maximum column index occurring in any of the files.</source>
          <target state="translated">El número de características a utilizar.Si no hay ninguna,se deducirá del índice máximo de la columna que aparece en cualquiera de los archivos.</target>
        </trans-unit>
        <trans-unit id="98598b839c3ae52a8bef761a8c292e4971f87fa1" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. n_features is only required if &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed a non-default value.</source>
          <target state="translated">La cantidad de funciones que se utilizar&amp;aacute;n. Si es Ninguno, se deducir&amp;aacute;. Este argumento es &amp;uacute;til para cargar varios archivos que son subconjuntos de un conjunto de datos dividido m&amp;aacute;s grande: es posible que cada subconjunto no tenga ejemplos de todas las caracter&amp;iacute;sticas, por lo que la forma inferida puede variar de un segmento a otro. n_features solo es necesario si el &lt;code&gt;offset&lt;/code&gt; o la &lt;code&gt;length&lt;/code&gt; se pasan a un valor no predeterminado.</target>
        </trans-unit>
        <trans-unit id="3f0c441872fc889415f948d3a194dda00a747f75" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65e177587b4163893d6713a7981471e039d2ab1a" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="deca4fdfd1ca37b7e04bbcf3985c1b98fb8a3a95" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">El n&amp;uacute;mero de funciones cuando se realiza el &lt;code&gt;fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="000ec70bd77b0bf4abd3e711fc085eb14c1166a9" translate="yes" xml:space="preserve">
          <source>The number of features.</source>
          <target state="translated">El número de características.</target>
        </trans-unit>
        <trans-unit id="9910eba7c229f9255e2c9649c38205ffc855802c" translate="yes" xml:space="preserve">
          <source>The number of features. Should be at least 5.</source>
          <target state="translated">El número de características.Debería ser al menos 5.</target>
        </trans-unit>
        <trans-unit id="6b858bf37b4f474c4599d32f8587786621c82cda" translate="yes" xml:space="preserve">
          <source>The number of informative features, i.e., the number of features used to build the linear model used to generate the output.</source>
          <target state="translated">El número de características informativas,es decir,el número de características utilizadas para construir el modelo lineal utilizado para generar la salida.</target>
        </trans-unit>
        <trans-unit id="1a77292033054c608772a804dab5a2e1fdf27c5c" translate="yes" xml:space="preserve">
          <source>The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension &lt;code&gt;n_informative&lt;/code&gt;. For each cluster, informative features are drawn independently from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube.</source>
          <target state="translated">El n&amp;uacute;mero de caracter&amp;iacute;sticas informativas. Cada clase est&amp;aacute; compuesta por varios grupos gaussianos, cada uno ubicado alrededor de los v&amp;eacute;rtices de un hipercubo en un subespacio de dimensi&amp;oacute;n &lt;code&gt;n_informative&lt;/code&gt; . Para cada grupo, las caracter&amp;iacute;sticas informativas se extraen independientemente de N (0, 1) y luego se combinan linealmente al azar dentro de cada grupo para agregar covarianza. Luego, los grupos se colocan en los v&amp;eacute;rtices del hipercubo.</target>
        </trans-unit>
        <trans-unit id="8860e037fbe039fe78d5e3c48f8e1848feb12312" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The best results are kept.</source>
          <target state="translated">El número de inicializaciones a realizar.Los mejores resultados se mantienen.</target>
        </trans-unit>
        <trans-unit id="51a26d62cae82295bdfa00f11021a221252f4d93" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The result with the highest lower bound value on the likelihood is kept.</source>
          <target state="translated">El número de inicializaciones a realizar.Se mantiene el resultado con el valor límite inferior más alto de la probabilidad.</target>
        </trans-unit>
        <trans-unit id="38ff309a985e30ab2e29b9afc192f79c8b9a0c6a" translate="yes" xml:space="preserve">
          <source>The number of integer to sample.</source>
          <target state="translated">El número de entero a muestrear.</target>
        </trans-unit>
        <trans-unit id="a52a9529854ceea9a883fc2df829925e52c70f47" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before this call to partial_fit. This is optional: if no number is passed, the memory of the object is used.</source>
          <target state="translated">El número de iteración en los lotes de datos que se ha realizado antes de esta llamada a partial_fit.Esto es opcional:si no se pasa ningún número,se utiliza la memoria del objeto.</target>
        </trans-unit>
        <trans-unit id="99c30bd94e39fd4b9ed3f7e2a7d5b85b28ed521a" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a86dc5cbf697553ae74ea1f2c0f3574967cc15f4" translate="yes" xml:space="preserve">
          <source>The number of iterations as selected by early stopping, depending on the &lt;code&gt;early_stopping&lt;/code&gt; parameter. Otherwise it corresponds to max_iter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3bc7b28196b3922c89415e845096f6cf78b8faf" translate="yes" xml:space="preserve">
          <source>The number of iterations corresponding to the best stress. Returned only if &lt;code&gt;return_n_iter&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de iteraciones correspondientes a la mejor tensi&amp;oacute;n. Se devuelve solo si &lt;code&gt;return_n_iter&lt;/code&gt; se establece en &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="09e334ba47a4a0e3959de08638739eb9eaaab635" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by lars_path to find the grid of alphas for each target.</source>
          <target state="translated">El número de iteraciones tomadas por lars_path para encontrar la cuadrícula de alfas para cada objetivo.</target>
        </trans-unit>
        <trans-unit id="b1098a1922ae37c291de7345b0094c64c3a02834" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha.</source>
          <target state="translated">El número de iteraciones tomadas por el optimizador de descenso de coordenadas para alcanzar la tolerancia especificada para cada alfa.</target>
        </trans-unit>
        <trans-unit id="cb53eef7959113120386cfa7066166d0b269478f" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when &lt;code&gt;return_n_iter&lt;/code&gt; is set to True).</source>
          <target state="translated">El n&amp;uacute;mero de iteraciones tomadas por el optimizador de descenso de coordenadas para alcanzar la tolerancia especificada para cada alfa. (Se devuelve cuando &lt;code&gt;return_n_iter&lt;/code&gt; se establece en True).</target>
        </trans-unit>
        <trans-unit id="33de6e55bb60c2bc5ac457a31b105deefc3f996b" translate="yes" xml:space="preserve">
          <source>The number of iterations the solver has ran.</source>
          <target state="translated">El número de iteraciones que ha realizado el solucionador.</target>
        </trans-unit>
        <trans-unit id="28e3bc9f7f876eba064c747240fb8d2ff652b8df" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel all &lt;code&gt;estimators&lt;/code&gt;&lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;code&gt;joblib.parallel_backend&lt;/code&gt; context. -1 means using all processors. See Glossary for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2412be87a40770d353886ae29c8c16a95b728197" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbb843cd2b1d5904ac077f6964fae31def7713e0" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt; of all &lt;code&gt;estimators&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;code&gt;joblib.parallel_backend&lt;/code&gt; context. -1 means using all processors. See Glossary for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db80deec4964c14c90bbb2ba0d38dab8d92c99f4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se ejecutar&amp;aacute;n en paralelo para &lt;code&gt;fit&lt;/code&gt; . &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="f140b3227d03089cb6e6210a5db41bedf6b9aa63" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9c5c499bb768493660dc4f85003a1f155b88c8c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e98b60a4f63df4b45688240803363e4326e4c3c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="631e50edfdf734f4505b382652be3db79f846fe9" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;a href=&quot;#sklearn.ensemble.IsolationForest.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.IsolationForest.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="828e81cc3e32985aa18f25c0aa8cb224a18c0ff7" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se ejecutar&amp;aacute;n en paralelo tanto para &lt;code&gt;fit&lt;/code&gt; como para &lt;code&gt;predict&lt;/code&gt; . &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="c43ba6eb14ab669b0479493b6caf6c135c124b5c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None`&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se ejecutar&amp;aacute;n en paralelo tanto para &lt;code&gt;fit&lt;/code&gt; como para &lt;code&gt;predict&lt;/code&gt; . &lt;code&gt;None`&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="b1734bf8c02c7376c103570d8af07531d90ec5dd" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30ff6a93f265905272a4a92b5eefd941a956fb0c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8618a7401b1bd61324e6d152687e714d9dfac99" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c242a688459caf97557bbb76035f6cc722015df4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91298d7ca6e21328dab1603b8e897f672cc46e7d" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c706846846b90bc0cd14952c05d5f24ee8d014f" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n para el c&amp;aacute;lculo. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="2f8f980d4703b1ec3df64fe85f466c6c6170bcab" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6833984bce80d83c32632eedfcb38fdab54d1b8b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. If multiple initializations are used (&lt;code&gt;n_init&lt;/code&gt;), each run of the algorithm is computed in parallel.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n para el c&amp;aacute;lculo. Si se utilizan m&amp;uacute;ltiples inicializaciones ( &lt;code&gt;n_init&lt;/code&gt; ), cada ejecuci&amp;oacute;n del algoritmo se calcula en paralelo.</target>
        </trans-unit>
        <trans-unit id="6691b8acedaea4810fafdb230140a5bee73c0f13" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n para el c&amp;aacute;lculo. Hace cada variable de destino en y en paralelo. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="be06d9a31cbe7a4bf21b6f4456cf1df2e9b8ee45" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3d27815195706836eb73a8c598c8129b01e46ca" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n para el c&amp;aacute;lculo. Esto solo proporcionar&amp;aacute; una aceleraci&amp;oacute;n para n_targets&amp;gt; 1 y suficientes problemas grandes. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="ba376dc0fa282138bebfa832fa05fd19bdc385f3" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aea0fe9213de8cdb23ff0f2fe182f38ade8bf1a8" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.</source>
          <target state="translated">El número de trabajos a utilizar para el cómputo.Esto funciona descomponiendo la matriz por pares en n_trabajos incluso rebanadas y calculándolas en paralelo.</target>
        </trans-unit>
        <trans-unit id="355b7ccfb662137f73492d9f4ce45fbb16afbbbc" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.</source>
          <target state="translated">El número de trabajos a utilizar para el cómputo.Esto funciona computando cada uno de los n_init que se ejecutan en paralelo.</target>
        </trans-unit>
        <trans-unit id="a5a46de70d97dcbce1cd57e62c5d0b48ff30934b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos que se utilizar&amp;aacute;n en el paso E. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="b51be92d99796ca039296711c164e0b4cf278e0a" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27ef8dc8b90e4adb5cf60c4b5861107cfc3fcd66" translate="yes" xml:space="preserve">
          <source>The number of leaves in the tree</source>
          <target state="translated">El número de hojas del árbol</target>
        </trans-unit>
        <trans-unit id="aa280d8fa86c4668dcae50e829dc6fe4b82c5c09" translate="yes" xml:space="preserve">
          <source>The number of longitudes (x) and latitudes (y) in the grid</source>
          <target state="translated">El número de longitudes (x)y latitudes (y)en la cuadrícula</target>
        </trans-unit>
        <trans-unit id="c1079f8f5554d74d12065ae30d0d0d5ad6da869f" translate="yes" xml:space="preserve">
          <source>The number of mixture components.</source>
          <target state="translated">El número de componentes de la mezcla.</target>
        </trans-unit>
        <trans-unit id="1b5111fc1060a675cec3a1f3743c5b7235e4d74d" translate="yes" xml:space="preserve">
          <source>The number of mixture components. Depending on the data and the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; the model can decide to not use all the components by setting some component &lt;code&gt;weights_&lt;/code&gt; to values very close to zero. The number of effective components is therefore smaller than n_components.</source>
          <target state="translated">El n&amp;uacute;mero de componentes de la mezcla. Dependiendo de los datos y del valor de &lt;code&gt;weight_concentration_prior&lt;/code&gt; , el modelo puede decidir no usar todos los componentes estableciendo algunos de los componentes &lt;code&gt;weights_&lt;/code&gt; en valores muy cercanos a cero. Por tanto, el n&amp;uacute;mero de componentes efectivos es menor que n_components.</target>
        </trans-unit>
        <trans-unit id="add0998b97eff8ce13b181824a749694c3eae657" translate="yes" xml:space="preserve">
          <source>The number of nearest neighbors to return</source>
          <target state="translated">El número de vecinos más cercanos a regresar</target>
        </trans-unit>
        <trans-unit id="15fc684195ef8537dde92583b88b14e2ce9227a5" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">El número de vecinos considerados (parámetro n_vecinos)se establece típicamente 1)mayor que el número mínimo de muestras que un cúmulo tiene que contener,de modo que otras muestras pueden ser valores atípicos locales en relación con este cúmulo,y 2)menor que el número máximo de muestras cercanas que pueden ser potencialmente valores atípicos locales.En la práctica,por lo general no se dispone de esa información,y la toma de n_vecinos=20 parece funcionar bien en general.</target>
        </trans-unit>
        <trans-unit id="acccc5b01db683b034e704a8a9a779e161564526" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered, (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">El número de vecinos considerados,(parámetro n_vecinos)se establece típicamente 1)mayor que el número mínimo de muestras que un cúmulo tiene que contener,de modo que otras muestras pueden ser valores atípicos locales en relación con este cúmulo,y 2)menor que el número máximo de muestras cercanas que pueden ser potencialmente valores atípicos locales.En la práctica,por lo general no se dispone de esa información,y la toma de n_vecinos=20 parece funcionar bien en general.</target>
        </trans-unit>
        <trans-unit id="877763fdbf37d108bc07acba2c3c7a43a6b1f5d0" translate="yes" xml:space="preserve">
          <source>The number of occurrences of each label in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de apariciones de cada etiqueta en &lt;code&gt;y_true&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d77e127c483a951faddd4898060a91624b32c7e2" translate="yes" xml:space="preserve">
          <source>The number of outlying points matters, but also how much they are outliers.</source>
          <target state="translated">El número de puntos periféricos es importante,pero también lo mucho que son periféricos.</target>
        </trans-unit>
        <trans-unit id="28c74611a0fbfe9d7c9bc15166aba9285108f840" translate="yes" xml:space="preserve">
          <source>The number of outputs when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">El n&amp;uacute;mero de salidas cuando se realiza el &lt;code&gt;fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="bb79176b795c17c39b28af054f09df09e008175b" translate="yes" xml:space="preserve">
          <source>The number of outputs.</source>
          <target state="translated">El número de salidas.</target>
        </trans-unit>
        <trans-unit id="b09418506b739dbda708239c423400be69d20b7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search.</source>
          <target state="translated">El número de trabajos paralelos para la búsqueda de vecinos.</target>
        </trans-unit>
        <trans-unit id="f8c9b6b47de0b026f28768b5a0afa5c012cbf5fc" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n para la b&amp;uacute;squeda de vecinos. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="7704c4671581e69d735cac67f4c8350e13fe7ef2" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Affects only &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; methods.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n para la b&amp;uacute;squeda de vecinos. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles. Afecta solo a los m&amp;eacute;todos &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;kneighbors_graph&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="361059c7cf2c1088b100ce86f03251d9b9b3606a" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n para la b&amp;uacute;squeda de vecinos. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles. No afecta el m&amp;eacute;todo de &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0d88911bb44ab8ec6ef4a44ece33bbbd3a1ce8ce" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n para la b&amp;uacute;squeda de vecinos. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles. No afecta el m&amp;eacute;todo de &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="1e03b6a0d6693562cf04d81d449658ec683196ea" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab2a0224fab84a593d45a145e0638371b6911fd8" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecd1b3d7da18bb6dc421488bb596e855ecb80e43" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f8cf8af9be5d62ff8935fbef652326c9adba1d1" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. If &lt;code&gt;-1&lt;/code&gt;, then the number of jobs is set to the number of CPU cores.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e352fe55bf07363046d5192af0f663c879039cfb" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. This parameter has no impact when &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; or (&lt;code&gt;metric=&quot;euclidean&quot;&lt;/code&gt; and &lt;code&gt;method=&quot;exact&quot;&lt;/code&gt;). &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92152098131975c56771bce4e909262b46d5eb7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">El n&amp;uacute;mero de trabajos paralelos que se ejecutar&amp;aacute;n. &lt;code&gt;None&lt;/code&gt; significa 1 a menos que est&amp;eacute; en un contexto &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;-1&lt;/code&gt; significa usar todos los procesadores. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;glosario&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="d31f80403a0bc612aeee728ef96a0071578ad09f" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99143dd67a79337f4ad9e3dbea2cd1e515a938df" translate="yes" xml:space="preserve">
          <source>The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21.</source>
          <target state="translated">El número de pasadas sobre los datos de entrenamiento (alias épocas).Por defecto,ninguno.Depreciado,se eliminará en 0.21.</target>
        </trans-unit>
        <trans-unit id="ec5cdfb884714451da411c4ff8cb1db87b4e7636" translate="yes" xml:space="preserve">
          <source>The number of redundant features. These features are generated as random linear combinations of the informative features.</source>
          <target state="translated">El número de características redundantes.Estos rasgos se generan como combinaciones lineales aleatorias de los rasgos informativos.</target>
        </trans-unit>
        <trans-unit id="47b1c5caf88dbd07fabbf6968de5281c23c7d24f" translate="yes" xml:space="preserve">
          <source>The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar.</source>
          <target state="translated">El número de objetivos de regresión,es decir,la dimensión del vector de salida y asociado a una muestra.Por defecto,la salida es un escalar.</target>
        </trans-unit>
        <trans-unit id="338ec305a58ac58159822d262713a3e274e16037" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed.</source>
          <target state="translated">El n&amp;uacute;mero de reinicios del optimizador para encontrar los par&amp;aacute;metros del kernel que maximizan la probabilidad log-marginal. La primera ejecuci&amp;oacute;n del optimizador se realiza a partir de los par&amp;aacute;metros iniciales del kernel, los restantes (si los hay) de thetas se muestrean log-uniform aleatoriamente desde el espacio de valores theta permitidos. Si es mayor que 0, todos los l&amp;iacute;mites deben ser finitos. Tenga en cuenta que n_restarts_optimizer == 0 implica que se realiza una ejecuci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="a4b3a5a4d2704e96fc904a7b55b3f1c5b2c8d507" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed.</source>
          <target state="translated">El n&amp;uacute;mero de reinicios del optimizador para encontrar los par&amp;aacute;metros del kernel que maximizan la probabilidad log-marginal. La primera ejecuci&amp;oacute;n del optimizador se realiza a partir de los par&amp;aacute;metros iniciales del kernel, los restantes (si los hay) de thetas se muestrean log-uniform aleatoriamente desde el espacio de valores theta permitidos. Si es mayor que 0, todos los l&amp;iacute;mites deben ser finitos. Tenga en cuenta que n_restarts_optimizer = 0 implica que se realiza una ejecuci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="8f64e7c256c29c0afec3f69dafbe77018e516c64" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters in the checkerboard structure.</source>
          <target state="translated">El número de grupos de filas y columnas en la estructura del tablero.</target>
        </trans-unit>
        <trans-unit id="1d47b9ac186fe69411b80e5cb5c0bc35de2b1552" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters.</source>
          <target state="translated">El número de grupos de filas y columnas.</target>
        </trans-unit>
        <trans-unit id="a93f4e954bb39f85a0cd6723eb5913eb20116e8e" translate="yes" xml:space="preserve">
          <source>The number of sample points on the S curve.</source>
          <target state="translated">El número de puntos de muestra en la curva S.</target>
        </trans-unit>
        <trans-unit id="e771006aa290a36177ef8a2fd15f85ed45a9f7ce" translate="yes" xml:space="preserve">
          <source>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.</source>
          <target state="translated">El número de muestras (o el peso total)en un barrio para que un punto sea considerado como un punto central.Esto incluye el punto en sí mismo.</target>
        </trans-unit>
        <trans-unit id="d8023265c3023688c589a292c9dfac5ffedb5a44" translate="yes" xml:space="preserve">
          <source>The number of samples drawn from the Gaussian process</source>
          <target state="translated">El número de muestras extraídas del proceso Gaussiano</target>
        </trans-unit>
        <trans-unit id="c514b9b07551e0f76131a511ab26abfabcd4aa8a" translate="yes" xml:space="preserve">
          <source>The number of samples in a neighborhood for a point to be considered as a core point. Also, up and down steep regions can&amp;rsquo;t have more then &lt;code&gt;min_samples&lt;/code&gt; consecutive non-steep points. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9e5e159d14938dc3482cf1b66194258f9217ba8" translate="yes" xml:space="preserve">
          <source>The number of samples in a neighborhood for a point to be considered as a core point. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1589955effed900fd5766b276491de7c2d2b9bf4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator for each feature. If there are not missing samples, the &lt;code&gt;n_samples_seen&lt;/code&gt; will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">El n&amp;uacute;mero de muestras procesadas por el estimador para cada caracter&amp;iacute;stica. Si no faltan muestras, &lt;code&gt;n_samples_seen&lt;/code&gt; ser&amp;aacute; un n&amp;uacute;mero entero, de lo contrario ser&amp;aacute; una matriz. Se restablecer&amp;aacute; en nuevas llamadas para adaptarse, pero se incrementar&amp;aacute; en las llamadas de ajuste &lt;code&gt;partial_fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8b8db8f325de293d35a9c6b4d95ec39fb2cca6ee" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. It will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5282c1e1c469ae21abb6fc766981198bf00b68c4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">El n&amp;uacute;mero de muestras procesadas por el estimador. Se restablecer&amp;aacute; en nuevas llamadas para adaptarse, pero se incrementar&amp;aacute; en las llamadas de ajuste &lt;code&gt;partial_fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="baac00734c9f0ef53943bb384323fdc39b43973f" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator (with replacement by default, see &lt;code&gt;bootstrap&lt;/code&gt; for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1793fd9997ff1f0552981d710a775a55fe6d48a0" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator.</source>
          <target state="translated">El número de muestras a extraer de X para entrenar a cada estimador de base.</target>
        </trans-unit>
        <trans-unit id="bb3a932b7568c921848c0d1cfe6540980845aa8f" translate="yes" xml:space="preserve">
          <source>The number of samples to take in each batch.</source>
          <target state="translated">El número de muestras a tomar en cada lote.</target>
        </trans-unit>
        <trans-unit id="45c4c8ec08a2e1d782bf77a47114ec6ebd4dca5e" translate="yes" xml:space="preserve">
          <source>The number of samples to use for each batch. Only used when calling &lt;code&gt;fit&lt;/code&gt;. If &lt;code&gt;batch_size&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, then &lt;code&gt;batch_size&lt;/code&gt; is inferred from the data and set to &lt;code&gt;5 * n_features&lt;/code&gt;, to provide a balance between approximation accuracy and memory consumption.</source>
          <target state="translated">El n&amp;uacute;mero de muestras que se utilizar&amp;aacute;n para cada lote. Solo se usa cuando se llama a &lt;code&gt;fit&lt;/code&gt; . Si &lt;code&gt;batch_size&lt;/code&gt; es &lt;code&gt;None&lt;/code&gt; , entonces &lt;code&gt;batch_size&lt;/code&gt; se infiere de los datos y se establece en &lt;code&gt;5 * n_features&lt;/code&gt; , para proporcionar un equilibrio entre la precisi&amp;oacute;n de aproximaci&amp;oacute;n y el consumo de memoria.</target>
        </trans-unit>
        <trans-unit id="cc9506bec678834c1e3b7d03354e1ff3a3f08ddc" translate="yes" xml:space="preserve">
          <source>The number of samples to use. If not given, all samples are used.</source>
          <target state="translated">El número de muestras a utilizar.Si no se da,se utilizan todas las muestras.</target>
        </trans-unit>
        <trans-unit id="06d71b7513bf6cfbd6babc125146f20e54cecd08" translate="yes" xml:space="preserve">
          <source>The number of samples.</source>
          <target state="translated">El número de muestras.</target>
        </trans-unit>
        <trans-unit id="8ed3bb3b4a6145be5f5af9755587163fa8bebbaa" translate="yes" xml:space="preserve">
          <source>The number of seconds contained in delta</source>
          <target state="translated">El número de segundos contenidos en el delta</target>
        </trans-unit>
        <trans-unit id="fd2b02981da97aea3b937c6f113a2aa5413af4a0" translate="yes" xml:space="preserve">
          <source>The number of selected features with cross-validation.</source>
          <target state="translated">El número de características seleccionadas con validación cruzada.</target>
        </trans-unit>
        <trans-unit id="f83194da71427b41aa36e9d801ef17814b93c795" translate="yes" xml:space="preserve">
          <source>The number of selected features.</source>
          <target state="translated">El número de características seleccionadas.</target>
        </trans-unit>
        <trans-unit id="ecf3b2d99c2ce29bc7f1b5f51d42517d75e68e85" translate="yes" xml:space="preserve">
          <source>The number of stages of the final model is available at the attribute &lt;code&gt;n_estimators_&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero de etapas del modelo final est&amp;aacute; disponible en el atributo &lt;code&gt;n_estimators_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d8f394636e1a93d63e3434a6391ec5ef2f73741a" translate="yes" xml:space="preserve">
          <source>The number of threads used by the OpenBLAS, MKL or BLIS libraries can be set via the &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt;, &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt;, and &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt; environment variables.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40db2965d8f58e28ab3da3567d512db3201d5fa4" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed.</source>
          <target state="translated">El número de veces que se refina la red.No se usa si se pasan valores explícitos de alfas.</target>
        </trans-unit>
        <trans-unit id="642e04b02615d230b9669f1186145c403a47fbce" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed. Range is [1, inf).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="089ef760cb35bf7968ad0395345a0886662c0be1" translate="yes" xml:space="preserve">
          <source>The number of tree that are built at each iteration. For regressors, this is always 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20cb5259cf581cee4993b651401d828750438d9c" translate="yes" xml:space="preserve">
          <source>The number of tree that are built at each iteration. This is equal to 1 for binary classification, and to &lt;code&gt;n_classes&lt;/code&gt; for multiclass classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b5e4a652e0296cb387fe06bf82a965799e670b1" translate="yes" xml:space="preserve">
          <source>The number of trees in the forest.</source>
          <target state="translated">El número de árboles en el bosque.</target>
        </trans-unit>
        <trans-unit id="b609a37dc2d7220a5b1e0ac4ad20a19f7617a90e" translate="yes" xml:space="preserve">
          <source>The number of weak learners (i.e. regression trees) is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;; &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;The size of each tree&lt;/a&gt; can be controlled either by setting the tree depth via &lt;code&gt;max_depth&lt;/code&gt; or by setting the number of leaf nodes via &lt;code&gt;max_leaf_nodes&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;shrinkage&lt;/a&gt; .</source>
          <target state="translated">El n&amp;uacute;mero de alumnos d&amp;eacute;biles (es decir, &amp;aacute;rboles de regresi&amp;oacute;n) se controla mediante el par&amp;aacute;metro &lt;code&gt;n_estimators&lt;/code&gt; ; &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;El tama&amp;ntilde;o de cada &amp;aacute;rbol&lt;/a&gt; se puede controlar configurando la profundidad del &amp;aacute;rbol a trav&amp;eacute;s de &lt;code&gt;max_depth&lt;/code&gt; o configurando el n&amp;uacute;mero de nodos de hojas a trav&amp;eacute;s de &lt;code&gt;max_leaf_nodes&lt;/code&gt; . El &lt;code&gt;learning_rate&lt;/code&gt; es un par&amp;aacute;metro hiper en el intervalo (0,0, 1,0] que los controles overfitting a trav&amp;eacute;s de &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;la contracci&amp;oacute;n&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="68e68bbcb31faf3cd13be5ec90f20eecf860f6d7" translate="yes" xml:space="preserve">
          <source>The number of weak learners is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the &lt;code&gt;base_estimator&lt;/code&gt; parameter. The main parameters to tune to obtain good results are &lt;code&gt;n_estimators&lt;/code&gt; and the complexity of the base estimators (e.g., its depth &lt;code&gt;max_depth&lt;/code&gt; or minimum required number of samples to consider a split &lt;code&gt;min_samples_split&lt;/code&gt;).</source>
          <target state="translated">El n&amp;uacute;mero de estudiantes d&amp;eacute;biles se controla mediante el par&amp;aacute;metro &lt;code&gt;n_estimators&lt;/code&gt; . El par&amp;aacute;metro &lt;code&gt;learning_rate&lt;/code&gt; controla la contribuci&amp;oacute;n de los estudiantes d&amp;eacute;biles en la combinaci&amp;oacute;n final. De forma predeterminada, los estudiantes d&amp;eacute;biles son tocones de decisi&amp;oacute;n. Se pueden especificar diferentes alumnos d&amp;eacute;biles mediante el par&amp;aacute;metro &lt;code&gt;base_estimator&lt;/code&gt; . Los principales par&amp;aacute;metros a ajustar para obtener buenos resultados son &lt;code&gt;n_estimators&lt;/code&gt; y la complejidad de los estimadores base (por ejemplo, su profundidad &lt;code&gt;max_depth&lt;/code&gt; o el n&amp;uacute;mero m&amp;iacute;nimo requerido de muestras para considerar una divisi&amp;oacute;n &lt;code&gt;min_samples_split&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="a6d1422bf72f5a61faa255a9a1207169e4a394c6" translate="yes" xml:space="preserve">
          <source>The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets.</source>
          <target state="translated">El objeto resuelve el mismo problema que el objeto LassoCV.Sin embargo,a diferencia del LassoCV,encuentra los valores alfa relevantes por sí mismo.En general,debido a esta propiedad,será más estable.Sin embargo,es más frágil a los conjuntos de datos fuertemente multicolineales.</target>
        </trans-unit>
        <trans-unit id="909e016dde1f323cbfe3daf2401dd2bee2829e0c" translate="yes" xml:space="preserve">
          <source>The object to use to fit the data.</source>
          <target state="translated">El objeto a utilizar para ajustar los datos.</target>
        </trans-unit>
        <trans-unit id="d5c8a5f63b0e142ed66ba0f0aa5318c460719fdf" translate="yes" xml:space="preserve">
          <source>The object&amp;rsquo;s &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; attributes store the best mean score and the parameters setting corresponding to that score:</source>
          <target state="translated">Los atributos &lt;code&gt;best_score_&lt;/code&gt; y &lt;code&gt;best_params_&lt;/code&gt; del objeto almacenan la mejor puntuaci&amp;oacute;n media y la configuraci&amp;oacute;n de los par&amp;aacute;metros correspondientes a esa puntuaci&amp;oacute;n:</target>
        </trans-unit>
        <trans-unit id="512dd720938772db73af76fb4221b6596459608c" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H.</source>
          <target state="translated">La función objetivo se minimiza con una minimización alterna de W y H.</target>
        </trans-unit>
        <trans-unit id="f16ae566357b7a1e9f3616f2cbcfd46a6904b9f5" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H. If H is given and update_H=False, it solves for W only.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a139874cf1d9e2c13462d6567d8563d658f0907b" translate="yes" xml:space="preserve">
          <source>The objective function is:</source>
          <target state="translated">La función del objetivo es:</target>
        </trans-unit>
        <trans-unit id="bf56422fecab64934517c1fdfbcaed66ab27df08" translate="yes" xml:space="preserve">
          <source>The objective function to minimize is in this case</source>
          <target state="translated">La función objetiva a minimizar es en este caso</target>
        </trans-unit>
        <trans-unit id="e546a5e9110cb4077ff5ab03d80b93cb6429070c" translate="yes" xml:space="preserve">
          <source>The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.</source>
          <target state="translated">Se supone que las observaciones están causadas por una transformación lineal de factores latentes de menor dimensión y ruido gaussiano añadido.Sin pérdida de generalidad los factores se distribuyen según un Gaussiano con media cero y covarianza unitaria.El ruido también es de media cero y tiene una matriz de covarianza diagonal arbitraria.</target>
        </trans-unit>
        <trans-unit id="f8b5be5464139b25f15f97e4d9d483501d55af35" translate="yes" xml:space="preserve">
          <source>The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="translated">Las observaciones a agrupar.Hay que señalar que los datos se convertirán en orden C,lo que provocará una copia en memoria si los datos dados no son C-contiguos.</target>
        </trans-unit>
        <trans-unit id="5af240b8e218ff237309779b56f83d5e52d1af7b" translate="yes" xml:space="preserve">
          <source>The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.</source>
          <target state="translated">Las observaciones,las distancias de Mahalanobis de las que calculamos.Se supone que las observaciones se extraen de la misma distribución que los datos utilizados en el ajuste.</target>
        </trans-unit>
        <trans-unit id="eb71736c8c9acd5b1d75a4e7144f466f3b859a53" translate="yes" xml:space="preserve">
          <source>The obtained score is always strictly greater than 0 and the best value is 1.</source>
          <target state="translated">La puntuación obtenida es siempre estrictamente superior a 0 y el mejor valor es 1.</target>
        </trans-unit>
        <trans-unit id="b7dea734e0307eec19c3b5341e6f81839af6082a" translate="yes" xml:space="preserve">
          <source>The one-vs-the-rest meta-classifier also implements a &lt;code&gt;predict_proba&lt;/code&gt; method, so long as such a method is implemented by the base classifier. This method returns probabilities of class membership in both the single label and multilabel case. Note that in the multilabel case, probabilities are the marginal probability that a given sample falls in the given class. As such, in the multilabel case the sum of these probabilities over all possible labels for a given sample &lt;em&gt;will not&lt;/em&gt; sum to unity, as they do in the single label case.</source>
          <target state="translated">El &lt;code&gt;predict_proba&lt;/code&gt; uno contra el resto tambi&amp;eacute;n implementa un m&amp;eacute;todo predict_proba , siempre que dicho m&amp;eacute;todo sea implementado por el clasificador base. Este m&amp;eacute;todo devuelve probabilidades de pertenencia a una clase tanto en el caso de etiqueta &amp;uacute;nica como en el de m&amp;uacute;ltiples etiquetas. Tenga en cuenta que en el caso de varias etiquetas, las probabilidades son la probabilidad marginal de que una muestra dada caiga en la clase dada. Como tal, en el caso de m&amp;uacute;ltiples etiquetas, la suma de estas probabilidades sobre todas las etiquetas posibles para una muestra dada &lt;em&gt;no&lt;/em&gt; sumar&amp;aacute; la unidad, como ocurre en el caso de una sola etiqueta.</target>
        </trans-unit>
        <trans-unit id="f36ee9570dbac199195d25256879fa51a751bbaa" translate="yes" xml:space="preserve">
          <source>The opposite LOF of the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (&lt;code&gt;negative_outlier_factor_&lt;/code&gt; close to -1), while outliers tend to have a larger LOF score.</source>
          <target state="translated">El LOF opuesto de las muestras de entrenamiento. Cuanto m&amp;aacute;s alto, m&amp;aacute;s normal. Inliers tienden a tener una puntuaci&amp;oacute;n de LOF cerca de 1 ( &lt;code&gt;negative_outlier_factor_&lt;/code&gt; cerca de -1), mientras que los valores at&amp;iacute;picos tienden a tener una puntuaci&amp;oacute;n LOF m&amp;aacute;s grande.</target>
        </trans-unit>
        <trans-unit id="df3e454f1090a47509e70dbea510891595829b28" translate="yes" xml:space="preserve">
          <source>The opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal.</source>
          <target state="translated">Lo opuesto al Factor Local Atípico de cada muestra de entrada.Cuanto más bajo,más anormal.</target>
        </trans-unit>
        <trans-unit id="4312ae849a138f7db034f6fd7f5a1ea0b817b171" translate="yes" xml:space="preserve">
          <source>The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:</source>
          <target state="translated">El algoritmo óptimo para un determinado conjunto de datos es una elección complicada,y depende de una serie de factores:</target>
        </trans-unit>
        <trans-unit id="e994897b226c2495f8cea3f1e46f2c7029c61954" translate="yes" xml:space="preserve">
          <source>The optimal lambda parameter for minimizing skewness is estimated on each feature independently using maximum likelihood.</source>
          <target state="translated">El parámetro lambda óptimo para reducir al mínimo la asimetría se estima en cada característica de forma independiente utilizando la máxima probabilidad.</target>
        </trans-unit>
        <trans-unit id="033713ef73311880bab79fd88513749d67a56824" translate="yes" xml:space="preserve">
          <source>The optimization objective for Lasso is:</source>
          <target state="translated">El objetivo de optimización para Lasso es:</target>
        </trans-unit>
        <trans-unit id="64214421bf615c105d2891f743437d06253a6ade" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskElasticNet is:</source>
          <target state="translated">El objetivo de optimización de la MultiTaskElasticNet es:</target>
        </trans-unit>
        <trans-unit id="f43132c3f7097ed8020d37840c051e421e41e300" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskLasso is:</source>
          <target state="translated">El objetivo de optimización de la MultiTaskLasso es:</target>
        </trans-unit>
        <trans-unit id="3bee4ea6ed3cad366ecf4d67dfc05469de43ad46" translate="yes" xml:space="preserve">
          <source>The optimization objective for the case method=&amp;rsquo;lasso&amp;rsquo; is:</source>
          <target state="translated">El objetivo de optimizaci&amp;oacute;n para el m&amp;eacute;todo de caso = 'lazo' es:</target>
        </trans-unit>
        <trans-unit id="8d89fe15a9f2092d4f8a7ef569d775bf0279d26d" translate="yes" xml:space="preserve">
          <source>The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:</source>
          <target state="translated">El argumento extra opcional se añadirá al mensaje de depreciación y a la cadena de documentación.Nota:para usar esto con el valor por defecto de extra,pon un vacío de paréntesis:</target>
        </trans-unit>
        <trans-unit id="0c86ba504c90ec1412c0ccc3d7f0b2f074294dc1" translate="yes" xml:space="preserve">
          <source>The optional parameter &lt;code&gt;whiten=True&lt;/code&gt; makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.</source>
          <target state="translated">El par&amp;aacute;metro opcional &lt;code&gt;whiten=True&lt;/code&gt; hace posible proyectar los datos en el espacio singular mientras escala cada componente a la varianza de la unidad. Esto suele ser &amp;uacute;til si los modelos posteriores hacen suposiciones s&amp;oacute;lidas sobre la isotrop&amp;iacute;a de la se&amp;ntilde;al: este es, por ejemplo, el caso de las m&amp;aacute;quinas de vectores de soporte con el kernel RBF y el algoritmo de agrupaci&amp;oacute;n de K-Means.</target>
        </trans-unit>
        <trans-unit id="c77f2b2e59e77aff75fd77a2825914acd9eb1242" translate="yes" xml:space="preserve">
          <source>The order in which the features will be imputed. Possible values:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="846ba284e005e0c78a1fd443a812172705bbc8f3" translate="yes" xml:space="preserve">
          <source>The order of labels in the classifier chain.</source>
          <target state="translated">El orden de las etiquetas en la cadena de clasificación.</target>
        </trans-unit>
        <trans-unit id="7b082acdda498538bd71b7e32b0b230c2144c284" translate="yes" xml:space="preserve">
          <source>The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.:</source>
          <target state="translated">El orden de la cadena puede establecerse explícitamente proporcionando una lista de números enteros.Por ejemplo,para una cadena de longitud 5.:</target>
        </trans-unit>
        <trans-unit id="f9d11a930ecaf4c38c05b12592342da86a8c77ff" translate="yes" xml:space="preserve">
          <source>The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the &lt;code&gt;transformers&lt;/code&gt; list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the &lt;code&gt;passthrough&lt;/code&gt; keyword. Those columns specified with &lt;code&gt;passthrough&lt;/code&gt; are added at the right to the output of the transformers.</source>
          <target state="translated">El orden de las columnas en la matriz de caracter&amp;iacute;sticas transformadas sigue el orden de c&amp;oacute;mo se especifican las columnas en la lista de &lt;code&gt;transformers&lt;/code&gt; . Las columnas de la matriz de caracter&amp;iacute;sticas original que no se especifican se eliminan de la matriz de caracter&amp;iacute;sticas transformada resultante, a menos que se especifiquen en la palabra clave de &lt;code&gt;passthrough&lt;/code&gt; . Las columnas especificadas con &lt;code&gt;passthrough&lt;/code&gt; se agregan a la derecha de la salida de los transformadores.</target>
        </trans-unit>
        <trans-unit id="0b5c0a29228cf2f99af9ecdff2f5b2a0dc08ed2d" translate="yes" xml:space="preserve">
          <source>The original data</source>
          <target state="translated">Los datos originales</target>
        </trans-unit>
        <trans-unit id="77422b337aacebf704cab947c2765e7ef78426db" translate="yes" xml:space="preserve">
          <source>The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.</source>
          <target state="translated">El conjunto de datos original consistía en 92 x 112,mientras que la versión disponible aquí consiste en imágenes de 64x64.</target>
        </trans-unit>
        <trans-unit id="11407c8a67a7b9eefbd2b65794ca1ecbfa48a97d" translate="yes" xml:space="preserve">
          <source>The original formulation of the hashing trick by Weinberger et al. used two separate hash functions \(h\) and \(\xi\) to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits.</source>
          <target state="translated">La formulación original del truco del hash de Weinberger y otros,usaba dos funciones de hash separadas \ ~-h y x para determinar el índice de la columna y el signo de una característica,respectivamente.La presente implementación funciona bajo el supuesto de que el bit de signo de MurmurHash3 es independiente de sus otros bits.</target>
        </trans-unit>
        <trans-unit id="62507dafeac3a5da5ac8979b39262545fa83f997" translate="yes" xml:space="preserve">
          <source>The original image data. For color images, the last dimension specifies the channel: a RGB image would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8eec938f1b6ddec4315f3fac08b8dc8e8e67d5d5" translate="yes" xml:space="preserve">
          <source>The original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 47.</source>
          <target state="translated">Las imágenes originales son de 250 x 250 píxeles,pero los argumentos de rebanar y redimensionar por defecto las reducen a 62 x 47.</target>
        </trans-unit>
        <trans-unit id="a555f40ab758aba55bcba7bb9ac36af67b065d6a" translate="yes" xml:space="preserve">
          <source>The other kernels</source>
          <target state="translated">Los otros núcleos</target>
        </trans-unit>
        <trans-unit id="5978e5bbc3d0aa56b4c3321d1d9ccbd8b149a1a9" translate="yes" xml:space="preserve">
          <source>The outer product of the row and column label vectors shows a representation of the checkerboard structure.</source>
          <target state="translated">El producto exterior de los vectores de las etiquetas de filas y columnas muestra una representación de la estructura del tablero de damas.</target>
        </trans-unit>
        <trans-unit id="4e7d9f946f101134a65fd7d38a82dce953516bd7" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">La salida &lt;code&gt;y&lt;/code&gt; se crea de acuerdo con la f&amp;oacute;rmula:</target>
        </trans-unit>
        <trans-unit id="7348f35a4e7d1a3450461b231ee28ef95517ba57" translate="yes" xml:space="preserve">
          <source>The output is generated by applying a (potentially biased) random linear regression model with &lt;code&gt;n_informative&lt;/code&gt; nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.</source>
          <target state="translated">La salida se genera aplicando un modelo de regresi&amp;oacute;n lineal aleatoria (potencialmente sesgado) con &lt;code&gt;n_informative&lt;/code&gt; regresores informativos distintos de cero a la entrada generada previamente y algo de ruido centrado en gauss con alguna escala ajustable.</target>
        </trans-unit>
        <trans-unit id="0c62eccd54e33fb989ef31175162303c11637d7c" translate="yes" xml:space="preserve">
          <source>The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If &lt;code&gt;flip_sign&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.</source>
          <target state="translated">La salida de una descomposici&amp;oacute;n de valor singular es &amp;uacute;nica hasta una permutaci&amp;oacute;n de los signos de los vectores singulares. Si &lt;code&gt;flip_sign&lt;/code&gt; se establece en &lt;code&gt;True&lt;/code&gt; , la ambig&amp;uuml;edad del signo se resuelve al hacer que las cargas m&amp;aacute;s grandes para cada componente en los vectores singulares de la izquierda sean positivas.</target>
        </trans-unit>
        <trans-unit id="ab5b2ab18fdef999b51de35c5e9c33e7d3ac7cc7" translate="yes" xml:space="preserve">
          <source>The output of the 3 models are combined in a 2D graph where nodes represents the stocks and edges the:</source>
          <target state="translated">La salida de los 3 modelos se combinan en un gráfico 2D donde los nodos representan las existencias y los bordes:</target>
        </trans-unit>
        <trans-unit id="c9b52f3c910ded5afcb915db265ed0583f446660" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to as the 1-of-K coding scheme.</source>
          <target state="translated">La salida de la transformación se conoce a veces como el esquema de codificación 1-de-K.</target>
        </trans-unit>
        <trans-unit id="f172299244e465e38a0859a0349f26df08f75701" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to by some authors as the 1-of-K coding scheme.</source>
          <target state="translated">Algunos autores se refieren a veces a la salida de la transformación como el esquema de codificación 1-de-K.</target>
        </trans-unit>
        <trans-unit id="58983c4b722f7fa5c156e992e47b9ca634d33156" translate="yes" xml:space="preserve">
          <source>The output values.</source>
          <target state="translated">Los valores de salida.</target>
        </trans-unit>
        <trans-unit id="f348d2a86dcf3bc20a82250a4d5068aa50c67aad" translate="yes" xml:space="preserve">
          <source>The overall complexity of Isomap is \(O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]\).</source>
          <target state="translated">La complejidad general del Isomap es \ ~ (O[D \log(k)N \log(N)]+O[N^2(k+\log(N))]+O[d N^2]\).</target>
        </trans-unit>
        <trans-unit id="9cec4ae56de9cf1cd9cf2f8a0ff2e2e24864f054" translate="yes" xml:space="preserve">
          <source>The overall complexity of MLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\).</source>
          <target state="translated">La complejidad general de MLLE es \N \N \N-O[D \N-N \N-N \N-N \N-O[D N k^3]+O[N (k-D)k^2]+O[d N^2]\N).</target>
        </trans-unit>
        <trans-unit id="7fa546000c6a79308906d0c040bf81047f6b149e" translate="yes" xml:space="preserve">
          <source>The overall complexity of spectral embedding is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">La complejidad general de la incrustación espectral es \ ~ (O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]\).</target>
        </trans-unit>
        <trans-unit id="808f28c633edaf7537ca8395b6bc52f0086ed496" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard HLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\).</source>
          <target state="translated">La complejidad general de la norma HLLE es \N(O[D \log(k)N \log(N)]+O[D N k^3]+O[N d^6]+O[d N^2]\N).</target>
        </trans-unit>
        <trans-unit id="63617858af3d41882021a1ab37e78e76cce39983" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">La complejidad general de la norma LLE es \ ~ (O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]\).</target>
        </trans-unit>
        <trans-unit id="b88c53c3806a592f7e00e19aef010a599cfaf4dc" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LTSA is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]\).</source>
          <target state="translated">La complejidad general del estándar LTSA es \ ~ (O[D \log(k)N \log(N)]+O[D N k^3]+O[k^2 d]+O[d N^2]\).</target>
        </trans-unit>
        <trans-unit id="fc12a97c8a559f9672542a072947ef2eae0adcac" translate="yes" xml:space="preserve">
          <source>The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:</source>
          <target state="translated">El valor p,que se aproxima a la probabilidad de que la puntuación se obtenga por casualidad.Se calcula como:</target>
        </trans-unit>
        <trans-unit id="7f9aaf22278cea2b541fbd8b88b09de54aad3e99" translate="yes" xml:space="preserve">
          <source>The parallel version of K-Means is broken on OS X when &lt;code&gt;numpy&lt;/code&gt; uses the &lt;code&gt;Accelerate&lt;/code&gt; Framework. This is expected behavior: &lt;code&gt;Accelerate&lt;/code&gt; can be called after a fork but you need to execv the subprocess with the Python binary (which multiprocessing does not do under posix).</source>
          <target state="translated">La versi&amp;oacute;n paralela de K-Means se rompe en OS X cuando &lt;code&gt;numpy&lt;/code&gt; usa &lt;code&gt;Accelerate&lt;/code&gt; Framework. Este es el comportamiento esperado: se puede llamar a &lt;code&gt;Accelerate&lt;/code&gt; despu&amp;eacute;s de una bifurcaci&amp;oacute;n, pero es necesario ejecutar el subproceso con el binario de Python (lo que no ocurre con el multiprocesamiento en posix).</target>
        </trans-unit>
        <trans-unit id="6d1486e3a09b61694eecc888bffbc6ccf12d8263" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf&quot; id=&quot;id20&quot;&gt;[HTF]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id21&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26cab4ba2ce5f7f1aa5cff1bedcda88264af63ea" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="translated">El par&amp;aacute;metro &lt;code&gt;learning_rate&lt;/code&gt; interact&amp;uacute;a fuertemente con el par&amp;aacute;metro &lt;code&gt;n_estimators&lt;/code&gt; , el n&amp;uacute;mero de estudiantes d&amp;eacute;biles que encajar. Los valores m&amp;aacute;s peque&amp;ntilde;os de &lt;code&gt;learning_rate&lt;/code&gt; requieren un mayor n&amp;uacute;mero de estudiantes d&amp;eacute;biles para mantener un error de entrenamiento constante. La evidencia emp&amp;iacute;rica sugiere que valores peque&amp;ntilde;os de tasa de &lt;code&gt;learning_rate&lt;/code&gt; favorecen un mejor error de prueba. &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; recomienda establecer la tasa de aprendizaje en una peque&amp;ntilde;a constante (por ejemplo, &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt; ) y elegir &lt;code&gt;n_estimators&lt;/code&gt; mediante una parada temprana. Para una discusi&amp;oacute;n m&amp;aacute;s detallada de la interacci&amp;oacute;n entre &lt;code&gt;learning_rate&lt;/code&gt; y &lt;code&gt;n_estimators&lt;/code&gt; , consulte &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0e6ab9c66156a9d2feae55ed2060a2c6f7d6986c" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;memory&lt;/code&gt; is needed in order to cache the transformers. &lt;code&gt;memory&lt;/code&gt; can be either a string containing the directory where to cache the transformers or a &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; object:</source>
          <target state="translated">La &lt;code&gt;memory&lt;/code&gt; par&amp;aacute;metros es necesaria para almacenar en cach&amp;eacute; los transformadores. &lt;code&gt;memory&lt;/code&gt; puede ser una cadena que contenga el directorio donde almacenar en cach&amp;eacute; los transformadores o un objeto &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="171048c7012440ddda965396141222fe52434637" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;normalize&lt;/code&gt; allows to report ratios instead of counts. The confusion matrix can be normalized in 3 different ways: &lt;code&gt;'pred'&lt;/code&gt;, &lt;code&gt;'true'&lt;/code&gt;, and &lt;code&gt;'all'&lt;/code&gt; which will divide the counts by the sum of each columns, rows, or the entire matrix, respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a19846a3c7376460662acabedd23579aba492f87" translate="yes" xml:space="preserve">
          <source>The parameter \(\nu\) is also called the &lt;strong&gt;learning rate&lt;/strong&gt; because it scales the step length the gradient descent procedure; it can be set via the &lt;code&gt;learning_rate&lt;/code&gt; parameter.</source>
          <target state="translated">El par&amp;aacute;metro \ (\ nu \) tambi&amp;eacute;n se denomina &lt;strong&gt;tasa de aprendizaje&lt;/strong&gt; porque escala la longitud del paso del procedimiento de descenso de gradiente; se puede configurar mediante el par&amp;aacute;metro &lt;code&gt;learning_rate&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="76b14eb14f0e0be16176b9f0182e58f523e33b2d" translate="yes" xml:space="preserve">
          <source>The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers.</source>
          <target state="translated">El parámetro épsilon controla el número de muestras que deben clasificarse como valores atípicos.Cuanto más pequeña es la épsilon,más robusta es para los valores atípicos.</target>
        </trans-unit>
        <trans-unit id="33a23a95b6f6e50b11cce3982dee22dc3afbeaef" translate="yes" xml:space="preserve">
          <source>The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.</source>
          <target state="translated">La cuadrícula de parámetros a explorar,como un diccionario que mapea los parámetros del estimador a secuencias de valores permitidos.</target>
        </trans-unit>
        <trans-unit id="95ad4f92539995843fbdd8a8c8263fdd9652cae5" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. More specifically, the optimization objective is:</source>
          <target state="translated">El parámetro l1_ratio corresponde a alpha en el paquete glmnet R,mientras que alpha corresponde al parámetro lambda en glmnet.Más específicamente,el objetivo de optimización es:</target>
        </trans-unit>
        <trans-unit id="444f64a3bffea2bee3d3973fdc03c3dbbbbf54d9" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio &amp;lt;= 0.01 is not reliable, unless you supply your own sequence of alpha.</source>
          <target state="translated">El par&amp;aacute;metro l1_ratio corresponde a alpha en el paquete glmnet R mientras que alpha corresponde al par&amp;aacute;metro lambda en glmnet. Espec&amp;iacute;ficamente, l1_ratio = 1 es la penalizaci&amp;oacute;n del lazo. Actualmente, l1_ratio &amp;lt;= 0.01 no es confiable, a menos que proporcione su propia secuencia de alfa.</target>
        </trans-unit>
        <trans-unit id="3d75707f4ee017e7f35982b539c7d0e6825a3d30" translate="yes" xml:space="preserve">
          <source>The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized.</source>
          <target state="translated">El parámetro que controla la suavidad de la función aprendida.Cuanto más pequeño es nu,menos suave es la función aproximada.Para nu=inf,el núcleo se convierte en equivalente al núcleo RBF y para nu=0,5 al núcleo exponencial absoluto.Los valores intermedios importantes son nu=1,5 (funciones una vez diferenciables)y nu=2,5 (funciones dos veces diferenciables).Obsérvese que los valores de nu que no están en [0,5,1,5,2,5,inf]incurren en un coste computacional considerablemente mayor (aprox.10 veces más alto)ya que requieren evaluar la función de Bessel modificada.Además,a diferencia de l,nu se mantiene fijo en su valor inicial y no se optimiza.</target>
        </trans-unit>
        <trans-unit id="fa3c09390eafe53f0b58881f0d7db646d5796fe2" translate="yes" xml:space="preserve">
          <source>The parameters \(\sigma_y\) and \(\mu_y\) are estimated using maximum likelihood.</source>
          <target state="translated">Los parámetros \N -sigma_y\N-y \N -más_menos_y\N-se estiman usando la máxima probabilidad.</target>
        </trans-unit>
        <trans-unit id="d8334dfb20de589f58500a915aef89a4fab7377d" translate="yes" xml:space="preserve">
          <source>The parameters \(\theta_y\) is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:</source>
          <target state="translated">Los parámetros se estiman mediante una versión suavizada de la máxima probabilidad,es decir,el recuento de la frecuencia relativa:</target>
        </trans-unit>
        <trans-unit id="c10d9b59570fee3848efafd20062e78feab2baf1" translate="yes" xml:space="preserve">
          <source>The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model, the regularization parameters \(\alpha\) and \(\lambda\) being estimated by maximizing the &lt;em&gt;log marginal likelihood&lt;/em&gt;. The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters \(\alpha\) and \(\lambda\) is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters &lt;code&gt;alpha_init&lt;/code&gt; and &lt;code&gt;lambda_init&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="162d2ed9f70c881f87134a87c0c741cb23832c22" translate="yes" xml:space="preserve">
          <source>The parameters implementation of the &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">La implementaci&amp;oacute;n de par&amp;aacute;metros de la clase &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; &lt;/a&gt; propone dos tipos de prior para la distribuci&amp;oacute;n de pesos: un modelo de mezcla finita con distribuci&amp;oacute;n de Dirichlet y un modelo de mezcla infinita con el Proceso de Dirichlet. En la pr&amp;aacute;ctica, el algoritmo de inferencia del proceso de Dirichlet es aproximado y utiliza una distribuci&amp;oacute;n truncada con un n&amp;uacute;mero m&amp;aacute;ximo fijo de componentes (denominada representaci&amp;oacute;n de ruptura de palos). El n&amp;uacute;mero de componentes realmente utilizados casi siempre depende de los datos.</target>
        </trans-unit>
        <trans-unit id="957eda9a79d6f1f87ea7b634983b57c94627c96b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.</source>
          <target state="translated">Los parámetros del estimador utilizado para aplicar estos métodos se optimizan mediante una búsqueda cruzada de cuadrículas validadas sobre una cuadrícula de parámetros.</target>
        </trans-unit>
        <trans-unit id="6f20d8d61d83d6376fa9caf869f91f4640e0cc5b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.</source>
          <target state="translated">Los parámetros del estimador utilizado para aplicar estos métodos se optimizan mediante una búsqueda validada cruzada sobre los ajustes de los parámetros.</target>
        </trans-unit>
        <trans-unit id="1d4a34fa151b21edbbd9fa634476deabc1cb44cf" translate="yes" xml:space="preserve">
          <source>The parameters of the power transformation for the selected features.</source>
          <target state="translated">Los parámetros de la transformación de la energía para las características seleccionadas.</target>
        </trans-unit>
        <trans-unit id="62f294aa209942b08fddf4e74d29c13f78a9e67d" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter.</source>
          <target state="translated">Los parámetros seleccionados son los que maximizan la puntuación de los datos retenidos,según el parámetro de puntuación.</target>
        </trans-unit>
        <trans-unit id="b6d7555a36c15ae2ffe9751024a0eebcf2421d57" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead.</source>
          <target state="translated">Los parámetros seleccionados son los que maximizan el puntaje de los datos omitidos,a menos que se pase un puntaje explícito,en cuyo caso se utiliza en su lugar.</target>
        </trans-unit>
        <trans-unit id="34d594dbc00b197d06cf788b61a5dfe36db335bf" translate="yes" xml:space="preserve">
          <source>The parameters that have been evaluated.</source>
          <target state="translated">Los parámetros que han sido evaluados.</target>
        </trans-unit>
        <trans-unit id="f139c5090bf57fe74c58422c7266093265927a67" translate="yes" xml:space="preserve">
          <source>The parent of each node. Only returned when a connectivity matrix is specified, elsewhere &amp;lsquo;None&amp;rsquo; is returned.</source>
          <target state="translated">El padre de cada nodo. Solo se devuelve cuando se especifica una matriz de conectividad, en otros lugares se devuelve 'Ninguno'.</target>
        </trans-unit>
        <trans-unit id="491910df08517ded9cf6622a10e93f79d8d7b51e" translate="yes" xml:space="preserve">
          <source>The partial depdendence curves can be plotted for the multi-layer perceptron. In this case, &lt;code&gt;line_kw&lt;/code&gt; is passed to &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to change the color of the curve.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89bfd729c83f5c14808628511ada19e6e2b220e5" translate="yes" xml:space="preserve">
          <source>The partial dependence function evaluated on the &lt;code&gt;grid&lt;/code&gt;. For regression and binary classification &lt;code&gt;n_classes==1&lt;/code&gt;.</source>
          <target state="translated">La funci&amp;oacute;n de dependencia parcial evaluada en la &lt;code&gt;grid&lt;/code&gt; . Para regresi&amp;oacute;n y clasificaci&amp;oacute;n binaria &lt;code&gt;n_classes==1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="95b5d0f6fa140b6ee1f70868d5a14424c22e2d4c" translate="yes" xml:space="preserve">
          <source>The partial dependence of the response \(f\) at a point \(x_S\) is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="053dbcb3edc2e230ef9a4eff0313b65bae4689ca" translate="yes" xml:space="preserve">
          <source>The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">Los algoritmos pasivo-agresivos son una familia de algoritmos para el aprendizaje a gran escala. Son similares al Perceptron en que no requieren una tasa de aprendizaje. Sin embargo, contrariamente a la Perceptron, incluyen un par&amp;aacute;metro de regularizaci&amp;oacute;n &lt;code&gt;C&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cf6ba8c5f2e782ae102fc993ff0f91af87853f26" translate="yes" xml:space="preserve">
          <source>The path of the base directory to use as a data store or None. If None is given, no caching is done and the Memory object is completely transparent. This option replaces cachedir since version 0.12.</source>
          <target state="translated">La ruta del directorio base para usar como almacén de datos o ninguno.Si se da None,no se hace ningún caching y el objeto de la memoria es completamente transparente.Esta opción reemplaza al cacheo desde la versión 0.12.</target>
        </trans-unit>
        <trans-unit id="e60c1638cc188f171e30720c51f9233ac8e0591d" translate="yes" xml:space="preserve">
          <source>The path to scikit-learn data dir.</source>
          <target state="translated">El camino a la dirección de datos de aprendizaje científico.</target>
        </trans-unit>
        <trans-unit id="d92bdbe34c5616d0c8598712ff33d12e5d0daade" translate="yes" xml:space="preserve">
          <source>The path to the location of the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2f9a75941863d91af30a0efa7c5bf671f8aabb3" translate="yes" xml:space="preserve">
          <source>The path to the location of the target.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e58c61969eaf91a150b8c119238c04ee82b41f7" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="780a6be1d44fa6c4bc32e0e6a573d3b4ad24b942" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to &amp;lsquo;l2&amp;rsquo; which is the standard regularizer for linear SVM models. &amp;lsquo;l1&amp;rsquo; and &amp;lsquo;elasticnet&amp;rsquo; might bring sparsity to the model (feature selection) not achievable with &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="translated">La penalizaci&amp;oacute;n (tambi&amp;eacute;n conocida como t&amp;eacute;rmino de regularizaci&amp;oacute;n) que se utilizar&amp;aacute;. El valor predeterminado es 'l2', que es el regularizador est&amp;aacute;ndar para los modelos SVM lineales. 'l1' y 'elasticnet' pueden traer escasez al modelo (selecci&amp;oacute;n de caracter&amp;iacute;sticas) que no se puede lograr con 'l2'.</target>
        </trans-unit>
        <trans-unit id="5ec62ab0e251a48390ce125b16975a3fa4c7ca91" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to None.</source>
          <target state="translated">La pena (también conocida como término de regularización)que se utilizará.Por defecto,ninguna.</target>
        </trans-unit>
        <trans-unit id="3874ef3081cc70f2fd57e0725f8bff89bf9a04cd" translate="yes" xml:space="preserve">
          <source>The performance is may slightly worse for the randomized search, and is likely due to a noise effect and would not carry over to a held-out test set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="195880b4735384ca5f4a3196f2d3109be0fc5155" translate="yes" xml:space="preserve">
          <source>The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.</source>
          <target state="translated">El rendimiento es ligeramente peor para la búsqueda aleatoria,aunque lo más probable es que se trate de un efecto de ruido y no se trasladaría a un conjunto de pruebas de retención.</target>
        </trans-unit>
        <trans-unit id="66758ccad6c0faa66ee36e2739cca75d7cb80119" translate="yes" xml:space="preserve">
          <source>The performance measure reported by &lt;em&gt;k&lt;/em&gt;-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</source>
          <target state="translated">La medida de rendimiento informada por la validaci&amp;oacute;n cruzada de &lt;em&gt;k&lt;/em&gt; veces es el promedio de los valores calculados en el ciclo. Este enfoque puede ser costoso desde el punto de vista computacional, pero no desperdicia demasiados datos (como es el caso cuando se arregla un conjunto de validaci&amp;oacute;n arbitrario), lo cual es una gran ventaja en problemas como la inferencia inversa donde el n&amp;uacute;mero de muestras es muy peque&amp;ntilde;o.</target>
        </trans-unit>
        <trans-unit id="773c4b551d63eb1e8cc16b04d1476c48de12db0a" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b50299f6e3a476dff9e3b98d46cee16ed70fcdb" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="translated">Se compara el rendimiento de los algoritmos SAMME y SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; . SAMME.R usa las estimaciones de probabilidad para actualizar el modelo aditivo, mientras que SAMME usa solo las clasificaciones. Como ilustra el ejemplo, el algoritmo SAMME.R generalmente converge m&amp;aacute;s r&amp;aacute;pido que SAMME, logrando un error de prueba menor con menos iteraciones de impulso. El error de cada algoritmo en el conjunto de prueba despu&amp;eacute;s de cada iteraci&amp;oacute;n de refuerzo se muestra a la izquierda, el error de clasificaci&amp;oacute;n en el conjunto de prueba de cada &amp;aacute;rbol se muestra en el medio y el peso de refuerzo de cada &amp;aacute;rbol se muestra a la derecha. Todos los &amp;aacute;rboles tienen un peso de uno en el algoritmo SAMME.R y, por lo tanto, no se muestran.</target>
        </trans-unit>
        <trans-unit id="54c9475018dc0386efaf6207cbf2038791834d22" translate="yes" xml:space="preserve">
          <source>The performance of the models can be evaluated by their ability to yield well-calibrated predictions and a good ranking.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f1cd52ca3b14d7125644c3724ffbf78c03b2ab3" translate="yes" xml:space="preserve">
          <source>The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step.</source>
          <target state="translated">El rendimiento de los hiperparámetros seleccionados y del modelo entrenado se mide entonces en un conjunto de evaluación dedicado que no se utilizó durante el paso de selección del modelo.</target>
        </trans-unit>
        <trans-unit id="dd106efb0f7012bff421a0ad8f3697f8283515ed" translate="yes" xml:space="preserve">
          <source>The periodicity of the kernel.</source>
          <target state="translated">La periodicidad del núcleo.</target>
        </trans-unit>
        <trans-unit id="5dce1e0dbd7277c2f73689bcac70f69df5d2fc02" translate="yes" xml:space="preserve">
          <source>The perplexity is defined as \(k=2^{(S)}\) where \(S\) is the Shannon entropy of the conditional probability distribution. The perplexity of a \(k\)-sided die is \(k\), so that \(k\) is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.</source>
          <target state="translated">La perplejidad se define como \(k=2^{(S)}\)donde \N(S)es la entropía Shannon de la distribución de probabilidad condicional.La perplejidad de un dado con un lado es de tal manera que es efectivamente el número de vecinos más cercanos que t-SNE considera al generar las probabilidades condicionales.Las perplejidades más grandes conducen a más vecinos cercanos y menos sensibles a la estructura pequeña.Por el contrario,una perplejidad menor considera un número menor de vecinos,y por lo tanto ignora más información global a favor del vecindario local.A medida que los tamaños de los conjuntos de datos sean mayores,se requerirán más puntos para obtener una muestra razonable del vecindario local y,por lo tanto,es posible que se requieran perplejidades mayores.De manera similar,los conjuntos de datos más ruidosos requerirán mayores valores de perplejidad para abarcar suficientes vecinos locales para ver más allá del ruido de fondo.</target>
        </trans-unit>
        <trans-unit id="61a03c034b8bc8a8d92d85436ffe8ab0dc0dda5b" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significanlty different results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08299376158415917838ffdf1e208cdfe76446d4" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter.</source>
          <target state="translated">La perplejidad está relacionada con el número de vecinos más cercanos que se utiliza en otros múltiples algoritmos de aprendizaje.Los conjuntos de datos más grandes generalmente requieren una mayor perplejidad.Considere la posibilidad de seleccionar un valor entre 5 y 50.La elección no es extremadamente crítica ya que el t-SNE es bastante insensible a este parámetro.</target>
        </trans-unit>
        <trans-unit id="042f3b8255d615a1ffcd846018bae060090b690e" translate="yes" xml:space="preserve">
          <source>The physical location of boston csv dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0881df888192122fb3f0dbde6c8cd7ff6f0357c" translate="yes" xml:space="preserve">
          <source>The pipeline below extracts the subject and body from each post using &lt;code&gt;SubjectBodyExtractor&lt;/code&gt;, producing a (n_samples, 2) array. This array is then used to compute standard bag-of-words features for the subject and body as well as text length and number of sentences on the body, using &lt;code&gt;ColumnTransformer&lt;/code&gt;. We combine them, with weights, then train a classifier on the combined set of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d1257b149fa9d88b1966c8dbdc55bf21ea6a0db" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed.</source>
          <target state="translated">El marcador de posici&amp;oacute;n de los valores faltantes. Se &lt;code&gt;missing_values&lt;/code&gt; todas las apariciones de missing_values .</target>
        </trans-unit>
        <trans-unit id="9c95ae315d785709e445ae217c5567fb1ce65f07" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For missing values encoded as np.nan, use the string value &amp;ldquo;NaN&amp;rdquo;.</source>
          <target state="translated">El marcador de posici&amp;oacute;n de los valores faltantes. Se &lt;code&gt;missing_values&lt;/code&gt; todas las apariciones de missing_values . Para los valores faltantes codificados como np.nan, utilice el valor de cadena &quot;NaN&quot;.</target>
        </trans-unit>
        <trans-unit id="7d178852ef63f5a60aca24eb2ef7cd366b0501af" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For pandas&amp;rsquo; dataframes with nullable integer dtypes with missing values, &lt;code&gt;missing_values&lt;/code&gt; should be set to &lt;code&gt;np.nan&lt;/code&gt;, since &lt;code&gt;pd.NA&lt;/code&gt; will be converted to &lt;code&gt;np.nan&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="182f79373d80c85b96a18c9f29af644fec99f0de" translate="yes" xml:space="preserve">
          <source>The plot above tells us about dependencies between a specific feature and the target when all other features remain constant, i.e., &lt;strong&gt;conditional dependencies&lt;/strong&gt;. An increase of the AGE will induce a decrease of the WAGE when all other features remain constant. On the contrary, an increase of the EXPERIENCE will induce an increase of the WAGE when all other features remain constant. Also, AGE, EXPERIENCE and EDUCATION are the three variables that most influence the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50d8f581357d70f0923b3a8bf25734f11fbbdc88" translate="yes" xml:space="preserve">
          <source>The plot represents the learning curve of the classifier: the evolution of classification accuracy over the course of the mini-batches. Accuracy is measured on the first 1000 samples, held out as a validation set.</source>
          <target state="translated">El gráfico representa la curva de aprendizaje del clasificador:la evolución de la precisión de la clasificación a lo largo de los mini-lotes.La precisión se mide en las primeras 1000 muestras,que se mantienen como un conjunto de validación.</target>
        </trans-unit>
        <trans-unit id="2108780ce5e305bd6eefeccab3f1f12e712db7ba" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.</source>
          <target state="translated">La trama muestra los límites de decisión para el análisis discriminante lineal y el análisis discriminante cuadrático.La fila inferior demuestra que el Análisis Discriminante Lineal sólo puede aprender límites lineales,mientras que el Análisis Discriminante Cuadrático puede aprender límites cuadráticos y por lo tanto es más flexible.</target>
        </trans-unit>
        <trans-unit id="b078f7cb7ea97d6d1e43a3acd4d110579a0862d7" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17f022b1b904616a8feb650bfceadf60a7908c45" translate="yes" xml:space="preserve">
          <source>The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), avg. occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and avg. rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="translated">La gr&amp;aacute;fica muestra cuatro gr&amp;aacute;ficas de dependencia parcial unidireccionales y una bidireccional. Las variables objetivo para el PDP unidireccional son: ingreso medio ( &lt;code&gt;MedInc&lt;/code&gt; ), avg. ocupantes por hogar ( &lt;code&gt;AvgOccup&lt;/code&gt; ), edad promedio del hogar ( &lt;code&gt;HouseAge&lt;/code&gt; ) y promedio. habitaciones por hogar ( &lt;code&gt;AveRooms&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="d1f9d8f4302df476eee3089e3160330e3191c729" translate="yes" xml:space="preserve">
          <source>The plot shows the regions where the discretized encoding is constant.</source>
          <target state="translated">La trama muestra las regiones donde la codificación discretizada es constante.</target>
        </trans-unit>
        <trans-unit id="a1523290796b6a8ba4024eaf4b48817249c66e13" translate="yes" xml:space="preserve">
          <source>The plots below illustrate the effect the parameter &lt;code&gt;C&lt;/code&gt; has on the separation line. A large value of &lt;code&gt;C&lt;/code&gt; basically tells our model that we do not have that much faith in our data&amp;rsquo;s distribution, and will only consider points close to line of separation.</source>
          <target state="translated">Las gr&amp;aacute;ficas siguientes ilustran el efecto que tiene el par&amp;aacute;metro &lt;code&gt;C&lt;/code&gt; en la l&amp;iacute;nea de separaci&amp;oacute;n. Un gran valor de &lt;code&gt;C&lt;/code&gt; b&amp;aacute;sicamente le dice a nuestro modelo que no tenemos tanta fe en la distribuci&amp;oacute;n de nuestros datos y que solo consideraremos puntos cercanos a la l&amp;iacute;nea de separaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="faaabc25a5f9dd9badc9ef9f18d9770e507882de" translate="yes" xml:space="preserve">
          <source>The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.</source>
          <target state="translated">Los gráficos muestran en primer lugar lo que un algoritmo K-means produciría usando tres grupos.Luego se muestra cuál es el efecto de una mala inicialización en el proceso de clasificación:Al establecer n_init a sólo 1 (el valor por defecto es 10),se reduce la cantidad de veces que el algoritmo se ejecutará con diferentes semillas centroides.El siguiente gráfico muestra lo que el uso de ocho cúmulos podría entregar y finalmente la verdad del terreno.</target>
        </trans-unit>
        <trans-unit id="3ad47df403d87eb821e2edd090b9e74720bc0be8" translate="yes" xml:space="preserve">
          <source>The plots represent the distribution of the prediction latency as a boxplot.</source>
          <target state="translated">Las gráficas representan la distribución de la latencia de la predicción como una gráfica de caja.</target>
        </trans-unit>
        <trans-unit id="9d34744e2f178eb734dfa28bde9ee7e67cdcc28f" translate="yes" xml:space="preserve">
          <source>The plots show four 1-way and two 1-way partial dependence plots (omitted for &lt;a href=&quot;../../modules/generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt;&lt;code&gt;MLPRegressor&lt;/code&gt;&lt;/a&gt; due to computation time). The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), average occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and average rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee583f2ef106ae04159c6d135c18b8bf01049699" translate="yes" xml:space="preserve">
          <source>The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.</source>
          <target state="translated">Las tramas muestran puntos de entrenamiento en colores sólidos y puntos de prueba semitransparentes.La parte inferior derecha muestra la precisión de la clasificación en el conjunto de pruebas.</target>
        </trans-unit>
        <trans-unit id="a10b5c6300ecb650c0782e5a6bff1ffc576100bb" translate="yes" xml:space="preserve">
          <source>The point cloud spanned by the observations above is very flat in one direction: one of the three univariate features can almost be exactly computed using the other two. PCA finds the directions in which the data is not &lt;em&gt;flat&lt;/em&gt;</source>
          <target state="translated">La nube de puntos abarcada por las observaciones anteriores es muy plana en una direcci&amp;oacute;n: una de las tres caracter&amp;iacute;sticas univariadas se puede calcular casi exactamente con las otras dos. PCA encuentra las direcciones en las que los datos no son &lt;em&gt;planos&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="9874b880b8ee39ca50e864dab01006b5213a2351" translate="yes" xml:space="preserve">
          <source>The points.</source>
          <target state="translated">Los puntos.</target>
        </trans-unit>
        <trans-unit id="0cdbbfd6a3924811880d5b83f6e26dafaaff0147" translate="yes" xml:space="preserve">
          <source>The polynomial kernel is defined as:</source>
          <target state="translated">El núcleo polinómico se define como:</target>
        </trans-unit>
        <trans-unit id="8b34b53d18875cd269c1341b177c3bcbb9230141" translate="yes" xml:space="preserve">
          <source>The pooled values for each feature cluster.</source>
          <target state="translated">Los valores agrupados para cada grupo de características.</target>
        </trans-unit>
        <trans-unit id="6285f4c6cbbb9a8676e04ca09291d710e5d4992f" translate="yes" xml:space="preserve">
          <source>The possible options are &amp;lsquo;hinge&amp;rsquo;, &amp;lsquo;log&amp;rsquo;, &amp;lsquo;modified_huber&amp;rsquo;, &amp;lsquo;squared_hinge&amp;rsquo;, &amp;lsquo;perceptron&amp;rsquo;, or a regression loss: &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;.</source>
          <target state="translated">Las opciones posibles son 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron' o una p&amp;eacute;rdida de regresi&amp;oacute;n: 'squared_loss', 'huber', 'epsilon_insensitive' o 'squared_epsilon_insensitive'.</target>
        </trans-unit>
        <trans-unit id="6a0542e47ae26fdcd6f04cfe1be082ea4e8e2319" translate="yes" xml:space="preserve">
          <source>The power determines the underlying target distribution according to the following table:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a756ae9d1d42224e6a27c29b135c093bd570bbe" translate="yes" xml:space="preserve">
          <source>The power of the Minkowski metric to be used to calculate distance between points.</source>
          <target state="translated">El poder de la métrica de Minkowski para ser usada para calcular la distancia entre puntos.</target>
        </trans-unit>
        <trans-unit id="581738f7dfe64a35233afe8207a1e82379bfb8cd" translate="yes" xml:space="preserve">
          <source>The power transform is useful as a transformation in modeling problems where homoscedasticity and normality are desired. Below are examples of Box-Cox and Yeo-Johnwon applied to six different probability distributions: Lognormal, Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.</source>
          <target state="translated">La transformación de potencia es útil como una transformación en los problemas de modelado donde se desea la homosexualidad y la normalidad.A continuación se presentan ejemplos de Box-Cox y Yeo-Johnwon aplicados a seis distribuciones de probabilidad diferentes:Lognormal,Chi-cuadrado,Weibull,Gaussiana,Uniforme y Bimodal.</target>
        </trans-unit>
        <trans-unit id="837e2e1ea652ea6696a91670a638bda69523a446" translate="yes" xml:space="preserve">
          <source>The power transform method. Available methods are:</source>
          <target state="translated">El método de transformación de energía.Los métodos disponibles son:</target>
        </trans-unit>
        <trans-unit id="1605256f2d1d73782f41770777e3757d50960cf1" translate="yes" xml:space="preserve">
          <source>The power transform method. Currently, &amp;lsquo;box-cox&amp;rsquo; (Box-Cox transform) is the only option available.</source>
          <target state="translated">El m&amp;eacute;todo de transformaci&amp;oacute;n de energ&amp;iacute;a. Actualmente, 'box-cox' (transformaci&amp;oacute;n de Box-Cox) es la &amp;uacute;nica opci&amp;oacute;n disponible.</target>
        </trans-unit>
        <trans-unit id="9e3ef4e072a07501cd2ec598f0a1011b6178f209" translate="yes" xml:space="preserve">
          <source>The precision is the ratio &lt;code&gt;tp / (tp + fp)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fp&lt;/code&gt; the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.</source>
          <target state="translated">La precisi&amp;oacute;n es la relaci&amp;oacute;n &lt;code&gt;tp / (tp + fp)&lt;/code&gt; donde &lt;code&gt;tp&lt;/code&gt; es el n&amp;uacute;mero de verdaderos positivos y &lt;code&gt;fp&lt;/code&gt; el n&amp;uacute;mero de falsos positivos. La precisi&amp;oacute;n es intuitivamente la capacidad del clasificador de no etiquetar como positiva una muestra que es negativa.</target>
        </trans-unit>
        <trans-unit id="5e318fd9419ebb0c7685cd2fcf271eb0864ec5bb" translate="yes" xml:space="preserve">
          <source>The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">Las matrices de precisi&amp;oacute;n para cada componente de la mezcla. Una matriz de precisi&amp;oacute;n es la inversa de una matriz de covarianza. Una matriz de covarianza es sim&amp;eacute;trica positiva definida, por lo que la mezcla de gaussiano puede parametrizarse de manera equivalente mediante las matrices de precisi&amp;oacute;n. El almacenamiento de las matrices de precisi&amp;oacute;n en lugar de las matrices de covarianza hace que sea m&amp;aacute;s eficiente calcular la probabilidad logar&amp;iacute;tmica de nuevas muestras en el momento de la prueba. La forma depende de &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="027e42693446de348f6d01928068eb7453ef8a97" translate="yes" xml:space="preserve">
          <source>The precision matrix associated to the current covariance object.</source>
          <target state="translated">La matriz de precisión asociada al objeto de covarianza actual.</target>
        </trans-unit>
        <trans-unit id="d78de957c617714f761992022534f1c5cbc37dc0" translate="yes" xml:space="preserve">
          <source>The precision of each components on the mean distribution (Gaussian).</source>
          <target state="translated">La precisión de cada componente en la distribución media (Gaussiana).</target>
        </trans-unit>
        <trans-unit id="208aaa08d6ec7d945a950063ea09933d0bd5ac66" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;.</source>
          <target state="translated">La precisi&amp;oacute;n antes de la distribuci&amp;oacute;n media (gaussiana). Controla la extensi&amp;oacute;n hasta donde se pueden colocar los medios. Los valores m&amp;aacute;s peque&amp;ntilde;os concentran las medias de cada grupo alrededor de &lt;code&gt;mean_prior&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b72296dd4015be0343e22e7dc45bdf0f7e18f582" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to 1.</source>
          <target state="translated">La precisi&amp;oacute;n antes de la distribuci&amp;oacute;n media (gaussiana). Controla la extensi&amp;oacute;n hasta donde se pueden colocar los medios. Los valores m&amp;aacute;s peque&amp;ntilde;os concentran las medias de cada grupo alrededor de &lt;code&gt;mean_prior&lt;/code&gt; . El valor del par&amp;aacute;metro debe ser mayor que 0. Si es Ninguno, se establece en 1.</target>
        </trans-unit>
        <trans-unit id="99b687233f264a106cecce1d5b60aee20c688782" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. Larger values concentrate the cluster means around &lt;code&gt;mean_prior&lt;/code&gt;. If mean_precision_prior is set to None, &lt;code&gt;mean_precision_prior_&lt;/code&gt; is set to 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88b322c6c16cb10f58188170d43f7d8832a9357f" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. Larger values concentrate the cluster means around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it is set to 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03d920e00078b8bd9b4faa0b1aa14a8c65b257bb" translate="yes" xml:space="preserve">
          <source>The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</source>
          <target state="translated">La curva de precisión-recordatorio muestra el equilibrio entre la precisión y el recordatorio para diferentes umbrales.Un área alta bajo la curva representa tanto una alta memoria como una alta precisión,donde la alta precisión se relaciona con una baja tasa de falsos positivos,y la alta memoria se relaciona con una baja tasa de falsos negativos.Las puntuaciones altas de ambas muestran que el clasificador está produciendo resultados precisos (alta precisión),así como la mayoría de los resultados positivos (alta recuperación).</target>
        </trans-unit>
        <trans-unit id="b03d17dd4f0a50b42b8760dd1442678e73495423" translate="yes" xml:space="preserve">
          <source>The predicted class C for each sample in X is returned.</source>
          <target state="translated">Se devuelve la clase C prevista para cada muestra en X.</target>
        </trans-unit>
        <trans-unit id="bfeb54e7bff0bb15dd098a7327cddb86a1801899" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the base estimators in the ensemble.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como el registro de la media de las probabilidades de clase pronosticadas de los estimadores de base en el conjunto.</target>
        </trans-unit>
        <trans-unit id="cd67555dc49cf41a5e5df91097299e3752772d2d" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como el tronco de la media de las probabilidades de clase pronosticadas de los árboles del bosque.</target>
        </trans-unit>
        <trans-unit id="eab6e37cbb9069ff9afad75097cea6fc5d34926b" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.</source>
          <target state="translated">Las probabilidades de registro de clase pronosticadas de una muestra de entrada se calculan como la media ponderada de las probabilidades de registro de clase pronosticadas de los clasificadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="47d729548b0a8e226d9e4c530c0297fbe0f5665f" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.</source>
          <target state="translated">La clase prevista de una muestra de entrada es un voto de los árboles del bosque,ponderado por sus estimaciones de probabilidad.Es decir,la clase pronosticada es la que tiene la estimación de probabilidad media más alta entre los árboles.</target>
        </trans-unit>
        <trans-unit id="228bf14c189aaaaad2dc0e65c7c1dff58773904b" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting.</source>
          <target state="translated">La clase predicha de una muestra de entrada se calcula como la clase con la probabilidad media predicha m&amp;aacute;s alta. Si los estimadores de base no implementan un m&amp;eacute;todo &lt;code&gt;predict_proba&lt;/code&gt; , entonces recurre a la votaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="8652ca51db9ef5a2b15410ff134f217292f6ef55" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.</source>
          <target state="translated">La clase prevista de una muestra de entrada se calcula como la predicción media ponderada de los clasificadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="46c6bfcd5571fcbc8ac1e94c5ec5ef7b9ca3bcfc" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como la media de las probabilidades de clase pronosticadas de los árboles del bosque.La probabilidad de clase de un solo árbol es la fracción de muestras de la misma clase en una hoja.</target>
        </trans-unit>
        <trans-unit id="90e3cd15b2277da446a86ed04d9b97d9385dbcbd" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the base estimators in the ensemble. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting and the predicted class probabilities of an input sample represents the proportion of estimators predicting each class.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como las probabilidades de clase pronosticadas medias de los estimadores base en el conjunto. Si los estimadores base no implementan un m&amp;eacute;todo &lt;code&gt;predict_proba&lt;/code&gt; , entonces se recurre a la votaci&amp;oacute;n y las probabilidades de clase predichas de una muestra de entrada representan la proporci&amp;oacute;n de estimadores que predicen cada clase.</target>
        </trans-unit>
        <trans-unit id="f43014d849d28fe556560f6e74f971c02171e223" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.</source>
          <target state="translated">Las probabilidades de clase pronosticadas de una muestra de entrada se calculan como la media ponderada de las probabilidades de clase pronosticadas de los clasificadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="232fb255d370f3424586a7b0c3f74d049bd6d607" translate="yes" xml:space="preserve">
          <source>The predicted class probability is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">La probabilidad de clase pronosticada es la fracción de muestras de la misma clase en una hoja.</target>
        </trans-unit>
        <trans-unit id="45d2cef0bd7682bfccc693d1957f2c12cdb9bae8" translate="yes" xml:space="preserve">
          <source>The predicted class.</source>
          <target state="translated">La clase prevista.</target>
        </trans-unit>
        <trans-unit id="d01b8d940e0e34c3c7942798bc90fe03e260970d" translate="yes" xml:space="preserve">
          <source>The predicted classes, or the predict values.</source>
          <target state="translated">Las clases predichas,o los valores predichos.</target>
        </trans-unit>
        <trans-unit id="b68f3b27cbad35418e1a35aab9bbe1837a195e37" translate="yes" xml:space="preserve">
          <source>The predicted classes.</source>
          <target state="translated">Las clases previstas.</target>
        </trans-unit>
        <trans-unit id="f9c849804a5f2e4c77a6bd9f1a72aeb60a174b11" translate="yes" xml:space="preserve">
          <source>The predicted log-probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;. Equivalent to log(predict_proba(X))</source>
          <target state="translated">El logaritmo de probabilidad predicho de la muestra para cada clase en el modelo, donde las clases se ordenan como en &lt;code&gt;self.classes_&lt;/code&gt; . Equivalente a log (predict_proba (X))</target>
        </trans-unit>
        <trans-unit id="b73f981e520b253c83fbe81831bba280a3db569a" translate="yes" xml:space="preserve">
          <source>The predicted probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;.</source>
          <target state="translated">La probabilidad predicha de la muestra para cada clase en el modelo, donde las clases se ordenan como en &lt;code&gt;self.classes_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a286cd68d524dde2fbaba23c990f981ec35b78f3" translate="yes" xml:space="preserve">
          <source>The predicted probas.</source>
          <target state="translated">Las probas predichas.</target>
        </trans-unit>
        <trans-unit id="53b10e885ebd4a72b834dc0bd6649d47d09469b6" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.</source>
          <target state="translated">El objetivo de regresión previsto de una muestra de entrada se calcula como la media de los objetivos de regresión previstos de los estimadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="d575d8b045eb46db33f9cbaec364a954b218830a" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.</source>
          <target state="translated">El objetivo de regresión previsto de una muestra de entrada se calcula como la media de los objetivos de regresión previstos de los árboles del bosque.</target>
        </trans-unit>
        <trans-unit id="21875214f30fbe829fb7e391b984beb01fcc0748" translate="yes" xml:space="preserve">
          <source>The predicted regression value of an input sample is computed as the weighted median prediction of the classifiers in the ensemble.</source>
          <target state="translated">El valor de regresión previsto de una muestra de entrada se calcula como la predicción de la mediana ponderada de los clasificadores del conjunto.</target>
        </trans-unit>
        <trans-unit id="d728cdaebb39fe6a42651804a843b92d06b095fc" translate="yes" xml:space="preserve">
          <source>The predicted regression values.</source>
          <target state="translated">Los valores de regresión predichos.</target>
        </trans-unit>
        <trans-unit id="eb2e0fb384bae49f48977b71692091d27df9ee48" translate="yes" xml:space="preserve">
          <source>The predicted target values.</source>
          <target state="translated">Los valores previstos del objetivo.</target>
        </trans-unit>
        <trans-unit id="16c2ec05bdd825f5e946bffd1661391a4efc1866" translate="yes" xml:space="preserve">
          <source>The predicted value of the input samples.</source>
          <target state="translated">El valor predicho de las muestras de entrada.</target>
        </trans-unit>
        <trans-unit id="71f7a8826bad533ae16d312cbce730009c206751" translate="yes" xml:space="preserve">
          <source>The predicted values.</source>
          <target state="translated">Los valores predichos.</target>
        </trans-unit>
        <trans-unit id="d3f70f498146a996702d8957eebe13ff93b2e60c" translate="yes" xml:space="preserve">
          <source>The prediction interpolates the observations (at least for regular kernels).</source>
          <target state="translated">La predicción interpola las observaciones (al menos para los núcleos regulares).</target>
        </trans-unit>
        <trans-unit id="0a5a460e70a5f18f6e563f520727c4e907b64c8a" translate="yes" xml:space="preserve">
          <source>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.</source>
          <target state="translated">La predicción es probabilística (gaussiana),de modo que se pueden calcular intervalos de confianza empíricos y decidir sobre la base de éstos si se debe reajustar (ajuste en línea,ajuste adaptativo)la predicción en alguna región de interés.</target>
        </trans-unit>
        <trans-unit id="888b741b5759a3600a521b02ebf591af7172583e" translate="yes" xml:space="preserve">
          <source>The prediction is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92250eb4257ffd9737b8e4be4f65018174b8c07a" translate="yes" xml:space="preserve">
          <source>The predictions for all the points in the grid, averaged over all samples in X (or over the training data if &lt;code&gt;method&lt;/code&gt; is &amp;lsquo;recursion&amp;rsquo;). &lt;code&gt;n_outputs&lt;/code&gt; corresponds to the number of classes in a multi-class setting, or to the number of tasks for multi-output regression. For classical regression and binary classification &lt;code&gt;n_outputs==1&lt;/code&gt;. &lt;code&gt;n_values_feature_j&lt;/code&gt; corresponds to the size &lt;code&gt;values[j]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f4304f9d52554d908e981e82f60b25cc189525a" translate="yes" xml:space="preserve">
          <source>The present version of SpectralClustering requires the number of clusters to be specified in advance. It works well for a small number of clusters, but is not advised for many clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22369282bb204be005e939800f6b06d8c430453c" translate="yes" xml:space="preserve">
          <source>The previously introduced metrics are &lt;strong&gt;not normalized with regards to random labeling&lt;/strong&gt;: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular &lt;strong&gt;random labeling won&amp;rsquo;t yield zero scores especially when the number of clusters is large&lt;/strong&gt;.</source>
          <target state="translated">Las m&amp;eacute;tricas introducidas anteriormente &lt;strong&gt;no&lt;/strong&gt; est&amp;aacute;n &lt;strong&gt;normalizadas con respecto al etiquetado aleatorio&lt;/strong&gt; : esto significa que, dependiendo del n&amp;uacute;mero de muestras, grupos y clases de verdad del terreno, un etiquetado completamente aleatorio no siempre producir&amp;aacute; los mismos valores de homogeneidad, integridad y, por lo tanto, medida v. En particular, &lt;strong&gt;el etiquetado aleatorio no arrojar&amp;aacute; puntajes cero, especialmente cuando el n&amp;uacute;mero de grupos es grande&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="b298c69bd549b9b894cc39801befe9de3ee0bc57" translate="yes" xml:space="preserve">
          <source>The primal problem can be equivalently formulated as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd0e1bbfb34ee27f92c4a3d8c327812d7edee1f8" translate="yes" xml:space="preserve">
          <source>The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as &lt;em&gt;non-generalizing&lt;/em&gt; machine learning methods, since they simply &amp;ldquo;remember&amp;rdquo; all of its training data (possibly transformed into a fast indexing structure such as a &lt;a href=&quot;#ball-tree&quot;&gt;Ball Tree&lt;/a&gt; or &lt;a href=&quot;#kd-tree&quot;&gt;KD Tree&lt;/a&gt;).</source>
          <target state="translated">El principio detr&amp;aacute;s de los m&amp;eacute;todos de vecino m&amp;aacute;s cercano es encontrar un n&amp;uacute;mero predefinido de muestras de entrenamiento m&amp;aacute;s cercanas en distancia al nuevo punto y predecir la etiqueta a partir de ellas. El n&amp;uacute;mero de muestras puede ser una constante definida por el usuario (aprendizaje del vecino m&amp;aacute;s cercano k) o variar seg&amp;uacute;n la densidad local de puntos (aprendizaje del vecino basado en el radio). En general, la distancia puede ser cualquier medida m&amp;eacute;trica: la distancia euclidiana est&amp;aacute;ndar es la opci&amp;oacute;n m&amp;aacute;s com&amp;uacute;n. Los m&amp;eacute;todos basados ​​en vecinos se conocen como m&amp;eacute;todos de aprendizaje autom&amp;aacute;tico &lt;em&gt;no generalizantes&lt;/em&gt; , ya que simplemente &quot;recuerdan&quot; todos sus datos de entrenamiento (posiblemente transformados en una estructura de indexaci&amp;oacute;n r&amp;aacute;pida como un &lt;a href=&quot;#ball-tree&quot;&gt;&amp;aacute;rbol de bolas&lt;/a&gt; o un &amp;aacute;rbol &lt;a href=&quot;#kd-tree&quot;&gt;KD&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="947ce8b8522668844673470c4e6efaad3bb4bafb" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel are shown in the following figure:</source>
          <target state="translated">El anterior y posterior de un GP resultante de un kernel &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt; se muestran en la siguiente figura:</target>
        </trans-unit>
        <trans-unit id="43f31a8e2502a5518bdd46434b1800e86463052e" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:</source>
          <target state="translated">El anterior y posterior de un GP resultante de un núcleo ExpSineSquared se muestran en la siguiente figura:</target>
        </trans-unit>
        <trans-unit id="f92d48b9507eee6a0b35b438f1fb3d6ff89e93fd" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart).</source>
          <target state="translated">El anterior del número de grados de libertad en las distribuciones de covarianza (Wishart).</target>
        </trans-unit>
        <trans-unit id="f920ca641ad93ad7a821ae4139b67430b9eddb8f" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart). If it is None, it&amp;rsquo;s set to &lt;code&gt;n_features&lt;/code&gt;.</source>
          <target state="translated">El anterior del n&amp;uacute;mero de grados de libertad en las distribuciones de covarianza (Wishart). Si es None, se establece en &lt;code&gt;n_features&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cf6f1da4c11f5b6aa97c72a194e67d10417600f3" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). If it is None, the emiprical covariance prior is initialized using the covariance of X. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">El prior sobre la distribuci&amp;oacute;n de covarianza (Wishart). Si es None, la covarianza emiprica a priori se inicializa usando la covarianza de X. La forma depende de &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="449bf6ea1f50ae651db1aabfda8187878d697851" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">El prior sobre la distribuci&amp;oacute;n de covarianza (Wishart). La forma depende de &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="a7d0d50e2fe2007735b69660533329d841055128" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian).</source>
          <target state="translated">El anterior sobre la distribución media (Gaussiano).</target>
        </trans-unit>
        <trans-unit id="330376751a07bab7a9ae7af5823384716a02e1f4" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it is set to the mean of X.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70a81456bc5946b8879a5b610e7810c1053668bd" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it&amp;rsquo;s set to the mean of X.</source>
          <target state="translated">El anterior sobre la distribuci&amp;oacute;n media (gaussiana). Si es Ninguno, se establece en la media de X.</target>
        </trans-unit>
        <trans-unit id="0c84abbb5dade5fc9d4b47758b34408cb97bc08f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian.</source>
          <target state="translated">Los priors sobre \ (\ alpha \) y \ (\ lambda \) se eligen para ser &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;distribuciones gamma&lt;/a&gt; , el conjugado previo para la precisi&amp;oacute;n del gaussiano.</target>
        </trans-unit>
        <trans-unit id="4f2acfb8ac15388ba72cc76c4859e9942703930f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian. The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69c3bd38e0a5cd7a1c6a9d7a1b4382f7891ca245" translate="yes" xml:space="preserve">
          <source>The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.</source>
          <target state="translated">El modelo de probabilidad se crea utilizando la validación cruzada,de modo que los resultados pueden ser ligeramente diferentes de los obtenidos por la predicción.Además,producirá resultados sin sentido en conjuntos de datos muy pequeños.</target>
        </trans-unit>
        <trans-unit id="a6cf9fb7eddd86de2e07f31468eef83947604765" translate="yes" xml:space="preserve">
          <source>The probability of category \(t\) in feature \(i\) given class \(c\) is estimated as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a547056b7368b638d698d05d3f24b68fc3e86df" translate="yes" xml:space="preserve">
          <source>The probability of each class being drawn. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">La probabilidad de que se extraiga cada clase. Solo se devuelve si &lt;code&gt;return_distributions=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="91c6b31a68e2490c2f85bd0b221deab07bc16086" translate="yes" xml:space="preserve">
          <source>The probability of each feature being drawn given each class. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">La probabilidad de que se dibuje cada caracter&amp;iacute;stica dada cada clase. Solo se devuelve si &lt;code&gt;return_distributions=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e0b4e863c306f0bcfef6f345210b44e3fb52ffa9" translate="yes" xml:space="preserve">
          <source>The probability that a coefficient is zero (see notes). Larger values enforce more sparsity.</source>
          <target state="translated">La probabilidad de que un coeficiente sea cero (ver notas).Los valores más grandes obligan a una mayor dispersión.</target>
        </trans-unit>
        <trans-unit id="f5e975aa46dbf2f661a2bf284eb823b950bfb65a" translate="yes" xml:space="preserve">
          <source>The problem of correlated variables</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4678dc803cddccad6a08944794f2a1c194908b2a" translate="yes" xml:space="preserve">
          <source>The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.</source>
          <target state="translated">Se sabe que el problema de aprender un árbol de decisiones óptimas es NP-completo bajo varios aspectos de la optimización e incluso para conceptos simples.Por consiguiente,los prácticos algoritmos de aprendizaje de árboles de decisión se basan en algoritmos heurísticos como el algoritmo de la codicia,en el que se toman decisiones óptimas a nivel local en cada nodo.Esos algoritmos no pueden garantizar la devolución del árbol de decisión globalmente óptimo.Esto puede mitigarse mediante la formación de múltiples árboles en un conjunto de aprendizaje,en el que las características y muestras se muestrean al azar con sustitución.</target>
        </trans-unit>
        <trans-unit id="6c819ed9fb97cd33099d0eff9842389e345641fd" translate="yes" xml:space="preserve">
          <source>The problem solved in clustering</source>
          <target state="translated">El problema resuelto en la agrupación</target>
        </trans-unit>
        <trans-unit id="2c5b556d82e8f03aa8505b7b204354187717fb43" translate="yes" xml:space="preserve">
          <source>The problem solved in supervised learning</source>
          <target state="translated">El problema resuelto en el aprendizaje supervisado</target>
        </trans-unit>
        <trans-unit id="ab484ff296e26d980b5f93762f848e40818065e8" translate="yes" xml:space="preserve">
          <source>The progress meter: the higher the value of &lt;code&gt;verbose&lt;/code&gt;, the more messages:</source>
          <target state="translated">El medidor de progreso: cuanto mayor sea el valor de &lt;code&gt;verbose&lt;/code&gt; , m&amp;aacute;s mensajes:</target>
        </trans-unit>
        <trans-unit id="4163ad4952f27df98667c245d9d8133cfc5f4bae" translate="yes" xml:space="preserve">
          <source>The project mailing list</source>
          <target state="translated">La lista de correo del proyecto</target>
        </trans-unit>
        <trans-unit id="c7689a2c8d3ddf3814b537c25de9417bcceff1dc" translate="yes" xml:space="preserve">
          <source>The projected data.</source>
          <target state="translated">Los datos proyectados.</target>
        </trans-unit>
        <trans-unit id="8bfacd813c2539118e4e65b3f249a31e3f90cba3" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;(n_sample + n_features + 1) / 2&lt;/code&gt;. The parameter must be in the range (0, 1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1179732ddee68f2a030de922cca4a7f3acc9e816" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: [n_sample + n_features + 1] / 2</source>
          <target state="translated">La proporción de puntos que se incluirán en el apoyo de la estimación bruta de MCD.El valor por defecto es Ninguno,lo que implica que se utilizará el valor mínimo de support_fraction dentro del algoritmo:n_muestra+n_características+1]/2</target>
        </trans-unit>
        <trans-unit id="5f706b185c5fa578abf1be5586afd62f084c2356" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;.</source>
          <target state="translated">La proporci&amp;oacute;n de puntos que se incluir&amp;aacute;n en el apoyo de la estimaci&amp;oacute;n bruta de MCD. Si es None, se utilizar&amp;aacute; el valor m&amp;iacute;nimo de support_fraction dentro del algoritmo: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2ecaf585d98315eb2e879bcbfec7a3d49600f7b8" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;. Range is (0, 1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd542c4196f5720eadadf90803a23b09f997f270" translate="yes" xml:space="preserve">
          <source>The proportion of samples whose class is the positive class, in each bin (fraction of positives).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6be8a7dea2b475115d00cec72a7cea2ea475dbdd" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;early_stopping&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5ef332af0dc668a636cc813bd9c0d699622c033" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;n_iter_no_change&lt;/code&gt; is set to an integer.</source>
          <target state="translated">La proporci&amp;oacute;n de datos de entrenamiento que se deben reservar como conjunto de validaci&amp;oacute;n para la detenci&amp;oacute;n anticipada. Debe estar entre 0 y 1. Solo se usa si &lt;code&gt;n_iter_no_change&lt;/code&gt; se establece en un n&amp;uacute;mero entero.</target>
        </trans-unit>
        <trans-unit id="e7040634736e41a179ebec81405480b75c6b1afc" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True</source>
          <target state="translated">La proporción de los datos de entrenamiento que se reservan como validación para la detención temprana.Debe estar entre 0 y 1.Sólo se utiliza si la parada temprana es verdadera</target>
        </trans-unit>
        <trans-unit id="22eea34be82cd504d8c4cf88134dfff15444db48" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.</source>
          <target state="translated">La proporción de los datos de entrenamiento que se reservan como validación para la detención temprana.Debe estar entre 0 y 1.Sólo se usa si la parada temprana es verdadera.</target>
        </trans-unit>
        <trans-unit id="3b21c544ac1d6b2edc3be7d45619235a28b43e4c" translate="yes" xml:space="preserve">
          <source>The proportions of samples assigned to each class. If None, then classes are balanced. Note that if &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt;, then the last class weight is automatically inferred. More than &lt;code&gt;n_samples&lt;/code&gt; samples may be returned if the sum of &lt;code&gt;weights&lt;/code&gt; exceeds 1.</source>
          <target state="translated">Las proporciones de muestras asignadas a cada clase. Si es Ninguno, las clases est&amp;aacute;n equilibradas. Tenga en cuenta que si &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt; , el &amp;uacute;ltimo peso de la clase se infiere autom&amp;aacute;ticamente. Se pueden devolver m&amp;aacute;s de &lt;code&gt;n_samples&lt;/code&gt; si la suma de &lt;code&gt;weights&lt;/code&gt; excede 1.</target>
        </trans-unit>
        <trans-unit id="e063e4585132e12e4e3b8bc3e0e5f8f9333735ee" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of &lt;code&gt;components_&lt;/code&gt;. It is the linear operator that maps independent sources to the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b5cae22ba8a7c5a2306b5698bd81974f62f5c35" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting it to &amp;lsquo;passthrough&amp;rsquo; or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="824b6a86b9524b4fdb9e2919749fc7db8e534162" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.</source>
          <target state="translated">El prop&amp;oacute;sito de la canalizaci&amp;oacute;n es reunir varios pasos que se pueden validar juntos mientras se establecen diferentes par&amp;aacute;metros. Para ello, permite configurar los par&amp;aacute;metros de los distintos pasos utilizando sus nombres y el nombre del par&amp;aacute;metro separados por un '__', como en el ejemplo siguiente. El estimador de un paso se puede reemplazar por completo configurando el par&amp;aacute;metro con su nombre en otro estimador, o un transformador eliminado estableciendo en Ninguno.</target>
        </trans-unit>
        <trans-unit id="7aabe2d97cf9256a5a6bd2e4c38d95ce8f939f1c" translate="yes" xml:space="preserve">
          <source>The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd8f7005fb1918c1319c0b334e68b25127992a8d" translate="yes" xml:space="preserve">
          <source>The python source code used to generate the model</source>
          <target state="translated">El código fuente de la pitón utilizado para generar el modelo</target>
        </trans-unit>
        <trans-unit id="a971c90794b3ab81f6079922073a4f8be4eaa91d" translate="yes" xml:space="preserve">
          <source>The qualitative difference between these models can also be visualized by comparing the histogram of observed target values with that of predicted values:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7347085b6a9f16c7df450dca9dbc878bc14f5cc" translate="yes" xml:space="preserve">
          <source>The quantile to predict using the &amp;ldquo;quantile&amp;rdquo; strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.</source>
          <target state="translated">El cuantil a predecir utilizando la estrategia de &quot;cuantiles&quot;. Un cuantil de 0.5 corresponde a la mediana, mientras que 0.0 al m&amp;iacute;nimo y 1.0 al m&amp;aacute;ximo.</target>
        </trans-unit>
        <trans-unit id="eabfb5a10c049cb3056c46f05114ad3228125c8e" translate="yes" xml:space="preserve">
          <source>The quantity \(\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}\) is the derivative of the loss with respect to its second parameter, evaluated at \(F_{m-1}(x)\). It is easy to compute for any given \(F_{m - 1}(x_i)\) in a closed form since the loss is differentiable. We will denote it by \(g_i\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38be089aaf53753add8a6e12d9d6e104537de3bf" translate="yes" xml:space="preserve">
          <source>The quantity that we use is the daily variation in quote price: quotes that are linked tend to cofluctuate during a day.</source>
          <target state="translated">La cantidad que utilizamos es la variación diaria del precio de la cotización:las cotizaciones que están vinculadas tienden a cofluir durante un día.</target>
        </trans-unit>
        <trans-unit id="96cf03fa14346bfc08bd6f97110fef23b56f72d1" translate="yes" xml:space="preserve">
          <source>The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.</source>
          <target state="translated">El punto o puntos de consulta.Si no se proporciona,se devuelven los vecinos de cada punto indexado.En este caso,el punto de consulta no se considera su propio vecino.</target>
        </trans-unit>
        <trans-unit id="30a7cdee7bb9697635b16e9d03b7cdd4b400cabd" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. the training samples.</source>
          <target state="translated">La muestra o muestras de consulta para calcular el Factor Atípico Local w.r.t.las muestras de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="e9edc4411fbea590103c5860156a749b07db92a2" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. to the training samples.</source>
          <target state="translated">La muestra o muestras de consulta para calcular el factor atípico local w.r.t.a las muestras de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="2f63e59d1f59e9a5989ccb3ba903c403d9c311f4" translate="yes" xml:space="preserve">
          <source>The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.</source>
          <target state="translated">El radio del subconjunto obtenido mediante la fusión de una nueva muestra y del subconjunto más cercano debe ser menor que el umbral.De lo contrario,se inicia un nuevo subclúster.Fijar este valor en muy bajo promueve la división y viceversa.</target>
        </trans-unit>
        <trans-unit id="c1fb03bbf68de1d40f0cd10af0722ed019408483" translate="yes" xml:space="preserve">
          <source>The random forest regressor will only ever predict values within the range of observations or closer to zero for each of the targets. As a result the predictions are biased towards the centre of the circle.</source>
          <target state="translated">El regresor forestal aleatorio sólo podrá predecir valores dentro del rango de observaciones o más cercanos a cero para cada uno de los objetivos.Como resultado,las predicciones están sesgadas hacia el centro del círculo.</target>
        </trans-unit>
        <trans-unit id="ff0bc6a79a727353502babbe6e55a993a0f80508" translate="yes" xml:space="preserve">
          <source>The random number generator is used to generate random chain orders.</source>
          <target state="translated">El generador de números aleatorios se utiliza para generar órdenes en cadena aleatorias.</target>
        </trans-unit>
        <trans-unit id="31617e37a4673ca35baf50a5963330e598289ccc" translate="yes" xml:space="preserve">
          <source>The random symmetric, positive-definite matrix.</source>
          <target state="translated">La matriz simétrica aleatoria,positiva-definida.</target>
        </trans-unit>
        <trans-unit id="0ae670050b82bcbccc27a159ae39c91afa76e218" translate="yes" xml:space="preserve">
          <source>The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.</source>
          <target state="translated">La búsqueda aleatoria y la búsqueda por cuadrículas exploran exactamente el mismo espacio de parámetros.El resultado en los ajustes de los parámetros es bastante similar,mientras que el tiempo de ejecución de la búsqueda aleatoria es drásticamente menor.</target>
        </trans-unit>
        <trans-unit id="6f118fa702c125f64290c65f38abc4f6dddff279" translate="yes" xml:space="preserve">
          <source>The raw (unadjusted) Rand index is then given by:</source>
          <target state="translated">El índice bruto (no ajustado)de Rand viene dado por:</target>
        </trans-unit>
        <trans-unit id="6b2fe9bd420d4a6948923a1a40e37c6224028547" translate="yes" xml:space="preserve">
          <source>The raw RI score is then &amp;ldquo;adjusted for chance&amp;rdquo; into the ARI score using the following scheme:</source>
          <target state="translated">Luego, la puntuaci&amp;oacute;n RI bruta se &quot;ajusta por azar&quot; en la puntuaci&amp;oacute;n ARI mediante el siguiente esquema:</target>
        </trans-unit>
        <trans-unit id="c2c7c9eff2c5366f7dc8bd26b18e5e786e16eeff" translate="yes" xml:space="preserve">
          <source>The raw image data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31fb3fd5063abf3dc0c0004645bb280ed7e9b6d1" translate="yes" xml:space="preserve">
          <source>The raw predicted values (i.e. the sum of the trees leaves) for each sample. n_trees_per_iteration is equal to the number of classes in multiclass classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="884b3f9d013732b636db9c9b452d7d3559d50f2d" translate="yes" xml:space="preserve">
          <source>The raw robust estimated covariance before correction and re-weighting.</source>
          <target state="translated">La robusta covarianza bruta estimada antes de la corrección y la reponderación.</target>
        </trans-unit>
        <trans-unit id="70b59f0ad9598471a02599d6a34b12e103ef557b" translate="yes" xml:space="preserve">
          <source>The raw robust estimated location before correction and re-weighting.</source>
          <target state="translated">La localización robusta bruta estimada antes de la corrección y la reponderación.</target>
        </trans-unit>
        <trans-unit id="246acb046d6b9bd62c3702633fac1169a8d836d0" translate="yes" xml:space="preserve">
          <source>The real data lies in the &lt;code&gt;filenames&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; attributes. The target attribute is the integer index of the category:</source>
          <target state="translated">Los datos reales se encuentran en los &lt;code&gt;filenames&lt;/code&gt; y &lt;code&gt;target&lt;/code&gt; atributos de destino . El atributo de destino es el &amp;iacute;ndice entero de la categor&amp;iacute;a:</target>
        </trans-unit>
        <trans-unit id="480e842bb5c6e42d98f06aa4f6c096c0c7aba356" translate="yes" xml:space="preserve">
          <source>The recall is the ratio &lt;code&gt;tp / (tp + fn)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fn&lt;/code&gt; the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</source>
          <target state="translated">La recuperaci&amp;oacute;n es la relaci&amp;oacute;n &lt;code&gt;tp / (tp + fn)&lt;/code&gt; donde &lt;code&gt;tp&lt;/code&gt; es el n&amp;uacute;mero de verdaderos positivos y &lt;code&gt;fn&lt;/code&gt; el n&amp;uacute;mero de falsos negativos. El retiro es intuitivamente la capacidad del clasificador de encontrar todas las muestras positivas.</target>
        </trans-unit>
        <trans-unit id="6f99b6066a1cdf81f95a9e89b923c52bb3877c2c" translate="yes" xml:space="preserve">
          <source>The reconstructed image.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65f6a86aee18eed07b01f195c73d746d5f2ca909" translate="yes" xml:space="preserve">
          <source>The reconstructed points using the metric MDS and non metric MDS are slightly shifted to avoid overlapping.</source>
          <target state="translated">Los puntos reconstruidos usando el MDS métrico y el MDS no métrico están ligeramente desplazados para evitar superposiciones.</target>
        </trans-unit>
        <trans-unit id="cefc093c3489c62b159a3ce090f6d8b10ecaa3b1" translate="yes" xml:space="preserve">
          <source>The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a \(d\)-dimensional manifold embedded in a \(D\)-dimensional parameter space, the reconstruction error will decrease as &lt;code&gt;n_components&lt;/code&gt; is increased until &lt;code&gt;n_components == d&lt;/code&gt;.</source>
          <target state="translated">El error de reconstrucci&amp;oacute;n calculado por cada rutina se puede utilizar para elegir la dimensi&amp;oacute;n de salida &amp;oacute;ptima. Para una variedad dimensional \ (d \) incrustada en un espacio de par&amp;aacute;metros dimensionales \ (D \), el error de reconstrucci&amp;oacute;n disminuir&amp;aacute; a medida que &lt;code&gt;n_components&lt;/code&gt; aumente hasta &lt;code&gt;n_components == d&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cd5de02ec688b7a05331073a7d6e7032a5c96c24" translate="yes" xml:space="preserve">
          <source>The reconstruction with L1 penalization gives a result with zero error (all pixels are successfully labeled with 0 or 1), even if noise was added to the projections. In comparison, an L2 penalization (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt;&lt;/a&gt;) produces a large number of labeling errors for the pixels. Important artifacts are observed on the reconstructed image, contrary to the L1 penalization. Note in particular the circular artifact separating the pixels in the corners, that have contributed to fewer projections than the central disk.</source>
          <target state="translated">La reconstrucci&amp;oacute;n con penalizaci&amp;oacute;n L1 da un resultado con cero error (todos los p&amp;iacute;xeles se etiquetan correctamente con 0 o 1), incluso si se agreg&amp;oacute; ruido a las proyecciones. En comparaci&amp;oacute;n, una penalizaci&amp;oacute;n L2 ( &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; &lt;/a&gt; ) produce una gran cantidad de errores de etiquetado para los p&amp;iacute;xeles. Se observan artefactos importantes en la imagen reconstruida, contrario a la penalizaci&amp;oacute;n L1. N&amp;oacute;tese en particular el artefacto circular que separa los p&amp;iacute;xeles en las esquinas, que han contribuido a menos proyecciones que el disco central.</target>
        </trans-unit>
        <trans-unit id="cc89055f829a16401789a0501b3aaaa652548713" translate="yes" xml:space="preserve">
          <source>The reduced distance, defined for some metrics, is a computationally more efficient measure which preserves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is the squared-euclidean distance.</source>
          <target state="translated">La distancia reducida,definida para algunas métricas,es una medida computacionalmente más eficiente que preserva el rango de la distancia verdadera.Por ejemplo,en la métrica de la distancia euclidiana,la distancia reducida es la distancia euclidiana al cuadrado.</target>
        </trans-unit>
        <trans-unit id="d3411437239f8f2dc8ee2706670c4e4a91db1791" translate="yes" xml:space="preserve">
          <source>The reduced samples.</source>
          <target state="translated">Las muestras reducidas.</target>
        </trans-unit>
        <trans-unit id="67c2217cec61f41257c896a393db0ce694f3f635" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;GridSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">El estimador reajustado est&amp;aacute; disponible en el atributo &lt;code&gt;best_estimator_&lt;/code&gt; y permite usar &lt;code&gt;predict&lt;/code&gt; directamente en esta instancia de &lt;code&gt;GridSearchCV&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1178e040e21448dfc3d87635a64add70ee2fc71f" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;RandomizedSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">El estimador reajustado est&amp;aacute; disponible en el atributo &lt;code&gt;best_estimator_&lt;/code&gt; y permite usar &lt;code&gt;predict&lt;/code&gt; directamente en esta instancia de &lt;code&gt;RandomizedSearchCV&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d35d9dbef92f31bfc55b28e05410d9f2634f5c0d" translate="yes" xml:space="preserve">
          <source>The regression target for each sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb2b5c40285ce2a0d935c8174b2b1df487e2e554" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical.</source>
          <target state="translated">El objetivo de regresión o las etiquetas de clasificación,si procede.El tipo D es flotante si es numérico,y el objeto si es categórico.</target>
        </trans-unit>
        <trans-unit id="f49726f7b2ee9ab1b3928455718be0471fabc15a" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical. If &lt;code&gt;as_frame&lt;/code&gt; is True, &lt;code&gt;target&lt;/code&gt; is a pandas object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3833ac2bfe2b88ee813e1c7e696adf8c3874fe96" translate="yes" xml:space="preserve">
          <source>The regression target.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b42b60d45f4e29bdab4ac89a5e2270a5774add83" translate="yes" xml:space="preserve">
          <source>The regression target. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas Series.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c39a60b37913113f18fb812e5aaafb85e77ce6aa" translate="yes" xml:space="preserve">
          <source>The regression targets. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="444eaf2365e5ec07e25a0ed19fde31ede0366773" translate="yes" xml:space="preserve">
          <source>The regressor is used to predict and the &lt;code&gt;inverse_func&lt;/code&gt; or &lt;code&gt;inverse_transform&lt;/code&gt; is applied before returning the prediction.</source>
          <target state="translated">El regresor se usa para predecir y se aplica &lt;code&gt;inverse_func&lt;/code&gt; o &lt;code&gt;inverse_transform&lt;/code&gt; antes de devolver la predicci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="edcb36e5cf0943282d84be3c17128a4f09595f3b" translate="yes" xml:space="preserve">
          <source>The regressor that is used for calibration depends on the &lt;code&gt;method&lt;/code&gt; parameter. &lt;code&gt;'sigmoid'&lt;/code&gt; corresponds to a parametric approach based on Platt&amp;rsquo;s logistic model &lt;a href=&quot;#id8&quot; id=&quot;id4&quot;&gt;3&lt;/a&gt;, i.e. \(p(y_i = 1 | f_i)\) is modeled as \(\sigma(A f_i + B)\) where \(\sigma\) is the logistic function, and \(A\) and \(B\) are real numbers to be determined when fitting the regressor via maximum likelihood. &lt;code&gt;'isotonic'&lt;/code&gt; will instead fit a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function (see &lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt;&lt;code&gt;sklearn.isotonic&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ab5d35ab966f753794908f22950ec4d3c66e8fc" translate="yes" xml:space="preserve">
          <source>The regressor to stacked the base estimators fitted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f20e2edcb6e4e7d6bdbe8de69b9b24c7725d7e1" translate="yes" xml:space="preserve">
          <source>The regularised covariance is:</source>
          <target state="translated">La covarianza regularizada es:</target>
        </trans-unit>
        <trans-unit id="4dfecc4cc3d2dd0e68757701d8e0aef7bde77c4d" translate="yes" xml:space="preserve">
          <source>The regularization mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 &amp;lt; l1_ratio &amp;lt; 1, the penalty is a combination of L1 and L2.</source>
          <target state="translated">El par&amp;aacute;metro de mezcla de regularizaci&amp;oacute;n, con 0 &amp;lt;= l1_ratio &amp;lt;= 1. Para l1_ratio = 0, la penalizaci&amp;oacute;n es una penalizaci&amp;oacute;n L2 por elementos (tambi&amp;eacute;n conocida como Norma Frobenius). Para l1_ratio = 1 es una penalizaci&amp;oacute;n L1 por elementos. Para 0 &amp;lt;l1_ratio &amp;lt;1, la penalizaci&amp;oacute;n es una combinaci&amp;oacute;n de L1 y L2.</target>
        </trans-unit>
        <trans-unit id="f1711bca786b8ddb98e81cd17b706bc6925eee44" translate="yes" xml:space="preserve">
          <source>The regularization parameter C in the LogisticRegression. When C is an array, fit will take each regularization parameter in C one by one for LogisticRegression and store results for each one in &lt;code&gt;all_scores_&lt;/code&gt;, where columns and rows represent corresponding reg_parameters and features.</source>
          <target state="translated">El par&amp;aacute;metro de regularizaci&amp;oacute;n C en LogisticRegression. Cuando C es una matriz, fit tomar&amp;aacute; cada par&amp;aacute;metro de regularizaci&amp;oacute;n en C uno por uno para LogisticRegression y almacenar&amp;aacute; los resultados de cada uno en &lt;code&gt;all_scores_&lt;/code&gt; , donde las columnas y filas representan los par&amp;aacute;metros reg_parameters y caracter&amp;iacute;sticas correspondientes.</target>
        </trans-unit>
        <trans-unit id="7865779166b38ce9dcfe2e41f3eba5295cbd3874" translate="yes" xml:space="preserve">
          <source>The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling.</source>
          <target state="translated">El parámetro de regularización alfa en el Lasso.Advertencia:este no es el parámetro alfa del artículo de selección de estabilidad que está escalando.</target>
        </trans-unit>
        <trans-unit id="0ed37f5cbd4d43ac0f2b6fe6eac86f942c91256a" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance.</source>
          <target state="translated">El parámetro de regularización:cuanto más alto el alfa,más regularización,más escasa la covarianza inversa.</target>
        </trans-unit>
        <trans-unit id="e124bf83498b6636f4567895eaf37c151f534f9f" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. Range is (0, inf].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="688891a69c745428fa6d147d1a8f3e33c50a1bf6" translate="yes" xml:space="preserve">
          <source>The regularization reduces the influence of correlated variables on the model because the weight is shared between the two predictive variables, so neither alone would have strong weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a1b8c0c02e4613adc42b58f49402bda71930b51" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is given by:</source>
          <target state="translated">La covarianza regularizada (encogida)viene dada por:</target>
        </trans-unit>
        <trans-unit id="0a5a9291b6a07681a32efc9a88d6f2d7eab96435" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is:</source>
          <target state="translated">La covarianza regularizada (encogida)es:</target>
        </trans-unit>
        <trans-unit id="5061099398d935daccb5dfd0421b959c5f17fce1" translate="yes" xml:space="preserve">
          <source>The regularized covariance is given by:</source>
          <target state="translated">La covarianza regularizada está dada por:</target>
        </trans-unit>
        <trans-unit id="bba73ee876f52c89494b029f9c7d24e1239abc01" translate="yes" xml:space="preserve">
          <source>The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.</source>
          <target state="translated">El regularizador es una penalización añadida a la función de pérdida que encoge los parámetros del modelo hacia el vector cero utilizando la norma euclidiana cuadrada L2 o la norma absoluta L1 o una combinación de ambas (Red Elástica).Si la actualización del parámetro cruza el valor de 0,0 debido al regularizador,la actualización se trunca a 0,0 para permitir el aprendizaje de modelos escasos y lograr la selección de características en línea.</target>
        </trans-unit>
        <trans-unit id="dccfd8ff5de1af25843574f310f33b70fd7f2fcc" translate="yes" xml:space="preserve">
          <source>The relationship between recall and precision can be observed in the stairstep area of the plot - at the edges of these steps a small change in the threshold considerably reduces precision, with only a minor gain in recall.</source>
          <target state="translated">La relación entre el recuerdo y la precisión puede observarse en el área de los escalones de la trama-en los bordes de estos escalones un pequeño cambio en el umbral reduce considerablemente la precisión,con sólo una pequeña ganancia en el recuerdo.</target>
        </trans-unit>
        <trans-unit id="e4d930448408dd13aba80cfbcc12949bce572769" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile if &lt;code&gt;effective_rank&lt;/code&gt; is not None.</source>
          <target state="translated">La importancia relativa de la cola gruesa y ruidosa del perfil de valores singulares si &lt;code&gt;effective_rank&lt;/code&gt; no es Ninguno.</target>
        </trans-unit>
        <trans-unit id="5b4ee2ad411363b437ce1738486767191903cc20" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile.</source>
          <target state="translated">La importancia relativa de la cola gorda y ruidosa del perfil de valores singulares.</target>
        </trans-unit>
        <trans-unit id="1bf8ab4629789502195bd390130a1b2aa7e78e4e" translate="yes" xml:space="preserve">
          <source>The relative increment in the results before declaring convergence.</source>
          <target state="translated">El incremento relativo de los resultados antes de declarar la convergencia.</target>
        </trans-unit>
        <trans-unit id="ae073568b2b2b27a72266212b55fc2cb2d4cd169" translate="yes" xml:space="preserve">
          <source>The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The &lt;strong&gt;expected fraction of the samples&lt;/strong&gt; they contribute to can thus be used as an estimate of the &lt;strong&gt;relative importance of the features&lt;/strong&gt;. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.</source>
          <target state="translated">El rango relativo (es decir, la profundidad) de una caracter&amp;iacute;stica utilizada como nodo de decisi&amp;oacute;n en un &amp;aacute;rbol puede usarse para evaluar la importancia relativa de esa caracter&amp;iacute;stica con respecto a la predictibilidad de la variable objetivo. Las caracter&amp;iacute;sticas utilizadas en la parte superior del &amp;aacute;rbol contribuyen a la decisi&amp;oacute;n de predicci&amp;oacute;n final de una fracci&amp;oacute;n mayor de las muestras de entrada. Por tanto, la &lt;strong&gt;fracci&amp;oacute;n esperada de las muestras a las&lt;/strong&gt; que contribuyen puede utilizarse como una estimaci&amp;oacute;n de la &lt;strong&gt;importancia relativa de las caracter&amp;iacute;sticas&lt;/strong&gt; . En scikit-learn, la fracci&amp;oacute;n de muestras a las que contribuye una caracter&amp;iacute;stica se combina con la disminuci&amp;oacute;n de la impureza al dividirlas para crear una estimaci&amp;oacute;n normalizada del poder predictivo de esa caracter&amp;iacute;stica.</target>
        </trans-unit>
        <trans-unit id="81bcb12f84ef974910f69bd2abe477a2b6a71621" translate="yes" xml:space="preserve">
          <source>The remaining columns can be used to predict the frequency of claim events. Those columns are very heterogeneous with a mix of categorical and numeric variables with different scales, possibly very unevenly distributed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fc6d12d2c584bef7c5a793374c798219700e42f" translate="yes" xml:space="preserve">
          <source>The remaining singular values&amp;rsquo; tail is fat, decreasing as:</source>
          <target state="translated">La cola de los valores singulares restantes es gruesa, disminuyendo como:</target>
        </trans-unit>
        <trans-unit id="ec1c6c51e9847818ba009f4ace94e61e7f83ab56" translate="yes" xml:space="preserve">
          <source>The reported averages include macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label), and sample average (only for multilabel classification). Micro average (averaging the total true positives, false negatives and false positives) is only shown for multi-label or multi-class with a subset of classes, because it corresponds to accuracy otherwise. See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="006806051caab55f94063393a09eaea2afaeb022" translate="yes" xml:space="preserve">
          <source>The reported averages include micro average (averaging the total true positives, false negatives and false positives), macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label) and sample average (only for multilabel classification). See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="translated">Los promedios informados incluyen micropromedio (promediando el total de verdaderos positivos, falsos negativos y falsos positivos), macropromedio (promediando la media no ponderada por etiqueta), promedio ponderado (promediando la media ponderada de soporte por etiqueta) y promedio de la muestra (solo para etiquetas m&amp;uacute;ltiples clasificaci&amp;oacute;n). Consulte tambi&amp;eacute;n &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt; para obtener m&amp;aacute;s detalles sobre los promedios.</target>
        </trans-unit>
        <trans-unit id="8f7b2580f38f68e2972536327271d77d45324cd3" translate="yes" xml:space="preserve">
          <source>The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.</source>
          <target state="translated">La matriz residual del bloque X (Xk+1)se obtiene por la deflación en la puntuación actual de X:x_puntuación.</target>
        </trans-unit>
        <trans-unit id="18a2377c2d81e0ebb00644fcb1d33204e8d98249" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented.</source>
          <target state="translated">La matriz residual del bloque Y (Yk+1)se obtiene por deflación en la puntuación X actual.Esto realiza la regresión PLS conocida como PLS2.Este modo está orientado a la predicción.</target>
        </trans-unit>
        <trans-unit id="953a5ab9533846fbfb5f76815143b2efa25824fa" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score.</source>
          <target state="translated">La matriz residual del bloque Y (Yk+1)se obtiene por deflación en la puntuación actual de Y.</target>
        </trans-unit>
        <trans-unit id="4786c768ceb1dac6ca78b9b9b4bd8e26183b7ef9" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.</source>
          <target state="translated">La matriz residual del bloque Y (Yk+1)se obtiene por deflación en la puntuación actual de Y.Esto realiza una versión simétrica canónica de la regresión PLS.Pero ligeramente diferente a la CCA.Esto se utiliza principalmente para el modelado.</target>
        </trans-unit>
        <trans-unit id="f3c685007c5e417136c9d43a236103e7a7414e81" translate="yes" xml:space="preserve">
          <source>The result is quite similar to the non-normalized case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3050c2e87895e094a17bdbd4ce41119b71fa1e6b" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; is much more strongly biased: the difference is reminiscent of the local intensity value of the original image.</source>
          <target state="translated">El resultado de la &lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;regresi&amp;oacute;n de &amp;aacute;ngulo m&amp;iacute;nimo&lt;/a&gt; est&amp;aacute; mucho m&amp;aacute;s sesgado: la diferencia recuerda el valor de intensidad local de la imagen original.</target>
        </trans-unit>
        <trans-unit id="2844a2c76b7226f0533d494b8e60503f59b9c4e8" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; may be different from those obtained using &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; as the elements are grouped in different ways. The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; takes an average over cross-validation folds, whereas &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; is not an appropriate measure of generalisation error.</source>
          <target state="translated">El resultado de &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; puede ser diferente de los obtenidos con &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; ya que los elementos se agrupan de diferentes formas. La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; toma un promedio de los pliegues de validaci&amp;oacute;n cruzada, mientras que &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; simplemente devuelve las etiquetas (o probabilidades) de varios modelos distintos sin distinci&amp;oacute;n. Por tanto, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; no es una medida apropiada de error de generalizaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="a51791cdd2f13d5121648ef37a39961811acb402" translate="yes" xml:space="preserve">
          <source>The result of calling &lt;code&gt;fit&lt;/code&gt; on a &lt;code&gt;GridSearchCV&lt;/code&gt; object is a classifier that we can use to &lt;code&gt;predict&lt;/code&gt;:</source>
          <target state="translated">El resultado de llamar a &lt;code&gt;fit&lt;/code&gt; en un objeto &lt;code&gt;GridSearchCV&lt;/code&gt; es un clasificador que podemos usar para &lt;code&gt;predict&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="88a0848975f5bdc5b1a985f762b1245644b8930f" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to &lt;code&gt;np.diag(self(X))&lt;/code&gt;; however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0c66fbb583b621a3ed191db16f52e8d9fa96072" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="translated">El resultado de este método es idéntico al de np.diag(self(X));sin embargo,puede evaluarse de manera más eficiente ya que sólo se evalúa la diagonal.</target>
        </trans-unit>
        <trans-unit id="629d23e2107cca0c4a5d2061641bb34723082f2c" translate="yes" xml:space="preserve">
          <source>The result points are &lt;em&gt;not&lt;/em&gt; necessarily sorted by distance to their query point.</source>
          <target state="translated">Los puntos de resultado &lt;em&gt;no se&lt;/em&gt; ordenan necesariamente por distancia al punto de consulta.</target>
        </trans-unit>
        <trans-unit id="97bd456bf5cbdca967559f8f3d5ed5bef7911efc" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabasz score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc861dceeba49e88611e2b04a0b3ec8cf37af89c" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabaz score.</source>
          <target state="translated">La puntuación resultante de Calinski-Harabaz.</target>
        </trans-unit>
        <trans-unit id="91d97654f948931244cdd794d37fd9ac58fe871c" translate="yes" xml:space="preserve">
          <source>The resulting Davies-Bouldin score.</source>
          <target state="translated">El resultado de la puntuación de Davies-Bouldin.</target>
        </trans-unit>
        <trans-unit id="cfd8b506a89d0c11900fefa11e6003ff64d7a508" translate="yes" xml:space="preserve">
          <source>The resulting Fowlkes-Mallows score.</source>
          <target state="translated">La puntuación resultante de Fowlkes-Mallows.</target>
        </trans-unit>
        <trans-unit id="82343548ff27f39a450415f81d355404a35e8f50" translate="yes" xml:space="preserve">
          <source>The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.</source>
          <target state="translated">La estructura resultante de los bíceps es bloque-diagonal,ya que cada fila y cada columna pertenece exactamente a un bíceps.</target>
        </trans-unit>
        <trans-unit id="a10cedad8ec8047b7f7badb13dd314c46b9b1d6c" translate="yes" xml:space="preserve">
          <source>The resulting counts are normalized using &lt;a href=&quot;sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;sklearn.preprocessing.normalize&lt;/code&gt;&lt;/a&gt; unless normalize is set to False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af7844c7ade28bb6e966130c36de65d0b613a4b9" translate="yes" xml:space="preserve">
          <source>The resulting dataset contains ordinal attributes which can be further used in a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">El conjunto de datos resultante contiene atributos ordinales que se pueden usar m&amp;aacute;s en un &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2b9ee0d5d80ffdad0cbeed5368095411cec46727" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_exp(X, Y) = k(X, Y) ** exponent</source>
          <target state="translated">El núcleo resultante se define como k_exp(X,Y)=k(X,Y)**exponente</target>
        </trans-unit>
        <trans-unit id="5f2e205585c93945d55d6e2cae73c2d9f60e4b99" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_prod(X, Y) = k1(X, Y) * k2(X, Y)</source>
          <target state="translated">El núcleo resultante se define como k_prod(X,Y)=k1(X,Y)*k2(X,Y)</target>
        </trans-unit>
        <trans-unit id="65c4e9abf2f65b5e239efef34833d44fb903de78" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_sum(X, Y) = k1(X, Y) + k2(X, Y)</source>
          <target state="translated">El núcleo resultante se define como k_sum(X,Y)=k1(X,Y)+k2(X,Y)</target>
        </trans-unit>
        <trans-unit id="fb9ca9651a528caa2f6deb96ee39233de0fa48d4" translate="yes" xml:space="preserve">
          <source>The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;. The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. The parameters are estimated by maximizing the &lt;em&gt;marginal log likelihood&lt;/em&gt;.</source>
          <target state="translated">El modelo resultante se llama &lt;em&gt;Regresi&amp;oacute;n de la cresta bayesiana&lt;/em&gt; y es similar a la &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; cl&amp;aacute;sica . Los par&amp;aacute;metros \ (w \), \ (\ alpha \) y \ (\ lambda \) se estiman conjuntamente durante el ajuste del modelo. Los hiperpar&amp;aacute;metros restantes son los par&amp;aacute;metros de los gamma previos sobre \ (\ alpha \) y \ (\ lambda \). Por lo general, se eligen para que &lt;em&gt;no sean informativos&lt;/em&gt; . Los par&amp;aacute;metros se estiman maximizando la &lt;em&gt;probabilidad logar&amp;iacute;tmica marginal&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="2ff5ad65586245c919e0a8e1ef11c4948b5606a5" translate="yes" xml:space="preserve">
          <source>The resulting patches are allocated in a dedicated array.</source>
          <target state="translated">Los parches resultantes se asignan en una matriz dedicada.</target>
        </trans-unit>
        <trans-unit id="64dfac2f5dc0c167f2eb731c8522fdbbf80022e7" translate="yes" xml:space="preserve">
          <source>The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data.</source>
          <target state="translated">El transformador resultante ha aprendido entonces una incrustación categórica supervisada,dispersa y de alta dimensión de los datos.</target>
        </trans-unit>
        <trans-unit id="52f668ebefc79801d78ca36ee52d3c9f5a955a8d" translate="yes" xml:space="preserve">
          <source>The results from OPTICS &lt;code&gt;cluster_optics_dbscan&lt;/code&gt; method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f9dd224b05fada5edc12dcb7a4c8d5421d61c2d" translate="yes" xml:space="preserve">
          <source>The return value is a cross-validator which generates the train/test splits via the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">El valor de retorno es un validador cruzado que genera las divisiones de tren / prueba a trav&amp;eacute;s del m&amp;eacute;todo de &lt;code&gt;split&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="63a7df0fd8542a7b486adfe1466de16955c3587a" translate="yes" xml:space="preserve">
          <source>The returned dataset is a &lt;code&gt;scikit-learn&lt;/code&gt; &amp;ldquo;bunch&amp;rdquo;: a simple holder object with fields that can be both accessed as python &lt;code&gt;dict&lt;/code&gt; keys or &lt;code&gt;object&lt;/code&gt; attributes for convenience, for instance the &lt;code&gt;target_names&lt;/code&gt; holds the list of the requested category names:</source>
          <target state="translated">El conjunto de datos devuelto es un &quot;grupo&quot; de &lt;code&gt;scikit-learn&lt;/code&gt; : un objeto contenedor simple con campos a los que se puede acceder como claves de &lt;code&gt;dict&lt;/code&gt; ado de Python o atributos de &lt;code&gt;object&lt;/code&gt; por conveniencia, por ejemplo, &lt;code&gt;target_names&lt;/code&gt; contiene la lista de los nombres de categor&amp;iacute;a solicitados:</target>
        </trans-unit>
        <trans-unit id="c5d262a3b65ccb350b8a1d10b0eaf6eba41fc62d" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by label of classes.</source>
          <target state="translated">Las estimaciones devueltas para todas las clases están ordenadas por etiqueta de clases.</target>
        </trans-unit>
        <trans-unit id="8520c10c8edb7f58383ef36900c902f5398bf785" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by the label of classes.</source>
          <target state="translated">Las estimaciones devueltas para todas las clases están ordenadas por la etiqueta de clases.</target>
        </trans-unit>
        <trans-unit id="9fa1a308376d0d44aa58fe9b54fd102c1f0b956a" translate="yes" xml:space="preserve">
          <source>The returned object is a MemorizedFunc object, that is callable (behaves like a function), but offers extra methods for cache lookup and management. See the documentation for &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt;&lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">El objeto devuelto es un objeto MemorizedFunc, que se puede llamar (se comporta como una funci&amp;oacute;n), pero ofrece m&amp;eacute;todos adicionales para la b&amp;uacute;squeda y administraci&amp;oacute;n de cach&amp;eacute;. Consulte la documentaci&amp;oacute;n de &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt; &lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="4a1e44716730f4f771067bf0b441674e2034e883" translate="yes" xml:space="preserve">
          <source>The richer dictionary on the right is not larger in size, heavier subsampling is performed in order to stay on the same order of magnitude.</source>
          <target state="translated">El diccionario más rico de la derecha no es de mayor tamaño,se realiza un submuestreo más pesado para mantenerse en el mismo orden de magnitud.</target>
        </trans-unit>
        <trans-unit id="6f396634dd18eef11c3d60404319f2831b13040b" translate="yes" xml:space="preserve">
          <source>The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around &lt;code&gt;x=2&lt;/code&gt;). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance.</source>
          <target state="translated">Las cifras de la derecha corresponden a las mismas parcelas pero utilizando en su lugar un conjunto de &amp;aacute;rboles de decisi&amp;oacute;n de ensacado. En ambas figuras podemos observar que el t&amp;eacute;rmino de sesgo es mayor que en el caso anterior. En la figura superior derecha, la diferencia entre la predicci&amp;oacute;n promedio (en cian) y el mejor modelo posible es mayor (por ejemplo, observe el desplazamiento alrededor de &lt;code&gt;x=2&lt;/code&gt; ). En la figura inferior derecha, la curva de sesgo tambi&amp;eacute;n es ligeramente m&amp;aacute;s alta que en la figura inferior izquierda. Sin embargo, en t&amp;eacute;rminos de varianza, el haz de predicciones es m&amp;aacute;s estrecho, lo que sugiere que la varianza es menor. De hecho, como confirma la figura inferior derecha, el t&amp;eacute;rmino de varianza (en verde) es m&amp;aacute;s bajo que para &amp;aacute;rboles de decisi&amp;oacute;n individuales. En general, la descomposici&amp;oacute;n de sesgo-varianza ya no es la misma. La compensaci&amp;oacute;n es mejor para el ensacado: promediar varios &amp;aacute;rboles de decisi&amp;oacute;n que se ajustan a las copias de arranque del conjunto de datos aumenta ligeramente el t&amp;eacute;rmino de sesgo, pero permite una mayor reducci&amp;oacute;n de la varianza, lo que da como resultado un error cuadr&amp;aacute;tico medio general m&amp;aacute;s bajo (compare las curvas rojas en las cifras). La salida del script tambi&amp;eacute;n confirma esta intuici&amp;oacute;n. El error total del conjunto de ensacado es menor que el error total de un solo &amp;aacute;rbol de decisi&amp;oacute;n,y esta diferencia, de hecho, se debe principalmente a una variaci&amp;oacute;n reducida.</target>
        </trans-unit>
        <trans-unit id="4281560832d700a912bb9768ad9253748f3986db" translate="yes" xml:space="preserve">
          <source>The right plot shows the mean squared error between the coefficients found by the model and the chosen vector w. Less regularised models retrieve the exact coefficients (error is equal to 0), stronger regularised models increase the error.</source>
          <target state="translated">El gráfico de la derecha muestra el error cuadrático medio entre los coeficientes encontrados por el modelo y el vector w elegido.Los modelos menos regularizados recuperan los coeficientes exactos (el error es igual a 0),los modelos regularizados más fuertes aumentan el error.</target>
        </trans-unit>
        <trans-unit id="e91440f2fdf83c048c6ec145aa4ba8b2408d7a77" translate="yes" xml:space="preserve">
          <source>The robust MCD, that has a low error provided \(n_\text{samples} &amp;gt; 5n_\text{features}\)</source>
          <target state="translated">El MCD robusto, que tiene un error bajo proporcionado \ (n_ \ text {samples}&amp;gt; 5n_ \ text {features} \)</target>
        </trans-unit>
        <trans-unit id="ba79522dd51eaaef6a47e20f449051b56df7c07d" translate="yes" xml:space="preserve">
          <source>The roc curve requires either the probabilities or the non-thresholded decision values from the estimator. Since the logistic regression provides a decision function, we will use it to plot the roc curve:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17f09e161fdb50b4bf88ea2669cf0485f026fd48" translate="yes" xml:space="preserve">
          <source>The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.</source>
          <target state="translated">Las filas son las muestras y las columnas son:Longitud del Sépalo,Ancho del Sépalo,Longitud del Pétalo y Ancho del Pétalo.</target>
        </trans-unit>
        <trans-unit id="075c19426795ecca36fadcd5142db0c818eae0c2" translate="yes" xml:space="preserve">
          <source>The s parameter used to randomly scale the penalty of different features. Should be between 0 and 1.</source>
          <target state="translated">El parámetro s utilizado para escalar aleatoriamente la pena de diferentes características.Debería estar entre 0 y 1.</target>
        </trans-unit>
        <trans-unit id="04a6b9de3c4814c0e0ce533f438ce9705b113ee4" translate="yes" xml:space="preserve">
          <source>The same as the min_samples given to OPTICS. Up and down steep regions can&amp;rsquo;t have more then &lt;code&gt;min_samples&lt;/code&gt; consecutive non-steep points. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e103a284fd35ca6afde93378ca71e413fedf538" translate="yes" xml:space="preserve">
          <source>The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).</source>
          <target state="translated">El mismo grupo no aparecerá en dos pliegues diferentes (el número de grupos distintos tiene que ser al menos igual al número de pliegues).</target>
        </trans-unit>
        <trans-unit id="0bd48a9cd63a739602e7de633cedbe34c0721a4f" translate="yes" xml:space="preserve">
          <source>The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:</source>
          <target state="translated">La misma instancia del transformador puede aplicarse entonces a algunos nuevos datos de prueba no vistos durante la llamada de ajuste:se aplicarán las mismas operaciones de escalado y desplazamiento para ser consistentes con la transformación realizada en los datos del tren:</target>
        </trans-unit>
        <trans-unit id="9b63d107562f8bc53dcee73bc223f241497e945e" translate="yes" xml:space="preserve">
          <source>The same parameter &lt;code&gt;target&lt;/code&gt; is used to specify the target in multi-output regression settings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4fc830eaa32e3715aa0bbc8cdaaf20cd8fc44d0" translate="yes" xml:space="preserve">
          <source>The same probability calibration procedure is available for all estimators via the &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; (see &lt;a href=&quot;calibration#calibration&quot;&gt;Probability calibration&lt;/a&gt;). In the case of &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt;, this procedure is builtin in &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; which is used under the hood, so it does not rely on scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c16a50cac2b1dc1101733e88917cf565ad0ad55a" translate="yes" xml:space="preserve">
          <source>The sample counts that are shown are weighted with any sample_weights that might be present.</source>
          <target state="translated">Los recuentos de muestras que se muestran se ponderan con los pesos_de_muestras que puedan estar presentes.</target>
        </trans-unit>
        <trans-unit id="da56cc1e3278be97e394d14d7afaac125d975593" translate="yes" xml:space="preserve">
          <source>The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible.</source>
          <target state="translated">La ponderación de la muestra reajusta el parámetro C,lo que significa que el clasificador pone más énfasis en acertar estos puntos.El efecto puede ser a menudo sutil.Para enfatizar el efecto aquí,particularmente ponderamos los valores atípicos,haciendo muy visible la deformación del límite de decisión.</target>
        </trans-unit>
        <trans-unit id="67f0f1888d07b20765c25e213bac379b2147854b" translate="yes" xml:space="preserve">
          <source>The sampled subsets of integer. The subset of selected integer might not be randomized, see the method argument.</source>
          <target state="translated">Los subconjuntos muestreados de números enteros.El subconjunto de números enteros seleccionados podría no ser aleatorio,véase el argumento del método.</target>
        </trans-unit>
        <trans-unit id="582c77a6e6e223c6a451ff779c949c2d01a1e4b1" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="translated">Las muestras en este conjunto de datos corresponden a parches de bosque de 30 &amp;times; 30 m en los EE. UU., Recolectados para la tarea de predecir el tipo de cobertura de cada parche, es decir, la especie de &amp;aacute;rbol dominante. Hay siete tipos encubiertos, lo que lo convierte en un problema de clasificaci&amp;oacute;n multiclase. Cada muestra tiene 54 caracter&amp;iacute;sticas, descritas en la &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;p&amp;aacute;gina de inicio del conjunto de datos&lt;/a&gt; . Algunas de las caracter&amp;iacute;sticas son indicadores booleanos, mientras que otras son medidas discretas o continuas.</target>
        </trans-unit>
        <trans-unit id="5425b30895b660513d41a7b82e02c9858a9f43c3" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5340a26524629a68efc3f526b09b13eeae9b4043" translate="yes" xml:space="preserve">
          <source>The samples that are used to train the calibrator should not be used to train the target classifier.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3faf112740a094590f64233617c65a5fa097ee8f" translate="yes" xml:space="preserve">
          <source>The samples.</source>
          <target state="translated">Las muestras.</target>
        </trans-unit>
        <trans-unit id="7217248266c27250d74fa132fb5cef7e3435697a" translate="yes" xml:space="preserve">
          <source>The scalar parameter to validate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1aa98782517735bf609d139ad8030ba79e53e38f" translate="yes" xml:space="preserve">
          <source>The scaler instance can then be used on new data to transform it the same way it did on the training set:</source>
          <target state="translated">La instancia del escalador puede entonces utilizarse en nuevos datos para transformarlos de la misma manera que lo hizo en el conjunto de entrenamiento:</target>
        </trans-unit>
        <trans-unit id="fae25cf01c011421f9b169e383c16cc5b0e7dc5d" translate="yes" xml:space="preserve">
          <source>The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data:</source>
          <target state="translated">El proyecto scikit-learn proporciona un conjunto de herramientas de aprendizaje automático que pueden utilizarse tanto para la detección de novedades como de anomalías.Esta estrategia se implementa con objetos que aprenden de forma no supervisada a partir de los datos:</target>
        </trans-unit>
        <trans-unit id="b2bcbd53d39d84e38eaccf61f40a5b7e3d9f1072" translate="yes" xml:space="preserve">
          <source>The scikit-learn provides an object &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt; that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.</source>
          <target state="translated">Scikit-learn proporciona una &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt; &lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt; &lt;/a&gt; objeto. ElipticEnvelope que se ajusta a una estimaci&amp;oacute;n de covarianza robusta a los datos y, por lo tanto, ajusta una elipse a los puntos de datos centrales, ignorando los puntos fuera del modo central.</target>
        </trans-unit>
        <trans-unit id="c6f20ecec2f178819b742529ff67a9012c4e74b9" translate="yes" xml:space="preserve">
          <source>The score above which features should be selected.</source>
          <target state="translated">La puntuación por encima de la cual se deben seleccionar los rasgos.</target>
        </trans-unit>
        <trans-unit id="fbc2c7bdd77bdb62a30109845f32d883a40f3289" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split.</source>
          <target state="translated">La matriz de puntajes para los resultados de los exámenes en cada división de cv.</target>
        </trans-unit>
        <trans-unit id="a1547f496d2e14d7ede805b073598361b82ebf35" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split. Suffix &lt;code&gt;_score&lt;/code&gt; in &lt;code&gt;test_score&lt;/code&gt; changes to a specific metric like &lt;code&gt;test_r2&lt;/code&gt; or &lt;code&gt;test_auc&lt;/code&gt; if there are multiple scoring metrics in the scoring parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e1ecd73a82717f9c044c3e124c2c92104db3c6b" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. Suffix &lt;code&gt;_score&lt;/code&gt; in &lt;code&gt;train_score&lt;/code&gt; changes to a specific metric like &lt;code&gt;train_r2&lt;/code&gt; or &lt;code&gt;train_auc&lt;/code&gt; if there are multiple scoring metrics in the scoring parameter. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21374122c0da50ea660a65ea3274fa15ca9abbe5" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">La matriz de puntuaci&amp;oacute;n para las puntuaciones de los trenes en cada divisi&amp;oacute;n de cv. Esto solo est&amp;aacute; disponible si el par&amp;aacute;metro &lt;code&gt;return_train_score&lt;/code&gt; es &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f9f0e8fbf15571f500f19345d44e98db663f1949" translate="yes" xml:space="preserve">
          <source>The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.</source>
          <target state="translated">El puntaje está limitado entre -1 por agrupación incorrecta y +1 por agrupación altamente densa.Las puntuaciones alrededor de cero indican agrupaciones superpuestas.</target>
        </trans-unit>
        <trans-unit id="10419dac5247bd799d768931a57ece7edc36ff14" translate="yes" xml:space="preserve">
          <source>The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.</source>
          <target state="translated">La puntuación se define como la relación entre la dispersión dentro de un grupo y la dispersión entre grupos.</target>
        </trans-unit>
        <trans-unit id="24504b3de92e08246a270b0dc2b57eafc2174bab" translate="yes" xml:space="preserve">
          <source>The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5fc2e46656049d91b95dd6363e2ef13687eb710" translate="yes" xml:space="preserve">
          <source>The score is defined as the ratio of within-cluster distances to between-cluster distances.</source>
          <target state="translated">La puntuación se define como la relación entre las distancias dentro del grupo y las distancias entre los grupos.</target>
        </trans-unit>
        <trans-unit id="a9ccc42adfd31440d43d06c5c4aab96f5e8222f0" translate="yes" xml:space="preserve">
          <source>The score is fast to compute</source>
          <target state="translated">La puntuación es rápida de calcular</target>
        </trans-unit>
        <trans-unit id="ef88d78d9d28409ef934e001658916bd55503917" translate="yes" xml:space="preserve">
          <source>The score is fast to compute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="298450881e1d83a617f17a47cba5b823081f8332" translate="yes" xml:space="preserve">
          <source>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.</source>
          <target state="translated">El puntaje es más alto cuando los cúmulos son densos y están bien separados,lo que se relaciona con un concepto estándar de un cúmulo.</target>
        </trans-unit>
        <trans-unit id="8ebec04d9506729ea096f793265427e501ac6359" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - \text{n\_classes}}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="translated">El puntaje var&amp;iacute;a de 0 a 1, o cuando se usa &lt;code&gt;adjusted=True&lt;/code&gt; , se vuelve a escalar al rango \ (\ frac {1} {1 - \ text {n \ _classes}} \) a 1, inclusive, con rendimiento al azar puntuaci&amp;oacute;n 0.</target>
        </trans-unit>
        <trans-unit id="f66ce641ead1aaeebf85e08eeb401e8169ff494e" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - n\_classes}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99f201771620c880cb8fcf0a4290517d17eb61e7" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.</source>
          <target state="translated">La puntuación va de 0 a 1.Un valor alto indica una buena similitud entre dos grupos.</target>
        </trans-unit>
        <trans-unit id="67703aed8c056d27307c0c36c3685d8f65de746b" translate="yes" xml:space="preserve">
          <source>The scorer callable object / function must have its signature as &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">El objeto / funci&amp;oacute;n invocable anotador debe tener su firma como &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5e77d6fe812bc8f046068c5fa078dda3a1146e44" translate="yes" xml:space="preserve">
          <source>The scorer.</source>
          <target state="translated">El anotador.</target>
        </trans-unit>
        <trans-unit id="97894cb494476d0c2ed229f8ae81e55aa78ff1c3" translate="yes" xml:space="preserve">
          <source>The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the &lt;code&gt;scoring&lt;/code&gt; parameter. Empty if no early stopping or if &lt;code&gt;validation_fraction&lt;/code&gt; is None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04400e5c44d656378bb2e0f77c0e1b05794d2b51" translate="yes" xml:space="preserve">
          <source>The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the &lt;code&gt;scoring&lt;/code&gt; parameter. If &lt;code&gt;scoring&lt;/code&gt; is not &amp;lsquo;loss&amp;rsquo;, scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd1c18a79b397962bdc5e0312d35f12a484f084a" translate="yes" xml:space="preserve">
          <source>The scores for each feature along the path.</source>
          <target state="translated">Las puntuaciones de cada característica a lo largo del camino.</target>
        </trans-unit>
        <trans-unit id="81a1b1406082100f034b3df6c2ec24562a123854" translate="yes" xml:space="preserve">
          <source>The scores obtained for each permutations.</source>
          <target state="translated">Las puntuaciones obtenidas para cada permutación.</target>
        </trans-unit>
        <trans-unit id="12753b21f50efe6accfab886a283f46c3614c6fe" translate="yes" xml:space="preserve">
          <source>The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does not attempt to completely filter the outliers but lessen their effect.</source>
          <target state="translated">Las puntuaciones de HuberRegressor no pueden compararse directamente con las de TheilSen y RANSAC porque no intenta filtrar completamente los valores atípicos sino disminuir su efecto.</target>
        </trans-unit>
        <trans-unit id="48d352a6767e745c328aec9195da7218a62bd613" translate="yes" xml:space="preserve">
          <source>The scores of all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at keys ending in &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; (&lt;code&gt;'mean_test_precision'&lt;/code&gt;, &lt;code&gt;'rank_test_precision'&lt;/code&gt;, etc&amp;hellip;)</source>
          <target state="translated">Las puntuaciones de todos los anotadores est&amp;aacute;n disponibles en el &lt;code&gt;cv_results_&lt;/code&gt; en las claves que terminan en &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; ( &lt;code&gt;'mean_test_precision'&lt;/code&gt; , &lt;code&gt;'rank_test_precision'&lt;/code&gt; , etc.)</target>
        </trans-unit>
        <trans-unit id="1676248dcc2b8fd0688c9d8da4bc193152185788" translate="yes" xml:space="preserve">
          <source>The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on.</source>
          <target state="translated">La búsqueda del parámetro de penalización óptimo (alfa)se realiza en una cuadrícula refinada iterativamente:primero se calculan las puntuaciones cruzadas validadas en una cuadrícula,luego una nueva cuadrícula refinada se centra en el máximo,y así sucesivamente.</target>
        </trans-unit>
        <trans-unit id="929b69ae47a09aabafb756d8ff0633d79e90c985" translate="yes" xml:space="preserve">
          <source>The searched parameter.</source>
          <target state="translated">El parámetro buscado.</target>
        </trans-unit>
        <trans-unit id="0d9a4b24aef1596fe1105c7b9e28ffdd6e589d45" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the product-kernel</source>
          <target state="translated">El segundo núcleo de base del núcleo del producto</target>
        </trans-unit>
        <trans-unit id="2d0afedea31e6e04d617f4edd60a150835c5916d" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the sum-kernel</source>
          <target state="translated">El segundo núcleo de base del núcleo de la suma</target>
        </trans-unit>
        <trans-unit id="544eb81771c23c2f620b00755dbcd12f00cacc66" translate="yes" xml:space="preserve">
          <source>The second example shows the ability of the Minimum Covariance Determinant robust estimator of covariance to concentrate on the main mode of the data distribution: the location seems to be well estimated, although the covariance is hard to estimate due to the banana-shaped distribution. Anyway, we can get rid of some outlying observations. The One-Class SVM is able to capture the real data structure, but the difficulty is to adjust its kernel bandwidth parameter so as to obtain a good compromise between the shape of the data scatter matrix and the risk of over-fitting the data.</source>
          <target state="translated">El segundo ejemplo muestra la capacidad del estimador robusto del Determinante de Covarianza Mínima de la covarianza para concentrarse en el modo principal de la distribución de los datos:la ubicación parece estar bien estimada,aunque la covarianza es difícil de estimar debido a la distribución en forma de plátano.De todos modos,podemos deshacernos de algunas observaciones periféricas.El SVM de una clase es capaz de capturar la estructura real de los datos,pero la dificultad es ajustar su parámetro de ancho de banda del núcleo para obtener un buen compromiso entre la forma de la matriz de dispersión de datos y el riesgo de sobreajustar los datos.</target>
        </trans-unit>
        <trans-unit id="5a46c67db9268b64cb76670e9e1b845e906b608f" translate="yes" xml:space="preserve">
          <source>The second figure shows the calibration curve of a linear support-vector classifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian naive Bayes: the calibration curve has a sigmoid curve, which is typical for an under-confident classifier. In the case of LinearSVC, this is caused by the margin property of the hinge loss, which lets the model focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">La segunda figura muestra la curva de calibración de un clasificador vectorial de soporte lineal (LinearSVC).LinearSVC muestra el comportamiento opuesto al de los Bayes gausianos ingenuos:la curva de calibración tiene una curva sigmoide,que es típica de un clasificador poco seguro.En el caso de LinearSVC,esto se debe a la propiedad de margen de la pérdida de la bisagra,que permite al modelo centrarse en muestras duras que están cerca del límite de decisión (los vectores de apoyo).</target>
        </trans-unit>
        <trans-unit id="e56142dda4889d67d8f5448567e56cb678c48e3c" translate="yes" xml:space="preserve">
          <source>The second figure shows the log-marginal-likelihood for different choices of the kernel&amp;rsquo;s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.</source>
          <target state="translated">La segunda figura muestra la probabilidad log-marginal para diferentes elecciones de los hiperpar&amp;aacute;metros del kernel, destacando las dos opciones de los hiperpar&amp;aacute;metros utilizados en la primera figura mediante puntos negros.</target>
        </trans-unit>
        <trans-unit id="69a6b8988bb4a9da22ae7a8129c60d4bfa19ab9d" translate="yes" xml:space="preserve">
          <source>The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:</source>
          <target state="translated">El segundo cargador suele utilizarse para la tarea de verificación de la cara:cada muestra es un par de dos cuadros pertenecientes o no a la misma persona:</target>
        </trans-unit>
        <trans-unit id="80a536b57ba05b05dbab01bd549517eccd6caab7" translate="yes" xml:space="preserve">
          <source>The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process prior fit with variational inference. The low value of the concentration prior makes the model favor a lower number of active components. This models &amp;ldquo;decides&amp;rdquo; to focus its modeling power on the big picture of the structure of the dataset: groups of points with alternating directions modeled by non-diagonal covariance matrices. Those alternating directions roughly capture the alternating nature of the original sine signal.</source>
          <target state="translated">El segundo modelo es un modelo de mezcla gaussiana bayesiana con un ajuste previo del proceso de Dirichlet con inferencia variacional. El bajo valor de la concentraci&amp;oacute;n previa hace que el modelo favorezca un menor n&amp;uacute;mero de componentes activos. Este modelo &quot;decide&quot; enfocar su poder de modelado en el panorama general de la estructura del conjunto de datos: grupos de puntos con direcciones alternas modelados por matrices de covarianza no diagonales. Esas direcciones alternas capturan aproximadamente la naturaleza alterna de la se&amp;ntilde;al sinusoidal original.</target>
        </trans-unit>
        <trans-unit id="ce63f012cd3f9d5ba6717aef4dc1ee5e80e67c9d" translate="yes" xml:space="preserve">
          <source>The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">El segundo tiene un nivel de ruido más bajo y una escala de longitud más corta,lo que explica la mayor parte de la variación por la relación funcional libre de ruido.El segundo modelo tiene una mayor probabilidad;sin embargo,dependiendo del valor inicial de los hiperparámetros,la optimización basada en el gradiente podría también converger hacia la solución de alto ruido.Por consiguiente,es importante repetir la optimización varias veces para diferentes inicializaciones.</target>
        </trans-unit>
        <trans-unit id="d5dcf1b11e556ea9365934aae9c750b0508ecb89" translate="yes" xml:space="preserve">
          <source>The second plot demonstrate one single run of the &lt;code&gt;MiniBatchKMeans&lt;/code&gt; estimator using a &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; and &lt;code&gt;n_init=1&lt;/code&gt;. This run leads to a bad convergence (local optimum) with estimated centers stuck between ground truth clusters.</source>
          <target state="translated">El segundo gr&amp;aacute;fico demuestra una sola ejecuci&amp;oacute;n del estimador &lt;code&gt;MiniBatchKMeans&lt;/code&gt; usando un &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; y &lt;code&gt;n_init=1&lt;/code&gt; . Esta ejecuci&amp;oacute;n conduce a una mala convergencia (&amp;oacute;ptimo local) con centros estimados atrapados entre grupos de verdad del suelo.</target>
        </trans-unit>
        <trans-unit id="41338f596d871499307d96f69f697b91afdfa1c4" translate="yes" xml:space="preserve">
          <source>The second plot is a heatmap of the classifier&amp;rsquo;s cross-validation accuracy as a function of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from \(10^{-3}\) to \(10^3\) is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.</source>
          <target state="translated">El segundo gr&amp;aacute;fico es un mapa de calor de la precisi&amp;oacute;n de la validaci&amp;oacute;n cruzada del clasificador en funci&amp;oacute;n de &lt;code&gt;C&lt;/code&gt; y &lt;code&gt;gamma&lt;/code&gt; . Para este ejemplo, exploramos una cuadr&amp;iacute;cula relativamente grande con fines ilustrativos. En la pr&amp;aacute;ctica, una cuadr&amp;iacute;cula logar&amp;iacute;tmica de \ (10 ​​^ {- 3} \) a \ (10 ​​^ 3 \) suele ser suficiente. Si los mejores par&amp;aacute;metros se encuentran en los l&amp;iacute;mites de la cuadr&amp;iacute;cula, se puede extender en esa direcci&amp;oacute;n en una b&amp;uacute;squeda posterior.</target>
        </trans-unit>
        <trans-unit id="57fb8e025f7ec94b6d1e30748cbff5561f1e9901" translate="yes" xml:space="preserve">
          <source>The second plot shows that an increase of the admissible distortion &lt;code&gt;eps&lt;/code&gt; allows to reduce drastically the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; for a given number of samples &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">El segundo gr&amp;aacute;fico muestra que un aumento de la distorsi&amp;oacute;n admisible &lt;code&gt;eps&lt;/code&gt; permite reducir dr&amp;aacute;sticamente el n&amp;uacute;mero m&amp;iacute;nimo de dimensiones &lt;code&gt;n_components&lt;/code&gt; para un n&amp;uacute;mero dado de muestras &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9c6c0ec72e2e2da2def0a20a979b162b66b4f25f" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span.</source>
          <target state="translated">El segundo gráfico visualizó las superficies de decisión del SVM del núcleo RBF y del SVM lineal con mapas aproximados del núcleo.El gráfico muestra las superficies de decisión de los clasificadores proyectados en los dos primeros componentes principales de los datos.Esta visualización debe tomarse con un grano de sal ya que es sólo un interesante corte a través de la superficie de decisión en 64 dimensiones.Nótese en particular que un punto de datos (representado como un punto)no se clasifica necesariamente en la región en la que se encuentra,ya que no se encontrará en el plano que abarcan los dos primeros componentes principales.</target>
        </trans-unit>
        <trans-unit id="0b0b4cc48e44d58bc4674ef40664981912a3f333" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span. The usage of &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7aa029b23a556153d8e76aebf5393705fc5a931" translate="yes" xml:space="preserve">
          <source>The second use case is to build a completely custom scorer object from a simple python function using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;, which can take several parameters:</source>
          <target state="translated">El segundo caso de uso es construir un objeto de marcador completamente personalizado a partir de una funci&amp;oacute;n de Python simple usando &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt; , que puede tomar varios par&amp;aacute;metros:</target>
        </trans-unit>
        <trans-unit id="c00cf63770db0bb2225569e421e3076426129a55" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios para agregar un peque&amp;ntilde;o ruido a las variables continuas para eliminar los valores repetidos. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="06cdf003d2aac9179d5700a91f249ff0a94d6f8e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que selecciona una caracter&amp;iacute;stica aleatoria para actualizar. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; . Se usa cuando la &lt;code&gt;selection&lt;/code&gt; == 'aleatoria'</target>
        </trans-unit>
        <trans-unit id="0bc2f2e46810cbdc913e04d68694f60b2b83e0d8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que selecciona una caracter&amp;iacute;stica aleatoria para actualizar. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; . Se usa cuando la &lt;code&gt;selection&lt;/code&gt; == 'aleatoria'.</target>
        </trans-unit>
        <trans-unit id="fceac838e81f76b0b51f388983ff2416b1824960" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9b5b4205f687d2590f4c80ac04919e87b2e36db" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data for the dual coordinate descent (if &lt;code&gt;dual=True&lt;/code&gt;). When &lt;code&gt;dual=False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se usar&amp;aacute; al mezclar los datos para el descenso de coordenadas dual (si &lt;code&gt;dual=True&lt;/code&gt; ). Cuando &lt;code&gt;dual=False&lt;/code&gt; , la implementaci&amp;oacute;n subyacente de &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; no es aleatoria y &lt;code&gt;random_state&lt;/code&gt; no tiene ning&amp;uacute;n efecto sobre los resultados. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="213c180fc69b83cb51964cde95f090b02ff40db1" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data, i.e. getting the random vectors to initialize the algorithm. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f24e839d9f0c07b405eb078c6f50d58ca84b7fe6" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se utilizar&amp;aacute; al mezclar los datos. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="58318e33f8140e9303ba15931c5e93b11c5df63e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo; or &amp;lsquo;liblinear&amp;rsquo;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se utilizar&amp;aacute; al mezclar los datos. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; . Se usa cuando &lt;code&gt;solver&lt;/code&gt; == 'sag' o 'liblinear'.</target>
        </trans-unit>
        <trans-unit id="1e64edd7c479eb06717df1495b4d044bfaa961d0" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se utilizar&amp;aacute; al mezclar los datos. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; . Se usa cuando &lt;code&gt;solver&lt;/code&gt; == 'sag'.</target>
        </trans-unit>
        <trans-unit id="3a35c2036466dbf3b68131c5150065896eadbbdd" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use. Randomizes selection of estimator features if n_nearest_features is not None, the &lt;code&gt;imputation_order&lt;/code&gt; if &lt;code&gt;random&lt;/code&gt;, and the sampling from posterior if &lt;code&gt;sample_posterior&lt;/code&gt; is True. Use an integer for determinism. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db7139d208134a3c43811feaafd4124e4ee369a8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator used when shuffling the data for probability estimates. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">La semilla del generador de n&amp;uacute;meros pseudoaleatorios que se usa al mezclar los datos para estimaciones de probabilidad. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f1665069fc9c4c9d9dabb36f1931e17db4945528" translate="yes" xml:space="preserve">
          <source>The set of F values.</source>
          <target state="translated">El conjunto de valores F.</target>
        </trans-unit>
        <trans-unit id="d03a062cac2973dfcc9173b5fc93f7520db21987" translate="yes" xml:space="preserve">
          <source>The set of labels can be different for each output variable. For instance, a sample could be assigned &amp;ldquo;pear&amp;rdquo; for an output variable that takes possible values in a finite set of species such as &amp;ldquo;pear&amp;rdquo;, &amp;ldquo;apple&amp;rdquo;; and &amp;ldquo;blue&amp;rdquo; or &amp;ldquo;green&amp;rdquo; for a second output variable that takes possible values in a finite set of colors such as &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;blue&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo;&amp;hellip;</source>
          <target state="translated">El conjunto de etiquetas puede ser diferente para cada variable de salida. Por ejemplo, a una muestra se le podr&amp;iacute;a asignar &quot;pera&quot; para una variable de salida que toma valores posibles en un conjunto finito de especies como &quot;pera&quot;, &quot;manzana&quot;; y &quot;azul&quot; o &quot;verde&quot; para una segunda variable de salida que toma valores posibles en un conjunto finito de colores como &quot;verde&quot;, &quot;rojo&quot;, &quot;azul&quot;, &quot;amarillo&quot; ...</target>
        </trans-unit>
        <trans-unit id="143c46009e14705cc3449ca19bc2f349662d5283" translate="yes" xml:space="preserve">
          <source>The set of labels for each sample such that &lt;code&gt;y[i]&lt;/code&gt; consists of &lt;code&gt;classes_[j]&lt;/code&gt; for each &lt;code&gt;yt[i, j] == 1&lt;/code&gt;.</source>
          <target state="translated">El conjunto de etiquetas para cada muestra de modo que &lt;code&gt;y[i]&lt;/code&gt; consta de &lt;code&gt;classes_[j]&lt;/code&gt; para cada &lt;code&gt;yt[i, j] == 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2d10d69a1c09af3eb9fb75910b7cd4ebd942ac2e" translate="yes" xml:space="preserve">
          <source>The set of labels to include when &lt;code&gt;average != 'binary'&lt;/code&gt;, and their order if &lt;code&gt;average is None&lt;/code&gt;. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">El conjunto de etiquetas que se incluir&amp;aacute;n cuando &lt;code&gt;average != 'binary'&lt;/code&gt; , y su orden si &lt;code&gt;average is None&lt;/code&gt; . Las etiquetas presentes en los datos se pueden excluir, por ejemplo, para calcular un promedio multiclase ignorando una clase negativa mayoritaria, mientras que las etiquetas que no est&amp;aacute;n presentes en los datos dar&amp;aacute;n como resultado 0 componentes en un promedio macro. Para objetivos de etiquetas m&amp;uacute;ltiples, las etiquetas son &amp;iacute;ndices de columna. De forma predeterminada, todas las etiquetas en &lt;code&gt;y_true&lt;/code&gt; e &lt;code&gt;y_pred&lt;/code&gt; se utilizan en orden ordenado.</target>
        </trans-unit>
        <trans-unit id="fc018ce1a1ad3dcbb813a9b943d602142a80bfe5" translate="yes" xml:space="preserve">
          <source>The set of p-values.</source>
          <target state="translated">El conjunto de valores p.</target>
        </trans-unit>
        <trans-unit id="ae935a83c454932c18aabaf5527bf9d40a05884a" translate="yes" xml:space="preserve">
          <source>The set of regressors that will be tested sequentially.</source>
          <target state="translated">El conjunto de regresores que serán probados secuencialmente.</target>
        </trans-unit>
        <trans-unit id="418cd7fd5c32b01bfeb33989472034a267c4e833" translate="yes" xml:space="preserve">
          <source>The shape (Nx, Ny) array of pairwise distances between points in X and Y.</source>
          <target state="translated">La forma (Nx,Ny)de la matriz de distancias en pares entre los puntos en X y Y.</target>
        </trans-unit>
        <trans-unit id="313d9c9325eeb4a6e4291910f1f9fa2e1a31f92e" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;(n_classes-1, n_SV)&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_classes - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_classes - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be8e19650b3e56af8959e9d1bdb9305ca252ca97" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_class - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_class - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="translated">La forma de &lt;code&gt;dual_coef_&lt;/code&gt; es &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; con un dise&amp;ntilde;o algo dif&amp;iacute;cil de entender. Las columnas corresponden a los vectores de soporte involucrados en cualquiera de los &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;uno contra uno&amp;rdquo;. Cada uno de los vectores de soporte se utiliza en clasificadores &lt;code&gt;n_class - 1&lt;/code&gt; . Las entradas &lt;code&gt;n_class - 1&lt;/code&gt; en cada fila corresponden a los coeficientes duales para estos clasificadores.</target>
        </trans-unit>
        <trans-unit id="79705c107a83df8d45ef47d82375b7c707f4d5ae" translate="yes" xml:space="preserve">
          <source>The shape of the result.</source>
          <target state="translated">La forma del resultado.</target>
        </trans-unit>
        <trans-unit id="770a88634d7b4928209cc52a1f8aea4bce68a8e8" translate="yes" xml:space="preserve">
          <source>The shift offset allows a zero threshold for being an outlier. Only available for novelty detection (when novelty is set to True). The argument X is supposed to contain &lt;em&gt;new data&lt;/em&gt;: if X contains a point from training, it considers the later in its own neighborhood. Also, the samples in X are not considered in the neighborhood of any point.</source>
          <target state="translated">El desplazamiento de cambio permite un umbral cero por ser un valor at&amp;iacute;pico. Solo disponible para la detecci&amp;oacute;n de novedades (cuando la novedad se establece en Verdadero). Se supone que el argumento X contiene &lt;em&gt;nuevos datos&lt;/em&gt; : si X contiene un punto del entrenamiento, considera el &amp;uacute;ltimo en su propio vecindario. Adem&amp;aacute;s, las muestras en X no se consideran pr&amp;oacute;ximas a ning&amp;uacute;n punto.</target>
        </trans-unit>
        <trans-unit id="fd58d8b5ba5725b529e01c853ef09676528984ae" translate="yes" xml:space="preserve">
          <source>The shifted opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">El desplazamiento opuesto al Factor Local Atípico de cada muestra de entrada.Cuanto más bajo,más anormal.Las puntuaciones negativas representan los valores atípicos,las positivas representan los valores atípicos.</target>
        </trans-unit>
        <trans-unit id="432ee6647b81dbc5a7cbb417e7251dacced20941" translate="yes" xml:space="preserve">
          <source>The signed distance to the hyperplane (computed as the dot product between the coefficients and the input sample, plus the intercept) is given by &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt;&lt;code&gt;SGDClassifier.decision_function&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13c37fa5739748416ca3f9521f51dec2de533ef6" translate="yes" xml:space="preserve">
          <source>The similarity of two sets of biclusters.</source>
          <target state="translated">La similitud de dos conjuntos de bíceps.</target>
        </trans-unit>
        <trans-unit id="7becf18fe4f5e5f9bf982cb2425cc3a834e0c404" translate="yes" xml:space="preserve">
          <source>The simplest metric &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; model, called &lt;em&gt;absolute MDS&lt;/em&gt;, disparities are defined by \(\hat{d}_{ij} = S_{ij}\). With absolute MDS, the value \(S_{ij}\) should then correspond exactly to the distance between point \(i\) and \(j\) in the embedding point.</source>
          <target state="translated">El modelo de &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt; m&amp;eacute;trico m&amp;aacute;s simple , llamado &lt;em&gt;MDS absoluto&lt;/em&gt; , las disparidades se definen mediante \ (\ hat {d} _ {ij} = S_ {ij} \). Con MDS absoluto, el valor \ (S_ {ij} \) deber&amp;iacute;a corresponder exactamente a la distancia entre el punto \ (i \) y \ (j \) en el punto de inserci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="1d805d3b9d3166d024fed65dbe7dc1ddecd0fb40" translate="yes" xml:space="preserve">
          <source>The simplest possible classifier is the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;nearest neighbor&lt;/a&gt;: given a new observation &lt;code&gt;X_test&lt;/code&gt;, find in the training set (i.e. the data used to train the estimator) the observation with the closest feature vector. (Please see the &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors section&lt;/a&gt; of the online Scikit-learn documentation for more information about this type of classifier.)</source>
          <target state="translated">El clasificador m&amp;aacute;s simple posible es el &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;vecino m&amp;aacute;s cercano&lt;/a&gt; : dada una nueva observaci&amp;oacute;n &lt;code&gt;X_test&lt;/code&gt; , encuentre en el conjunto de entrenamiento (es decir, los datos usados ​​para entrenar al estimador) la observaci&amp;oacute;n con el vector de caracter&amp;iacute;sticas m&amp;aacute;s cercano. (Consulte la &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;secci&amp;oacute;n Vecinos m&amp;aacute;s cercanos&lt;/a&gt; de la documentaci&amp;oacute;n en l&amp;iacute;nea de Scikit-learn para obtener m&amp;aacute;s informaci&amp;oacute;n sobre este tipo de clasificador).</target>
        </trans-unit>
        <trans-unit id="1fac5c5ae1360f0509b6fb6c9bee7a89fd16eea5" translate="yes" xml:space="preserve">
          <source>The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost.</source>
          <target state="translated">La forma más simple de lograr esta reducción de la dimensionalidad es tomando una proyección aleatoria de los datos.Aunque esto permite cierto grado de visualización de la estructura de los datos,la aleatoriedad de la elección deja mucho que desear.En una proyección aleatoria,es probable que se pierda la estructura más interesante dentro de los datos.</target>
        </trans-unit>
        <trans-unit id="a8c7e68caf35057fba3785bb16a14da344816784" translate="yes" xml:space="preserve">
          <source>The simplest way to use cross-validation is to call the &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper function on the estimator and the dataset.</source>
          <target state="translated">La forma m&amp;aacute;s sencilla de utilizar la validaci&amp;oacute;n cruzada es llamar a la funci&amp;oacute;n auxiliar &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; en el estimador y el conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="6b61610397fd4bcbb9699a5ba6221c6a9dfd8212" translate="yes" xml:space="preserve">
          <source>The singular value decomposition, \(A_n = U \Sigma V^\top\), provides the partitions of the rows and columns of \(A\). A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.</source>
          <target state="translated">La descomposición del valor singular,\ ~-A_n=U \Sigma V^top\),proporciona las particiones de las filas y columnas de \ ~-A.Un subconjunto de los vectores singulares de la izquierda da las particiones de las filas,y un subconjunto de los vectores singulares de la derecha da las particiones de las columnas.</target>
        </trans-unit>
        <trans-unit id="f785df3fd21ed481c16151f3b91e613616eec382" translate="yes" xml:space="preserve">
          <source>The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the &lt;code&gt;n_components&lt;/code&gt; variables in the lower-dimensional space.</source>
          <target state="translated">Los valores singulares correspondientes a cada uno de los componentes seleccionados. Los valores singulares son iguales a las 2 normas de las &lt;code&gt;n_components&lt;/code&gt; variables en el espacio de menor dimensi&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="fda37f3c2ceb4ddf1d93860ac5de12a5ffc950e8" translate="yes" xml:space="preserve">
          <source>The size of &lt;code&gt;grid_scores_&lt;/code&gt; is equal to &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt;, where step is the number of features removed at each iteration.</source>
          <target state="translated">El tama&amp;ntilde;o de &lt;code&gt;grid_scores_&lt;/code&gt; es igual a &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt; , donde step es el n&amp;uacute;mero de caracter&amp;iacute;sticas eliminadas en cada iteraci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="2f08a32d3411460a5643fb826528c94534f8d8f2" translate="yes" xml:space="preserve">
          <source>The size of the image that will be reconstructed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9867bd15789365ba5d58a7dbdcd5815e87ddbf26" translate="yes" xml:space="preserve">
          <source>The size of the model with the default parameters is \(O( M * N * log (N) )\), where \(M\) is the number of trees and \(N\) is the number of samples. In order to reduce the size of the model, you can change these parameters: &lt;code&gt;min_samples_split&lt;/code&gt;, &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt; and &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="translated">El tama&amp;ntilde;o del modelo con los par&amp;aacute;metros predeterminados es \ (O (M * N * log (N)) \), donde \ (M \) es el n&amp;uacute;mero de &amp;aacute;rboles y \ (N \) es el n&amp;uacute;mero de muestras. Para reducir el tama&amp;ntilde;o del modelo, puede cambiar estos par&amp;aacute;metros: &lt;code&gt;min_samples_split&lt;/code&gt; , &lt;code&gt;max_leaf_nodes&lt;/code&gt; , &lt;code&gt;max_depth&lt;/code&gt; y &lt;code&gt;min_samples_leaf&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1f17f4e69474d346e8934f03ff8be8d8add88c9f" translate="yes" xml:space="preserve">
          <source>The size of the random matrix to generate.</source>
          <target state="translated">El tamaño de la matriz aleatoria a generar.</target>
        </trans-unit>
        <trans-unit id="b31452681b2b7098990045ccc11256312f76473c" translate="yes" xml:space="preserve">
          <source>The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth &lt;code&gt;h&lt;/code&gt; can capture interactions of order &lt;code&gt;h&lt;/code&gt; . There are two ways in which the size of the individual regression trees can be controlled.</source>
          <target state="translated">El tama&amp;ntilde;o de los alumnos de la base del &amp;aacute;rbol de regresi&amp;oacute;n define el nivel de interacciones variables que puede capturar el modelo de aumento de gradiente. En general, un &amp;aacute;rbol de profundidad &lt;code&gt;h&lt;/code&gt; puede capturar interacciones de orden &lt;code&gt;h&lt;/code&gt; . Hay dos formas de controlar el tama&amp;ntilde;o de los &amp;aacute;rboles de regresi&amp;oacute;n individuales.</target>
        </trans-unit>
        <trans-unit id="eb0fc3f910e5dcbfcfe6af4540bbfd0a452530ef" translate="yes" xml:space="preserve">
          <source>The size of the sample to use when computing the Silhouette Coefficient on a random subset of the data. If &lt;code&gt;sample_size is None&lt;/code&gt;, no sampling is used.</source>
          <target state="translated">El tama&amp;ntilde;o de la muestra que se utilizar&amp;aacute; al calcular el coeficiente de silueta en un subconjunto aleatorio de datos. Si &lt;code&gt;sample_size is None&lt;/code&gt; , no se usa muestreo.</target>
        </trans-unit>
        <trans-unit id="b3605bfba06cab15c1207c4102bf5bbb9eee0a53" translate="yes" xml:space="preserve">
          <source>The size of the set to sample from.</source>
          <target state="translated">El tamaño del conjunto del que se va a tomar la muestra.</target>
        </trans-unit>
        <trans-unit id="bec91be0bb08923b4b038e4efb20c2c584713078" translate="yes" xml:space="preserve">
          <source>The size of the trees can be controlled through the &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt;, and &lt;code&gt;min_samples_leaf&lt;/code&gt; parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5051ed3b169b87ed97be7e20bda0cdf9d82ecae" translate="yes" xml:space="preserve">
          <source>The size, the distance and the shape of clusters may vary upon initialization, perplexity values and does not always convey a meaning.</source>
          <target state="translated">El tamaño,la distancia y la forma de los racimos pueden variar en el momento de la inicialización,lo que genera perplejidad y no siempre transmite un significado.</target>
        </trans-unit>
        <trans-unit id="6830208b16eb851287384293d35fab1e64fb8f9a" translate="yes" xml:space="preserve">
          <source>The skewed chi squared kernel is given by:</source>
          <target state="translated">El núcleo cuadrado del chi sesgado viene dado por:</target>
        </trans-unit>
        <trans-unit id="b37812e3809837f255e9952013df77f9cb7868cc" translate="yes" xml:space="preserve">
          <source>The smaller the Brier score, the better, hence the naming with &amp;ldquo;loss&amp;rdquo;. Across all items in a set N predictions, the Brier score measures the mean squared difference between (1) the predicted probability assigned to the possible outcomes for item i, and (2) the actual outcome. Therefore, the lower the Brier score is for a set of predictions, the better the predictions are calibrated. Note that the Brier score always takes on a value between zero and one, since this is the largest possible difference between a predicted probability (which must be between zero and one) and the actual outcome (which can take on values of only 0 and 1). The Brier loss is composed of refinement loss and calibration loss. The Brier score is appropriate for binary and categorical outcomes that can be structured as true or false, but is inappropriate for ordinal variables which can take on three or more values (this is because the Brier score assumes that all possible outcomes are equivalently &amp;ldquo;distant&amp;rdquo; from one another). Which label is considered to be the positive label is controlled via the parameter pos_label, which defaults to 1. Read more in the &lt;a href=&quot;../calibration#calibration&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bdf0bd9101d435249fe017ac2196fd5049ca1688" translate="yes" xml:space="preserve">
          <source>The smoothing priors \(\alpha \ge 0\) accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting \(\alpha = 1\) is called Laplace smoothing, while \(\alpha &amp;lt; 1\) is called Lidstone smoothing.</source>
          <target state="translated">El suavizado a priors \ (\ alpha \ ge 0 \) tiene en cuenta las caracter&amp;iacute;sticas que no est&amp;aacute;n presentes en las muestras de aprendizaje y evita probabilidades cero en c&amp;aacute;lculos posteriores. La configuraci&amp;oacute;n de \ (\ alpha = 1 \) se denomina suavizado de Laplace, mientras que \ (\ alpha &amp;lt;1 \) se denomina suavizado de Lidstone.</target>
        </trans-unit>
        <trans-unit id="4842d9fce84143997f4f4321a8717acf3b670822" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For L1 penalization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="translated">El solucionador &quot;liblinear&quot; utiliza un algoritmo de descenso de coordenadas (CD) y se basa en la excelente &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;biblioteca&lt;/a&gt; C ++ LIBLINEAR , que se env&amp;iacute;a con scikit-learn. Sin embargo, el algoritmo de CD implementado en liblinear no puede aprender un verdadero modelo multinomial (multiclase); en cambio, el problema de optimizaci&amp;oacute;n se descompone en una forma de &quot;uno frente al resto&quot;, por lo que se entrenan clasificadores binarios separados para todas las clases. Esto sucede bajo el cap&amp;oacute;, por lo que las instancias de &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; que&lt;/a&gt; usan este solucionador se comportan como clasificadores multiclase. Para la penalizaci&amp;oacute;n de L1, &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt; &lt;/a&gt; permite calcular el l&amp;iacute;mite inferior de C para obtener un modelo no &quot;nulo&quot; (todos los pesos de caracter&amp;iacute;sticas a cero).</target>
        </trans-unit>
        <trans-unit id="26f79d036c2795565ba6a21b798f1790f5f71dab" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For \(\ell_1\) regularization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed7150245f4b6e455142137cba3f8af716071c7a" translate="yes" xml:space="preserve">
          <source>The solver for weight optimization.</source>
          <target state="translated">El solucionador para la optimización del peso.</target>
        </trans-unit>
        <trans-unit id="7622dc087d4f210bd1e051afc82030dfdfb796a8" translate="yes" xml:space="preserve">
          <source>The solver is selected by a default policy based on &lt;code&gt;X.shape&lt;/code&gt; and &lt;code&gt;n_components&lt;/code&gt;: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient &amp;lsquo;randomized&amp;rsquo; method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7be0c6e2677e69b218f8a95f391cfbcac99c36a1" translate="yes" xml:space="preserve">
          <source>The solvers implemented in the class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; are &amp;ldquo;liblinear&amp;rdquo;, &amp;ldquo;newton-cg&amp;rdquo;, &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;saga&amp;rdquo;:</source>
          <target state="translated">Los solucionadores implementados en la clase &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; son &quot;liblinear&quot;, &quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;sag&quot; y &quot;saga&quot;:</target>
        </trans-unit>
        <trans-unit id="2562620e10231c5093b1e84475f4d077e3e41917" translate="yes" xml:space="preserve">
          <source>The sought maximum memory for temporary distance matrix chunks. When None (default), the value of &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; is used.</source>
          <target state="translated">La memoria m&amp;aacute;xima buscada para fragmentos de matriz de distancia temporal. Cuando Ninguno (predeterminado), se &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; el valor de sklearn.get_config () ['working_memory'] .</target>
        </trans-unit>
        <trans-unit id="d35f1b775d7d16bbabc524099db10f7048897e6d" translate="yes" xml:space="preserve">
          <source>The source can also be found &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;on Github&lt;/a&gt;.</source>
          <target state="translated">La fuente tambi&amp;eacute;n se puede encontrar &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;en Github&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="351456aed7b7d0fc1d0034839135c70dde192f05" translate="yes" xml:space="preserve">
          <source>The source of this tutorial can be found within your scikit-learn folder:</source>
          <target state="translated">La fuente de este tutorial se encuentra en su carpeta de aprendizaje de ciencias:</target>
        </trans-unit>
        <trans-unit id="aa3459bc05f5ce02810c5dff6ad6cfbeb1ca9a04" translate="yes" xml:space="preserve">
          <source>The spacing between points of the grid, in degrees</source>
          <target state="translated">El espacio entre los puntos de la cuadrícula,en grados</target>
        </trans-unit>
        <trans-unit id="464028094b69433f3db44d7a263916deccf86cb6" translate="yes" xml:space="preserve">
          <source>The sparse code factor in the matrix factorization.</source>
          <target state="translated">El factor de código escaso en la factorización de la matriz.</target>
        </trans-unit>
        <trans-unit id="f9d9c0a7d6ff3b6246478d922234360e15ed6ab9" translate="yes" xml:space="preserve">
          <source>The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-zero items (X).</source>
          <target state="translated">El código es tan escaso que cada columna de esta matriz tiene exactamente n_nonzero_coefs elementos que no son cero (X).</target>
        </trans-unit>
        <trans-unit id="ee7d500960f94d10a937d21e436dcc2adbbc9a2a" translate="yes" xml:space="preserve">
          <source>The sparse codes</source>
          <target state="translated">Los códigos dispersos</target>
        </trans-unit>
        <trans-unit id="98979bbd88c557cc69074b91f3080d6fb357dded" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results from the dense implementation, due to a shrunk learning rate for the intercept. See &lt;a href=&quot;#implementation-details&quot;&gt;Implementation details&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e1ce62165b4e0dbd0b6b1199e83c88e1c88959d" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results than the dense implementation due to a shrunk learning rate for the intercept.</source>
          <target state="translated">La implementación dispersa produce resultados ligeramente diferentes que la implementación densa,debido a la disminución de la tasa de aprendizaje de la intercepción.</target>
        </trans-unit>
        <trans-unit id="e4890de0c56a0e7645deea9088355f84adac629f" translate="yes" xml:space="preserve">
          <source>The sparse vector</source>
          <target state="translated">El vector disperso</target>
        </trans-unit>
        <trans-unit id="df0fd56f5b70016c4466d03cfc8859f7c3ea7663" translate="yes" xml:space="preserve">
          <source>The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself.</source>
          <target state="translated">La escasez se impone en realidad sobre el factor coloso de la matriz.Por lo tanto,el alfa no se traduce directamente en la fracción de relleno de la propia matriz.</target>
        </trans-unit>
        <trans-unit id="56d99fb198a4da207bde97c2ceeeb3653451f496" translate="yes" xml:space="preserve">
          <source>The sparsity-inducing \(\ell_1\) norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter &lt;code&gt;alpha&lt;/code&gt;. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero.</source>
          <target state="translated">La norma \ (\ ell_1 \) que induce a la dispersi&amp;oacute;n tambi&amp;eacute;n evita el ruido de los componentes de aprendizaje cuando hay pocas muestras de entrenamiento disponibles. El grado de penalizaci&amp;oacute;n (y por lo tanto la escasez) se puede ajustar a trav&amp;eacute;s del hiperpar&amp;aacute;metro &lt;code&gt;alpha&lt;/code&gt; . Los valores peque&amp;ntilde;os conducen a una factorizaci&amp;oacute;n suavemente regularizada, mientras que los valores m&amp;aacute;s grandes reducen muchos coeficientes a cero.</target>
        </trans-unit>
        <trans-unit id="1e3f3420100e85d456b50524681a1e1c7bbc338a" translate="yes" xml:space="preserve">
          <source>The split code for a single sample has length &lt;code&gt;2 * n_components&lt;/code&gt; and is constructed using the following rule: First, the regular code of length &lt;code&gt;n_components&lt;/code&gt; is computed. Then, the first &lt;code&gt;n_components&lt;/code&gt; entries of the &lt;code&gt;split_code&lt;/code&gt; are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative.</source>
          <target state="translated">El c&amp;oacute;digo dividido para una sola muestra tiene una longitud de &lt;code&gt;2 * n_components&lt;/code&gt; y se construye usando la siguiente regla: Primero, se calcula el c&amp;oacute;digo regular de longitud &lt;code&gt;n_components&lt;/code&gt; . Luego, las primeras &lt;code&gt;n_components&lt;/code&gt; entradas del &lt;code&gt;split_code&lt;/code&gt; se rellenan con la parte positiva del vector de c&amp;oacute;digo regular. La segunda mitad del c&amp;oacute;digo dividido se llena con la parte negativa del vector de c&amp;oacute;digo, solo con un signo positivo. Por lo tanto, split_code no es negativo.</target>
        </trans-unit>
        <trans-unit id="c55afca5a7a433d8d42b73a1df4af26f8dc38160" translate="yes" xml:space="preserve">
          <source>The stacked regressor will combine the strengths of the different regressors. However, we also see that training the stacked regressor is much more computationally expensive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="619ea10ad996c929a97e389d99c020b2211157d4" translate="yes" xml:space="preserve">
          <source>The standard LLE algorithm comprises three stages:</source>
          <target state="translated">El algoritmo estándar del LLE comprende tres etapas:</target>
        </trans-unit>
        <trans-unit id="a5bdc246aecc7e3bec9121428d3f9c450814299f" translate="yes" xml:space="preserve">
          <source>The standard deviation of the clusters.</source>
          <target state="translated">La desviación estándar de los grupos.</target>
        </trans-unit>
        <trans-unit id="b61516300476f785958503bfbc63e8e96de11d18" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise applied to the output.</source>
          <target state="translated">La desviación estándar del ruido gaussiano aplicada a la salida.</target>
        </trans-unit>
        <trans-unit id="e5c05e9131e22b6718043e99a2bae1b92b538981" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise.</source>
          <target state="translated">La desviación estándar del ruido gaussiano.</target>
        </trans-unit>
        <trans-unit id="0d973eb25a07dcdadcaeac90be3869c3ac64dae0" translate="yes" xml:space="preserve">
          <source>The standard score of a sample &lt;code&gt;x&lt;/code&gt; is calculated as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3689d32cff3e62dded279fb6b657e94942cdc7dc" translate="yes" xml:space="preserve">
          <source>The stepwise interpolating function that covers the input domain &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">La funci&amp;oacute;n de paso a paso de interpolaci&amp;oacute;n que cubre el dominio de entrada &lt;code&gt;X&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a2cf8fe1dea678ae31186ca36392deb4d997cfe8" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d8cf8d82bcb0bc25727f81a7a8d626fa0a70639" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.</source>
          <target state="translated">El criterio de parada. Si no es None, las iteraciones se detendr&amp;aacute;n cuando (loss&amp;gt; previous_loss - tol). Por defecto es Ninguno. El valor predeterminado es 1e-3 desde 0,21.</target>
        </trans-unit>
        <trans-unit id="b0435766401c2c671f3d0dbc7668ffa0a2dc3aba" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, training will stop when (loss &amp;gt; best_loss - tol) for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba1aa4a75fb51f2479b447aa001f5e6386a1f799" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization.</source>
          <target state="translated">La estrategia a utilizar para asignar etiquetas en el espacio de incrustación.Hay dos maneras de asignar etiquetas después de la incrustación laplaciana.Se pueden aplicar los medios k y es una opción popular.Pero también puede ser sensible a la inicialización.La discretización es otro enfoque que es menos sensible a la inicialización aleatoria.</target>
        </trans-unit>
        <trans-unit id="0a02a9e0399d776c0fdc23972d432af0d079552e" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the &amp;lsquo;Multiclass spectral clustering&amp;rsquo; paper referenced below for more details on the discretization approach.</source>
          <target state="translated">La estrategia a utilizar para asignar etiquetas en el espacio de incrustaci&amp;oacute;n. Hay dos formas de asignar etiquetas despu&amp;eacute;s de la incrustaci&amp;oacute;n laplaciana. k-means se puede aplicar y es una opci&amp;oacute;n popular. Pero tambi&amp;eacute;n puede ser sensible a la inicializaci&amp;oacute;n. La discretizaci&amp;oacute;n es otro enfoque que es menos sensible a la inicializaci&amp;oacute;n aleatoria. Consulte el documento 'Agrupaci&amp;oacute;n espectral multiclase' al que se hace referencia a continuaci&amp;oacute;n para obtener m&amp;aacute;s detalles sobre el enfoque de discretizaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="067b32fbfa4aafe8139a50a2cd0dc62d48f6ce7c" translate="yes" xml:space="preserve">
          <source>The strategy used to choose the split at each node. Supported strategies are &amp;ldquo;best&amp;rdquo; to choose the best split and &amp;ldquo;random&amp;rdquo; to choose the best random split.</source>
          <target state="translated">La estrategia utilizada para elegir la divisi&amp;oacute;n en cada nodo. Las estrategias admitidas son &quot;mejores&quot; para elegir la mejor divisi&amp;oacute;n y &quot;aleatorias&quot; para elegir la mejor divisi&amp;oacute;n aleatoria.</target>
        </trans-unit>
        <trans-unit id="e4b87188ae01dfb2ea23e8aa9287a121677cbc35" translate="yes" xml:space="preserve">
          <source>The strength of recall versus precision in the F-score.</source>
          <target state="translated">La fuerza de la memoria versus la precisión en el puntaje F.</target>
        </trans-unit>
        <trans-unit id="9380c762fbcadf9826438d5ff503ef39938c2642" translate="yes" xml:space="preserve">
          <source>The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.</source>
          <target state="translated">La fuerza del algoritmo de la LOF es que tiene en cuenta las propiedades locales y globales de los conjuntos de datos:puede funcionar bien incluso en conjuntos de datos en los que las muestras anormales tienen diferentes densidades subyacentes.La pregunta no es cuán aislada está la muestra,sino cuán aislada está con respecto al vecindario circundante.</target>
        </trans-unit>
        <trans-unit id="ef559a266ab9339add9416970528df4288b9fe07" translate="yes" xml:space="preserve">
          <source>The string to decode</source>
          <target state="translated">La cadena a decodificar</target>
        </trans-unit>
        <trans-unit id="3b937bbc7005ed347df9fefe128290b88cb139ff" translate="yes" xml:space="preserve">
          <source>The string to decode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71af025de90c113777cd083adeb4a7dc41c643ae" translate="yes" xml:space="preserve">
          <source>The string value &amp;ldquo;auto&amp;rdquo; determines whether y should increase or decrease based on the Spearman correlation estimate&amp;rsquo;s sign.</source>
          <target state="translated">El valor de la cadena &quot;auto&quot; determina si y debe aumentar o disminuir seg&amp;uacute;n el signo de la estimaci&amp;oacute;n de correlaci&amp;oacute;n de Spearman.</target>
        </trans-unit>
        <trans-unit id="68913ceeaf791c0f89f5da0e6ea267a6c64604e5" translate="yes" xml:space="preserve">
          <source>The submatrix corresponding to bicluster i.</source>
          <target state="translated">La submatriz correspondiente al bicluster i.</target>
        </trans-unit>
        <trans-unit id="a10436d8ac7a1230da5ccfb4c02351e0bfbb4701" translate="yes" xml:space="preserve">
          <source>The subset of drawn features for each base estimator.</source>
          <target state="translated">El subconjunto de características dibujadas para cada estimador base.</target>
        </trans-unit>
        <trans-unit id="57675bd8615ef838a99f713d239feb9810e82664" translate="yes" xml:space="preserve">
          <source>The subset of drawn samples for each base estimator.</source>
          <target state="translated">El subconjunto de muestras extraídas para cada estimador de base.</target>
        </trans-unit>
        <trans-unit id="fce75057d74e03086c8b482c64f0a007dbeb6ebe" translate="yes" xml:space="preserve">
          <source>The sum of all predictions also confirms the calibration issue of the &lt;code&gt;Ridge&lt;/code&gt; model: it under-estimates by more than 3% the total number of claims in the test set while the other three models can approximately recover the total number of claims of the test portfolio.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58b06737b1ee81d6af2156c3790929db0bc0c464" translate="yes" xml:space="preserve">
          <source>The sum of the features (number of words if documents) is drawn from a Poisson distribution with this expected value.</source>
          <target state="translated">La suma de las características (número de palabras si se trata de documentos)se extrae de una distribución de Poisson con este valor esperado.</target>
        </trans-unit>
        <trans-unit id="690aa5752a21a529c84a7d2bed72d6956dfbf2f1" translate="yes" xml:space="preserve">
          <source>The support is the number of occurrences of each class in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">El soporte es el n&amp;uacute;mero de ocurrencias de cada clase en &lt;code&gt;y_true&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b8f29f24bf343b8a8c6a9702bbb3373c30b3886a" translate="yes" xml:space="preserve">
          <source>The support vector machines in scikit-learn support both dense (&lt;code&gt;numpy.ndarray&lt;/code&gt; and convertible to that by &lt;code&gt;numpy.asarray&lt;/code&gt;) and sparse (any &lt;code&gt;scipy.sparse&lt;/code&gt;) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered &lt;code&gt;numpy.ndarray&lt;/code&gt; (dense) or &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (sparse) with &lt;code&gt;dtype=float64&lt;/code&gt;.</source>
          <target state="translated">Las m&amp;aacute;quinas de vectores de soporte en scikit-learn admiten vectores de muestra densos ( &lt;code&gt;numpy.ndarray&lt;/code&gt; y convertibles a eso por &lt;code&gt;numpy.asarray&lt;/code&gt; ) y dispersos (cualquier &lt;code&gt;scipy.sparse&lt;/code&gt; ) como entrada. Sin embargo, para usar una SVM para hacer predicciones para datos escasos, debe haber encajado en dichos datos. Para un rendimiento &amp;oacute;ptimo, use &lt;code&gt;numpy.ndarray&lt;/code&gt; (denso) ordenado en C o &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (disperso) con &lt;code&gt;dtype=float64&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b8e93494175fc764f9336519319d2c72af2d3469" translate="yes" xml:space="preserve">
          <source>The target features for which the partial dependecy should be computed (size should be smaller than 3 for visual renderings).</source>
          <target state="translated">Las características del objetivo para las que se debe computar la dependencia parcial (el tamaño debe ser inferior a 3 para las representaciones visuales).</target>
        </trans-unit>
        <trans-unit id="1676d735c85e09d0307a407a6c8fc0482ea454c1" translate="yes" xml:space="preserve">
          <source>The target features for which to create the PDPs. If features[i] is an int or a string, a one-way PDP is created; if features[i] is a tuple, a two-way PDP is created. Each tuple must be of size 2. if any entry is a string, then it must be in &lt;code&gt;feature_names&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6694d34ee3bb1ae73f71f475a9064ea6bb5aa61" translate="yes" xml:space="preserve">
          <source>The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.</source>
          <target state="translated">El objetivo se predice mediante la interpolación local de los objetivos asociados de los vecinos más cercanos en el conjunto de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="fd8f31b4b903aeb83268043a3b52655fbcfeb540" translate="yes" xml:space="preserve">
          <source>The target labels (integer index).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9040adccd40d7354a58b523dff53abedde8f2d61" translate="yes" xml:space="preserve">
          <source>The target labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="836251639a1d2dd67d806e7910ad6656af016095" translate="yes" xml:space="preserve">
          <source>The target values (class labels in classification, real numbers in regression).</source>
          <target state="translated">Los valores objetivo (etiquetas de clase en la clasificación,números reales en la regresión).</target>
        </trans-unit>
        <trans-unit id="581ab16d01401b52de3a8a770522ae3215b0d2a3" translate="yes" xml:space="preserve">
          <source>The target values (class labels) as integers or strings.</source>
          <target state="translated">Los valores objetivo (etiquetas de clase)como números enteros o cadenas.</target>
        </trans-unit>
        <trans-unit id="c42151a6b9abff52d7ec8ba07a5200409a48a3c6" translate="yes" xml:space="preserve">
          <source>The target values (class labels).</source>
          <target state="translated">Los valores objetivo (etiquetas de clase).</target>
        </trans-unit>
        <trans-unit id="af111c076ceef9b210aa6e236368271feda238ba" translate="yes" xml:space="preserve">
          <source>The target values (integers that correspond to classes in classification, real numbers in regression).</source>
          <target state="translated">Los valores objetivo (números enteros que corresponden a clases en la clasificación,números reales en la regresión).</target>
        </trans-unit>
        <trans-unit id="94e263fd180ba7303394b0318de33e42ec7bd528" translate="yes" xml:space="preserve">
          <source>The target values (real numbers).</source>
          <target state="translated">Los valores objetivo (números reales).</target>
        </trans-unit>
        <trans-unit id="4f52bd7c58bfce68aafb35b3e2f5dd531ddc572f" translate="yes" xml:space="preserve">
          <source>The target values (real numbers). Use &lt;code&gt;dtype=np.float64&lt;/code&gt; and &lt;code&gt;order='C'&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">Los valores objetivo (n&amp;uacute;meros reales). Utilice &lt;code&gt;dtype=np.float64&lt;/code&gt; y &lt;code&gt;order='C'&lt;/code&gt; para obtener la m&amp;aacute;xima eficiencia.</target>
        </trans-unit>
        <trans-unit id="53a447e32fbe96637e9d7be27a641b24353eba6e" translate="yes" xml:space="preserve">
          <source>The target values.</source>
          <target state="translated">Los valores del objetivo.</target>
        </trans-unit>
        <trans-unit id="863f0df40c91ba7090e8eb769163050f217d7bc5" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems.</source>
          <target state="translated">La variable objetivo para los problemas de aprendizaje supervisado.</target>
        </trans-unit>
        <trans-unit id="f60efda845afa3f0dc7adc040cd9b2450fbcc2aa" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems. Stratification is done based on the y labels.</source>
          <target state="translated">La variable objetivo para los problemas de aprendizaje supervisado.La estratificación se hace en base a las etiquetas y.</target>
        </trans-unit>
        <trans-unit id="aa20ad6bc67663b59dda9b76a4a065ca1f86af8f" translate="yes" xml:space="preserve">
          <source>The target variable is the median house value for California districts.</source>
          <target state="translated">La variable objetivo es el valor medio de la vivienda para los distritos de California.</target>
        </trans-unit>
        <trans-unit id="40a696d29fc2e13fd302babe7b309a6f769a208a" translate="yes" xml:space="preserve">
          <source>The target variable to try to predict in the case of supervised learning.</source>
          <target state="translated">La variable objetivo a tratar de predecir en el caso del aprendizaje supervisado.</target>
        </trans-unit>
        <trans-unit id="a34872d2673c11b79098f51c73d69310c6a49da3" translate="yes" xml:space="preserve">
          <source>The task at hand is to predict disease progression from physiological variables.</source>
          <target state="translated">La tarea que tenemos entre manos es predecir la progresión de la enfermedad a partir de variables fisiológicas.</target>
        </trans-unit>
        <trans-unit id="609ac3008273ee503e4809fb5a54a64b3f8be85d" translate="yes" xml:space="preserve">
          <source>The ten features are standard independent Gaussian and the target &lt;code&gt;y&lt;/code&gt; is defined by:</source>
          <target state="translated">Las diez caracter&amp;iacute;sticas son gaussianas independientes est&amp;aacute;ndar y el objetivo &lt;code&gt;y&lt;/code&gt; est&amp;aacute; definido por:</target>
        </trans-unit>
        <trans-unit id="b7581ec7a4d469cfac096612cf94e3958274d6aa" translate="yes" xml:space="preserve">
          <source>The term &amp;ldquo;discrete features&amp;rdquo; is used instead of naming them &amp;ldquo;categorical&amp;rdquo;, because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that.</source>
          <target state="translated">El t&amp;eacute;rmino &quot;caracter&amp;iacute;sticas discretas&quot; se utiliza en lugar de denominarlas &quot;categ&amp;oacute;ricas&quot;, porque describe la esencia con mayor precisi&amp;oacute;n. Por ejemplo, las intensidades de p&amp;iacute;xeles de una imagen son caracter&amp;iacute;sticas discretas (pero dif&amp;iacute;cilmente categ&amp;oacute;ricas) y obtendr&amp;aacute; mejores resultados si las marca como tales. Tambi&amp;eacute;n tenga en cuenta que tratar una variable continua como discreta y viceversa generalmente dar&amp;aacute; resultados incorrectos, as&amp;iacute; que est&amp;eacute; atento a eso.</target>
        </trans-unit>
        <trans-unit id="2014676021228002e8224c1f06ee12bb4596d24b" translate="yes" xml:space="preserve">
          <source>The term \((x-\mu_k)^t \Sigma^{-1} (x-\mu_k)\) corresponds to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mahalanobis_distance&quot;&gt;Mahalanobis Distance&lt;/a&gt; between the sample \(x\) and the mean \(\mu_k\). The Mahalanobis distance tells how close \(x\) is from \(\mu_k\), while also accounting for the variance of each feature. We can thus interpret LDA as assigning \(x\) to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1748689c159b7c280435a293a84c60a8fa11ddf6" translate="yes" xml:space="preserve">
          <source>The test points for the data. Same format as the training data.</source>
          <target state="translated">Los puntos de prueba para los datos.El mismo formato que los datos de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="c4412981e13c1e09172f9e595c07a27a82a32abb" translate="yes" xml:space="preserve">
          <source>The testing set indices for that split.</source>
          <target state="translated">Las pruebas establecieron índices para esa división.</target>
        </trans-unit>
        <trans-unit id="c079148b8f468ab34a1c911c5b93e7fb1e4fbf43" translate="yes" xml:space="preserve">
          <source>The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; takes an &lt;code&gt;encoding&lt;/code&gt; parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (&lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt;).</source>
          <target state="translated">Los extractores de funciones de texto en scikit-learn saben c&amp;oacute;mo decodificar archivos de texto, pero solo si usted les dice en qu&amp;eacute; codificaci&amp;oacute;n est&amp;aacute;n los archivos. &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; toma un par&amp;aacute;metro de &lt;code&gt;encoding&lt;/code&gt; para este prop&amp;oacute;sito. Para archivos de texto modernos, la codificaci&amp;oacute;n correcta es probablemente UTF-8, que por lo tanto es la predeterminada ( &lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="5da204cc914d9d1b370d2a7e3d3f9fed2399ad38" translate="yes" xml:space="preserve">
          <source>The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow.</source>
          <target state="translated">La teoría dice que para lograr la consistencia de la predicción,el parámetro de penalización debe mantenerse constante a medida que el número de muestras crece.</target>
        </trans-unit>
        <trans-unit id="ed8c4048d538066672a56cef623687584e4d51a1" translate="yes" xml:space="preserve">
          <source>The third figure compares kernel density estimates for a distribution of 100 samples in 1 dimension. Though this example uses 1D distributions, kernel density estimation is easily and efficiently extensible to higher dimensions as well.</source>
          <target state="translated">La tercera figura compara las estimaciones de la densidad del núcleo para una distribución de 100 muestras en una dimensión.Aunque este ejemplo utiliza distribuciones 1D,la estimación de la densidad del núcleo es fácil y eficiente de extender a dimensiones mayores también.</target>
        </trans-unit>
        <trans-unit id="0742a25a1bc73abd7670f1b33a5e8f933c99aa55" translate="yes" xml:space="preserve">
          <source>The third model is also a Bayesian Gaussian mixture model with a Dirichlet process prior but this time the value of the concentration prior is higher giving the model more liberty to model the fine-grained structure of the data. The result is a mixture with a larger number of active components that is similar to the first model where we arbitrarily decided to fix the number of components to 10.</source>
          <target state="translated">El tercer modelo es también un modelo de mezcla bayesiana gaussiana con un proceso Dirichlet previo,pero esta vez el valor de la concentración previa es mayor,lo que da al modelo más libertad para modelar la estructura de grano fino de los datos.El resultado es una mezcla con un mayor número de componentes activos que es similar al primer modelo en el que arbitrariamente decidimos fijar el número de componentes en 10.</target>
        </trans-unit>
        <trans-unit id="f1f7cd9ed7ac82273a71a8430edb1e09f61b8cab" translate="yes" xml:space="preserve">
          <source>The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If &amp;ldquo;median&amp;rdquo; (resp. &amp;ldquo;mean&amp;rdquo;), then the &lt;code&gt;threshold&lt;/code&gt; value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., &amp;ldquo;1.25*mean&amp;rdquo;) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, &amp;ldquo;mean&amp;rdquo; is used by default.</source>
          <target state="translated">El valor de umbral que se utilizar&amp;aacute; para la selecci&amp;oacute;n de funciones. Los rasgos cuya importancia es mayor o igual se mantienen mientras que los dem&amp;aacute;s se descartan. Si es &quot;mediana&quot; (resp. &quot;Media&quot;), entonces el valor &lt;code&gt;threshold&lt;/code&gt; es la mediana (resp. La media) de las caracter&amp;iacute;sticas importantes. Tambi&amp;eacute;n se puede utilizar un factor de escala (por ejemplo, &quot;1,25 * media&quot;). Si Ninguno y si el estimador tiene una penalizaci&amp;oacute;n de par&amp;aacute;metro establecida en l1, ya sea expl&amp;iacute;cita o impl&amp;iacute;citamente (por ejemplo, Lasso), el umbral utilizado es 1e-5. De lo contrario, se utiliza &quot;mean&quot; de forma predeterminada.</target>
        </trans-unit>
        <trans-unit id="91a1a785c1bc7a11d4e20e6e119c885bf4142111" translate="yes" xml:space="preserve">
          <source>The threshold value used for feature selection.</source>
          <target state="translated">El valor umbral utilizado para la selección de características.</target>
        </trans-unit>
        <trans-unit id="5a48126d50b83afd18e220a4fcf0cc7ffc7180d9" translate="yes" xml:space="preserve">
          <source>The time complexity of this implementation is &lt;code&gt;O(d ** 2)&lt;/code&gt; assuming d ~ n_features ~ n_components.</source>
          <target state="translated">La complejidad temporal de esta implementaci&amp;oacute;n es &lt;code&gt;O(d ** 2)&lt;/code&gt; asumiendo d ~ n_features ~ n_components.</target>
        </trans-unit>
        <trans-unit id="ff8097e0ec52614ae168c4cd444f04e3f78b14e2" translate="yes" xml:space="preserve">
          <source>The time for fitting the estimator on the train set for each cv split.</source>
          <target state="translated">El tiempo para ajustar el estimador en el tren establecido para cada división de cv.</target>
        </trans-unit>
        <trans-unit id="dc0535c66e14595ad26a449dc475b0e83c5091ba" translate="yes" xml:space="preserve">
          <source>The time for scoring the estimator on the test set for each cv split. (Note time for scoring on the train set is not included even if &lt;code&gt;return_train_score&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">El tiempo para calificar el estimador en el conjunto de prueba para cada divisi&amp;oacute;n de cv. (Tenga en cuenta que el tiempo de puntuaci&amp;oacute;n en el conjunto de trenes no se incluye incluso si &lt;code&gt;return_train_score&lt;/code&gt; se establece en &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1cd8d7a025bee860396ab665c8f7316a009f2f5a" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;.</source>
          <target state="translated">La tolerancia para el solucionador de red el&amp;aacute;stica utilizada para calcular la direcci&amp;oacute;n de descenso. Este par&amp;aacute;metro controla la precisi&amp;oacute;n de la direcci&amp;oacute;n de b&amp;uacute;squeda para una actualizaci&amp;oacute;n de columna determinada, no de la estimaci&amp;oacute;n general del par&amp;aacute;metro. Solo se usa para mode = 'cd'.</target>
        </trans-unit>
        <trans-unit id="1c748ca3956e12feeccc0881efb2e7b986b52b0e" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;. Range is (0, inf].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54800ee92ded7e21b226653650e94d2e245f5dc0" translate="yes" xml:space="preserve">
          <source>The tolerance for the optimization: if the updates are smaller than &lt;code&gt;tol&lt;/code&gt;, the optimization code checks the dual gap for optimality and continues until it is smaller than &lt;code&gt;tol&lt;/code&gt;.</source>
          <target state="translated">La tolerancia para la optimizaci&amp;oacute;n: si las actualizaciones son m&amp;aacute;s peque&amp;ntilde;as que &lt;code&gt;tol&lt;/code&gt; , el c&amp;oacute;digo de optimizaci&amp;oacute;n verifica la optimizaci&amp;oacute;n del espacio dual y contin&amp;uacute;a hasta que es m&amp;aacute;s peque&amp;ntilde;o que &lt;code&gt;tol&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7abdf76511114d1a589dafaf8c3b9fea94410d4e" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped.</source>
          <target state="translated">La tolerancia para declarar la convergencia:si la brecha dual baja de este valor,las iteraciones se detienen.</target>
        </trans-unit>
        <trans-unit id="c8679f3dc5795753fd4b3924c7477d6451516504" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. Range is (0, inf].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1174cbfc6c5cd8bb9e51e269de526360c060c73a" translate="yes" xml:space="preserve">
          <source>The tomography projection operation is a linear transformation. In addition to the data-fidelity term corresponding to a linear regression, we penalize the L1 norm of the image to account for its sparsity. The resulting optimization problem is called the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;. We use the class &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt;&lt;/a&gt;, that uses the coordinate descent algorithm. Importantly, this implementation is more computationally efficient on a sparse matrix, than the projection operator used here.</source>
          <target state="translated">La operaci&amp;oacute;n de proyecci&amp;oacute;n de tomograf&amp;iacute;a es una transformaci&amp;oacute;n lineal. Adem&amp;aacute;s del t&amp;eacute;rmino de fidelidad de datos correspondiente a una regresi&amp;oacute;n lineal, penalizamos la norma L1 de la imagen para dar cuenta de su escasez. El problema de optimizaci&amp;oacute;n resultante se llama &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lazo&lt;/a&gt; . Usamos la clase &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt; &lt;/a&gt; , que usa el algoritmo de descenso de coordenadas. Es importante destacar que esta implementaci&amp;oacute;n es m&amp;aacute;s eficiente computacionalmente en una matriz dispersa que el operador de proyecci&amp;oacute;n que se usa aqu&amp;iacute;.</target>
        </trans-unit>
        <trans-unit id="6067d7bbaca20238e188da5c4c1f4f9b915cbe46" translate="yes" xml:space="preserve">
          <source>The total number of features.</source>
          <target state="translated">El número total de características.</target>
        </trans-unit>
        <trans-unit id="52b059ac9ef2b76cd7b57a438578db960faf18a1" translate="yes" xml:space="preserve">
          <source>The total number of features. These comprise &lt;code&gt;n_informative&lt;/code&gt; informative features, &lt;code&gt;n_redundant&lt;/code&gt; redundant features, &lt;code&gt;n_repeated&lt;/code&gt; duplicated features and &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; useless features drawn at random.</source>
          <target state="translated">El n&amp;uacute;mero total de funciones. Estos comprenden &lt;code&gt;n_informative&lt;/code&gt; caracter&amp;iacute;sticas informativas informativas, &lt;code&gt;n_redundant&lt;/code&gt; caracter&amp;iacute;sticas redundantes redundantes, &lt;code&gt;n_repeated&lt;/code&gt; caracter&amp;iacute;sticas duplicadas &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; y n_features-n_informative-n_redundant-n_repeated caracter&amp;iacute;sticas in&amp;uacute;tiles extra&amp;iacute;das al azar.</target>
        </trans-unit>
        <trans-unit id="1b95194d002b1fa90370f5ac460e11ae527d46c4" translate="yes" xml:space="preserve">
          <source>The total number of input features.</source>
          <target state="translated">El número total de características de entrada.</target>
        </trans-unit>
        <trans-unit id="75f4c14304ea0320f6751402bd1abbcf764fb2d4" translate="yes" xml:space="preserve">
          <source>The total number of points equally divided among classes.</source>
          <target state="translated">El número total de puntos dividido equitativamente entre las clases.</target>
        </trans-unit>
        <trans-unit id="6fd1a66b2c352f2c105828a73ec9201a20677da3" translate="yes" xml:space="preserve">
          <source>The total number of points generated.</source>
          <target state="translated">El número total de puntos generados.</target>
        </trans-unit>
        <trans-unit id="b463685bc7194f4e1bd9c9e9566855d8af2442c3" translate="yes" xml:space="preserve">
          <source>The total number of points generated. If odd, the inner circle will have one point more than the outer circle.</source>
          <target state="translated">El número total de puntos generados.Si es impar,el círculo interior tendrá un punto más que el círculo exterior.</target>
        </trans-unit>
        <trans-unit id="30df1d74b9688e22831b35a50354a3af5ea3a9b9" translate="yes" xml:space="preserve">
          <source>The total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features.</source>
          <target state="translated">El número total de características de salida de polinomios.El número de características de salida se calcula por iteración sobre todas las combinaciones de tamaño adecuado de las características de entrada.</target>
        </trans-unit>
        <trans-unit id="2341cd15b4f720c4e4ca39627124f453602ba043" translate="yes" xml:space="preserve">
          <source>The traditional way to compute the principal eigenvector is to use the power iteration method:</source>
          <target state="translated">La forma tradicional de calcular el vector principal es usar el método de iteración de energía:</target>
        </trans-unit>
        <trans-unit id="486bc8b3be25b906a521777296790c0e7db3392b" translate="yes" xml:space="preserve">
          <source>The training algorithm implemented in &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt; is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:</source>
          <target state="translated">El algoritmo de entrenamiento implementado en &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; se conoce como m&amp;aacute;xima verosimilitud estoc&amp;aacute;stica (SML) o divergencia contrastante persistente (PCD). La optimizaci&amp;oacute;n de la probabilidad m&amp;aacute;xima directamente no es factible debido a la forma de la probabilidad de los datos:</target>
        </trans-unit>
        <trans-unit id="03936b9df8c7a04402a186159fb53bde128b3907" translate="yes" xml:space="preserve">
          <source>The training data</source>
          <target state="translated">Los datos de entrenamiento</target>
        </trans-unit>
        <trans-unit id="a7dac9ee1d012d3a04e96a076bd4bb0255f4fa3a" translate="yes" xml:space="preserve">
          <source>The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.</source>
          <target state="translated">Los datos de entrenamiento contienen valores atípicos que se definen como observaciones que están lejos de las demás.Los estimadores de detección de valores atípicos intentan así ajustarse a las regiones en las que los datos de formación están más concentrados,ignorando las observaciones desviadas.</target>
        </trans-unit>
        <trans-unit id="58b640148d7ae1e7f191ee9d800aaef902a6aef0" translate="yes" xml:space="preserve">
          <source>The training data is not polluted by outliers and we are interested in detecting whether a &lt;strong&gt;new&lt;/strong&gt; observation is an outlier. In this context an outlier is also called a novelty.</source>
          <target state="translated">Los datos de entrenamiento no est&amp;aacute;n contaminados por valores at&amp;iacute;picos y estamos interesados ​​en detectar si una &lt;strong&gt;nueva&lt;/strong&gt; observaci&amp;oacute;n es un valor at&amp;iacute;pico. En este contexto, un valor at&amp;iacute;pico tambi&amp;eacute;n se denomina novedad.</target>
        </trans-unit>
        <trans-unit id="431a9d8e158a3b53d56cef19b322f4a781297a84" translate="yes" xml:space="preserve">
          <source>The training data, e.g. a reference to an immutable snapshot</source>
          <target state="translated">Los datos de entrenamiento,por ejemplo,una referencia a una instantánea inmutable</target>
        </trans-unit>
        <trans-unit id="44e706b0239b6ac2fad3df8ff059d735dbfbf296" translate="yes" xml:space="preserve">
          <source>The training input samples.</source>
          <target state="translated">Las muestras de entrada del entrenamiento.</target>
        </trans-unit>
        <trans-unit id="92c6c3d94fd9608db44f0ede45f9bbe4de664d1d" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">Las muestras de entrada de formaci&amp;oacute;n. Internamente, se convertir&amp;aacute; a &lt;code&gt;dtype=np.float32&lt;/code&gt; y si se proporciona una matriz dispersa a una &lt;code&gt;csc_matrix&lt;/code&gt; dispersa .</target>
        </trans-unit>
        <trans-unit id="fdce812a7f2c7f82334342af14e6e75d01b61ef1" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">Las muestras de entrada de formaci&amp;oacute;n. Internamente, su dtype se convertir&amp;aacute; a &lt;code&gt;dtype=np.float32&lt;/code&gt; . Si se proporciona una matriz dispersa, se convertir&amp;aacute; en una &lt;code&gt;csc_matrix&lt;/code&gt; dispersa .</target>
        </trans-unit>
        <trans-unit id="549582a79c03c6cf3a88a3f5ae8ab3477fe6f4f0" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.</source>
          <target state="translated">Las muestras de entrada del entrenamiento.Las matrices dispersas son aceptadas sólo si son apoyadas por el estimador de base.</target>
        </trans-unit>
        <trans-unit id="cc9cba55947c2b2da5cd927ed2d1f239e6fce1e8" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed9fac6749e01b46bdd0e4eebbecd7ed42fc3bac" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.</source>
          <target state="translated">Las muestras de entrada del entrenamiento.La matriz dispersa puede ser CSC,CSR,COO,DOK,o LIL.DOK y LIL se convierten en CSR.</target>
        </trans-unit>
        <trans-unit id="dba95905d10a3a6d61cb924560b16f35840a95de" translate="yes" xml:space="preserve">
          <source>The training points for the data. Each point has three fields:</source>
          <target state="translated">Los puntos de entrenamiento para los datos.Cada punto tiene tres campos:</target>
        </trans-unit>
        <trans-unit id="768a232e21b799d7ab4a21a202aa20c0c2da04e0" translate="yes" xml:space="preserve">
          <source>The training samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49afe52f19d67077f586b0d86a3a80bb7b9051b3" translate="yes" xml:space="preserve">
          <source>The training set has size &lt;code&gt;i * n_samples // (n_splits + 1)
+ n_samples % (n_splits + 1)&lt;/code&gt; in the &lt;code&gt;i``th split,
with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">El conjunto de entrenamiento tiene un tama&amp;ntilde;o &lt;code&gt;i * n_samples // (n_splits + 1) + n_samples % (n_splits + 1)&lt;/code&gt; en la &lt;code&gt;i``th split, with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt; , donde &lt;code&gt;n_samples&lt;/code&gt; es el n&amp;uacute;mero de muestras.</target>
        </trans-unit>
        <trans-unit id="7d0d65f5b4df90d39c92efe2c541dfa70477a3f3" translate="yes" xml:space="preserve">
          <source>The training set indices for that split.</source>
          <target state="translated">El entrenamiento estableció índices para esa división.</target>
        </trans-unit>
        <trans-unit id="1fe1c9ebc66c3c60358a4b7d7ce6958b71f5be6a" translate="yes" xml:space="preserve">
          <source>The transformation can be triggered by setting either &lt;code&gt;transformer&lt;/code&gt; or the pair of functions &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt;. However, setting both options will raise an error.</source>
          <target state="translated">La transformaci&amp;oacute;n se puede activar configurando &lt;code&gt;transformer&lt;/code&gt; o el par de funciones &lt;code&gt;func&lt;/code&gt; e &lt;code&gt;inverse_func&lt;/code&gt; . Sin embargo, configurar ambas opciones generar&amp;aacute; un error.</target>
        </trans-unit>
        <trans-unit id="cb596196fe310821d43e26f57899432a8af2c024" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1ce2cd56f04ea729576a873f5ef18af794ea62c" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. The cumulative density function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="translated">La transformación se aplica a cada característica de forma independiente.La función de densidad acumulada de un rasgo se utiliza para proyectar los valores originales.Los valores de las características de los datos nuevos/no vistos que caen por debajo o por encima del rango ajustado se asignarán a los límites de la distribución de salida.Nótese que esta transformación es no lineal.Puede distorsionar las correlaciones lineales entre las variables medidas en la misma escala,pero hace que las variables medidas a diferentes escalas sean más directamente comparables.</target>
        </trans-unit>
        <trans-unit id="019c47d3f3ad8207e8989b103cc42bdf22c099af" translate="yes" xml:space="preserve">
          <source>The transformation is calculated as (when &lt;code&gt;axis=0&lt;/code&gt;):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac658687b2a734f60273ed8ccd4ce7f47f7a43fa" translate="yes" xml:space="preserve">
          <source>The transformation is given by (when &lt;code&gt;axis=0&lt;/code&gt;):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ddd72b82186574150d900e6dfd8721345fa62073" translate="yes" xml:space="preserve">
          <source>The transformation is given by:</source>
          <target state="translated">La transformación viene dada por:</target>
        </trans-unit>
        <trans-unit id="1db1e441acce63afd6d696933447c0b2d453bb8c" translate="yes" xml:space="preserve">
          <source>The transformed data</source>
          <target state="translated">Los datos transformados</target>
        </trans-unit>
        <trans-unit id="f7cd2d51dfcb247e9b3665d7843f0082b5c854bd" translate="yes" xml:space="preserve">
          <source>The transformed data is a sparse graph as returned by kneighbors_graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e2ab39a27e391675e94304e8d0476203d9b90e4" translate="yes" xml:space="preserve">
          <source>The transformed data is a sparse graph as returned by radius_neighbors_graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a52b9359f518099e81b711eeaccc5e0c7c17eb0" translate="yes" xml:space="preserve">
          <source>The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.</source>
          <target state="translated">Los datos transformados se utilizan luego para entrenar un clasificador Bayes ingenuo,y se observa una clara diferencia en la precisión de las predicciones en que el conjunto de datos que se escalan antes de la ACP supera ampliamente a la versión no escalada.</target>
        </trans-unit>
        <trans-unit id="80e852f03673351dd5ee00932a57dc4a209cdf2d" translate="yes" xml:space="preserve">
          <source>The transformed data.</source>
          <target state="translated">Los datos transformados.</target>
        </trans-unit>
        <trans-unit id="831c10597508e086776cd00760f56c708f8d2bba" translate="yes" xml:space="preserve">
          <source>The tree algorithm to use. Valid options are [&amp;lsquo;kd_tree&amp;rsquo;|&amp;rsquo;ball_tree&amp;rsquo;|&amp;rsquo;auto&amp;rsquo;]. Default is &amp;lsquo;auto&amp;rsquo;.</source>
          <target state="translated">El algoritmo de &amp;aacute;rbol a utilizar. Las opciones v&amp;aacute;lidas son ['kd_tree' | 'ball_tree' | 'auto']. El valor predeterminado es &quot;autom&amp;aacute;tico&quot;.</target>
        </trans-unit>
        <trans-unit id="d745501ed4c4af3a67688bd307560f44fb29c904" translate="yes" xml:space="preserve">
          <source>The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node.</source>
          <target state="translated">La estructura de datos del árbol consiste en nodos,y cada nodo consta de varios subconjuntos.El número máximo de subconjuntos en un nodo está determinado por el factor de ramificación.Cada subclúster mantiene una suma lineal,una suma cuadrada y el número de muestras en ese subclúster.Además,cada subclúster también puede tener un nodo como hijo,si el subclúster no es miembro de un nodo de hoja.</target>
        </trans-unit>
        <trans-unit id="e124a1b9cc4902e35946b8c1931bf2b92e9cefc7" translate="yes" xml:space="preserve">
          <source>The tree-based model is significantly better at ranking policyholders by risk while the two linear models perform similarly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbc6ef1abc08faa2debda525a78475d34850618e" translate="yes" xml:space="preserve">
          <source>The true probability in each bin (fraction of positives).</source>
          <target state="translated">La verdadera probabilidad en cada recipiente (fracción de positivos).</target>
        </trans-unit>
        <trans-unit id="89aec1004fa9767eced66561c3ed3161000c45a1" translate="yes" xml:space="preserve">
          <source>The true score without permuting targets.</source>
          <target state="translated">La verdadera puntuación sin objetivos permanentes.</target>
        </trans-unit>
        <trans-unit id="78ab726a9fdf47809b691c3859f0df06c5754377" translate="yes" xml:space="preserve">
          <source>The trustworthiness is within [0, 1]. It is defined as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="311da69a1a719737450cc586668ba277c4bde011" translate="yes" xml:space="preserve">
          <source>The tutorial folder should contain the following sub-folders:</source>
          <target state="translated">La carpeta del tutorial debe contener las siguientes subcarpetas:</target>
        </trans-unit>
        <trans-unit id="2e8b6623a8185bbb598cfdf6b3f71f1ee2c2a837" translate="yes" xml:space="preserve">
          <source>The two figures below plot the values of &lt;code&gt;C&lt;/code&gt; on the &lt;code&gt;x-axis&lt;/code&gt; and the corresponding cross-validation scores on the &lt;code&gt;y-axis&lt;/code&gt;, for several different fractions of a generated data-set.</source>
          <target state="translated">Las dos figuras siguientes trazan los valores de &lt;code&gt;C&lt;/code&gt; en el &lt;code&gt;x-axis&lt;/code&gt; las puntuaciones de validaci&amp;oacute;n cruzada correspondientes en el &lt;code&gt;y-axis&lt;/code&gt; , para varias fracciones diferentes de un conjunto de datos generado.</target>
        </trans-unit>
        <trans-unit id="726ed01e2c2e9b244f4346dd4c7877757cc3867c" translate="yes" xml:space="preserve">
          <source>The two linear regressors &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt; now support sample weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85a91cdcd45557b2849d30b64bc99a3236c9a910" translate="yes" xml:space="preserve">
          <source>The two plots differ only in the area in the middle where the classes are tied. If &lt;code&gt;break_ties=False&lt;/code&gt;, all input in that area would be classified as one class, whereas if &lt;code&gt;break_ties=True&lt;/code&gt;, the tie-breaking mechanism will create a non-convex decision boundary in that area.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04a77900ed6f82fb8a154b287d08593a39a5648c" translate="yes" xml:space="preserve">
          <source>The two sample image.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ed5804ea6261422d9ec471fffa8f8a41a307d64" translate="yes" xml:space="preserve">
          <source>The two species are:</source>
          <target state="translated">Las dos especies son:</target>
        </trans-unit>
        <trans-unit id="4aa36c5d8992165f7895075f7b16a37f9864b092" translate="yes" xml:space="preserve">
          <source>The type of criterion to use.</source>
          <target state="translated">El tipo de criterio a utilizar.</target>
        </trans-unit>
        <trans-unit id="6bc8093d847369045cfca5abe49cd94f035f902d" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument.</source>
          <target state="translated">El tipo de valores de las características.Pasado a los constructores de matrices Numpy array/scipy.sparse como el argumento dtype.</target>
        </trans-unit>
        <trans-unit id="51cef5c60b8823e16a67a0821a4b5311abdcd5ed" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.</source>
          <target state="translated">El tipo de valores de las características.Pasado a los constructores de matrices scipy.sparse como el argumento dtype.No lo configure como bool,np.boolean o cualquier tipo de entero sin signo.</target>
        </trans-unit>
        <trans-unit id="25d83e9ee3b7a100418b9b6d2bb7936470b09825" translate="yes" xml:space="preserve">
          <source>The type of norm used to compute the error. Available error types: - &amp;lsquo;frobenius&amp;rsquo; (default): sqrt(tr(A^t.A)) - &amp;lsquo;spectral&amp;rsquo;: sqrt(max(eigenvalues(A^t.A)) where A is the error &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt;.</source>
          <target state="translated">El tipo de norma utilizada para calcular el error. Tipos de error disponibles: - 'frobenius' (predeterminado): sqrt (tr (A ^ tA)) - 'spectral': sqrt (max (valores propios (A ^ tA)) donde A es el error &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8b60d3555697082fbd56cd614358874d006ccd25" translate="yes" xml:space="preserve">
          <source>The type of the hyperparameter. Currently, only &amp;ldquo;numeric&amp;rdquo; hyperparameters are supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0b5e941b7e074477ab9a734aa8fbc101aeb347c" translate="yes" xml:space="preserve">
          <source>The unchanged dictionary atoms</source>
          <target state="translated">Los átomos del diccionario sin cambios</target>
        </trans-unit>
        <trans-unit id="17cd05cd0020cc8838dc1f102077fa8c28f81758" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="776a52daee9321c0497d40c79109e87f34af6b7e" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="translated">La implementaci&amp;oacute;n subyacente de &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; utiliza un generador de n&amp;uacute;meros aleatorios para seleccionar caracter&amp;iacute;sticas cuando se ajusta el modelo con un descenso de coordenadas dual (es decir, cuando &lt;code&gt;dual&lt;/code&gt; se establece en &lt;code&gt;True&lt;/code&gt; ). Por lo tanto, no es raro tener resultados ligeramente diferentes para los mismos datos de entrada. Si eso sucede, intente con un par&amp;aacute;metro tol m&amp;aacute;s peque&amp;ntilde;o. Esta aleatoriedad tambi&amp;eacute;n se puede controlar con el par&amp;aacute;metro &lt;code&gt;random_state&lt;/code&gt; . Cuando &lt;code&gt;dual&lt;/code&gt; se establece en &lt;code&gt;False&lt;/code&gt; , la implementaci&amp;oacute;n subyacente de &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; no es aleatoria y &lt;code&gt;random_state&lt;/code&gt; no tiene ning&amp;uacute;n efecto en los resultados.</target>
        </trans-unit>
        <trans-unit id="68c40938fee4ae1976f2ebe09bc5c5a404f12d37" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter.</source>
          <target state="translated">La implementaci&amp;oacute;n de C subyacente utiliza un generador de n&amp;uacute;meros aleatorios para seleccionar caracter&amp;iacute;sticas al ajustar el modelo. Por lo tanto, no es raro tener resultados ligeramente diferentes para los mismos datos de entrada. Si eso sucede, intente con un par&amp;aacute;metro &lt;code&gt;tol&lt;/code&gt; m&amp;aacute;s peque&amp;ntilde;o .</target>
        </trans-unit>
        <trans-unit id="2f30e1e8a65635cf877334fb55d57ec3cedb0460" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.</source>
          <target state="translated">La implementación C subyacente utiliza un generador de números aleatorios para seleccionar las características cuando se ajusta el modelo.Por lo tanto,no es raro tener resultados ligeramente diferentes para los mismos datos de entrada.Si eso ocurre,inténtelo con un parámetro de tolerancia más pequeño.</target>
        </trans-unit>
        <trans-unit id="4ed849766bc74a6b6038e401c289ce378db99dcc" translate="yes" xml:space="preserve">
          <source>The underlying Tree object. Please refer to &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; for attributes of Tree object and &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;Understanding the decision tree structure&lt;/a&gt; for basic usage of these attributes.</source>
          <target state="translated">El objeto Tree subyacente. Consulte la &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; para conocer los atributos del objeto &amp;Aacute;rbol y &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;Comprender la estructura del &amp;aacute;rbol de decisiones&lt;/a&gt; para el uso b&amp;aacute;sico de estos atributos.</target>
        </trans-unit>
        <trans-unit id="eaa9f70240ca573ce446579a8e3077ce3fd3b2de" translate="yes" xml:space="preserve">
          <source>The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.</source>
          <target state="translated">La implementación subyacente es MurmurHash3_x86_32 generando un hash de 32 bits de baja latencia adecuado para la implementación de tablas de búsqueda,filtros Bloom,sketch de conteo min o hash de características.</target>
        </trans-unit>
        <trans-unit id="8b27403493b3e1cf67d088cabd49f20f60beabb5" translate="yes" xml:space="preserve">
          <source>The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy.</source>
          <target state="translated">La implementación subyacente,liblinear,utiliza una escasa representación interna para los datos que incurrirán en una copia de memoria.</target>
        </trans-unit>
        <trans-unit id="7c5718e72f8aabec82565a4976b3e3b79f8616d0" translate="yes" xml:space="preserve">
          <source>The unique classes labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22bad6cd0a0fca07f198954a6d755d20a0363412" translate="yes" xml:space="preserve">
          <source>The univariate position of the sample according to the main dimension of the points in the manifold.</source>
          <target state="translated">La posición univariante de la muestra según la dimensión principal de los puntos del colector.</target>
        </trans-unit>
        <trans-unit id="e1bff9cf5ae34029868daf8dfe6afee04fc2666d" translate="yes" xml:space="preserve">
          <source>The unmixing matrix.</source>
          <target state="translated">La matriz de desmiembración.</target>
        </trans-unit>
        <trans-unit id="4c5425dc309e3cab0153df4bbf4dcfe21bae1aa0" translate="yes" xml:space="preserve">
          <source>The unsupervised data reduction and the supervised estimator can be chained in one step. See &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline: chaining estimators&lt;/a&gt;.</source>
          <target state="translated">La reducci&amp;oacute;n de datos no supervisada y el estimador supervisado se pueden encadenar en un solo paso. Consulte &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline: encadenamiento de estimadores&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="fca2372915cd9dbf5bde8febd27812d65a387223" translate="yes" xml:space="preserve">
          <source>The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for &lt;code&gt;x&lt;/code&gt; to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide).</source>
          <target state="translated">La figura superior izquierda ilustra las predicciones (en rojo oscuro) de un &amp;uacute;nico &amp;aacute;rbol de decisi&amp;oacute;n entrenado sobre un conjunto de datos aleatorio LS (los puntos azules) de un problema de regresi&amp;oacute;n 1d de juguete. Tambi&amp;eacute;n ilustra las predicciones (en rojo claro) de otros &amp;aacute;rboles de decisi&amp;oacute;n individuales entrenados sobre otras (y diferentes) instancias LS del problema dibujadas al azar. Intuitivamente, el t&amp;eacute;rmino de varianza aqu&amp;iacute; corresponde al ancho del haz de predicciones (en rojo claro) de los estimadores individuales. Cuanto mayor es la varianza, m&amp;aacute;s sensibles son las predicciones para &lt;code&gt;x&lt;/code&gt; a peque&amp;ntilde;os cambios en el conjunto de entrenamiento. El t&amp;eacute;rmino de sesgo corresponde a la diferencia entre la predicci&amp;oacute;n media del estimador (en cian) y el mejor modelo posible (en azul oscuro). En este problema, podemos observar que el sesgo es bastante bajo (las curvas cian y azul est&amp;aacute;n cerca una de la otra) mientras que la varianza es grande (el haz rojo es bastante ancho).</target>
        </trans-unit>
        <trans-unit id="30828e1dddfa129352e0424777c8d521d86c8855" translate="yes" xml:space="preserve">
          <source>The usage and the parameters of &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; are described below. The 2 most important parameters of these estimators are &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;learning_rate&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1125be6cb70e3f587339b27b5931d4f81c0d0d9b" translate="yes" xml:space="preserve">
          <source>The usage of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="translated">El uso de &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; se describe en detalle en &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7012369902c9ebdfcf4a40c832356073db31ae9a" translate="yes" xml:space="preserve">
          <source>The usage of centroid distance limits the distance metric to Euclidean space.</source>
          <target state="translated">El uso de la distancia centroide limita la distancia métrica al espacio euclidiano.</target>
        </trans-unit>
        <trans-unit id="4f2ed911398b7e07d993b089b964fc574c420c21" translate="yes" xml:space="preserve">
          <source>The usage of the &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt;&lt;code&gt;SkewedChi2Sampler&lt;/code&gt;&lt;/a&gt; is the same as the usage described above for the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;. The only difference is in the free parameter, that is called \(c\). For a motivation for this mapping and the mathematical details see &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt;.</source>
          <target state="translated">El uso de &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt; &lt;code&gt;SkewedChi2Sampler&lt;/code&gt; &lt;/a&gt; es el mismo que el descrito anteriormente para &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; . La &amp;uacute;nica diferencia est&amp;aacute; en el par&amp;aacute;metro libre, que se llama \ (c \). Para una motivaci&amp;oacute;n para este mapeo y los detalles matem&amp;aacute;ticos, consulte &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e4fafa67af5e9d647520b3a289f2e58cae35673f" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/miscellaneous/plot_multioutput_face_completion#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fabe9be99adc410c1907ea1daa7576972e4b19e8" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">El uso de vecinos m&amp;aacute;s cercanos de m&amp;uacute;ltiples salidas para la regresi&amp;oacute;n se demuestra en &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Compleci&amp;oacute;n&lt;/a&gt; de caras con estimadores de m&amp;uacute;ltiples salidas . En este ejemplo, las entradas X son los p&amp;iacute;xeles de la mitad superior de las caras y las salidas Y son los p&amp;iacute;xeles de la mitad inferior de esas caras.</target>
        </trans-unit>
        <trans-unit id="810478c2cd15f9d623ca524c5ee19e348c649a7e" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/miscellaneous/plot_multioutput_face_completion#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adaab44ae672254ed2b0f1474d8d83b9d0ff364a" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">El uso de &amp;aacute;rboles de m&amp;uacute;ltiples salidas para la clasificaci&amp;oacute;n se demuestra en la &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;finalizaci&amp;oacute;n de Face con estimadores de m&amp;uacute;ltiples salidas&lt;/a&gt; . En este ejemplo, las entradas X son los p&amp;iacute;xeles de la mitad superior de las caras y las salidas Y son los p&amp;iacute;xeles de la mitad inferior de esas caras.</target>
        </trans-unit>
        <trans-unit id="e18a96fd5f8412fd8540943bfa9bb84448ef6eef" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for regression is demonstrated in &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;Multi-output Decision Tree Regression&lt;/a&gt;. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.</source>
          <target state="translated">El uso de &amp;aacute;rboles de m&amp;uacute;ltiples salidas para la regresi&amp;oacute;n se demuestra en &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;Regresi&amp;oacute;n de &amp;aacute;rboles de decisi&amp;oacute;n de m&amp;uacute;ltiples salidas&lt;/a&gt; . En este ejemplo, la entrada X es un valor real &amp;uacute;nico y las salidas Y son el seno y el coseno de X.</target>
        </trans-unit>
        <trans-unit id="6d9188616eccce81f2896ac591395f1642a23655" translate="yes" xml:space="preserve">
          <source>The used categories can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute.</source>
          <target state="translated">Las categor&amp;iacute;as utilizadas se pueden encontrar en el atributo &lt;code&gt;categories_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ada4ba90a5864aca46a2fd4279e91da24ee2ef32" translate="yes" xml:space="preserve">
          <source>The user-provided initial means, defaults to None, If it None, means are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">Los medios iniciales proporcionados por el usuario, por defecto es Ninguno, si es Ninguno, los medios se inicializan utilizando el m&amp;eacute;todo &lt;code&gt;init_params&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a1cbbec0505b4f4802950c392df8579b38a3220d" translate="yes" xml:space="preserve">
          <source>The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the &amp;lsquo;init_params&amp;rsquo; method. The shape depends on &amp;lsquo;covariance_type&amp;rsquo;:</source>
          <target state="translated">Las precisiones iniciales proporcionadas por el usuario (inversas de las matrices de covarianza), por defecto son Ninguna. Si es None, las precisiones se inicializan usando el m&amp;eacute;todo 'init_params'. La forma depende de 'covariance_type':</target>
        </trans-unit>
        <trans-unit id="930a8f090bbd3f5ab357e6a55436cb80968a37a5" translate="yes" xml:space="preserve">
          <source>The user-provided initial weights, defaults to None. If it None, weights are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">Los pesos iniciales proporcionados por el usuario, predeterminados son Ninguno. Si es None, los pesos se inicializan usando el m&amp;eacute;todo &lt;code&gt;init_params&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="845cdaf2c42f719deb3141928cabec5996b4eedb" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.</source>
          <target state="translated">La estimación habitual de la máxima probabilidad de covarianza puede regularizarse utilizando la contracción.Ledoit y Wolf propusieron una fórmula cercana para calcular el parámetro de contracción asintóticamente óptimo (minimizando un criterio de MSE),lo que dio como resultado la estimación de covarianza de Ledoit-Wolf.</target>
        </trans-unit>
        <trans-unit id="5d9f8abe9cd1fbb176b951540baae1f3defa7962" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set.</source>
          <target state="translated">La estimaci&amp;oacute;n de m&amp;aacute;xima verosimilitud de la covarianza habitual es muy sensible a la presencia de valores at&amp;iacute;picos en el conjunto de datos. En tal caso, ser&amp;iacute;a mejor utilizar un estimador robusto de covarianza para garantizar que la estimaci&amp;oacute;n sea resistente a observaciones &quot;err&amp;oacute;neas&quot; en el conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="b7108164734a4610e8bf46680745d5c565a5fe9c" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set. &lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;#id5&quot; id=&quot;id2&quot;&gt;2&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf61b06542d3e7e313e8463ad6cb36679683eb16" translate="yes" xml:space="preserve">
          <source>The utility function &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt;&lt;code&gt;make_pipeline&lt;/code&gt;&lt;/a&gt; is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:</source>
          <target state="translated">La funci&amp;oacute;n de utilidad &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt; &lt;code&gt;make_pipeline&lt;/code&gt; &lt;/a&gt; es una forma abreviada de construir tuber&amp;iacute;as; toma un n&amp;uacute;mero variable de estimadores y devuelve una canalizaci&amp;oacute;n, completando los nombres autom&amp;aacute;ticamente:</target>
        </trans-unit>
        <trans-unit id="e6fa170ec90d321cecf4d3db13ca13c99c71342b" translate="yes" xml:space="preserve">
          <source>The valid distance metrics, and the function they map to, are:</source>
          <target state="translated">Las métricas de distancia válidas,y la función a la que se asignan,son:</target>
        </trans-unit>
        <trans-unit id="7aae4519886038a4fe27266a449c263068305fb0" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.</source>
          <target state="translated">El valor 2 tiene la puntuación más alta:aparece dos veces con pesos de 1,5 y 2:la suma de estos es 3.</target>
        </trans-unit>
        <trans-unit id="1614b1f299556b57f6975870b8797f657adfc906" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.5.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf2aaa6a69c13c7a1da598bbc487a099c24264e9" translate="yes" xml:space="preserve">
          <source>The value 4 appears three times: with uniform weights, the result is simply the mode of the distribution.</source>
          <target state="translated">El valor 4 aparece tres veces:con pesos uniformes,el resultado es simplemente el modo de la distribución.</target>
        </trans-unit>
        <trans-unit id="ee4d7bc4aea75382ac7c13c2d3ccdb7c85b05695" translate="yes" xml:space="preserve">
          <source>The value by which &lt;code&gt;|y - X'w - c|&lt;/code&gt; is scaled down.</source>
          <target state="translated">El valor por el cual &lt;code&gt;|y - X'w - c|&lt;/code&gt; se reduce.</target>
        </trans-unit>
        <trans-unit id="208f4b20d150446e32489b5141e41ca0c231e069" translate="yes" xml:space="preserve">
          <source>The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor.</source>
          <target state="translated">El valor del criterio de inercia asociado a la partición elegida (si compute_labels se establece en True).La inercia se define como la suma de las distancias cuadradas de las muestras a su vecino más cercano.</target>
        </trans-unit>
        <trans-unit id="605e5e84334ac11a6b1bcf93811952a1f4c93232" translate="yes" xml:space="preserve">
          <source>The value of the information criteria (&amp;lsquo;aic&amp;rsquo;, &amp;lsquo;bic&amp;rsquo;) across all alphas. The alpha which has the smallest information criterion is chosen. This value is larger by a factor of &lt;code&gt;n_samples&lt;/code&gt; compared to Eqns. 2.15 and 2.16 in (Zou et al, 2007).</source>
          <target state="translated">El valor de los criterios de informaci&amp;oacute;n ('aic', 'bic') en todos los alfa. Se elige el alfa que tiene el criterio de informaci&amp;oacute;n m&amp;aacute;s peque&amp;ntilde;o. Este valor es mayor en un factor de &lt;code&gt;n_samples&lt;/code&gt; comparaci&amp;oacute;n con las ecuaciones. 2,15 y 2,16 en (Zou et al, 2007).</target>
        </trans-unit>
        <trans-unit id="31a3294fc25a32d76657f5de9f45e5deb67968f8" translate="yes" xml:space="preserve">
          <source>The value of the largest coefficient.</source>
          <target state="translated">El valor del mayor coeficiente.</target>
        </trans-unit>
        <trans-unit id="35a39b75c4cf1051868175c0eb7c4d08a5d8c4c6" translate="yes" xml:space="preserve">
          <source>The value of the smallest coefficient.</source>
          <target state="translated">El valor del coeficiente más pequeño.</target>
        </trans-unit>
        <trans-unit id="77ad2ae374f9b7e58f24db15820679db8e02f6eb" translate="yes" xml:space="preserve">
          <source>The values at which the partial dependence should be evaluated are directly generated from &lt;code&gt;X&lt;/code&gt;. For 2-way partial dependence, a 2D-grid of values is generated. The &lt;code&gt;values&lt;/code&gt; field returned by &lt;a href=&quot;generated/sklearn.inspection.partial_dependence#sklearn.inspection.partial_dependence&quot;&gt;&lt;code&gt;sklearn.inspection.partial_dependence&lt;/code&gt;&lt;/a&gt; gives the actual values used in the grid for each target feature. They also correspond to the axis of the plots.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d764819e9c8551a1ae19a24b5d4cd3ac77d8b2aa" translate="yes" xml:space="preserve">
          <source>The values corresponding the quantiles of reference.</source>
          <target state="translated">Los valores correspondientes a los cuantiles de referencia.</target>
        </trans-unit>
        <trans-unit id="e64c591fdc6bfea9ce8a9d182cdb9d3354d8a90b" translate="yes" xml:space="preserve">
          <source>The values listed by the ValueError exception correspond to the functions measuring prediction accuracy described in the following sections. The scorer objects for those functions are stored in the dictionary &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt;.</source>
          <target state="translated">Los valores enumerados por la excepci&amp;oacute;n ValueError corresponden a las funciones que miden la precisi&amp;oacute;n de la predicci&amp;oacute;n descritas en las siguientes secciones. Los objetos de &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt; para esas funciones se almacenan en el diccionario sklearn.metrics.SCORERS .</target>
        </trans-unit>
        <trans-unit id="ad8b80eff2782593fcb8941c5fc504eacc683633" translate="yes" xml:space="preserve">
          <source>The values of the parameter that will be evaluated.</source>
          <target state="translated">Los valores del parámetro que será evaluado.</target>
        </trans-unit>
        <trans-unit id="380d80dd5b496982ade02999d670873884707a35" translate="yes" xml:space="preserve">
          <source>The values of this array sum to 1, unless all trees are single node trees consisting of only the root node, in which case it will be an array of zeros.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="896e61b11bd1cccb5014a25cacfbd49baded2da1" translate="yes" xml:space="preserve">
          <source>The values to be assigned to each cluster of samples</source>
          <target state="translated">Los valores que se asignarán a cada grupo de muestras</target>
        </trans-unit>
        <trans-unit id="15b198c6985fde1d17e7089abe8db26ba2c88b68" translate="yes" xml:space="preserve">
          <source>The values with which the grid has been created. The generated grid is a cartesian product of the arrays in &lt;code&gt;values&lt;/code&gt;. &lt;code&gt;len(values) ==
len(features)&lt;/code&gt;. The size of each array &lt;code&gt;values[j]&lt;/code&gt; is either &lt;code&gt;grid_resolution&lt;/code&gt;, or the number of unique values in &lt;code&gt;X[:, j]&lt;/code&gt;, whichever is smaller.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de1379c1aa59c6a0f9d415d70cec6205d70e58b1" translate="yes" xml:space="preserve">
          <source>The variance for each feature in the training set. Used to compute &lt;code&gt;scale_&lt;/code&gt;. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_std=False&lt;/code&gt;.</source>
          <target state="translated">La varianza de cada caracter&amp;iacute;stica en el conjunto de entrenamiento. Se usa para calcular &lt;code&gt;scale_&lt;/code&gt; . Igual a &lt;code&gt;None&lt;/code&gt; cuando &lt;code&gt;with_std=False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6909f327165fbbe5fe9d7a24773b9839e3b76030" translate="yes" xml:space="preserve">
          <source>The variance of the training samples transformed by a projection to each component.</source>
          <target state="translated">La variación de las muestras de entrenamiento transformadas por una proyección a cada componente.</target>
        </trans-unit>
        <trans-unit id="73778e0f44de95a7d306fdc5c4bc68ceb4bf8f71" translate="yes" xml:space="preserve">
          <source>The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">Los valores variables de los coeficientes a lo largo del camino. No est&amp;aacute; presente si el par&amp;aacute;metro &lt;code&gt;fit_path&lt;/code&gt; es &lt;code&gt;False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f193fe45abdd49c251c120544e6bc124439f7f88" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;lsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;lsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="translated">El vector \ (h_i \) se llama &quot;latente&quot; porque no se observa. \ (\ epsilon \) se considera un t&amp;eacute;rmino de ruido distribuido seg&amp;uacute;n un gaussiano con media 0 y covarianza \ (\ Psi \) (es decir, \ (\ epsilon \ sim \ mathcal {N} (0, \ Psi) \)), \ (\ mu \) es un vector de desplazamiento arbitrario. Este modelo se llama &quot;generativo&quot; porque describe c&amp;oacute;mo se genera \ (x_i \) a partir de \ (h_i \). Si usamos todas las \ (x_i \) como columnas para formar una matriz \ (\ mathbf {X} \) y todas las \ (h_i \) como columnas de una matriz \ (\ mathbf {H} \ ) entonces podemos escribir (con \ (\ mathbf {M} \) y \ (\ mathbf {E} \) adecuadamente definidos):</target>
        </trans-unit>
        <trans-unit id="928beebc49e69bf6a914c81639fd968b851e773a" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;rsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;rsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4545cfee48c8d7d2034c72caee40a8d82cfcf4ce" translate="yes" xml:space="preserve">
          <source>The verbose setting on the MiniBatchKMeans enables us to see that some clusters are reassigned during the successive calls to partial-fit. This is because the number of patches that they represent has become too low, and it is better to choose a random new cluster.</source>
          <target state="translated">El verborreico ajuste en el MiniBatchKMeans nos permite ver que algunos grupos son reasignados durante las sucesivas llamadas a ajuste parcial.Esto se debe a que el número de parches que representan se ha vuelto demasiado bajo,y es mejor elegir un nuevo clúster al azar.</target>
        </trans-unit>
        <trans-unit id="e4c09c360935c392206a8639f0a0b8f4c48b519d" translate="yes" xml:space="preserve">
          <source>The verbosity level</source>
          <target state="translated">El nivel de verbosidad</target>
        </trans-unit>
        <trans-unit id="4b4be8a44f0a986b95a00a99e1db7cc81cee43d8" translate="yes" xml:space="preserve">
          <source>The verbosity level.</source>
          <target state="translated">El nivel de verborrea.</target>
        </trans-unit>
        <trans-unit id="d073e4bf5a007c3a6f9ea54d0f642d54d81c6316" translate="yes" xml:space="preserve">
          <source>The verbosity level. If not zero, print some information about the fitting process.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bca220a25d0b85cbc18bcc37bc0c66babd623c39" translate="yes" xml:space="preserve">
          <source>The verbosity level. The default, zero, means silent mode.</source>
          <target state="translated">El nivel de verborrea.El valor predeterminado,cero,significa modo silencioso.</target>
        </trans-unit>
        <trans-unit id="b790263c39903969cb477edb620406690c93cb40" translate="yes" xml:space="preserve">
          <source>The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported.</source>
          <target state="translated">El nivel de verbosidad:si no es cero,se imprimen mensajes de progreso.Por encima de 50,la salida se envía a stdout.La frecuencia de los mensajes aumenta con el nivel de verbosidad.Si es superior a 10,se informa de todas las iteraciones.</target>
        </trans-unit>
        <trans-unit id="ac67f0eccad920e701e068f47d28e090c55e23b1" translate="yes" xml:space="preserve">
          <source>The verbosity mode of the function. By default that of the memory object is used.</source>
          <target state="translated">El modo de verbosidad de la función.Por defecto se utiliza el del objeto de memoria.</target>
        </trans-unit>
        <trans-unit id="31fbaba32a3bec24c49d9ccad5d6e7edcbc904dc" translate="yes" xml:space="preserve">
          <source>The versions of scikit-learn and its dependencies</source>
          <target state="translated">Las versiones de scikit-learn y sus dependencias</target>
        </trans-unit>
        <trans-unit id="588dcf117cf46b2a6deec167a78bc5c1266a3688" translate="yes" xml:space="preserve">
          <source>The visualization is fit automatically to the size of the axis. Use the &lt;code&gt;figsize&lt;/code&gt; or &lt;code&gt;dpi&lt;/code&gt; arguments of &lt;code&gt;plt.figure&lt;/code&gt; to control the size of the rendering.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a1686cc9631522f215a91033f5d185a9b750f85" translate="yes" xml:space="preserve">
          <source>The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns:</source>
          <target state="translated">El vocabulario extraído por este vectorizador es,por lo tanto,mucho más grande y ahora puede resolver las ambigüedades codificadas en los patrones de posicionamiento local:</target>
        </trans-unit>
        <trans-unit id="16d6455593e440b1ece6b5d9b609b990b16728ff" translate="yes" xml:space="preserve">
          <source>The weighted average probabilities for a sample would then be calculated as follows:</source>
          <target state="translated">Las probabilidades medias ponderadas de una muestra se calcularían entonces de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="cf3727918257ef88c9160136d7b98b7188c886ca" translate="yes" xml:space="preserve">
          <source>The weighted impurity decrease equation is the following:</source>
          <target state="translated">La ecuación de disminución de impurezas ponderada es la siguiente:</target>
        </trans-unit>
        <trans-unit id="b71363fa16752021c27a44ccc8c03e740b0054e3" translate="yes" xml:space="preserve">
          <source>The weights \(w\) of the model can be access:</source>
          <target state="translated">Se puede acceder a los pesos del modelo:</target>
        </trans-unit>
        <trans-unit id="dff6c2479ef55aa391ea0314f017ab45501554fd" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd37cf293f530a725eccba4fc386300500a89671" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None)</source>
          <target state="translated">Los pesos de cada observación en X.Si no hay ninguna,todas las observaciones tienen el mismo peso (por defecto:ninguna)</target>
        </trans-unit>
        <trans-unit id="89b091775b7b9350f3e527eec283b3139c887c00" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cdf718f4170abd3f842260826cc1d9da2bb111a" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fd9c75eb2401f4a1ca174b965b1602b9d82941e" translate="yes" xml:space="preserve">
          <source>The weights of each feature computed by the &lt;code&gt;fit&lt;/code&gt; method call are stored in a model attribute:</source>
          <target state="translated">Los pesos de cada caracter&amp;iacute;stica calculados por la llamada al m&amp;eacute;todo de &lt;code&gt;fit&lt;/code&gt; se almacenan en un atributo de modelo:</target>
        </trans-unit>
        <trans-unit id="44235eb4c2d368f2a7e225131bb791a1dc59a457" translate="yes" xml:space="preserve">
          <source>The weights of each mixture components.</source>
          <target state="translated">Los pesos de cada componente de la mezcla.</target>
        </trans-unit>
        <trans-unit id="dd426f25816730ede96a98b838186f9fc1553a50" translate="yes" xml:space="preserve">
          <source>The wine dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">El conjunto de datos del vino es un clásico y muy fácil conjunto de datos de clasificación multiclase.</target>
        </trans-unit>
        <trans-unit id="9f15574c628488edf549f3131c3e1a8bfce37b9c" translate="yes" xml:space="preserve">
          <source>The word &amp;ldquo;article&amp;rdquo; is a significant feature, based on how often people quote previous posts like this: &amp;ldquo;In article [article ID], [name] &amp;lt;[e-mail address]&amp;gt; wrote:&amp;rdquo;</source>
          <target state="translated">La palabra &quot;art&amp;iacute;culo&quot; es una caracter&amp;iacute;stica importante, seg&amp;uacute;n la frecuencia con la que las personas citan publicaciones anteriores como esta: &quot;En el art&amp;iacute;culo [ID del art&amp;iacute;culo], [nombre] &amp;lt;[direcci&amp;oacute;n de correo electr&amp;oacute;nico]&amp;gt; escribi&amp;oacute;:&quot;</target>
        </trans-unit>
        <trans-unit id="364df8b6c4c1dfcb405abf5ac32289b3f8338a10" translate="yes" xml:space="preserve">
          <source>The word &lt;em&gt;restricted&lt;/em&gt; refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:</source>
          <target state="translated">La palabra &lt;em&gt;restringida se&lt;/em&gt; refiere a la estructura bipartita del modelo, que proh&amp;iacute;be la interacci&amp;oacute;n directa entre unidades ocultas o entre unidades visibles. Esto significa que se asumen las siguientes independientes condicionales:</target>
        </trans-unit>
        <trans-unit id="92b782ef9bfc8a9813d7ea0d8efb9106d8f1896e" translate="yes" xml:space="preserve">
          <source>The word boundaries-aware variant &lt;code&gt;char_wb&lt;/code&gt; is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw &lt;code&gt;char&lt;/code&gt; variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations.</source>
          <target state="translated">La variante &lt;code&gt;char_wb&lt;/code&gt; que reconoce los l&amp;iacute;mites de las palabras es especialmente interesante para los idiomas que utilizan espacios en blanco para la separaci&amp;oacute;n de palabras, ya que genera caracter&amp;iacute;sticas significativamente menos ruidosas que la variante &lt;code&gt;char&lt;/code&gt; sin procesar en ese caso. Para tales lenguajes, puede aumentar tanto la precisi&amp;oacute;n predictiva como la velocidad de convergencia de los clasificadores entrenados con dichas caracter&amp;iacute;sticas, al tiempo que conserva la solidez con respecto a errores ortogr&amp;aacute;ficos y derivaciones de palabras.</target>
        </trans-unit>
        <trans-unit id="8e8bd120180a9cd30000d6c7bc4b945e5f585fc6" translate="yes" xml:space="preserve">
          <source>The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, &amp;lsquo;How slow is the k-means method?&amp;rsquo; SoCG2006)</source>
          <target state="translated">La complejidad del peor caso viene dada por O (n ^ (k + 2 / p)) con n = n_samples, p = n_features. (D. Arthur y S. Vassilvitskii, '&amp;iquest;Qu&amp;eacute; tan lento es el m&amp;eacute;todo k-medias?' SoCG2006)</target>
        </trans-unit>
        <trans-unit id="08b4bc51642b52ec53b43a5b2dca7586d296a859" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimator: robust multivariate regression model.</source>
          <target state="translated">Estimador Theil-Sen:modelo de regresión multivariante robusto.</target>
        </trans-unit>
        <trans-unit id="26357ed7a886288385aec0522bda7ee91031a5a9" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</source>
          <target state="translated">Estimadores de Theil-Sen en un modelo de regresi&amp;oacute;n lineal m&amp;uacute;ltiple, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang y Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bfbe9912c49db3d4af2258ea17cbacd11611139b" translate="yes" xml:space="preserve">
          <source>Theil-Sen Regression</source>
          <target state="translated">La regresión de Theil-Sen</target>
        </trans-unit>
        <trans-unit id="43407a9ed591c227c94e72cd0fbe8ae10a8a282d" translate="yes" xml:space="preserve">
          <source>TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs worse than OLS.</source>
          <target state="translated">El TheilSen es bueno para los pequeños valores atípicos,tanto en la dirección X como en la Y,pero tiene un punto de ruptura por encima del cual se desempeña peor que el OLS.</target>
        </trans-unit>
        <trans-unit id="bae71614b2af83560543a8e9bca47722a78d80c8" translate="yes" xml:space="preserve">
          <source>Their harmonic mean called &lt;strong&gt;V-measure&lt;/strong&gt; is computed by &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt;&lt;code&gt;v_measure_score&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">Su media arm&amp;oacute;nica llamada &lt;strong&gt;medida V&lt;/strong&gt; se calcula mediante &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt; &lt;code&gt;v_measure_score&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="de0b6ab722b9ec4be95cb7086fea273ea373e1de" translate="yes" xml:space="preserve">
          <source>Then fire an ipython shell and run the work-in-progress script with:</source>
          <target state="translated">Entonces dispara una concha de ipython y ejecuta el guión de trabajo en curso con:</target>
        </trans-unit>
        <trans-unit id="8f78bb6cc31961e9507c2d8e418f53fe822a9ecd" translate="yes" xml:space="preserve">
          <source>Then one can show that to classify a data point after scaling is equivalent to finding the estimated class mean \(\mu^*_k\) which is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the \(K-1\) affine subspace \(H_K\) generated by all the \(\mu^*_k\) for all classes. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a \(K-1\) dimensional space.</source>
          <target state="translated">Entonces se puede mostrar que clasificar un punto de datos después de escalar es equivalente a encontrar la media estimada de la clase \(\mu^*_k\)que está más cerca del punto de datos en la distancia euclidiana.Pero esto se puede hacer igual de bien después de proyectar en el subespacio afín (K-1)generado por todas las clases.Esto muestra que,implícito en el clasificador LDA,hay una reducción de la dimensionalidad por proyección lineal en un espacio dimensional (K-1).</target>
        </trans-unit>
        <trans-unit id="37e493a483dcc9fefbfec81c47f8c835f7340e3f" translate="yes" xml:space="preserve">
          <source>Then the Davies-Bouldin index is defined as:</source>
          <target state="translated">Entonces el índice Davies-Bouldin se define como:</target>
        </trans-unit>
        <trans-unit id="54059dffef3b64a1ba00cd6fbe5ba85d85229195" translate="yes" xml:space="preserve">
          <source>Then the metrics are defined as:</source>
          <target state="translated">Entonces las métricas se definen como:</target>
        </trans-unit>
        <trans-unit id="8da3ae858e8aee4768b456c38c7b9a98b999a912" translate="yes" xml:space="preserve">
          <source>Then the multiclass MCC is defined as:</source>
          <target state="translated">Entonces el MCC multiclase se define como:</target>
        </trans-unit>
        <trans-unit id="826d143749061228ef1caa51bd344b5a543a3ad8" translate="yes" xml:space="preserve">
          <source>Then the rows of \(Z\) are clustered using &lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;. The first &lt;code&gt;n_rows&lt;/code&gt; labels provide the row partitioning, and the remaining &lt;code&gt;n_columns&lt;/code&gt; labels provide the column partitioning.</source>
          <target state="translated">Luego, las filas de \ (Z \) se agrupan usando &lt;a href=&quot;clustering#k-means&quot;&gt;k-medias&lt;/a&gt; . Las primeras etiquetas &lt;code&gt;n_rows&lt;/code&gt; proporcionan la partici&amp;oacute;n de filas y las etiquetas &lt;code&gt;n_columns&lt;/code&gt; restantes proporcionan la partici&amp;oacute;n de columnas.</target>
        </trans-unit>
        <trans-unit id="35fd57830f62bfb9e9a1c9481c01f75fad3f7e92" translate="yes" xml:space="preserve">
          <source>Then we check the performance of the computed model plotting its predictions on the test set and computing, for example, the median absolute error of the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04ebdc92c95e8a854e544219afce2fe077f07d46" translate="yes" xml:space="preserve">
          <source>Then we check the quality of the predictions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7548beecedd6ae525f17db272cd47fef5592b2e" translate="yes" xml:space="preserve">
          <source>Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:</source>
          <target state="translated">Luego,aplicando la norma euclidiana (L2),obtenemos los siguientes tf-idfs para el documento 1:</target>
        </trans-unit>
        <trans-unit id="743e0b74d42270bf2752b1121cf66d48c177b1df" translate="yes" xml:space="preserve">
          <source>Then, the &lt;code&gt;raw_X&lt;/code&gt; to be fed to &lt;code&gt;FeatureHasher.transform&lt;/code&gt; can be constructed using:</source>
          <target state="translated">Luego, el &lt;code&gt;raw_X&lt;/code&gt; que se alimentar&amp;aacute; a &lt;code&gt;FeatureHasher.transform&lt;/code&gt; se puede construir usando:</target>
        </trans-unit>
        <trans-unit id="36e93c1a3cbd0ad36880c0132d911de26dc0ca83" translate="yes" xml:space="preserve">
          <source>Then, we identify features &lt;code&gt;X&lt;/code&gt; and targets &lt;code&gt;y&lt;/code&gt;: the column WAGE is our target variable (i.e., the variable which we want to predict).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2abd121a4ed8d06cf0817807518539ab09e24fe" translate="yes" xml:space="preserve">
          <source>Then, we introspect the information regarding each column data type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4665e6f004c55fa84e60f1409a5b03c15037dd8c" translate="yes" xml:space="preserve">
          <source>Theoretical bounds</source>
          <target state="translated">Límites teóricos</target>
        </trans-unit>
        <trans-unit id="3ebe4a41a0b35192395aac8a80293b4ff462a23b" translate="yes" xml:space="preserve">
          <source>There are 3 different APIs for evaluating the quality of a model&amp;rsquo;s predictions:</source>
          <target state="translated">Hay 3 API diferentes para evaluar la calidad de las predicciones de un modelo:</target>
        </trans-unit>
        <trans-unit id="96e1915039a69b2e8b64f25478a0431f9e517cc9" translate="yes" xml:space="preserve">
          <source>There are \(K\) topics in the corpus.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa910de5e10f2b04644e63996efaedb310779ee6" translate="yes" xml:space="preserve">
          <source>There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let &lt;code&gt;D&lt;/code&gt; be the distance, and &lt;code&gt;S&lt;/code&gt; be the kernel:</source>
          <target state="translated">Hay varias formas de convertir entre una m&amp;eacute;trica de distancia y una medida de similitud, como un kernel. Sea &lt;code&gt;D&lt;/code&gt; la distancia y &lt;code&gt;S&lt;/code&gt; el n&amp;uacute;cleo:</target>
        </trans-unit>
        <trans-unit id="daf4e822cec5d40489080bfb83cea014bb21271c" translate="yes" xml:space="preserve">
          <source>There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):</source>
          <target state="translated">También hay un par de contras (frente al uso de un CountVectorizer con un vocabulario en memoria):</target>
        </trans-unit>
        <trans-unit id="17151dd0dcece68b8abecb55a500335bd2f90b73" translate="yes" xml:space="preserve">
          <source>There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.</source>
          <target state="translated">Hay conceptos que son difíciles de aprender porque los árboles de decisión no los expresan fácilmente,como los problemas de XOR,paridad o multiplexores.</target>
        </trans-unit>
        <trans-unit id="fbd19b439fe6ba80531cd23629fc7a9a883a8dcf" translate="yes" xml:space="preserve">
          <source>There are different things to keep in mind when dealing with data corrupted by outliers:</source>
          <target state="translated">Hay diferentes cosas que hay que tener en cuenta cuando se trata de datos corrompidos por valores atípicos:</target>
        </trans-unit>
        <trans-unit id="8e7f48473f9869b2775936845c434aa7ef66ad5e" translate="yes" xml:space="preserve">
          <source>There are four more hyperparameters, \(\alpha_1\), \(\alpha_2\), \(\lambda_1\) and \(\lambda_2\) of the gamma prior distributions over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. By default \(\alpha_1 = \alpha_2 = \lambda_1 = \lambda_2 = 10^{-6}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fcc62fa36cf231416a60ac162100095676c743b" translate="yes" xml:space="preserve">
          <source>There are many learning routines which rely on nearest neighbors at their core. One example is &lt;a href=&quot;density#kernel-density&quot;&gt;kernel density estimation&lt;/a&gt;, discussed in the &lt;a href=&quot;density#density-estimation&quot;&gt;density estimation&lt;/a&gt; section.</source>
          <target state="translated">Hay muchas rutinas de aprendizaje que dependen fundamentalmente de los vecinos m&amp;aacute;s cercanos. Un ejemplo es la &lt;a href=&quot;density#kernel-density&quot;&gt;estimaci&amp;oacute;n de la densidad del n&amp;uacute;cleo&lt;/a&gt; , que se analiza en la secci&amp;oacute;n de &lt;a href=&quot;density#density-estimation&quot;&gt;estimaci&amp;oacute;n de&lt;/a&gt; la densidad .</target>
        </trans-unit>
        <trans-unit id="de49a8d62debf6dd1584d9f929c607a2c0abdd42" translate="yes" xml:space="preserve">
          <source>There are many well-established imputation packages in the R data science ecosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns out to be a particular instance of different sequential imputation algorithms that can all be implemented with &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; by passing in different regressors to be used for predicting missing feature values. In the case of missForest, this regressor is a Random Forest. See &lt;a href=&quot;../auto_examples/impute/plot_iterative_imputer_variants_comparison#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py&quot;&gt;Imputing missing values with variants of IterativeImputer&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afc5c09497df5828bed5e53fb9ba7340fbd12b56" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. It does not aim to be a general, &amp;lsquo;one-size-fits-all&amp;rsquo; solution as some tasks may require a more custom solution. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84513801465c2346e62e2a9ecaecce1cb0c3994b" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt;.</source>
          <target state="translated">Hay varios problemas conocidos en nuestra lista de palabras vac&amp;iacute;as en 'ingl&amp;eacute;s' proporcionada. Consulte &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="69da86c48a6fab38a24a0a096ce607f0a2966cc1" translate="yes" xml:space="preserve">
          <source>There are several possibilities to do that, two of which are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87d60889b216a9f6a8543438325d8902e749b92a" translate="yes" xml:space="preserve">
          <source>There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).</source>
          <target state="translated">Hay diez imágenes diferentes de cada uno de los 40 sujetos distintos.Para algunos sujetos,las imágenes se tomaron en momentos diferentes,variando la iluminación,las expresiones faciales (ojos abiertos/cerrados,sonriendo/no sonriendo)y los detalles faciales (gafas/sin gafas).Todas las imágenes se tomaron contra un fondo oscuro y homogéneo con los sujetos en posición vertical,frontal (con tolerancia para algún movimiento lateral).</target>
        </trans-unit>
        <trans-unit id="840b342f8bab010ed9a33830388b79c022d751bb" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers linear kernels, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="translated">Hay tres implementaciones diferentes de Regresi&amp;oacute;n de vectores de soporte: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt; proporciona una implementaci&amp;oacute;n m&amp;aacute;s r&amp;aacute;pida que &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; pero solo considera kernels lineales, mientras que &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt; implementa una formulaci&amp;oacute;n ligeramente diferente a &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt; . Consulte &lt;a href=&quot;#svm-implementation-details&quot;&gt;Detalles de implementaci&amp;oacute;n&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="7c46376033bb18a7479bbb076a0191da5ac66026" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers the linear kernel, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6325cb8ad2d750db2de1f50457999e3ed60c95ad" translate="yes" xml:space="preserve">
          <source>There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.</source>
          <target state="translated">Hay tres tipos principales de interfaces de conjuntos de datos que pueden utilizarse para obtener conjuntos de datos dependiendo del tipo de conjunto de datos deseado.</target>
        </trans-unit>
        <trans-unit id="f738d3558005f6c63ce087749d6fa9a898307784" translate="yes" xml:space="preserve">
          <source>There are two main methods to approximate the integral above, namely the &amp;lsquo;brute&amp;rsquo; and &amp;lsquo;recursion&amp;rsquo; methods. The &lt;code&gt;method&lt;/code&gt; parameter controls which method to use.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d867308d6cfc04930e6d66867b250110233d07e" translate="yes" xml:space="preserve">
          <source>There are two options to assign labels:</source>
          <target state="translated">Hay dos opciones para asignar etiquetas:</target>
        </trans-unit>
        <trans-unit id="60a8f9c96bc7f93958ce08005db84d1ac5e8a2b4" translate="yes" xml:space="preserve">
          <source>There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.</source>
          <target state="translated">Hay dos maneras de evaluar un resultado de biclustering:interno y externo.Las medidas internas,como la estabilidad de los conglomerados,dependen sólo de los datos y del propio resultado.Actualmente no hay medidas internas de biclustering en scikit-learn.Las medidas externas se refieren a una fuente externa de información,como la verdadera solución.Cuando se trabaja con datos reales,la solución verdadera suele ser desconocida,pero los datos artificiales de bicluster pueden ser útiles para evaluar los algoritmos precisamente porque la solución verdadera es conocida.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
