<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="e3ab7886f45f0e5bcfcb494c5dda13f6aee54058" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;mean_fit_time&lt;/code&gt;, &lt;code&gt;std_fit_time&lt;/code&gt;, &lt;code&gt;mean_score_time&lt;/code&gt; and &lt;code&gt;std_score_time&lt;/code&gt; are all in seconds.</source>
          <target state="translated">El &lt;code&gt;mean_fit_time&lt;/code&gt; , &lt;code&gt;std_fit_time&lt;/code&gt; , &lt;code&gt;mean_score_time&lt;/code&gt; y &lt;code&gt;std_score_time&lt;/code&gt; est&amp;aacute;n todos en segundos.</target>
        </trans-unit>
        <trans-unit id="5067a7dbb7441ffa442bc35298e784d3bbb5d81c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how &lt;code&gt;X&lt;/code&gt; values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predictions will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predictions will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo; a &lt;code&gt;ValueError&lt;/code&gt; is raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4dc710fdb008b124893854b1db0d772123e2f23c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how x-values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predicted y-values will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predicted y-values will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo;, allow &lt;code&gt;interp1d&lt;/code&gt; to throw ValueError.</source>
          <target state="translated">El par&amp;aacute;metro &lt;code&gt;out_of_bounds&lt;/code&gt; maneja c&amp;oacute;mo se manejan los valores x fuera del dominio de entrenamiento. Cuando se establece en &quot;nan&quot;, los valores de y predichos ser&amp;aacute;n NaN. Cuando se establece en &quot;clip&quot;, los valores y predichos se establecer&amp;aacute;n en el valor correspondiente al punto final del intervalo de tren m&amp;aacute;s cercano. Cuando se establece en &quot;subir&quot;, permite que &lt;code&gt;interp1d&lt;/code&gt; lance ValueError.</target>
        </trans-unit>
        <trans-unit id="3c0e73047db48d9d2c7fabbdd5b52d96e2b806a9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;partial_fit&lt;/code&gt; method call of naive Bayes models introduces some computational overhead. It is recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows.</source>
          <target state="translated">El &lt;code&gt;partial_fit&lt;/code&gt; llamada a un m&amp;eacute;todo de Bayes modelos ingenuos introduce cierta sobrecarga computacional. Se recomienda utilizar tama&amp;ntilde;os de fragmentos de datos lo m&amp;aacute;s grandes posible, es decir, seg&amp;uacute;n lo permita la RAM disponible.</target>
        </trans-unit>
        <trans-unit id="12fb3214c445789e7d1fb007eb13c0f9c87b3271" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;penalty&lt;/code&gt; parameter determines the regularization to be used (see description above in the classification section).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c075895944959b3ce70972d2605db496c74ee36b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; that implements the same operation using the &lt;code&gt;Transformer&lt;/code&gt; API (even though the &lt;code&gt;fit&lt;/code&gt; method is useless in this case: the class is stateless as this operation treats samples independently).</source>
          <target state="translated">El m&amp;oacute;dulo de &lt;code&gt;preprocessing&lt;/code&gt; proporciona adem&amp;aacute;s un &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt; clase de utilidad que implementa la misma operaci&amp;oacute;n usando la API de &lt;code&gt;Transformer&lt;/code&gt; (aunque el m&amp;eacute;todo de &lt;code&gt;fit&lt;/code&gt; es in&amp;uacute;til en este caso: la clase no tiene estado ya que esta operaci&amp;oacute;n trata las muestras de forma independiente).</target>
        </trans-unit>
        <trans-unit id="deb9c47cdc32b652eb6b8e87508a50e73ddd6520" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;Transformer&lt;/code&gt; API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">El m&amp;oacute;dulo de &lt;code&gt;preprocessing&lt;/code&gt; proporciona adem&amp;aacute;s una clase de utilidad &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt; que implementa la API de &lt;code&gt;Transformer&lt;/code&gt; para calcular la media y la desviaci&amp;oacute;n est&amp;aacute;ndar en un conjunto de entrenamiento para poder luego volver a aplicar la misma transformaci&amp;oacute;n en el conjunto de prueba. Por lo tanto, esta clase es adecuada para su uso en los primeros pasos de un &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="1c988d2d2cf78f045059c60f8f22ddfe99f6b9b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;random_state&lt;/code&gt; parameter defaults to &lt;code&gt;None&lt;/code&gt;, meaning that the shuffling will be different every time &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; is iterated. However, &lt;code&gt;GridSearchCV&lt;/code&gt; will use the same shuffling for each set of parameters validated by a single call to its &lt;code&gt;fit&lt;/code&gt; method.</source>
          <target state="translated">El par&amp;aacute;metro &lt;code&gt;random_state&lt;/code&gt; est&amp;aacute; predeterminado en &lt;code&gt;None&lt;/code&gt; , lo que significa que la mezcla ser&amp;aacute; diferente cada vez que se &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; . Sin embargo, &lt;code&gt;GridSearchCV&lt;/code&gt; usar&amp;aacute; la misma mezcla para cada conjunto de par&amp;aacute;metros validados por una sola llamada a su m&amp;eacute;todo de &lt;code&gt;fit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="269b993dc29ab1401256004d5e02aa3f5322b3b6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remainder&lt;/code&gt; parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:</source>
          <target state="translated">El par&amp;aacute;metro &lt;code&gt;remainder&lt;/code&gt; se puede establecer en un estimador para transformar las columnas de calificaci&amp;oacute;n restantes. Los valores transformados se a&amp;ntilde;aden al final de la transformaci&amp;oacute;n:</target>
        </trans-unit>
        <trans-unit id="ac9a3f7a78a3eea81f462ceabf4e7d7b7695c3e1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;roc_auc_score&lt;/code&gt; function can also be used in multi-class classification. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the multiclass ROC AUC scores are computed from the probability estimates that a sample belongs to a particular class according to the model. The OvO and OvR algorithms support weighting uniformly (&lt;code&gt;average='macro'&lt;/code&gt;) and weighting by the prevalence (&lt;code&gt;average='weighted'&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5cf025ac399ddc1c71c004c7cb8bd73171357561" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;shrinkage&lt;/code&gt; parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</source>
          <target state="translated">El par&amp;aacute;metro de &lt;code&gt;shrinkage&lt;/code&gt; tambi&amp;eacute;n se puede establecer manualmente entre 0 y 1. En particular, un valor de 0 corresponde a ninguna contracci&amp;oacute;n (lo que significa que se utilizar&amp;aacute; la matriz de covarianza emp&amp;iacute;rica) y un valor de 1 corresponde a una contracci&amp;oacute;n completa (lo que significa que la diagonal La matriz de varianzas se utilizar&amp;aacute; como una estimaci&amp;oacute;n de la matriz de covarianzas). Establecer este par&amp;aacute;metro en un valor entre estos dos extremos estimar&amp;aacute; una versi&amp;oacute;n reducida de la matriz de covarianza.</target>
        </trans-unit>
        <trans-unit id="ac317fd98a77c706ac5d8a7e74a96d7bdbebfe03" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;3&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b439b9c10b67475737ae3e151e90a6ed815edbd1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt;.</source>
          <target state="translated">El paquete &lt;code&gt;sklearn.covariance&lt;/code&gt; implementa un estimador robusto de covarianza, el Determinante de Covarianza M&amp;iacute;nimo &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="6908af1748b7fe666814b53ca6e0a8cab66eb012" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package embeds some small toy datasets as introduced in the &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Getting Started&lt;/a&gt; section.</source>
          <target state="translated">El paquete &lt;code&gt;sklearn.datasets&lt;/code&gt; incrusta algunos peque&amp;ntilde;os conjuntos de datos de juguetes, como se present&amp;oacute; en la secci&amp;oacute;n &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Introducci&amp;oacute;n&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="82c9f1d0e48a88b08f4d85a858e0d70b67f55eec" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package is able to download datasets from the repository using the function &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">El paquete &lt;code&gt;sklearn.datasets&lt;/code&gt; puede descargar conjuntos de datos del repositorio mediante la funci&amp;oacute;n &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="8a80b4da04f92037fdd1ffd5771651becd3a649e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.preprocessing&lt;/code&gt; package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</source>
          <target state="translated">El paquete &lt;code&gt;sklearn.preprocessing&lt;/code&gt; proporciona varias funciones de utilidad comunes y clases de transformadores para cambiar los vectores de caracter&amp;iacute;sticas sin procesar en una representaci&amp;oacute;n que sea m&amp;aacute;s adecuada para los estimadores posteriores.</target>
        </trans-unit>
        <trans-unit id="9a666cbe94ae4d4ef285ecec57ff29087d1b1c92" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;stop_words_&lt;/code&gt; attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.</source>
          <target state="translated">El atributo &lt;code&gt;stop_words_&lt;/code&gt; puede aumentar de tama&amp;ntilde;o y aumentar el tama&amp;ntilde;o del modelo durante el decapado. Este atributo se proporciona solo para la introspecci&amp;oacute;n y se puede eliminar de forma segura usando delattr o establecer en Ninguno antes del decapado.</target>
        </trans-unit>
        <trans-unit id="a2df492be59e0b5e94ab9ece47f7ab58734f8d4a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;svm.OneClassSVM&lt;/code&gt; is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.</source>
          <target state="translated">Se sabe que &lt;code&gt;svm.OneClassSVM&lt;/code&gt; es sensible a valores at&amp;iacute;picos y, por lo tanto, no funciona muy bien para la detecci&amp;oacute;n de valores at&amp;iacute;picos. Este estimador es m&amp;aacute;s adecuado para la detecci&amp;oacute;n de novedades cuando el conjunto de entrenamiento no est&amp;aacute; contaminado por valores at&amp;iacute;picos. Dicho esto, la detecci&amp;oacute;n de valores at&amp;iacute;picos en alta dimensi&amp;oacute;n, o sin suposiciones sobre la distribuci&amp;oacute;n de los datos subyacentes, es muy desafiante, y una SVM de una clase podr&amp;iacute;a dar resultados &amp;uacute;tiles en estas situaciones dependiendo del valor de sus hiperpar&amp;aacute;metros.</target>
        </trans-unit>
        <trans-unit id="ae04908924f55e8ffa997283ff07a48e73585d0d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;tree_disp&lt;/code&gt; and &lt;code&gt;mlp_disp&lt;/code&gt;&lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt; objects contain all the computed information needed to recreate the partial dependence curves. This means we can easily create additional plots without needing to recompute the curves.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0172c625369e43fa830669101114953189c751e" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;kernel function&lt;/em&gt; can be any of the following:</source>
          <target state="translated">La &lt;em&gt;funci&amp;oacute;n del kernel&lt;/em&gt; puede ser cualquiera de las siguientes:</target>
        </trans-unit>
        <trans-unit id="9153a9bf6c588ab4d00c19f8aab755b4d4790f32" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;reachability&lt;/em&gt; distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining &lt;em&gt;reachability&lt;/em&gt; distances and data set &lt;code&gt;ordering_&lt;/code&gt; produces a &lt;em&gt;reachability plot&lt;/em&gt;, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. &amp;lsquo;Cutting&amp;rsquo; the reachability plot at a single value produces DBSCAN like results; all points above the &amp;lsquo;cut&amp;rsquo; are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter &lt;code&gt;xi&lt;/code&gt;. There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the &lt;code&gt;cluster_hierarchy_&lt;/code&gt; parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6eff32d476fb25ebcdfdcb46623a8711aa972809" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;conditional entropy of clusters given class&lt;/strong&gt;\(H(K|C)\) and the &lt;strong&gt;entropy of clusters&lt;/strong&gt;\(H(K)\) are defined in a symmetric manner.</source>
          <target state="translated">La &lt;strong&gt;entrop&amp;iacute;a condicional de los conglomerados dada la clase&lt;/strong&gt; \ (H (K | C) \) y la &lt;strong&gt;entrop&amp;iacute;a de los conglomerados&lt;/strong&gt; \ (H (K) \) se definen de manera sim&amp;eacute;trica.</target>
        </trans-unit>
        <trans-unit id="5adbfb7d3b47e961b1e75927796552dd26ad41e4" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;exposure&lt;/strong&gt; is the duration of the insurance coverage of a given policy, in years.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a256791561f56278bffe5a6063bb1e7227590f9a" translate="yes" xml:space="preserve">
          <source>The AGE and EXPERIENCE coefficients are affected by strong variability which might be due to the collinearity between the 2 features: as AGE and EXPERIENCE vary together in the data, their effect is difficult to tease apart.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e076b7d1d24895a169d091c6b0d7ae51431e1b5b" translate="yes" xml:space="preserve">
          <source>The AGE coefficient is expressed in &amp;ldquo;dollars/hour per living years&amp;rdquo; while the EDUCATION one is expressed in &amp;ldquo;dollars/hour per years of education&amp;rdquo;. This representation of the coefficients has the benefit of making clear the practical predictions of the model: an increase of \(1\) year in AGE means a decrease of \(0.030867\) dollars/hour, while an increase of \(1\) year in EDUCATION means an increase of \(0.054699\) dollars/hour. On the other hand, categorical variables (as UNION or SEX) are adimensional numbers taking either the value 0 or 1. Their coefficients are expressed in dollars/hour. Then, we cannot compare the magnitude of different coefficients since the features have different natural scales, and hence value ranges, because of their different unit of measure. This is more visible if we plot the coefficients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2143ba51f2aad99e99e8cd60a62ccfdecbf0f265" translate="yes" xml:space="preserve">
          <source>The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative.</source>
          <target state="translated">El AMI devuelve un valor de 1 cuando las dos particiones son idénticas (es decir,perfectamente coincidentes).Las particiones aleatorias (etiquetadas independientemente)tienen un AMI esperado de alrededor de 0 en promedio,por lo tanto puede ser negativo.</target>
        </trans-unit>
        <trans-unit id="ad838931339007a1bda099c18480308bbb327339" translate="yes" xml:space="preserve">
          <source>The API is experimental (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00e3722885bdadae5430c6fecaacfc1cced893f6" translate="yes" xml:space="preserve">
          <source>The API is experimental in version 0.20 (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">La API es experimental en la versión 0.20 (en particular la estructura de valor de retorno),y podría tener pequeños cambios incompatibles con el pasado en futuras versiones.</target>
        </trans-unit>
        <trans-unit id="e07138bd94d8d9dd92474342c51f134fa3423d8d" translate="yes" xml:space="preserve">
          <source>The Ames housing dataset is not shipped with scikit-learn and therefore we will fetch it from &lt;a href=&quot;https://www.openml.org/d/42165&quot;&gt;OpenML&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22556f00c2cdfc8c60673e1c663299619a64901c" translate="yes" xml:space="preserve">
          <source>The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a &lt;a href=&quot;#bgmm&quot;&gt;Variational Bayesian Gaussian mixture&lt;/a&gt; avoids the specification of the number of components for a Gaussian mixture model.</source>
          <target state="translated">El criterio BIC se puede utilizar para seleccionar el n&amp;uacute;mero de componentes en una mezcla gaussiana de manera eficiente. En teor&amp;iacute;a, recupera el n&amp;uacute;mero real de componentes solo en el r&amp;eacute;gimen asint&amp;oacute;tico (es decir, si hay muchos datos disponibles y asumiendo que los datos se generaron realmente a partir de una mezcla de distribuci&amp;oacute;n gaussiana). Tenga en cuenta que el uso de una &lt;a href=&quot;#bgmm&quot;&gt;mezcla Gaussiana Bayesiana Variacional&lt;/a&gt; evita la especificaci&amp;oacute;n del n&amp;uacute;mero de componentes para un modelo de mezcla Gaussiana.</target>
        </trans-unit>
        <trans-unit id="c912c462511f5921d016fed26135f626922ccc57" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.</source>
          <target state="translated">La implementación de Barnes-Hut sólo funciona cuando la dimensionalidad del objetivo es de 3 o menos.El caso 2D es típico cuando se construyen visualizaciones.</target>
        </trans-unit>
        <trans-unit id="49027a83447d6307ec530f0dd81bd5ad76360faa" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.</source>
          <target state="translated">El método de Barnes-Hut t-SNE está limitado a incrustaciones bidimensionales o tridimensionales.</target>
        </trans-unit>
        <trans-unit id="f05f901deabcb1ee376f8697fc79932cbf746ce6" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is \(O[d N log(N)]\), where \(d\) is the number of output dimensions and \(N\) is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is \(O[d N^2]\), but has several other notable differences:</source>
          <target state="translated">El t-SNE de Barnes-Hut que ha sido implementado aquí es usualmente mucho más lento que otros múltiples algoritmos de aprendizaje.La optimización es bastante difícil y el cálculo del gradiente es &quot;O[d N log(N)]²&quot;,donde &quot;d&quot; es el número de dimensiones de salida y &quot;N&quot; es el número de muestras.El método de Barnes-Hut mejora el método exacto donde la complejidad del t-SNE es (O[d N^2]\N),pero tiene varias otras diferencias notables:</target>
        </trans-unit>
        <trans-unit id="e724094f6d5c410991717a9e2fc045d6798def45" translate="yes" xml:space="preserve">
          <source>The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.</source>
          <target state="translated">El algoritmo de Birch tiene dos parámetros,el umbral y el factor de ramificación.El factor de ramificación limita el número de subconjuntos en un nodo y el umbral limita la distancia entre la muestra que entra y los subconjuntos existentes.</target>
        </trans-unit>
        <trans-unit id="8fa70be719c3475e8e012f73acc572a21c03cdd8" translate="yes" xml:space="preserve">
          <source>The Boston house-price data has been used in many machine learning papers that address regression problems.</source>
          <target state="translated">Los datos de precios de las casas de Boston se han utilizado en muchos trabajos de aprendizaje de máquinas que abordan los problemas de regresión.</target>
        </trans-unit>
        <trans-unit id="455243540177422da87f9c3aa93de30456a2bae3" translate="yes" xml:space="preserve">
          <source>The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &amp;lsquo;Hedonic prices and the demand for clean air&amp;rsquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics &amp;hellip;&amp;rsquo;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.</source>
          <target state="translated">Los datos de precios de la vivienda en Boston de Harrison, D. y Rubinfeld, DL &amp;laquo;Precios hed&amp;oacute;nicos y demanda de aire limpio&amp;raquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Utilizado en Belsley, Kuh &amp;amp; Welsch, 'Regression diagnostics ...', Wiley, 1980. NB Se utilizan varias transformaciones en la tabla de las p&amp;aacute;ginas 244-261 de este &amp;uacute;ltimo.</target>
        </trans-unit>
        <trans-unit id="7bc398b9797cca816dee96d9ed40f902ff743772" translate="yes" xml:space="preserve">
          <source>The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt;&lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e6c056d908bee991f648998f41ff72e8a7301c2" translate="yes" xml:space="preserve">
          <source>The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:</source>
          <target state="translated">Los subconjuntos de la CF contienen la información necesaria para la agrupación,lo que evita la necesidad de mantener todos los datos de entrada en la memoria.Esta información incluye:</target>
        </trans-unit>
        <trans-unit id="b64782d12e8b4e97de28df15435a193ab8a372e9" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b96c855c122e21633007587f3600e092339a5383" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">El índice de Calinski-Harabaz es generalmente más alto para los conglomerados convexos que otros conceptos de conglomerados,como los conglomerados basados en la densidad como los obtenidos a través del DBSCAN.</target>
        </trans-unit>
        <trans-unit id="6cdbc3a888667e8344981c2a8742122994627087" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al.</source>
          <target state="translated">El clasificador de Complemento Ingenuo de Bayes descrito en Rennie et al.</target>
        </trans-unit>
        <trans-unit id="2b0b8b0a4dd5523203a6ee4b5d195c2e6a35c0fe" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al. (2003).</source>
          <target state="translated">El clasificador de complemento ingenuo de Bayes descrito en Rennie et al.(2003).</target>
        </trans-unit>
        <trans-unit id="3398d87a3aceb1ea8375c52fcc6a67cec5c63df9" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier was designed to correct the &amp;ldquo;severe assumptions&amp;rdquo; made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.</source>
          <target state="translated">El clasificador Complement Naive Bayes fue dise&amp;ntilde;ado para corregir las &amp;ldquo;suposiciones severas&amp;rdquo; hechas por el clasificador est&amp;aacute;ndar Multinomial Naive Bayes. Es especialmente adecuado para conjuntos de datos desequilibrados.</target>
        </trans-unit>
        <trans-unit id="7c176cfd9628fae51161daa03c193aeae01d49ea" translate="yes" xml:space="preserve">
          <source>The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \(k\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.</source>
          <target state="translated">El método de la Divergencia Contrastante sugiere detener la cadena después de un pequeño número de iteraciones,\(k\),por lo general incluso 1.Este método es rápido y tiene una baja varianza,pero las muestras están lejos de la distribución del modelo.</target>
        </trans-unit>
        <trans-unit id="1d09474bd461f8756796df6e9a77749a8489b6d3" translate="yes" xml:space="preserve">
          <source>The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than &lt;code&gt;eps&lt;/code&gt; to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than &lt;code&gt;eps&lt;/code&gt; from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering.</source>
          <target state="translated">El algoritmo DBSCAN es determinista, siempre genera los mismos cl&amp;uacute;steres cuando se le dan los mismos datos en el mismo orden. Sin embargo, los resultados pueden diferir cuando los datos se proporcionan en un orden diferente. Primero, aunque las muestras centrales siempre se asignar&amp;aacute;n a los mismos conglomerados, las etiquetas de esos conglomerados depender&amp;aacute;n del orden en el que se encuentren esas muestras en los datos. En segundo lugar, y lo que es m&amp;aacute;s importante, los grupos a los que se asignan las muestras no centrales pueden diferir seg&amp;uacute;n el orden de los datos. Esto suceder&amp;iacute;a cuando una muestra no central tiene una distancia menor que &lt;code&gt;eps&lt;/code&gt; a dos muestras centrales en diferentes grupos. Por la desigualdad triangular, esas dos muestras centrales deben estar m&amp;aacute;s distantes que &lt;code&gt;eps&lt;/code&gt; unos de otros, o estar&amp;iacute;an en el mismo grupo. La muestra no central se asigna a cualquier grupo que se genere primero en una pasada a trav&amp;eacute;s de los datos, por lo que los resultados depender&amp;aacute;n del orden de los datos.</target>
        </trans-unit>
        <trans-unit id="b5783662e991f62ab3cc63e2843b11c10be0af6e" translate="yes" xml:space="preserve">
          <source>The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN.</source>
          <target state="translated">El índice Davies-Boulding es generalmente más alto para los cúmulos convexos que otros conceptos de cúmulos,como los cúmulos basados en la densidad como los obtenidos del DBSCAN.</target>
        </trans-unit>
        <trans-unit id="9dae6187a2e4264ce3764c47d6ced56e22112bc3" translate="yes" xml:space="preserve">
          <source>The Digit Dataset</source>
          <target state="translated">El conjunto de datos de los dígitos</target>
        </trans-unit>
        <trans-unit id="15949e73904f01f7a29e0206a3232b388b3af43d" translate="yes" xml:space="preserve">
          <source>The Dirichlet process prior allows to define an infinite number of components and automatically selects the correct number of components: it activates a component only if it is necessary.</source>
          <target state="translated">El proceso Dirichlet prior permite definir un número infinito de componentes y selecciona automáticamente el número correcto de componentes:activa un componente sólo si es necesario.</target>
        </trans-unit>
        <trans-unit id="a257bf9309fabcccadc69c1bb14244b979e6f754" translate="yes" xml:space="preserve">
          <source>The Discounted Cumulative Gain divided by the Ideal Discounted Cumulative Gain (the DCG obtained for a perfect ranking), in order to have a score between 0 and 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2e49448333a2cd92baf05825e927765f1835316" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is commonly combined with exponentiation.</source>
          <target state="translated">El núcleo de DotProduct se combina comúnmente con la exponenciación.</target>
        </trans-unit>
        <trans-unit id="842c0c72bca462c8ce3e9ff0cead821c33430a8a" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0^2. For sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">El núcleo de DotProduct no es estacionario y puede obtenerse a partir de una regresión lineal poniendo N(0,1)a priori en los coeficientes de x_d (d=1,...,D)y a priori de N(0,sigma_0^2)en el sesgo.El núcleo de DotProduct es invariable a una rotación de las coordenadas sobre el origen,pero no a las traslaciones.Está parametrizado por un parámetro sigma_0^2.Para sigma_0^2 =0,el núcleo se llama el núcleo lineal homogéneo,de lo contrario es no homogéneo.El núcleo viene dado por</target>
        </trans-unit>
        <trans-unit id="078b4d2b566f931479e95c97a38f05103a2ecb7d" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0 \(\sigma\) which controls the inhomogenity of the kernel. For \(\sigma_0^2 =0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c219b28ff4dd0afb8b865b96896a279315780153" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.</source>
          <target state="translated">El par&amp;aacute;metro de mezcla de Elastic Net, con 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio = 0 corresponde a la penalizaci&amp;oacute;n L2, l1_ratio = 1 a L1. El valor predeterminado es 0,15.</target>
        </trans-unit>
        <trans-unit id="bc71ca7c36fa98be5da488c2c0295a9dca2a49c6" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if &lt;code&gt;penalty&lt;/code&gt; is &amp;lsquo;elasticnet&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="557cde32667038df92bd17ed0a660f8397aa9f15" translate="yes" xml:space="preserve">
          <source>The Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. Setting &lt;code&gt;l1_ratio=0&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while setting &lt;code&gt;l1_ratio=1&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71317f65b5997839fb7f2d86b8c73dc4399ad254" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2.</source>
          <target state="translated">El par&amp;aacute;metro de mezcla de ElasticNet, con 0 &amp;lt;l1_ratio &amp;lt;= 1. Para l1_ratio = 1, la penalizaci&amp;oacute;n es una penalizaci&amp;oacute;n L1 / L2. Para l1_ratio = 0 es una penalizaci&amp;oacute;n L2. Para &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; , la penalizaci&amp;oacute;n es una combinaci&amp;oacute;n de L1 / L2 y L2.</target>
        </trans-unit>
        <trans-unit id="f913d5ab29d41b1d0b8510f8960f88320058e481" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in &lt;code&gt;[.1, .5, .7,
.9, .95, .99, 1]&lt;/code&gt;</source>
          <target state="translated">El par&amp;aacute;metro de mezcla de ElasticNet, con 0 &amp;lt;l1_ratio &amp;lt;= 1. Para l1_ratio = 1, la penalizaci&amp;oacute;n es una penalizaci&amp;oacute;n L1 / L2. Para l1_ratio = 0 es una penalizaci&amp;oacute;n L2. Para &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; , la penalizaci&amp;oacute;n es una combinaci&amp;oacute;n de L1 / L2 y L2. Este par&amp;aacute;metro puede ser una lista, en cuyo caso los diferentes valores se prueban mediante validaci&amp;oacute;n cruzada y se utiliza el que da la mejor puntuaci&amp;oacute;n de predicci&amp;oacute;n. Tenga en cuenta que una buena elecci&amp;oacute;n de lista de valores para l1_ratio es a menudo poner m&amp;aacute;s valores cerca de 1 (es decir, Lasso) y menos cerca de 0 (es decir, Ridge), como en &lt;code&gt;[.1, .5, .7, .9, .95, .99, 1]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="62ded71b403044434993092527c854b8f53e5c17" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. For &lt;code&gt;l1_ratio = 0&lt;/code&gt; the penalty is an L2 penalty. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; it is an L1 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">El par&amp;aacute;metro de mezcla de &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; , con 0 &amp;lt;= l1_ratio &amp;lt;= 1 . Para &lt;code&gt;l1_ratio = 0&lt;/code&gt; la penalizaci&amp;oacute;n es una penalizaci&amp;oacute;n L2. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; es una penalizaci&amp;oacute;n L1. Para &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; , la penalizaci&amp;oacute;n es una combinaci&amp;oacute;n de L1 y L2.</target>
        </trans-unit>
        <trans-unit id="706ede4c7ae02c53bd29139976a5c9ab95013fca" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a periodicity parameter periodicity&amp;gt;0. Only the isotropic variant where l is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">El kernel ExpSineSquared permite modelar funciones peri&amp;oacute;dicas. Est&amp;aacute; parametrizado por un par&amp;aacute;metro de escala de longitud length_scale&amp;gt; 0 y un par&amp;aacute;metro de periodicidad periodicity&amp;gt; 0. Por el momento, solo se admite la variante isotr&amp;oacute;pica donde l es un escalar. El kernel dado por:</target>
        </trans-unit>
        <trans-unit id="648ff6d48b0697b9c2efa5d7b7f9a5acf1d3efbe" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows one to model functions which repeat themselves exactly. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a periodicity parameter \(p&amp;gt;0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31f2158684fd941b75a9b2f295ee3be83d4fb497" translate="yes" xml:space="preserve">
          <source>The Exponentiation kernel takes one base kernel and a scalar parameter \(p\) and combines them via</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd1aecbb19e2286cfafdebb8292f1071a3eb508c" translate="yes" xml:space="preserve">
          <source>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</source>
          <target state="translated">La puntuación F-beta puede interpretarse como una media armónica ponderada de la precisión y la memoria,donde una puntuación F-beta alcanza su mejor valor en 1 y la peor puntuación en 0.</target>
        </trans-unit>
        <trans-unit id="fcc8a075bfea5f42d0e38256200386182d33cc42" translate="yes" xml:space="preserve">
          <source>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</source>
          <target state="translated">La puntuación F-beta es la media armónica ponderada de precisión y recuerdo,alcanzando su valor óptimo en 1 y su peor valor en 0.</target>
        </trans-unit>
        <trans-unit id="553773b1d0f7903e424f857dbba13f4b2343a5a1" translate="yes" xml:space="preserve">
          <source>The F-beta score weights recall more than precision by a factor of &lt;code&gt;beta&lt;/code&gt;. &lt;code&gt;beta == 1.0&lt;/code&gt; means recall and precision are equally important.</source>
          <target state="translated">Las ponderaciones de puntuaci&amp;oacute;n F-beta recuerdan m&amp;aacute;s que precisi&amp;oacute;n por un factor de &lt;code&gt;beta&lt;/code&gt; . &lt;code&gt;beta == 1.0&lt;/code&gt; significa que la memoria y la precisi&amp;oacute;n son igualmente importantes.</target>
        </trans-unit>
        <trans-unit id="3f53f44feaa854c0fdf5365ef5e43bc7fa440b01" translate="yes" xml:space="preserve">
          <source>The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:</source>
          <target state="translated">La puntuación de la F1 puede interpretarse como un promedio ponderado de la precisión y la memoria,donde la puntuación de la F1 alcanza su mejor valor en 1 y la peor puntuación en 0.La contribución relativa de la precisión y la memoria a la puntuación de la F1 son iguales.La fórmula para la puntuación de la F1 es:</target>
        </trans-unit>
        <trans-unit id="518509d08bd98ece386f11a6583f35b4f559fdac" translate="yes" xml:space="preserve">
          <source>The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:</source>
          <target state="translated">La figura a continuación muestra cuatro parcelas de dependencia parcial de una dirección y otra de dos direcciones para el conjunto de datos de viviendas de California:</target>
        </trans-unit>
        <trans-unit id="8d40f6c8f7d760d5a9c3769f72b85293905bcf21" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in a 2-dimensional parameter space (\(m=2\)) when \(R(w) = 1\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2280ec0b9d5ba5eef024fd7f6041defdab8c9d48" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in the parameter space when \(R(w) = 1\).</source>
          <target state="translated">La siguiente figura muestra los contornos de los diferentes términos de regularización en el espacio de parámetros cuando \(R(w)=1\).</target>
        </trans-unit>
        <trans-unit id="af21e4c771514d1aca6cd1c14dcff1ce67a0e477" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt;&lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt;&lt;/a&gt;) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:</source>
          <target state="translated">El &amp;iacute;ndice de Fowlkes- &lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt; &lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt; &lt;/a&gt; ( sklearn.metrics.fowlkes_mallows_score ) se puede utilizar cuando se conocen las asignaciones de la clase de verdad b&amp;aacute;sica de las muestras. El FMI de puntuaci&amp;oacute;n de Fowlkes-Mallows se define como la media geom&amp;eacute;trica de la precisi&amp;oacute;n por pares y la recuperaci&amp;oacute;n:</target>
        </trans-unit>
        <trans-unit id="bfdc8ed29be1f90cbfb4dc9576ce17a736f93294" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall:</source>
          <target state="translated">El índice Fowlkes-Mallows (FMI)se define como la media geométrica entre la precisión y el recuerdo:</target>
        </trans-unit>
        <trans-unit id="fb7ab21b6034e9b9d6397353d40d09f0a02e126e" translate="yes" xml:space="preserve">
          <source>The French Motor Third-Party Liability Claims dataset</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90c804ef552ac504772d87a0c90e2454ea50931f" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">Se supone que la media previa de GP es cero. La covarianza del prior se especifica pasando un objeto del &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; . Los hiperpar&amp;aacute;metros del kernel se optimizan durante el ajuste de GaussianProcessRegressor maximizando la probabilidad log-marginal (LML) basada en el &lt;code&gt;optimizer&lt;/code&gt; pasado . Como el LML puede tener m&amp;uacute;ltiples &amp;oacute;ptimos locales, el optimizador puede iniciarse repetidamente especificando &lt;code&gt;n_restarts_optimizer&lt;/code&gt; . La primera ejecuci&amp;oacute;n siempre se realiza a partir de los valores de hiperpar&amp;aacute;metros iniciales del kernel; las ejecuciones posteriores se realizan a partir de valores de hiperpar&amp;aacute;metros que se han elegido aleatoriamente del rango de valores permitidos. Si los hiperpar&amp;aacute;metros iniciales deben mantenerse fijos, &lt;code&gt;None&lt;/code&gt; se puede pasar como optimizador.</target>
        </trans-unit>
        <trans-unit id="060746131ee7579fe50d723eccf0216e72cc0775" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9ff401cc389e116b429c312d580f50e1ed79479" translate="yes" xml:space="preserve">
          <source>The Gini coefficient (based on the area under the curve) can be used as a model selection metric to quantify the ability of the model to rank policyholders. Note that this metric does not reflect the ability of the models to make accurate predictions in terms of absolute value of total claim amounts but only in terms of relative amounts as a ranking metric.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18dcaaaea80b191d9100a9a4ae22f95aa636616a" translate="yes" xml:space="preserve">
          <source>The Gini index reflects the ability of a model to rank predictions irrespective of their absolute values, and therefore only assess their ranking power.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01a974069deaad9f4a696951139fa6f180c4610d" translate="yes" xml:space="preserve">
          <source>The HLLE algorithm comprises three stages:</source>
          <target state="translated">El algoritmo HLLE comprende tres etapas:</target>
        </trans-unit>
        <trans-unit id="eaec5120030b979edc1dcb8e844874e0db8c76f1" translate="yes" xml:space="preserve">
          <source>The Hamming loss is the fraction of labels that are incorrectly predicted.</source>
          <target state="translated">La pérdida de Hamming es la fracción de las etiquetas que se predicen incorrectamente.</target>
        </trans-unit>
        <trans-unit id="304ac7ce83417558cbe44e7723fbf3a93a036677" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss, when &lt;code&gt;normalize&lt;/code&gt; parameter is set to True. It is always between 0 and 1, lower being better.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee89b1587a7c7b8d189cccf628a41e55c5d7ebe4" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1.</source>
          <target state="translated">La pérdida de Hamming es superior a la pérdida del subconjunto cero uno.Cuando se normaliza sobre las muestras,la pérdida de Hamming está siempre entre 0 y 1.</target>
        </trans-unit>
        <trans-unit id="e3001f46869621e71f2b9c72a2ca6b3e38966a32" translate="yes" xml:space="preserve">
          <source>The Haversine (or great circle) distance is the angular distance between two points on the surface of a sphere. The first distance of each point is assumed to be the latitude, the second is the longitude, given in radians. The dimension of the data must be 2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f575d5541e123fdf35ce0dd5c6fb4373f14dde3" translate="yes" xml:space="preserve">
          <source>The Huber Regressor optimizes the squared loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; and the absolute loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt;, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.</source>
          <target state="translated">El regresor de Huber optimiza la p&amp;eacute;rdida al cuadrado de las muestras donde &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; y la p&amp;eacute;rdida absoluta para las muestras donde &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt; , donde w y sigma son par&amp;aacute;metros a optimizar. El par&amp;aacute;metro sigma asegura que si y se escala hacia arriba o hacia abajo por un cierto factor, no es necesario cambiar la escala de &amp;eacute;psilon para lograr la misma robustez. Tenga en cuenta que esto no tiene en cuenta el hecho de que las diferentes caracter&amp;iacute;sticas de X pueden ser de diferentes escalas.</target>
        </trans-unit>
        <trans-unit id="a428acf86562c566ebfc0a4a1d3e42c700a1a75f" translate="yes" xml:space="preserve">
          <source>The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter &lt;code&gt;epsilon&lt;/code&gt;. This parameter depends on the scale of the target variables.</source>
          <target state="translated">Las funciones de p&amp;eacute;rdida insensibles a Huber y &amp;eacute;psilon se pueden utilizar para una regresi&amp;oacute;n robusta. El ancho de la regi&amp;oacute;n insensible debe especificarse mediante el par&amp;aacute;metro &lt;code&gt;epsilon&lt;/code&gt; . Este par&amp;aacute;metro depende de la escala de las variables objetivo.</target>
        </trans-unit>
        <trans-unit id="0a2150cee6da11610a14f68e745e5d0639a39459" translate="yes" xml:space="preserve">
          <source>The Iris Dataset</source>
          <target state="translated">El conjunto de datos del Iris</target>
        </trans-unit>
        <trans-unit id="7c0ef25371cb6aecb434ff0290ad88905a1c2367" translate="yes" xml:space="preserve">
          <source>The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.</source>
          <target state="translated">El conjunto de datos del Iris representa 3 tipos de flores de Iris (Setosa,Versicolor y Virginica)con 4 atributos:longitud del sépalo,anchura del sépalo,longitud del pétalo y anchura del pétalo.</target>
        </trans-unit>
        <trans-unit id="3877561d1fbee6aff8708ef4ec5fcabe6115833e" translate="yes" xml:space="preserve">
          <source>The IsolationForest &amp;lsquo;isolates&amp;rsquo; observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</source>
          <target state="translated">IsolationForest 'a&amp;iacute;sla' observaciones seleccionando aleatoriamente una caracter&amp;iacute;stica y luego seleccionando aleatoriamente un valor dividido entre los valores m&amp;aacute;ximo y m&amp;iacute;nimo de la caracter&amp;iacute;stica seleccionada.</target>
        </trans-unit>
        <trans-unit id="9e47d533732129c7b308a9673031913c04e17afd" translate="yes" xml:space="preserve">
          <source>The Isomap algorithm comprises three stages:</source>
          <target state="translated">El algoritmo del Isomap comprende tres etapas:</target>
        </trans-unit>
        <trans-unit id="43f0627e56441244b0d44b83d4abbf8510d9eac0" translate="yes" xml:space="preserve">
          <source>The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">El &amp;iacute;ndice de Jaccard [1], o coeficiente de similitud de Jaccard, definido como el tama&amp;ntilde;o de la intersecci&amp;oacute;n dividido por el tama&amp;ntilde;o de la uni&amp;oacute;n de dos conjuntos de etiquetas, se utiliza para comparar el conjunto de etiquetas pronosticadas para una muestra con el conjunto de etiquetas correspondiente en &lt;code&gt;y_true&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="54b0fb2f21ad6f516f16dd3562af3d3b867660fe" translate="yes" xml:space="preserve">
          <source>The Jaccard similarity coefficient of the \(i\)-th samples, with a ground truth label set \(y_i\) and predicted label set \(\hat{y}_i\), is defined as</source>
          <target state="translated">El coeficiente de similitud de las muestras de Jaccard,con un conjunto de etiquetas de verdad y de predicción,se define como</target>
        </trans-unit>
        <trans-unit id="1b6ab133b67b3354e1eaee98fb0609deafb4a4e0" translate="yes" xml:space="preserve">
          <source>The Johnson-Lindenstrauss bound for embedding with random projections</source>
          <target state="translated">El Johnson-Lindenstrauss destinado a la incrustación con proyecciones aleatorias</target>
        </trans-unit>
        <trans-unit id="e9607194f44934f99bc18e8a1d649dcabdd2e40a" translate="yes" xml:space="preserve">
          <source>The K-means algorithm aims to choose centroids that minimise the &lt;strong&gt;inertia&lt;/strong&gt;, or &lt;strong&gt;within-cluster sum-of-squares criterion&lt;/strong&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e420c9d276fc6531e2d5a1fd3367ac4465f247c" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">El conjunto de datos de KDD Cup '99 se cre&amp;oacute; procesando las partes tcpdump del conjunto de datos de evaluaci&amp;oacute;n del sistema de detecci&amp;oacute;n de intrusiones (IDS) DARPA de 1998, creado por MIT Lincoln Lab [1]. Los datos artificiales (descritos en la &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;p&amp;aacute;gina de inicio del conjunto de datos&lt;/a&gt; ) se generaron utilizando una red cerrada y ataques inyectados a mano para producir una gran cantidad de diferentes tipos de ataques con actividad normal en segundo plano. Como el objetivo inicial era producir un gran conjunto de entrenamiento para algoritmos de aprendizaje supervisado, existe una gran proporci&amp;oacute;n (80,1%) de datos anormales que no son realistas en el mundo real e inapropiados para la detecci&amp;oacute;n de anomal&amp;iacute;as sin supervisi&amp;oacute;n que tiene como objetivo detectar datos 'anormales'. es decir</target>
        </trans-unit>
        <trans-unit id="c580c78b1750c851ebf65b6b32d643554e25c44f" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42488af2a88df53e7b3301c7d5a125afdc9cd2ce" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.</source>
          <target state="translated">La divergencia Kullback-Leibler (KL)de las probabilidades conjuntas en el espacio original y en el espacio incrustado será minimizada por el descenso del gradiente.Nótese que la divergencia KL no es convexa,es decir,múltiples reinicios con diferentes inicializaciones terminarán en mínimos locales de la divergencia KL.Por lo tanto,a veces es útil probar diferentes semillas y seleccionar la incrustación con la menor divergencia de KL.</target>
        </trans-unit>
        <trans-unit id="c1f022c5d8701b9b00fce305ae262139841dede3" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use 0 for no regularization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b439efc6a95c460b62f48e9b2ed58ff5ae6d780" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use &lt;code&gt;0&lt;/code&gt; for no regularization (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a35ab88c062292779ce626a30b50f45159613b2" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c5e824f9a2e5664a81c2abf8d8de696419cc018" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">El modelo LARS se puede utilizar utilizando el estimador &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt; , o su implementaci&amp;oacute;n de bajo nivel &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5604e6549ba6e538c77a361942c228433b92a65e" translate="yes" xml:space="preserve">
          <source>The LTSA algorithm comprises three stages:</source>
          <target state="translated">El algoritmo de la LTSA comprende tres etapas:</target>
        </trans-unit>
        <trans-unit id="5921a49032d4468242b223b9e118c35472eff0fa" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation consist of retrieving the path with function &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">El algoritmo Lars proporciona la ruta completa de los coeficientes a lo largo del par&amp;aacute;metro de regularizaci&amp;oacute;n casi gratis, por lo que una operaci&amp;oacute;n com&amp;uacute;n consiste en recuperar la ruta con la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="066ac8e9a006935363e41b727fa078b6cbe4a689" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation is to retrieve the path with one of the functions &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9503ea7db44738c355b5bdf4d0264864cc9e2161" translate="yes" xml:space="preserve">
          <source>The Lasso is a linear model that estimates sparse coefficients with l1 regularization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f5aa687d48724082c940128def6bdb3f6066405" translate="yes" xml:space="preserve">
          <source>The Lasso optimization function varies for mono and multi-outputs.</source>
          <target state="translated">La función de optimización del lazo varía para las salidas mono y multi-salidas.</target>
        </trans-unit>
        <trans-unit id="3c1b2c3b1551762e6f1da2d8a29e9acf6404c123" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">El solucionador de lazo a utilizar:descenso coordinado o LARS.Utilice LARS para gráficos subyacentes muy escasos,donde el número de características es mayor que el número de muestras.En otros lugares prefiera el cd que es más estable numéricamente.</target>
        </trans-unit>
        <trans-unit id="2de8b3228aef7c3b031e79cb56d858268d66f64e" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p &amp;gt; n. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">El solucionador de Lasso para usar: descenso de coordenadas o LARS. Utilice LARS para gr&amp;aacute;ficos subyacentes muy dispersos, donde p&amp;gt; n. En otros lugares prefiere cd, que es m&amp;aacute;s estable num&amp;eacute;ricamente.</target>
        </trans-unit>
        <trans-unit id="59ce670bca31c79340517b84d85b37c8ca4886a2" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c4c48b93f6a66f7991382ba54fcfe5fb18a6ebb" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">El estimador LedoitWolf de la matriz de covarianza puede ser calculado en una muestra con el &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt; funci&amp;oacute;n de la &lt;code&gt;sklearn.covariance&lt;/code&gt; paquete, o puede ser obtenida de otra manera por el ajuste de una &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt; objeto a la misma muestra.</target>
        </trans-unit>
        <trans-unit id="aec2f6a4d0fb4e3ecc2ba56ea33e122b851a535b" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset constains two small dataset:</source>
          <target state="translated">El conjunto de datos de Linnerud contiene dos pequeños conjuntos de datos:</target>
        </trans-unit>
        <trans-unit id="6be9aee52b8392e35ed4aeda9bdbad80150f0295" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0460202c91e1eee28482597354a8366af4e5eb02" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for novelty detection. Note that when LOF is used for novelty detection you MUST not use predict, decision_function and score_samples on the training set as this would lead to wrong results. You must only use these methods on new unseen data (which are not in the training set). See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for outlier detection.</source>
          <target state="translated">El algoritmo Local Outlier Factor (LOF) es un m&amp;eacute;todo de detecci&amp;oacute;n de anomal&amp;iacute;as no supervisado que calcula la desviaci&amp;oacute;n de densidad local de un punto de datos dado con respecto a sus vecinos. Considera como valores at&amp;iacute;picos las muestras que tienen una densidad sustancialmente m&amp;aacute;s baja que sus vecinas. Este ejemplo muestra c&amp;oacute;mo utilizar LOF para la detecci&amp;oacute;n de novedades. Tenga en cuenta que cuando se utiliza LOF para la detecci&amp;oacute;n de novedades, no DEBE utilizar predict, decision_function y score_samples en el conjunto de entrenamiento, ya que esto conducir&amp;iacute;a a resultados incorrectos. Solo debe utilizar estos m&amp;eacute;todos en datos nuevos no vistos (que no est&amp;aacute;n en el conjunto de entrenamiento). Consulte &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;la Gu&amp;iacute;a del usuario&lt;/a&gt; : para obtener detalles sobre la diferencia entre la detecci&amp;oacute;n de valores at&amp;iacute;picos y la detecci&amp;oacute;n de novedades, y c&amp;oacute;mo utilizar LOF para la detecci&amp;oacute;n de valores at&amp;iacute;picos.</target>
        </trans-unit>
        <trans-unit id="d0e3831e71a419f2c6e3df80ad462189c4208703" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection.</source>
          <target state="translated">El algoritmo Local Outlier Factor (LOF) es un m&amp;eacute;todo de detecci&amp;oacute;n de anomal&amp;iacute;as no supervisado que calcula la desviaci&amp;oacute;n de densidad local de un punto de datos dado con respecto a sus vecinos. Considera como valores at&amp;iacute;picos las muestras que tienen una densidad sustancialmente m&amp;aacute;s baja que sus vecinas. Este ejemplo muestra c&amp;oacute;mo usar LOF para la detecci&amp;oacute;n de valores at&amp;iacute;picos, que es el caso de uso predeterminado de este estimador en scikit-learn. Tenga en cuenta que cuando se utiliza LOF para la detecci&amp;oacute;n de valores at&amp;iacute;picos, no tiene m&amp;eacute;todos de predicci&amp;oacute;n, funci&amp;oacute;n de decisi&amp;oacute;n y muestras de puntuaci&amp;oacute;n. Consulte &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;la Gu&amp;iacute;a del usuario&lt;/a&gt; : para obtener detalles sobre la diferencia entre la detecci&amp;oacute;n de valores at&amp;iacute;picos y la detecci&amp;oacute;n de novedades, y c&amp;oacute;mo utilizar LOF para la detecci&amp;oacute;n de novedades.</target>
        </trans-unit>
        <trans-unit id="e334ea68d0fb9bc258e189ffac7524671f8600d5" translate="yes" xml:space="preserve">
          <source>The MLLE algorithm comprises three stages:</source>
          <target state="translated">El algoritmo MLLE comprende tres etapas:</target>
        </trans-unit>
        <trans-unit id="514baa8efb372e193dd4209109d33c4d6c755f88" translate="yes" xml:space="preserve">
          <source>The Matplotlib Figure object.</source>
          <target state="translated">El objeto de la figura Matplotlib.</target>
        </trans-unit>
        <trans-unit id="5adf2471ff49dbf27acc1a8b4862b530e5b52366" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).</source>
          <target state="translated">El coeficiente de correlación de Matthews (+1 representa una predicción perfecta,0 una predicción media aleatoria y -1 y una predicción inversa).</target>
        </trans-unit>
        <trans-unit id="21efcbf7188af02e8a17818fd892c631c3cc436c" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. [source: Wikipedia]</source>
          <target state="translated">El coeficiente de correlación de Matthews se utiliza en el aprendizaje automático como medida de la calidad de las clasificaciones binarias y multiclase.Tiene en cuenta los verdaderos y falsos positivos y negativos y se considera en general una medida equilibrada que puede utilizarse incluso si las clases son de tamaños muy diferentes.El MCC es en esencia un valor de coeficiente de correlación entre -1 y +1.Un coeficiente de +1 representa una predicción perfecta,0 una predicción media aleatoria y -1 una predicción inversa.La estadística también se conoce como el coeficiente phi.[fuente:Wikipedia]</target>
        </trans-unit>
        <trans-unit id="6faebd1cede47746805364be40ce28b059dea40d" translate="yes" xml:space="preserve">
          <source>The Mean Squared Error (in the sense of the Frobenius norm) between &lt;code&gt;self&lt;/code&gt; and &lt;code&gt;comp_cov&lt;/code&gt; covariance estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3bca24cc7fb644916020d52b90a1ddb7136c5be" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.</source>
          <target state="translated">El estimador de la covarianza mínima determinante de la covarianza se aplicará a los datos distribuidos en Gauss,pero podría seguir siendo pertinente en los datos extraídos de una distribución unimodal y simétrica.No está pensado para ser utilizado con datos multimodales (es probable que el algoritmo utilizado para ajustar un objeto MinCovDet falle en ese caso).Hay que considerar los métodos de búsqueda de proyecciones para tratar con conjuntos de datos multimodales.</target>
        </trans-unit>
        <trans-unit id="451133ac20cdcd95892f17968e1c348a4b0f4b8f" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">El estimador de determinante de covarianza m&amp;iacute;nima (MCD) ha sido introducido por PJRousseuw en &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e452597962db33c03eccea44c483adb03cd8dfa7" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;3&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d811e159b509e69af06fa8b1a6b421d8a9ea40ca" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in [1].</source>
          <target state="translated">El estimador del determinante de la covarianza mínima (MCD)ha sido introducido por P.J.Rousseuw en [1].</target>
        </trans-unit>
        <trans-unit id="ecfb18631be5aab9ee26a60642b369e5e4e48a3b" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;3&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92f2af418b12a7a58727045874e736340d33efbd" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">El estimador de determinante de covarianza m&amp;iacute;nima es un estimador robusto de la covarianza de un conjunto de datos introducido por PJ Rousseeuw en &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt; . La idea es encontrar una proporci&amp;oacute;n dada (h) de observaciones &quot;buenas&quot; que no sean valores at&amp;iacute;picos y calcular su matriz de covarianza emp&amp;iacute;rica. Esta matriz de covarianza emp&amp;iacute;rica luego se reescala para compensar la selecci&amp;oacute;n de observaciones realizada (&quot;paso de consistencia&quot;). Habiendo calculado el estimador del determinante m&amp;iacute;nimo de covarianza, se pueden ponderar las observaciones de acuerdo con su distancia de Mahalanobis, lo que lleva a una estimaci&amp;oacute;n reponderada de la matriz de covarianza del conjunto de datos (&amp;ldquo;paso de reponderaci&amp;oacute;n&amp;rdquo;).</target>
        </trans-unit>
        <trans-unit id="26a5ec87ffc45e2d236a64ca891cc8c307910b23" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples} - n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples} + n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance. After a correction step aiming at compensating the fact that the estimates were learned from only a portion of the initial data, we end up with robust estimates of the data set location and covariance.</source>
          <target state="translated">El estimador de determinante de covarianza m&amp;iacute;nimo es un punto de ruptura alto y robusto (es decir, se puede utilizar para estimar la matriz de covarianza de conjuntos de datos altamente contaminados, hasta \ (\ frac {n_ \ text {samples} - n_ \ text {features} -) 1} {2} \) outliers) estimador de covarianza. La idea es encontrar \ (\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \) observaciones cuya covarianza emp&amp;iacute;rica tiene el determinante m&amp;aacute;s peque&amp;ntilde;o, produciendo un subconjunto &quot;puro&quot; de observaciones de las cuales para calcular estimaciones est&amp;aacute;ndar de ubicaci&amp;oacute;n y covarianza. Despu&amp;eacute;s de un paso de correcci&amp;oacute;n con el objetivo de compensar el hecho de que las estimaciones se aprendieron de solo una parte de los datos iniciales, terminamos con estimaciones s&amp;oacute;lidas de la ubicaci&amp;oacute;n y la covarianza del conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="868c6b821132444289ad5893f70525225594997c" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples}-n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples}+n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance.</source>
          <target state="translated">El estimador de determinante de covarianza m&amp;iacute;nimo es un punto de ruptura alto y robusto (es decir, se puede usar para estimar la matriz de covarianza de conjuntos de datos altamente contaminados, hasta \ (\ frac {n_ \ text {samples} -n_ \ text {features} - 1} {2} \) outliers) estimador de covarianza. La idea es encontrar \ (\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \) observaciones cuya covarianza emp&amp;iacute;rica tiene el determinante m&amp;aacute;s peque&amp;ntilde;o, produciendo un subconjunto &quot;puro&quot; de observaciones de las cuales para calcular estimaciones est&amp;aacute;ndar de ubicaci&amp;oacute;n y covarianza.</target>
        </trans-unit>
        <trans-unit id="b2ce758bd900de6631654a32b9b8962161ffb45f" translate="yes" xml:space="preserve">
          <source>The Mutual Information is a measure of the similarity between two labels of the same data. Where \(|U_i|\) is the number of the samples in cluster \(U_i\) and \(|V_j|\) is the number of the samples in cluster \(V_j\), the Mutual Information between clusterings \(U\) and \(V\) is given as:</source>
          <target state="translated">La información mutua es una medida de la similitud entre dos etiquetas de los mismos datos.Donde \ ~ es el número de las muestras en el grupo \ ~ (U_i)y \ ~-(V_j)es el número de las muestras en el grupo \ ~-(V_j),la Información Mutua entre las agrupaciones \ ~-(U)y \ ~-(V)se da como:</target>
        </trans-unit>
        <trans-unit id="4607a569600ef039f2071e80730b55efccd8dd8d" translate="yes" xml:space="preserve">
          <source>The Nystroem method, as implemented in &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; uses the &lt;code&gt;rbf&lt;/code&gt; kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter &lt;code&gt;n_components&lt;/code&gt;.</source>
          <target state="translated">El m&amp;eacute;todo Nystroem, implementado en &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; ,&lt;/a&gt; es un m&amp;eacute;todo general para aproximaciones de kernels de bajo rango. Lo logra esencialmente submuestreando los datos en los que se eval&amp;uacute;a el kernel. Por defecto, &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; usa el kernel &lt;code&gt;rbf&lt;/code&gt; , pero puede usar cualquier funci&amp;oacute;n del kernel o una matriz de kernel precalculada. El n&amp;uacute;mero de muestras utilizadas, que tambi&amp;eacute;n es la dimensionalidad de las caracter&amp;iacute;sticas calculadas, viene dada por el par&amp;aacute;metro &lt;code&gt;n_components&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="acd97cd02b3cf6e9137b2a9dda0c8e62c31c52e4" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad47de32445041d6ef99f77621d867a03cd9751f" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">El estimador de la OEA de la matriz de covarianza puede ser calculado en una muestra con el &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt; funci&amp;oacute;n de la &lt;code&gt;sklearn.covariance&lt;/code&gt; paquete, o puede ser obtenida de otra manera por el montaje de un &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt; objeto a la misma muestra.</target>
        </trans-unit>
        <trans-unit id="6f45a4e82bc4c1b489f40318ecca4028acd0f46e" translate="yes" xml:space="preserve">
          <source>The One-Class SVM has been introduced by Sch&amp;ouml;lkopf et al. for that purpose and implemented in the &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; module in the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The \(\nu\) parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.</source>
          <target state="translated">El SVM de una clase ha sido introducido por Sch&amp;ouml;lkopf et al. para ese prop&amp;oacute;sito e implementado en el m&amp;oacute;dulo &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; en el objeto &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt; . Requiere la elecci&amp;oacute;n de un kernel y un par&amp;aacute;metro escalar para definir una frontera. Por lo general, se elige el kernel RBF, aunque no existe una f&amp;oacute;rmula o algoritmo exactos para establecer su par&amp;aacute;metro de ancho de banda. Este es el valor predeterminado en la implementaci&amp;oacute;n de scikit-learn. El par&amp;aacute;metro \ (\ nu \), tambi&amp;eacute;n conocido como el margen de la SVM de una clase, corresponde a la probabilidad de encontrar una observaci&amp;oacute;n nueva, pero regular, fuera de la frontera.</target>
        </trans-unit>
        <trans-unit id="ce00ae4f245475e7026810e89464783863843edd" translate="yes" xml:space="preserve">
          <source>The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.</source>
          <target state="translated">El PCA hace una reducción de dimensionalidad sin supervisión,mientras que la regresión logística hace la predicción.</target>
        </trans-unit>
        <trans-unit id="04f0f7d7b53f41cd954be6291735d195a21f52b7" translate="yes" xml:space="preserve">
          <source>The Poisson deviance cannot be computed on non-positive values predicted by the model. For models that do return a few non-positive predictions (e.g. &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;) we ignore the corresponding samples, meaning that the obtained Poisson deviance is approximate. An alternative approach could be to use &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt;&lt;code&gt;TransformedTargetRegressor&lt;/code&gt;&lt;/a&gt; meta-estimator to map &lt;code&gt;y_pred&lt;/code&gt; to a strictly positive domain.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc5a34aa26f7428f3e35a346a70f3689822fe771" translate="yes" xml:space="preserve">
          <source>The Poisson deviance computed as an evaluation metric reflects both the calibration and the ranking power of the model. It also makes a linear assumption on the ideal relationship between the expected value and the variance of the response variable. For the sake of conciseness we did not check whether this assumption holds.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="450889f232106f285e0f6ca8296ef879dd035563" translate="yes" xml:space="preserve">
          <source>The Probability Density Functions (PDF) of these distributions are illustrated in the following figure,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="935532caca213849eac9af7588fc52963397d517" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11e4fa9675506dfc6e48ea958dcc499e9784ad90" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;sklearn.multioutput.multioutputregressor#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a83459e6ef7baec271df4e0325c8da4e8711242a" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33c279bdcb922f55f225b113a682e1a5cf28cc6d" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter length_scale&amp;gt;0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">El kernel RBF es un kernel estacionario. Tambi&amp;eacute;n se conoce como el n&amp;uacute;cleo &quot;exponencial al cuadrado&quot;. Est&amp;aacute; parametrizado por un par&amp;aacute;metro de escala de longitud length_scale&amp;gt; 0, que puede ser un escalar (variante isotr&amp;oacute;pica del kernel) o un vector con el mismo n&amp;uacute;mero de dimensiones que las entradas X (variante anisotr&amp;oacute;pica del kernel). El n&amp;uacute;cleo est&amp;aacute; dado por:</target>
        </trans-unit>
        <trans-unit id="f47997c4010f2e20accd04690970d6547a71bfc2" translate="yes" xml:space="preserve">
          <source>The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.</source>
          <target state="translated">El núcleo RBF producirá un gráfico totalmente conectado que se representa en la memoria por una densa matriz.Esta matriz puede ser muy grande y,combinada con el costo de realizar un cálculo de multiplicación de la matriz completa para cada iteración del algoritmo,puede llevar a tiempos de ejecución prohibitivamente largos.Por otra parte,el núcleo KNN producirá una matriz densa mucho más fácil de memorizar que puede reducir drásticamente los tiempos de ejecución.</target>
        </trans-unit>
        <trans-unit id="512de356729b7d551b43d5d76bd8681dea941e19" translate="yes" xml:space="preserve">
          <source>The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.</source>
          <target state="translated">La GBR intenta maximizar la probabilidad de los datos utilizando un modelo gr&amp;aacute;fico particular. El algoritmo de aprendizaje de par&amp;aacute;metros utilizado ( &lt;a href=&quot;#sml&quot;&gt;estoc&amp;aacute;stico de m&amp;aacute;xima verosimilitud&lt;/a&gt; ) evita que las representaciones se desv&amp;iacute;en de los datos de entrada, lo que las hace capturar regularidades interesantes, pero hace que el modelo sea menos &amp;uacute;til para conjuntos de datos peque&amp;ntilde;os y, por lo general, no es &amp;uacute;til para la estimaci&amp;oacute;n de densidad.</target>
        </trans-unit>
        <trans-unit id="95d48012cfd5cdfda48a80ad45de8292db32665c" translate="yes" xml:space="preserve">
          <source>The R^2 score or ndarray of scores if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">La puntuaci&amp;oacute;n R ^ 2 o ndarray de puntuaciones si 'multioutput' es 'raw_values'.</target>
        </trans-unit>
        <trans-unit id="57cf85e67b74d4b36ce4e00a60cc2afb37409d42" translate="yes" xml:space="preserve">
          <source>The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.</source>
          <target state="translated">El índice Rand calcula una medida de similitud entre dos agrupaciones considerando todos los pares de muestras y contando los pares que se asignan en las mismas o diferentes agrupaciones en las agrupaciones pronosticadas y verdaderas.</target>
        </trans-unit>
        <trans-unit id="21d3f8892ca41bf337636e90759152be22d9788c" translate="yes" xml:space="preserve">
          <source>The RandomTreesEmbedding, from the &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module, is not technically a manifold embedding method, as it learn a high-dimensional representation on which we apply a dimensionality reduction method. However, it is often useful to cast a dataset into a representation in which the classes are linearly-separable.</source>
          <target state="translated">El RandomTreesEmbedding, del m&amp;oacute;dulo &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; , no es t&amp;eacute;cnicamente un m&amp;eacute;todo de incrustaci&amp;oacute;n m&amp;uacute;ltiple, ya que aprende una representaci&amp;oacute;n de alta dimensi&amp;oacute;n en la que aplicamos un m&amp;eacute;todo de reducci&amp;oacute;n de dimensionalidad. Sin embargo, a menudo es &amp;uacute;til convertir un conjunto de datos en una representaci&amp;oacute;n en la que las clases son linealmente separables.</target>
        </trans-unit>
        <trans-unit id="9863f2af9163cbec042d3a8db1d9d0da7a79bddf" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length scales. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a scale mixture parameter \(\alpha&amp;gt;0\). Only the isotropic variant where length_scale \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ddaea903844387569c2d558e504bb67163af0e53" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a scale mixture parameter alpha&amp;gt;0. Only the isotropic variant where length_scale is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">El kernel RationalQuadratic puede verse como una mezcla de escalas (una suma infinita) de kernels RBF con diferentes escalas de longitud caracter&amp;iacute;sticas. Est&amp;aacute; parametrizado por un par&amp;aacute;metro de escala de longitud length_scale&amp;gt; 0 y un par&amp;aacute;metro de mezcla de escala alpha&amp;gt; 0. En este momento, solo se admite la variante isotr&amp;oacute;pica donde length_scale es un escalar. El kernel dado por:</target>
        </trans-unit>
        <trans-unit id="4290d451a4bc3d8974b88b0877f706f5c5e0d4c1" translate="yes" xml:space="preserve">
          <source>The SAGA solver supports both float64 and float32 bit arrays.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66cebb865fc92b18216814d7cca47981b6cb6b41" translate="yes" xml:space="preserve">
          <source>The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the &lt;em&gt;stress&lt;/em&gt;) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.</source>
          <target state="translated">El algoritmo SMACOF (Scaling by MAjorizing a COmplicated Function) es un algoritmo de escalado multidimensional que minimiza una funci&amp;oacute;n objetivo (el &lt;em&gt;estr&amp;eacute;s&lt;/em&gt; ) utilizando una t&amp;eacute;cnica de mayorizaci&amp;oacute;n. La mayorizaci&amp;oacute;n de la tensi&amp;oacute;n, tambi&amp;eacute;n conocida como Transformada de Guttman, garantiza una convergencia mon&amp;oacute;tona de la tensi&amp;oacute;n y es m&amp;aacute;s poderosa que las t&amp;eacute;cnicas tradicionales como el descenso de gradientes.</target>
        </trans-unit>
        <trans-unit id="e2c4971923aff3f7fe2f8678fb678a8a1fe12716" translate="yes" xml:space="preserve">
          <source>The SMACOF algorithm for metric MDS can summarized by the following steps:</source>
          <target state="translated">El algoritmo SMACOF para el MDS métrico puede resumirse en los siguientes pasos:</target>
        </trans-unit>
        <trans-unit id="9991accadfafce42fd0097ac5bf1452a92166503" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient &lt;em&gt;s&lt;/em&gt; for a single sample is then given as:</source>
          <target state="translated">El coeficiente de silueta &lt;em&gt;s&lt;/em&gt; para una sola muestra se da como:</target>
        </trans-unit>
        <trans-unit id="977018d27e576be441891586d29895a67820e9bb" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.</source>
          <target state="translated">El Coeficiente de Silueta para un conjunto de muestras se da como la media del Coeficiente de Silueta para cada muestra.</target>
        </trans-unit>
        <trans-unit id="a8aa62253fcb64807b1a0f2a014811a76f4c09cc" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.</source>
          <target state="translated">El Coeficiente de Silueta es una medida de lo bien que las muestras se agrupan con muestras que son similares a ellas mismas.Se dice que los modelos de agrupación con un alto Coeficiente de Silueta son densos,donde las muestras en el mismo grupo son similares entre sí,y bien separados,donde las muestras en diferentes grupos no son muy similares entre sí.</target>
        </trans-unit>
        <trans-unit id="5da8604230381cdf66d3aec27ea08ace466e606c" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">El coeficiente de silueta se calcula utilizando la distancia media dentro del conglomerado ( &lt;code&gt;a&lt;/code&gt; ) y la distancia media del conglomerado m&amp;aacute;s cercano ( &lt;code&gt;b&lt;/code&gt; ) para cada muestra. El coeficiente de silueta para una muestra es &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; . Tenga en cuenta que el coeficiente de silueta solo se define si el n&amp;uacute;mero de etiquetas es 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</target>
        </trans-unit>
        <trans-unit id="4273a368cac49099b8caa7c819b214a9d85c0ff4" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. To clarify, &lt;code&gt;b&lt;/code&gt; is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">El coeficiente de silueta se calcula utilizando la distancia media dentro del conglomerado ( &lt;code&gt;a&lt;/code&gt; ) y la distancia media del conglomerado m&amp;aacute;s cercano ( &lt;code&gt;b&lt;/code&gt; ) para cada muestra. El coeficiente de silueta para una muestra es &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; . Para aclarar, &lt;code&gt;b&lt;/code&gt; es la distancia entre una muestra y el conglomerado m&amp;aacute;s cercano del que la muestra no forma parte. Tenga en cuenta que el coeficiente de silueta solo se define si el n&amp;uacute;mero de etiquetas es 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</target>
        </trans-unit>
        <trans-unit id="74a577cf3f9facf0caa36a3cd46ce91a25197ef1" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">El Coeficiente de Silueta es generalmente más alto para cúmulos convexos que otros conceptos de cúmulos,como los cúmulos basados en la densidad como los obtenidos a través del DBSCAN.</target>
        </trans-unit>
        <trans-unit id="b08373a116f8a28b8f59b4a939f8e86bc18a285c" translate="yes" xml:space="preserve">
          <source>The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result.</source>
          <target state="translated">El coeficiente de correlación de Spearman se estima a partir de los datos,y se utiliza como resultado el signo de la estimación resultante.</target>
        </trans-unit>
        <trans-unit id="1aca461bec942a3ea49c28ca350e80c03610ed92" translate="yes" xml:space="preserve">
          <source>The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:</source>
          <target state="translated">El algoritmo de incrustación espectral (Eigenmaps Laplacianos)comprende tres etapas:</target>
        </trans-unit>
        <trans-unit id="5c2cf1989e094ea2eafd60dd4bfcc701bf5b4819" translate="yes" xml:space="preserve">
          <source>The Stack Exchange family of sites hosts &lt;a href=&quot;https://meta.stackexchange.com/q/130524&quot;&gt;multiple subdomains for Machine Learning questions&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="530048073ab2f0fd4beae1a41150629fce3bbc04" translate="yes" xml:space="preserve">
          <source>The TF-IDF vectorized posts form a word frequency matrix, which is then biclustered using Dhillon&amp;rsquo;s Spectral Co-Clustering algorithm. The resulting document-word biclusters indicate subsets words used more often in those subsets documents.</source>
          <target state="translated">Las publicaciones vectorizadas TF-IDF forman una matriz de frecuencia de palabras, que luego se biclusa utilizando el algoritmo Spectral Co-Clustering de Dhillon. Los biclusters de documentos y palabras resultantes indican subconjuntos de palabras que se utilizan con m&amp;aacute;s frecuencia en esos subconjuntos de documentos.</target>
        </trans-unit>
        <trans-unit id="a51b4bd7337a8a43bf76bb00c70355d636e98cf2" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">La medida V es en realidad equivalente a la informaci&amp;oacute;n mutua (NMI) discutida anteriormente, siendo la funci&amp;oacute;n de agregaci&amp;oacute;n la media aritm&amp;eacute;tica &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="4af22841a0f64abdacec5694995ece03a0f97488" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id16&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28e22a992327910c0281c7997fd0381a055b2da5" translate="yes" xml:space="preserve">
          <source>The V-measure is the harmonic mean between homogeneity and completeness:</source>
          <target state="translated">La medida V es la media armónica entre la homogeneidad y la completitud:</target>
        </trans-unit>
        <trans-unit id="8e9e469c4bec48fbecca2280bcc57b2302181e26" translate="yes" xml:space="preserve">
          <source>The WAGE is increasing when EDUCATION is increasing. Note that the dependence between WAGE and EDUCATION represented here is a marginal dependence, i.e., it describes the behavior of a specific variable without keeping the others fixed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d831d6fbe54cab7c78889ea027eda02af846327" translate="yes" xml:space="preserve">
          <source>The Yeo-Johnson transform is given by:</source>
          <target state="translated">La transformación de Yeo-Johnson está dada por:</target>
        </trans-unit>
        <trans-unit id="dd310b3ef8cade9b7b44463cf24d9960d1a07902" translate="yes" xml:space="preserve">
          <source>The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \(Z\):</source>
          <target state="translated">Los vectores singulares,a partir del segundo,proporcionan la información de partición deseada.Se usan para formar la matriz:</target>
        </trans-unit>
        <trans-unit id="0151312e57232e026343f2a42d71b34eecec6ec6" translate="yes" xml:space="preserve">
          <source>The \(\nu\)-SVC formulation &lt;a href=&quot;#id17&quot; id=&quot;id8&quot;&gt;15&lt;/a&gt; is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bae7eee8e57958cb7a015c09e85982ab959fe35" translate="yes" xml:space="preserve">
          <source>The \(k\)-neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; is the most commonly used technique. The optimal choice of the value \(k\) is highly data-dependent: in general a larger \(k\) suppresses the effects of noise, but makes the classification boundaries less distinct.</source>
          <target state="translated">La clasificaci&amp;oacute;n \ (k \) - vecinos en &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt; es la t&amp;eacute;cnica m&amp;aacute;s utilizada. La elecci&amp;oacute;n &amp;oacute;ptima del valor \ (k \) depende en gran medida de los datos: en general, un \ (k \) mayor suprime los efectos del ruido, pero hace que los l&amp;iacute;mites de clasificaci&amp;oacute;n sean menos distintos.</target>
        </trans-unit>
        <trans-unit id="e8862ed89c87547157e194d96a8f490026b2e0fb" translate="yes" xml:space="preserve">
          <source>The ability to reproduce the data of the regularized model is similar to the one of the non-regularized model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ea48a47287b56e3f66c9ec81d51c291e8fedf59" translate="yes" xml:space="preserve">
          <source>The above vectorization scheme is simple but the fact that it holds an &lt;strong&gt;in- memory mapping from the string tokens to the integer feature indices&lt;/strong&gt; (the &lt;code&gt;vocabulary_&lt;/code&gt; attribute) causes several &lt;strong&gt;problems when dealing with large datasets&lt;/strong&gt;:</source>
          <target state="translated">El esquema de vectorizaci&amp;oacute;n anterior es simple, pero el hecho de que contiene un &lt;strong&gt;mapeo en memoria de los tokens de cadena a los &amp;iacute;ndices de caracter&amp;iacute;sticas enteras&lt;/strong&gt; (el atributo &lt;code&gt;vocabulary_&lt;/code&gt; ) causa varios &lt;strong&gt;problemas cuando se trata de grandes conjuntos de datos&lt;/strong&gt; :</target>
        </trans-unit>
        <trans-unit id="f78fe42a387595f69f94294d9712aa8fff9cf7ee" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores during early stopping. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a971e24181d9be44e4bb51f3963013ec9c9d01d5" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58279b870becff96bab0fc34e17d7f045af03b34" translate="yes" xml:space="preserve">
          <source>The abstract base class for all kernels is &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt;. Kernel implements a similar interface as &lt;code&gt;Estimator&lt;/code&gt;, providing the methods &lt;code&gt;get_params()&lt;/code&gt;, &lt;code&gt;set_params()&lt;/code&gt;, and &lt;code&gt;clone()&lt;/code&gt;. This allows setting kernel values also via meta-estimators such as &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;GridSearch&lt;/code&gt;. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with &lt;code&gt;k1__&lt;/code&gt; and parameters of the right operand with &lt;code&gt;k2__&lt;/code&gt;. An additional convenience method is &lt;code&gt;clone_with_theta(theta)&lt;/code&gt;, which returns a cloned version of the kernel but with the hyperparameters set to &lt;code&gt;theta&lt;/code&gt;. An illustrative example:</source>
          <target state="translated">La clase base abstracta para todos los n&amp;uacute;cleos es &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt; . Kernel implementa una interfaz similar a &lt;code&gt;Estimator&lt;/code&gt; , proporcionando los m&amp;eacute;todos &lt;code&gt;get_params()&lt;/code&gt; , &lt;code&gt;set_params()&lt;/code&gt; y &lt;code&gt;clone()&lt;/code&gt; . Esto permite configurar los valores del kernel tambi&amp;eacute;n a trav&amp;eacute;s de &lt;code&gt;GridSearch&lt;/code&gt; como &lt;code&gt;Pipeline&lt;/code&gt; o GridSearch . Tenga en cuenta que debido a la estructura anidada de los n&amp;uacute;cleos (mediante la aplicaci&amp;oacute;n de operadores del n&amp;uacute;cleo, ver m&amp;aacute;s abajo), los nombres de los par&amp;aacute;metros del n&amp;uacute;cleo pueden volverse relativamente complicados. En general, para un operador de kernel binario, los par&amp;aacute;metros del operando izquierdo tienen el prefijo &lt;code&gt;k1__&lt;/code&gt; y los par&amp;aacute;metros del operando derecho con &lt;code&gt;k2__&lt;/code&gt; . Un m&amp;eacute;todo de conveniencia adicional es &lt;code&gt;clone_with_theta(theta)&lt;/code&gt; , que devuelve una versi&amp;oacute;n clonada del kernel pero con los hiperpar&amp;aacute;metros establecidos en &lt;code&gt;theta&lt;/code&gt; . Un ejemplo ilustrativo:</target>
        </trans-unit>
        <trans-unit id="d6dfb2a865fc7561c9990f60831354c8c2b2acf1" translate="yes" xml:space="preserve">
          <source>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: &amp;ldquo;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&amp;rdquo;, Optimization Methods and Software 1, 1992, 23-34].</source>
          <target state="translated">El programa lineal real utilizado para obtener el plano de separaci&amp;oacute;n en el espacio tridimensional es el que se describe en: [KP Bennett y OL Mangasarian: &amp;ldquo;Discriminaci&amp;oacute;n de programaci&amp;oacute;n lineal robusta de dos conjuntos linealmente inseparables&amp;rdquo;, M&amp;eacute;todos de optimizaci&amp;oacute;n y software 1, 1992, 23- 34].</target>
        </trans-unit>
        <trans-unit id="5c16c787f9e7f87c21ce4f86533b13082a6022ce" translate="yes" xml:space="preserve">
          <source>The actual number of iteration performed by the solver. Only returned if &lt;code&gt;return_n_iter&lt;/code&gt; is True.</source>
          <target state="translated">El n&amp;uacute;mero real de iteraciones realizadas por el solucionador. Solo se devuelve si &lt;code&gt;return_n_iter&lt;/code&gt; es True.</target>
        </trans-unit>
        <trans-unit id="9cc109786e99b4c544e4621619329291c5062f5c" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68fff8e45d31e6c0edd22bde7c766ffe083896ee" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6a597b70b1c3dba7fc91a33b2b458a1718ae376" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion.</source>
          <target state="translated">El número real de iteraciones para alcanzar el criterio de parada.</target>
        </trans-unit>
        <trans-unit id="be3e6ed927427ff958a3dc691ec6f2ca38b153e8" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">El número real de iteraciones para alcanzar el criterio de parada.Para los ajustes multiclase,es el máximo sobre cada ajuste binario.</target>
        </trans-unit>
        <trans-unit id="905122fa4ae495ec172d69201d288d9cbd8eb107" translate="yes" xml:space="preserve">
          <source>The actual number of neighbors used for &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; queries.</source>
          <target state="translated">El n&amp;uacute;mero real de vecinos utilizados para consultas de &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="d1e2217f69f6ffa7102e10d2dcd6eed0b5a03524" translate="yes" xml:space="preserve">
          <source>The actual number of quantiles used to discretize the cumulative distribution function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99f948dd8ff8ebdf8ccc1268f27f992c4d54a1e0" translate="yes" xml:space="preserve">
          <source>The actual number of samples</source>
          <target state="translated">El número real de muestras</target>
        </trans-unit>
        <trans-unit id="83f4574f37ccf239bd98ad8929c323f413ece25a" translate="yes" xml:space="preserve">
          <source>The actual number of samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="257683b273bf01e4ad40af7f18f19c426c837478" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel as used here is given by</source>
          <target state="translated">El núcleo cuadrado de chi aditivo,como se usa aquí,viene dado por</target>
        </trans-unit>
        <trans-unit id="721e722946fa16cdc1d4e80a122a59df383d3b35" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel is a kernel on histograms, often used in computer vision.</source>
          <target state="translated">El núcleo cuadrado de chi aditivo es un núcleo en los histogramas,a menudo utilizado en la visión por ordenador.</target>
        </trans-unit>
        <trans-unit id="509930cd9616089e31b59f87837fde9a2850682a" translate="yes" xml:space="preserve">
          <source>The additive version of this kernel</source>
          <target state="translated">La versión aditiva de este núcleo</target>
        </trans-unit>
        <trans-unit id="479259eedb54024948ef963b0d114b00fbb05f1b" translate="yes" xml:space="preserve">
          <source>The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split the graph into comparably sized components.</source>
          <target state="translated">La matriz de adyacencia se utiliza para calcular un gráfico normalizado Laplaciano cuyo espectro (especialmente los vectores propios asociados a los valores propios más pequeños)tiene una interpretación en términos del número mínimo de cortes necesarios para dividir el gráfico en componentes de tamaño comparable.</target>
        </trans-unit>
        <trans-unit id="b4f6e52ff52334d95b6f16934aecd26800758ca5" translate="yes" xml:space="preserve">
          <source>The adjacency matrix of the graph to embed.</source>
          <target state="translated">La matriz de adyacencia del gráfico a incrustar.</target>
        </trans-unit>
        <trans-unit id="f8c06d6c9bebb104301c962acd25687eac4074b4" translate="yes" xml:space="preserve">
          <source>The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).</source>
          <target state="translated">De este modo,se garantiza que el índice Rand ajustado tenga un valor cercano a 0,0 para el etiquetado aleatorio,independientemente del número de agrupaciones y muestras,y exactamente 1,0 cuando las agrupaciones son idénticas (hasta una permutación).</target>
        </trans-unit>
        <trans-unit id="83e2303d70450b875b04241e83f4a4153932e0a9" translate="yes" xml:space="preserve">
          <source>The advantage of using approximate explicit feature maps compared to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;kernel trick&lt;/a&gt;, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; can make non-linear learning on large datasets possible.</source>
          <target state="translated">La ventaja de utilizar mapas de caracter&amp;iacute;sticas expl&amp;iacute;citos aproximados en comparaci&amp;oacute;n con el &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;truco&lt;/a&gt; del kernel , que utiliza mapas de caracter&amp;iacute;sticas impl&amp;iacute;citamente, es que las asignaciones expl&amp;iacute;citas pueden ser m&amp;aacute;s adecuadas para el aprendizaje en l&amp;iacute;nea y pueden reducir significativamente el costo de aprendizaje con conjuntos de datos muy grandes. Las SVM est&amp;aacute;ndar con kernel no se adaptan bien a grandes conjuntos de datos, pero utilizando un mapa de kernel aproximado es posible utilizar SVM lineales mucho m&amp;aacute;s eficientes. En particular, la combinaci&amp;oacute;n de aproximaciones de mapas del n&amp;uacute;cleo con &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; puede hacer posible el aprendizaje no lineal en grandes conjuntos de datos.</target>
        </trans-unit>
        <trans-unit id="e8cf8d3d0c8dfd7b6a3c8351c56803f2fdf0d2d8" translate="yes" xml:space="preserve">
          <source>The advantages of Bayesian Regression are:</source>
          <target state="translated">Las ventajas de la Regresión Bayesiana son:</target>
        </trans-unit>
        <trans-unit id="d7269fefe21b18a1d9b1e8a11da2f35edf3e36ec" translate="yes" xml:space="preserve">
          <source>The advantages of GBRT are:</source>
          <target state="translated">Las ventajas de la TRBG son:</target>
        </trans-unit>
        <trans-unit id="6bdcff89c23af609c3f964058bbddc38e45558f8" translate="yes" xml:space="preserve">
          <source>The advantages of Gaussian processes are:</source>
          <target state="translated">Las ventajas de los procesos gausianos son</target>
        </trans-unit>
        <trans-unit id="c632bf8abb99b63a04554183dcc32e66bb0cf004" translate="yes" xml:space="preserve">
          <source>The advantages of LARS are:</source>
          <target state="translated">Las ventajas de LARS son:</target>
        </trans-unit>
        <trans-unit id="54923c815d40d3138e9fbe2ad92c97dd60ef1b2e" translate="yes" xml:space="preserve">
          <source>The advantages of Multi-layer Perceptron are:</source>
          <target state="translated">Las ventajas del Perceptrón Multicapa son:</target>
        </trans-unit>
        <trans-unit id="1dabd14761114fadc83f1114e989744a341117db" translate="yes" xml:space="preserve">
          <source>The advantages of Stochastic Gradient Descent are:</source>
          <target state="translated">Las ventajas del descenso de gradiente estocástico son:</target>
        </trans-unit>
        <trans-unit id="a12cea148a9927c20c87185a7ad428a4bd953760" translate="yes" xml:space="preserve">
          <source>The advantages of support vector machines are:</source>
          <target state="translated">Las ventajas de las máquinas de vector de apoyo son</target>
        </trans-unit>
        <trans-unit id="b3ce019dea8f1fa7178760b3ed7e1bed715f9894" translate="yes" xml:space="preserve">
          <source>The affinity matrix describing the relationship of the samples to embed. &lt;strong&gt;Must be symmetric&lt;/strong&gt;.</source>
          <target state="translated">La matriz de afinidad que describe la relaci&amp;oacute;n de las muestras a incrustar. &lt;strong&gt;Debe ser sim&amp;eacute;trico&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="f8f51936a34abb4d8304c65310991fbeb781a2b1" translate="yes" xml:space="preserve">
          <source>The algorithm automatically sets the number of clusters, instead of relying on a parameter &lt;code&gt;bandwidth&lt;/code&gt;, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided &lt;code&gt;estimate_bandwidth&lt;/code&gt; function, which is called if the bandwidth is not set.</source>
          <target state="translated">El algoritmo establece autom&amp;aacute;ticamente el n&amp;uacute;mero de cl&amp;uacute;steres, en lugar de depender de un &lt;code&gt;bandwidth&lt;/code&gt; par&amp;aacute;metro , que dicta el tama&amp;ntilde;o de la regi&amp;oacute;n para buscar. Este par&amp;aacute;metro se puede configurar manualmente, pero se puede estimar utilizando la funci&amp;oacute;n de ancho de banda &lt;code&gt;estimate_bandwidth&lt;/code&gt; proporcionada , que se llama si no se establece el ancho de banda.</target>
        </trans-unit>
        <trans-unit id="999ae100a463cc0ff60234ca1a1687522770fa40" translate="yes" xml:space="preserve">
          <source>The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is &amp;ldquo;n_samples choose n_subsamples&amp;rdquo;, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.</source>
          <target state="translated">El algoritmo calcula soluciones de m&amp;iacute;nimos cuadrados en subconjuntos con tama&amp;ntilde;o n_submuestras de las muestras en X. Cualquier valor de n_subsamples entre el n&amp;uacute;mero de caracter&amp;iacute;sticas y muestras conduce a un estimador con un compromiso entre robustez y eficiencia. Dado que el n&amp;uacute;mero de soluciones de m&amp;iacute;nimos cuadrados es &quot;n_samples elige n_subsamples&quot;, puede ser extremadamente grande y, por lo tanto, puede limitarse con max_subpopulation. Si se alcanza este l&amp;iacute;mite, los subconjuntos se eligen al azar. En un paso final, se calcula la mediana espacial (o mediana L1) de todas las soluciones de m&amp;iacute;nimos cuadrados.</target>
        </trans-unit>
        <trans-unit id="19207c781cd295df6565aab588711bf1a9323bef" translate="yes" xml:space="preserve">
          <source>The algorithm can also be understood through the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi diagrams&lt;/a&gt;. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.</source>
          <target state="translated">El algoritmo tambi&amp;eacute;n se puede entender a trav&amp;eacute;s del concepto de &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;diagramas&lt;/a&gt; de Voronoi . Primero, se calcula el diagrama de Voronoi de los puntos utilizando los centroides actuales. Cada segmento del diagrama de Voronoi se convierte en un grupo independiente. En segundo lugar, los centroides se actualizan a la media de cada segmento. Luego, el algoritmo repite esto hasta que se cumple un criterio de parada. Por lo general, el algoritmo se detiene cuando la disminuci&amp;oacute;n relativa de la funci&amp;oacute;n objetivo entre iteraciones es menor que el valor de tolerancia dado. Este no es el caso en esta implementaci&amp;oacute;n: la iteraci&amp;oacute;n se detiene cuando los centroides se mueven menos que la tolerancia.</target>
        </trans-unit>
        <trans-unit id="ac387733191797f4111e67a021bb9641f7899ac2" translate="yes" xml:space="preserve">
          <source>The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">El algoritmo empleado para resolver este problema es el algoritmo GLasso, del art&amp;iacute;culo Friedman 2008 Biostatistics. Es el mismo algoritmo que en el paquete R &lt;code&gt;glasso&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2d7b95845283c4d98cc4167d4e0748ee6c69e84f" translate="yes" xml:space="preserve">
          <source>The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. &amp;ldquo;Algorithms for computing the sample variance: Analysis and recommendations.&amp;rdquo; The American Statistician 37.3 (1983): 242-247:</source>
          <target state="translated">El algoritmo para media incremental y std se da en la Ecuaci&amp;oacute;n 1.5a, b en Chan, Tony F., Gene H. Golub y Randall J. LeVeque. &quot;Algoritmos para calcular la varianza de la muestra: an&amp;aacute;lisis y recomendaciones&quot;. The American Statistician 37.3 (1983): 242-247:</target>
        </trans-unit>
        <trans-unit id="c4e655f6aa7cf22a58a123dc60b12c815d9f7df3" translate="yes" xml:space="preserve">
          <source>The algorithm is adapted from Guyon [1] and was designed to generate the &amp;ldquo;Madelon&amp;rdquo; dataset.</source>
          <target state="translated">El algoritmo est&amp;aacute; adaptado de Guyon [1] y fue dise&amp;ntilde;ado para generar el conjunto de datos &quot;Madelon&quot;.</target>
        </trans-unit>
        <trans-unit id="da28c971693f926f3030184039a925bcc2b1ca76" translate="yes" xml:space="preserve">
          <source>The algorithm is from Marsland [1].</source>
          <target state="translated">El algoritmo es de Marsland [1].</target>
        </trans-unit>
        <trans-unit id="94c105f8aebba6a360eae1c93878348f10f58fca" translate="yes" xml:space="preserve">
          <source>The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.</source>
          <target state="translated">El algoritmo no es altamente escalable,ya que requiere múltiples búsquedas del vecino más cercano durante la ejecución del algoritmo.Se garantiza que el algoritmo convergerá,sin embargo,el algoritmo dejará de iterarse cuando el cambio en los centroides sea pequeño.</target>
        </trans-unit>
        <trans-unit id="da3ddd72ab3f78c12c541d3f34c2389bc8e810e8" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including features at each step, the estimated coefficients are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dc6012ab672dce7fe920dc60add16368612e345" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">El algoritmo es similar a la regresi&amp;oacute;n progresiva por pasos, pero en lugar de incluir variables en cada paso, los par&amp;aacute;metros estimados se incrementan en una direcci&amp;oacute;n equiangular a las correlaciones de cada uno con el residual.</target>
        </trans-unit>
        <trans-unit id="0335c4d6784d824f2c45b2464a3517f723de00c5" translate="yes" xml:space="preserve">
          <source>The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.</source>
          <target state="translated">El algoritmo es estocástico y los reinicios múltiples con diferentes semillas pueden producir diferentes incrustaciones.Sin embargo,es perfectamente legítimo elegir la incrustación con el menor error.</target>
        </trans-unit>
        <trans-unit id="7c7de93467b285ca3f0b63a2764ebf7e5ab511ed" translate="yes" xml:space="preserve">
          <source>The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \(b\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.</source>
          <target state="translated">El algoritmo itera entre dos pasos principales,similar al de la vainilla k-means.En el primer paso,se toman muestras al azar del conjunto de datos,para formar un mini lote.Luego se asignan al centroide más cercano.En el segundo paso,se actualizan los centroides.A diferencia de k-means,esto se hace sobre una base de cada muestra.Para cada muestra del mini-lotes,el centroide asignado se actualiza tomando el promedio de la muestra y todas las muestras anteriores asignadas a ese centroide.Esto tiene el efecto de disminuir la tasa de cambio de un centroide a lo largo del tiempo.Estos pasos se realizan hasta que se alcanza la convergencia o un número predeterminado de iteraciones.</target>
        </trans-unit>
        <trans-unit id="eac8adf7ae77ba9768138371c6c71edd5a3a76a7" translate="yes" xml:space="preserve">
          <source>The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.</source>
          <target state="translated">El algoritmo divide las filas y columnas de una matriz de manera que una matriz de tablero de control constante de bloques correspondiente proporcione una buena aproximación a la matriz original.</target>
        </trans-unit>
        <trans-unit id="36533938d0e94afa89a69e7b9609c61646ab71f5" translate="yes" xml:space="preserve">
          <source>The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.</source>
          <target state="translated">El algoritmo divide los datos completos de la muestra de entrada en un conjunto de valores atípicos,que pueden estar sujetos a ruido,y valores atípicos,que son causados,por ejemplo,por mediciones erróneas o hipótesis inválidas sobre los datos.El modelo resultante se estima entonces sólo a partir de los valores atípicos determinados.</target>
        </trans-unit>
        <trans-unit id="76e87cc3ec2713663d8449fef17b4e0c4101c305" translate="yes" xml:space="preserve">
          <source>The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.</source>
          <target state="translated">El algoritmo se detiene cuando alcanza un número máximo preestablecido de iteraciones;o cuando la mejora de la pérdida es inferior a un número determinado y pequeño.</target>
        </trans-unit>
        <trans-unit id="841b170fb7ba2429816cb7c51af4dbae57b471af" translate="yes" xml:space="preserve">
          <source>The algorithm supports sample weights, which can be given by a parameter &lt;code&gt;sample_weight&lt;/code&gt;. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \(X\).</source>
          <target state="translated">El algoritmo admite pesos de muestra, que se pueden proporcionar mediante un par&amp;aacute;metro &lt;code&gt;sample_weight&lt;/code&gt; . Esto permite asignar m&amp;aacute;s peso a algunas muestras al calcular los centros de cl&amp;uacute;steres y los valores de inercia. Por ejemplo, asignar un peso de 2 a una muestra es equivalente a agregar un duplicado de esa muestra al conjunto de datos \ (X \).</target>
        </trans-unit>
        <trans-unit id="a8238e48cf484817aaf23aa2adc5e4a70681d78f" translate="yes" xml:space="preserve">
          <source>The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.</source>
          <target state="translated">El algoritmo que será utilizado por el módulo NearestNeighbors para calcular distancias puntuales y encontrar los vecinos más cercanos.Vea la documentación del módulo NearestNeighbors para más detalles.</target>
        </trans-unit>
        <trans-unit id="3602b334476c395c578aab7a7d09d75f92100f28" translate="yes" xml:space="preserve">
          <source>The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.</source>
          <target state="translated">El algoritmo trata la matriz de datos de entrada como un gráfico bipartito:las filas y columnas de la matriz corresponden a los dos conjuntos de vértices,y cada entrada corresponde a un borde entre una fila y una columna.El algoritmo se aproxima al corte normalizado de este gráfico para encontrar subgráficos pesados.</target>
        </trans-unit>
        <trans-unit id="e4086364ea17b4442e0265a08361358aa304a247" translate="yes" xml:space="preserve">
          <source>The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop.</source>
          <target state="translated">El algoritmo utilizado para estimar los pesos.Se llamará n_componentes veces,es decir,una vez por cada iteración del bucle exterior.</target>
        </trans-unit>
        <trans-unit id="e4e3e486ab0114bca02d02635f9410b77ca99a25" translate="yes" xml:space="preserve">
          <source>The algorithm used to fit the model is coordinate descent.</source>
          <target state="translated">El algoritmo utilizado para ajustar el modelo es el descenso de coordenadas.</target>
        </trans-unit>
        <trans-unit id="17e35eb9c6004e2d60b73104c93585aacf4a2e98" translate="yes" xml:space="preserve">
          <source>The algorithmic complexity of affinity propagation is quadratic in the number of points.</source>
          <target state="translated">La complejidad algorítmica de la propagación de la afinidad es cuadrática en el número de puntos.</target>
        </trans-unit>
        <trans-unit id="4c657d81ec38ba678d89f7b7de187e376e50c0f3" translate="yes" xml:space="preserve">
          <source>The algorithms for regression and classification only differ in the concrete loss function used.</source>
          <target state="translated">Los algoritmos de regresión y clasificación sólo difieren en la función de pérdida concreta utilizada.</target>
        </trans-unit>
        <trans-unit id="50542b2bc530bb15fc1bcd67d7b073d61bea03ab" translate="yes" xml:space="preserve">
          <source>The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.</source>
          <target state="translated">El parámetro alfa en el artículo de selección de estabilidad utilizado para escalar aleatoriamente las características.Debería estar entre 0 y 1.</target>
        </trans-unit>
        <trans-unit id="35d70c04237d10e5da496ff67ac57fce44665490" translate="yes" xml:space="preserve">
          <source>The alpha parameter of the GraphicalLasso setting the sparsity of the model is set by internal cross-validation in the GraphicalLassoCV. As can be seen on figure 2, the grid to compute the cross-validation score is iteratively refined in the neighborhood of the maximum.</source>
          <target state="translated">El parámetro alfa del GraphicalLasso que establece la dispersión del modelo se establece por validación cruzada interna en el GraphicalLassoCV.Como puede verse en la figura 2,la cuadrícula para calcular la puntuación de validación cruzada se refina iterativamente en la vecindad del máximo.</target>
        </trans-unit>
        <trans-unit id="2075fb3135d145905cc5a41db16871a4bf5168da" translate="yes" xml:space="preserve">
          <source>The alpha-quantile of the huber loss function and the quantile loss function. Only if &lt;code&gt;loss='huber'&lt;/code&gt; or &lt;code&gt;loss='quantile'&lt;/code&gt;.</source>
          <target state="translated">El alfa-cuantil de la funci&amp;oacute;n de p&amp;eacute;rdida de huber y la funci&amp;oacute;n de p&amp;eacute;rdida de cuantiles. Solo si &lt;code&gt;loss='huber'&lt;/code&gt; o &lt;code&gt;loss='quantile'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="28a2d4f18d224d5f96bb30d3faa50cdca5ce2935" translate="yes" xml:space="preserve">
          <source>The alphas along the path where models are computed.</source>
          <target state="translated">Los alfas a lo largo de la ruta donde se calculan los modelos.</target>
        </trans-unit>
        <trans-unit id="4390acc7061cc013783802ff89b6303810b0044d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set.</source>
          <target state="translated">La cantidad de contaminación del conjunto de datos,es decir,la proporción de valores atípicos en el conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="a3cf6bdbcc68281bb427886cdd573766d75b2a0b" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Range is (0, 0.5).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db48eba23a1b19c738dbbe1eb3626efb11bd507f" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. If &amp;lsquo;auto&amp;rsquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">La cantidad de contaminaci&amp;oacute;n del conjunto de datos, es decir, la proporci&amp;oacute;n de valores at&amp;iacute;picos en el conjunto de datos. Se utiliza al ajustar para definir el umbral en la funci&amp;oacute;n de decisi&amp;oacute;n. Si es 'autom&amp;aacute;tico', el umbral de la funci&amp;oacute;n de decisi&amp;oacute;n se determina como en el documento original.</target>
        </trans-unit>
        <trans-unit id="f37071a773077642311acdcd66bbd39bda02514d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b69accc98489279e23dd931886f63d4aafd913d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. If &amp;ldquo;auto&amp;rdquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">La cantidad de contaminaci&amp;oacute;n del conjunto de datos, es decir, la proporci&amp;oacute;n de valores at&amp;iacute;picos en el conjunto de datos. Cuando se ajusta, se utiliza para definir el umbral de la funci&amp;oacute;n de decisi&amp;oacute;n. Si es &quot;autom&amp;aacute;tico&quot;, el umbral de la funci&amp;oacute;n de decisi&amp;oacute;n se determina como en el documento original.</target>
        </trans-unit>
        <trans-unit id="a618727acf5955128fa616ec196fb65bef331d06" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the scores of the samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="057670e523b801c82d76306d2019fbc0b1e55fa7" translate="yes" xml:space="preserve">
          <source>The amount of penalization chosen by cross validation</source>
          <target state="translated">La cantidad de penalización elegida por validación cruzada</target>
        </trans-unit>
        <trans-unit id="0a8889ae5955aac97641248ca40c9031408985ae" translate="yes" xml:space="preserve">
          <source>The amount of variance explained by each of the selected components.</source>
          <target state="translated">El monto de la variación explicada por cada uno de los componentes seleccionados.</target>
        </trans-unit>
        <trans-unit id="0c35119665a5a383bf2a2ea2491d6548297b77b9" translate="yes" xml:space="preserve">
          <source>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</source>
          <target state="translated">La puntuación de la anomalía de una muestra de entrada se calcula como la puntuación media de la anomalía de los árboles del bosque.</target>
        </trans-unit>
        <trans-unit id="f5bff141bb39bd3c0bc3c7fd23ee875e735e27a8" translate="yes" xml:space="preserve">
          <source>The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.</source>
          <target state="translated">La puntuación de la anomalía de cada muestra se llama Factor Atípico Local.Mide la desviación local de la densidad de una muestra dada con respecto a sus vecinos.Es local en el sentido de que la puntuación de la anomalía depende de cuán aislado esté el objeto con respecto al vecindario circundante.Más precisamente,la localidad viene dada por los vecinos más cercanos,cuya distancia se utiliza para estimar la densidad local.Comparando la densidad local de una muestra con las densidades locales de sus vecinos,se pueden identificar muestras que tienen una densidad sustancialmente menor que sus vecinos.Estos se consideran valores atípicos.</target>
        </trans-unit>
        <trans-unit id="9d2b05e02bf58b122225781451ec470ef3bbad43" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal.</source>
          <target state="translated">La puntuación de la anomalía de las muestras de entrada.Cuanto más bajo,más anormal.</target>
        </trans-unit>
        <trans-unit id="7ea9f6456fff678e2acc40131f68202326a3c878" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">La puntuación de la anomalía de las muestras de entrada.Cuanto más bajo,más anormal.Las puntuaciones negativas representan los valores atípicos,las positivas representan los valores atípicos.</target>
        </trans-unit>
        <trans-unit id="f8f5cba472ebac3a5205adfc98746f94cfb55c44" translate="yes" xml:space="preserve">
          <source>The approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; can be combined with the approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; to yield an approximate feature map for the exponentiated chi squared kernel. See the &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; for details and &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; for combination with the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">El mapa de caracter&amp;iacute;sticas aproximado proporcionado por &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt; se puede combinar con el mapa de caracter&amp;iacute;sticas aproximado proporcionado por &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; para producir un mapa de caracter&amp;iacute;sticas aproximado para el n&amp;uacute;cleo de chi cuadrado exponencial. Consulte &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; para obtener m&amp;aacute;s detalles y &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; para la combinaci&amp;oacute;n con el &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="98f8783bf76339c13180f5a0c21989e5d9843d3d" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the data by linear combinations.</source>
          <target state="translated">El número aproximado de vectores singulares necesarios para explicar la mayoría de los datos mediante combinaciones lineales.</target>
        </trans-unit>
        <trans-unit id="01540c9a0c0d75da953bc96aafe62b761a8754d9" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.</source>
          <target state="translated">El número aproximado de vectores singulares necesarios para explicar la mayoría de los datos de entrada mediante combinaciones lineales.El uso de este tipo de espectro singular en la entrada permite al generador reproducir las correlaciones que a menudo se observan en la práctica.</target>
        </trans-unit>
        <trans-unit id="9b1212b40ba5f9b8205d7b64e2e9c20d35b415b3" translate="yes" xml:space="preserve">
          <source>The arithmetic mean is the sum of the elements along the axis divided by the number of elements.</source>
          <target state="translated">La media aritmética es la suma de los elementos a lo largo del eje dividida por el número de elementos.</target>
        </trans-unit>
        <trans-unit id="35a07a5d9cec4bce6db2b0fe073857a7979c5b16" translate="yes" xml:space="preserve">
          <source>The array has 0.16% of non zero values.</source>
          <target state="translated">La matriz tiene un 0,16% de valores no nulos.</target>
        </trans-unit>
        <trans-unit id="a77140549775c60e078b0c26c1b965d02c08ebb3" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="449fd4c0217076d90d22633d6cb32b7300c30336" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations, shape = X.shape[:-1]</source>
          <target state="translated">El conjunto de evaluaciones de (log)-densidad,forma=X.forma[:-1]</target>
        </trans-unit>
        <trans-unit id="d00a652e1f004fd3ddfd653b6794ad2aba8108c7" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations.</source>
          <target state="translated">El conjunto de evaluaciones de logaritmos (densidad).</target>
        </trans-unit>
        <trans-unit id="0e2b34732ef7f3aed76135ded438ba1b0e2806d3" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations. These are normalized to be probability densities, so values will be low for high-dimensional data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="531360783ebbdd4b65c83458a2d186f114e4086b" translate="yes" xml:space="preserve">
          <source>The automatic estimation from Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604 by Thomas P. Minka is also compared.</source>
          <target state="translated">La estimación automática de la elección automática de la dimensionalidad para PCA.NIPS 2000:598-604 de Thomas P.Minka también se compara.</target>
        </trans-unit>
        <trans-unit id="37bf2d4736a8a0a6781e876ebdb656796aca8913" translate="yes" xml:space="preserve">
          <source>The available cross validation iterators are introduced in the following section.</source>
          <target state="translated">Los iteradores de validación cruzada disponibles se presentan en la siguiente sección.</target>
        </trans-unit>
        <trans-unit id="52d681893d146fe7dad6c4424a94a534bab69431" translate="yes" xml:space="preserve">
          <source>The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.</source>
          <target state="translated">La complejidad media viene dada por O(k n T),donde n es el número de muestras y T es el número de iteración.</target>
        </trans-unit>
        <trans-unit id="d9092bb0da47977d9b43f41f12fd7dcc75d26801" translate="yes" xml:space="preserve">
          <source>The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with &lt;code&gt;n_labels&lt;/code&gt; as its expected value, but samples are bounded (using rejection sampling) by &lt;code&gt;n_classes&lt;/code&gt;, and must be nonzero if &lt;code&gt;allow_unlabeled&lt;/code&gt; is False.</source>
          <target state="translated">El n&amp;uacute;mero medio de etiquetas por instancia. M&amp;aacute;s precisamente, el n&amp;uacute;mero de etiquetas por muestra se extrae de una distribuci&amp;oacute;n de Poisson con &lt;code&gt;n_labels&lt;/code&gt; como su valor esperado, pero las muestras est&amp;aacute;n delimitadas (mediante muestreo de rechazo) por &lt;code&gt;n_classes&lt;/code&gt; y deben ser distintas de cero si &lt;code&gt;allow_unlabeled&lt;/code&gt; es False.</target>
        </trans-unit>
        <trans-unit id="50521765649b7ccece3cb452f18896c14c55c5d8" translate="yes" xml:space="preserve">
          <source>The average precision score in multi-label settings</source>
          <target state="translated">El puntaje promedio de precisión en los ajustes de múltiples etiquetas</target>
        </trans-unit>
        <trans-unit id="66fdff3c7719e9049c28a125538dc0d9761fb599" translate="yes" xml:space="preserve">
          <source>The averaged NDCG scores for all samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f192c1d1db49dfae3574eb70a5374ae8608eb71" translate="yes" xml:space="preserve">
          <source>The averaged intercept term.</source>
          <target state="translated">El término de intercepción promedio.</target>
        </trans-unit>
        <trans-unit id="9595497d17dae79a02bc4117e4b89bb9f68b4736" translate="yes" xml:space="preserve">
          <source>The averaged intercept term. Only available if &lt;code&gt;average=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c831f95b759b1e5121cd7c90ec4906a408d65670" translate="yes" xml:space="preserve">
          <source>The averaged sample DCG scores.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f1e3570b35bda8ded1b9651eb310bcefe0589f6" translate="yes" xml:space="preserve">
          <source>The axes with which the grid has been created or None if the grid has been given.</source>
          <target state="translated">Los ejes con los que se ha creado la cuadrícula o ninguno si se ha dado la cuadrícula.</target>
        </trans-unit>
        <trans-unit id="da553a7d6d695e12dca5e56a77d2e07dce1c7a46" translate="yes" xml:space="preserve">
          <source>The axis along which &lt;code&gt;X&lt;/code&gt; will be subsampled. &lt;code&gt;axis=0&lt;/code&gt; will select rows while &lt;code&gt;axis=1&lt;/code&gt; will select columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2af1a0461163ef9a917dd17f9a6c7032c07c79bb" translate="yes" xml:space="preserve">
          <source>The axis along which to impute.</source>
          <target state="translated">El eje a lo largo del cual imputar.</target>
        </trans-unit>
        <trans-unit id="efae1008b127cbb418b23469c7f222e2a6660b67" translate="yes" xml:space="preserve">
          <source>The bag of words representation is quite simplistic but surprisingly useful in practice.</source>
          <target state="translated">La representación de la bolsa de palabras es bastante simplista pero sorprendentemente útil en la práctica.</target>
        </trans-unit>
        <trans-unit id="a41734ff5b2ce49bfc6f83ebb1bb324e76d37d70" translate="yes" xml:space="preserve">
          <source>The bags of words representation implies that &lt;code&gt;n_features&lt;/code&gt; is the number of distinct words in the corpus: this number is typically larger than 100,000.</source>
          <target state="translated">La representaci&amp;oacute;n de bolsas de palabras implica que &lt;code&gt;n_features&lt;/code&gt; es el n&amp;uacute;mero de palabras distintas en el corpus: este n&amp;uacute;mero suele ser superior a 100.000.</target>
        </trans-unit>
        <trans-unit id="eac13f6898f72e0eaad5c85bd1b456a5882d1a72" translate="yes" xml:space="preserve">
          <source>The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.</source>
          <target state="translated">La precisión equilibrada en los problemas de clasificación binaria y multiclase para hacer frente a los conjuntos de datos desequilibrados.Se define como el promedio de memoria obtenido en cada clase.</target>
        </trans-unit>
        <trans-unit id="fd188197709d9753bf6d2d7d8ebbb1b211377cd3" translate="yes" xml:space="preserve">
          <source>The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.</source>
          <target state="translated">El ancho de banda aquí actúa como un parámetro de suavizado,controlando el equilibrio entre el sesgo y la varianza en el resultado.Un gran ancho de banda lleva a una distribución de densidad muy suave (es decir,de alto sesgo).Un ancho de banda pequeño lleva a una distribución de densidad poco suave (es decir,de alta varianza).</target>
        </trans-unit>
        <trans-unit id="220a3cae014ca1f6f44493020405f4e460f4038d" translate="yes" xml:space="preserve">
          <source>The bandwidth of the kernel.</source>
          <target state="translated">El ancho de banda del núcleo.</target>
        </trans-unit>
        <trans-unit id="eaa1089b1fdc51cb43f528072a19cd3f3ec50c61" translate="yes" xml:space="preserve">
          <source>The bandwidth parameter.</source>
          <target state="translated">El parámetro del ancho de banda.</target>
        </trans-unit>
        <trans-unit id="ca9efc037882c6303cbf300aed185a1d9a7dd7ae" translate="yes" xml:space="preserve">
          <source>The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classifier.</source>
          <target state="translated">El diagrama de barras indica la precisión,el tiempo de entrenamiento (normalizado)y el tiempo de prueba (normalizado)de cada clasificador.</target>
        </trans-unit>
        <trans-unit id="6ef0fbe42e5306306c086ebee574632386d4e293" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center. This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">El clasificador base es un clasificador forestal aleatorio con 25 estimadores base (&amp;aacute;rboles). Si este clasificador se entrena en los 800 puntos de datos de entrenamiento, tiene demasiada confianza en sus predicciones y, por lo tanto, incurre en una gran p&amp;eacute;rdida de registros. Calibrar un clasificador id&amp;eacute;ntico, que fue entrenado en 600 puntos de datos, con m&amp;eacute;todo = 'sigmoide' en los 200 puntos de datos restantes, reduce la confianza de las predicciones, es decir, mueve los vectores de probabilidad desde los bordes del simplex hacia el centro. Esta calibraci&amp;oacute;n da como resultado una menor p&amp;eacute;rdida de registros. Tenga en cuenta que una alternativa habr&amp;iacute;a sido aumentar el n&amp;uacute;mero de estimadores base, lo que habr&amp;iacute;a resultado en una disminuci&amp;oacute;n similar en la p&amp;eacute;rdida logar&amp;iacute;tmica.</target>
        </trans-unit>
        <trans-unit id="3f89ef1e6cb2a54f74eb0a982beea8ffd3333592" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center:</source>
          <target state="translated">El clasificador base es un clasificador forestal aleatorio con 25 estimadores base (&amp;aacute;rboles). Si este clasificador se entrena en los 800 puntos de datos de entrenamiento, tiene demasiada confianza en sus predicciones y, por lo tanto, incurre en una gran p&amp;eacute;rdida de registros. La calibraci&amp;oacute;n de un clasificador id&amp;eacute;ntico, que se entren&amp;oacute; en 600 puntos de datos, con m&amp;eacute;todo = 'sigmoide' en los 200 puntos de datos restantes, reduce la confianza de las predicciones, es decir, mueve los vectores de probabilidad desde los bordes del simplex hacia el centro:</target>
        </trans-unit>
        <trans-unit id="2000944fc04d6f6e7393d659c7173e725c27d458" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdab13e5017fd96224833fdcfcd75615e152e1af" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</source>
          <target state="translated">El estimador base a partir del cual se construye el conjunto potenciado. Se requiere soporte para la ponderaci&amp;oacute;n de la muestra, as&amp;iacute; como los atributos &lt;code&gt;classes_&lt;/code&gt; y &lt;code&gt;n_classes_&lt;/code&gt; adecuados . Si es &lt;code&gt;None&lt;/code&gt; , entonces el estimador base es &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7ad7363185e9af79d948e6b752e7cc25ad567f4b" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39ba9d176a83a208ed96d60f33bec321ac051ac5" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</source>
          <target state="translated">El estimador base a partir del cual se construye el conjunto potenciado. Se requiere soporte para la ponderaci&amp;oacute;n de la muestra. Si es &lt;code&gt;None&lt;/code&gt; , entonces el estimador base es &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="09e53f3d87743d060ad05fb04ae38587d00307b0" translate="yes" xml:space="preserve">
          <source>The base estimator from which the classifier chain is built.</source>
          <target state="translated">El estimador de base a partir del cual se construye la cadena clasificadora.</target>
        </trans-unit>
        <trans-unit id="c229b061f41bb16a20d56916c466508eaa778c17" translate="yes" xml:space="preserve">
          <source>The base estimator from which the ensemble is grown.</source>
          <target state="translated">El estimador base a partir del cual el conjunto crece.</target>
        </trans-unit>
        <trans-unit id="5066de8ea8cb5c40493cb3ffbb0e912b0ff83ff9" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This can be both a fitted (if &lt;code&gt;prefit&lt;/code&gt; is set to True) or a non-fitted estimator. The estimator must have either a &lt;code&gt;feature_importances_&lt;/code&gt; or &lt;code&gt;coef_&lt;/code&gt; attribute after fitting.</source>
          <target state="translated">El estimador base a partir del cual se construye el transformador. Puede ser tanto un &lt;code&gt;prefit&lt;/code&gt; ajustado (si prefit se establece en True) como un estimador no ajustado. El estimador debe tener un atributo &lt;code&gt;feature_importances_&lt;/code&gt; o &lt;code&gt;coef_&lt;/code&gt; despu&amp;eacute;s del ajuste.</target>
        </trans-unit>
        <trans-unit id="8935e197daa0e6c85ff9f9b0e235723b5ff13893" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the &lt;code&gt;SelectFromModel&lt;/code&gt;, i.e when prefit is False.</source>
          <target state="translated">El estimador base a partir del cual se construye el transformador. Esto se almacena solo cuando se pasa un estimador no ajustado a &lt;code&gt;SelectFromModel&lt;/code&gt; , es decir, cuando prefit es False.</target>
        </trans-unit>
        <trans-unit id="8062156aa44a8a655e18edffbba0a99e0e098c8a" translate="yes" xml:space="preserve">
          <source>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.</source>
          <target state="translated">El estimador de la base para encajar en subconjuntos aleatorios del conjunto de datos.Si no hay ninguno,entonces el estimador de base es un árbol de decisión.</target>
        </trans-unit>
        <trans-unit id="d80c39c018abc387662fda22711c8b7bb4707717" translate="yes" xml:space="preserve">
          <source>The base kernel</source>
          <target state="translated">El núcleo base</target>
        </trans-unit>
        <trans-unit id="1b7bc12e1d1eb0bb8e39e15748d7f9b27d93ef1a" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns uniform weights to each neighbor. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.</source>
          <target state="translated">La clasificaci&amp;oacute;n b&amp;aacute;sica de vecinos m&amp;aacute;s cercanos utiliza ponderaciones uniformes: es decir, el valor asignado a un punto de consulta se calcula a partir de un voto de mayor&amp;iacute;a simple de los vecinos m&amp;aacute;s cercanos. En algunas circunstancias, es mejor ponderar a los vecinos de modo que los vecinos m&amp;aacute;s cercanos contribuyan m&amp;aacute;s al ajuste. Esto se puede lograr mediante la palabra clave &lt;code&gt;weights&lt;/code&gt; . El valor predeterminado, &lt;code&gt;weights = 'uniform'&lt;/code&gt; , asigna pesos uniformes a cada vecino. &lt;code&gt;weights = 'distance'&lt;/code&gt; asigna pesos proporcionales a la inversa de la distancia desde el punto de consulta. Alternativamente, se puede proporcionar una funci&amp;oacute;n definida por el usuario de la distancia para calcular los pesos.</target>
        </trans-unit>
        <trans-unit id="650ceb2a1485e5f890a3dcff86c2748037640c3b" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns equal weights to all points. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.</source>
          <target state="translated">La regresi&amp;oacute;n b&amp;aacute;sica de vecinos m&amp;aacute;s cercanos utiliza ponderaciones uniformes: es decir, cada punto en la vecindad local contribuye de manera uniforme a la clasificaci&amp;oacute;n de un punto de consulta. En algunas circunstancias, puede ser ventajoso ponderar puntos de manera que los puntos cercanos contribuyan m&amp;aacute;s a la regresi&amp;oacute;n que los puntos lejanos. Esto se puede lograr mediante la palabra clave &lt;code&gt;weights&lt;/code&gt; . El valor predeterminado, &lt;code&gt;weights = 'uniform'&lt;/code&gt; , asigna pesos iguales a todos los puntos. &lt;code&gt;weights = 'distance'&lt;/code&gt; asigna pesos proporcionales a la inversa de la distancia desde el punto de consulta. Alternativamente, se puede proporcionar una funci&amp;oacute;n de distancia definida por el usuario, que se utilizar&amp;aacute; para calcular los pesos.</target>
        </trans-unit>
        <trans-unit id="1cb5ab68855135ab83ff44b066a7bba092358a07" translate="yes" xml:space="preserve">
          <source>The behavior of &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; is summarized in the following table.</source>
          <target state="translated">El comportamiento de &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; se resume en la siguiente tabla.</target>
        </trans-unit>
        <trans-unit id="9c1d95adae9acea07102a8bfe1db513459ebeaee" translate="yes" xml:space="preserve">
          <source>The behavior of the model is very sensitive to the &lt;code&gt;gamma&lt;/code&gt; parameter. If &lt;code&gt;gamma&lt;/code&gt; is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with &lt;code&gt;C&lt;/code&gt; will be able to prevent overfitting.</source>
          <target state="translated">El comportamiento del modelo es muy sensible al par&amp;aacute;metro &lt;code&gt;gamma&lt;/code&gt; . Si la &lt;code&gt;gamma&lt;/code&gt; es demasiado grande, el radio del &amp;aacute;rea de influencia de los vectores de soporte solo incluye el vector de soporte en s&amp;iacute; y ninguna regularizaci&amp;oacute;n con &lt;code&gt;C&lt;/code&gt; podr&amp;aacute; evitar el sobreajuste.</target>
        </trans-unit>
        <trans-unit id="2ab99fa4c73a04aa03f4ac31b15851c1ddbe51a1" translate="yes" xml:space="preserve">
          <source>The below plot uses the first two features. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;here&lt;/a&gt; for more information on this dataset.</source>
          <target state="translated">La siguiente gr&amp;aacute;fica utiliza las dos primeras caracter&amp;iacute;sticas. Consulte &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;aqu&amp;iacute;&lt;/a&gt; para obtener m&amp;aacute;s informaci&amp;oacute;n sobre este conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="4992d67c6e1aff2322ba4c5bb3c9e0c82a7a36cf" translate="yes" xml:space="preserve">
          <source>The best model is selected by cross-validation.</source>
          <target state="translated">El mejor modelo se selecciona por validación cruzada.</target>
        </trans-unit>
        <trans-unit id="1a3f42d6835c4f6686d54e99d9f9cd0e3a597fec" translate="yes" xml:space="preserve">
          <source>The best performance is 1 with &lt;code&gt;normalize == True&lt;/code&gt; and the number of samples with &lt;code&gt;normalize == False&lt;/code&gt;.</source>
          <target state="translated">El mejor rendimiento es 1 con &lt;code&gt;normalize == True&lt;/code&gt; y el n&amp;uacute;mero de muestras con &lt;code&gt;normalize == False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4b2ab3d16cd4ed6ed82ae137ea7b0ef6d79ff832" translate="yes" xml:space="preserve">
          <source>The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.</source>
          <target state="translated">El mejor valor p posible es 1/(n_permutaciones+1),el peor es 1.0.</target>
        </trans-unit>
        <trans-unit id="ed9ac623a873633695e926c14de64904b54167f4" translate="yes" xml:space="preserve">
          <source>The best possible score is 1.0, lower values are worse.</source>
          <target state="translated">La mejor puntuación posible es 1,0,los valores más bajos son peores.</target>
        </trans-unit>
        <trans-unit id="9230aaef9f0a1fa5e57a82e7f44b36cb5d04d36e" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.</source>
          <target state="translated">El mejor valor es 1 y el peor valor es -1.Los valores cercanos a 0 indican cúmulos superpuestos.</target>
        </trans-unit>
        <trans-unit id="6e2eac60da3011cdc30f86be85be8ffa7e5f1da5" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.</source>
          <target state="translated">El mejor valor es 1 y el peor valor es -1.Los valores cercanos a 0 indican cúmulos superpuestos.Los valores negativos generalmente indican que una muestra ha sido asignada al cúmulo equivocado,ya que un cúmulo diferente es más similar.</target>
        </trans-unit>
        <trans-unit id="b4a689b86b5cda51ef6628ef622a0daf756424bd" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0 when &lt;code&gt;adjusted=False&lt;/code&gt;.</source>
          <target state="translated">El mejor valor es 1 y el peor valor es 0 cuando se &lt;code&gt;adjusted=False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="488558d8da77f986fd351608404cb6a07bd4fe58" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0.</source>
          <target state="translated">El mejor valor es 1 y el peor valor es 0.</target>
        </trans-unit>
        <trans-unit id="b82813aa45a878f8df07ac95972e2e5116e63bed" translate="yes" xml:space="preserve">
          <source>The bias term in the underlying linear model.</source>
          <target state="translated">El término de sesgo en el modelo lineal subyacente.</target>
        </trans-unit>
        <trans-unit id="d5675ace96b0ccc75bff7a0b67255343283bf5b9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each column.</source>
          <target state="translated">La etiqueta del bicluster de cada columna.</target>
        </trans-unit>
        <trans-unit id="d135838b65ddeda2dac8521941211167551d33d9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each row.</source>
          <target state="translated">La etiqueta del bicluster de cada fila.</target>
        </trans-unit>
        <trans-unit id="b216c2ddafb03b97c09e36e1d0b96ac53ff1f7ba" translate="yes" xml:space="preserve">
          <source>The bins have identical widths.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0eb1458ca2a62b6254994f184c01ee65afc52b4" translate="yes" xml:space="preserve">
          <source>The bins have the same number of samples and depend on &lt;code&gt;y_prob&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f153639172a63c8dba2ed27e16715aaa100936d" translate="yes" xml:space="preserve">
          <source>The bipartite structure allows for the use of efficient block Gibbs sampling for inference.</source>
          <target state="translated">La estructura bipartita permite el uso de un eficiente muestreo de Gibbs en bloque para la inferencia.</target>
        </trans-unit>
        <trans-unit id="a902e8cbc26950a56283f916fe814a1e5f8ed790" translate="yes" xml:space="preserve">
          <source>The bottleneck of a gradient boosting procedure is building the decision trees. Building a traditional decision tree (as in the other GBDTs &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;) requires sorting the samples at each node (for each feature). Sorting is needed so that the potential gain of a split point can be computed efficiently. Splitting a single node has thus a complexity of \(\mathcal{O}(n_\text{features} \times n \log(n))\) where \(n\) is the number of samples at the node.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b104bf424ac624d7987a6846ffafb73fcb6a3323" translate="yes" xml:space="preserve">
          <source>The bounding box for each cluster center when centers are generated at random.</source>
          <target state="translated">El cuadro delimitador de cada centro de cúmulo cuando los centros se generan al azar.</target>
        </trans-unit>
        <trans-unit id="e73634a1f979122b85e5e2e94c3d89b631e61d63" translate="yes" xml:space="preserve">
          <source>The boxes represent repeated sampling.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de87510aaa8a4634fe7b670444f1141d0b934e04" translate="yes" xml:space="preserve">
          <source>The breast cancer dataset is a classic and very easy binary classification dataset.</source>
          <target state="translated">El conjunto de datos del cáncer de mama es un clásico y muy fácil conjunto de datos de clasificación binaria.</target>
        </trans-unit>
        <trans-unit id="2045b20286b76d60e5fa1f74a1df8fc02a45c66b" translate="yes" xml:space="preserve">
          <source>The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the &amp;ldquo;calibration&amp;rdquo; of a set of probabilistic predictions.</source>
          <target state="translated">La p&amp;eacute;rdida de puntaje brier tambi&amp;eacute;n est&amp;aacute; entre 0 y 1 y cuanto menor es el puntaje (la diferencia de cuadr&amp;aacute;tico medio es menor), m&amp;aacute;s precisa es la predicci&amp;oacute;n. Puede considerarse como una medida de la &quot;calibraci&amp;oacute;n&quot; de un conjunto de predicciones probabil&amp;iacute;sticas.</target>
        </trans-unit>
        <trans-unit id="b76bd58217534fc8775d360bccb78e720219d44b" translate="yes" xml:space="preserve">
          <source>The calibration is based on the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; method of the &lt;code&gt;base_estimator&lt;/code&gt; if it exists, else on &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6452eadf0058adcc76ac85d9ade38659e522fb0c" translate="yes" xml:space="preserve">
          <source>The calibration of the model can be assessed by plotting the mean observed value vs the mean predicted value on groups of test samples binned by predicted risk.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74481417cca6e6adb9f25dfbb8e31b8fc201eb01" translate="yes" xml:space="preserve">
          <source>The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function.</source>
          <target state="translated">El llamado a usar para la transformación inversa.Se le pasarán los mismos argumentos que a la transformación inversa,con los arcos y los kwargs reenviados.Si inverse_func es None,entonces inverse_func será la función de identidad.</target>
        </trans-unit>
        <trans-unit id="81ea3433c521d90153f147dfae64d0df290c9757" translate="yes" xml:space="preserve">
          <source>The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function.</source>
          <target state="translated">El llamado a usar para la transformación.Se le pasarán los mismos argumentos que a la transformación,con los args y kwargs reenviados.Si func es None,entonces func será la función de identidad.</target>
        </trans-unit>
        <trans-unit id="a1419fd6aa69dd9ba89b3d48a7257eafd52e186a" translate="yes" xml:space="preserve">
          <source>The categorical Naive Bayes classifier is suitable for classification with discrete features that are categorically distributed. The categories of each feature are drawn from a categorical distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f38539ea641dd8b50f2f7d85a7f466ba670ba50" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;).</source>
          <target state="translated">Las categor&amp;iacute;as de cada caracter&amp;iacute;stica determinadas durante el ajuste (en orden de las caracter&amp;iacute;sticas en X y correspondientes a la salida de la &lt;code&gt;transform&lt;/code&gt; aci&amp;oacute;n ).</target>
        </trans-unit>
        <trans-unit id="12b8e6304a220ce13ab40041e41390ae67d86b4e" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;). This includes the category specified in &lt;code&gt;drop&lt;/code&gt; (if any).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11dc40b09acf079281dfd9a81c3951e435409b7d" translate="yes" xml:space="preserve">
          <source>The centers of each cluster. Only returned if &lt;code&gt;return_centers=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d4177cfd25fe2d42f6a09b10ba8f700ec03cc45" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is given by</source>
          <target state="translated">El núcleo del chi cuadrado viene dado por</target>
        </trans-unit>
        <trans-unit id="e654b311ef425d553f0f6705f2dd43fc95968d41" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is most commonly used on histograms (bags) of visual words.</source>
          <target state="translated">El núcleo de chi cuadrado se usa más comúnmente en los histogramas (bolsas)de palabras visuales.</target>
        </trans-unit>
        <trans-unit id="085e153138bf3768d8fc53856c1f1238c6d3caff" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt;&lt;code&gt;chi2_kernel&lt;/code&gt;&lt;/a&gt; and then passed to an &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt;:</source>
          <target state="translated">El kernel chi-cuadrado es una opci&amp;oacute;n muy popular para entrenar SVM no lineales en aplicaciones de visi&amp;oacute;n por computadora. Se puede calcular usando &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt; &lt;code&gt;chi2_kernel&lt;/code&gt; &lt;/a&gt; y luego pasar a &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; &lt;/a&gt; con &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="9233e345f2318b5fc92b12dd745f5adf2df3c33c" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative. This kernel is most commonly applied to histograms.</source>
          <target state="translated">El núcleo de chi cuadrado se calcula entre cada par de filas en X e Y.X e Y tienen que ser no negativos.Este núcleo se aplica más comúnmente a los histogramas.</target>
        </trans-unit>
        <trans-unit id="a8ba729b106653acd0bc97ad998e8dac7b2b635e" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is given by:</source>
          <target state="translated">El núcleo chi-cuadrado está dado por:</target>
        </trans-unit>
        <trans-unit id="959a186ffdea8e09c8857bbd475b2cf453d363af" translate="yes" xml:space="preserve">
          <source>The child estimator template used to create the collection of fitted sub-estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ac1548c51d0c4529f9b6250f5d16db6b392dc40" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_features&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_features&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_features]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_features + i&lt;/code&gt;</source>
          <target state="translated">Los hijos de cada nodo no hoja. Los valores inferiores a &lt;code&gt;n_features&lt;/code&gt; corresponden a las hojas del &amp;aacute;rbol que son las muestras originales. Un nodo &lt;code&gt;i&lt;/code&gt; mayor o igual que &lt;code&gt;n_features&lt;/code&gt; es un nodo no hoja y tiene hijos &lt;code&gt;children_[i - n_features]&lt;/code&gt; . Alternativamente, en la i-&amp;eacute;sima iteraci&amp;oacute;n, los ni&amp;ntilde;os [i] [0] y los ni&amp;ntilde;os [i] [1] se fusionan para formar el nodo &lt;code&gt;n_features + i&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c8989abb78a441b49758b99ec48d8b090a3ce248" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_samples&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_samples&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_samples]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_samples + i&lt;/code&gt;</source>
          <target state="translated">Los hijos de cada nodo no hoja. Los valores inferiores a &lt;code&gt;n_samples&lt;/code&gt; corresponden a las hojas del &amp;aacute;rbol que son las muestras originales. Un nodo &lt;code&gt;i&lt;/code&gt; mayor o igual que &lt;code&gt;n_samples&lt;/code&gt; es un nodo no hoja y tiene hijos &lt;code&gt;children_[i - n_samples]&lt;/code&gt; . Alternativamente, en la i-&amp;eacute;sima iteraci&amp;oacute;n, los ni&amp;ntilde;os [i] [0] y los ni&amp;ntilde;os [i] [1] se fusionan para formar el nodo &lt;code&gt;n_samples + i&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f3473fe6f2b500963d93290672a7a3ef3f212da2" translate="yes" xml:space="preserve">
          <source>The choice of features is not particularly helpful, but serves to illustrate the technique.</source>
          <target state="translated">La elección de las características no es particularmente útil,pero sirve para ilustrar la técnica.</target>
        </trans-unit>
        <trans-unit id="15475f4de31a0dbbaa5b0396fc18010629c57dd3" translate="yes" xml:space="preserve">
          <source>The choice of the distribution depends on the problem at hand:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f460fe2e0e34556a3c6a8a6d017a5f63eeb9ea9" translate="yes" xml:space="preserve">
          <source>The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">La descomposici&amp;oacute;n cholesky de las matrices de precisi&amp;oacute;n de cada componente de la mezcla. Una matriz de precisi&amp;oacute;n es la inversa de una matriz de covarianza. Una matriz de covarianza es sim&amp;eacute;trica positiva definida, por lo que la mezcla de gaussiano puede parametrizarse de manera equivalente mediante las matrices de precisi&amp;oacute;n. El almacenamiento de las matrices de precisi&amp;oacute;n en lugar de las matrices de covarianza hace que sea m&amp;aacute;s eficiente calcular la probabilidad logar&amp;iacute;tmica de nuevas muestras en el momento de la prueba. La forma depende de &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="8e7b3855f3de3292a39f43c9800aebcf16bc77cb" translate="yes" xml:space="preserve">
          <source>The claim &lt;strong&gt;frequency&lt;/strong&gt; is the number of claims divided by the exposure, typically measured in number of claims per year.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54b709e28c0bc302f278f75cf2f9281781fc40ea" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; used with the optional parameter &lt;code&gt;svd_solver='randomized'&lt;/code&gt; is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; usada con el par&amp;aacute;metro opcional &lt;code&gt;svd_solver='randomized'&lt;/code&gt; es muy &amp;uacute;til en ese caso: dado que vamos a eliminar la mayor&amp;iacute;a de los vectores singulares, es mucho m&amp;aacute;s eficiente limitar el c&amp;aacute;lculo a una estimaci&amp;oacute;n aproximada de los vectores singulares que vamos a hacer. siga realizando la transformaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="9afc238719eb2575519db8e476eb36c39ec071a2" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt;&lt;code&gt;DictVectorizer&lt;/code&gt;&lt;/a&gt; can be used to convert feature arrays represented as lists of standard Python &lt;code&gt;dict&lt;/code&gt; objects to the NumPy/SciPy representation used by scikit-learn estimators.</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt; &lt;code&gt;DictVectorizer&lt;/code&gt; &lt;/a&gt; se puede utilizar para convertir matrices de caracter&amp;iacute;sticas representadas como listas de objetos &lt;code&gt;dict&lt;/code&gt; est&amp;aacute;ndar de Python a la representaci&amp;oacute;n NumPy / SciPy utilizada por los estimadores scikit-learn.</target>
        </trans-unit>
        <trans-unit id="0b3d7d58279a2952a288a94db8a135a534d78ec7" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; is a high-speed, low-memory vectorizer that uses a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;feature hashing&lt;/a&gt;, or the &amp;ldquo;hashing trick&amp;rdquo;. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no &lt;code&gt;inverse_transform&lt;/code&gt; method.</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt; es un vectorizador de alta velocidad y poca memoria que utiliza una t&amp;eacute;cnica conocida como &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;funci&amp;oacute;n hash&lt;/a&gt; , o el &quot;truco hash&quot;. En lugar de construir una tabla hash de las caracter&amp;iacute;sticas encontradas en el entrenamiento, como hacen los vectorizadores, las instancias de &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt; aplican una funci&amp;oacute;n hash a las caracter&amp;iacute;sticas para determinar su &amp;iacute;ndice de columna en matrices de muestra directamente. El resultado es una mayor velocidad y un uso reducido de la memoria, a expensas de la capacidad de inspecci&amp;oacute;n; el hash no recuerda c&amp;oacute;mo se ve&amp;iacute;an las caracter&amp;iacute;sticas de entrada y no tiene un m&amp;eacute;todo &lt;code&gt;inverse_transform&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="46148e4d160f9510def06597f1921af425f74fcb" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing function to data. It solves the following problem:</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; &lt;/a&gt; ajusta una funci&amp;oacute;n no decreciente a los datos. Resuelve el siguiente problema:</target>
        </trans-unit>
        <trans-unit id="0773ec8e22bf216d4df607f3329b18b1168f7c09" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing real function to 1-dimensional data. It solves the following problem:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04846022f8485d75461ddcb301a4717897728672" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; implements this component wise deterministic sampling. Each component is sampled \(n\) times, yielding \(2n+1\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \(n\) is usually chosen to be 1 or 2, transforming the dataset to size &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (in the case of \(n=2\)).</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt; implementa este muestreo determinista sabio de componentes. Cada componente se muestrea \ (n \) veces, produciendo \ (2n + 1 \) dimensiones por dimensi&amp;oacute;n de entrada (el m&amp;uacute;ltiplo de dos proviene de la parte real y compleja de la transformada de Fourier). En la literatura, \ (n \) generalmente se elige como 1 o 2, transformando el conjunto de datos al tama&amp;ntilde;o de &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (en el caso de \ (n = 2 \)).</target>
        </trans-unit>
        <trans-unit id="9fdc0ed638cb0889fa5320ee5ada72b80a16402f" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt;&lt;code&gt;ElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt; &lt;code&gt;ElasticNetCV&lt;/code&gt; &lt;/a&gt; se puede utilizar para establecer los par&amp;aacute;metros &lt;code&gt;alpha&lt;/code&gt; (\ (\ alpha \)) y &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ rho \)) mediante validaci&amp;oacute;n cruzada.</target>
        </trans-unit>
        <trans-unit id="c89182d0633b09742e745213024a53ccec4fee0a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt;&lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt; &lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt; &lt;/a&gt; se puede utilizar para configurar los par&amp;aacute;metros &lt;code&gt;alpha&lt;/code&gt; (\ (\ alpha \)) y &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ rho \)) mediante validaci&amp;oacute;n cruzada.</target>
        </trans-unit>
        <trans-unit id="6c66ee2c7a53fbda2b2b2da602d1e782a1fcc1b3" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; implementa una rutina de aprendizaje SGD de primer orden. El algoritmo itera sobre los ejemplos de entrenamiento y para cada ejemplo actualiza los par&amp;aacute;metros del modelo de acuerdo con la regla de actualizaci&amp;oacute;n dada por</target>
        </trans-unit>
        <trans-unit id="f128e57d09cf8b511f002f70c40429b250044323" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; implementa una rutina de aprendizaje de descenso de gradiente estoc&amp;aacute;stico simple que admite diferentes funciones de p&amp;eacute;rdida y penalizaciones para la clasificaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="3fc72b8fffc5a512701e2a944bd472c169bc1771" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; trained with the hinge loss, equivalent to a linear SVM.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9be5b4c1c13de0290ee1bddf8fed519138b844e9" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; is well suited for regression problems with a large number of training samples (&amp;gt; 10.000), for other problems we recommend &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; implementa una rutina de aprendizaje de descenso de gradiente estoc&amp;aacute;stico simple que admite diferentes funciones de p&amp;eacute;rdida y penalizaciones para adaptarse a modelos de regresi&amp;oacute;n lineal. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; es adecuado para problemas de regresi&amp;oacute;n con una gran cantidad de muestras de entrenamiento (&amp;gt; 10.000), para otros problemas recomendamos &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; o &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt; &lt;code&gt;ElasticNet&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5ca16e4c08c0574f508b6c526beeb2111eade54a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;OneClassSVM&lt;/code&gt;&lt;/a&gt; implements a One-Class SVM which is used in outlier detection.</source>
          <target state="translated">La clase &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;OneClassSVM&lt;/code&gt; &lt;/a&gt; implementa un SVM de una clase que se utiliza en la detecci&amp;oacute;n de valores at&amp;iacute;picos.</target>
        </trans-unit>
        <trans-unit id="5eaffd69df806c5b8353c88358cfd0a49b275168" translate="yes" xml:space="preserve">
          <source>The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in &lt;code&gt;gbrt.classes_&lt;/code&gt;.</source>
          <target state="translated">La etiqueta de clase para la que se deben calcular los PDP. Solo si gbrt es un modelo de varias clases. Debe estar en &lt;code&gt;gbrt.classes_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2a31ab08188f24bd31e99f218cab5467eddf2ef0" translate="yes" xml:space="preserve">
          <source>The class labels.</source>
          <target state="translated">Las etiquetas de la clase.</target>
        </trans-unit>
        <trans-unit id="7c57202048886b9c3874e17b392c104a7a21da98" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="075fd1f474c95d9f3895b75ead4b1f9c99be54a8" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">Las probabilidades logar&amp;iacute;tmicas de la clase de las muestras de entrada. El orden de las clases corresponde al del atributo &lt;code&gt;classes_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b5b69c523a5547b26d366d35f11f7dab77d71024" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. The smaller \(\nu\), the less smooth the approximated function is. As \(\nu\rightarrow\infty\), the kernel becomes equivalent to the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel. When \(\nu = 1/2\), the Mat&amp;eacute;rn kernel becomes identical to the absolute exponential kernel. Important intermediate values are \(\nu=1.5\) (once differentiable functions) and \(\nu=2.5\) (twice differentiable functions).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9e49144e04860d9a1f4a8512901a503e5d41c6f" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the RBF and the absolute exponential kernel parameterized by an additional parameter nu. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).</source>
          <target state="translated">La clase de núcleos Matern es una generalización del RBF y del núcleo exponencial absoluto parametrizado por un parámetro adicional nu.Cuanto más pequeño sea nu,menos suave es la función aproximada.Para nu=inf,el núcleo se convierte en equivalente al núcleo RBF y para nu=0.5 al núcleo exponencial absoluto.Los valores intermedios importantes son nu=1,5 (funciones una vez diferenciables)y nu=2,5 (funciones dos veces diferenciables).</target>
        </trans-unit>
        <trans-unit id="e48205f573bda857c27ec3937eb80960daed3556" translate="yes" xml:space="preserve">
          <source>The class ordering is preserved:</source>
          <target state="translated">El orden de la clase se mantiene:</target>
        </trans-unit>
        <trans-unit id="0401a90c4f50bc0cacd4ce11e3ddd76a2d543405" translate="yes" xml:space="preserve">
          <source>The class prior probabilities. By default, the class proportions are inferred from the training data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9e493bffc66549584cbf656024fe268e745fcd4" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a7a001ab832443f9ea75be4af1e68e4b9eb028c" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81478ef84d2d645ae9c4e632247f7f3bc9052a92" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute.</source>
          <target state="translated">Las probabilidades de clase de las muestras de entrada. El orden de las salidas es el mismo que el del atributo &lt;code&gt;classes_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="eb02560f00e2596a11150c32ba96675012f41151" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ea2403e3ddde25cdaa7213df112f25928f5232d" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">Las probabilidades de clase de las muestras de entrada. El orden de las clases corresponde al del atributo &lt;code&gt;classes_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="64b22dfb54911895a611d3b9cfbaf3a3f14d97f5" translate="yes" xml:space="preserve">
          <source>The class to report if &lt;code&gt;average='binary'&lt;/code&gt; and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting &lt;code&gt;labels=[pos_label]&lt;/code&gt; and &lt;code&gt;average != 'binary'&lt;/code&gt; will report scores for that label only.</source>
          <target state="translated">La clase para informar si &lt;code&gt;average='binary'&lt;/code&gt; y los datos son binarios. Si los datos son multiclase o multilabel, esto se ignorar&amp;aacute;; el establecimiento de &lt;code&gt;labels=[pos_label]&lt;/code&gt; y &lt;code&gt;average != 'binary'&lt;/code&gt; informar&amp;aacute; las puntuaciones de esa etiqueta &amp;uacute;nicamente.</target>
        </trans-unit>
        <trans-unit id="b02b6e7e5cca0ff24c2691f6e7d2bc9c247d17d1" translate="yes" xml:space="preserve">
          <source>The class to use to build the returned adjacency matrix.</source>
          <target state="translated">La clase que se usará para construir la matriz de adyacencia devuelta.</target>
        </trans-unit>
        <trans-unit id="08bf02b9f7a917eed20c9c94c384791c0ed4578f" translate="yes" xml:space="preserve">
          <source>The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.</source>
          <target state="translated">La clase con respecto a la cual realizamos un ajuste de uno contra todos.Si no hay ninguno,entonces se asume que el problema dado es binario.</target>
        </trans-unit>
        <trans-unit id="26dc8660d1112d77620f09027ce7b9fbee6027a3" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; fits a logistic regression model, while with &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; it fits a linear support vector machine (SVM).</source>
          <target state="translated">Las clases &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; proporcionan funcionalidad para ajustar modelos lineales para clasificaci&amp;oacute;n y regresi&amp;oacute;n utilizando diferentes funciones de p&amp;eacute;rdida (convexas) y diferentes penalizaciones. Por ejemplo, con &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; , &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; se&lt;/a&gt; ajusta a un modelo de regresi&amp;oacute;n log&amp;iacute;stica, mientras que con &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; se ajusta a una m&amp;aacute;quina de vectores de soporte lineal (SVM).</target>
        </trans-unit>
        <trans-unit id="d29e2c71b01d97c9c93dc5aad937ec38e91b7ca9" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide two criteria to stop the algorithm when a given level of convergence is reached:</source>
          <target state="translated">Las clases &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; proporcionan dos criterios para detener el algoritmo cuando se alcanza un nivel determinado de convergencia:</target>
        </trans-unit>
        <trans-unit id="d7ef8ada32aed500913b0cf053368e18ea306f33" translate="yes" xml:space="preserve">
          <source>The classes in &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can handle either NumPy arrays or &lt;code&gt;scipy.sparse&lt;/code&gt; matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.</source>
          <target state="translated">Las clases en &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt; pueden manejar matrices NumPy o matrices &lt;code&gt;scipy.sparse&lt;/code&gt; como entrada. Para matrices densas, se admiten una gran cantidad de m&amp;eacute;tricas de distancia posibles. Para matrices dispersas, se admiten m&amp;eacute;tricas arbitrarias de Minkowski para las b&amp;uacute;squedas.</target>
        </trans-unit>
        <trans-unit id="f39d71815c55889dd46cb64855cc1d40f837d2d9" translate="yes" xml:space="preserve">
          <source>The classes in the &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators&amp;rsquo; accuracy scores or to boost their performance on very high-dimensional datasets.</source>
          <target state="translated">Las clases del m&amp;oacute;dulo &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; &lt;/a&gt; se pueden utilizar para la selecci&amp;oacute;n de caracter&amp;iacute;sticas / reducci&amp;oacute;n de dimensionalidad en conjuntos de muestras, ya sea para mejorar las puntuaciones de precisi&amp;oacute;n de los estimadores o para aumentar su rendimiento en conjuntos de datos de muy alta dimensi&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="df672af473b055efb339763c1ea56e27edeec8c9" translate="yes" xml:space="preserve">
          <source>The classes in this submodule allow to approximate the embedding \(\phi\), thereby working explicitly with the representations \(\phi(x_i)\), which obviates the need to apply the kernel or store training examples.</source>
          <target state="translated">Las clases de este submódulo permiten aproximarse a la incrustación \ ~ (\ ~-Phi),trabajando así explícitamente con las representaciones \ ~ (\ ~-Phi (x_i)),lo que evita la necesidad de aplicar el núcleo o almacenar los ejemplos de formación.</target>
        </trans-unit>
        <trans-unit id="41647bba96e968e0bd58965ae65f2bc0365d42c6" translate="yes" xml:space="preserve">
          <source>The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</source>
          <target state="translated">Las etiquetas de clase (problema de salida única),o una lista de matrices de etiquetas de clase (problema de salida múltiple).</target>
        </trans-unit>
        <trans-unit id="772daeb9b0eaa408b264b0fa548b1cbd24779805" translate="yes" xml:space="preserve">
          <source>The classes labels.</source>
          <target state="translated">Las etiquetas de las clases.</target>
        </trans-unit>
        <trans-unit id="13f18ce429d97546a8b16cf132e8e6c03147cb50" translate="yes" xml:space="preserve">
          <source>The classic implementation of the clustering method based on the Lloyd&amp;rsquo;s algorithm. It consumes the whole set of input data at each iteration.</source>
          <target state="translated">La implementaci&amp;oacute;n cl&amp;aacute;sica del m&amp;eacute;todo de agrupamiento basado en el algoritmo de Lloyd's. Consume todo el conjunto de datos de entrada en cada iteraci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="b3e4f700832d6cdb3a215961499bbf68e1aa40b8" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b0c76ea0ae326d19f535ab9520b9dd024cf9124" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">La clasificaci&amp;oacute;n se realiza proyectando a los dos primeros componentes principales encontrados por PCA y CCA con fines de visualizaci&amp;oacute;n, seguido de usar el metaclasificador &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt; usando dos SVC con kernels lineales para aprender un modelo discriminativo para cada clase. Tenga en cuenta que PCA se usa para realizar una reducci&amp;oacute;n de dimensionalidad no supervisada, mientras que CCA se usa para realizar una reducci&amp;oacute;n supervisada.</target>
        </trans-unit>
        <trans-unit id="3f7b4b923ee0ab9dbf257d91692081f3d6b6945f" translate="yes" xml:space="preserve">
          <source>The classification target. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas Series.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c83c4f2209684d200eb3e2732a4357005bab63e" translate="yes" xml:space="preserve">
          <source>The classifier which predicts given the output of &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9bcc8debd316bfd1c4b9e73630f6ba7215b9e3d" translate="yes" xml:space="preserve">
          <source>The classifier whose output decision function needs to be calibrated to offer more accurate predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.</source>
          <target state="translated">El clasificador cuya función de decisión de salida debe ser calibrada para ofrecer salidas predict_proba más precisas.Si cv=prefit,el clasificador debe haber sido ajustado ya en los datos.</target>
        </trans-unit>
        <trans-unit id="4188695350bfd2d35a500d1eee9968d78806da89" translate="yes" xml:space="preserve">
          <source>The classifier whose output need to be calibrated to provide more accurate &lt;code&gt;predict_proba&lt;/code&gt; outputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="917e6ef2892859cfeed06565c1a2e19769195da1" translate="yes" xml:space="preserve">
          <source>The cluster ordered list of sample indices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88e39a3dfd7087282361f3b39109354f7ff32ede" translate="yes" xml:space="preserve">
          <source>The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs.</source>
          <target state="translated">El código que figura a continuación también ilustra cómo la construcción y el cómputo de las predicciones pueden ser paralelos dentro de múltiples trabajos.</target>
        </trans-unit>
        <trans-unit id="6a1cc4b5491e9b42fd445850086762fcb248e836" translate="yes" xml:space="preserve">
          <source>The code below plots the dependency of y against individual x_i and normalized values of univariate F-tests statistics and mutual information.</source>
          <target state="translated">En el código que figura a continuación se traza la dependencia de y con respecto a los valores individuales x_i y normalizados de las estadísticas de las pruebas F univariadas y la información mutua.</target>
        </trans-unit>
        <trans-unit id="4b0a65eb61a5277b5e28aef5c3b0e7ea577bdc07" translate="yes" xml:space="preserve">
          <source>The code-examples in the above tutorials are written in a &lt;em&gt;python-console&lt;/em&gt; format. If you wish to easily execute these examples in &lt;strong&gt;IPython&lt;/strong&gt;, use:</source>
          <target state="translated">Los ejemplos de c&amp;oacute;digo de los tutoriales anteriores est&amp;aacute;n escritos en formato de &lt;em&gt;consola de Python&lt;/em&gt; . Si desea ejecutar f&amp;aacute;cilmente estos ejemplos en &lt;strong&gt;IPython&lt;/strong&gt; , use:</target>
        </trans-unit>
        <trans-unit id="ce298b790cf6df954f5ce5058b04debe04deb707" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">El coeficiente R^2 se define como (1-u/v),donde u es la suma residual de los cuadrados ((y_true-y_pred)**2).sum()y v es la suma de regresión de los cuadrados ((y_true-y_true.mean())**2).sum().La mejor puntuación posible es 1.0 y puede ser negativa (porque el modelo puede ser arbitrariamente peor).Un modelo constante que siempre predice el valor esperado de y,sin tener en cuenta las características de entrada,obtendría una puntuación R^2 de 0,0.</target>
        </trans-unit>
        <trans-unit id="48121e6567bc65fc6ea38acd7d461ac73e2ea472" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">El coeficiente R^2 se define como (1-u/v),donde u es la suma residual de los cuadrados ((y_true-y_pred)**2).sum()y v es la suma total de los cuadrados ((y_true-y_true.mean())**2).sum().La mejor puntuación posible es 1.0 y puede ser negativa (porque el modelo puede ser arbitrariamente peor).Un modelo constante que siempre predice el valor esperado de y,sin tener en cuenta las características de entrada,obtendría una puntuación R^2 de 0,0.</target>
        </trans-unit>
        <trans-unit id="a7b8f1c4eb5f60ca5d7c57d0dcd2ad470def6e62" translate="yes" xml:space="preserve">
          <source>The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0dabbd241b37afcd99edc97579580c22f2703f9" translate="yes" xml:space="preserve">
          <source>The coefficient of the underlying linear model. It is returned only if coef is True.</source>
          <target state="translated">El coeficiente del modelo lineal subyacente.Sólo se devuelve si el coeficiente es verdadero.</target>
        </trans-unit>
        <trans-unit id="8b0531322a0447d32da022aa29f51bdb5853bf2b" translate="yes" xml:space="preserve">
          <source>The coefficients \(w\) of the model can be accessed:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bf2184b15d694a8d6ef65832d1a84477f9388bd" translate="yes" xml:space="preserve">
          <source>The coefficients are significantly different. AGE and EXPERIENCE coefficients are both positive but they now have less influence on the prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2933da174c8fa332cc9ad841c0c25a1ab1996d2" translate="yes" xml:space="preserve">
          <source>The coefficients can be forced to be positive.</source>
          <target state="translated">Los coeficientes pueden ser forzados a ser positivos.</target>
        </trans-unit>
        <trans-unit id="3c89d0ba57b5481659a50cc73b873694c355fd9b" translate="yes" xml:space="preserve">
          <source>The coefficients of the linear model: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</source>
          <target state="translated">Los coeficientes del modelo lineal: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="5395d49e7d69c07e1b2416a99bbd87627621ae2f" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the coefficient of determination are also calculated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7757730afe97cb8593dea4757d82727872c7529" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the variance score are also calculated.</source>
          <target state="translated">También se calculan los coeficientes,la suma residual de los cuadrados y la puntuación de la varianza.</target>
        </trans-unit>
        <trans-unit id="f15e1f2c7fb96cd43c32056a6c407c9c1c570883" translate="yes" xml:space="preserve">
          <source>The collection of fitted base estimators.</source>
          <target state="translated">La colección de estimadores de base ajustados.</target>
        </trans-unit>
        <trans-unit id="4d0721afe1a467c5d2eb2443395ac7ab7a4e3f8b" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &amp;lsquo;drop&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e97d4f2642ef3372648a38fef3350e771647c3d6" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">La colecci&amp;oacute;n de subestimadores ajustados como se define en &lt;code&gt;estimators&lt;/code&gt; que no son &lt;code&gt;None&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0b6e390487ac628e8895818624f2b32fd0c95d36" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators.</source>
          <target state="translated">La colección de subestimadores equipados.</target>
        </trans-unit>
        <trans-unit id="929cef17b33a8aadbb4cff8f87d4813d9e794dca" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators. &lt;code&gt;loss_.K&lt;/code&gt; is 1 for binary classification, otherwise n_classes.</source>
          <target state="translated">La colecci&amp;oacute;n de subestimadores ajustados. &lt;code&gt;loss_.K&lt;/code&gt; es 1 para la clasificaci&amp;oacute;n binaria; de lo contrario, n_classes.</target>
        </trans-unit>
        <trans-unit id="34213cac4f6d6096a4a72655d4c46d8da28f907c" translate="yes" xml:space="preserve">
          <source>The collection of fitted transformers as tuples of (name, fitted_transformer, column). &lt;code&gt;fitted_transformer&lt;/code&gt; can be an estimator, &amp;lsquo;drop&amp;rsquo;, or &amp;lsquo;passthrough&amp;rsquo;. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (&amp;lsquo;remainder&amp;rsquo;, transformer, remaining_columns) corresponding to the &lt;code&gt;remainder&lt;/code&gt; parameter. If there are remaining columns, then &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt;, otherwise &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt;.</source>
          <target state="translated">La colecci&amp;oacute;n de transformadores ajustados como tuplas de (nombre, transformador_ajustado, columna). &lt;code&gt;fitted_transformer&lt;/code&gt; puede ser un estimador, 'drop' o 'passthrough'. En caso de que no se hayan seleccionado columnas, este ser&amp;aacute; el transformador no equipado. Si quedan columnas, el elemento final es una tupla de la forma: ('resto', transformador, columnas_restante) correspondiente al par&amp;aacute;metro &lt;code&gt;remainder&lt;/code&gt; . Si quedan columnas, entonces &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt; , de lo contrario &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3626dc1999e74681fe5d5d9d41be14f043b3b76d" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the image, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd0348cc380532b01c9102e25a743ff6f1c3fc49" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the images, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;n_samples * max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6d471d12278a874c4225d83bbd25d7f04a0a976" translate="yes" xml:space="preserve">
          <source>The color map illustrates the decision function learned by the SVC.</source>
          <target state="translated">El mapa de colores ilustra la función de decisión aprendida por el SVC.</target>
        </trans-unit>
        <trans-unit id="093f5986f8d055d5b47f11ad021ce6ecfcef91e9" translate="yes" xml:space="preserve">
          <source>The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.</source>
          <target state="translated">Las columnas del indicador[n_nodes_ptr[i]:n_nodes_ptr[i+1]]dan el valor del indicador para el estimador i-ésimo.</target>
        </trans-unit>
        <trans-unit id="f0cca3e664aa52c0e892615c12365c0a6af40452" translate="yes" xml:space="preserve">
          <source>The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.</source>
          <target state="translated">La combinación utilizada en este ejemplo no es particularmente útil en este conjunto de datos y sólo se utiliza para ilustrar el uso de FeatureUnion.</target>
        </trans-unit>
        <trans-unit id="238b85659a6f96b691291ac49250c2a48af41530" translate="yes" xml:space="preserve">
          <source>The complete set of patches. If the patches contain colour information, channels are indexed along the last dimension: RGB patches would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4849a6d39c196dd952c4e98da8a227fce685751" translate="yes" xml:space="preserve">
          <source>The complexity parameter \(\alpha \geq 0\) controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4524a6f67fb21e2d5fb712c219044b7460dbad0e" translate="yes" xml:space="preserve">
          <source>The components of the random matrix are drawn from N(0, 1 / n_components).</source>
          <target state="translated">Los componentes de la matriz aleatoria se extraen de N(0,1/n_componentes).</target>
        </trans-unit>
        <trans-unit id="4d179dd17ec2e9e8ac79ee8fb26a8b8727033655" translate="yes" xml:space="preserve">
          <source>The compromise between l1 and l2 penalization chosen by cross validation</source>
          <target state="translated">El compromiso entre la penalización de l1 y l2 elegido por la validación cruzada</target>
        </trans-unit>
        <trans-unit id="37c8e8aa42af430070b2b87be4c5db5db294cc7b" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;fit&lt;/code&gt; is:</source>
          <target state="translated">El c&amp;aacute;lculo durante el &lt;code&gt;fit&lt;/code&gt; es:</target>
        </trans-unit>
        <trans-unit id="cc728704a5b1deaa221b61ada3dd42ad8e661fbd" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;predict&lt;/code&gt; is:</source>
          <target state="translated">El c&amp;aacute;lculo durante la &lt;code&gt;predict&lt;/code&gt; es:</target>
        </trans-unit>
        <trans-unit id="cc5659751ed96e6a9a511b247760eba7bd9fab63" translate="yes" xml:space="preserve">
          <source>The computation of Davies-Bouldin is simpler than that of Silhouette scores.</source>
          <target state="translated">El cálculo de Davies-Bouldin es más simple que el de las partituras de Silhouette.</target>
        </trans-unit>
        <trans-unit id="d526bd934b93818cdf8a3fccada8e6b2b34d84f2" translate="yes" xml:space="preserve">
          <source>The computational overhead of each SVD is &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt;, but only 2 * batch_size samples remain in memory at a time. There will be &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD computations to get the principal components, versus 1 large SVD of complexity &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; for PCA.</source>
          <target state="translated">La sobrecarga computacional de cada SVD es &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt; , pero solo 2 * muestras de batch_size permanecen en la memoria a la vez. Habr&amp;aacute; &lt;code&gt;n_samples / batch_size&lt;/code&gt; c&amp;aacute;lculos de SVD para obtener los componentes principales, frente a 1 SVD grande de complejidad &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; para PCA.</target>
        </trans-unit>
        <trans-unit id="2fa163515de812b4d78a2bbcef87e239a36ae4d1" translate="yes" xml:space="preserve">
          <source>The concept of early stopping is simple. We specify a &lt;code&gt;validation_fraction&lt;/code&gt; which denotes the fraction of the whole dataset that will be kept aside from training to assess the validation loss of the model. The gradient boosting model is trained using the training set and evaluated using the validation set. When each additional stage of regression tree is added, the validation set is used to score the model. This is continued until the scores of the model in the last &lt;code&gt;n_iter_no_change&lt;/code&gt; stages do not improve by atleast &lt;code&gt;tol&lt;/code&gt;. After that the model is considered to have converged and further addition of stages is &amp;ldquo;stopped early&amp;rdquo;.</source>
          <target state="translated">El concepto de parada anticipada es simple. Especificamos una &lt;code&gt;validation_fraction&lt;/code&gt; que denota la fracci&amp;oacute;n de todo el conjunto de datos que se mantendr&amp;aacute; aparte del entrenamiento para evaluar la p&amp;eacute;rdida de validaci&amp;oacute;n del modelo. El modelo de aumento de gradiente se entrena con el conjunto de entrenamiento y se eval&amp;uacute;a con el conjunto de validaci&amp;oacute;n. Cuando se agrega cada etapa adicional del &amp;aacute;rbol de regresi&amp;oacute;n, el conjunto de validaci&amp;oacute;n se usa para calificar el modelo. Esto se contin&amp;uacute;a hasta que las puntuaciones del modelo en las &amp;uacute;ltimas etapas &lt;code&gt;n_iter_no_change&lt;/code&gt; no mejoran al menos en &lt;code&gt;tol&lt;/code&gt; . Despu&amp;eacute;s de eso, se considera que el modelo ha convergido y la adici&amp;oacute;n adicional de etapas se &quot;detiene antes&quot;.</target>
        </trans-unit>
        <trans-unit id="feb60f640c62fd0856cb50e8401267d4f55d4774" translate="yes" xml:space="preserve">
          <source>The concrete &lt;code&gt;LossFunction&lt;/code&gt; object.</source>
          <target state="translated">El objeto &lt;code&gt;LossFunction&lt;/code&gt; concreto .</target>
        </trans-unit>
        <trans-unit id="4f21e1340272b60cf06a69153cc19d0140d625cf" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">La funci&amp;oacute;n de p&amp;eacute;rdida de hormig&amp;oacute;n se puede configurar mediante el par&amp;aacute;metro de &lt;code&gt;loss&lt;/code&gt; . &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; admite las siguientes funciones de p&amp;eacute;rdida:</target>
        </trans-unit>
        <trans-unit id="c24a50928ec0729629e45e8b53b6d8b9f0112f79" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">La funci&amp;oacute;n de p&amp;eacute;rdida de hormig&amp;oacute;n se puede configurar mediante el par&amp;aacute;metro de &lt;code&gt;loss&lt;/code&gt; . &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; admite las siguientes funciones de p&amp;eacute;rdida:</target>
        </trans-unit>
        <trans-unit id="a5b98bb304bc3ec6a422167859a2f5da5580fa47" translate="yes" xml:space="preserve">
          <source>The concrete penalty can be set via the &lt;code&gt;penalty&lt;/code&gt; parameter. SGD supports the following penalties:</source>
          <target state="translated">La penalizaci&amp;oacute;n concreta se puede establecer mediante el par&amp;aacute;metro de &lt;code&gt;penalty&lt;/code&gt; . SGD admite las siguientes sanciones:</target>
        </trans-unit>
        <trans-unit id="c0c66ccf788e88932593e02308e1dcae0bb5c1f0" translate="yes" xml:space="preserve">
          <source>The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:</source>
          <target state="translated">La distribución de probabilidad condicional de cada unidad viene dada por la función de activación del sigmoide logístico de la entrada que recibe:</target>
        </trans-unit>
        <trans-unit id="6e94f7f28c2cc9bab8c93a85a4438b802d82d10d" translate="yes" xml:space="preserve">
          <source>The confidence score for a sample is the signed distance of that sample to the hyperplane.</source>
          <target state="translated">La puntuación de confianza de una muestra es la distancia firmada de esa muestra al hiperplano.</target>
        </trans-unit>
        <trans-unit id="b129f98741f30e6ae05e0872d86b02b5a921e905" translate="yes" xml:space="preserve">
          <source>The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;&lt;/a&gt; to restrict merging to nearest neighbors as in &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;this example&lt;/a&gt;, or using &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt;&lt;/a&gt; to enable only merging of neighboring pixels on an image, as in the &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;coin&lt;/a&gt; example.</source>
          <target state="translated">Las restricciones de conectividad se imponen a trav&amp;eacute;s de una matriz de conectividad: una matriz dispersa scipy que tiene elementos solo en la intersecci&amp;oacute;n de una fila y una columna con &amp;iacute;ndices del conjunto de datos que deben conectarse. Esta matriz se puede construir a partir de informaci&amp;oacute;n a priori: por ejemplo, es posible que desee agrupar p&amp;aacute;ginas web fusionando &amp;uacute;nicamente p&amp;aacute;ginas con un enlace que apunte de una a otra. Tambi&amp;eacute;n se puede aprender de los datos, por ejemplo, usando &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; &lt;/a&gt; para restringir la fusi&amp;oacute;n a los vecinos m&amp;aacute;s cercanos como en &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;este ejemplo&lt;/a&gt; , o usando &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt; &lt;/a&gt; para habilitar solo la fusi&amp;oacute;n de p&amp;iacute;xeles vecinos en una imagen, como en el ejemplo de la &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;moneda&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="cba08514635a6da99fcbf046720afd85a65680ea" translate="yes" xml:space="preserve">
          <source>The constant value which defines the covariance: k(x_1, x_2) = constant_value</source>
          <target state="translated">El valor constante que define la covarianza:k(x_1,x_2)=valor_constante</target>
        </trans-unit>
        <trans-unit id="83836055dd86bf18d0180179c00f49511034950c" translate="yes" xml:space="preserve">
          <source>The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings.</source>
          <target state="translated">La tabla de contingencia calculada suele utilizarse en el cálculo de una estadística de similitud (como las otras enumeradas en este documento)entre las dos agrupaciones.</target>
        </trans-unit>
        <trans-unit id="ca0505a0687635a00a190438e7d26956e48c468c" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</source>
          <target state="translated">El umbral de convergencia.Las iteraciones EM se detendrán cuando la ganancia media del límite inferior esté por debajo de este umbral.</target>
        </trans-unit>
        <trans-unit id="0e26ef32f94bda64e8f6e396c3c303a3311ac4e1" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold.</source>
          <target state="translated">El umbral de convergencia.Las iteraciones EM se detendrán cuando la ganancia media del límite inferior de la probabilidad (de los datos de entrenamiento con respecto al modelo)esté por debajo de este umbral.</target>
        </trans-unit>
        <trans-unit id="80f05388942910150044c6d47584d8bddc73ef4c" translate="yes" xml:space="preserve">
          <source>The converse mapping from feature name to column index is stored in the &lt;code&gt;vocabulary_&lt;/code&gt; attribute of the vectorizer:</source>
          <target state="translated">El mapeo inverso del nombre de la caracter&amp;iacute;stica al &amp;iacute;ndice de la columna se almacena en el atributo &lt;code&gt;vocabulary_&lt;/code&gt; del vectorizador:</target>
        </trans-unit>
        <trans-unit id="42e30f7491cd0bbd1be9eaa1008d97761b319a5f" translate="yes" xml:space="preserve">
          <source>The converted and validated X.</source>
          <target state="translated">La X convertida y validada.</target>
        </trans-unit>
        <trans-unit id="f4a00aeed47e5a0cd669edcdec893bcad4bc87b6" translate="yes" xml:space="preserve">
          <source>The converted and validated array.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="043f2ec629cc510b9c1127fe6b10b6fe4448d241" translate="yes" xml:space="preserve">
          <source>The converted and validated y.</source>
          <target state="translated">El convertido y validado y.</target>
        </trans-unit>
        <trans-unit id="81a7bc326e697c66d8802c043c85fabeabbf241b" translate="yes" xml:space="preserve">
          <source>The converted dataname.</source>
          <target state="translated">El nombre de datos convertido.</target>
        </trans-unit>
        <trans-unit id="223ada656bcdd3fee604b90acc8ae4baf52723e0" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44e5d228f37bd8405cfefcfa348ce3d27d939cb4" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e11d4d7608fe1fa28c8a673b1389ac6181cb7877" translate="yes" xml:space="preserve">
          <source>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights \(w_1\), \(w_2\), &amp;hellip;, \(w_N\) to each of the training samples. Initially, those weights are all set to \(w_i = 1/N\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence &lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;.</source>
          <target state="translated">El principio central de AdaBoost es ajustar una secuencia de estudiantes d&amp;eacute;biles (es decir, modelos que son solo un poco mejores que las adivinanzas aleatorias, como peque&amp;ntilde;os &amp;aacute;rboles de decisi&amp;oacute;n) en versiones modificadas repetidamente de los datos. Las predicciones de todos ellos luego se combinan mediante un voto mayoritario ponderado (o suma) para producir la predicci&amp;oacute;n final. Las modificaciones de datos en cada una de las llamadas iteraciones de impulso consisten en aplicar pesos \ (w_1 \), \ (w_2 \),&amp;hellip;, \ (w_N \) a cada una de las muestras de entrenamiento. Inicialmente, todos esos pesos se establecen en \ (w_i = 1 / N \), de modo que el primer paso simplemente entrena a un alumno d&amp;eacute;bil en los datos originales. Para cada iteraci&amp;oacute;n sucesiva, los pesos de la muestra se modifican individualmente y el algoritmo de aprendizaje se vuelve a aplicar a los datos reponderados. En un paso dadolos ejemplos de entrenamiento que fueron predichos incorrectamente por el modelo impulsado inducido en el paso anterior tienen sus pesos aumentados, mientras que los pesos disminuyen para aquellos que se predijeron correctamente. A medida que avanzan las iteraciones, los ejemplos que son dif&amp;iacute;ciles de predecir reciben una influencia cada vez mayor. Por lo tanto, cada alumno d&amp;eacute;bil posterior se ve obligado a concentrarse en los ejemplos que los anteriores en la secuencia se pierden.&lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2a94b871b5efce504e5077120d25fb83e224b7ee" translate="yes" xml:space="preserve">
          <source>The corpus is a collection of \(D\) documents.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81757ae28a30627b83ded3cb76d108f655541caf" translate="yes" xml:space="preserve">
          <source>The correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y)).</source>
          <target state="translated">Se calcula la correlación entre cada regresor y el objetivo,es decir,((X[:,i]-media(X[:,i]))*(y-media_y))/(std(X[:,i])*std(y)).</target>
        </trans-unit>
        <trans-unit id="302d596f93db30183d7b9abc1999d9cfb95f2d1f" translate="yes" xml:space="preserve">
          <source>The corresponding image is:</source>
          <target state="translated">La imagen correspondiente es:</target>
        </trans-unit>
        <trans-unit id="9a9313289c48f29b6d60eb0302d43d099de5f93d" translate="yes" xml:space="preserve">
          <source>The corresponding training labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f6395cc147644bb3051e08a46acbc9cd47145af" translate="yes" xml:space="preserve">
          <source>The cosine distance is defined as &lt;code&gt;1 - cosine_similarity&lt;/code&gt;: the lowest value is 0 (identical point) but it is bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only on their relative angles.</source>
          <target state="translated">La distancia del coseno se define como &lt;code&gt;1 - cosine_similarity&lt;/code&gt; : el valor m&amp;aacute;s bajo es 0 (punto id&amp;eacute;ntico) pero est&amp;aacute; delimitado por encima de 2 para los puntos m&amp;aacute;s lejanos. Su valor no depende de la norma de los puntos vectoriales sino solo de sus &amp;aacute;ngulos relativos.</target>
        </trans-unit>
        <trans-unit id="c4b95b481d689de7dde58522ad60a16deab9f841" translate="yes" xml:space="preserve">
          <source>The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm</source>
          <target state="translated">La distancia del coseno equivale a la mitad de la distancia euclidiana al cuadrado si cada muestra se normaliza a la norma unitaria</target>
        </trans-unit>
        <trans-unit id="59fadedc84e4bf70f9182651c0ff609a02cebdec" translate="yes" xml:space="preserve">
          <source>The cost complexity measure of a single node is \(R_\alpha(t)=R(t)+\alpha\). The branch, \(T_t\), is defined to be a tree where node \(t\) is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, \(R(T_t)&amp;lt;R(t)\). However, the cost complexity measure of a node, \(t\), and its branch, \(T_t\), can be equal depending on \(\alpha\). We define the effective \(\alpha\) of a node to be the value where they are equal, \(R_\alpha(T_t)=R_\alpha(t)\) or \(\alpha_{eff}(t)=\frac{R(t)-R(T_t)}{|T|-1}\). A non-terminal node with the smallest value of \(\alpha_{eff}\) is the weakest link and will be pruned. This process stops when the pruned tree&amp;rsquo;s minimal \(\alpha_{eff}\) is greater than the &lt;code&gt;ccp_alpha&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de99d6d126cf03f253c85bcda8fea4fdd47045eb" translate="yes" xml:space="preserve">
          <source>The cost function of an isomap embedding is</source>
          <target state="translated">La función de coste de una incrustación de isomapa es</target>
        </trans-unit>
        <trans-unit id="40200d49debb9a752670d9f4e91dc77cd9d66f0a" translate="yes" xml:space="preserve">
          <source>The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.</source>
          <target state="translated">El costo de la utilización del árbol (es decir,la predicción de datos)es logarítmico en cuanto al número de puntos de datos utilizados para entrenar el árbol.</target>
        </trans-unit>
        <trans-unit id="c9b6813659bffb0c08aeb974fcecdaaf97e786b0" translate="yes" xml:space="preserve">
          <source>The covariance matrix of a data set is known to be well approximated by the classical &lt;em&gt;maximum likelihood estimator&lt;/em&gt; (or &amp;ldquo;empirical covariance&amp;rdquo;), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population&amp;rsquo;s covariance matrix.</source>
          <target state="translated">Se sabe que la matriz de covarianza de un conjunto de datos est&amp;aacute; bien aproximada por el &lt;em&gt;estimador de m&amp;aacute;xima verosimilitud&lt;/em&gt; cl&amp;aacute;sica (o &quot;covarianza emp&amp;iacute;rica&quot;), siempre que el n&amp;uacute;mero de observaciones sea lo suficientemente grande en comparaci&amp;oacute;n con el n&amp;uacute;mero de caracter&amp;iacute;sticas (las variables que describen las observaciones). M&amp;aacute;s precisamente, el estimador de m&amp;aacute;xima verosimilitud de una muestra es un estimador insesgado de la matriz de covarianza de la poblaci&amp;oacute;n correspondiente.</target>
        </trans-unit>
        <trans-unit id="ff9747571dfcab155502f0d51c20ced0a4eda87c" translate="yes" xml:space="preserve">
          <source>The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions.</source>
          <target state="translated">La matriz de covarianza será este valor multiplicado por la matriz de unidades.Este conjunto de datos sólo produce distribuciones normales simétricas.</target>
        </trans-unit>
        <trans-unit id="d4f25f1f739fb6455b136c8d6d48a91032e89970" translate="yes" xml:space="preserve">
          <source>The covariance of each mixture component. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">La covarianza de cada componente de la mezcla. La forma depende de &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="37e0aa90ea88b46d75d1f0f3b55b29aae27815f8" translate="yes" xml:space="preserve">
          <source>The covariance to compare with.</source>
          <target state="translated">La covarianza para comparar.</target>
        </trans-unit>
        <trans-unit id="a8664fdb20de58ef24fb2b254f60a073e2dad9f4" translate="yes" xml:space="preserve">
          <source>The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the canonical correlation analysis (CCA).</source>
          <target state="translated">El módulo de descomposición cruzada contiene dos familias principales de algoritmos:el de mínimos cuadrados parciales (PLS)y el de análisis de correlación canónica (CCA).</target>
        </trans-unit>
        <trans-unit id="5725093c72b0120c682574584b056d0b295efdd9" translate="yes" xml:space="preserve">
          <source>The cross validation score obtained on the training data</source>
          <target state="translated">La puntuación de validación cruzada obtenida en los datos de entrenamiento</target>
        </trans-unit>
        <trans-unit id="7d9ee6d7f0b44964f013a45e604e6c29cc8a3f8f" translate="yes" xml:space="preserve">
          <source>The cross-validation can then be performed easily:</source>
          <target state="translated">La validación cruzada puede entonces realizarse fácilmente:</target>
        </trans-unit>
        <trans-unit id="58fa7e854fcd94e774f9d9c000c917c76d680086" translate="yes" xml:space="preserve">
          <source>The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b91dc81955b1dbc1a750c7c0ae1e516f1cb5789e" translate="yes" xml:space="preserve">
          <source>The cross-validation score can be directly calculated using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper. Given an estimator, the cross-validation object and the input dataset, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</source>
          <target state="translated">La puntuaci&amp;oacute;n de validaci&amp;oacute;n cruzada se puede calcular directamente utilizando el ayudante &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; . Dado un estimador, el objeto de validaci&amp;oacute;n cruzada y el conjunto de datos de entrada, &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; divide los datos repetidamente en un conjunto de entrenamiento y de prueba, entrena al estimador utilizando el conjunto de entrenamiento y calcula las puntuaciones seg&amp;uacute;n el conjunto de pruebas para cada iteraci&amp;oacute;n de validaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="0c9e67494c034c3aceceb77302443d5c2bb54301" translate="yes" xml:space="preserve">
          <source>The cross-validation scores such that &lt;code&gt;grid_scores_[i]&lt;/code&gt; corresponds to the CV score of the i-th subset of features.</source>
          <target state="translated">Los puntajes de validaci&amp;oacute;n cruzada tales que &lt;code&gt;grid_scores_[i]&lt;/code&gt; corresponde al puntaje CV del i-&amp;eacute;simo subconjunto de caracter&amp;iacute;sticas.</target>
        </trans-unit>
        <trans-unit id="7186db76e260a93ceff6e7a012a18028bce16f5b" translate="yes" xml:space="preserve">
          <source>The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see &lt;code&gt;NearestNeighbors&lt;/code&gt;.</source>
          <target state="translated">La implementaci&amp;oacute;n actual usa &amp;aacute;rboles de bolas y &amp;aacute;rboles kd para determinar la vecindad de los puntos, lo que evita calcular la matriz de distancia completa (como se hizo en las versiones de scikit-learn antes de 0.14). Se conserva la posibilidad de utilizar m&amp;eacute;tricas personalizadas; para obtener m&amp;aacute;s informaci&amp;oacute;n, consulte &lt;code&gt;NearestNeighbors&lt;/code&gt; m&amp;aacute;s cercanos .</target>
        </trans-unit>
        <trans-unit id="e3ec89890503565d6b8dae28b11789379133ea78" translate="yes" xml:space="preserve">
          <source>The current loss computed with the loss function.</source>
          <target state="translated">La pérdida actual calculada con la función de pérdida.</target>
        </trans-unit>
        <trans-unit id="a415ef6f77731ce51df5a94c5015135ebd3f462c" translate="yes" xml:space="preserve">
          <source>The curse of dimensionality</source>
          <target state="translated">La maldición de la dimensionalidad</target>
        </trans-unit>
        <trans-unit id="3eaad00bc0bba0577db9b826b646317a24a90409" translate="yes" xml:space="preserve">
          <source>The data</source>
          <target state="translated">Los datos</target>
        </trans-unit>
        <trans-unit id="973622d96c176a0a77b0dde8f6175466c5afec2c" translate="yes" xml:space="preserve">
          <source>The data is always a 2D array, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt;, although the original data may have had a different shape. In the case of the digits, each original sample is an image of shape &lt;code&gt;(8, 8)&lt;/code&gt; and can be accessed using:</source>
          <target state="translated">Los datos son siempre una matriz 2D, forma &lt;code&gt;(n_samples, n_features)&lt;/code&gt; , aunque los datos originales pueden haber tenido una forma diferente. En el caso de los d&amp;iacute;gitos, cada muestra original es una imagen de forma &lt;code&gt;(8, 8)&lt;/code&gt; y se puede acceder mediante:</target>
        </trans-unit>
        <trans-unit id="0e6af14908ee7619e613da95e5ef5e8434ef93d9" translate="yes" xml:space="preserve">
          <source>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.</source>
          <target state="translated">Se supone que los datos no son negativos,y a menudo se normalizan para tener una norma L1 de uno.La normalización se racionaliza con la conexión a la distancia chi cuadrado,que es una distancia entre distribuciones de probabilidad discretas.</target>
        </trans-unit>
        <trans-unit id="aa2161d468d436ce31e1b1a4a535d19d4acf7995" translate="yes" xml:space="preserve">
          <source>The data is generated with the &lt;code&gt;make_checkerboard&lt;/code&gt; function, then shuffled and passed to the Spectral Biclustering algorithm. The rows and columns of the shuffled matrix are rearranged to show the biclusters found by the algorithm.</source>
          <target state="translated">Los datos se generan con la funci&amp;oacute;n &lt;code&gt;make_checkerboard&lt;/code&gt; , luego se barajan y pasan al algoritmo Spectral Biclustering. Las filas y columnas de la matriz barajada se reorganizan para mostrar los biclusters encontrados por el algoritmo.</target>
        </trans-unit>
        <trans-unit id="cbdb41e13849fa39e43a0c0831b005495a0d0342" translate="yes" xml:space="preserve">
          <source>The data is split according to the cv parameter. Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="549dd87c3787b6b83f2456e4cff9d8aa392d12c1" translate="yes" xml:space="preserve">
          <source>The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.</source>
          <target state="translated">Los datos son los resultados de un análisis químico de vinos cultivados en la misma región de Italia por tres cultivadores diferentes.Hay trece medidas diferentes tomadas para los diferentes componentes encontrados en los tres tipos de vino.</target>
        </trans-unit>
        <trans-unit id="92b1cf272aa9964f4bcd0a98099303474f1eb6db" translate="yes" xml:space="preserve">
          <source>The data list to learn.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="279fbf7584e00e713412e8ff4546300536ffeb5a" translate="yes" xml:space="preserve">
          <source>The data matrix</source>
          <target state="translated">La matriz de datos</target>
        </trans-unit>
        <trans-unit id="8a93d3171d08360f1c2b565ec2f68ceaa0cb40fb" translate="yes" xml:space="preserve">
          <source>The data matrix to learn.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a36ea8c8057a9ae699a69f099ee316c3295afaf3" translate="yes" xml:space="preserve">
          <source>The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.</source>
          <target state="translated">La matriz de datos,con p características y n muestras.El conjunto de datos debe ser el que se utilizó para calcular las estimaciones brutas.</target>
        </trans-unit>
        <trans-unit id="f0ed1480aeed96966c83d245c0e82839723677ea" translate="yes" xml:space="preserve">
          <source>The data matrix.</source>
          <target state="translated">La matriz de datos.</target>
        </trans-unit>
        <trans-unit id="3e01104c886db328397aa7550432fcf04c711803" translate="yes" xml:space="preserve">
          <source>The data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70cee63c42777050ee29bb5c41f5ed813382b6c9" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is int</source>
          <target state="translated">Los datos de la matriz de dispersión devuelta.Por defecto es int</target>
        </trans-unit>
        <trans-unit id="13b8870b16632bb144f99a987da0a17b912fd261" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is the dtype of img</source>
          <target state="translated">Los datos de la matriz de dispersión devuelta.Por defecto es el tipo de img</target>
        </trans-unit>
        <trans-unit id="a9c1f6d9c961581c21421066f6ce8f7c709084a9" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained.</source>
          <target state="translated">Los datos sobre los que se entren&amp;oacute; &lt;code&gt;gbrt&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c66d20ce5405cff4e02a00c321afd4016f52379a" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained. It is used to generate a &lt;code&gt;grid&lt;/code&gt; for the &lt;code&gt;target_variables&lt;/code&gt;. The &lt;code&gt;grid&lt;/code&gt; comprises &lt;code&gt;grid_resolution&lt;/code&gt; equally spaced points between the two &lt;code&gt;percentiles&lt;/code&gt;.</source>
          <target state="translated">Los datos sobre los que se entren&amp;oacute; &lt;code&gt;gbrt&lt;/code&gt; . Se utiliza para generar una &lt;code&gt;grid&lt;/code&gt; para &lt;code&gt;target_variables&lt;/code&gt; . La &lt;code&gt;grid&lt;/code&gt; comprende &lt;code&gt;grid_resolution&lt;/code&gt; puntos igualmente espaciados entre los dos &lt;code&gt;percentiles&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2ffaf51af185adce5e53447e39433481435abf03" translate="yes" xml:space="preserve">
          <source>The data samples transformed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65c0e8f3b46fedbf2f41b7e1589997d8de30db77" translate="yes" xml:space="preserve">
          <source>The data set contains images of hand-written digits: 10 classes where each class refers to a digit.</source>
          <target state="translated">El conjunto de datos contiene imágenes de dígitos escritos a mano:10 clases donde cada clase se refiere a un dígito.</target>
        </trans-unit>
        <trans-unit id="a49ccf8db4843f7af3fe75d2e385d4c5ae4a247b" translate="yes" xml:space="preserve">
          <source>The data that should be scaled.</source>
          <target state="translated">Los datos que deben ser escalados.</target>
        </trans-unit>
        <trans-unit id="4362ce6dec3d09ef8de03aa7af16c30845dd1b85" translate="yes" xml:space="preserve">
          <source>The data that should be transformed back.</source>
          <target state="translated">Los datos que deben ser transformados de nuevo.</target>
        </trans-unit>
        <trans-unit id="47ba75ee664e21a51401b4c1203a08e7a876f44e" translate="yes" xml:space="preserve">
          <source>The data to be transformed by subset.</source>
          <target state="translated">Los datos a transformar por subconjunto.</target>
        </trans-unit>
        <trans-unit id="03bb048cfbbc3212fd60edf266a3822d3176ef87" translate="yes" xml:space="preserve">
          <source>The data to be transformed using a power transformation.</source>
          <target state="translated">Los datos a ser transformados usando una transformación de energía.</target>
        </trans-unit>
        <trans-unit id="73ce115221fb870e05c90b8f043ca278fdee74a1" translate="yes" xml:space="preserve">
          <source>The data to be transformed.</source>
          <target state="translated">Los datos a transformar.</target>
        </trans-unit>
        <trans-unit id="df7f442215a9627450351a5239ccb7b837681c69" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">Los datos a binarizar,elemento por elemento.las matrices scipy.sparse deben estar en formato CSR para evitar una copia innecesaria.</target>
        </trans-unit>
        <trans-unit id="5401bf0fdb954957c9a3f345344e508dbd6bac54" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy.</source>
          <target state="translated">Los datos a binarizar,elemento por elemento.Las matrices scipy.sparse deben estar en formato CSR o CSC para evitar una copia innecesaria.</target>
        </trans-unit>
        <trans-unit id="515108ae5aad51ada5b234ccff65d93cdd42711e" translate="yes" xml:space="preserve">
          <source>The data to center and scale.</source>
          <target state="translated">Los datos para centrar y escalar.</target>
        </trans-unit>
        <trans-unit id="975edd1be086184330a8a3ebbf935588408d4a98" translate="yes" xml:space="preserve">
          <source>The data to determine the categories of each feature.</source>
          <target state="translated">Los datos para determinar las categorías de cada característica.</target>
        </trans-unit>
        <trans-unit id="a77a7e1c81043cc97f700ad0d213253f486b563a" translate="yes" xml:space="preserve">
          <source>The data to encode.</source>
          <target state="translated">Los datos a codificar.</target>
        </trans-unit>
        <trans-unit id="d843c1d7e5e8986d5ad23251cff2a94ef8ee7955" translate="yes" xml:space="preserve">
          <source>The data to fit.</source>
          <target state="translated">Los datos para encajar.</target>
        </trans-unit>
        <trans-unit id="e7fbe72ba2fcf8fff679b5e28b0fef9589e7589d" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be for example a list, or an array.</source>
          <target state="translated">Los datos para encajar.Puede ser,por ejemplo,una lista,o una matriz.</target>
        </trans-unit>
        <trans-unit id="00725d5de44209593a99040fefb16433276154c2" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be, for example a list, or an array at least 2d.</source>
          <target state="translated">Los datos para encajar.Puede ser,por ejemplo,una lista,o una matriz de al menos 2d.</target>
        </trans-unit>
        <trans-unit id="53846ac28d489b02b9949ef562a942f522e52d48" translate="yes" xml:space="preserve">
          <source>The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">Los datos a normalizar,elemento por elemento.las matrices scipy.sparse deben estar en formato CSR para evitar una copia innecesaria.</target>
        </trans-unit>
        <trans-unit id="c13e1ef3ee3dcba79bb567a64bd572c32814ce01" translate="yes" xml:space="preserve">
          <source>The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">Los datos a normalizar,fila por fila.Las matrices scipy.sparse deben estar en formato CSR para evitar una copia innecesaria.</target>
        </trans-unit>
        <trans-unit id="89435a85b63a4550536a3843494521cdfd812011" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a092ffb0fb8aec81b90c0802c73b2ae6cfa80dcc" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row. Sparse input should preferably be in CSC format.</source>
          <target state="translated">Los datos a transformar,fila por fila.Los datos escasos deben estar preferentemente en formato CSC.</target>
        </trans-unit>
        <trans-unit id="3f0555e5cf2c3f2eb80a653fe1f67911e223ec90" translate="yes" xml:space="preserve">
          <source>The data to transform.</source>
          <target state="translated">Los datos a transformar.</target>
        </trans-unit>
        <trans-unit id="f0a016c9443bbddef85796fd3691360a3c817ea0" translate="yes" xml:space="preserve">
          <source>The data used to compute the mean and standard deviation used for later scaling along the features axis.</source>
          <target state="translated">Los datos utilizados para calcular la media y la desviación estándar utilizados para la posterior escalada a lo largo del eje de las características.</target>
        </trans-unit>
        <trans-unit id="55438370e51e74e9e8aa5e6ed6a37b2c111bd898" translate="yes" xml:space="preserve">
          <source>The data used to compute the median and quantiles used for later scaling along the features axis.</source>
          <target state="translated">Los datos utilizados para calcular la mediana y los cuantiles utilizados para la posterior escalada a lo largo del eje de las características.</target>
        </trans-unit>
        <trans-unit id="0ae3bec0fd19bdece5355d54cf80d1ae7bcc7c26" translate="yes" xml:space="preserve">
          <source>The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</source>
          <target state="translated">Los datos utilizados para calcular el mínimo y el máximo por característica utilizados para la posterior ampliación a lo largo del eje de las características.</target>
        </trans-unit>
        <trans-unit id="e26762b4f27727d3210d3fe2a64ad24b366c1062" translate="yes" xml:space="preserve">
          <source>The data used to estimate the optimal transformation parameters.</source>
          <target state="translated">Los datos utilizados para estimar los parámetros de transformación óptimos.</target>
        </trans-unit>
        <trans-unit id="56c320de81ba4f246c360453cbd2da4e63d63c7f" translate="yes" xml:space="preserve">
          <source>The data used to fit the model. If &lt;code&gt;copy_X=False&lt;/code&gt;, then &lt;code&gt;X_fit_&lt;/code&gt; is a reference. This attribute is used for the calls to transform.</source>
          <target state="translated">Los datos utilizados para ajustar el modelo. Si &lt;code&gt;copy_X=False&lt;/code&gt; , entonces &lt;code&gt;X_fit_&lt;/code&gt; es una referencia. Este atributo se utiliza para las llamadas a transformar.</target>
        </trans-unit>
        <trans-unit id="03376550c822ad450e2d14840686210c4804ebc6" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis.</source>
          <target state="translated">Los datos utilizados para escalar a lo largo del eje de las características.</target>
        </trans-unit>
        <trans-unit id="c4374e3fd9ee0863ad791a18672a80272e651c9f" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;. Additionally, the sparse matrix needs to be nonnegative if &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; is False.</source>
          <target state="translated">Los datos utilizados para escalar a lo largo del eje de caracter&amp;iacute;sticas. Si se proporciona una matriz dispersa, se convertir&amp;aacute; en una &lt;code&gt;csc_matrix&lt;/code&gt; dispersa . Adem&amp;aacute;s, la matriz dispersa debe ser no negativa si &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; es False.</target>
        </trans-unit>
        <trans-unit id="848647eb355f23af31ef1bf06cbb1e0d945eea47" translate="yes" xml:space="preserve">
          <source>The data used to scale along the specified axis.</source>
          <target state="translated">Los datos utilizados para escalar a lo largo del eje especificado.</target>
        </trans-unit>
        <trans-unit id="db728424571c6d0aa73c940cbe613870747b4067" translate="yes" xml:space="preserve">
          <source>The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)</source>
          <target state="translated">Los datos se utilizaron con muchos otros para comparar varios clasificadores.Las clases son separables,aunque sólo RDA ha logrado una clasificación 100% correcta.(RDA:100%,QDA 99.4%,LDA 98.9%,1NN 96.1% (datos transformados en z))(Todos los resultados usando la técnica de dejar-uno-fuera)</target>
        </trans-unit>
        <trans-unit id="660fe59aa4f1107f7d968ac4c0471ddbff4317b5" translate="yes" xml:space="preserve">
          <source>The data.</source>
          <target state="translated">Los datos.</target>
        </trans-unit>
        <trans-unit id="02bda070eb404181c9796aadeceacf78aac05356" translate="yes" xml:space="preserve">
          <source>The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a &lt;code&gt;sample_weight&lt;/code&gt; when fitting DBSCAN.</source>
          <target state="translated">El conjunto de datos se puede comprimir, ya sea eliminando los duplicados exactos si estos ocurren en sus datos, o usando BIRCH. Entonces solo tiene un n&amp;uacute;mero relativamente peque&amp;ntilde;o de representantes para una gran cantidad de puntos. A continuaci&amp;oacute;n, puede proporcionar un &lt;code&gt;sample_weight&lt;/code&gt; al ajustar DBSCAN.</target>
        </trans-unit>
        <trans-unit id="6bbac4d66b74c67e38ab2b8e6e075ffda05e6522" translate="yes" xml:space="preserve">
          <source>The dataset is called &amp;ldquo;Twenty Newsgroups&amp;rdquo;. Here is the official description, quoted from the &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;website&lt;/a&gt;:</source>
          <target state="translated">El conjunto de datos se llama &quot;Veinte grupos de noticias&quot;. Aqu&amp;iacute; est&amp;aacute; la descripci&amp;oacute;n oficial, citada del &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;sitio web&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="c4573e45f21f00151e7512eaf1b6ae1cd41e22bd" translate="yes" xml:space="preserve">
          <source>The dataset is from Zhu et al [1].</source>
          <target state="translated">El conjunto de datos es de Zhu et al [1].</target>
        </trans-unit>
        <trans-unit id="1a602d6b3832c4acaa760453be6cb4d26914f03f" translate="yes" xml:space="preserve">
          <source>The dataset is generated using the &lt;code&gt;make_biclusters&lt;/code&gt; function, which creates a matrix of small values and implants bicluster with large values. The rows and columns are then shuffled and passed to the Spectral Co-Clustering algorithm. Rearranging the shuffled matrix to make biclusters contiguous shows how accurately the algorithm found the biclusters.</source>
          <target state="translated">El conjunto de datos se genera utilizando la funci&amp;oacute;n &lt;code&gt;make_biclusters&lt;/code&gt; , que crea una matriz de valores peque&amp;ntilde;os e implanta bicluster con valores grandes. Luego, las filas y columnas se barajan y se pasan al algoritmo de agrupaci&amp;oacute;n conjunta espectral. Reorganizar la matriz barajada para hacer biclusters contiguos muestra la precisi&amp;oacute;n con la que el algoritmo encontr&amp;oacute; los biclusters.</target>
        </trans-unit>
        <trans-unit id="4131a4e690502c19383d02f0465052ba8df886e1" translate="yes" xml:space="preserve">
          <source>The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">El conjunto de datos est&amp;aacute; estructurado de manera que los puntos cercanos en orden de &amp;iacute;ndice est&amp;eacute;n cerca en el espacio de par&amp;aacute;metros, lo que lleva a una matriz de aproximadamente bloques en diagonal de K vecinos m&amp;aacute;s cercanos. Un gr&amp;aacute;fico tan disperso es &amp;uacute;til en una variedad de circunstancias que hacen uso de relaciones espaciales entre puntos para el aprendizaje no supervisado: en particular, consulte &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="774649dea26c780c5e43d2464445b85ef137fcca" translate="yes" xml:space="preserve">
          <source>The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classification).</source>
          <target state="translated">El conjunto de datos es el conjunto de datos de Boston Housing (resp.20 grupos de noticias)para la regresión (resp.clasificación).</target>
        </trans-unit>
        <trans-unit id="c0f7ef482ab49522d3cd87d64587ff804f6727c1" translate="yes" xml:space="preserve">
          <source>The dataset used for evaluation is a 2D grid of isotropic Gaussian clusters widely spaced.</source>
          <target state="translated">El conjunto de datos utilizado para la evaluación es una cuadrícula 2D de cúmulos gausianos isotrópicos ampliamente espaciados.</target>
        </trans-unit>
        <trans-unit id="c07599d3a55d8254ba468cc109830533370f5dcf" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is Reuters-21578 as provided by the UCI ML repository. It will be automatically downloaded and uncompressed on first run.</source>
          <target state="translated">El conjunto de datos utilizado en este ejemplo es el Reuters-21578 proporcionado por el depósito de la UCI ML.Se descargará automáticamente y se descomprimirá en la primera ejecución.</target>
        </trans-unit>
        <trans-unit id="f610543312a82d7ead69951d61b82950d647ad3c" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, aka &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">El conjunto de datos que se utiliza en este ejemplo es un extracto preprocesado de las &quot;Caras etiquetadas en la naturaleza&quot;, tambi&amp;eacute;n conocido como &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="257296081e1c4b1a90a4ff7a918d81d5746f6705" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, also known as &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">El conjunto de datos utilizado en este ejemplo es un extracto preprocesado de &quot;Caras etiquetadas en la naturaleza&quot;, tambi&amp;eacute;n conocido como &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="0e491b7a83df0aa734f40ae016e056d9d5ef3d92" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.</source>
          <target state="translated">El conjunto de datos utilizado en este ejemplo es el conjunto de datos de los 20 grupos de noticias que se descargarán automáticamente y luego se almacenarán en caché y se reutilizarán para el ejemplo de clasificación de documentos.</target>
        </trans-unit>
        <trans-unit id="b391209f39032c4c37a7d267548501e64198b514" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.</source>
          <target state="translated">El conjunto de datos utilizado en este ejemplo es el conjunto de datos de los 20 grupos de noticias.Se descargará automáticamente,y luego se almacenará en caché.</target>
        </trans-unit>
        <trans-unit id="b6261e60f08853c79c52801fadd1554784335b15" translate="yes" xml:space="preserve">
          <source>The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).</source>
          <target state="translated">El conjunto de datos utilizado es el conjunto de datos del vino disponible en la UCI.Este conjunto de datos tiene características continuas que son heterogéneas en escala debido a las diferentes propiedades que miden (es decir,el contenido de alcohol y el ácido málico).</target>
        </trans-unit>
        <trans-unit id="2320c3b81471635dac46a3209d55417f5e2427c2" translate="yes" xml:space="preserve">
          <source>The dataset will be downloaded from the &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 homepage&lt;/a&gt; if necessary. The compressed size is about 656 MB.</source>
          <target state="translated">El conjunto de datos se descargar&amp;aacute; de la &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;p&amp;aacute;gina de inicio de rcv1&lt;/a&gt; si es necesario. El tama&amp;ntilde;o comprimido es de aproximadamente 656 MB.</target>
        </trans-unit>
        <trans-unit id="aadaaf4f9f002ac5fc13c03e41ad65b9ea025276" translate="yes" xml:space="preserve">
          <source>The dataset: wages</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a104c196a5a6b4961911da3fe75edb0c176425f0" translate="yes" xml:space="preserve">
          <source>The datasets also contain a full description in their &lt;code&gt;DESCR&lt;/code&gt; attribute and some contain &lt;code&gt;feature_names&lt;/code&gt; and &lt;code&gt;target_names&lt;/code&gt;. See the dataset descriptions below for details.</source>
          <target state="translated">Los conjuntos de datos tambi&amp;eacute;n contienen una descripci&amp;oacute;n completa en su atributo &lt;code&gt;DESCR&lt;/code&gt; y algunos contienen &lt;code&gt;feature_names&lt;/code&gt; y &lt;code&gt;target_names&lt;/code&gt; . Consulte las descripciones del conjunto de datos a continuaci&amp;oacute;n para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="266ce3ccfa4a57091d07a7fe8bfc007064ff062e" translate="yes" xml:space="preserve">
          <source>The decision function computed the final estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26c7542245412a79b0296d695d58227ed1491d25" translate="yes" xml:space="preserve">
          <source>The decision function is equal (up to a constant factor) to the log-posterior of the model, i.e. &lt;code&gt;log p(y = k | x)&lt;/code&gt;. In a binary classification setting this instead corresponds to the difference &lt;code&gt;log p(y = 1 | x) - log p(y = 0 | x)&lt;/code&gt;. See &lt;a href=&quot;../lda_qda#lda-qda-math&quot;&gt;Mathematical formulation of the LDA and QDA classifiers&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5da29d39d3005818dbe52be4adbd180f7105540" translate="yes" xml:space="preserve">
          <source>The decision function is:</source>
          <target state="translated">La función de decisión es:</target>
        </trans-unit>
        <trans-unit id="fdcfe75a710515596cfa4af6113e103288190672" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13c22c73db1fc45ace2498b233349bae422df959" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0e6f8aea5cc6c4291ed60eb8cccabc092233917" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The columns correspond to the classes in sorted order, as they appear in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">La funci&amp;oacute;n de decisi&amp;oacute;n de las muestras de entrada. Las columnas corresponden a las clases en orden ordenado, tal como aparecen en el atributo &lt;code&gt;classes_&lt;/code&gt; . La regresi&amp;oacute;n y la clasificaci&amp;oacute;n binaria son casos especiales con &lt;code&gt;k == 1&lt;/code&gt; , de lo contrario &lt;code&gt;k==n_classes&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ae769935874da2534c2ccb6ca3e5794a84e4d439" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06ac27333a17bb0e5a70b7bdd7a4a5eafb52284d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">La funci&amp;oacute;n de decisi&amp;oacute;n de las muestras de entrada. El orden de las salidas es el mismo que el del atributo &lt;code&gt;classes_&lt;/code&gt; . La clasificaci&amp;oacute;n binaria es un caso especial con &lt;code&gt;k == 1&lt;/code&gt; , de lo contrario &lt;code&gt;k==n_classes&lt;/code&gt; . Para la clasificaci&amp;oacute;n binaria, los valores m&amp;aacute;s cercanos a -1 o 1 significan m&amp;aacute;s como la primera o la segunda clase en &lt;code&gt;classes_&lt;/code&gt; , respectivamente.</target>
        </trans-unit>
        <trans-unit id="7a79704b70e2487a7278def4173b216c7abdf67d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="271144c81e33420b0281bbd1b4c29fbb1374c6ac" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">La funci&amp;oacute;n de decisi&amp;oacute;n de las muestras de entrada. El orden de las clases corresponde al del atributo &lt;code&gt;classes_&lt;/code&gt; . La regresi&amp;oacute;n y la clasificaci&amp;oacute;n binaria son casos especiales con &lt;code&gt;k == 1&lt;/code&gt; , de lo contrario &lt;code&gt;k==n_classes&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c04fb9b1c64a374fbc2424dc1dc2e69edec92fae" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">La funci&amp;oacute;n de decisi&amp;oacute;n de las muestras de entrada. El orden de las clases corresponde al del atributo &lt;code&gt;classes_&lt;/code&gt; . La regresi&amp;oacute;n y la clasificaci&amp;oacute;n binaria producen una matriz de formas [n_samples].</target>
        </trans-unit>
        <trans-unit id="7a4d1c2d06404b468f740213ed27426daa73066a" translate="yes" xml:space="preserve">
          <source>The decision rule for Bernoulli naive Bayes is based on</source>
          <target state="translated">La regla de decisión para el ingenuo Bayes de Bernoulli se basa en</target>
        </trans-unit>
        <trans-unit id="5c243c13540ddb2ab9ba5e07515f5ccc5bddcdc4" translate="yes" xml:space="preserve">
          <source>The decision tree estimator to be exported. It can be an instance of DecisionTreeClassifier or DecisionTreeRegressor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e0b3ca71cdfb4dae3b0e05518ff98bb98da9e44" translate="yes" xml:space="preserve">
          <source>The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve:</source>
          <target state="translated">La estructura del árbol de decisiones puede analizarse para obtener una mayor comprensión de la relación entre las características y el objetivo a predecir.En este ejemplo,mostramos cómo recuperar:</target>
        </trans-unit>
        <trans-unit id="7930dea6ad7960124a06f25452c69dc408a6af03" translate="yes" xml:space="preserve">
          <source>The decision tree to be exported to GraphViz.</source>
          <target state="translated">El árbol de decisiones que se exportará a GraphViz.</target>
        </trans-unit>
        <trans-unit id="096b758652a55c20db43d0b3794cde42f9ae935e" translate="yes" xml:space="preserve">
          <source>The decision tree to be plotted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41d54cbac8ff4bb674faa62fe3a273da70a2c8ec" translate="yes" xml:space="preserve">
          <source>The decision values for the samples are computed by adding the normalized sum of pair-wise classification confidence levels to the votes in order to disambiguate between the decision values when the votes for all the classes are equal leading to a tie.</source>
          <target state="translated">Los valores de decisión de las muestras se calculan sumando la suma normalizada de los niveles de confianza de las clasificaciones por pares a los votos para desambiguar entre los valores de decisión cuando los votos de todas las clases son iguales,lo que conduce a un empate.</target>
        </trans-unit>
        <trans-unit id="9db4a0cde49703606f721a9d732403d53161167d" translate="yes" xml:space="preserve">
          <source>The decoding strategy depends on the vectorizer parameters.</source>
          <target state="translated">La estrategia de decodificación depende de los parámetros del vectorizador.</target>
        </trans-unit>
        <trans-unit id="8ab5c7033c87e7cea9882c536950d8e60cf30550" translate="yes" xml:space="preserve">
          <source>The default coding of images is based on the &lt;code&gt;uint8&lt;/code&gt; dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; don&amp;rsquo;t forget to scale to the range 0 - 1 as done in the following example.</source>
          <target state="translated">La codificaci&amp;oacute;n predeterminada de im&amp;aacute;genes se basa en el tipo &lt;code&gt;uint8&lt;/code&gt; dtype para ahorrar memoria. A menudo, los algoritmos de aprendizaje autom&amp;aacute;tico funcionan mejor si la entrada se convierte primero en una representaci&amp;oacute;n de punto flotante. Adem&amp;aacute;s, si planea usar &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; , no olvide escalar al rango 0 - 1 como se hace en el siguiente ejemplo.</target>
        </trans-unit>
        <trans-unit id="ec537a685ef4f8a1d5936e658cd9e5b93a584d77" translate="yes" xml:space="preserve">
          <source>The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:</source>
          <target state="translated">La configuración por defecto muestra la cadena extrayendo palabras de al menos 2 letras.La función específica que realiza este paso puede ser solicitada explícitamente:</target>
        </trans-unit>
        <trans-unit id="3d025c574ea3cc62ecf87d5b0a9f83d7b3c75d6a" translate="yes" xml:space="preserve">
          <source>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module for the list of possible cross-validation objects.</source>
          <target state="translated">El generador de validaci&amp;oacute;n cruzada predeterminado utilizado es K-Folds estratificado. Si se proporciona un n&amp;uacute;mero entero, entonces es el n&amp;uacute;mero de pliegues utilizados. Consulte el m&amp;oacute;dulo &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt; m&amp;oacute;dulo para obtener la lista de posibles objetos de validaci&amp;oacute;n cruzada.</target>
        </trans-unit>
        <trans-unit id="24e256639376dfcfda93b74d1117680d7d248e59" translate="yes" xml:space="preserve">
          <source>The default dataset is the 20 newsgroups dataset. To run the example on the digits dataset, pass the &lt;code&gt;--use-digits-dataset&lt;/code&gt; command line argument to this script.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53050da8c5d49a141b5e2d80a70c0fa67fd7eb00" translate="yes" xml:space="preserve">
          <source>The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the &amp;ndash;twenty-newsgroups command line argument to this script.</source>
          <target state="translated">El conjunto de datos predeterminado es el conjunto de datos de d&amp;iacute;gitos. Para ejecutar el ejemplo en el conjunto de datos de veinte grupos de noticias, pase el argumento de la l&amp;iacute;nea de comandos &amp;ndash;twenty-newsgroups a este script.</target>
        </trans-unit>
        <trans-unit id="8018aa0e057c3b81a78860eae3f5892dde5e1c45" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this estimator.&amp;rdquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="713044bd0fefe6c8ff38e7a00814412176e5caf0" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this method.&amp;rdquo;</source>
          <target state="translated">El mensaje de error predeterminado es: &amp;ldquo;Esta instancia de% (name) s a&amp;uacute;n no se ha instalado. Llame a 'fit' con los argumentos apropiados antes de usar este m&amp;eacute;todo &quot;.</target>
        </trans-unit>
        <trans-unit id="bd1a08c69ea4f3f536ea9f567759a2eca8958b5b" translate="yes" xml:space="preserve">
          <source>The default parameters (n_samples / n_features / n_components) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).</source>
          <target state="translated">Los parámetros por defecto (n_muestras/n_características/n_componentes)deberían hacer el ejemplo ejecutable en un par de decenas de segundos.Puedes intentar aumentar las dimensiones del problema,pero ten en cuenta que la complejidad temporal es polinómica en el NMF.En LDA,la complejidad temporal es proporcional a (n_muestras*iteraciones).</target>
        </trans-unit>
        <trans-unit id="6bb3234d6819185e21b8b8e0778371ddb1ae1e23" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net &lt;a href=&quot;#id15&quot; id=&quot;id1&quot;&gt;11&lt;/a&gt; solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e964bf4dcf7d784d0dee842886fe27c5060b1af1" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">La configuraci&amp;oacute;n predeterminada es &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; . La penalizaci&amp;oacute;n L1 conduce a soluciones escasas, llevando la mayor&amp;iacute;a de los coeficientes a cero. Elastic Net resuelve algunas deficiencias de la penalizaci&amp;oacute;n L1 en presencia de atributos altamente correlacionados. El par&amp;aacute;metro &lt;code&gt;l1_ratio&lt;/code&gt; controla la combinaci&amp;oacute;n convexa de penalizaci&amp;oacute;n L1 y L2.</target>
        </trans-unit>
        <trans-unit id="d600d29df52c52da3fc6f38fdcf73036ecf8cbdf" translate="yes" xml:space="preserve">
          <source>The default slice is a rectangular shape around the face, removing most of the background:</source>
          <target state="translated">El corte predeterminado es una forma rectangular alrededor de la cara,eliminando la mayor parte del fondo:</target>
        </trans-unit>
        <trans-unit id="f2f8fa0d6181b12438bd7660924ca4c2f89302ad" translate="yes" xml:space="preserve">
          <source>The default solver is &amp;lsquo;svd&amp;rsquo;. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the &amp;lsquo;svd&amp;rsquo; solver cannot be used with shrinkage.</source>
          <target state="translated">El solucionador predeterminado es 'svd'. Puede realizar tanto clasificaci&amp;oacute;n como transformaci&amp;oacute;n, y no depende del c&amp;aacute;lculo de la matriz de covarianza. Esto puede ser una ventaja en situaciones en las que la cantidad de funciones es grande. Sin embargo, el solucionador 'svd' no se puede utilizar con contracci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="2307e8c1575f885543e9c12c3e37adbd9d66c3dd" translate="yes" xml:space="preserve">
          <source>The default strategy implements one step of the bootstrapping procedure.</source>
          <target state="translated">La estrategia por defecto implementa un paso del procedimiento de bootstrapping.</target>
        </trans-unit>
        <trans-unit id="f17725906dcf13fd5a8abccea776a77c1270d7a9" translate="yes" xml:space="preserve">
          <source>The default value &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; uses &lt;code&gt;n_features&lt;/code&gt; rather than &lt;code&gt;n_features / 3&lt;/code&gt;. The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].</source>
          <target state="translated">El valor predeterminado &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; usa &lt;code&gt;n_features&lt;/code&gt; en lugar de &lt;code&gt;n_features / 3&lt;/code&gt; . Este &amp;uacute;ltimo se sugiri&amp;oacute; originalmente en [1], mientras que el primero se justific&amp;oacute; emp&amp;iacute;ricamente m&amp;aacute;s recientemente en [2].</target>
        </trans-unit>
        <trans-unit id="71b5a78b2592e847cac7e9a1c29294e2be25d4eb" translate="yes" xml:space="preserve">
          <source>The default value of &lt;code&gt;copy&lt;/code&gt; changed from False to True in 0.23.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ad5c264779356671425d0f732ca8de7347758f3" translate="yes" xml:space="preserve">
          <source>The default values for the parameters controlling the size of the trees (e.g. &lt;code&gt;max_depth&lt;/code&gt;, &lt;code&gt;min_samples_leaf&lt;/code&gt;, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.</source>
          <target state="translated">Los valores predeterminados para los par&amp;aacute;metros que controlan el tama&amp;ntilde;o de los &amp;aacute;rboles (por ejemplo , &lt;code&gt;max_depth&lt;/code&gt; , &lt;code&gt;min_samples_leaf&lt;/code&gt; , etc.) conducen a &amp;aacute;rboles completamente desarrollados y sin podar que pueden ser potencialmente muy grandes en algunos conjuntos de datos. Para reducir el consumo de memoria, la complejidad y el tama&amp;ntilde;o de los &amp;aacute;rboles deben controlarse estableciendo esos valores de par&amp;aacute;metros.</target>
        </trans-unit>
        <trans-unit id="4209f696edb2b5615bc39c07cc72cc1b347ebd54" translate="yes" xml:space="preserve">
          <source>The definitive description of key concepts and API elements for using scikit-learn and developing compatible tools.</source>
          <target state="translated">La descripción definitiva de los conceptos clave y los elementos de la API para el uso de scikit-learn y el desarrollo de herramientas compatibles.</target>
        </trans-unit>
        <trans-unit id="767ee5df767908c4f8729c932c4a3d808fe558cd" translate="yes" xml:space="preserve">
          <source>The degree of the polynomial features. Default = 2.</source>
          <target state="translated">El grado de los rasgos polinómicos.Por defecto=2.</target>
        </trans-unit>
        <trans-unit id="c0ae4038f8b612638e22a63002b9a0592ebf4ed6" translate="yes" xml:space="preserve">
          <source>The density of w, between 0 and 1</source>
          <target state="translated">La densidad de w,entre 0 y 1</target>
        </trans-unit>
        <trans-unit id="1385416aaed19b5930c017ff407294aed6e786b1" translate="yes" xml:space="preserve">
          <source>The depth of a tree is the maximum distance between the root and any leaf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64211b28a272c8cc95fc17412f0b29158a9e57aa" translate="yes" xml:space="preserve">
          <source>The desired absolute tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 0.</source>
          <target state="translated">La deseada tolerancia absoluta del resultado.Una mayor tolerancia generalmente conducirá a una ejecución más rápida.El valor por defecto es 0.</target>
        </trans-unit>
        <trans-unit id="d976f3ec62dae27876361a8dc74b8b1f3089859a" translate="yes" xml:space="preserve">
          <source>The desired relative tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 1E-8.</source>
          <target state="translated">La tolerancia relativa deseada del resultado.Una mayor tolerancia generalmente conducirá a una ejecución más rápida.El valor por defecto es 1E-8.</target>
        </trans-unit>
        <trans-unit id="e06750706d15ac4ccfc7d440948abe624b2a5a8e" translate="yes" xml:space="preserve">
          <source>The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:</source>
          <target state="translated">El conjunto de datos de la diabetes consiste en 10 variables fisiológicas (edad,sexo,peso,presión sanguínea)medidas en 442 pacientes,y una indicación de la progresión de la enfermedad después de un año:</target>
        </trans-unit>
        <trans-unit id="9fc4977037d4a4b0eddabbe726bd2d0de7d06481" translate="yes" xml:space="preserve">
          <source>The dict at &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; gives the parameter setting for the best model, that gives the highest mean score (&lt;code&gt;search.best_score_&lt;/code&gt;).</source>
          <target state="translated">El dict en &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; proporciona la configuraci&amp;oacute;n de par&amp;aacute;metros para el mejor modelo, que da la puntuaci&amp;oacute;n media m&amp;aacute;s alta ( &lt;code&gt;search.best_score_&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="bfaa3d320392a9f6c8addb9130a129b654790105" translate="yes" xml:space="preserve">
          <source>The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.</source>
          <target state="translated">Los átomos del diccionario utilizados para la codificación dispersa.Se supone que las líneas están normalizadas a la norma de la unidad.</target>
        </trans-unit>
        <trans-unit id="7373631cc038017694fe3a2fdfb8d4bfc8e42605" translate="yes" xml:space="preserve">
          <source>The dictionary factor in the matrix factorization.</source>
          <target state="translated">El factor del diccionario en la factorización de la matriz.</target>
        </trans-unit>
        <trans-unit id="09a0fc68f904b631d6aa1871e7b81e42ac764443" translate="yes" xml:space="preserve">
          <source>The dictionary is fitted on the distorted left half of the image, and subsequently used to reconstruct the right half. Note that even better performance could be achieved by fitting to an undistorted (i.e. noiseless) image, but here we start from the assumption that it is not available.</source>
          <target state="translated">El diccionario se ajusta en la mitad izquierda distorsionada de la imagen,y posteriormente se utiliza para reconstruir la mitad derecha.Obsérvese que se podría lograr un rendimiento aún mejor ajustándolo a una imagen no distorsionada (es decir,sin ruido),pero aquí partimos del supuesto de que no está disponible.</target>
        </trans-unit>
        <trans-unit id="03531d72d72ed916646e794ac15c0622fe9c9b59" translate="yes" xml:space="preserve">
          <source>The dictionary learning objects offer, via the &lt;code&gt;split_code&lt;/code&gt; parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.</source>
          <target state="translated">Los objetos de aprendizaje del diccionario ofrecen, a trav&amp;eacute;s del par&amp;aacute;metro &lt;code&gt;split_code&lt;/code&gt; , la posibilidad de separar los valores positivos y negativos en los resultados de la codificaci&amp;oacute;n escasa. Esto es &amp;uacute;til cuando el aprendizaje de diccionario se usa para extraer caracter&amp;iacute;sticas que se usar&amp;aacute;n para el aprendizaje supervisado, porque permite que el algoritmo de aprendizaje asigne diferentes pesos a las cargas negativas de un &amp;aacute;tomo en particular, desde a la correspondiente carga positiva.</target>
        </trans-unit>
        <trans-unit id="c5012873b1ea68e5deed2b04cdb4e90687b2c340" translate="yes" xml:space="preserve">
          <source>The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.</source>
          <target state="translated">La matriz del diccionario con la que resolver la escasa codificación de los datos.Algunos de los algoritmos asumen filas normalizadas para una salida significativa.</target>
        </trans-unit>
        <trans-unit id="f137f3a53fdf8d92f7463e89f6383b5a88f3c320" translate="yes" xml:space="preserve">
          <source>The dictionary with normalized components (D).</source>
          <target state="translated">El diccionario con componentes normalizados (D).</target>
        </trans-unit>
        <trans-unit id="d49d4bb0bf7407d291ade3beb50a3cbdf55548f8" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size &lt;code&gt;p&lt;/code&gt; unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups.</source>
          <target state="translated">La diferencia entre LeavePGroupsOut y GroupShuffleSplit es que el primero genera divisiones usando todos los subconjuntos de grupos &amp;uacute;nicos de tama&amp;ntilde;o &lt;code&gt;p&lt;/code&gt; , mientras que GroupShuffleSplit genera un n&amp;uacute;mero determinado por el usuario de divisiones de prueba aleatorias, cada una con una fracci&amp;oacute;n determinada por el usuario de grupos &amp;uacute;nicos.</target>
        </trans-unit>
        <trans-unit id="f8e596d2cfcc0c0ae56904e18bdffdb0d463a655" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with all the samples assigned to &lt;code&gt;p&lt;/code&gt; different values of the groups while the latter uses samples all assigned the same groups.</source>
          <target state="translated">La diferencia entre LeavePGroupsOut y LeaveOneGroupOut es que el primero construye los conjuntos de prueba con todas las muestras asignadas a &lt;code&gt;p&lt;/code&gt; valores diferentes de los grupos, mientras que el segundo usa muestras todas asignadas a los mismos grupos.</target>
        </trans-unit>
        <trans-unit id="123fc66dcb39304255b8e25c6c10c654dbb2b0f3" translate="yes" xml:space="preserve">
          <source>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \(P(x_i \mid y)\).</source>
          <target state="translated">Los diferentes clasificadores ingenuos de Bayes se diferencian principalmente por las suposiciones que hacen con respecto a la distribución de \ ~ (P(x_i \mid y)\N).</target>
        </trans-unit>
        <trans-unit id="fc85907a08e94b0f17af8c0c01aad9ef06729185" translate="yes" xml:space="preserve">
          <source>The digits dataset is made of 1797 8x8 images of hand-written digits</source>
          <target state="translated">El conjunto de datos de los dígitos está hecho de 1797 8x8 imágenes de dígitos escritos a mano</target>
        </trans-unit>
        <trans-unit id="46ad3d3589f97f144056aa3785c730497272c03e" translate="yes" xml:space="preserve">
          <source>The dimension of the projected subspace.</source>
          <target state="translated">La dimensión del subespacio proyectado.</target>
        </trans-unit>
        <trans-unit id="5572a493038acd1179abd12e1cd0c48e6709dea1" translate="yes" xml:space="preserve">
          <source>The dimension of the projection subspace.</source>
          <target state="translated">La dimensión de la proyección subespacial.</target>
        </trans-unit>
        <trans-unit id="984d56bddca949d9f1aa278aca87eb46a4072ffd" translate="yes" xml:space="preserve">
          <source>The dimensionality of the resulting representation is &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt;. If &lt;code&gt;max_leaf_nodes == None&lt;/code&gt;, the number of leaf nodes is at most &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt;.</source>
          <target state="translated">La dimensionalidad de la representaci&amp;oacute;n resultante es &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt; . Si &lt;code&gt;max_leaf_nodes == None&lt;/code&gt; , el n&amp;uacute;mero de nodos hoja es como m&amp;aacute;ximo &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="068779d9c792d7cec6633fa1a442091cb7f6f6f3" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.</source>
          <target state="translated">Las dimensiones y la distribución de las matrices de Proyecciones Aleatorias se controlan para preservar las distancias en pares entre dos muestras cualesquiera del conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="1fc3554ec8e7cb3510bb9bcb462301a368807c4a" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.</source>
          <target state="translated">Las dimensiones y la distribución de las matrices de proyecciones aleatorias se controlan para preservar las distancias en pares entre dos muestras cualesquiera del conjunto de datos.Así pues,la proyección aleatoria es una técnica de aproximación adecuada para el método basado en la distancia.</target>
        </trans-unit>
        <trans-unit id="42010c6a5240a459a14ffe4007a4a9d645a82c6f" translate="yes" xml:space="preserve">
          <source>The dimensions of one patch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16b41d4b4452bd551af457fee2f8b60407215654" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet).</source>
          <target state="translated">La concentración de dirichlet de cada componente en la distribución del peso (Dirichlet).</target>
        </trans-unit>
        <trans-unit id="6b13240ddd6531c8fdb797758cabfbb4633f5e80" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;:</source>
          <target state="translated">La concentraci&amp;oacute;n de dirichlet de cada componente en la distribuci&amp;oacute;n de peso (Dirichlet). El tipo depende de &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="b470809fd29296951902d887ca553f032b8a40c4" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to &lt;code&gt;1. / n_components&lt;/code&gt;.</source>
          <target state="translated">La concentraci&amp;oacute;n de dirichlet de cada componente en la distribuci&amp;oacute;n de peso (Dirichlet). Esto se denomina com&amp;uacute;nmente gamma en la literatura. La concentraci&amp;oacute;n m&amp;aacute;s alta pone m&amp;aacute;s masa en el centro y conducir&amp;aacute; a que m&amp;aacute;s componentes est&amp;eacute;n activos, mientras que un par&amp;aacute;metro de concentraci&amp;oacute;n m&amp;aacute;s bajo conducir&amp;aacute; a m&amp;aacute;s masa en el borde de la mezcla pesa simplex. El valor del par&amp;aacute;metro debe ser mayor que 0. Si es Ninguno, se establece en &lt;code&gt;1. / n_components&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cbf40d766d38685ebd59a9222ccafd32de07a34f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Bayesian regression include:</source>
          <target state="translated">Las desventajas de la regresión Bayesiana incluyen:</target>
        </trans-unit>
        <trans-unit id="ec84046fbad625e7f2f2f6744eab5866f4d72369" translate="yes" xml:space="preserve">
          <source>The disadvantages of GBRT are:</source>
          <target state="translated">Las desventajas de la TRBG son:</target>
        </trans-unit>
        <trans-unit id="cfaf1a3af8c2483843ec10c36faa80968b886bf2" translate="yes" xml:space="preserve">
          <source>The disadvantages of Gaussian processes include:</source>
          <target state="translated">Las desventajas de los procesos gausianos incluyen:</target>
        </trans-unit>
        <trans-unit id="c8e0c1974308fc5ae09e195effcd7396a8db8601" translate="yes" xml:space="preserve">
          <source>The disadvantages of Multi-layer Perceptron (MLP) include:</source>
          <target state="translated">Las desventajas del Perceptrón Multicapa (MLP)incluyen:</target>
        </trans-unit>
        <trans-unit id="c71179ab39a085455ac6f6888a2f6fd3afe84d6f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Stochastic Gradient Descent include:</source>
          <target state="translated">Las desventajas del descenso de gradiente estocástico incluyen:</target>
        </trans-unit>
        <trans-unit id="84c28c658fb7eb00eb9347365efe97c2f04c30f3" translate="yes" xml:space="preserve">
          <source>The disadvantages of decision trees include:</source>
          <target state="translated">Las desventajas de los árboles de decisión incluyen:</target>
        </trans-unit>
        <trans-unit id="47feb0b4f4476b90d13a7091a56a1b0f68ad3a62" translate="yes" xml:space="preserve">
          <source>The disadvantages of support vector machines include:</source>
          <target state="translated">Las desventajas de las máquinas de vectores de apoyo incluyen:</target>
        </trans-unit>
        <trans-unit id="266f0e7f0f21de8e86a9f2ce2320de66f17358df" translate="yes" xml:space="preserve">
          <source>The disadvantages of the LARS method include:</source>
          <target state="translated">Las desventajas del método LARS incluyen:</target>
        </trans-unit>
        <trans-unit id="fe9e13a1ad2c431f3f290607f881625ead9408c0" translate="yes" xml:space="preserve">
          <source>The disadvantages to using t-SNE are roughly:</source>
          <target state="translated">Las desventajas de usar el t-SNE son más o menos:</target>
        </trans-unit>
        <trans-unit id="cd2f4d25623f857cb298f31868b60b26f2d776d4" translate="yes" xml:space="preserve">
          <source>The display objects store the computed values that were passed as arguments. This allows for the visualizations to be easliy combined using matplotlib&amp;rsquo;s API. In the following example, we place the displays next to each other in a row.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7ea673185d2a40501e91d63f573c417ac055cd1" translate="yes" xml:space="preserve">
          <source>The distance metric to use</source>
          <target state="translated">La métrica de distancia a utilizar</target>
        </trans-unit>
        <trans-unit id="fade8506c426ca456f7b5f3ebd671dcd06e51421" translate="yes" xml:space="preserve">
          <source>The distance metric to use. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0063e1a06973d2332dc280d77426f57f299b9005" translate="yes" xml:space="preserve">
          <source>The distance metric to use. Note that not all metrics are valid with all algorithms. Refer to the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; for a description of available algorithms. Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is &amp;lsquo;euclidean&amp;rsquo;.</source>
          <target state="translated">La m&amp;eacute;trica de distancia que se utilizar&amp;aacute;. Tenga en cuenta que no todas las m&amp;eacute;tricas son v&amp;aacute;lidas con todos los algoritmos. Consulte la documentaci&amp;oacute;n de &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; para obtener una descripci&amp;oacute;n de los algoritmos disponibles. Tenga en cuenta que la normalizaci&amp;oacute;n de la salida de densidad es correcta solo para la m&amp;eacute;trica de distancia euclidiana. El valor predeterminado es 'euclidiano'.</target>
        </trans-unit>
        <trans-unit id="3c4790f8b52fd2ea34a9c9dfcea72e27e7f13da0" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the p param equal to 2.)</source>
          <target state="translated">La m&amp;eacute;trica de distancia utilizada para calcular los k-Vecinos para cada punto de muestra. La clase DistanceMetric proporciona una lista de m&amp;eacute;tricas disponibles. La distancia predeterminada es 'euclidiana' (m&amp;eacute;trica 'minkowski' con el par&amp;aacute;metro p igual a 2.)</target>
        </trans-unit>
        <trans-unit id="f687007319ee4f163c7b6668932fdc048712a932" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the param equal to 2.)</source>
          <target state="translated">La m&amp;eacute;trica de distancia utilizada para calcular los vecinos dentro de un radio determinado para cada punto de muestra. La clase DistanceMetric proporciona una lista de m&amp;eacute;tricas disponibles. La distancia predeterminada es 'euclidiana' (m&amp;eacute;trica 'minkowski' con el par&amp;aacute;metro igual a 2.)</target>
        </trans-unit>
        <trans-unit id="8c96fb568d2fd154ab3cf2ef1e009cb92f123321" translate="yes" xml:space="preserve">
          <source>The distance metric used. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a36fd43a3aae80628a6e6b30a7a5575b2bb66390" translate="yes" xml:space="preserve">
          <source>The distances between the row vectors of &lt;code&gt;X&lt;/code&gt; and the row vectors of &lt;code&gt;Y&lt;/code&gt; can be evaluated using &lt;a href=&quot;generated/sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;pairwise_distances&lt;/code&gt;&lt;/a&gt;. If &lt;code&gt;Y&lt;/code&gt; is omitted the pairwise distances of the row vectors of &lt;code&gt;X&lt;/code&gt; are calculated. Similarly, &lt;a href=&quot;generated/sklearn.metrics.pairwise.pairwise_kernels#sklearn.metrics.pairwise.pairwise_kernels&quot;&gt;&lt;code&gt;pairwise.pairwise_kernels&lt;/code&gt;&lt;/a&gt; can be used to calculate the kernel between &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; using different kernel functions. See the API reference for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e12a027c556a152948ba7c771e2cb65d4ac3e99" translate="yes" xml:space="preserve">
          <source>The distinct labels used in classifying instances.</source>
          <target state="translated">Las distintas etiquetas utilizadas para clasificar las instancias.</target>
        </trans-unit>
        <trans-unit id="2333c2cc37e50157e20c383f74b00f781cb37a1b" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; is asserted by the fact that &lt;code&gt;p&lt;/code&gt; is defining an eps-embedding with good probability as defined by:</source>
          <target state="translated">La distorsi&amp;oacute;n introducida por una proyecci&amp;oacute;n aleatoria &lt;code&gt;p&lt;/code&gt; se afirma por el hecho de que &lt;code&gt;p&lt;/code&gt; est&amp;aacute; definiendo una incrustaci&amp;oacute;n eps con buena probabilidad definida por:</target>
        </trans-unit>
        <trans-unit id="befef2d542b10663820383e2f1e59843332d14a0" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; only changes the distance between two points by a factor (1 +- eps) in an euclidean space with good probability. The projection &lt;code&gt;p&lt;/code&gt; is an eps-embedding as defined by:</source>
          <target state="translated">La distorsi&amp;oacute;n introducida por una proyecci&amp;oacute;n aleatoria &lt;code&gt;p&lt;/code&gt; solo cambia la distancia entre dos puntos por un factor (1 + - eps) en un espacio euclidiano con buena probabilidad. La proyecci&amp;oacute;n &lt;code&gt;p&lt;/code&gt; es una incrustaci&amp;oacute;n eps definida por:</target>
        </trans-unit>
        <trans-unit id="b6d88e183a439e17415e4b06c82155c9e34fa344" translate="yes" xml:space="preserve">
          <source>The distributions in &lt;code&gt;scipy.stats&lt;/code&gt; prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via &lt;code&gt;np.random.seed&lt;/code&gt; or set using &lt;code&gt;np.random.set_state&lt;/code&gt;. However, beginning scikit-learn 0.18, the &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module sets the random state provided by the user if scipy &amp;gt;= 0.16 is also available.</source>
          <target state="translated">Las distribuciones en &lt;code&gt;scipy.stats&lt;/code&gt; anteriores a la versi&amp;oacute;n scipy 0.16 no permiten especificar un estado aleatorio. En su lugar, usan el estado aleatorio num&amp;eacute;rico global, que se puede sembrar a trav&amp;eacute;s de &lt;code&gt;np.random.seed&lt;/code&gt; o establecer usando &lt;code&gt;np.random.set_state&lt;/code&gt; . Sin embargo, a partir de scikit-learn 0.18, el m&amp;oacute;dulo &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt; establece el estado aleatorio proporcionado por el usuario si scipy&amp;gt; = 0.16 tambi&amp;eacute;n est&amp;aacute; disponible.</target>
        </trans-unit>
        <trans-unit id="e4d3a4f449c4df99f288fa9346370fef4e50edc3" translate="yes" xml:space="preserve">
          <source>The dual gap at the end of the optimization for the optimal alpha (&lt;code&gt;alpha_&lt;/code&gt;).</source>
          <target state="translated">La brecha dual al final de la optimizaci&amp;oacute;n para el alfa &amp;oacute;ptimo ( &lt;code&gt;alpha_&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="1c5b7990e4f078095970dbd44954a4ff0f6bdb22" translate="yes" xml:space="preserve">
          <source>The dual gaps at the end of the optimization for each alpha.</source>
          <target state="translated">Los dos huecos al final de la optimización para cada alfa.</target>
        </trans-unit>
        <trans-unit id="eceb770d3ab0f05ce2328559f3653f372ef1e2c9" translate="yes" xml:space="preserve">
          <source>The dual problem is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa2b6f5781584ece88d8eb541efa4219ed937f23" translate="yes" xml:space="preserve">
          <source>The dual problem to the primal is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c16a68f2d6169f4f9aa27b4b94f9b3db38d73c1f" translate="yes" xml:space="preserve">
          <source>The dummy regression model predicts a constant frequency. This model does not attribute the same tied rank to all samples but is none-the-less globally well calibrated (to estimate the mean frequency of the entire population).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5eac57a33eeb0d09a6d5ebfcbe611bef2da07d8" translate="yes" xml:space="preserve">
          <source>The edges of each bin. Contain arrays of varying shapes &lt;code&gt;(n_bins_, )&lt;/code&gt; Ignored features will have empty arrays.</source>
          <target state="translated">Los bordes de cada contenedor. Contener matrices de diferentes formas &lt;code&gt;(n_bins_, )&lt;/code&gt; caracter&amp;iacute;sticas ignoradas tendr&amp;aacute;n matrices vac&amp;iacute;as.</target>
        </trans-unit>
        <trans-unit id="50c07dc6eab301b9d9953e2bd5849c6aa3abe8d7" translate="yes" xml:space="preserve">
          <source>The effect of the transformer is weaker than on the synthetic data. However, the transform induces a decrease of the MAE.</source>
          <target state="translated">El efecto del transformador es más débil que en los datos sintéticos.Sin embargo,la transformación induce una disminución del MAE.</target>
        </trans-unit>
        <trans-unit id="8b7256d0b08ed885751aee08f85ac36bb70ae336" translate="yes" xml:space="preserve">
          <source>The effective size of the batch is computed here. If there are no more jobs to dispatch, return False, else return True.</source>
          <target state="translated">El tamaño efectivo del lote se calcula aquí.Si no hay más trabajos que despachar,devuelva Falso,si no devuelva Verdadero.</target>
        </trans-unit>
        <trans-unit id="a044cf1265a1684d79e7b48898279d20ad6f0dac" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities</source>
          <target state="translated">La estrategia de descomposición del valor propio a utilizar.AMG requiere que se instale pyamg.Puede ser más rápido en problemas muy grandes y escasos,pero también puede llevar a inestabilidades</target>
        </trans-unit>
        <trans-unit id="1dc685b8ff4dd3ccaa26f5f653c44ef0324bd82f" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities.</source>
          <target state="translated">La estrategia de descomposición del valor propio a utilizar.AMG requiere que se instale pyamg.Puede ser más rápido en problemas muy grandes y escasos,pero también puede llevar a inestabilidades.</target>
        </trans-unit>
        <trans-unit id="e9d800f8ff5b9787b828397b459d27c3b21cb754" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f14ed9796539634c3492800089020cfe2ebbd7af" translate="yes" xml:space="preserve">
          <source>The elastic net optimization function varies for mono and multi-outputs.</source>
          <target state="translated">La función de optimización de la red elástica varía para las salidas mono y multiples.</target>
        </trans-unit>
        <trans-unit id="4d9a50594cec642f130c977c2c5c095fc619b302" translate="yes" xml:space="preserve">
          <source>The elements of the estimators parameter, having been fitted on the training data. If an estimator has been set to &lt;code&gt;'drop'&lt;/code&gt;, it will not appear in &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0191353bc3bca50b627784afbd555eb11c1f3dd" translate="yes" xml:space="preserve">
          <source>The empirical covariance matrix of a sample can be computed using the &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt;&lt;code&gt;empirical_covariance&lt;/code&gt;&lt;/a&gt; function of the package, or by fitting an &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; object to the data sample with the &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt;&lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Be careful that results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately. More precisely, if &lt;code&gt;assume_centered=False&lt;/code&gt;, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and &lt;code&gt;assume_centered=True&lt;/code&gt; should be used.</source>
          <target state="translated">La matriz de covarianza emp&amp;iacute;rica de una muestra se puede calcular utilizando la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt; &lt;code&gt;empirical_covariance&lt;/code&gt; &lt;/a&gt; del paquete, o ajustando un objeto &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; &lt;/a&gt; a la muestra de datos con el m&amp;eacute;todo &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt; &lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt; &lt;/a&gt; . Tenga cuidado de que los resultados dependan de si los datos est&amp;aacute;n centrados, por lo que es posible que desee utilizar el par&amp;aacute;metro &lt;code&gt;assume_centered&lt;/code&gt; precisi&amp;oacute;n. M&amp;aacute;s precisamente, si &lt;code&gt;assume_centered=False&lt;/code&gt; , se supone que el conjunto de prueba tiene el mismo vector medio que el conjunto de entrenamiento. De lo contrario, ambos deben estar centrados por el usuario y se debe utilizar &lt;code&gt;assume_centered=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d4b0da98d5cb047aa932ef62858edca40e188517" translate="yes" xml:space="preserve">
          <source>The encoded signal (Y).</source>
          <target state="translated">La señal codificada (Y).</target>
        </trans-unit>
        <trans-unit id="cc2fdf3023f61dc3df7bba02538cce691e7cf2cd" translate="yes" xml:space="preserve">
          <source>The energy function measures the quality of a joint assignment:</source>
          <target state="translated">La función de energía mide la calidad de una asignación conjunta:</target>
        </trans-unit>
        <trans-unit id="1bfb5cae50ef0b1032c729ea4ab68ff502e7f906" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;test_fold[i]&lt;/code&gt; represents the index of the test set that sample &lt;code&gt;i&lt;/code&gt; belongs to. It is possible to exclude sample &lt;code&gt;i&lt;/code&gt; from any test set (i.e. include sample &lt;code&gt;i&lt;/code&gt; in every training set) by setting &lt;code&gt;test_fold[i]&lt;/code&gt; equal to -1.</source>
          <target state="translated">La entrada &lt;code&gt;test_fold[i]&lt;/code&gt; representa el &amp;iacute;ndice del conjunto de prueba al que pertenece la muestra &lt;code&gt;i&lt;/code&gt; . Es posible excluir la muestra &lt;code&gt;i&lt;/code&gt; de cualquier conjunto de prueba (es decir, incluir la muestra &lt;code&gt;i&lt;/code&gt; en cada conjunto de entrenamiento) estableciendo &lt;code&gt;test_fold[i]&lt;/code&gt; igual a -1.</target>
        </trans-unit>
        <trans-unit id="503425d1f7e07b98f32316b1d85343640a4e1294" translate="yes" xml:space="preserve">
          <source>The equivalence between &lt;code&gt;alpha&lt;/code&gt; and the regularization parameter of SVM, &lt;code&gt;C&lt;/code&gt; is given by &lt;code&gt;alpha = 1 / C&lt;/code&gt; or &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt;, depending on the estimator and the exact objective function optimized by the model.</source>
          <target state="translated">La equivalencia entre &lt;code&gt;alpha&lt;/code&gt; y el par&amp;aacute;metro de regularizaci&amp;oacute;n de SVM, &lt;code&gt;C&lt;/code&gt; viene dada por &lt;code&gt;alpha = 1 / C&lt;/code&gt; o &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt; , dependiendo del estimador y la funci&amp;oacute;n objetiva exacta optimizada por el modelo.</target>
        </trans-unit>
        <trans-unit id="2fec157ea85a1f513b11852f437f8dc13c9361bb" translate="yes" xml:space="preserve">
          <source>The error message or a substring of the error message.</source>
          <target state="translated">El mensaje de error o una subcadena del mensaje de error.</target>
        </trans-unit>
        <trans-unit id="67a9569df158acc253e75d9fa812424d1f2b58c6" translate="yes" xml:space="preserve">
          <source>The estimated (sparse) precision matrix.</source>
          <target state="translated">La matriz de precisión estimada (escasa).</target>
        </trans-unit>
        <trans-unit id="cdf0d65945f589fd5da3781e66a0a81a30e934b1" translate="yes" xml:space="preserve">
          <source>The estimated covariance matrix.</source>
          <target state="translated">La matriz de covarianza estimada.</target>
        </trans-unit>
        <trans-unit id="4b02a2f0d176ac6fb9b484de9fcc2a6b0be67a5a" translate="yes" xml:space="preserve">
          <source>The estimated labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77a18d9bfc3d1fc8448c4fb559b59c860611bbc7" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;.</source>
          <target state="translated">La covarianza de ruido estimada siguiendo el modelo probabil&amp;iacute;stico PCA de Tipping y Bishop 1999. Ver &amp;ldquo;Reconocimiento de patrones y aprendizaje autom&amp;aacute;tico&amp;rdquo; de C. Bishop, 12.2.1 p. 574 o &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="735cdf6c1c61f38b1092aa1bd4262b34c0d5f0f3" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;. It is required to compute the estimated data covariance and score samples.</source>
          <target state="translated">La covarianza de ruido estimada siguiendo el modelo probabil&amp;iacute;stico PCA de Tipping y Bishop 1999. Ver &amp;ldquo;Reconocimiento de patrones y aprendizaje autom&amp;aacute;tico&amp;rdquo; de C. Bishop, 12.2.1 p. 574 o &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt; . Es necesario calcular la covarianza de datos estimada y las muestras de puntuaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="cc49065d7efcb582d87899f7997c53b1f8d60c78" translate="yes" xml:space="preserve">
          <source>The estimated noise variance for each feature.</source>
          <target state="translated">La variación de ruido estimada para cada característica.</target>
        </trans-unit>
        <trans-unit id="960e70b3d078f74487289ecbb537290d1f7473d4" translate="yes" xml:space="preserve">
          <source>The estimated number of components. Relevant when &lt;code&gt;n_components=None&lt;/code&gt;.</source>
          <target state="translated">El n&amp;uacute;mero estimado de componentes. Relevante cuando &lt;code&gt;n_components=None&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="df011fa4ccaad62cb4a83a0955409a4c4b3035c8" translate="yes" xml:space="preserve">
          <source>The estimated number of components. When n_components is set to &amp;lsquo;mle&amp;rsquo; or a number between 0 and 1 (with svd_solver == &amp;lsquo;full&amp;rsquo;) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.</source>
          <target state="translated">El n&amp;uacute;mero estimado de componentes. Cuando n_components se establece en 'mle' o un n&amp;uacute;mero entre 0 y 1 (con svd_solver == 'full'), este n&amp;uacute;mero se estima a partir de los datos de entrada. De lo contrario, es igual al par&amp;aacute;metro n_components, o al valor menor de n_features y n_samples si n_components es None.</target>
        </trans-unit>
        <trans-unit id="cd106f5afe35a52902113758dbc47d95941530e6" translate="yes" xml:space="preserve">
          <source>The estimated number of connected components in the graph.</source>
          <target state="translated">El número estimado de componentes conectados en el gráfico.</target>
        </trans-unit>
        <trans-unit id="54994eb61f1dce6c8894f0827b84346580c11cb2" translate="yes" xml:space="preserve">
          <source>The estimation of the EXPERIENCE coefficient is now less variable and remain important for all models trained during cross-validation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a98c4d1bc7caeb1163bf14c014655348909c79f3" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts.</source>
          <target state="translated">La estimación del modelo se hace calculando las pendientes e intercepciones de una subpoblación de todas las combinaciones posibles de puntos de submuestra p.Si se ajusta una intercepción,p debe ser mayor o igual a n_características+1.La pendiente e intercepción final se define entonces como la mediana espacial de estas pendientes e intercepciones.</target>
        </trans-unit>
        <trans-unit id="aed02faa2e2402cc32a5968b7e86a54cff535c4e" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.</source>
          <target state="translated">La estimación del modelo se realiza maximizando iterativamente la logopositividad marginal de las observaciones.</target>
        </trans-unit>
        <trans-unit id="44fe06813d3627aaa56531c52961387365e10d66" translate="yes" xml:space="preserve">
          <source>The estimation of the number of degrees of freedom is given by:</source>
          <target state="translated">La estimación del número de grados de libertad viene dada por:</target>
        </trans-unit>
        <trans-unit id="2bed56c931836016f42a33afdedfac5cb8f555f4" translate="yes" xml:space="preserve">
          <source>The estimator also implements &lt;code&gt;partial_fit&lt;/code&gt;, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.</source>
          <target state="translated">El estimador tambi&amp;eacute;n implementa &lt;code&gt;partial_fit&lt;/code&gt; , que actualiza el diccionario iterando solo una vez sobre un mini-lote. Esto se puede usar para el aprendizaje en l&amp;iacute;nea cuando los datos no est&amp;aacute;n disponibles desde el principio o cuando los datos no caben en la memoria.</target>
        </trans-unit>
        <trans-unit id="efc1e3841cf31ff119d8e77fb55d7093c1df4dd4" translate="yes" xml:space="preserve">
          <source>The estimator objects for each cv split. This is available only if &lt;code&gt;return_estimator&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Los objetos del estimador para cada divisi&amp;oacute;n de cv. Esto solo est&amp;aacute; disponible si el par&amp;aacute;metro &lt;code&gt;return_estimator&lt;/code&gt; se establece en &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f33cc8b973e26f8a535386dbd610021f87dfdf8f" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned</source>
          <target state="translated">El estimador o grupo de estimadores a ser clonado</target>
        </trans-unit>
        <trans-unit id="c1ff61d32d7a7a2deb658878854ba14950f2c891" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65944411996b76786bcb63db12ac31b08b55648a" translate="yes" xml:space="preserve">
          <source>The estimator that provides the initial predictions. Set via the &lt;code&gt;init&lt;/code&gt; argument or &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">El estimador que proporciona las predicciones iniciales. Se establece mediante el argumento &lt;code&gt;init&lt;/code&gt; o &lt;code&gt;loss.init_estimator&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f7ae9218cda2abb5c086ad75a4ab1e94b87ebec1" translate="yes" xml:space="preserve">
          <source>The estimator to use at each step of the round-robin imputation. If &lt;code&gt;sample_posterior&lt;/code&gt; is True, the estimator must support &lt;code&gt;return_std&lt;/code&gt; in its &lt;code&gt;predict&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e68c5f93929a570a29c52208fc20a3ad933e6e26" translate="yes" xml:space="preserve">
          <source>The estimator to visualize.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5314881b9357d8103e571e2b27bb56496dd3b052" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute, but can be accessed by index or name by indexing (with &lt;code&gt;[idx]&lt;/code&gt;) the Pipeline:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ade400270e658832b6e128c7ef187979eae33356" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute:</source>
          <target state="translated">Los estimadores de una tuber&amp;iacute;a se almacenan como una lista en el atributo de &lt;code&gt;steps&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="bc948c82039e32a77f975544660f3043a6111d3d" translate="yes" xml:space="preserve">
          <source>The estimators of the pipeline can be retrieved by index:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a46349c122935a13db8091d877d4179b37995289" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. For example, it is possible to use these estimators to turn a binary classifier or a regressor into a multiclass classifier. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime performance improves.</source>
          <target state="translated">Los estimadores proporcionados en este módulo son metaestimadores:requieren que se proporcione un estimador base en su constructor.Por ejemplo,es posible utilizar estos estimadores para convertir un clasificador binario o un regresor en un clasificador multiclase.También es posible utilizar estos estimadores con estimadores multiclase con la esperanza de que su precisión o rendimiento en tiempo de ejecución mejore.</target>
        </trans-unit>
        <trans-unit id="1868049da2c813b87c2e3d48a5c71f72cc892852" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. The meta-estimator extends single output estimators to multioutput estimators.</source>
          <target state="translated">Los estimadores proporcionados en este módulo son metaestimadores:requieren que se proporcione un estimador base en su constructor.El metaestimador extiende los estimadores de salida única a los estimadores de salida múltiple.</target>
        </trans-unit>
        <trans-unit id="fe77b9f934c3d4135d1310eb25c7f2c922f35971" translate="yes" xml:space="preserve">
          <source>The exact API of all functions and classes, as given by the docstrings. The API documents expected types and allowed features for all functions, and all parameters available for the algorithms.</source>
          <target state="translated">El API exacto de todas las funciones y clases,tal y como se indica en los documentos.La API documenta los tipos esperados y las características permitidas para todas las funciones,y todos los parámetros disponibles para los algoritmos.</target>
        </trans-unit>
        <trans-unit id="213a31afd656f2a0c8b37dd55b40fc5308db3ca1" translate="yes" xml:space="preserve">
          <source>The exact additive chi squared kernel.</source>
          <target state="translated">El núcleo exacto de chi cuadrado aditivo.</target>
        </trans-unit>
        <trans-unit id="14b1a49c77fdcb8a0d3244090e7c6df0b2a6cd07" translate="yes" xml:space="preserve">
          <source>The exact chi squared kernel.</source>
          <target state="translated">El núcleo exacto del chi al cuadrado.</target>
        </trans-unit>
        <trans-unit id="21c56a998f59a2c6be6550db485efcc395359e33" translate="yes" xml:space="preserve">
          <source>The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of &lt;code&gt;n_estimators&lt;/code&gt; at which the error stabilizes.</source>
          <target state="translated">El siguiente ejemplo demuestra c&amp;oacute;mo se puede medir el error OOB al agregar cada &amp;aacute;rbol nuevo durante el entrenamiento. La gr&amp;aacute;fica resultante permite al practicante aproximarse a un valor adecuado de &lt;code&gt;n_estimators&lt;/code&gt; en el que se estabiliza el error.</target>
        </trans-unit>
        <trans-unit id="556b3073adbd8a84d1d4b2fe21db4807c2daf4c8" translate="yes" xml:space="preserve">
          <source>The example below uses a support vector classifier with a non-linear kernel to build a model with optimized hyperparameters by grid search. We compare the performance of non-nested and nested CV strategies by taking the difference between their scores.</source>
          <target state="translated">En el ejemplo que figura a continuación se utiliza un clasificador de vectores de apoyo con un núcleo no lineal para construir un modelo con hiperparámetros optimizados mediante la búsqueda de cuadrículas.Comparamos el rendimiento de las estrategias CV no anidadas y anidadas tomando la diferencia entre sus puntuaciones.</target>
        </trans-unit>
        <trans-unit id="4cd198cf307950e0cb10d5d2e6b86c91624772aa" translate="yes" xml:space="preserve">
          <source>The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features.</source>
          <target state="translated">El ejemplo compara el resultado de la predicción de la regresión lineal (modelo lineal)y el árbol de decisión (modelo basado en el árbol)con y sin la discretización de las características de valor real.</target>
        </trans-unit>
        <trans-unit id="3101cd4fceb55d1be63d1bff5cb3f45996fd9aa0" translate="yes" xml:space="preserve">
          <source>The example demonstrates syntax and speed only; it doesn&amp;rsquo;t actually do anything useful with the extracted vectors. See the example scripts {document_classification_20newsgroups,clustering}.py for actual learning on text documents.</source>
          <target state="translated">El ejemplo demuestra sintaxis y velocidad &amp;uacute;nicamente; en realidad, no hace nada &amp;uacute;til con los vectores extra&amp;iacute;dos. Vea los scripts de ejemplo {document_classification_20newsgroups, clustering} .py para obtener informaci&amp;oacute;n real sobre documentos de texto.</target>
        </trans-unit>
        <trans-unit id="32ed3af5edbdaa27f9eba4e5a30255c6afab4414" translate="yes" xml:space="preserve">
          <source>The example is engineered to show the effect of the choice of different metrics. It is applied to waveforms, which can be seen as high-dimensional vector. Indeed, the difference between metrics is usually more pronounced in high dimension (in particular for euclidean and cityblock).</source>
          <target state="translated">El ejemplo está diseñado para mostrar el efecto de la elección de diferentes métricas.Se aplica a las formas de onda,que pueden verse como un vector de alta dimensión.De hecho,la diferencia entre las métricas suele ser más pronunciada en la alta dimensión (en particular para euclidiano y cityblock).</target>
        </trans-unit>
        <trans-unit id="36ed82faabe70da57df289b8a54bb16e8830773a" translate="yes" xml:space="preserve">
          <source>The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge.</source>
          <target state="translated">El ejemplo muestra que las predicciones en la cresta están fuertemente influenciadas por los valores atípicos presentes en el conjunto de datos.El regresor de Huber está menos influenciado por los valores atípicos ya que el modelo utiliza la pérdida lineal para éstos.A medida que el parámetro épsilon aumenta para el regresor de Huber,la función de decisión se aproxima a la de la cresta.</target>
        </trans-unit>
        <trans-unit id="889bc091af3b0106e91a81884d921d1b2df9bdd6" translate="yes" xml:space="preserve">
          <source>The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected &lt;code&gt;n_components=5&lt;/code&gt; which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.</source>
          <target state="translated">Los siguientes ejemplos comparan los modelos de mezcla gaussiana con un n&amp;uacute;mero fijo de componentes con los modelos de mezcla gaussiana variacional con un proceso de Dirichlet previo. Aqu&amp;iacute;, una mezcla gaussiana cl&amp;aacute;sica se ajusta con 5 componentes en un conjunto de datos compuesto por 2 grupos. Podemos ver que la mezcla gaussiana variacional con un proceso de Dirichlet previo es capaz de limitarse a solo 2 componentes mientras que la mezcla gaussiana ajusta los datos con un n&amp;uacute;mero fijo de componentes que tiene que ser configurado a priori por el usuario. En este caso, el usuario ha seleccionado &lt;code&gt;n_components=5&lt;/code&gt; que no coincide con la verdadera distribuci&amp;oacute;n generativa de este conjunto de datos de juguetes. Tenga en cuenta que con muy pocas observaciones, los modelos de mezcla gaussiana variacional con un proceso de Dirichlet anterior pueden adoptar una posici&amp;oacute;n conservadora y ajustarse solo a un componente.</target>
        </trans-unit>
        <trans-unit id="2cf55b61801dba638e3df4951d7127011c13c263" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation &lt;a href=&quot;#veb2009&quot; id=&quot;id13&quot;&gt;[VEB2009]&lt;/a&gt;. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5c4a6233958dced6338c85ecba7ef5860ffcbbc" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">El valor esperado de la información mutua puede ser calculado usando la siguiente ecuación [VEB2009].En esta ecuación,\(a_i=|U_i|\N (el número de elementos en \NU_i\N))y \N \N |V_j=|V_j|\N (el número de elementos en \NV_j)).</target>
        </trans-unit>
        <trans-unit id="5075a63ec7be1cbe42a641e5ae277f9e4b63f160" translate="yes" xml:space="preserve">
          <source>The experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The first figure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the assumption of feature-independence and result in an overly confident classifier, which is indicated by the typical transposed-sigmoid curve.</source>
          <target state="translated">El experimento se realiza en un conjunto de datos artificiales para la clasificación binaria con 100.000 muestras (1.000 de ellas se utilizan para el ajuste del modelo)con 20 características.De las 20 características,sólo 2 son informativas y 10 son redundantes.La primera figura muestra las probabilidades estimadas obtenidas con la regresión logística,Bayes gaussianas ingenuas,y Bayes gaussianas ingenuas con la calibración isotónica y la calibración sigmoide.El rendimiento de la calibración se evalúa con la puntuación de Brier,que figura en la leyenda (cuanto más pequeña,mejor).Se puede observar aquí que la regresión logística está bien calibrada mientras que el Bayes ingenuo gaussiano crudo se desempeña muy mal.Esto se debe a las características redundantes que violan el supuesto de independencia de las características y dan lugar a un clasificador demasiado confiado,lo que se indica en la típica curva transpuesta-sigmoide.</target>
        </trans-unit>
        <trans-unit id="b61a812b707dfd6196631d345ad23c1f697f044e" translate="yes" xml:space="preserve">
          <source>The experimental data presents a long tail distribution for &lt;code&gt;y&lt;/code&gt;. In all models, we predict the expected frequency of a random variable, so we will have necessarily fewer extreme values than for the observed realizations of that random variable. This explains that the mode of the histograms of model predictions doesn&amp;rsquo;t necessarily correspond to the smallest value. Additionally, the normal distribution used in &lt;code&gt;Ridge&lt;/code&gt; has a constant variance, while for the Poisson distribution used in &lt;code&gt;PoissonRegressor&lt;/code&gt; and &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;, the variance is proportional to the predicted expected value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1db213d528f30cf745c1554ad2a43991932646f" translate="yes" xml:space="preserve">
          <source>The explained variance or ndarray if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">La varianza explicada o ndarray si 'multioutput' es 'raw_values'.</target>
        </trans-unit>
        <trans-unit id="dff7adf6114ae66681ccc96cdb5c8abce36c0555" translate="yes" xml:space="preserve">
          <source>The explicit constant as predicted by the &amp;ldquo;constant&amp;rdquo; strategy. This parameter is useful only for the &amp;ldquo;constant&amp;rdquo; strategy.</source>
          <target state="translated">La constante expl&amp;iacute;cita seg&amp;uacute;n lo predicho por la estrategia &quot;constante&quot;. Este par&amp;aacute;metro es &amp;uacute;til solo para la estrategia &quot;constante&quot;.</target>
        </trans-unit>
        <trans-unit id="96483cbe0c7b434b8387960f89d9c2700f0af59b" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate [default 0.5].</source>
          <target state="translated">El exponente de la tasa de aprendizaje de escala inversa [por defecto 0,5].</target>
        </trans-unit>
        <trans-unit id="fe088102cec241da3f7e07b043fd901378b61425" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fa85799d065de026e0253585d451384c9ee6013" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to &amp;lsquo;invscaling&amp;rsquo;. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">El exponente de la tasa de aprendizaje de escala inversa. Se utiliza para actualizar la tasa de aprendizaje efectiva cuando learning_rate se establece en 'invscaling'. Solo se usa cuando solver = 'sgd'.</target>
        </trans-unit>
        <trans-unit id="c7acf68e47ad4fe8cc8cf54d82e56ffc97c60a11" translate="yes" xml:space="preserve">
          <source>The exponent for the base kernel</source>
          <target state="translated">El exponente del núcleo base</target>
        </trans-unit>
        <trans-unit id="fe68de8b995171a61c550ec2f7815b01a2d28ec7" translate="yes" xml:space="preserve">
          <source>The exponentiated version of the kernel, which is usually preferable.</source>
          <target state="translated">La versión exponencial del núcleo,que suele ser preferible.</target>
        </trans-unit>
        <trans-unit id="d1490a4bcb15c22959ee30800b231e4413480004" translate="yes" xml:space="preserve">
          <source>The external estimator fit on the reduced dataset.</source>
          <target state="translated">El estimador externo encaja en el conjunto de datos reducido.</target>
        </trans-unit>
        <trans-unit id="18d0cd5609d8f78695dc43242ffb9cf995a65ee9" translate="yes" xml:space="preserve">
          <source>The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):</source>
          <target state="translated">Los vectores TF-IDF extraídos son muy escasos,con un promedio de 159 componentes no nulos por muestra en un espacio de más de 30000 dimensiones (menos del 0,5% de características no nulas):</target>
        </trans-unit>
        <trans-unit id="e5bc1ae1e0e90623cdbf461e900aded771c1b79d" translate="yes" xml:space="preserve">
          <source>The extracted dataset will only retain pictures of people that have at least &lt;code&gt;min_faces_per_person&lt;/code&gt; different pictures.</source>
          <target state="translated">El conjunto de datos extra&amp;iacute;do solo retendr&amp;aacute; im&amp;aacute;genes de personas que tengan al menos &lt;code&gt;min_faces_per_person&lt;/code&gt; im&amp;aacute;genes diferentes.</target>
        </trans-unit>
        <trans-unit id="413a1ee9e7f0b0741202aa45f471425edb5c2085" translate="yes" xml:space="preserve">
          <source>The extraction method used to extract clusters using the calculated reachability and ordering. Possible values are &amp;ldquo;xi&amp;rdquo; and &amp;ldquo;dbscan&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cb7e9ef70987101280d538c40ac3f0463557635" translate="yes" xml:space="preserve">
          <source>The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.</source>
          <target state="translated">El factor que multiplica el tamaño del hipercubo.Los valores más grandes dispersan los cúmulos/clases y facilitan la tarea de clasificación.</target>
        </trans-unit>
        <trans-unit id="65d18b0a37e6414a591311e75d9ded04a4293e5e" translate="yes" xml:space="preserve">
          <source>The factory can be any callable that takes no argument and return an instance of &lt;code&gt;ParallelBackendBase&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cd5de0c793252ae54dc423f82b86e165f05af5f" translate="yes" xml:space="preserve">
          <source>The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&amp;rsquo;s paper. Note that it&amp;rsquo;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.</source>
          <target state="translated">La famosa base de datos Iris, utilizada por primera vez por Sir RA Fisher. El conjunto de datos se tom&amp;oacute; del art&amp;iacute;culo de Fisher. Tenga en cuenta que es lo mismo que en R, pero no como en el Repositorio de aprendizaje autom&amp;aacute;tico de UCI, que tiene dos puntos de datos incorrectos.</target>
        </trans-unit>
        <trans-unit id="64135ddd2f4a94ed0611a8692e8099bf4451f815" translate="yes" xml:space="preserve">
          <source>The feature (e.g. &lt;code&gt;[0]&lt;/code&gt;) or pair of interacting features (e.g. &lt;code&gt;[(0, 1)]&lt;/code&gt;) for which the partial dependency should be computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9c1f9439f6a49e182ea739a4c5151e4783dfb88" translate="yes" xml:space="preserve">
          <source>The feature importance scores of a fit gradient boosting model can be accessed via the &lt;code&gt;feature_importances_&lt;/code&gt; property:</source>
          <target state="translated">Se puede acceder a las puntuaciones de importancia de la caracter&amp;iacute;stica de un modelo de aumento de gradiente de ajuste a trav&amp;eacute;s de la propiedad &lt;code&gt;feature_importances_&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="29293679b6f2571a09f6b43f7ca55454fa94d5ff" translate="yes" xml:space="preserve">
          <source>The feature importances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="596d7f9b612d1877ad65dac2d6189030efea06e6" translate="yes" xml:space="preserve">
          <source>The feature matrix &lt;code&gt;X&lt;/code&gt; should be standardized before fitting. This ensures that the penalty treats features equally.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40d3ffb0824ad292679a9b4129b9e33a1cef7858" translate="yes" xml:space="preserve">
          <source>The feature matrix. Categorical features are encoded as ordinals.</source>
          <target state="translated">La matriz de características.Los rasgos categóricos están codificados como ordinales.</target>
        </trans-unit>
        <trans-unit id="0a487efba080872bb151e33f60795da2b55ed658" translate="yes" xml:space="preserve">
          <source>The feature ranking, such that &lt;code&gt;ranking_[i]&lt;/code&gt; corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.</source>
          <target state="translated">La clasificaci&amp;oacute;n de la caracter&amp;iacute;stica, tal que &lt;code&gt;ranking_[i]&lt;/code&gt; corresponde a la posici&amp;oacute;n de clasificaci&amp;oacute;n de la caracter&amp;iacute;stica i-&amp;eacute;sima. A las caracter&amp;iacute;sticas seleccionadas (es decir, las mejores estimadas) se les asigna el rango 1.</target>
        </trans-unit>
        <trans-unit id="c61b5bb2da747f9d4f147f5fd2e2f8308d20750d" translate="yes" xml:space="preserve">
          <source>The features and estimators that are experimental aren&amp;rsquo;t subject to deprecation cycles. Use them at your own risks!</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="acbf27a6428b599cd8900c8ca4f4ecae3f0cc8b0" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and &lt;code&gt;max_features=n_features&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">Las caracter&amp;iacute;sticas siempre se permutan aleatoriamente en cada divisi&amp;oacute;n. Por lo tanto, la mejor divisi&amp;oacute;n encontrada puede variar, incluso con los mismos datos de entrenamiento y &lt;code&gt;max_features=n_features&lt;/code&gt; , si la mejora del criterio es id&amp;eacute;ntica para varias divisiones enumeradas durante la b&amp;uacute;squeda de la mejor divisi&amp;oacute;n. Para obtener un comportamiento determinista durante el ajuste, &lt;code&gt;random_state&lt;/code&gt; debe ser fijo.</target>
        </trans-unit>
        <trans-unit id="c25846da01d7420f77d01daefe6ea3229aead8d7" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, &lt;code&gt;max_features=n_features&lt;/code&gt; and &lt;code&gt;bootstrap=False&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">Las caracter&amp;iacute;sticas siempre se permutan aleatoriamente en cada divisi&amp;oacute;n. Por lo tanto, la mejor divisi&amp;oacute;n encontrada puede variar, incluso con los mismos datos de entrenamiento, &lt;code&gt;max_features=n_features&lt;/code&gt; y &lt;code&gt;bootstrap=False&lt;/code&gt; , si la mejora del criterio es id&amp;eacute;ntica para varias divisiones enumeradas durante la b&amp;uacute;squeda de la mejor divisi&amp;oacute;n. Para obtener un comportamiento determinista durante el ajuste, &lt;code&gt;random_state&lt;/code&gt; debe ser fijo.</target>
        </trans-unit>
        <trans-unit id="79b5615baa935539329fdffd05466c4966130183" translate="yes" xml:space="preserve">
          <source>The features indices which will be returned when calling &lt;code&gt;transform&lt;/code&gt;. They are computed during &lt;code&gt;fit&lt;/code&gt;. For &lt;code&gt;features='all'&lt;/code&gt;, it is to &lt;code&gt;range(n_features)&lt;/code&gt;.</source>
          <target state="translated">Los &amp;iacute;ndices de caracter&amp;iacute;sticas que se devolver&amp;aacute;n al llamar a &lt;code&gt;transform&lt;/code&gt; . Se calculan durante el &lt;code&gt;fit&lt;/code&gt; . Para &lt;code&gt;features='all'&lt;/code&gt; , es &lt;code&gt;range(n_features)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7bb18d685bec7903d9b61cd6026a227300d22db3" translate="yes" xml:space="preserve">
          <source>The features of &lt;code&gt;X&lt;/code&gt; have been transformed from \([x_1, x_2]\) to \([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within any linear model.</source>
          <target state="translated">Las caracter&amp;iacute;sticas de &lt;code&gt;X&lt;/code&gt; se han transformado de \ ([x_1, x_2] \) a \ ([1, x_1, x_2, x_1 ^ 2, x_1 x_2, x_2 ^ 2] \) y ahora se pueden usar dentro de cualquier modelo lineal .</target>
        </trans-unit>
        <trans-unit id="b791adba0e25e8ba284f5da492871659f500ccc6" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2)\) to \((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).</source>
          <target state="translated">Las características de X se han transformado de \((X_1,X_2)\)a \((1,X_1,X_2,X_1^2,X_1X_2,X_2^2)\).</target>
        </trans-unit>
        <trans-unit id="0ec9e3dee3b599b742aa9c67dee3f4cab82e4d00" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2, X_3)\) to \((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).</source>
          <target state="translated">Las características de X se han transformado de \((X_1,X_2,X_3)\)a \((1,X_1,X_2,X_3,X_1X_2,X_1X_3,X_2X_3,X_1X_2X_3)\).</target>
        </trans-unit>
        <trans-unit id="eaccd9379a9dbc6cce41fc318001c8bf42339abe" translate="yes" xml:space="preserve">
          <source>The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.</source>
          <target state="translated">La figura que figura a continuación ilustra el efecto de la contracción y el submuestreo en la bondad de ajuste del modelo.Podemos ver claramente que la contracción supera a la no contracción.El submuestreo con la contracción puede aumentar aún más la precisión del modelo.El submuestreo sin encogimiento,por otro lado,no funciona bien.</target>
        </trans-unit>
        <trans-unit id="409a640cc1c72ef47cfd3f6ea68f986d77585eba" translate="yes" xml:space="preserve">
          <source>The figure below shows four one-way and one two-way partial dependence plots for the California housing dataset, with a &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50a711fcdd2b061e3d348535a1d7a4d9bddb7ed5" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">La siguiente figura muestra los resultados de aplicar &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; con p&amp;eacute;rdida por m&amp;iacute;nimos cuadrados y 500 alumnos base al conjunto de datos de precios de la vivienda de Boston ( &lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt; ). El gr&amp;aacute;fico de la izquierda muestra el tren y el error de prueba en cada iteraci&amp;oacute;n. El error de tren en cada iteraci&amp;oacute;n se almacena en el atributo &lt;code&gt;train_score_&lt;/code&gt; del modelo de aumento de gradiente. El error de prueba en cada iteraci&amp;oacute;n se puede obtener a trav&amp;eacute;s del m&amp;eacute;todo &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt; que devuelve un generador que arroja las predicciones en cada etapa. Parcelas como estas se pueden utilizar para determinar el n&amp;uacute;mero &amp;oacute;ptimo de &amp;aacute;rboles (es decir, &lt;code&gt;n_estimators&lt;/code&gt; ) mediante una parada temprana. El gr&amp;aacute;fico de la derecha muestra las caracter&amp;iacute;sticas importantes que se pueden obtener a trav&amp;eacute;s del &lt;code&gt;feature_importances_&lt;/code&gt; property.</target>
        </trans-unit>
        <trans-unit id="76597dd5c3aa4f5be5177f9ebf1af69e02fc15a8" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the impurity-based feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc8de6d2f1f6034186ab0af10d91754eadd34954" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">La figura muestra que ambos m&amp;eacute;todos aprenden modelos razonables de la funci&amp;oacute;n objetivo. GPR identifica correctamente que la periodicidad de la funci&amp;oacute;n es aproximadamente 2 * pi (6.28), mientras que KRR elige la periodicidad duplicada 4 * pi. Adem&amp;aacute;s de eso, GPR proporciona l&amp;iacute;mites de confianza razonables en la predicci&amp;oacute;n que no est&amp;aacute;n disponibles para KRR. Una diferencia importante entre los dos m&amp;eacute;todos es el tiempo requerido para ajustar y predecir: mientras que ajustar KRR es r&amp;aacute;pido en principio, la b&amp;uacute;squeda de cuadr&amp;iacute;cula para la optimizaci&amp;oacute;n de hiperpar&amp;aacute;metros escala exponencialmente con el n&amp;uacute;mero de hiperpar&amp;aacute;metros (&quot;maldici&amp;oacute;n de dimensionalidad&quot;). La optimizaci&amp;oacute;n basada en gradientes de los par&amp;aacute;metros en GPR no sufre esta escala exponencial y, por lo tanto, es considerablemente m&amp;aacute;s r&amp;aacute;pida en este ejemplo con espacio de hiperpar&amp;aacute;metros tridimensional. El tiempo para predecir es similar; sin embargo,generar la varianza de la distribuci&amp;oacute;n predictiva de GPR lleva mucho m&amp;aacute;s tiempo que simplemente predecir la media.</target>
        </trans-unit>
        <trans-unit id="7a70cd6dc569a4ad4c8d12310e45e3bf56f13e7d" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly \(2*\pi\) (6.28), while KRR chooses the doubled periodicity \(4*\pi\) . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">La figura muestra que ambos m&amp;eacute;todos aprenden modelos razonables de la funci&amp;oacute;n objetivo. GPR identifica correctamente que la periodicidad de la funci&amp;oacute;n es aproximadamente \ (2 * \ pi \) (6.28), mientras que KRR elige la periodicidad duplicada \ (4 * \ pi \). Adem&amp;aacute;s de eso, GPR proporciona l&amp;iacute;mites de confianza razonables en la predicci&amp;oacute;n que no est&amp;aacute;n disponibles para KRR. Una diferencia importante entre los dos m&amp;eacute;todos es el tiempo requerido para ajustar y predecir: mientras que ajustar KRR es r&amp;aacute;pido en principio, la b&amp;uacute;squeda de cuadr&amp;iacute;cula para la optimizaci&amp;oacute;n de hiperpar&amp;aacute;metros escala exponencialmente con el n&amp;uacute;mero de hiperpar&amp;aacute;metros (&quot;maldici&amp;oacute;n de dimensionalidad&quot;). La optimizaci&amp;oacute;n basada en gradientes de los par&amp;aacute;metros en GPR no sufre esta escala exponencial y, por lo tanto, es considerablemente m&amp;aacute;s r&amp;aacute;pida en este ejemplo con espacio de hiperpar&amp;aacute;metros tridimensional. El tiempo para predecir es similar; sin embargo,generar la varianza de la distribuci&amp;oacute;n predictiva de GPR lleva mucho m&amp;aacute;s tiempo que simplemente predecir la media.</target>
        </trans-unit>
        <trans-unit id="4c819c571e9cf27ccc7a880b7ae493959bac8667" translate="yes" xml:space="preserve">
          <source>The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding.</source>
          <target state="translated">La figura muestra la suma acumulativa de las mejoras negativas de OOB en función de la iteración de impulso.Como puedes ver,sigue la pérdida de la prueba para las primeras cien iteraciones pero luego se desvía de manera pesimista.La figura también muestra el rendimiento de la validación cruzada triple que normalmente da una mejor estimación de la pérdida de la prueba pero es computacionalmente más exigente.</target>
        </trans-unit>
        <trans-unit id="37bfa29a2e04a43879779df5389399bec3580356" translate="yes" xml:space="preserve">
          <source>The figure shows the trade-off between cross-validated score and the number of PCA components. The balanced case is when n_components=10 and accuracy=0.88, which falls into the range within 1 standard deviation of the best accuracy score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc5bec6cdf25a030ca7f5736bc605af5657e6e59" translate="yes" xml:space="preserve">
          <source>The figures below are used to illustrate the effect of scaling our &lt;code&gt;C&lt;/code&gt; to compensate for the change in the number of samples, in the case of using an &lt;code&gt;l1&lt;/code&gt; penalty, as well as the &lt;code&gt;l2&lt;/code&gt; penalty.</source>
          <target state="translated">Las figuras siguientes se utilizan para ilustrar el efecto de escalar nuestra &lt;code&gt;C&lt;/code&gt; para compensar el cambio en el n&amp;uacute;mero de muestras, en el caso de utilizar una penalizaci&amp;oacute;n &lt;code&gt;l1&lt;/code&gt; , as&amp;iacute; como la penalizaci&amp;oacute;n &lt;code&gt;l2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0b0d592cc5daeb578d9b82714e3800c3092f375f" translate="yes" xml:space="preserve">
          <source>The figures illustrate the interpolating property of the Gaussian Process model as well as its probabilistic nature in the form of a pointwise 95% confidence interval.</source>
          <target state="translated">Las figuras ilustran la propiedad interpoladora del modelo del Proceso Gaussiano,así como su naturaleza probabilística en forma de un intervalo de confianza puntual del 95%.</target>
        </trans-unit>
        <trans-unit id="e96dfdb18e509ef4ea2f5731757d5c9fff16f17b" translate="yes" xml:space="preserve">
          <source>The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.</source>
          <target state="translated">Las cifras muestran la matriz de confusión con y sin normalización por el tamaño del soporte de la clase (número de elementos en cada clase).Este tipo de normalización puede ser interesante en caso de desequilibrio de clases para tener una interpretación más visual de qué clase está siendo mal clasificada.</target>
        </trans-unit>
        <trans-unit id="e0f5d06a7d2ed2f3c6fb0ae21cf62a8ad73fbd7f" translate="yes" xml:space="preserve">
          <source>The filenames for the images.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c01e855d5c880b7b53c9ce8ac56ca5941d4bf79" translate="yes" xml:space="preserve">
          <source>The filenames holding the dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83d2d2e5c2f316a3531e949e8bac6210f7e21821" translate="yes" xml:space="preserve">
          <source>The files themselves are loaded in memory in the &lt;code&gt;data&lt;/code&gt; attribute. For reference the filenames are also available:</source>
          <target state="translated">Los propios archivos se cargan en la memoria en el atributo de &lt;code&gt;data&lt;/code&gt; . Como referencia, los nombres de archivo tambi&amp;eacute;n est&amp;aacute;n disponibles:</target>
        </trans-unit>
        <trans-unit id="4834eeb362cb8f80edd0fa52b738fe7db255a20e" translate="yes" xml:space="preserve">
          <source>The filesystem path to the root folder where MLComp datasets are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead.</source>
          <target state="translated">La ruta del sistema de archivos a la carpeta raíz donde se almacenan los conjuntos de datos de MLComp,si mlcomp_root es None,se busca en su lugar la variable de entorno MLCOMP_DATASETS_HOME.</target>
        </trans-unit>
        <trans-unit id="5e83705f6f5cf8453734b26cfa75b02cefbf9a06" translate="yes" xml:space="preserve">
          <source>The final sum of similarities is divided by the size of the larger set.</source>
          <target state="translated">La suma final de las similitudes se divide por el tamaño del conjunto mayor.</target>
        </trans-unit>
        <trans-unit id="6023cd4bbdadd0261fe4fb93bbac4f79dd623983" translate="yes" xml:space="preserve">
          <source>The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set).</source>
          <target state="translated">El valor final del criterio de inercia (suma de las distancias al cuadrado hasta el centroide más cercano para todas las observaciones del conjunto de entrenamiento).</target>
        </trans-unit>
        <trans-unit id="65db46a7a2384cdd4de97d3c70536cd9da4c8616" translate="yes" xml:space="preserve">
          <source>The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points).</source>
          <target state="translated">El valor final de la tensión (suma de la distancia al cuadrado de las disparidades y las distancias para todos los puntos constreñidos).</target>
        </trans-unit>
        <trans-unit id="379910ba877ee4fe0038b0877574596b2d4e7156" translate="yes" xml:space="preserve">
          <source>The first 4 plots use the &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; with different numbers of informative features, clusters per class and classes. The final 2 plots use &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e7a148ceab046969e5524f650bd9948ad6ae0ac" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;[.9, .1]&lt;/code&gt; in &lt;code&gt;y_pred&lt;/code&gt; denotes 90% probability that the first sample has label 0. The log loss is non-negative.</source>
          <target state="translated">El primer &lt;code&gt;[.9, .1]&lt;/code&gt; en &lt;code&gt;y_pred&lt;/code&gt; denota un 90% de probabilidad de que la primera muestra tenga la etiqueta 0. La p&amp;eacute;rdida logar&amp;iacute;tmica no es negativa.</target>
        </trans-unit>
        <trans-unit id="c41f4a219241dbe01041bd4816c28f057f1f8fc9" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;n_samples % n_splits&lt;/code&gt; folds have size &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt;, other folds have size &lt;code&gt;n_samples // n_splits&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">Los primeros pliegues &lt;code&gt;n_samples % n_splits&lt;/code&gt; tienen tama&amp;ntilde;o &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt; , otros pliegues tienen tama&amp;ntilde;o &lt;code&gt;n_samples // n_splits&lt;/code&gt; , donde &lt;code&gt;n_samples&lt;/code&gt; es el n&amp;uacute;mero de muestras.</target>
        </trans-unit>
        <trans-unit id="318062ceb48883ba19b024b3ac85479b5d766c8b" translate="yes" xml:space="preserve">
          <source>The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices. In general, multiple points can be queried at the same time.</source>
          <target state="translated">La primera matriz devuelta contiene las distancias a todos los puntos que están más cerca de 1,6,mientras que la segunda matriz devuelta contiene sus índices.En general,se pueden consultar varios puntos al mismo tiempo.</target>
        </trans-unit>
        <trans-unit id="b072f11f22e8fd40f8f011ced740f859610c5839" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the product-kernel</source>
          <target state="translated">El primer núcleo de base del núcleo del producto</target>
        </trans-unit>
        <trans-unit id="2843deb348cef2ebb64b5cb6c478da3efa931340" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the sum-kernel</source>
          <target state="translated">El primer núcleo de base del núcleo de la suma</target>
        </trans-unit>
        <trans-unit id="23baf5dbd5fca89380aba285e5a6dca77d9ff4b8" translate="yes" xml:space="preserve">
          <source>The first column of images shows true faces. The next columns illustrate how extremely randomized trees, k nearest neighbors, linear regression and ridge regression complete the lower half of those faces.</source>
          <target state="translated">La primera columna de imágenes muestra rostros reales.Las siguientes columnas ilustran cómo los árboles extremadamente aleatorios,los vecinos más cercanos k,la regresión lineal y la regresión de la cresta completan la mitad inferior de esas caras.</target>
        </trans-unit>
        <trans-unit id="758127cc4d0b762bc4a77876dd4551f141ccd7cd" translate="yes" xml:space="preserve">
          <source>The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.</source>
          <target state="translated">El primero corresponde a un modelo con un alto nivel de ruido y una gran escala de longitud,que explica todas las variaciones de los datos por el ruido.</target>
        </trans-unit>
        <trans-unit id="2d71e99749591e16661c08d2c19738355ed2fad8" translate="yes" xml:space="preserve">
          <source>The first element of each line can be used to store a target variable to predict.</source>
          <target state="translated">El primer elemento de cada línea puede ser usado para almacenar una variable objetivo para predecir.</target>
        </trans-unit>
        <trans-unit id="d151d2a5867c5ba125b375d964535691cae0b0d6" translate="yes" xml:space="preserve">
          <source>The first example illustrates how robust covariance estimation can help concentrating on a relevant cluster when another one exists. Here, many observations are confounded into one and break down the empirical covariance estimation. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">El primer ejemplo ilustra c&amp;oacute;mo una estimaci&amp;oacute;n de covarianza robusta puede ayudar a concentrarse en un grupo relevante cuando existe otro. Aqu&amp;iacute;, muchas observaciones se confunden en una y descomponen la estimaci&amp;oacute;n de covarianza emp&amp;iacute;rica. Por supuesto, algunas herramientas de cribado habr&amp;iacute;an se&amp;ntilde;alado la presencia de dos cl&amp;uacute;steres (M&amp;aacute;quinas de vectores de soporte, Modelos de mezcla gaussianos, detecci&amp;oacute;n de valores at&amp;iacute;picos univariados,&amp;hellip;). Pero si hubiera sido un ejemplo de alta dimensi&amp;oacute;n, ninguno de estos podr&amp;iacute;a aplicarse tan f&amp;aacute;cilmente.</target>
        </trans-unit>
        <trans-unit id="3ed755a713ce6230a16c443825e8cd0dee1503e8" translate="yes" xml:space="preserve">
          <source>The first example illustrates how the Minimum Covariance Determinant robust estimator can help concentrate on a relevant cluster when outlying points exist. Here the empirical covariance estimation is skewed by points outside of the main cluster. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecfbbb8300d863a918a602e1c4f90841d4b71a3f" translate="yes" xml:space="preserve">
          <source>The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):</source>
          <target state="translated">El primer cargador se utiliza para la tarea de identificación de rostros:una tarea de clasificación multiclase (por lo tanto,aprendizaje supervisado):</target>
        </trans-unit>
        <trans-unit id="1272790baab931c65e23f13d4b17128820578899" translate="yes" xml:space="preserve">
          <source>The first model is a classical Gaussian Mixture Model with 10 components fit with the Expectation-Maximization algorithm.</source>
          <target state="translated">El primer modelo es un modelo clásico de mezcla gaussiana con 10 componentes que se ajustan al algoritmo de Expectativa-Maximización.</target>
        </trans-unit>
        <trans-unit id="a28da4c4f339f2d2823befd3907c74a3bd7b1459" translate="yes" xml:space="preserve">
          <source>The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.</source>
          <target state="translated">El primer gráfico es una visualización de la función de decisión para una variedad de valores de parámetros en un problema de clasificación simplificada que implica sólo 2 características de entrada y 2 posibles clases de objetivo (clasificación binaria).Obsérvese que no es posible hacer este tipo de gráfico para problemas con más características o clases de objetivo.</target>
        </trans-unit>
        <trans-unit id="fdb1a20b80ba5f009466ef7acd3785afda979734" translate="yes" xml:space="preserve">
          <source>The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit &amp;ldquo;block&amp;rdquo; is stacked above each point on a regular grid. As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution. If we instead center each block on the point it represents, we get the estimate shown in the bottom left panel. This is a kernel density estimation with a &amp;ldquo;top hat&amp;rdquo; kernel. This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution.</source>
          <target state="translated">La primera gr&amp;aacute;fica muestra uno de los problemas con el uso de histogramas para visualizar la densidad de puntos en 1D. Intuitivamente, un histograma se puede considerar como un esquema en el que se apila una unidad de &quot;bloque&quot; sobre cada punto en una cuadr&amp;iacute;cula regular. Sin embargo, como muestran los dos paneles superiores, la elecci&amp;oacute;n de la cuadr&amp;iacute;cula para estos bloques puede llevar a ideas tremendamente divergentes sobre la forma subyacente de la distribuci&amp;oacute;n de densidad. Si, en cambio, centramos cada bloque en el punto que representa, obtenemos la estimaci&amp;oacute;n que se muestra en el panel inferior izquierdo. Esta es una estimaci&amp;oacute;n de la densidad del kernel con un kernel de &quot;sombrero de copa&quot;. Esta idea se puede generalizar a otras formas de kernel: el panel inferior derecho de la primera figura muestra una estimaci&amp;oacute;n de densidad de kernel gaussiana sobre la misma distribuci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="80a6f35d6de94c1655a0ab570a2d81b8e9f0c281" translate="yes" xml:space="preserve">
          <source>The first plot shows that with an increasing number of samples &lt;code&gt;n_samples&lt;/code&gt;, the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; increased logarithmically in order to guarantee an &lt;code&gt;eps&lt;/code&gt;-embedding.</source>
          <target state="translated">El primer gr&amp;aacute;fico muestra que con un n&amp;uacute;mero creciente de muestras &lt;code&gt;n_samples&lt;/code&gt; , el n&amp;uacute;mero m&amp;iacute;nimo de dimensiones &lt;code&gt;n_components&lt;/code&gt; aument&amp;oacute; logar&amp;iacute;tmicamente para garantizar una incrustaci&amp;oacute;n &lt;code&gt;eps&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0fa5069414b33f645c4a3a3261f97c50a380bf97" translate="yes" xml:space="preserve">
          <source>The first plot shows the best inertia reached for each combination of the model (&lt;code&gt;KMeans&lt;/code&gt; or &lt;code&gt;MiniBatchKMeans&lt;/code&gt;) and the init method (&lt;code&gt;init=&quot;random&quot;&lt;/code&gt; or &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt;) for increasing values of the &lt;code&gt;n_init&lt;/code&gt; parameter that controls the number of initializations.</source>
          <target state="translated">El primer gr&amp;aacute;fico muestra la mejor inercia alcanzada para cada combinaci&amp;oacute;n del modelo ( &lt;code&gt;KMeans&lt;/code&gt; o &lt;code&gt;MiniBatchKMeans&lt;/code&gt; ) y el m&amp;eacute;todo init ( &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; o &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt; ) para valores crecientes del par&amp;aacute;metro &lt;code&gt;n_init&lt;/code&gt; que controla el n&amp;uacute;mero de inicializaciones.</target>
        </trans-unit>
        <trans-unit id="35a5988878bfd98d1cc6f738d4af80878102b501" translate="yes" xml:space="preserve">
          <source>The first row of output array indicates that there are three samples whose true cluster is &amp;ldquo;a&amp;rdquo;. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is &amp;ldquo;b&amp;rdquo;. Of them, none is in predicted cluster 0, one is in 1 and two are in 2.</source>
          <target state="translated">La primera fila de la matriz de salida indica que hay tres muestras cuyo verdadero grupo es &quot;a&quot;. De ellos, dos est&amp;aacute;n en el grupo 0 previsto, uno est&amp;aacute; en 1 y ninguno est&amp;aacute; en 2. Y la segunda fila indica que hay tres muestras cuyo verdadero grupo es &quot;b&quot;. De ellos, ninguno est&amp;aacute; en el grupo 0 previsto, uno en 1 y dos en 2.</target>
        </trans-unit>
        <trans-unit id="94d22c244cece5d36684b54eb3a3c36ef460564d" translate="yes" xml:space="preserve">
          <source>The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.</source>
          <target state="translated">Las dos primeras funciones de pérdida son perezosas,sólo actualizan los parámetros del modelo si un ejemplo viola la restricción de margen,lo que hace que el entrenamiento sea muy eficiente y puede dar lugar a modelos más escasos,incluso cuando se utiliza la penalización L2.</target>
        </trans-unit>
        <trans-unit id="90159f341e51eef650aa99c9ddd91af9915c59a3" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the &lt;code&gt;transform&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="865ac3c349894331005e5c9a9918fbe1c4576705" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.</source>
          <target state="translated">El modelo ajustado también puede utilizarse para reducir la dimensionalidad de la entrada proyectándola hacia las direcciones más discriminatorias.</target>
        </trans-unit>
        <trans-unit id="47ade7eb6fd49b2a025d9a87896bfcf3ba24402d" translate="yes" xml:space="preserve">
          <source>The fitted model.</source>
          <target state="translated">El modelo a medida.</target>
        </trans-unit>
        <trans-unit id="474bb96d2aba10d56f1c494aef7da54c3ce49fa5" translate="yes" xml:space="preserve">
          <source>The flattened data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14120dbe50cfe7a93d61eb3782205eb1e9322e9d" translate="yes" xml:space="preserve">
          <source>The flexibility of controlling the smoothness of the learned function via \(\nu\) allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Mat&amp;eacute;rn kernel are shown in the following figure:</source>
          <target state="translated">La flexibilidad de controlar la fluidez de la funci&amp;oacute;n aprendida a trav&amp;eacute;s de \ (\ nu \) permite adaptarse a las propiedades de la verdadera relaci&amp;oacute;n funcional subyacente. El anterior y posterior de un GP resultante de un kernel de Mat&amp;eacute;rn se muestran en la siguiente figura:</target>
        </trans-unit>
        <trans-unit id="130cb7c407aba5d31bddc566759f6c2692fa934a" translate="yes" xml:space="preserve">
          <source>The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.</source>
          <target state="translated">El diagrama de flujo que figura a continuación tiene por objeto ofrecer a los usuarios una guía aproximada sobre la forma de abordar los problemas con respecto a qué estimadores probar con sus datos.</target>
        </trans-unit>
        <trans-unit id="f162534c96a36fe6b7c86b6775d49d882723bf35" translate="yes" xml:space="preserve">
          <source>The folder names are used as supervised signal label names. The individual file names are not important.</source>
          <target state="translated">Los nombres de las carpetas se utilizan como nombres de etiquetas de señales supervisadas.Los nombres de los archivos individuales no son importantes.</target>
        </trans-unit>
        <trans-unit id="f2db8b6a5929e1458b6943c5d307e1ce8a02db89" translate="yes" xml:space="preserve">
          <source>The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.</source>
          <target state="translated">Los pliegues están aproximadamente equilibrados en el sentido de que el número de grupos distintos es aproximadamente el mismo en cada pliegue.</target>
        </trans-unit>
        <trans-unit id="7910d480ac1425d50ff9fab631fdf912810ec26e" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if \(\hat{y}\) is the predicted value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b43c7b928f1a786f0281c067a04292ccfddc7963" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">A continuación se presenta un conjunto de métodos destinados a la regresión en los que se espera que el valor objetivo sea una combinación lineal de las variables de entrada.En la noción matemática,si \N el valor previsto es el valor predicho.</target>
        </trans-unit>
        <trans-unit id="81301a81f070c1a1a05c02b69a367906c9b88d21" translate="yes" xml:space="preserve">
          <source>The following clustering assignment is slightly better, since it is homogeneous but not complete:</source>
          <target state="translated">La siguiente asignación de agrupación es ligeramente mejor,ya que es homogénea pero no completa:</target>
        </trans-unit>
        <trans-unit id="563a9a6f3a15e2fd51ba9e9b647870430bf12e58" translate="yes" xml:space="preserve">
          <source>The following code defines a linear kernel and creates a classifier instance that will use that kernel:</source>
          <target state="translated">El siguiente código define un núcleo lineal y crea una instancia clasificadora que utilizará ese núcleo:</target>
        </trans-unit>
        <trans-unit id="b8e362395586349d1fcb8b526001a536e06f5d5d" translate="yes" xml:space="preserve">
          <source>The following code is a bit verbose, feel free to jump directly to the analysis of the &lt;a href=&quot;#results&quot;&gt;results&lt;/a&gt;.</source>
          <target state="translated">El siguiente c&amp;oacute;digo es un poco detallado, si&amp;eacute;ntase libre de saltar directamente al an&amp;aacute;lisis de los &lt;a href=&quot;#results&quot;&gt;resultados&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="b414b22c1d4fa210fe910463f00d0b843e42262f" translate="yes" xml:space="preserve">
          <source>The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the &lt;code&gt;groups&lt;/code&gt; parameter.</source>
          <target state="translated">Los siguientes divisores de validaci&amp;oacute;n cruzada se pueden utilizar para hacer eso. El identificador de agrupaci&amp;oacute;n de las muestras se especifica mediante el par&amp;aacute;metro de &lt;code&gt;groups&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6757b5cfa24458748d25f81af60d324180a74dc4" translate="yes" xml:space="preserve">
          <source>The following cross-validators can be used in such cases.</source>
          <target state="translated">En esos casos pueden utilizarse los siguientes validadores cruzados.</target>
        </trans-unit>
        <trans-unit id="6da3fe3659ba9973062031cbded9ac1ee9e80559" translate="yes" xml:space="preserve">
          <source>The following dataset has integer features, two of which are the same in every sample. These are removed with the default setting for threshold:</source>
          <target state="translated">El siguiente conjunto de datos tiene características enteras,dos de las cuales son iguales en cada muestra.Éstas se eliminan con el ajuste predeterminado para el umbral:</target>
        </trans-unit>
        <trans-unit id="54024b35aa398c3f2441f7e2cd8d0e6044cd9d46" translate="yes" xml:space="preserve">
          <source>The following estimators have built-in variable selection fitting procedures, but any estimator using a L1 or elastic-net penalty also performs variable selection: typically &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with an appropriate penalty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f50629b49593a5773731e8a4d6d7fbab3d06afa6" translate="yes" xml:space="preserve">
          <source>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</source>
          <target state="translated">El siguiente ejemplo demuestra cómo estimar la precisión de una máquina vectorial de soporte del núcleo lineal en el conjunto de datos del iris,dividiendo los datos,ajustando un modelo y calculando la puntuación 5 veces consecutivas (con diferentes divisiones cada vez):</target>
        </trans-unit>
        <trans-unit id="cd40f6b0be33059696fbdf36c2c2af62f27e4578" translate="yes" xml:space="preserve">
          <source>The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50761302c7bb4483750fc53752f14a1bf26cb023" translate="yes" xml:space="preserve">
          <source>The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.</source>
          <target state="translated">El siguiente ejemplo ilustra 16 componentes extra&amp;iacute;dos utilizando PCA escasa del conjunto de datos de caras de Olivetti. Se puede observar c&amp;oacute;mo el t&amp;eacute;rmino de regularizaci&amp;oacute;n induce muchos ceros. Adem&amp;aacute;s, la estructura natural de los datos hace que los coeficientes distintos de cero sean verticalmente adyacentes. El modelo no aplica esto matem&amp;aacute;ticamente: cada componente es un vector \ (h \ in \ mathbf {R} ^ {4096} \), y no existe una noci&amp;oacute;n de adyacencia vertical excepto durante la visualizaci&amp;oacute;n amigable para los humanos como im&amp;aacute;genes de 64x64 p&amp;iacute;xeles. El hecho de que los componentes que se muestran a continuaci&amp;oacute;n parezcan locales es el efecto de la estructura inherente de los datos, lo que hace que tales patrones locales minimicen el error de reconstrucci&amp;oacute;n. Existen normas que inducen a la escasez que toman en cuenta la adyacencia y diferentes tipos de estructura; ver &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt;para una revisi&amp;oacute;n de dichos m&amp;eacute;todos. Para obtener m&amp;aacute;s detalles sobre c&amp;oacute;mo utilizar Sparse PCA, consulte la secci&amp;oacute;n de Ejemplos a continuaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="218baecbc1e50d08c530ba0bc6ca1335ad228789" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b4e5d16e2ae8075091f7ec9c797f8dd043b5cfe" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;code&gt;VotingClassifier&lt;/code&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">El siguiente ejemplo ilustra c&amp;oacute;mo las regiones de decisi&amp;oacute;n pueden cambiar cuando se usa un &lt;code&gt;VotingClassifier&lt;/code&gt; suave basado en una m&amp;aacute;quina de vectores de soporte lineal, un &amp;aacute;rbol de decisi&amp;oacute;n y un clasificador vecino K-m&amp;aacute;s cercano:</target>
        </trans-unit>
        <trans-unit id="fcb0630f60eb0a1b1e5514ec57d9a7f6cbd21ce7" translate="yes" xml:space="preserve">
          <source>The following example illustrates the effect of scaling the regularization parameter when using &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; for &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;classification&lt;/a&gt;. For SVC classification, we are interested in a risk minimization for the equation:</source>
          <target state="translated">El siguiente ejemplo ilustra el efecto de escalar el par&amp;aacute;metro de regularizaci&amp;oacute;n al usar &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; para la &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;clasificaci&amp;oacute;n&lt;/a&gt; . Para la clasificaci&amp;oacute;n SVC, estamos interesados ​​en una minimizaci&amp;oacute;n de riesgos para la ecuaci&amp;oacute;n:</target>
        </trans-unit>
        <trans-unit id="35227ebb51bdeaa9a401c5e44c0ca9ca35e1f0e6" translate="yes" xml:space="preserve">
          <source>The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">El siguiente ejemplo muestra una representaci&amp;oacute;n codificada por colores de la importancia relativa de cada p&amp;iacute;xel individual para una tarea de reconocimiento facial utilizando un modelo &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e4e3a609b2ed09432a121fc3917b934875c3544c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit an AdaBoost classifier with 100 weak learners:</source>
          <target state="translated">El siguiente ejemplo muestra cómo encajar un clasificador AdaBoost con 100 alumnos débiles:</target>
        </trans-unit>
        <trans-unit id="08beea172467f4b9200a143a3d6a2c2ed164664c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the VotingRegressor:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="486be98f63ccda08601d245e4a85066d3abba297" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the majority rule classifier:</source>
          <target state="translated">El siguiente ejemplo muestra cómo encajar el clasificador de la regla de la mayoría:</target>
        </trans-unit>
        <trans-unit id="6a236adbadcca90d4c1ec977d69fa3c364957c12" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 most informative features in the Friedman #1 dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="376fe34989034d77dfd504e8201c3d4b2dfa1573" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.</source>
          <target state="translated">El siguiente ejemplo muestra cómo recuperar las 5 características informativas correctas en el conjunto de datos de Friedman #1.</target>
        </trans-unit>
        <trans-unit id="25b5efc453e06d92e8572618fefd1930c5c8aab3" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.</source>
          <target state="translated">El siguiente ejemplo muestra cómo recuperar las 5 características informativas a-priori desconocidas en el conjunto de datos de Friedman #1.</target>
        </trans-unit>
        <trans-unit id="e99af0f029f79715308d4d1c9ecbe2acb8983e4d" translate="yes" xml:space="preserve">
          <source>The following example will, for instance, transform some British spelling to American spelling:</source>
          <target state="translated">El siguiente ejemplo,por ejemplo,transformará alguna ortografía británica en ortografía americana:</target>
        </trans-unit>
        <trans-unit id="ad9262c5496a82045b0e3b64071f384beed06659" translate="yes" xml:space="preserve">
          <source>The following experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The figure shows the estimated probabilities obtained with logistic regression, a linear support-vector classifier (SVC), and linear SVC with both isotonic calibration and sigmoid calibration. The Brier score is a metric which is a combination of calibration loss and refinement loss, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt;, reported in the legend (the smaller the better). Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve.</source>
          <target state="translated">El siguiente experimento se realiza en un conjunto de datos artificial para la clasificaci&amp;oacute;n binaria con 100.000 muestras (1.000 de ellas se utilizan para el ajuste del modelo) con 20 caracter&amp;iacute;sticas. De las 20 funciones, solo 2 son informativas y 10 son redundantes. La figura muestra las probabilidades estimadas obtenidas con regresi&amp;oacute;n log&amp;iacute;stica, un clasificador de vectores de soporte lineal (SVC) y SVC lineal con calibraci&amp;oacute;n isot&amp;oacute;nica y calibraci&amp;oacute;n sigmoidea. La puntuaci&amp;oacute;n de Brier es una m&amp;eacute;trica que es una combinaci&amp;oacute;n de p&amp;eacute;rdida de calibraci&amp;oacute;n y p&amp;eacute;rdida de refinamiento, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt;, informado en la leyenda (cuanto m&amp;aacute;s peque&amp;ntilde;o, mejor). La p&amp;eacute;rdida de calibraci&amp;oacute;n se define como la desviaci&amp;oacute;n cuadr&amp;aacute;tica media de las probabilidades emp&amp;iacute;ricas derivadas de la pendiente de los segmentos ROC. La p&amp;eacute;rdida por refinamiento se puede definir como la p&amp;eacute;rdida &amp;oacute;ptima esperada medida por el &amp;aacute;rea bajo la curva de costo &amp;oacute;ptimo.</target>
        </trans-unit>
        <trans-unit id="7e7b7eaddffc735bc9fdc4de0de59e2218717940" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approximately seven times faster than fitting &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; since it has learned a sparse model using only approximately 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b3bbe94b0947452c8fc360afc3d0e6f8832ae22" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approx. seven times faster than fitting &lt;code&gt;SVR&lt;/code&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">La siguiente figura compara &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; y &lt;code&gt;SVR&lt;/code&gt; en un conjunto de datos artificial, que consiste en una funci&amp;oacute;n objetivo sinusoidal y un fuerte ruido agregado a cada quinto punto de datos. Se &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; modelo aprendido de KernelRidge y &lt;code&gt;SVR&lt;/code&gt; , donde tanto la complejidad / regularizaci&amp;oacute;n como el ancho de banda del kernel RBF se han optimizado mediante la b&amp;uacute;squeda en cuadr&amp;iacute;cula. Las funciones aprendidas son muy similares; sin embargo, la instalaci&amp;oacute;n de &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; es de aprox. siete veces m&amp;aacute;s r&amp;aacute;pido que ajustar &lt;code&gt;SVR&lt;/code&gt; (ambos con b&amp;uacute;squeda de cuadr&amp;iacute;cula). Sin embargo, la predicci&amp;oacute;n de 100000 valores objetivo es m&amp;aacute;s de tres veces m&amp;aacute;s r&amp;aacute;pida con SVR ya que ha aprendido un modelo escaso usando solo aprox. 1/3 de los 100 puntos de datos de entrenamiento como vectores de soporte.</target>
        </trans-unit>
        <trans-unit id="7f576baedfe43e5a873ff30c0a921c0894cd8a99" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zero entries in the coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yield scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b11b5ffd2f0d8dbd878722867b9b243b2e73a5ea" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zeros in W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yields scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">La siguiente figura compara la ubicación de los no-ceros en W obtenidos con un simple Lasso o un MultiTaskLasso.El Lazo estima que produce no-ceros dispersos mientras que los no-ceros del Lazo Multitarea son columnas completas.</target>
        </trans-unit>
        <trans-unit id="c66df17d5ba5efe635efde7f93de1fc62e75c222" translate="yes" xml:space="preserve">
          <source>The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">La siguiente figura ilustra ambos m&amp;eacute;todos en un conjunto de datos artificial, que consta de una funci&amp;oacute;n objetivo sinusoidal y ruido fuerte. La figura compara el modelo aprendido de KRR y GPR basado en un kernel ExpSineSquared, que es adecuado para aprender funciones peri&amp;oacute;dicas. Los hiperpar&amp;aacute;metros del kernel controlan la suavidad (length_scale) y la periodicidad del kernel (periodicidad). Adem&amp;aacute;s, el nivel de ruido de los datos es aprendido expl&amp;iacute;citamente por GPR por un componente adicional de WhiteKernel en el kernel y por el par&amp;aacute;metro de regularizaci&amp;oacute;n alfa de KRR.</target>
        </trans-unit>
        <trans-unit id="7af807be2b2e68501b3d7cc153c6244aca835299" translate="yes" xml:space="preserve">
          <source>The following guide focuses on &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, which might be preferred for small sample sizes since binning may lead to split points that are too approximate in this setting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2a01fd3fe0b5b9cd28910783764aacd71509b19" translate="yes" xml:space="preserve">
          <source>The following illustrate the probability density functions of the target before and after applying the logarithmic functions.</source>
          <target state="translated">A continuación se ilustran las funciones de densidad de probabilidad del objetivo antes y después de aplicar las funciones logarítmicas.</target>
        </trans-unit>
        <trans-unit id="b0293fdb9cc21af9db21d1f9f61ccde2a4565147" translate="yes" xml:space="preserve">
          <source>The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.</source>
          <target state="translated">La siguiente imagen muestra cómo se ve un diccionario aprendido de parches de imagen de 4x4 píxeles extraídos de parte de la imagen de la cara de un mapache.</target>
        </trans-unit>
        <trans-unit id="a988a0a50289dff96522065b976c597b6e4e63e8" translate="yes" xml:space="preserve">
          <source>The following image shows on the data above the estimated probability using a Gaussian naive Bayes classifier without calibration, with a sigmoid calibration and with a non-parametric isotonic calibration. One can observe that the non-parametric model provides the most accurate probability estimates for samples in the middle, i.e., 0.5.</source>
          <target state="translated">La siguiente imagen muestra en los datos anteriores la probabilidad estimada utilizando un clasificador gaussiano ingenuo de Bayes sin calibración,con una calibración sigmoide y con una calibración isotónica no paramétrica.Se puede observar que el modelo no paramétrico proporciona las estimaciones de probabilidad más precisas para las muestras en el medio,es decir,0,5.</target>
        </trans-unit>
        <trans-unit id="b45c55863d114ed9459a8475837831820ff65766" translate="yes" xml:space="preserve">
          <source>The following images demonstrate the benefit of probability calibration. The first image present a dataset with 2 classes and 3 blobs of data. The blob in the middle contains random samples of each class. The probability for the samples in this blob should be 0.5.</source>
          <target state="translated">Las siguientes imágenes demuestran el beneficio de la calibración de probabilidad.La primera imagen presenta un conjunto de datos con 2 clases y 3 bloques de datos.La mancha del medio contiene muestras aleatorias de cada clase.La probabilidad de las muestras en esta mancha debe ser de 0,5.</target>
        </trans-unit>
        <trans-unit id="ba49479bf1cc39dce48fe5a925ee39da6796af0a" translate="yes" xml:space="preserve">
          <source>The following lists the string metric identifiers and the associated distance metric classes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="807c0e6975f025a025e2951d9c9164c1f3454c71" translate="yes" xml:space="preserve">
          <source>The following loss functions are supported and can be specified using the parameter &lt;code&gt;loss&lt;/code&gt;:</source>
          <target state="translated">Las siguientes funciones de p&amp;eacute;rdida son compatibles y se pueden especificar mediante el par&amp;aacute;metro &lt;code&gt;loss&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="f56ebb3690526678e7e0211eddd43dd69c89de3e" translate="yes" xml:space="preserve">
          <source>The following parts are parallelized:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69eaa3387fa8263cde077e4bb7511faccc7173e0" translate="yes" xml:space="preserve">
          <source>The following plot compares how well the probabilistic predictions of different classifiers are calibrated, using &lt;a href=&quot;generated/sklearn.calibration.calibration_curve#sklearn.calibration.calibration_curve&quot;&gt;&lt;code&gt;calibration_curve&lt;/code&gt;&lt;/a&gt;. The x axis represents the average predicted probability in each bin. The y axis is the &lt;em&gt;fraction of positives&lt;/em&gt;, i.e. the proportion of samples whose class is the positive class (in each bin).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70624e811991b8fac875c2f0dd0db6a8fb2e1e2b" translate="yes" xml:space="preserve">
          <source>The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.</source>
          <target state="translated">Los siguientes gráficos demuestran el impacto del número de conglomerados y el número de muestras en varias métricas de evaluación del rendimiento de los conglomerados.</target>
        </trans-unit>
        <trans-unit id="661cd7d1cca4025c0fd4f4a1e938e7140f6ef24b" translate="yes" xml:space="preserve">
          <source>The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; differ slightly from the standard textbook notation that defines the idf as</source>
          <target state="translated">Las siguientes secciones contienen m&amp;aacute;s explicaciones y ejemplos que ilustran c&amp;oacute;mo la carretera TF-IDF se calculan exactamente y c&amp;oacute;mo la carretera TF-IDF computados en la scikit-learn &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt; y &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; &lt;/a&gt; difieren ligeramente de la notaci&amp;oacute;n de libro de texto est&amp;aacute;ndar que define la FDI como</target>
        </trans-unit>
        <trans-unit id="78ad5f101e233b65633dffc5c68bec29c1eae0b0" translate="yes" xml:space="preserve">
          <source>The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.</source>
          <target state="translated">En las siguientes secciones se enumeran las utilidades para generar índices que pueden utilizarse para generar divisiones de conjuntos de datos de acuerdo con diferentes estrategias de validación cruzada.</target>
        </trans-unit>
        <trans-unit id="999c74375c41b701ce62b520b704cf4b739a66b1" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean feature value of the two nearest neighbors of samples with missing values:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c75d78bf5e5b93a438a72e22f9eee2db09a82ed" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean value of the columns (axis 0) that contain the missing values:</source>
          <target state="translated">El siguiente fragmento muestra c&amp;oacute;mo reemplazar los valores perdidos, codificados como &lt;code&gt;np.nan&lt;/code&gt; , utilizando el valor medio de las columnas (eje 0) que contienen los valores perdidos:</target>
        </trans-unit>
        <trans-unit id="35e4b9d817818dd84b86f006c91682ff56db6021" translate="yes" xml:space="preserve">
          <source>The following subsections are only rough guidelines: the same estimator can fall into multiple categories, depending on its parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6c9fc0377f0a1e8adb0ffabc9be6ea63e8b1dd6" translate="yes" xml:space="preserve">
          <source>The following table lists some specific EDMs and their unit deviance (all of these are instances of the Tweedie family):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8aa5b699e7b8122b521ff7ffa5779f4500a14f28" translate="yes" xml:space="preserve">
          <source>The following table summarizes the penalties supported by each solver:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d55ab5466e0380dae4e49e3b2bfc5d99624d130" translate="yes" xml:space="preserve">
          <source>The following toy example demonstrates how the model ignores the samples with zero sample weights:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46781e9d9b616f25e94b9f1718ce2c06cffa693e" translate="yes" xml:space="preserve">
          <source>The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.</source>
          <target state="translated">Las dos referencias siguientes explican las iteraciones utilizadas en el solucionador de descenso de coordenadas de scikit-learn,así como el cálculo de la brecha de dualidad utilizado para el control de la convergencia.</target>
        </trans-unit>
        <trans-unit id="5c7ef4cd14485e27e01a8eae72d8f974a84e41ab" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d492404a63c6d65da32b70e6dcf87e75a94b6a4d" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;code&gt;SVR&lt;/code&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;code&gt;SVR&lt;/code&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">La forma del modelo aprendido por &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; es id&amp;eacute;ntica para soportar la regresi&amp;oacute;n vectorial ( &lt;code&gt;SVR&lt;/code&gt; ). Sin embargo, se usan diferentes funciones de p&amp;eacute;rdida: KRR usa p&amp;eacute;rdida de error al cuadrado mientras que la regresi&amp;oacute;n de vector de soporte usa \ (\ epsilon \) - p&amp;eacute;rdida insensible, ambas combinadas con regularizaci&amp;oacute;n l2. A diferencia de &lt;code&gt;SVR&lt;/code&gt; , el ajuste de &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; se puede realizar en forma cerrada y suele ser m&amp;aacute;s r&amp;aacute;pido para conjuntos de datos de tama&amp;ntilde;o mediano. Por otro lado, el modelo aprendido no es escaso y, por lo tanto, m&amp;aacute;s lento que SVR, que aprende un modelo escaso para \ (\ epsilon&amp;gt; 0 \), en el tiempo de predicci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="cb6eb14fd1b6ffac38e7d6c15d36b7076beafa85" translate="yes" xml:space="preserve">
          <source>The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon &amp;gt; 0, at prediction-time.</source>
          <target state="translated">La forma del modelo aprendido por KRR es id&amp;eacute;ntica al soporte de regresi&amp;oacute;n vectorial (SVR). Sin embargo, se utilizan diferentes funciones de p&amp;eacute;rdida: KRR utiliza la p&amp;eacute;rdida por error al cuadrado, mientras que la regresi&amp;oacute;n de vectores de soporte utiliza la p&amp;eacute;rdida insensible a &amp;eacute;psilon, ambas combinadas con la regularizaci&amp;oacute;n l2. A diferencia de SVR, el ajuste de un modelo KRR se puede realizar en forma cerrada y suele ser m&amp;aacute;s r&amp;aacute;pido para conjuntos de datos de tama&amp;ntilde;o mediano. Por otro lado, el modelo aprendido no es escaso y, por lo tanto, m&amp;aacute;s lento que SVR, que aprende un modelo escaso para &amp;eacute;psilon&amp;gt; 0, en el tiempo de predicci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="785bfd19f7d4d298d07ec2673131bad97bee7c7c" translate="yes" xml:space="preserve">
          <source>The form of these kernels is as follows:</source>
          <target state="translated">La forma de estos núcleos es la siguiente:</target>
        </trans-unit>
        <trans-unit id="cecc632cfa798838e6560a2f37e713a3ae87c1a6" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as idf(t) = log [ n / df(t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99fa4cbad7a403537a73165a2b1d717c58bb9b6b" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).</source>
          <target state="translated">La f&amp;oacute;rmula que se utiliza para calcular el tf-idf del t&amp;eacute;rmino t es tf-idf (d, t) = tf (t) * idf (d, t), y el idf se calcula como idf (d, t) = log [n / df (d, t)] + 1 (si &lt;code&gt;smooth_idf=False&lt;/code&gt; ), donde n es el n&amp;uacute;mero total de documentos y df (d, t) es la frecuencia del documento; la frecuencia del documento es el n&amp;uacute;mero de documentos d que contienen el t&amp;eacute;rmino t. El efecto de agregar &quot;1&quot; al idf en la ecuaci&amp;oacute;n anterior es que los t&amp;eacute;rminos con idf cero, es decir, los t&amp;eacute;rminos que aparecen en todos los documentos de un conjunto de entrenamiento, no se ignorar&amp;aacute;n por completo. (Tenga en cuenta que la f&amp;oacute;rmula idf anterior difiere de la notaci&amp;oacute;n est&amp;aacute;ndar de los libros de texto que define la idf como idf (d, t) = log [n / (df (d, t) + 1)]).</target>
        </trans-unit>
        <trans-unit id="2757038bccf5826fff274cfe8684f779a3893302" translate="yes" xml:space="preserve">
          <source>The formula used here does not correspond to the one given in the article. In the original article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn&amp;rsquo;t affect the value of the estimator.</source>
          <target state="translated">La f&amp;oacute;rmula utilizada aqu&amp;iacute; no corresponde a la dada en el art&amp;iacute;culo. En el art&amp;iacute;culo original, la f&amp;oacute;rmula (23) establece que 2 / p se multiplica por Trace (cov * cov) tanto en el numerador como en el denominador, pero esta operaci&amp;oacute;n se omite porque para una p grande, el valor de 2 / p es muy peque&amp;ntilde;o que no afecta el valor del estimador.</target>
        </trans-unit>
        <trans-unit id="25426ebad0f81610af376f208a1d7f7cdb7c96c5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. &lt;code&gt;subsample&lt;/code&gt; interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;. Choosing &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; leads to a reduction of variance and an increase in bias.</source>
          <target state="translated">La fracci&amp;oacute;n de muestras que se utilizar&amp;aacute; para ajustar a los alumnos de base individuales. Si es menor que 1.0, esto da como resultado un aumento de gradiente estoc&amp;aacute;stico. &lt;code&gt;subsample&lt;/code&gt; interact&amp;uacute;a con el par&amp;aacute;metro &lt;code&gt;n_estimators&lt;/code&gt; . La elecci&amp;oacute;n de una &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; conduce a una reducci&amp;oacute;n de la varianza y un aumento del sesgo.</target>
        </trans-unit>
        <trans-unit id="568fbbbfe83b2ef390104a99310641880cb62ce5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.</source>
          <target state="translated">La fracción de muestras que se utilizará en cada diseño aleatorio.Debe estar entre 0 y 1.Si es 1,se utilizan todas las muestras.</target>
        </trans-unit>
        <trans-unit id="52b16a6d5efc38a8b1d594773d860718286a7706" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class are randomly exchanged. Larger values introduce noise in the labels and make the classification task harder.</source>
          <target state="translated">La fracción de muestras cuya clase se intercambian al azar.Los valores más grandes introducen ruido en las etiquetas y hacen más difícil la tarea de clasificación.</target>
        </trans-unit>
        <trans-unit id="3e4c940191ac2ce629537cccbeb4e1290fc15133" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder. Note that the default setting flip_y &amp;gt; 0 might lead to less than n_classes in y in some cases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b8586795acd1de1d253165ce5ff74dbee8020e0" translate="yes" xml:space="preserve">
          <source>The free parameters in the model are C and epsilon.</source>
          <target state="translated">Los parámetros libres en el modelo son C y épsilon.</target>
        </trans-unit>
        <trans-unit id="852901dbf93c375963721fc84a4888e6b45c78dd" translate="yes" xml:space="preserve">
          <source>The full description of the dataset</source>
          <target state="translated">La descripción completa del conjunto de datos</target>
        </trans-unit>
        <trans-unit id="d9127a02d65d598dd00fad372a9aa50b138962b6" translate="yes" xml:space="preserve">
          <source>The full description of the dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19877feb57bb09f6d25fbaaa466f42d45ca4d944" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt;&lt;code&gt;img_to_graph&lt;/code&gt;&lt;/a&gt; returns such a matrix from a 2D or 3D image. Similarly, &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;grid_to_graph&lt;/code&gt;&lt;/a&gt; build a connectivity matrix for images given the shape of these image.</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt; &lt;code&gt;img_to_graph&lt;/code&gt; &lt;/a&gt; devuelve dicha matriz a partir de una imagen 2D o 3D. De manera similar, &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;grid_to_graph&lt;/code&gt; &lt;/a&gt; construye una matriz de conectividad para im&amp;aacute;genes dada la forma de estas im&amp;aacute;genes.</target>
        </trans-unit>
        <trans-unit id="0a9dad7d233dabf5b0bcafb3e330d5014a951e15" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt;&lt;code&gt;lasso_path&lt;/code&gt;&lt;/a&gt; is useful for lower-level tasks, as it computes the coefficients along the full path of possible values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bbdd98a7d5d87ddd3bbdfb8569481a5e5e04495" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt;&lt;code&gt;cohen_kappa_score&lt;/code&gt;&lt;/a&gt; computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen&amp;rsquo;s kappa&lt;/a&gt; statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt; &lt;code&gt;cohen_kappa_score&lt;/code&gt; &lt;/a&gt; calcula &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;la&lt;/a&gt; estad&amp;iacute;stica kappa de Cohen . Esta medida tiene como objetivo comparar las etiquetas de diferentes anotadores humanos, no un clasificador versus una verdad b&amp;aacute;sica.</target>
        </trans-unit>
        <trans-unit id="93f21bf6b976a0d8004246b9140ac3f9b0310fba" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt;&lt;code&gt;laplacian_kernel&lt;/code&gt;&lt;/a&gt; is a variant on the radial basis function kernel defined as:</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt; &lt;code&gt;laplacian_kernel&lt;/code&gt; &lt;/a&gt; es una variante del kernel de la funci&amp;oacute;n de base radial definida como:</target>
        </trans-unit>
        <trans-unit id="3fe1ad7d2ae4a1d31d30a7b5c63e81d53b7568bb" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt; computes the linear kernel, that is, a special case of &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;degree=1&lt;/code&gt; and &lt;code&gt;coef0=0&lt;/code&gt; (homogeneous). If &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are column vectors, their linear kernel is:</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt; calcula el kernel lineal, es decir, un caso especial de &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt; con &lt;code&gt;degree=1&lt;/code&gt; y &lt;code&gt;coef0=0&lt;/code&gt; (homog&amp;eacute;neo). Si &lt;code&gt;x&lt;/code&gt; y &lt;code&gt;y&lt;/code&gt; son vectores de columna, su n&amp;uacute;cleo lineal es:</target>
        </trans-unit>
        <trans-unit id="72295044331d14363e1e1fe37cea504256d7c67f" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt; calcula el kernel polinomial de grado d entre dos vectores. El n&amp;uacute;cleo polinomial representa la similitud entre dos vectores. Conceptualmente, los n&amp;uacute;cleos polinomiales consideran no solo la similitud entre vectores bajo la misma dimensi&amp;oacute;n, sino tambi&amp;eacute;n entre dimensiones. Cuando se usa en algoritmos de aprendizaje autom&amp;aacute;tico, esto permite tener en cuenta la interacci&amp;oacute;n de funciones.</target>
        </trans-unit>
        <trans-unit id="2f5b051b13aa3478b0b4a17587fa0bec1e6b87b4" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt;&lt;code&gt;rbf_kernel&lt;/code&gt;&lt;/a&gt; computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt; &lt;code&gt;rbf_kernel&lt;/code&gt; &lt;/a&gt; calcula el n&amp;uacute;cleo de la funci&amp;oacute;n de base radial (RBF) entre dos vectores. Este n&amp;uacute;cleo se define como:</target>
        </trans-unit>
        <trans-unit id="a4ac87fe4bd82908551f017a1e984b845dfe00ca" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt;&lt;code&gt;sigmoid_kernel&lt;/code&gt;&lt;/a&gt; computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt; &lt;code&gt;sigmoid_kernel&lt;/code&gt; &lt;/a&gt; calcula el kernel sigmoide entre dos vectores. El n&amp;uacute;cleo sigmoide tambi&amp;eacute;n se conoce como tangente hiperb&amp;oacute;lica o perceptr&amp;oacute;n multicapa (porque, en el campo de la red neuronal, se utiliza a menudo como funci&amp;oacute;n de activaci&amp;oacute;n neuronal). Se define como:</target>
        </trans-unit>
        <trans-unit id="2e327cd188c9064345dfa4a6a6764985ae429bc7" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;receiver operating characteristic curve, or ROC curve&lt;/a&gt;. Quoting Wikipedia :</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt; calcula la &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;curva caracter&amp;iacute;stica de funcionamiento&lt;/a&gt; del receptor, o curva ROC . Citando Wikipedia:</target>
        </trans-unit>
        <trans-unit id="a52cb65c85990e02ab77ee98f8132a1c3b7a7b6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; has a similar interface to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; tiene una interfaz similar a &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; , pero devuelve, para cada elemento en la entrada, la predicci&amp;oacute;n que se obtuvo para ese elemento cuando estaba en el conjunto de prueba. Solo se pueden usar las estrategias de validaci&amp;oacute;n cruzada que asignan todos los elementos a un conjunto de prueba exactamente una vez (de lo contrario, se genera una excepci&amp;oacute;n).</target>
        </trans-unit>
        <trans-unit id="9da333e152484b3ac872458a2a577c489d5042de" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt;&lt;code&gt;validation_curve&lt;/code&gt;&lt;/a&gt; can help in this case:</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt; &lt;code&gt;validation_curve&lt;/code&gt; &lt;/a&gt; puede ayudar en este caso:</target>
        </trans-unit>
        <trans-unit id="ef1bfedb8d96897b52fb746234adf67b6ec8f0b2" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;normalize&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset, either using the &lt;code&gt;l1&lt;/code&gt; or &lt;code&gt;l2&lt;/code&gt; norms:</source>
          <target state="translated">La funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt; &lt;code&gt;normalize&lt;/code&gt; &lt;/a&gt; proporciona una manera r&amp;aacute;pida y f&amp;aacute;cil de realizar esta operaci&amp;oacute;n en un &amp;uacute;nico conjunto de datos similar a una matriz, ya sea usando las normas &lt;code&gt;l1&lt;/code&gt; o &lt;code&gt;l2&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="621182b188f45e3dd8884ee1d03015ee530041c8" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset:</source>
          <target state="translated">La funci&amp;oacute;n de &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt; proporciona una forma r&amp;aacute;pida y sencilla de realizar esta operaci&amp;oacute;n en un &amp;uacute;nico conjunto de datos similar a una matriz:</target>
        </trans-unit>
        <trans-unit id="6ffd4cf30ad5a8e8b6ec54710ec8f9345471233e" translate="yes" xml:space="preserve">
          <source>The function &lt;code&gt;plot_regression_results&lt;/code&gt; is used to plot the predicted and true targets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14656519a034ad24c06a19460128dde85e00e72d" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">La funci&amp;oacute;n se basa en m&amp;eacute;todos no param&amp;eacute;tricos basados ​​en la estimaci&amp;oacute;n de entrop&amp;iacute;a a partir de distancias de los vecinos m&amp;aacute;s cercanos k como se describe en &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; y &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt; . Ambos m&amp;eacute;todos se basan en la idea propuesta originalmente en &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="cf1d60e35345e80734f9e0bef9342c1dede910a1" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">La funci&amp;oacute;n se basa en m&amp;eacute;todos no param&amp;eacute;tricos basados ​​en la estimaci&amp;oacute;n de entrop&amp;iacute;a a partir de distancias de los vecinos m&amp;aacute;s cercanos k como se describe en &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; y &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt; . Ambos m&amp;eacute;todos se basan en la idea propuesta originalmente en &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="fb8314efd5d7c36c4d228413206cd5331b82e456" translate="yes" xml:space="preserve">
          <source>The function requires either the argument &lt;code&gt;grid&lt;/code&gt; which specifies the values of the target features on which the partial dependence function should be evaluated or the argument &lt;code&gt;X&lt;/code&gt; which is a convenience mode for automatically creating &lt;code&gt;grid&lt;/code&gt; from the training data. If &lt;code&gt;X&lt;/code&gt; is given, the &lt;code&gt;axes&lt;/code&gt; value returned by the function gives the axis for each target feature.</source>
          <target state="translated">La funci&amp;oacute;n requiere la &lt;code&gt;grid&lt;/code&gt; argumentos que especifica los valores de las caracter&amp;iacute;sticas de destino en las que se debe evaluar la funci&amp;oacute;n de dependencia parcial o el argumento &lt;code&gt;X&lt;/code&gt; , que es un modo de conveniencia para crear autom&amp;aacute;ticamente una &lt;code&gt;grid&lt;/code&gt; partir de los datos de entrenamiento. Si se proporciona &lt;code&gt;X&lt;/code&gt; , el valor de los &lt;code&gt;axes&lt;/code&gt; devuelto por la funci&amp;oacute;n proporciona el eje para cada caracter&amp;iacute;stica de destino.</target>
        </trans-unit>
        <trans-unit id="d2fedea237bdc334f6322c4692b523df3b5f6fbf" translate="yes" xml:space="preserve">
          <source>The function to be decorated</source>
          <target state="translated">La función a ser decorada</target>
        </trans-unit>
        <trans-unit id="f7fc9606332ff37416d86060e52aa56595a5e3ce" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;friedman_mse&amp;rdquo; for the mean squared error with improvement score by Friedman, &amp;ldquo;mse&amp;rdquo; for mean squared error, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error. The default value of &amp;ldquo;friedman_mse&amp;rdquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">La funci&amp;oacute;n para medir la calidad de una divisi&amp;oacute;n. Los criterios admitidos son &quot;friedman_mse&quot; para el error cuadr&amp;aacute;tico medio con puntuaci&amp;oacute;n de mejora de Friedman, &quot;mse&quot; para el error cuadr&amp;aacute;tico medio y &quot;mae&quot; para el error absoluto medio. El valor predeterminado de &quot;friedman_mse&quot; es generalmente el mejor, ya que puede proporcionar una mejor aproximaci&amp;oacute;n en algunos casos.</target>
        </trans-unit>
        <trans-unit id="8c0bede693f27f1257cedfa964bd89180e6bfada" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain.</source>
          <target state="translated">La funci&amp;oacute;n para medir la calidad de una divisi&amp;oacute;n. Los criterios admitidos son &quot;gini&quot; para la impureza de Gini y &quot;entrop&amp;iacute;a&quot; para la ganancia de informaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="03e746b06910ee8c07bc239c0b780e5294140dfb" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain. Note: this parameter is tree-specific.</source>
          <target state="translated">La funci&amp;oacute;n para medir la calidad de una divisi&amp;oacute;n. Los criterios admitidos son &quot;gini&quot; para la impureza de Gini y &quot;entrop&amp;iacute;a&quot; para la ganancia de informaci&amp;oacute;n. Nota: este par&amp;aacute;metro es espec&amp;iacute;fico del &amp;aacute;rbol.</target>
        </trans-unit>
        <trans-unit id="c211c5fb9289c19b7ab7fb609c5f42d0fd7fb417" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, &amp;ldquo;friedman_mse&amp;rdquo;, which uses mean squared error with Friedman&amp;rsquo;s improvement score for potential splits, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.</source>
          <target state="translated">La funci&amp;oacute;n para medir la calidad de una divisi&amp;oacute;n. Los criterios admitidos son &quot;mse&quot; para el error cuadr&amp;aacute;tico medio, que es igual a la reducci&amp;oacute;n de la varianza como criterio de selecci&amp;oacute;n de caracter&amp;iacute;sticas y minimiza la p&amp;eacute;rdida L2 utilizando la media de cada nodo terminal, &quot;friedman_mse&quot;, que usa el error cuadr&amp;aacute;tico medio con la puntuaci&amp;oacute;n de mejora de Friedman para el potencial splits y &quot;mae&quot; para el error absoluto medio, lo que minimiza la p&amp;eacute;rdida L1 utilizando la mediana de cada nodo terminal.</target>
        </trans-unit>
        <trans-unit id="bf422f8c78875448c6e34d2c008baf4a5621c47d" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error.</source>
          <target state="translated">La funci&amp;oacute;n para medir la calidad de una divisi&amp;oacute;n. Los criterios admitidos son &quot;mse&quot; para el error cuadr&amp;aacute;tico medio, que es igual a la reducci&amp;oacute;n de la varianza como criterio de selecci&amp;oacute;n de caracter&amp;iacute;sticas, y &quot;mae&quot; para el error absoluto medio.</target>
        </trans-unit>
        <trans-unit id="8599be908b4b8dfee70cafa139c14ed8be2f9fde" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;lsquo;friedman_mse&amp;rsquo; for the mean squared error with improvement score by Friedman, &amp;lsquo;mse&amp;rsquo; for mean squared error, and &amp;lsquo;mae&amp;rsquo; for the mean absolute error. The default value of &amp;lsquo;friedman_mse&amp;rsquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b3c9548b1b42f5af71c7bfe5f885c712284a1e6" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;, or a tuple of such objects.</source>
          <target state="translated">La funci&amp;oacute;n que se aplica en cada fragmento de la matriz de distancia, reduci&amp;eacute;ndola a los valores necesarios. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; se llama repetidamente, donde &lt;code&gt;D_chunk&lt;/code&gt; es un corte vertical contiguo de la matriz de distancias por pares, comenzando en el &lt;code&gt;start&lt;/code&gt; fila . Deber&amp;iacute;a devolver una matriz, una lista o una matriz dispersa de longitud &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; , o una tupla de tales objetos.</target>
        </trans-unit>
        <trans-unit id="0114f911a9d8b12f31f5da2c60626b44d9e0ba26" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return one of: None; an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;; or a tuple of such objects. Returning None is useful for in-place operations, rather than reductions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34f983a96e6e1ba7a36ae52fb70df739e3a5aca9" translate="yes" xml:space="preserve">
          <source>The functional form of the G function used in the approximation to neg-entropy. Could be either &amp;lsquo;logcosh&amp;rsquo;, &amp;lsquo;exp&amp;rsquo;, or &amp;lsquo;cube&amp;rsquo;. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:</source>
          <target state="translated">La forma funcional de la funci&amp;oacute;n G utilizada en la aproximaci&amp;oacute;n a la neg-entrop&amp;iacute;a. Podr&amp;iacute;a ser 'logcosh', 'exp' o 'cube'. Tambi&amp;eacute;n puede proporcionar su propia funci&amp;oacute;n. Deber&amp;iacute;a devolver una tupla que contenga el valor de la funci&amp;oacute;n y de su derivada en el punto. Ejemplo:</target>
        </trans-unit>
        <trans-unit id="f3580ae3ff67a44925f6075f1fc46034cd1f3d14" translate="yes" xml:space="preserve">
          <source>The generated array.</source>
          <target state="translated">La matriz generada.</target>
        </trans-unit>
        <trans-unit id="22dd341f8c92381f47326cd6fbe4fff46dd59a6b" translate="yes" xml:space="preserve">
          <source>The generated matrix.</source>
          <target state="translated">La matriz generada.</target>
        </trans-unit>
        <trans-unit id="b670a7959baa25f4a9c290d5df951114cede8bad" translate="yes" xml:space="preserve">
          <source>The generated samples.</source>
          <target state="translated">Las muestras generadas.</target>
        </trans-unit>
        <trans-unit id="61a0705074aa05bf429a2ff6dd9f09842d1c86f2" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">El generador utilizado para inicializar los centros. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="17ea6c727e52953fb5e03ae24f1b3fcbc132ab70" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="473c7ed23812d4bf8fcf6c97ab0551b7301ccf95" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">El generador utilizado para inicializar el libro de c&amp;oacute;digos. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a0fb8deb3b7c972152c6c4cb8a6e10d0c386fcd5" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e831547e83750929880db075c40fc4aa61eb4b8" translate="yes" xml:space="preserve">
          <source>The generator used to randomize the design. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">El generador utilizado para aleatorizar el dise&amp;ntilde;o. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cd84f22d9e60e3f86bb2455d78aaf109647d76b1" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select a subset of samples. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;sample_size is not None&lt;/code&gt;.</source>
          <target state="translated">El generador utilizado para seleccionar aleatoriamente un subconjunto de muestras. Si es int, random_state es la semilla usada por el generador de n&amp;uacute;meros aleatorios; Si es una instancia de RandomState, random_state es el generador de n&amp;uacute;meros aleatorios; Si es None, el generador de n&amp;uacute;meros aleatorios es la instancia de RandomState utilizada por &lt;code&gt;np.random&lt;/code&gt; . Se usa cuando &lt;code&gt;sample_size is not None&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="36aa9d7d033388d006150bbc613bf9f61afb2f31" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">El generador utilizado para seleccionar aleatoriamente las muestras de los puntos de entrada para la estimaci&amp;oacute;n del ancho de banda. Utilice un int para hacer que la aleatoriedad sea determinista. Consulte el &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;glosario&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="eebbb245bcabbcd7cdd05b74aa699ac85737839b" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff752b880533b887a66601fd8d804091da8b44b7" translate="yes" xml:space="preserve">
          <source>The goal is to compare different estimators to see which one is best for the &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt; when using a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt;&lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt;&lt;/a&gt; estimator on the California housing dataset with a single value randomly removed from each row.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c9c479a6511fc655c70a7c51ab0a85c04034b21" translate="yes" xml:space="preserve">
          <source>The goal is to measure the latency one can expect when doing predictions either in bulk or atomic (i.e. one by one) mode.</source>
          <target state="translated">El objetivo es medir la latencia que se puede esperar cuando se hacen predicciones,ya sea a granel o en modo atómico (es decir,uno por uno).</target>
        </trans-unit>
        <trans-unit id="95b3715b7309073ce23a5e1ae6bd1c83fb5ab128" translate="yes" xml:space="preserve">
          <source>The goal of &lt;strong&gt;ensemble methods&lt;/strong&gt; is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</source>
          <target state="translated">El objetivo de los &lt;strong&gt;m&amp;eacute;todos&lt;/strong&gt; de &lt;strong&gt;conjunto&lt;/strong&gt; es combinar las predicciones de varios estimadores base construidos con un algoritmo de aprendizaje dado para mejorar la generalizaci&amp;oacute;n / robustez sobre un solo estimador.</target>
        </trans-unit>
        <trans-unit id="a736cc9a27a226bf9910cdd3892bdfd0fe07d378" translate="yes" xml:space="preserve">
          <source>The goal of NCA is to learn an optimal linear transformation matrix of size &lt;code&gt;(n_components, n_features)&lt;/code&gt;, which maximises the sum over all samples \(i\) of the probability \(p_i\) that \(i\) is correctly classified, i.e.:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4be143708c968fbd1f8e4e8288622de6f1251f81" translate="yes" xml:space="preserve">
          <source>The goal of this example is to analyze the graph of links inside wikipedia articles to rank articles by relative importance according to this eigenvector centrality.</source>
          <target state="translated">El objetivo de este ejemplo es analizar el gráfico de los enlaces dentro de los artículos de la wikipedia para clasificar los artículos por su importancia relativa según esta centralidad eigenvectorial.</target>
        </trans-unit>
        <trans-unit id="5095c3c5239296131fc48d25860f3771ab9b91e3" translate="yes" xml:space="preserve">
          <source>The goal of this example is to show intuitively how the metrics behave, and not to find good clusters for the digits. This is why the example works on a 2D embedding.</source>
          <target state="translated">El objetivo de este ejemplo es mostrar intuitivamente cómo se comportan las métricas,y no encontrar buenas agrupaciones para los dígitos.Por eso el ejemplo funciona en una incrustación 2D.</target>
        </trans-unit>
        <trans-unit id="20bd11aa678c9e04777a40ec0f194ab6b7581aa6" translate="yes" xml:space="preserve">
          <source>The goal of this guide is to explore some of the main &lt;code&gt;scikit-learn&lt;/code&gt; tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics.</source>
          <target state="translated">El objetivo de esta gu&amp;iacute;a es explorar algunas de las principales &lt;code&gt;scikit-learn&lt;/code&gt; en una sola tarea pr&amp;aacute;ctica: analizar una colecci&amp;oacute;n de documentos de texto (publicaciones de grupos de noticias) sobre veinte temas diferentes.</target>
        </trans-unit>
        <trans-unit id="c224302bdf144aacfdfc260a6638fe1107c2e8d4" translate="yes" xml:space="preserve">
          <source>The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</source>
          <target state="translated">El objetivo de utilizar tf-idf en lugar de las frecuencias brutas de ocurrencia de una ficha en un documento determinado es reducir el impacto de las fichas que se producen con mucha frecuencia en un corpus determinado y que,por lo tanto,empíricamente son menos informativas que las características que se producen en una pequeña fracción del corpus de formación.</target>
        </trans-unit>
        <trans-unit id="7c4e551c63ec44d66aa3a175c12b51d46fb0ca3e" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when &lt;code&gt;eval_gradient&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3325084d0d02139eb921a2545ba48cff39657720" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</source>
          <target state="translated">El gradiente del núcleo k(X,X)con respecto al hiperparámetro del núcleo.Sólo se devuelve cuando eval_gradient es True.</target>
        </trans-unit>
        <trans-unit id="1d28aeaa2593827c70ee0cbf35f8d5891fd13e08" translate="yes" xml:space="preserve">
          <source>The graph data is fetched from the DBpedia dumps. DBpedia is an extraction of the latent structured data of the Wikipedia content.</source>
          <target state="translated">Los datos del gráfico se obtienen de los vertederos de DBpedia.DBpedia es una extracción de los datos estructurados latentes del contenido de Wikipedia.</target>
        </trans-unit>
        <trans-unit id="94a8641249b9d38c741294dd700ba9c013e60d15" translate="yes" xml:space="preserve">
          <source>The graph should contain only one connect component, elsewhere the results make little sense.</source>
          <target state="translated">El gráfico debe contener sólo un componente de conexión,en otros lugares los resultados tienen poco sentido.</target>
        </trans-unit>
        <trans-unit id="0aca214d3abec7143ce0e3db3703c7759829a517" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level Bayesian model:</source>
          <target state="translated">El modelo gráfico de LDA es un modelo bayesiano de tres niveles:</target>
        </trans-unit>
        <trans-unit id="53c200c93d4342eee587d3c6f82bd52b691b78c8" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level generative model:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b70b496cfa90301063390d7d501e40e03165e505" translate="yes" xml:space="preserve">
          <source>The graphical model of an RBM is a fully-connected bipartite graph.</source>
          <target state="translated">El modelo gráfico de un RBM es un gráfico bipartito totalmente conectado.</target>
        </trans-unit>
        <trans-unit id="43e8165a75805bcff11ce07ae2103365830550d8" translate="yes" xml:space="preserve">
          <source>The grid of &lt;code&gt;target_variables&lt;/code&gt; values for which the partial dependecy should be evaluated (either &lt;code&gt;grid&lt;/code&gt; or &lt;code&gt;X&lt;/code&gt; must be specified).</source>
          <target state="translated">La cuadr&amp;iacute;cula de valores de &lt;code&gt;target_variables&lt;/code&gt; para los que se debe evaluar la dependencia parcial (se debe especificar la &lt;code&gt;grid&lt;/code&gt; o &lt;code&gt;X&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="1fb55e8edd66947697c84e91d6ac3fa247bd29e9" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting</source>
          <target state="translated">La cuadrícula de alfas utilizada para la adaptación</target>
        </trans-unit>
        <trans-unit id="c8ce42c94fb7c4644fc397d481823f721280ec22" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio</source>
          <target state="translated">La cuadrícula de alfas utilizada para el ajuste,para cada l1_ratio</target>
        </trans-unit>
        <trans-unit id="92edf0193fba09badba27bf7739525bce6da7601" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio.</source>
          <target state="translated">La cuadrícula de alfas utilizada para el ajuste,para cada l1_ratio.</target>
        </trans-unit>
        <trans-unit id="22daba72217df04ec1af1a8a340277c45da4b54e" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting.</source>
          <target state="translated">La cuadrícula de alfas utilizada para la adaptación.</target>
        </trans-unit>
        <trans-unit id="37c7f40c413d2aeb11b8645a838ea3c671b821b6" translate="yes" xml:space="preserve">
          <source>The grid points between 0 and 1: alpha/alpha_max</source>
          <target state="translated">La cuadrícula apunta entre 0 y 1:alpha/alpha_max</target>
        </trans-unit>
        <trans-unit id="1e90c7186240eacb6693334f5a2cb603522a3c95" translate="yes" xml:space="preserve">
          <source>The grid search instance behaves like a normal &lt;code&gt;scikit-learn&lt;/code&gt; model. Let&amp;rsquo;s perform the search on a smaller subset of the training data to speed up the computation:</source>
          <target state="translated">La instancia de b&amp;uacute;squeda de cuadr&amp;iacute;cula se comporta como un modelo de &lt;code&gt;scikit-learn&lt;/code&gt; normal . Realicemos la b&amp;uacute;squeda en un subconjunto m&amp;aacute;s peque&amp;ntilde;o de los datos de entrenamiento para acelerar el c&amp;aacute;lculo:</target>
        </trans-unit>
        <trans-unit id="0fb72e7d32db09c24209afb30ebd0c3ef9469a12" translate="yes" xml:space="preserve">
          <source>The grid search provided by &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; exhaustively generates candidates from a grid of parameter values specified with the &lt;code&gt;param_grid&lt;/code&gt; parameter. For instance, the following &lt;code&gt;param_grid&lt;/code&gt;:</source>
          <target state="translated">La b&amp;uacute;squeda de cuadr&amp;iacute;cula proporcionada por &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt; genera exhaustivamente candidatos a partir de una cuadr&amp;iacute;cula de valores de par&amp;aacute;metros especificados con el par&amp;aacute;metro &lt;code&gt;param_grid&lt;/code&gt; . Por ejemplo, el siguiente &lt;code&gt;param_grid&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="5d877967ec11cd12214237a4561547e48123553d" translate="yes" xml:space="preserve">
          <source>The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.</source>
          <target state="translated">Las directrices para elegir una métrica es utilizar una que maximice la distancia entre las muestras de las diferentes clases,y minimice la que hay dentro de cada clase.</target>
        </trans-unit>
        <trans-unit id="42b20fdbf19bf6fb28b9336fb1c931e7ebd9ff54" translate="yes" xml:space="preserve">
          <source>The handwritten digit dataset has 1797 total points. The model will be trained using all points, but only 30 will be labeled. Results in the form of a confusion matrix and a series of metrics over each class will be very good.</source>
          <target state="translated">El conjunto de datos de dígitos escritos a mano tiene un total de 1797 puntos.El modelo será entrenado usando todos los puntos,pero sólo 30 serán etiquetados.Los resultados en forma de una matriz de confusión y una serie de métricas sobre cada clase serán muy buenos.</target>
        </trans-unit>
        <trans-unit id="1dbebc05eff701a160c5a2149017afcbb50998d6" translate="yes" xml:space="preserve">
          <source>The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">La función de hachís empleada es la versión firmada de 32 bits de Murmurhash3.</target>
        </trans-unit>
        <trans-unit id="86a9e346ee809bf0b27001ea98677db7fe898a9e" translate="yes" xml:space="preserve">
          <source>The higher &lt;code&gt;p&lt;/code&gt; the less weight is given to extreme deviations between true and predicted targets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ddf3e17ce9800f43a67078b41067ddc0d8c6a4c" translate="yes" xml:space="preserve">
          <source>The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex.</source>
          <target state="translated">La mayor concentración pone más masa en el centro y hará que se activen más componentes,mientras que un parámetro de menor concentración hará que haya más masa en el borde del simplex.</target>
        </trans-unit>
        <trans-unit id="d55d0516aec7ebe6efc3bc1e9f0b83c115b36898" translate="yes" xml:space="preserve">
          <source>The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1006ebebebbef4245b7568132e1eabf566a79140" translate="yes" xml:space="preserve">
          <source>The highest p-value for features to be kept.</source>
          <target state="translated">El valor p más alto de las características que deben mantenerse.</target>
        </trans-unit>
        <trans-unit id="0c1d204d0071e8e2a8101579dd644bade4aef0d9" translate="yes" xml:space="preserve">
          <source>The highest uncorrected p-value for features to keep.</source>
          <target state="translated">El más alto valor p no corregido de las características a mantener.</target>
        </trans-unit>
        <trans-unit id="3708900f6a079bc16cd76c37da5647812d1a0427" translate="yes" xml:space="preserve">
          <source>The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights.</source>
          <target state="translated">El histograma de los pesos estimados tiene un pico,ya que en los pesos está implícito un antecedente que induce a la escasez.</target>
        </trans-unit>
        <trans-unit id="d24bdc7c6b1e4dbeb474c6e9f76042366464386e" translate="yes" xml:space="preserve">
          <source>The hyperparameters</source>
          <target state="translated">Los hiperparámetros</target>
        </trans-unit>
        <trans-unit id="25f29cead3cfba36338bcf026da5a846edf25636" translate="yes" xml:space="preserve">
          <source>The i-th score &lt;code&gt;train_score_[i]&lt;/code&gt; is the deviance (= loss) of the model at iteration &lt;code&gt;i&lt;/code&gt; on the in-bag sample. If &lt;code&gt;subsample == 1&lt;/code&gt; this is the deviance on the training data.</source>
          <target state="translated">La i-&amp;eacute;sima puntuaci&amp;oacute;n &lt;code&gt;train_score_[i]&lt;/code&gt; es la desviaci&amp;oacute;n (= p&amp;eacute;rdida) del modelo en la iteraci&amp;oacute;n &lt;code&gt;i&lt;/code&gt; en la muestra de la bolsa. Si &lt;code&gt;subsample == 1&lt;/code&gt; esta es la desviaci&amp;oacute;n de los datos de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="7768180d6d5c29f9be6eb9d80f99cbf93007e25a" translate="yes" xml:space="preserve">
          <source>The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.</source>
          <target state="translated">La presunción de i.i.d.se rompe si el proceso generativo subyacente da lugar a grupos de muestras dependientes.</target>
        </trans-unit>
        <trans-unit id="d5e94b9dd17268ac8457a9d26f68d07b8365584a" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f4d07aca22136c4b5a250fb6a846554681fb509" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="190c7529dfd22ae7ed70d4f84d1deb499abbe749" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;code&gt;VotingClassifier&lt;/code&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">La idea detr&amp;aacute;s de &lt;code&gt;VotingClassifier&lt;/code&gt; es combinar clasificadores de aprendizaje autom&amp;aacute;tico conceptualmente diferentes y usar un voto mayoritario o las probabilidades promedio predichas (voto suave) para predecir las etiquetas de clase. Un clasificador de este tipo puede ser &amp;uacute;til para un conjunto de modelos de igualmente buen desempe&amp;ntilde;o con el fin de equilibrar sus debilidades individuales.</target>
        </trans-unit>
        <trans-unit id="f4f9efa7d25a183372e2e9ce75bfe20581578033" translate="yes" xml:space="preserve">
          <source>The image as a numpy array: height x width x color</source>
          <target state="translated">La imagen como una matriz numérica:altura x anchura x color</target>
        </trans-unit>
        <trans-unit id="77bae2f833c03e6f85b6a43b3aa3ead8f81ce2e5" translate="yes" xml:space="preserve">
          <source>The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.</source>
          <target state="translated">La imagen se cuantifica a 256 niveles de gris y se almacena como números enteros de 8 bits sin signo;el cargador los convertirá en valores de punto flotante en el intervalo [0,1],que son más fáciles de trabajar para muchos algoritmos.</target>
        </trans-unit>
        <trans-unit id="6db946ce103c288b47eadb4f1488a8fdc91a7488" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients. See &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; for another implementation:</source>
          <target state="translated">La implementaci&amp;oacute;n en la clase &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; usa el descenso de coordenadas como algoritmo para ajustar los coeficientes. Consulte &lt;a href=&quot;#least-angle-regression&quot;&gt;Regresi&amp;oacute;n de &amp;aacute;ngulo m&amp;iacute;nimo&lt;/a&gt; para ver otra implementaci&amp;oacute;n:</target>
        </trans-unit>
        <trans-unit id="64dabeaa3f8e38c30333615b1b174cbd1348b11e" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">La implementaci&amp;oacute;n en la clase &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; &lt;/a&gt; usa el descenso de coordenadas como algoritmo para ajustar los coeficientes.</target>
        </trans-unit>
        <trans-unit id="555e24352725a2133c35e7a52f56a702cd7f81c4" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">La implementaci&amp;oacute;n en la clase &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; &lt;/a&gt; usa el descenso de coordenadas como algoritmo para ajustar los coeficientes.</target>
        </trans-unit>
        <trans-unit id="f9cb17c43bed4736104925d68075ebd6b07ace68" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt;. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:</source>
          <target state="translated">La implementaci&amp;oacute;n se basa en el algoritmo 2.1 de &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt; . Adem&amp;aacute;s de la API de estimadores est&amp;aacute;ndar de scikit-learn, GaussianProcessRegressor:</target>
        </trans-unit>
        <trans-unit id="70780cc3dac260d0fb4e85faed94f4134c9dc0ae" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">La aplicación se basa en el Algoritmo 2.1 de los Procesos Gausianos para el Aprendizaje Automático (GPML)de Rasmussen y Williams.</target>
        </trans-unit>
        <trans-unit id="1904d3648b9818da40270920e1855bd9dcd752d1" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">La aplicación se basa en los algoritmos 3.1,3.2 y 5.1 de los Procesos Gausianos para el Aprendizaje Automático (GPML)de Rasmussen y Williams.</target>
        </trans-unit>
        <trans-unit id="0eeedaeb1d413ea7240926a8ca5811e478d2d62b" translate="yes" xml:space="preserve">
          <source>The implementation is based on an ensemble of ExtraTreeRegressor. The maximum depth of each tree is set to &lt;code&gt;ceil(log_2(n))&lt;/code&gt; where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="166f55526ba60cbc129406f3899569f7b0eb4efd" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm.</source>
          <target state="translated">La aplicación se basa en la libsvm.</target>
        </trans-unit>
        <trans-unit id="cb75b7ddb016087965f5fad7be27fc6c1cb16290" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.</source>
          <target state="translated">La aplicación se basa en la libsvm.La complejidad del tiempo de ajuste es más que cuadrática con el número de muestras,lo que dificulta la escalada a un conjunto de datos con más de un par de 10000 muestras.</target>
        </trans-unit>
        <trans-unit id="fa8767a0fc5ecb59976beb65f6a5637eaddaef83" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVR&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDRegressor&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c37eaa47201071ab85ed7ee61d61deb2b696067c" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4dec30aaf65f05fda35ca6a3928a075e5f1587d" translate="yes" xml:space="preserve">
          <source>The implementation is designed to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e44f638bc5a5bdc8405d919df1e1a354ca31f099" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; is based on an ensemble of &lt;a href=&quot;generated/sklearn.tree.extratreeregressor#sklearn.tree.ExtraTreeRegressor&quot;&gt;&lt;code&gt;tree.ExtraTreeRegressor&lt;/code&gt;&lt;/a&gt;. Following Isolation Forest original paper, the maximum depth of each tree is set to \(\lceil \log_2(n) \rceil\) where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08d84e0cf0c38a38100a9dad48e0efc744fd2db8" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;.</source>
          <target state="translated">La implementaci&amp;oacute;n de &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt; en scikit-learn sigue una generalizaci&amp;oacute;n a un modelo de regresi&amp;oacute;n lineal multivariante &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; utilizando la mediana espacial, que es una generalizaci&amp;oacute;n de la mediana a m&amp;uacute;ltiples dimensiones &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="fda50d88e9c81601cfc445f1c078677bdd2f3d40" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id36&quot;&gt;12&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id37&quot;&gt;13&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb185b2e148e3613cdde933dd05a6b32e3b9acbe" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM&lt;/a&gt; of L&amp;eacute;on Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">La implementaci&amp;oacute;n de SGD est&amp;aacute; influenciada por el &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;SVM de gradiente estoc&amp;aacute;stico&lt;/a&gt;de L&amp;eacute;on Bottou. De manera similar a SvmSGD, el vector de peso se representa como el producto de un escalar y un vector que permite una actualizaci&amp;oacute;n de peso eficiente en el caso de la regularizaci&amp;oacute;n L2. En el caso de vectores de caracter&amp;iacute;sticas dispersos, la intersecci&amp;oacute;n se actualiza con una tasa de aprendizaje menor (multiplicada por 0,01) para tener en cuenta el hecho de que se actualiza con m&amp;aacute;s frecuencia. Los ejemplos de formaci&amp;oacute;n se recogen de forma secuencial y la tasa de aprendizaje se reduce despu&amp;eacute;s de cada ejemplo observado. Adoptamos el programa de tasas de aprendizaje de Shalev-Shwartz et al. 2007. Para la clasificaci&amp;oacute;n de clases m&amp;uacute;ltiples, se utiliza un enfoque de &amp;ldquo;uno contra todos&amp;rdquo;. Usamos el algoritmo de gradiente truncado propuesto por Tsuruoka et al. 2009 para regularizaci&amp;oacute;n L1 (y Elastic Net). El c&amp;oacute;digo est&amp;aacute; escrito en Cython.</target>
        </trans-unit>
        <trans-unit id="3db47613e45248213b8540e2c5b681f6afec29a2" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;code&gt;Stochastic Gradient SVM&lt;/code&gt; of &lt;a href=&quot;#id10&quot; id=&quot;id7&quot;&gt;7&lt;/a&gt;. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse input &lt;code&gt;X&lt;/code&gt;, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from &lt;a href=&quot;#id12&quot; id=&quot;id8&quot;&gt;8&lt;/a&gt;. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed in &lt;a href=&quot;#id13&quot; id=&quot;id9&quot;&gt;9&lt;/a&gt; for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="234c6abfe6fe107b5770fe372b7d9e14789fc069" translate="yes" xml:space="preserve">
          <source>The implementation of logistic regression in scikit-learn can be accessed from class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.</source>
          <target state="translated">Se puede acceder a la implementaci&amp;oacute;n de la regresi&amp;oacute;n log&amp;iacute;stica en scikit-learn desde la clase &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; . Esta implementaci&amp;oacute;n puede ajustarse a la regresi&amp;oacute;n log&amp;iacute;stica binaria, One-vs-Rest o multinomial con regularizaci&amp;oacute;n opcional L2 o L1.</target>
        </trans-unit>
        <trans-unit id="11a45f793137837fb140d6499ba2bfe32aacbae3" translate="yes" xml:space="preserve">
          <source>The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">La importancia de una característica se calcula como la reducción total (normalizada)del criterio que aporta esa característica.También se conoce como la importancia de Gini.</target>
        </trans-unit>
        <trans-unit id="285ba1aeef83b4f72aaed81188dabeb94680bb69" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator.</source>
          <target state="translated">La mejora en la p&amp;eacute;rdida (= desviaci&amp;oacute;n) en las muestras fuera de la bolsa en relaci&amp;oacute;n con la iteraci&amp;oacute;n anterior. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; es la mejora en la p&amp;eacute;rdida de la primera etapa sobre el estimador de &lt;code&gt;init&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="820150544fe550e2e074294e51088eda9cdc8014" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator. Only available if &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d60773790f2e7bcb156876ca46ea3f1191f69076" translate="yes" xml:space="preserve">
          <source>The impurity at \(m\) is computed using an impurity function \(H()\), the choice of which depends on the task being solved (classification or regression)</source>
          <target state="translated">La impureza en m se calcula usando una función de impureza,cuya elección depende de la tarea que se resuelva (clasificación o regresión).</target>
        </trans-unit>
        <trans-unit id="d750c1db4cd012b1969f8fbe25c31782ef1879ba" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importance ranks the numerical features to be the most important features. As a result, the non-predictive &lt;code&gt;random_num&lt;/code&gt; variable is ranked the most important!</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb74ef34fa0b599fafdb1111102755a10d538f2b" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore &lt;strong&gt;do not necessarily inform us on which features are most important to make good predictions on held-out dataset&lt;/strong&gt;. Secondly, &lt;strong&gt;they favor high cardinality features&lt;/strong&gt;, that is features with many unique values. &lt;a href=&quot;permutation_importance#permutation-importance&quot;&gt;Permutation feature importance&lt;/a&gt; is an alternative to impurity-based feature importance that does not suffer from these flaws. These two methods of obtaining feature importance are explored in: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a85ca2be296db64c4eea5e4c85e7d3c8770ab473" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f89a5a9d5e4b0a0db4c9a50975418d448408f475" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature if axis == 0.</source>
          <target state="translated">El valor de relleno de imputación para cada característica si el eje ==0.</target>
        </trans-unit>
        <trans-unit id="52ad716a60ba342ef84206f96283ef6240a59825" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature.</source>
          <target state="translated">El valor de relleno de imputación para cada característica.</target>
        </trans-unit>
        <trans-unit id="66edd575acaaa453b1fb3cfd5f9ed2a63e5987ae" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature. Computing statistics can result in &lt;code&gt;np.nan&lt;/code&gt; values. During &lt;a href=&quot;#sklearn.impute.SimpleImputer.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt;, features corresponding to &lt;code&gt;np.nan&lt;/code&gt; statistics will be discarded.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cd5e57b66f65afed5b5b066bdba074e0cee3d73" translate="yes" xml:space="preserve">
          <source>The imputation strategy.</source>
          <target state="translated">La estrategia de imputación.</target>
        </trans-unit>
        <trans-unit id="d67cf9b9c87d5f16b287aedd5a057873a2e85c3c" translate="yes" xml:space="preserve">
          <source>The imputed dataset. &lt;code&gt;n_output_features&lt;/code&gt; is the number of features that is not always missing during &lt;code&gt;fit&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc59b667379a9f19464aefa0413c2d5b611edad6" translate="yes" xml:space="preserve">
          <source>The imputed input data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa1b27a8286b1b98805decd6657b8fa02347921e" translate="yes" xml:space="preserve">
          <source>The index (of the &lt;code&gt;cv_results_&lt;/code&gt; arrays) which corresponds to the best candidate parameter setting.</source>
          <target state="translated">El &amp;iacute;ndice (de las matrices &lt;code&gt;cv_results_&lt;/code&gt; ) que corresponde a la mejor configuraci&amp;oacute;n de par&amp;aacute;metro candidato.</target>
        </trans-unit>
        <trans-unit id="d8f1885dbc976f2ceb3f7711b10b324e6546b97b" translate="yes" xml:space="preserve">
          <source>The index is computed only quantities and features inherent to the dataset.</source>
          <target state="translated">El índice se calcula sólo con las cantidades y características inherentes al conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="eaf4dd06369196819331db8b96a42731aed3d8e0" translate="yes" xml:space="preserve">
          <source>The index is defined as the average similarity between each cluster \(C_i\) for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of this index, similarity is defined as a measure \(R_{ij}\) that trades off:</source>
          <target state="translated">El índice se define como el promedio de similitud entre cada grupo (C_i)para (i=1,...,k)y el más similar (C_j).En el contexto de este índice,la similitud se define como una medida \ ~ (R_{\i})que se intercambia:</target>
        </trans-unit>
        <trans-unit id="7ad5522250591909d8f74ff707e1fa5837de3ae6" translate="yes" xml:space="preserve">
          <source>The index is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4af6c67193225ac6c1c304797e9ea3402785083" translate="yes" xml:space="preserve">
          <source>The index of the cluster.</source>
          <target state="translated">El índice del grupo.</target>
        </trans-unit>
        <trans-unit id="bf1ee02857437dc2bc3e741c8e3e56ef69d1ae10" translate="yes" xml:space="preserve">
          <source>The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.</source>
          <target state="translated">El valor índice de una palabra en el vocabulario está vinculado a su frecuencia en todo el corpus de formación.</target>
        </trans-unit>
        <trans-unit id="03c2241f87945c783a2d50da20a997d6e73ac147" translate="yes" xml:space="preserve">
          <source>The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don&amp;rsquo;t use this parameter unless you know what to do.</source>
          <target state="translated">Los &amp;iacute;ndices de las muestras de entrada de entrenamiento ordenadas. Si se cultivan muchos &amp;aacute;rboles en el mismo conjunto de datos, esto permite que el orden se almacene en cach&amp;eacute; entre &amp;aacute;rboles. Si es Ninguno, los datos se ordenar&amp;aacute;n aqu&amp;iacute;. No utilice este par&amp;aacute;metro a menos que sepa qu&amp;eacute; hacer.</target>
        </trans-unit>
        <trans-unit id="0f615a1a241d635bbd7f8073a9962f88455d5840" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each column.</source>
          <target state="translated">Los indicadores de la composición de los grupos de cada columna.</target>
        </trans-unit>
        <trans-unit id="7c7c8e5dd3219610397a571246f91a523e9b0296" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each row.</source>
          <target state="translated">Los indicadores de la composición de los grupos de cada fila.</target>
        </trans-unit>
        <trans-unit id="1d09056572af0ab7d260b8a9a458cace43041c54" translate="yes" xml:space="preserve">
          <source>The inertia matrix uses a Heapq-based representation.</source>
          <target state="translated">La matriz de inercia utiliza una representación basada en Heapq.</target>
        </trans-unit>
        <trans-unit id="4f68a65921a2da6924b081b35cf28ef22fa8adaa" translate="yes" xml:space="preserve">
          <source>The inferred value of max_features.</source>
          <target state="translated">El valor inferido de max_features.</target>
        </trans-unit>
        <trans-unit id="9af964a5bdc6c7f6e3e23dbd4ccea9f871ad0d0c" translate="yes" xml:space="preserve">
          <source>The initial coefficients to warm-start the optimization.</source>
          <target state="translated">Los coeficientes iniciales para iniciar la optimización.</target>
        </trans-unit>
        <trans-unit id="e326d26a40d08d9ebf56e15d175f98f61ace6983" translate="yes" xml:space="preserve">
          <source>The initial guess for the covariance.</source>
          <target state="translated">La conjetura inicial para la covarianza.</target>
        </trans-unit>
        <trans-unit id="a31ad0b6b83b24ae472799017e15e5984848ac41" translate="yes" xml:space="preserve">
          <source>The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)</source>
          <target state="translated">La estimación inicial de la variación del ruido para cada característica.Si no hay ninguna,el valor predeterminado es np.ones(n_funciones)</target>
        </trans-unit>
        <trans-unit id="5af1f38d3d578b440a3ef0c3e0d6f491e678143f" translate="yes" xml:space="preserve">
          <source>The initial intercept to warm-start the optimization.</source>
          <target state="translated">La intercepción inicial para iniciar la optimización.</target>
        </trans-unit>
        <trans-unit id="ffb7aede167c9f36a232ef307d2df81471bb1b4f" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.0 as eta0 is not used by the default schedule &amp;lsquo;optimal&amp;rsquo;.</source>
          <target state="translated">La tasa de aprendizaje inicial para los horarios &quot;constante&quot;, &quot;invscaling&quot; o &quot;adaptativo&quot;. El valor predeterminado es 0.0 ya que eta0 no se usa en el programa predeterminado &quot;&amp;oacute;ptimo&quot;.</target>
        </trans-unit>
        <trans-unit id="3434635df97d9946f02b9082e79d95e6999ccc03" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.01.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f396af141edab02acfc3d8c05ea1f6e174dd0d5b" translate="yes" xml:space="preserve">
          <source>The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">La tasa de aprendizaje inicial utilizada. Controla el tama&amp;ntilde;o del paso al actualizar los pesos. Solo se usa cuando solver = 'sgd' o 'adam'.</target>
        </trans-unit>
        <trans-unit id="8aa315cba5e51992abb63a7c33e92703aa91a364" translate="yes" xml:space="preserve">
          <source>The initial model \(F_{0}\) is problem specific, for least-squares regression one usually chooses the mean of the target values.</source>
          <target state="translated">El modelo inicial \N es específico para el problema,para la regresión por mínimos cuadrados se suele elegir la media de los valores objetivo.</target>
        </trans-unit>
        <trans-unit id="fcbcdc2001232ebf2d1267dbd5c1db8c05bef33b" translate="yes" xml:space="preserve">
          <source>The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument. The passed object has to implement &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">El modelo inicial tambi&amp;eacute;n se puede especificar mediante el argumento &lt;code&gt;init&lt;/code&gt; . El objeto pasado tiene que implementar el &lt;code&gt;fit&lt;/code&gt; y la &lt;code&gt;predict&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2b0323f433eec5618cbc80c984794d0b5575bec5" translate="yes" xml:space="preserve">
          <source>The initial transformation will be a random array of shape &lt;code&gt;(n_components, n_features)&lt;/code&gt;. Each value is sampled from the standard normal distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17bd189eec62d21587058ccd88b444461d73119b" translate="yes" xml:space="preserve">
          <source>The initial values of the coefficients.</source>
          <target state="translated">Los valores iniciales de los coeficientes.</target>
        </trans-unit>
        <trans-unit id="32a92ee0ef2ffe7a34cba56e64f123bd9b2ca181" translate="yes" xml:space="preserve">
          <source>The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the first layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the weight matrix as a 28x28 pixel image.</source>
          <target state="translated">Los datos de entrada consisten en dígitos escritos a mano de 28x28 píxeles,lo que da lugar a 784 características en el conjunto de datos.Por lo tanto,la matriz de peso de la primera capa tiene la forma (784,hidden_layer_sizes[0]).Por lo tanto,podemos visualizar una sola columna de la matriz de peso como una imagen de 28x28 píxeles.</target>
        </trans-unit>
        <trans-unit id="c8ff70c87eb4edb0d6df0b02362dd0f826eb86c0" translate="yes" xml:space="preserve">
          <source>The input data matrix</source>
          <target state="translated">La matriz de datos de entrada</target>
        </trans-unit>
        <trans-unit id="28402823f8037a445e34f883189d18c9c8586801" translate="yes" xml:space="preserve">
          <source>The input data to complete.</source>
          <target state="translated">Los datos de entrada para completar.</target>
        </trans-unit>
        <trans-unit id="10ea3f8f2392cebb3ea568adacc2ca87c3d24f21" translate="yes" xml:space="preserve">
          <source>The input data to project into a smaller dimensional space.</source>
          <target state="translated">Los datos de entrada para proyectarse en un espacio dimensional más pequeño.</target>
        </trans-unit>
        <trans-unit id="8b941f334dfce6c774093743936f754bf8137c6b" translate="yes" xml:space="preserve">
          <source>The input data.</source>
          <target state="translated">Los datos de entrada.</target>
        </trans-unit>
        <trans-unit id="a383a11075a72cc4c498a6da492b12429e0d8bd0" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods:</source>
          <target state="translated">La matriz de entrada se normaliza primero para hacer más obvio el patrón del tablero de ajedrez.Hay tres métodos posibles:</target>
        </trans-unit>
        <trans-unit id="5296b71b9c59f1ea88d1300f56d80b11acdd4263" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is preprocessed as follows:</source>
          <target state="translated">La matriz de entrada se preprocesa de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="1dfee05c320291671e1507b730cbc56e48fcc05b" translate="yes" xml:space="preserve">
          <source>The input samples with only the selected features.</source>
          <target state="translated">Las muestras de entrada con sólo las características seleccionadas.</target>
        </trans-unit>
        <trans-unit id="aa9c51d1052bb07178d99dc6b9998838f3fd04f4" translate="yes" xml:space="preserve">
          <source>The input samples.</source>
          <target state="translated">Las muestras de entrada.</target>
        </trans-unit>
        <trans-unit id="691f5d8dd518149d763a76be65224ad6a47a3de2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">Las muestras de entrada. Internamente, se convertir&amp;aacute; a &lt;code&gt;dtype=np.float32&lt;/code&gt; y si se proporciona una matriz dispersa a una &lt;code&gt;csr_matrix&lt;/code&gt; dispersa .</target>
        </trans-unit>
        <trans-unit id="80a0f34908e0a755318c3eb528444068386165b2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">Las muestras de entrada. Internamente, su dtype se convertir&amp;aacute; a &lt;code&gt;dtype=np.float32&lt;/code&gt; . Si se proporciona una matriz dispersa, se convertir&amp;aacute; en una &lt;code&gt;csr_matrix&lt;/code&gt; dispersa .</target>
        </trans-unit>
        <trans-unit id="ad241eeecc4b3d71885329d69a3ecbb931dd0903" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">Las muestras de entrada. Internamente, su dtype se convertir&amp;aacute; a &lt;code&gt;dtype=np.float32&lt;/code&gt; . Si se proporciona una matriz dispersa, se convertir&amp;aacute; en una &lt;code&gt;csr_matrix&lt;/code&gt; dispersa .</target>
        </trans-unit>
        <trans-unit id="574b784828dbe4184d33a76077708bbde6ac5b78" translate="yes" xml:space="preserve">
          <source>The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b99e327cb34f5c84fcbd4e91cfb496e38c8c302f" translate="yes" xml:space="preserve">
          <source>The input samples. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csc_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">Las muestras de entrada. Utilice &lt;code&gt;dtype=np.float32&lt;/code&gt; para obtener la m&amp;aacute;xima eficiencia. Tambi&amp;eacute;n se admiten matrices dispersas, use &lt;code&gt;csc_matrix&lt;/code&gt; disperso para una m&amp;aacute;xima eficiencia.</target>
        </trans-unit>
        <trans-unit id="6e6a9dea87222450502841b10c214ed0343cd360" translate="yes" xml:space="preserve">
          <source>The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt;&lt;code&gt;make_low_rank_matrix&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">El conjunto de entrada puede estar bien acondicionado (por defecto) o tener un perfil singular de cola de bajo rango. Consulte &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt; &lt;code&gt;make_low_rank_matrix&lt;/code&gt; &lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="289f5749e232fcbed1540450cf860647da777cb8" translate="yes" xml:space="preserve">
          <source>The input set is well conditioned, centered and gaussian with unit variance.</source>
          <target state="translated">El conjunto de entrada está bien acondicionado,centrado y gaussiano con variación de unidades.</target>
        </trans-unit>
        <trans-unit id="9196dd161fc035e27746b1e485ce67d2bc4a2f97" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.</source>
          <target state="translated">La entrada de este transformador debe ser un conjunto de números enteros o cadenas,que denoten los valores asumidos por las características categóricas (discretas).Los rasgos se convierten en enteros ordinales.Esto da como resultado una sola columna de números enteros (0 a n_categorías-1)por característica.</target>
        </trans-unit>
        <trans-unit id="a912a205d28c73bb81095b58a51d5b9233f6732c" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the &lt;code&gt;sparse&lt;/code&gt; parameter)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fca358f78b22e3193c17b331b39a22e0cf2c917" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array.</source>
          <target state="translated">La entrada a este transformador debe ser una matriz de enteros o cadenas, que denota los valores tomados por caracter&amp;iacute;sticas categ&amp;oacute;ricas (discretas). Las caracter&amp;iacute;sticas se codifican utilizando un esquema de codificaci&amp;oacute;n one-hot (tambi&amp;eacute;n conocido como 'one-of-K' o 'ficticio'). Esto crea una columna binaria para cada categor&amp;iacute;a y devuelve una matriz dispersa o una matriz densa.</target>
        </trans-unit>
        <trans-unit id="c9721cff229ea27eaddea830cb215d9ecb24c54d" translate="yes" xml:space="preserve">
          <source>The instance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6846270bbc73ec820141552eb32b90be4589180" translate="yes" xml:space="preserve">
          <source>The integer id or the string name metadata of the MLComp dataset to load</source>
          <target state="translated">El id entero o los metadatos del nombre de la cadena del conjunto de datos de MLComp para cargar</target>
        </trans-unit>
        <trans-unit id="0ff5072970d94a45dfd0b3dc59cc200193becdfd" translate="yes" xml:space="preserve">
          <source>The integer labels (0 or 1) for class membership of each sample.</source>
          <target state="translated">Las etiquetas de números enteros (0 o 1)para la pertenencia a una clase de cada muestra.</target>
        </trans-unit>
        <trans-unit id="a6afcce2561a80c34c914f95ed66620399eb0d7c" translate="yes" xml:space="preserve">
          <source>The integer labels for class membership of each sample.</source>
          <target state="translated">Las etiquetas de números enteros para la pertenencia a una clase de cada muestra.</target>
        </trans-unit>
        <trans-unit id="868af1be557e4830c6f25b4ff3211b7a9aad7eb0" translate="yes" xml:space="preserve">
          <source>The integer labels for cluster membership of each sample.</source>
          <target state="translated">Las etiquetas de números enteros para la pertenencia a grupos de cada muestra.</target>
        </trans-unit>
        <trans-unit id="05d4e37f01ed47119c132248581df1196214fb7d" translate="yes" xml:space="preserve">
          <source>The integer labels for quantile membership of each sample.</source>
          <target state="translated">Las etiquetas de números enteros para la composición del cuantil de cada muestra.</target>
        </trans-unit>
        <trans-unit id="7da460eaa0239b5647e6b97a087bcede08e5a0c5" translate="yes" xml:space="preserve">
          <source>The intercept of the model. Only returned if &lt;code&gt;return_intercept&lt;/code&gt; is True and if X is a scipy sparse array.</source>
          <target state="translated">La intersecci&amp;oacute;n del modelo. Solo se devuelve si &lt;code&gt;return_intercept&lt;/code&gt; es True y si X es una matriz dispersa scipy.</target>
        </trans-unit>
        <trans-unit id="f0408bacce1626a183a8eadd09dc2d6f8b9e549c" translate="yes" xml:space="preserve">
          <source>The intercept term.</source>
          <target state="translated">El término de intercepción.</target>
        </trans-unit>
        <trans-unit id="39d99a92784fd6547680c69e2a029d63ab730943" translate="yes" xml:space="preserve">
          <source>The inverse document frequency (IDF) vector; only defined if &lt;code&gt;use_idf&lt;/code&gt; is True.</source>
          <target state="translated">El vector de frecuencia de documento inverso (IDF); solo se define si &lt;code&gt;use_idf&lt;/code&gt; es True.</target>
        </trans-unit>
        <trans-unit id="70e42545b4f301fb133ffa67ef01dbd1c81a14a6" translate="yes" xml:space="preserve">
          <source>The inverse of the Box-Cox transformation is given by:</source>
          <target state="translated">El inverso de la transformación de Box-Cox viene dado por:</target>
        </trans-unit>
        <trans-unit id="1296b0fc0246edf109c97645c02261b1d3d028bb" translate="yes" xml:space="preserve">
          <source>The inverse of the Yeo-Johnson transformation is given by:</source>
          <target state="translated">El inverso de la transformación de Yeo-Johnson está dado por:</target>
        </trans-unit>
        <trans-unit id="8621ba40f721d68ec6f12f67b5e3cc856e6fc6d9" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">El conjunto de datos del iris es un clásico y muy fácil conjunto de datos de clasificación multiclase.</target>
        </trans-unit>
        <trans-unit id="82f3a3d98edcec3969a3e26fc0789c7c6c1f74ef" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:</source>
          <target state="translated">El conjunto de datos del iris es una tarea de clasificación que consiste en identificar 3 tipos diferentes de iris (Setosa,Versicolor y Vírgenica)a partir de su longitud y anchura de pétalos y sépalos:</target>
        </trans-unit>
        <trans-unit id="968fa226a66bed28f98c84df2ac306ac992e59c2" translate="yes" xml:space="preserve">
          <source>The isotonic regression optimization problem is defined by:</source>
          <target state="translated">El problema de la optimización de la regresión isotónica está definido por:</target>
        </trans-unit>
        <trans-unit id="710a9703f51d61fb5f41b8c77a926ca070febd6c" translate="yes" xml:space="preserve">
          <source>The iteration will stop when &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; where pg_i is the i-th component of the projected gradient.</source>
          <target state="translated">La iteraci&amp;oacute;n se detendr&amp;aacute; cuando &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; donde pg_i es el i-&amp;eacute;simo componente del gradiente proyectado.</target>
        </trans-unit>
        <trans-unit id="d1f978f02193a9d7a16e2ac4f28051f1be20fa33" translate="yes" xml:space="preserve">
          <source>The iterator consumption and dispatching is protected by the same lock so calling this function should be thread safe.</source>
          <target state="translated">El consumo del iterador y el envío está protegido por el mismo candado,por lo que llamar a esta función debería ser seguro para el hilo.</target>
        </trans-unit>
        <trans-unit id="e6bcf49b626a9dedb0663face3bd765b3dedb378" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the bias vector corresponding to layer i + 1.</source>
          <target state="translated">El i-ésimo elemento de la lista representa el vector de sesgo correspondiente a la capa i+1.</target>
        </trans-unit>
        <trans-unit id="4b396b269a89e1a7151872f147950b58bd31d2c2" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the weight matrix corresponding to layer i.</source>
          <target state="translated">El i-ésimo elemento de la lista representa la matriz de peso correspondiente a la capa i.</target>
        </trans-unit>
        <trans-unit id="8ce9197f7d2eba2389cfe44c6d9806383e84232a" translate="yes" xml:space="preserve">
          <source>The ith element represents the number of neurons in the ith hidden layer.</source>
          <target state="translated">El elemento ith representa el número de neuronas en la capa oculta ith.</target>
        </trans-unit>
        <trans-unit id="110dde91ce5735296cfcdbdf4849eb2634f008ea" translate="yes" xml:space="preserve">
          <source>The justification for the formula in the loss=&amp;rdquo;modified_huber&amp;rdquo; case is in the appendix B in: &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&lt;/a&gt;</source>
          <target state="translated">La justificaci&amp;oacute;n de la f&amp;oacute;rmula en el caso loss = &amp;rdquo;modified_huber&amp;rdquo; se encuentra en el ap&amp;eacute;ndice B en: &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="08f434f0a998f2afa3af3de6e921bb2935ec3174" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ad3a7b0acdb773b3a5a78e7f86e7401f887f5a4" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space. The K-means algorithm aims to choose centroids that minimise the &lt;em&gt;inertia&lt;/em&gt;, or within-cluster sum of squared criterion:</source>
          <target state="translated">El algoritmo de k-medias divide un conjunto de \ (N \) muestras \ (X \) en \ (K \) grupos disjuntos \ (C \), cada uno descrito por la media \ (\ mu_j \) de las muestras en el racimo. Los medios se denominan com&amp;uacute;nmente &quot;centroides&quot; del grupo; n&amp;oacute;tese que, en general, no son puntos de \ (X \), aunque viven en el mismo espacio. El algoritmo de K-medias tiene como objetivo elegir centroides que minimicen la &lt;em&gt;inercia&lt;/em&gt; , o el criterio de suma de cuadrados dentro del grupo:</target>
        </trans-unit>
        <trans-unit id="3042a74be3a11ce6ced2d21d393c8c55a0b044ff" translate="yes" xml:space="preserve">
          <source>The k-means problem is solved using either Lloyd&amp;rsquo;s or Elkan&amp;rsquo;s algorithm.</source>
          <target state="translated">El problema de k-medias se resuelve utilizando el algoritmo de Lloyd o el de Elkan.</target>
        </trans-unit>
        <trans-unit id="698fe2144ac8be0b801483c0a1d13dc4d07c73ce" translate="yes" xml:space="preserve">
          <source>The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).</source>
          <target state="translated">La puntuación kappa (véase la cadena de documentación)es un número entre -1 y 1.Las puntuaciones superiores a 0,8 se consideran generalmente como un buen acuerdo;cero o menos significa que no hay acuerdo (etiquetas prácticamente aleatorias).</target>
        </trans-unit>
        <trans-unit id="951d69c60ea24190bd0c5a4dcae9aeb94c1c913d" translate="yes" xml:space="preserve">
          <source>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</source>
          <target state="translated">La estadística kappa,que es un número entre -1 y 1.El valor máximo significa acuerdo completo;cero o menos significa acuerdo de azar.</target>
        </trans-unit>
        <trans-unit id="e7d87035112e23c601a476b538c50df786049966" translate="yes" xml:space="preserve">
          <source>The kernel density estimator can be used with any of the valid distance metrics (see &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt;&lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt;&lt;/a&gt; for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine distance&lt;/a&gt; which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:</source>
          <target state="translated">El estimador de densidad del kernel se puede utilizar con cualquiera de las m&amp;eacute;tricas de distancia v&amp;aacute;lidas (consulte &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt; &lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt; &lt;/a&gt; para obtener una lista de m&amp;eacute;tricas disponibles), aunque los resultados se normalizan correctamente solo para la m&amp;eacute;trica euclidiana. Una m&amp;eacute;trica particularmente &amp;uacute;til es la &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;distancia de Haversine,&lt;/a&gt; que mide la distancia angular entre puntos en una esfera. A continuaci&amp;oacute;n, se muestra un ejemplo del uso de una estimaci&amp;oacute;n de densidad de kernel para una visualizaci&amp;oacute;n de datos geoespaciales, en este caso la distribuci&amp;oacute;n de observaciones de dos especies diferentes en el continente sudamericano:</target>
        </trans-unit>
        <trans-unit id="f5acb5ee70e93822d6214e7faa0671c1edbb5e04" translate="yes" xml:space="preserve">
          <source>The kernel is composed of several terms that are responsible for explaining different properties of the signal:</source>
          <target state="translated">El núcleo está compuesto por varios términos que se encargan de explicar las diferentes propiedades de la señal:</target>
        </trans-unit>
        <trans-unit id="075d7f089a3306b950fac8e515a19a5954a7251d" translate="yes" xml:space="preserve">
          <source>The kernel is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2a264496619ccec6a15428322dff95c6847afef" translate="yes" xml:space="preserve">
          <source>The kernel specifying the covariance function of the GP. If None is passed, the kernel &amp;ldquo;1.0 * RBF(1.0)&amp;rdquo; is used as default. Note that the kernel&amp;rsquo;s hyperparameters are optimized during fitting.</source>
          <target state="translated">El kernel que especifica la funci&amp;oacute;n de covarianza de la GP. Si se pasa None, el kernel &amp;ldquo;1.0 * RBF (1.0)&amp;rdquo; se usa por defecto. Tenga en cuenta que los hiperpar&amp;aacute;metros del kernel se optimizan durante el ajuste.</target>
        </trans-unit>
        <trans-unit id="3f4b36434e95db38ce416f2948e370303e8d7ce4" translate="yes" xml:space="preserve">
          <source>The kernel to use. Valid kernels are [&amp;lsquo;gaussian&amp;rsquo;|&amp;rsquo;tophat&amp;rsquo;|&amp;rsquo;epanechnikov&amp;rsquo;|&amp;rsquo;exponential&amp;rsquo;|&amp;rsquo;linear&amp;rsquo;|&amp;rsquo;cosine&amp;rsquo;] Default is &amp;lsquo;gaussian&amp;rsquo;.</source>
          <target state="translated">El kernel a utilizar. Los n&amp;uacute;cleos v&amp;aacute;lidos son ['gaussiano' | 'tophat' | 'epanechnikov' | 'exponencial' | 'lineal' | 'coseno'] El valor predeterminado es 'gaussiano'.</target>
        </trans-unit>
        <trans-unit id="7d7a3328bfc6be5d4c52eb80a02db6810f6af97a" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers.</source>
          <target state="translated">El núcleo utilizado para la predicción.En el caso de la clasificación binaria,la estructura del núcleo es la misma que la pasada como parámetro pero con hiperparámetros optimizados.En caso de clasificación multiclase,se devuelve un CompoundKernel que consiste en los diferentes núcleos utilizados en los clasificadores de uno contra uno.</target>
        </trans-unit>
        <trans-unit id="5a1810e4f4370ea9d4b3eb4b69c5661264f9680f" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters</source>
          <target state="translated">El núcleo utilizado para la predicción.La estructura del núcleo es la misma que la que se pasó como parámetro pero con hiperparámetros optimizados</target>
        </trans-unit>
        <trans-unit id="317ceb705f764af792a5c1b02b0c75da4f7767d2" translate="yes" xml:space="preserve">
          <source>The key &lt;code&gt;'params'&lt;/code&gt; is used to store a list of parameter settings dicts for all the parameter candidates.</source>
          <target state="translated">La tecla &lt;code&gt;'params'&lt;/code&gt; se utiliza para almacenar una lista de dictados de configuraci&amp;oacute;n de par&amp;aacute;metros para todos los par&amp;aacute;metros candidatos.</target>
        </trans-unit>
        <trans-unit id="467dfd2cbee563f1dc16d74e0763e99176db4d68" translate="yes" xml:space="preserve">
          <source>The l1-penalized estimator can recover part of this off-diagonal structure. It learns a sparse precision. It is not able to recover the exact sparsity pattern: it detects too many non-zero coefficients. However, the highest non-zero coefficients of the l1 estimated correspond to the non-zero coefficients in the ground truth. Finally, the coefficients of the l1 precision estimate are biased toward zero: because of the penalty, they are all smaller than the corresponding ground truth value, as can be seen on the figure.</source>
          <target state="translated">El estimador l1-penalizado puede recuperar parte de esta estructura fuera de la diagonal.Aprende una precisión escasa.No es capaz de recuperar el patrón exacto de dispersión:detecta demasiados coeficientes distintos de cero.Sin embargo,los coeficientes no nulos más altos de l1 estimados corresponden a los coeficientes no nulos de la verdad del terreno.Finalmente,los coeficientes de la estimación de precisión de l1 están sesgados hacia cero:debido a la penalización,todos ellos son más pequeños que el valor correspondiente de la verdad sobre el terreno,como puede verse en la figura.</target>
        </trans-unit>
        <trans-unit id="3f115dd7f6ab226f3d1efb2261d982c44eb021f0" translate="yes" xml:space="preserve">
          <source>The label of the positive class</source>
          <target state="translated">La etiqueta de la clase positiva</target>
        </trans-unit>
        <trans-unit id="1daeeb6b0700e2caf583964dcec09c381eeb8ea9" translate="yes" xml:space="preserve">
          <source>The label of the positive class. Only applied to binary &lt;code&gt;y_true&lt;/code&gt;. For multilabel-indicator &lt;code&gt;y_true&lt;/code&gt;, &lt;code&gt;pos_label&lt;/code&gt; is fixed to 1.</source>
          <target state="translated">La etiqueta de la clase positiva. Solo se aplica al binario &lt;code&gt;y_true&lt;/code&gt; . Para el indicador de &lt;code&gt;y_true&lt;/code&gt; &lt;code&gt;pos_label&lt;/code&gt; y_true , pos_label se fija en 1.</target>
        </trans-unit>
        <trans-unit id="e8fa4295eafd0b055339770364309144cc85ebe4" translate="yes" xml:space="preserve">
          <source>The label of the positive class. When &lt;code&gt;pos_label=None&lt;/code&gt;, if y_true is in {-1, 1} or {0, 1}, &lt;code&gt;pos_label&lt;/code&gt; is set to 1, otherwise an error will be raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="844ad03ed5a5991044f5c2a9015a7d46ffc83176" translate="yes" xml:space="preserve">
          <source>The label sets.</source>
          <target state="translated">La etiqueta se fija.</target>
        </trans-unit>
        <trans-unit id="4e35273308f7d89cd0147256c1b5ea034960bf40" translate="yes" xml:space="preserve">
          <source>The labels assigned to samples. Points which are not included in any cluster are labeled as -1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34e6595f06fcaf1ac9d996e42fb91cf1dc35ee40" translate="yes" xml:space="preserve">
          <source>The labels of the clusters.</source>
          <target state="translated">Las etiquetas de los grupos.</target>
        </trans-unit>
        <trans-unit id="f920ebdbb736d4fac6e624f1f0a24fae7f686c27" translate="yes" xml:space="preserve">
          <source>The laplacian kernel is defined as:</source>
          <target state="translated">El núcleo lapón se define como:</target>
        </trans-unit>
        <trans-unit id="06271ccbc4593ebfa05907a5273f644dc124ef42" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the coefficient vector.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="321fd6d2804d3c8ea8df389dda895240e812e24c" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the parameter vector.</source>
          <target state="translated">La estimación del lazo resuelve así la minimización de la penalidad de los mínimos cuadrados con la adición de la norma del vector de parámetros.</target>
        </trans-unit>
        <trans-unit id="374010cdf4f6f26c62c956d36c36442b1f12b646" translate="yes" xml:space="preserve">
          <source>The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.</source>
          <target state="translated">La última característica implica que el Perceptrón es ligeramente más rápido de entrenar que el SGD con la pérdida de la bisagra y que los modelos resultantes son más escasos.</target>
        </trans-unit>
        <trans-unit id="6312a138a2adc5ee223c24bc72b6c9e5099fd7ea" translate="yes" xml:space="preserve">
          <source>The last dataset is an example of a &amp;lsquo;null&amp;rsquo; situation for clustering: the data is homogeneous, and there is no good clustering. For this example, the null dataset uses the same parameters as the dataset in the row above it, which represents a mismatch in the parameter values and the data structure.</source>
          <target state="translated">El &amp;uacute;ltimo conjunto de datos es un ejemplo de una situaci&amp;oacute;n 'nula' para la agrupaci&amp;oacute;n en cl&amp;uacute;steres: los datos son homog&amp;eacute;neos y no hay una buena agrupaci&amp;oacute;n. Para este ejemplo, el conjunto de datos nulo usa los mismos par&amp;aacute;metros que el conjunto de datos en la fila superior, lo que representa una discrepancia en los valores de los par&amp;aacute;metros y la estructura de datos.</target>
        </trans-unit>
        <trans-unit id="2cc9bc385685f028c8287b17d0d958b5050b56a0" translate="yes" xml:space="preserve">
          <source>The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.</source>
          <target state="translated">Los últimos valores de precisión y recuerdo son 1.y 0.respectivamente y no tienen un umbral correspondiente.Esto asegura que el gráfico comienza en el eje y.</target>
        </trans-unit>
        <trans-unit id="9ddff80583e9f2c8a48f3b64e4a9a06a8a8cefcd" translate="yes" xml:space="preserve">
          <source>The last two panels show how we can sample from the last two models. The resulting samples distributions do not look exactly like the original data distribution. The difference primarily stems from the approximation error we made by using a model that assumes that the data was generated by a finite number of Gaussian components instead of a continuous noisy sine curve.</source>
          <target state="translated">Los dos últimos paneles muestran cómo podemos tomar muestras de los dos últimos modelos.Las distribuciones de muestras resultantes no se ven exactamente como la distribución de datos original.La diferencia proviene principalmente del error de aproximación que hicimos al usar un modelo que asume que los datos fueron generados por un número finito de componentes gausianos en lugar de una curva sinusoidal continua y ruidosa.</target>
        </trans-unit>
        <trans-unit id="a4514c8e853c44e1e4a78bf33662f9b32277ba44" translate="yes" xml:space="preserve">
          <source>The latent variables of X.</source>
          <target state="translated">Las variables latentes de X.</target>
        </trans-unit>
        <trans-unit id="03a214378b3ef2e13a3cc6617d05b3fe221146c8" translate="yes" xml:space="preserve">
          <source>The learning rate \(\eta\) can be either constant or gradually decaying. For classification, the default learning rate schedule (&lt;code&gt;learning_rate='optimal'&lt;/code&gt;) is given by</source>
          <target state="translated">La tasa de aprendizaje \ (\ eta \) puede ser constante o decaer gradualmente. Para la clasificaci&amp;oacute;n, el programa de tasa de aprendizaje predeterminado ( &lt;code&gt;learning_rate='optimal'&lt;/code&gt; ) viene dado por</target>
        </trans-unit>
        <trans-unit id="36c5b5659a911c2db600cd9fbf9c0485b5053e95" translate="yes" xml:space="preserve">
          <source>The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a &amp;lsquo;ball&amp;rsquo; with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.</source>
          <target state="translated">La tasa de aprendizaje de t-SNE suele estar en el rango [10,0, 1000,0]. Si la tasa de aprendizaje es demasiado alta, los datos pueden parecer una &quot;bola&quot; con cualquier punto aproximadamente equidistante de sus vecinos m&amp;aacute;s cercanos. Si la tasa de aprendizaje es demasiado baja, la mayor&amp;iacute;a de los puntos pueden verse comprimidos en una nube densa con pocos valores at&amp;iacute;picos. Si la funci&amp;oacute;n de costo se atasca en un m&amp;iacute;nimo local malo, aumentar la tasa de aprendizaje puede ayudar.</target>
        </trans-unit>
        <trans-unit id="55a4138aa9f2827acbf6f24a7d7d78c5c9231271" translate="yes" xml:space="preserve">
          <source>The learning rate for weight updates. It is &lt;em&gt;highly&lt;/em&gt; recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.</source>
          <target state="translated">La tasa de aprendizaje para las actualizaciones de peso. Es &lt;em&gt;muy&lt;/em&gt; recomendable ajustar este hiperpar&amp;aacute;metro. Los valores razonables est&amp;aacute;n en el rango de 10 ** [0., -3.].</target>
        </trans-unit>
        <trans-unit id="aa4707838d034cc31029160760df726f17933ef5" translate="yes" xml:space="preserve">
          <source>The learning rate schedule:</source>
          <target state="translated">El calendario del ritmo de aprendizaje:</target>
        </trans-unit>
        <trans-unit id="3497cec934b609e53594ddceecdfac3a47086873" translate="yes" xml:space="preserve">
          <source>The learning rate, also known as &lt;em&gt;shrinkage&lt;/em&gt;. This is used as a multiplicative factor for the leaves values. Use &lt;code&gt;1&lt;/code&gt; for no shrinkage.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f65b2c5f287b4c02eec9ec5be4e57eb68fdc4c9" translate="yes" xml:space="preserve">
          <source>The least squares loss (along with the implicit use of the identity link function) of the Ridge regression model seems to cause this model to be badly calibrated. In particular, it tends to underestimate the risk and can even predict invalid negative frequencies.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed4ad540aa3d79251b7dbdba80cd56b31d18c164" translate="yes" xml:space="preserve">
          <source>The least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; this method has a cost of \(O(n_{\text{samples}} n_{\text{features}}^2)\), assuming that \(n_{\text{samples}} \geq n_{\text{features}}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="866660dd294846977c41d9eb05a5f823762d12dd" translate="yes" xml:space="preserve">
          <source>The left and right examples highlight the &lt;code&gt;n_labels&lt;/code&gt; parameter: more of the samples in the right plot have 2 or 3 labels.</source>
          <target state="translated">Los ejemplos de la izquierda y la derecha resaltan el par&amp;aacute;metro &lt;code&gt;n_labels&lt;/code&gt; : m&amp;aacute;s de las muestras en el gr&amp;aacute;fico de la derecha tienen 2 o 3 etiquetas.</target>
        </trans-unit>
        <trans-unit id="3656a1b3513126216409c003f2c19b4a1bde366e" translate="yes" xml:space="preserve">
          <source>The leftmost layer, known as the input layer, consists of a set of neurons \(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.</source>
          <target state="translated">La capa más a la izquierda,conocida como la capa de entrada,consiste en un conjunto de neuronas \ ~ x_1,x_2,...,x_m}{\i1}que representan las características de entrada.{\i}Cada neurona de la capa oculta transforma los valores de la capa anterior con una suma lineal ponderada \(w_1x_1+w_2x_2+...+w_mx_m\),seguida de una función de activación no lineal \N(g(\cdot):R \N-arrow R\N)-como la función de bronceado hiperbólico.La capa de salida recibe los valores de la última capa oculta y los transforma en valores de salida.</target>
        </trans-unit>
        <trans-unit id="7b2e6a226728025cdbf60737ae02746b4f5067e4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel.</source>
          <target state="translated">La escala de longitud del núcleo.</target>
        </trans-unit>
        <trans-unit id="658641ffb44e40c4155905b8a662123f9fbe36a4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension.</source>
          <target state="translated">La escala de longitud del núcleo.Si flota,se utiliza un núcleo isotrópico.Si es una matriz,se utiliza un núcleo anisotrópico en el que cada dimensión de l define la escala de longitud de la respectiva dimensión del rasgo.</target>
        </trans-unit>
        <trans-unit id="c8dac18109c7577d20154429eb0e5e79a703a997" translate="yes" xml:space="preserve">
          <source>The likelihood of the data set with &lt;code&gt;self.covariance_&lt;/code&gt; as an estimator of its covariance matrix.</source>
          <target state="translated">La probabilidad del conjunto de datos con &lt;code&gt;self.covariance_&lt;/code&gt; como estimador de su matriz de covarianza.</target>
        </trans-unit>
        <trans-unit id="9d0f580fc795d85a96fff8300e9ac1a9bc3bbb0b" translate="yes" xml:space="preserve">
          <source>The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.</source>
          <target state="translated">El modelo lineal entrenado en características polinómicas es capaz de recuperar exactamente los coeficientes polinómicos de entrada.</target>
        </trans-unit>
        <trans-unit id="9cf1fb22576b6c52a81ed3fb3cd57a1b7c534cc0" translate="yes" xml:space="preserve">
          <source>The linear models &lt;code&gt;LinearSVC()&lt;/code&gt; and &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; yield slightly different decision boundaries. This can be a consequence of the following differences:</source>
          <target state="translated">Los modelos lineales &lt;code&gt;LinearSVC()&lt;/code&gt; y &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; producen l&amp;iacute;mites de decisi&amp;oacute;n ligeramente diferentes. Esto puede ser consecuencia de las siguientes diferencias:</target>
        </trans-unit>
        <trans-unit id="e79cf5350a0d3a4c8abb9ba5736a33d5a0c53fd5" translate="yes" xml:space="preserve">
          <source>The linear models assume no interactions between the input variables which likely causes under-fitting. Inserting a polynomial feature extractor (&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt;) indeed increases their discrimative power by 2 points of Gini index. In particular it improves the ability of the models to identify the top 5% riskiest profiles.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="086fe9ce38700dda3eefe00cc625a2fbfd905812" translate="yes" xml:space="preserve">
          <source>The linear operator to apply to the data to get the independent sources. This is equal to the unmixing matrix when &lt;code&gt;whiten&lt;/code&gt; is False, and equal to &lt;code&gt;np.dot(unmixing_matrix, self.whitening_)&lt;/code&gt; when &lt;code&gt;whiten&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3db33e367d642c4ed744d98677d2b41b0f8967e" translate="yes" xml:space="preserve">
          <source>The linear transformation learned during fitting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3af4059661dbcc46e497f8eb762a99fb7562b4b" translate="yes" xml:space="preserve">
          <source>The link function is determined by the &lt;code&gt;link&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7828a03a3850299c9821c4410a12edc3f4715fc7" translate="yes" xml:space="preserve">
          <source>The link function of the GLM, i.e. mapping from linear predictor &lt;code&gt;X @ coeff + intercept&lt;/code&gt; to prediction &lt;code&gt;y_pred&lt;/code&gt;. Option &amp;lsquo;auto&amp;rsquo; sets the link depending on the chosen family as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df16104005b0ad36b12ecec77e07a777907b9657" translate="yes" xml:space="preserve">
          <source>The linkage distance threshold above which, clusters will not be merged. If not &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;n_clusters&lt;/code&gt; must be &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;compute_full_tree&lt;/code&gt; must be &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e68f70b09864b10f1aea37e78800b7b9e97f9614" translate="yes" xml:space="preserve">
          <source>The list of Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. A value of 0 is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while 1 is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b28aee2c83cc27005872232d54b931e2f0eba84" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each cross-validation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02b9c4dcf92e3aac8d7274c2bb8e453c5501153c" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each crossvalidation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">La lista de clasificadores calibrados,uno para cada pliegue de validación cruzada,que se ha instalado en todos menos en el pliegue de validación y se ha calibrado en el pliegue de validación.</target>
        </trans-unit>
        <trans-unit id="89a2f4c0656f755e155226e23cfb6b592b80d152" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end,
-start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after such nested smaller clusters. Since &lt;code&gt;labels&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(clusters) &amp;gt;
np.unique(labels)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b68d75689eb0ac219c6fca1ed0b8741a6d57de9" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end, -start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after those smaller ones. Since &lt;code&gt;labels_&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(cluster_hierarchy_) &amp;gt; np.unique(optics.labels_)&lt;/code&gt;. Please also note that these indices are of the &lt;code&gt;ordering_&lt;/code&gt;, i.e. &lt;code&gt;X[ordering_][start:end + 1]&lt;/code&gt; form a cluster. Only available when &lt;code&gt;cluster_method='xi'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="662dd2cdb21b969ce9a44b42150a945acbfed523" translate="yes" xml:space="preserve">
          <source>The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.</source>
          <target state="translated">La lista de valores de la función objetivo y la brecha dual en cada iteración.Se devuelve sólo si return_costs es True.</target>
        </trans-unit>
        <trans-unit id="177835998b74cd6b15a39a13b3bf0448af9a1ccf" translate="yes" xml:space="preserve">
          <source>The local outlier factor (LOF) of a sample captures its supposed &amp;lsquo;degree of abnormality&amp;rsquo;. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.</source>
          <target state="translated">El factor de valor at&amp;iacute;pico local (LOF) de una muestra captura su supuesto &quot;grado de anormalidad&quot;. Es el promedio de la relaci&amp;oacute;n entre la densidad de accesibilidad local de una muestra y las de sus k vecinos m&amp;aacute;s cercanos.</target>
        </trans-unit>
        <trans-unit id="f5a19f67242aeecdaccb15b2c01bb7b8a32da9cd" translate="yes" xml:space="preserve">
          <source>The log likelihood at each iteration.</source>
          <target state="translated">La probabilidad del registro en cada iteración.</target>
        </trans-unit>
        <trans-unit id="d6423c4071c8619e84a84f9bcba64b070be431d3" translate="yes" xml:space="preserve">
          <source>The log-marginal-likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt;</source>
          <target state="translated">La probabilidad log-marginal de &lt;code&gt;self.kernel_.theta&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28c02970f0172adf1161a050ed9ce282eb08888a" translate="yes" xml:space="preserve">
          <source>The log-posterior of LDA can also be written &lt;a href=&quot;#id7&quot; id=&quot;id1&quot;&gt;3&lt;/a&gt; as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9f3852ad5acaf7d90c57ecdaa290bd2f37a1610" translate="yes" xml:space="preserve">
          <source>The log-transformed bounds on the kernel&amp;rsquo;s hyperparameters theta</source>
          <target state="translated">Los l&amp;iacute;mites transformados logar&amp;iacute;tmicamente en los hiperpar&amp;aacute;metros del kernel theta</target>
        </trans-unit>
        <trans-unit id="997c53b0e19e31f9cbe762633463d7411a04ec12" translate="yes" xml:space="preserve">
          <source>The logarithm used is the natural logarithm (base-e).</source>
          <target state="translated">El logaritmo utilizado es el logaritmo natural (base-e).</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
