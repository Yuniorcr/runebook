<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="d1e3e38243b5c0275e9fe55822527a766daf84e5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.cross_val_score&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.model_selection.cross_val_score&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="cd66ce686ba75391d858a270576652221e9366cb" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.cross_validate&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64bdbb73303147fbfb7b4c72e2c5b9f862e6846a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.learning_curve&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.model_selection.learning_curve&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="058189237019f568402a807ff0f37d6659f730bd" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.permutation_test_score&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.model_selection.permutation_test_score&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a35d588868114f2ff75fd130567f7e47c8648990" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.train_test_split&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.model_selection.train_test_split&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c436bb41de4b1b0cbaf9f2a30c84739212eaf00c" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.validation_curve&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.model_selection.validation_curve&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="8c90ea159f2ef33465cc5505a41ebcb13d8fa556" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3b803ea3410b546ac8080c64d5a283e65161ec36" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.multioutput.ClassifierChain&lt;/code&gt;</source>
          <target state="translated">Ejemplos usando &lt;code&gt;sklearn.multioutput.ClassifierChain&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="175a335aedce67fab5414504b5b10913957177f2" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.multioutput.MultiOutputRegressor&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.multioutput.MultiOutputRegressor&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7e5276c814e14e2874f1abc67268ecba0a9384b5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.BernoulliNB&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.naive_bayes.BernoulliNB&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="07d68dd4fe4a915ea6b5e32e2eca6d299d857bb7" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.ComplementNB&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.naive_bayes.ComplementNB&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="af3404bd8b46bee3672ba86cfbae1ef5ccc5adfa" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.GaussianNB&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.naive_bayes.GaussianNB&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="141d21773d99f3975d70ea9e9ce62f28b143b5ad" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.MultinomialNB&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.naive_bayes.MultinomialNB&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="5d999f3df66c782268c5cd0602cb79c507903292" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KNeighborsClassifier&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.neighbors.KNeighborsClassifier&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f26682456d970fbdab48470c970d2b2cf9d0be01" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KNeighborsRegressor&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.neighbors.KNeighborsRegressor&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="da4ee64191a33554442daf81f4b75c328c16de8c" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KNeighborsTransformer&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b0ca2d2cfcae04b6e8f648035a263800eba906a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="135f507596f50f4128ab7f674fa719851e06ba93" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.LocalOutlierFactor&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.neighbors.LocalOutlierFactor&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2d07ff0ebbbb5614d7d55c03e06e43f1f3f6d8b5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.NearestCentroid&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.neighbors.NearestCentroid&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0749621ad934aca64b6d4bb597f488ca85f63f5a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.NeighborhoodComponentsAnalysis&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e409523b85d3bf8b35de4b9a79e58e19412df01" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="92b259d24e226f9725b7a2251b317d28f69ae1de" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neural_network.BernoulliRBM&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.neural_network.BernoulliRBM&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="bc26545aa247592ad5cae0c82982049aecb04005" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neural_network.MLPClassifier&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.neural_network.MLPClassifier&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="439d6a7e65183ef0435c18b6b323c74a5c51b635" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neural_network.MLPRegressor&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa6ee186b720b58c48c82ddfe44e7546cce24778" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.FeatureUnion&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.pipeline.FeatureUnion&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="998ec3583b26e3fb4a4ed0aff8fa32c106d8d3fd" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;</source>
          <target state="translated">Ejemplos usando &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ce16abd7de545850afcb200375d7496f51a99f86" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.make_pipeline&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.pipeline.make_pipeline&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="22b8f1c3009cd0b959254500e33f171fb13ddc5e" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.make_union&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.pipeline.make_union&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2542a7119b1badd4a5e17e03f5405355cc7a6b4e" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.FunctionTransformer&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.FunctionTransformer&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c5ebd1b718c2b56f55f671f8d4b702c99b6f3049" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.KBinsDiscretizer&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.KBinsDiscretizer&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c4381d9fd377759510c6b6b975b0aef8ecf5740c" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.MaxAbsScaler&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.MaxAbsScaler&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6c598f0cbf4e71a390fc3aa76e817310e670b681" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.MinMaxScaler&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.MinMaxScaler&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c1bc49d9937f0e2a5f4b0d4692671125b0d008b2" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.Normalizer&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.Normalizer&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="149136e5bc700b2e0a7747b2cd498f47536f44b4" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;</source>
          <target state="translated">Ejemplos usando &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="11c249d2a654ff2f1c2cc110185680b161c2af36" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56c4939b9d0afea5d1cebbe13cfe733258c34c00" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.PolynomialFeatures&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.PolynomialFeatures&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3a155bf7a30ca61ce1f29447001d55507068d346" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.PowerTransformer&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.PowerTransformer&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="deb84a64d1425831a8777aa592bd6255271f75f8" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.QuantileTransformer&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.QuantileTransformer&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6a98bd8bd28f139beba8d2222e8707ab286c5640" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.RobustScaler&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.RobustScaler&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="cc6025242e9ac27ec2143cf98f98385064f91436" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt;</source>
          <target state="translated">Ejemplos usando &lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="be97d0bdf7a32b2c8b6fa3970798bb89bf783b34" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.label_binarize&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.label_binarize&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="36fa08e3b378f54ed6dffaaf0e419793c6ba101a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.minmax_scale&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.minmax_scale&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="69e893f40e9fcd420f194c86a5ec119b1f003520" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.quantile_transform&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.quantile_transform&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f1b108e5d448ff373a393340cccb912e8ffb72af" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.scale&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.preprocessing.scale&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="955e7e901699a2db4e4172aa9ab76068eea1b92f" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4c4748cf4849c5fc863bc575791af8141a34cef5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="88c1a4bb06d05e08ee20fd615bb4b53da175dc3d" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.semi_supervised.LabelSpreading&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.semi_supervised.LabelSpreading&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="737ca8cb1ccf3eeb065178263662a6be9c3d9649" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.set_config&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cd962b7c530c80e05b210277444c2bad820ca1e" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="edbf5922b6f27be5c5403e10197b447705e6f9cb" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.NuSVC&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.svm.NuSVC&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="12c9b218e18b941578bc9aca3c23747e42f54c1b" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="dea2aede79f19fb95e5148ee5c11b3c7f223323a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6bc3b4bf10642b79be6b77851a23e142b0ca36cf" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.SVC&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.svm.SVC&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c4dcb2fd7042cd90b5cd989b10ea50e9efa48d50" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.SVR&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.svm.SVR&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2a2669f574a679afac893b53b03bdb1bd3060d05" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="15e7f4eb0d23fc3ecea22ca0e3a3ebdefcef3322" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.tree.DecisionTreeClassifier&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.tree.DecisionTreeClassifier&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="65c5a0a1817f47dc4a24c72d932486e31f108484" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.tree.DecisionTreeRegressor&lt;/code&gt;</source>
          <target state="translated">Ejemplos que usan &lt;code&gt;sklearn.tree.DecisionTreeRegressor&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="607ad6107d22cda5b789ada19b3e439fe3038b27" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.tree.plot_tree&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b4d8799f6ce5100bf207b65e66619307f668cac" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="668a018e39621dfc09781d5e1a018ed078089b72" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.Memory&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.utils.Memory&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7213df901109f2c1652eb65fa0f35fe0ff18cd9d" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.check_random_state&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.utils.check_random_state&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e9dab354869145a323333bb1a6f4abfc5f1cb7b1" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.estimator_checks.parametrize_with_checks&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2865372d64705a746f4e393e76c01c0a1fae3b37" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.extmath.density&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.utils.extmath.density&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="da99fb3fb0ad21b853976b89d77d08cdf2ec4f84" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.gen_even_slices&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="851edad00347cd3937bd7ea24424c348d471aca4" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.metaestimators.if_delegate_has_method&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08628ee68b552a536a7e40670b9091a9987d81dc" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.shuffle&lt;/code&gt;</source>
          <target state="translated">Ejemplos que utilizan &lt;code&gt;sklearn.utils.shuffle&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fb3447b632f6a431215776dcf254a01001a40c4f" translate="yes" xml:space="preserve">
          <source>Examples:</source>
          <target state="translated">Examples:</target>
        </trans-unit>
        <trans-unit id="2c25bc9aea0135b8a44078eaa686262d861b8d2c" translate="yes" xml:space="preserve">
          <source>Exception class to raise if estimator is used before fitting.</source>
          <target state="translated">Clase de excepción para aumentar si se usa el estimador antes de la adaptación.</target>
        </trans-unit>
        <trans-unit id="7233f2cfa7da608e08349effe3a0829359d064c0" translate="yes" xml:space="preserve">
          <source>Exception.with_traceback(tb) &amp;ndash; set self.__traceback__ to tb and return self.</source>
          <target state="translated">Exception.with_traceback (tb): establece self .__ traceback__ en tb y devuelve self.</target>
        </trans-unit>
        <trans-unit id="adb63819e55094a1c321051667ef69bc97910b3c" translate="yes" xml:space="preserve">
          <source>Exercise 1: Language identification</source>
          <target state="translated">Ejercicio 1:Identificación del lenguaje</target>
        </trans-unit>
        <trans-unit id="f2d31e2f590f63884cdac3b6e0353c56e95636fd" translate="yes" xml:space="preserve">
          <source>Exercise 2: Sentiment Analysis on movie reviews</source>
          <target state="translated">Ejercicio 2:Análisis de los sentimientos en las críticas de películas</target>
        </trans-unit>
        <trans-unit id="f076754d245dfa0bb055ce293bbe18bfa8d62689" translate="yes" xml:space="preserve">
          <source>Exercise 3: CLI text classification utility</source>
          <target state="translated">Ejercicio 3:Utilidad de clasificación de texto CLI</target>
        </trans-unit>
        <trans-unit id="4dc503dafcf231e8065504c4cd9f19a0dcdfc147" translate="yes" xml:space="preserve">
          <source>Exercises</source>
          <target state="translated">Exercises</target>
        </trans-unit>
        <trans-unit id="9b4e2cce8211934c05426343255dbbe40e6fc29d" translate="yes" xml:space="preserve">
          <source>Exercises for the tutorials</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e172501d8e170f5e501dbb7e10f621b359347edb" translate="yes" xml:space="preserve">
          <source>Exhaustive search over specified parameter values for an estimator.</source>
          <target state="translated">Búsqueda exhaustiva sobre los valores de los parámetros especificados para un estimador.</target>
        </trans-unit>
        <trans-unit id="827e74ef83aecac9c6f9fc861a3a010bc5589266" translate="yes" xml:space="preserve">
          <source>Exp-Sine-Squared kernel (aka periodic kernel).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9607bbaaa4524856cf2140445f92e407ff55546" translate="yes" xml:space="preserve">
          <source>Exp-Sine-Squared kernel.</source>
          <target state="translated">Núcleo Exp-Sine cuadrado.</target>
        </trans-unit>
        <trans-unit id="3b0c107b231b7c7d2dd1b666566c6604385763d3" translate="yes" xml:space="preserve">
          <source>Expected results for the top 5 most represented people in the dataset:</source>
          <target state="translated">Los resultados esperados para las 5 personas más representadas en el conjunto de datos:</target>
        </trans-unit>
        <trans-unit id="98312dc3857136b93e4f949a4e72a6712170156c" translate="yes" xml:space="preserve">
          <source>Explained variance regression score function</source>
          <target state="translated">Función de puntuación de regresión de la varianza explicada</target>
        </trans-unit>
        <trans-unit id="31162dbfd1baf8644ae388945633979e4d4b742f" translate="yes" xml:space="preserve">
          <source>Explicit feature map approximation for RBF kernels</source>
          <target state="translated">Aproximación explícita del mapa de características para los núcleos de RBF</target>
        </trans-unit>
        <trans-unit id="2139158fefd69dd7900f572f929357923b2e9c08" translate="yes" xml:space="preserve">
          <source>Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=&amp;rsquo;adam&amp;rsquo;</source>
          <target state="translated">La tasa de decaimiento exponencial para las estimaciones del vector de primer momento en adam debe estar en [0, 1). Solo se usa cuando solver = 'adam'</target>
        </trans-unit>
        <trans-unit id="3b517f2d5130d355e5abbd42b6b2d0de0b8022fe" translate="yes" xml:space="preserve">
          <source>Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=&amp;rsquo;adam&amp;rsquo;</source>
          <target state="translated">La tasa de decaimiento exponencial para las estimaciones del segundo vector de momento en ad&amp;aacute;n debe estar en [0, 1). Solo se usa cuando solver = 'adam'</target>
        </trans-unit>
        <trans-unit id="5be074c21803e5b45a92c260ef448b3876757aad" translate="yes" xml:space="preserve">
          <source>Exponential kernel (&lt;code&gt;kernel = 'exponential'&lt;/code&gt;)</source>
          <target state="translated">N&amp;uacute;cleo exponencial ( &lt;code&gt;kernel = 'exponential'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="4e6d09cb3f7c099f340e3cd011063b05dd20e736" translate="yes" xml:space="preserve">
          <source>Exponential loss (&lt;code&gt;'exponential'&lt;/code&gt;): The same loss function as &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt;&lt;code&gt;AdaBoostClassifier&lt;/code&gt;&lt;/a&gt;. Less robust to mislabeled examples than &lt;code&gt;'deviance'&lt;/code&gt;; can only be used for binary classification.</source>
          <target state="translated">P&amp;eacute;rdida exponencial ( &lt;code&gt;'exponential'&lt;/code&gt; ): la misma funci&amp;oacute;n de p&amp;eacute;rdida que &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt; &lt;code&gt;AdaBoostClassifier&lt;/code&gt; &lt;/a&gt; . Menos robusto a los ejemplos mal etiquetados que &lt;code&gt;'deviance'&lt;/code&gt; ; solo se puede utilizar para clasificaci&amp;oacute;n binaria.</target>
        </trans-unit>
        <trans-unit id="82a5349d4a42ae4b58ce233c1fe754cf20695efd" translate="yes" xml:space="preserve">
          <source>Exponentiate kernel by given exponent.</source>
          <target state="translated">Exponer el núcleo por un exponente dado.</target>
        </trans-unit>
        <trans-unit id="6a1fb392b4816003f7cab8689b69b73b81fd13d1" translate="yes" xml:space="preserve">
          <source>Export a decision tree in DOT format.</source>
          <target state="translated">Exportar un árbol de decisión en formato DOT.</target>
        </trans-unit>
        <trans-unit id="862ee3b17a826818107e616215cf3c3a954115aa" translate="yes" xml:space="preserve">
          <source>Exposure</source>
          <target state="translated">Exposure</target>
        </trans-unit>
        <trans-unit id="9eb0650b6756b55d46beda1492ea11acfe7e3431" translate="yes" xml:space="preserve">
          <source>Expresses to what extent the local structure is retained.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03b256629a71913d7df02c71e2c067b3de2f6cf2" translate="yes" xml:space="preserve">
          <source>External Resources, Videos and Talks</source>
          <target state="translated">Recursos externos,vídeos y charlas</target>
        </trans-unit>
        <trans-unit id="6a53253fde7542b58764813563b294277a2499e0" translate="yes" xml:space="preserve">
          <source>External Tutorials</source>
          <target state="translated">Tutoriales externos</target>
        </trans-unit>
        <trans-unit id="d29859b915eecd05832dcd65405884b6cc0d4c40" translate="yes" xml:space="preserve">
          <source>Extra keyword arguments will be passed to matplotlib&amp;rsquo;s &lt;code&gt;plot&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d6114c0a06ffd4fdec489d603f3c2293f085d8e" translate="yes" xml:space="preserve">
          <source>Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the &lt;code&gt;max_features&lt;/code&gt; randomly selected features and the best split among those is chosen. When &lt;code&gt;max_features&lt;/code&gt; is set 1, this amounts to building a totally random decision tree.</source>
          <target state="translated">Los &amp;aacute;rboles adicionales se diferencian de los &amp;aacute;rboles de decisi&amp;oacute;n cl&amp;aacute;sicos en la forma en que se construyen. Cuando se busca la mejor divisi&amp;oacute;n para separar las muestras de un nodo en dos grupos, se dibujan divisiones aleatorias para cada una de las caracter&amp;iacute;sticas &lt;code&gt;max_features&lt;/code&gt; seleccionadas al azar y se elige la mejor divisi&amp;oacute;n entre ellas. Cuando &lt;code&gt;max_features&lt;/code&gt; se establece en 1, esto equivale a construir un &amp;aacute;rbol de decisiones totalmente aleatorio.</target>
        </trans-unit>
        <trans-unit id="9955e456c4c0c464fbdd3775651693383fea52c1" translate="yes" xml:space="preserve">
          <source>Extract an ordered array of unique labels</source>
          <target state="translated">Extraer una serie ordenada de etiquetas únicas</target>
        </trans-unit>
        <trans-unit id="73640fcc3fc33a24884d5d906a4df64cbbe3523c" translate="yes" xml:space="preserve">
          <source>Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor.</source>
          <target state="translated">Extraer recuentos de fichas de los documentos de texto en bruto utilizando el vocabulario ajustado con fit o el proporcionado al constructor.</target>
        </trans-unit>
        <trans-unit id="752e876b40a502b1de5591e926917e4bc7914d7e" translate="yes" xml:space="preserve">
          <source>Extracting features from text files</source>
          <target state="translated">Extracción de características de los archivos de texto</target>
        </trans-unit>
        <trans-unit id="d28d7a25a071634693f533b81722275ae4a2006b" translate="yes" xml:space="preserve">
          <source>Extracting the clusters runs in linear time. Note that this results in &lt;code&gt;labels_&lt;/code&gt; which are close to a &lt;a href=&quot;sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; with similar settings and &lt;code&gt;eps&lt;/code&gt;, only if &lt;code&gt;eps&lt;/code&gt; is close to &lt;code&gt;max_eps&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1c8b18c9f903ec74b12bbc8b3c4f7151c923f8b" translate="yes" xml:space="preserve">
          <source>Extracts an ordered list of points and reachability distances, and performs initial clustering using &lt;code&gt;max_eps&lt;/code&gt; distance specified at OPTICS object instantiation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d1d5b7b89c16041c19e08ba2433ff6ccea2c09b" translate="yes" xml:space="preserve">
          <source>Extracts patches from a collection of images</source>
          <target state="translated">Extrae parches de una colección de imágenes</target>
        </trans-unit>
        <trans-unit id="20b1f470ded66ddce46bb1f3957f49e776657ce0" translate="yes" xml:space="preserve">
          <source>F values of features.</source>
          <target state="translated">Valores F de las características.</target>
        </trans-unit>
        <trans-unit id="94361d25a9823c7799f46133208e1994c901281f" translate="yes" xml:space="preserve">
          <source>F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task.</source>
          <target state="translated">Puntuación F-beta de la clase positiva en la clasificación binaria o promedio ponderado de la puntuación F-beta de cada clase para la tarea multiclase.</target>
        </trans-unit>
        <trans-unit id="b88bc0b15f26e6bb0b6f496fb42c5c9bb95eea78" translate="yes" xml:space="preserve">
          <source>F-value between label/feature for regression tasks.</source>
          <target state="translated">Valor F entre etiqueta/característica para las tareas de regresión.</target>
        </trans-unit>
        <trans-unit id="3ca92f821d80e2b3e5b34951989fdd6926d62950" translate="yes" xml:space="preserve">
          <source>F1 score of the positive class in binary classification or weighted average of the F1 scores of each class for the multiclass task.</source>
          <target state="translated">Puntuación F1 de la clase positiva en la clasificación binaria o promedio ponderado de las puntuaciones F1 de cada clase para la tarea multiclase.</target>
        </trans-unit>
        <trans-unit id="03688ba6aa340b87549088aa5739944cb6b1dc73" translate="yes" xml:space="preserve">
          <source>FAQ</source>
          <target state="translated">FAQ</target>
        </trans-unit>
        <trans-unit id="761451a93e0c15b8090688d5167bdaf6518e982d" translate="yes" xml:space="preserve">
          <source>FPR test stands for False Positive Rate test. It controls the total amount of false detections.</source>
          <target state="translated">La prueba FPR es una prueba de tasa de falsos positivos.Controla la cantidad total de detecciones falsas.</target>
        </trans-unit>
        <trans-unit id="385b798ea2337dc2cc46e2c01984384fa2b4a883" translate="yes" xml:space="preserve">
          <source>F_beta</source>
          <target state="translated">F_beta</target>
        </trans-unit>
        <trans-unit id="272ad30c6a89ef4d06060249f8d92df03b3df406" translate="yes" xml:space="preserve">
          <source>Face completion with a multi-output estimators</source>
          <target state="translated">Enfrentar la finalización con un estimador de salida múltiple</target>
        </trans-unit>
        <trans-unit id="0507c7e4982962e832b85d06191a2b4d2bebd20a" translate="yes" xml:space="preserve">
          <source>Face recognition with eigenfaces</source>
          <target state="translated">Reconocimiento del rostro con eigenfaces</target>
        </trans-unit>
        <trans-unit id="2a0151d21d57d7f913fb01048c891608a6dbd1dd" translate="yes" xml:space="preserve">
          <source>Face, a 1024 x 768 size image of a raccoon face, is used here to illustrate how &lt;code&gt;k&lt;/code&gt;-means is used for vector quantization.</source>
          <target state="translated">Cara, una imagen de tama&amp;ntilde;o 1024 x 768 de una cara de mapache, se usa aqu&amp;iacute; para ilustrar c&amp;oacute;mo se usa &lt;code&gt;k&lt;/code&gt; -medias para la cuantificaci&amp;oacute;n vectorial.</target>
        </trans-unit>
        <trans-unit id="e470f0e1bd32044d0ac0cbb19036da58e0de1f71" translate="yes" xml:space="preserve">
          <source>Faces dataset decompositions</source>
          <target state="translated">Enfrenta las descomposiciones de los conjuntos de datos</target>
        </trans-unit>
        <trans-unit id="05a6c8a2b029e9776dffd4a4e031ba78fd52aae9" translate="yes" xml:space="preserve">
          <source>Faces recognition example using eigenfaces and SVMs</source>
          <target state="translated">Ejemplo de reconocimiento de rostros utilizando eigenfaces y SVM</target>
        </trans-unit>
        <trans-unit id="1395bbf4ff3a0e8184103b7a1548ed436935fe5c" translate="yes" xml:space="preserve">
          <source>Factor Analysis (FA)</source>
          <target state="translated">Análisis factorial (FA)</target>
        </trans-unit>
        <trans-unit id="ef94831c8deb9ad37fe90ab4fff5e44828bcfdae" translate="yes" xml:space="preserve">
          <source>Factor analysis &lt;em&gt;can&lt;/em&gt; produce similar components (the columns of its loading matrix) to &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;. However, one can not make any general statements about these components (e.g. whether they are orthogonal):</source>
          <target state="translated">El an&amp;aacute;lisis factorial &lt;em&gt;puede&lt;/em&gt; producir componentes similares (las columnas de su matriz de carga) al &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; . Sin embargo, no se pueden hacer declaraciones generales sobre estos componentes (por ejemplo, si son ortogonales):</target>
        </trans-unit>
        <trans-unit id="d48954e3f19174382e76ae0104e93e844b15de73" translate="yes" xml:space="preserve">
          <source>FactorAnalysis performs a maximum likelihood estimate of the so-called &lt;code&gt;loading&lt;/code&gt; matrix, the transformation of the latent variables to the observed ones, using SVD based approach.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6452044c700752d1602ef512b59b643cc8ae2400" translate="yes" xml:space="preserve">
          <source>FactorAnalysis performs a maximum likelihood estimate of the so-called &lt;code&gt;loading&lt;/code&gt; matrix, the transformation of the latent variables to the observed ones, using expectation-maximization (EM).</source>
          <target state="translated">FactorAnalysis realiza una estimaci&amp;oacute;n de m&amp;aacute;xima verosimilitud de la denominada matriz de &lt;code&gt;loading&lt;/code&gt; , la transformaci&amp;oacute;n de las variables latentes a las observadas, utilizando la maximizaci&amp;oacute;n de expectativas (EM).</target>
        </trans-unit>
        <trans-unit id="cf3867c5acb3e93b6681ae294efcb69608ed1285" translate="yes" xml:space="preserve">
          <source>Factorization matrix, sometimes called &amp;lsquo;dictionary&amp;rsquo;.</source>
          <target state="translated">Matriz de factorizaci&amp;oacute;n, a veces llamada &quot;diccionario&quot;.</target>
        </trans-unit>
        <trans-unit id="c37fb3c7cf083e46f6a90c90a7e2709ff0a25468" translate="yes" xml:space="preserve">
          <source>False : never precompute distances</source>
          <target state="translated">Falso:nunca precalcule las distancias</target>
        </trans-unit>
        <trans-unit id="4031377a355ac026bbc01c0a8c66abf927d5347f" translate="yes" xml:space="preserve">
          <source>False : never precompute distances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f59f0377a675667ec374342cc81b9550d388560" translate="yes" xml:space="preserve">
          <source>False positive rate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25cefd866e6e705dad47ce327a0915fc7b301c18" translate="yes" xml:space="preserve">
          <source>False when &lt;code&gt;y&lt;/code&gt;&amp;rsquo;s shape is (n_samples, ) or (n_samples, 1) during fit otherwise True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b5c093eaf163ad057d3c3eb085ce8a982ff3356" translate="yes" xml:space="preserve">
          <source>False: accept both np.inf and np.nan in X.</source>
          <target state="translated">Falso:aceptar tanto np.inf como np.nan en X.</target>
        </trans-unit>
        <trans-unit id="e8392e19182108180157ea67d85038a25937bf1d" translate="yes" xml:space="preserve">
          <source>False: accepts np.inf, np.nan, pd.NA in X.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed9971d141108d2bfea275398777be51a58308ae" translate="yes" xml:space="preserve">
          <source>False: accepts np.inf, np.nan, pd.NA in array.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="804378d4f4edbfd6d3a924fd7282c07b3c8233b0" translate="yes" xml:space="preserve">
          <source>False: the results is casted to a signed int</source>
          <target state="translated">Falso:los resultados son vertidos a un int firmado</target>
        </trans-unit>
        <trans-unit id="b73e8a734794b535117e86ab6a6ba9f9a65b9a31" translate="yes" xml:space="preserve">
          <source>Fan, Rong-En, et al., &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf&quot;&gt;&amp;ldquo;LIBLINEAR: A library for large linear classification.&amp;rdquo;&lt;/a&gt;, Journal of machine learning research 9.Aug (2008): 1871-1874.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c87c316f44a57817f64366d0bec154e1f9523fd3" translate="yes" xml:space="preserve">
          <source>Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Here&amp;rsquo;s a &lt;code&gt;CountVectorizer&lt;/code&gt; with a tokenizer and lemmatizer using &lt;a href=&quot;http://www.nltk.org&quot;&gt;NLTK&lt;/a&gt;:</source>
          <target state="translated">El an&amp;aacute;lisis sofisticado a nivel de token, como la derivaci&amp;oacute;n, la lematizaci&amp;oacute;n, la divisi&amp;oacute;n compuesta, el filtrado basado en la parte del discurso, etc., no se incluyen en la base de c&amp;oacute;digo de scikit-learn, pero se pueden agregar personalizando el tokenizador o el analizador. Aqu&amp;iacute; hay un &lt;code&gt;CountVectorizer&lt;/code&gt; con un tokenizador y un lematizador usando &lt;a href=&quot;http://www.nltk.org&quot;&gt;NLTK&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="47db1ac61ed9cca1bf16f757f65fe37b19da02d2" translate="yes" xml:space="preserve">
          <source>Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Here&amp;rsquo;s a &lt;code&gt;CountVectorizer&lt;/code&gt; with a tokenizer and lemmatizer using &lt;a href=&quot;https://www.nltk.org/&quot;&gt;NLTK&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="000599c940f606b6105f7d1916791a94094aca12" translate="yes" xml:space="preserve">
          <source>Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for \(N\) samples in \(D\) dimensions, this approach scales as \(O[D N^2]\). Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples \(N\) grows, the brute-force approach quickly becomes infeasible. In the classes within &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt;, brute-force neighbors searches are specified using the keyword &lt;code&gt;algorithm = 'brute'&lt;/code&gt;, and are computed using the routines available in &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">El c&amp;aacute;lculo r&amp;aacute;pido de los vecinos m&amp;aacute;s cercanos es un &amp;aacute;rea activa de investigaci&amp;oacute;n en el aprendizaje autom&amp;aacute;tico. La implementaci&amp;oacute;n de b&amp;uacute;squeda de vecinos m&amp;aacute;s ingenua implica el c&amp;aacute;lculo de fuerza bruta de las distancias entre todos los pares de puntos en el conjunto de datos: para \ (N \) muestras en \ (D \) dimensiones, este enfoque se escala como \ (O [DN ^ 2] \). Las b&amp;uacute;squedas eficientes de vecinos por fuerza bruta pueden ser muy competitivas para muestras de datos peque&amp;ntilde;as. Sin embargo, a medida que aumenta el n&amp;uacute;mero de muestras \ (N \), el m&amp;eacute;todo de fuerza bruta se vuelve r&amp;aacute;pidamente inviable. En las clases dentro de &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt; , las b&amp;uacute;squedas de vecinos de fuerza bruta se especifican utilizando la palabra clave &lt;code&gt;algorithm = 'brute'&lt;/code&gt; , y se calculan utilizando las rutinas disponibles en &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2e91ff193b9320036698a113daf46d8118a44e7d" translate="yes" xml:space="preserve">
          <source>FastICA on 2D point clouds</source>
          <target state="translated">FastICA en nubes de puntos 2D</target>
        </trans-unit>
        <trans-unit id="6921319b009c74978f9e5104f25e9063c7cf2c6b" translate="yes" xml:space="preserve">
          <source>FastICA: a fast algorithm for Independent Component Analysis.</source>
          <target state="translated">FastICA:un algoritmo rápido para el análisis de componentes independientes.</target>
        </trans-unit>
        <trans-unit id="c2070bedc34b06cdbf740c2fed5e122d3efb93a7" translate="yes" xml:space="preserve">
          <source>Faster for large datasets</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fa15d231f996464c9e743e456c3d6855ab0d8f6" translate="yes" xml:space="preserve">
          <source>Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition Letters, 2006, 27(8):861-874.</source>
          <target state="translated">Fawcett T.Una introducción al análisis ROC.Pattern Recognition Letters,2006,27(8):861-874.</target>
        </trans-unit>
        <trans-unit id="0724b7da2e7893e2aa61b560332f137d5b0c3174" translate="yes" xml:space="preserve">
          <source>Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861-874.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6d44f9610f58495afc33ead5474258ef3212417" translate="yes" xml:space="preserve">
          <source>Fawcett, T., 2001. &lt;a href=&quot;http://ieeexplore.ieee.org/document/989510/&quot;&gt;Using rule sets to maximize ROC performance&lt;/a&gt; In Data Mining, 2001. Proceedings IEEE International Conference, pp. 131-138.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="777dc3527e71dea0036f160bc11c11f47afad8e4" translate="yes" xml:space="preserve">
          <source>Fawcett, T., 2006. &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S016786550500303X&quot;&gt;An introduction to ROC analysis.&lt;/a&gt; Pattern Recognition Letters, 27(8), pp. 861-874.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77834b29664ccb5e5cc9173718797e7ec674b1ed" translate="yes" xml:space="preserve">
          <source>Feature 0 (median income in a block) and feature 5 (number of households) of the &lt;a href=&quot;http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;California housing dataset&lt;/a&gt; have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.</source>
          <target state="translated">La caracter&amp;iacute;stica 0 (ingreso medio en un bloque) y la caracter&amp;iacute;stica 5 (n&amp;uacute;mero de hogares) del &lt;a href=&quot;http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;conjunto de datos de vivienda de California&lt;/a&gt; tienen escalas muy diferentes y contienen algunos valores at&amp;iacute;picos muy grandes. Estas dos caracter&amp;iacute;sticas generan dificultades para visualizar los datos y, lo que es m&amp;aacute;s importante, pueden degradar el rendimiento predictivo de muchos algoritmos de aprendizaje autom&amp;aacute;tico. Los datos sin escala tambi&amp;eacute;n pueden ralentizar o incluso prevenir la convergencia de muchos estimadores basados ​​en gradientes.</target>
        </trans-unit>
        <trans-unit id="d43b2b4b53fe06f630d6e6fea5ae39ec97ef2464" translate="yes" xml:space="preserve">
          <source>Feature 0 (median income in a block) and feature 5 (number of households) of the &lt;a href=&quot;https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;California housing dataset&lt;/a&gt; have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e2bfc0d574ebb3c0866208d70d9a4defc9229e8" translate="yes" xml:space="preserve">
          <source>Feature Selection</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3290cbe981e79caf9ab6f89a8812507aa114a727" translate="yes" xml:space="preserve">
          <source>Feature agglomeration</source>
          <target state="translated">Aglomeración de características</target>
        </trans-unit>
        <trans-unit id="81f8ec148b187c97182bf2c56b5ce15909bcbae4" translate="yes" xml:space="preserve">
          <source>Feature agglomeration vs. univariate selection</source>
          <target state="translated">Aglomeración de características vs.selección univariante</target>
        </trans-unit>
        <trans-unit id="49e1dcf68a4912e92e5e7f68710255b3ca05471e" translate="yes" xml:space="preserve">
          <source>Feature discretization</source>
          <target state="translated">La discretización de las características</target>
        </trans-unit>
        <trans-unit id="3c43171269f5f432d869bba89f82c3b54ca0debf" translate="yes" xml:space="preserve">
          <source>Feature extraction</source>
          <target state="translated">Extracción de características</target>
        </trans-unit>
        <trans-unit id="df5954aef1e3ea02a4e2fe61d1e0e10b9f726b5a" translate="yes" xml:space="preserve">
          <source>Feature extraction and normalization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aedfee8f7e4b62695a0b89930709eed1e654b00f" translate="yes" xml:space="preserve">
          <source>Feature extraction is very different from &lt;a href=&quot;feature_selection#feature-selection&quot;&gt;Feature selection&lt;/a&gt;: the former consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features.</source>
          <target state="translated">La extracci&amp;oacute;n de caracter&amp;iacute;sticas es muy diferente de la &lt;a href=&quot;feature_selection#feature-selection&quot;&gt;selecci&amp;oacute;n de caracter&amp;iacute;sticas&lt;/a&gt; : la primera consiste en transformar datos arbitrarios, como texto o im&amp;aacute;genes, en caracter&amp;iacute;sticas num&amp;eacute;ricas utilizables para el aprendizaje autom&amp;aacute;tico. Esta &amp;uacute;ltima es una t&amp;eacute;cnica de aprendizaje autom&amp;aacute;tico aplicada a estas caracter&amp;iacute;sticas.</target>
        </trans-unit>
        <trans-unit id="4c3a870a4e311a31f848b5314c8f3949d8e31e78" translate="yes" xml:space="preserve">
          <source>Feature hashing can be employed in document classification, but unlike &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;text.CountVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see &lt;a href=&quot;#hashing-vectorizer&quot;&gt;Vectorizing a large text corpus with the hashing trick&lt;/a&gt;, below, for a combined tokenizer/hasher.</source>
          <target state="translated">El hash de caracter&amp;iacute;sticas se puede emplear en la clasificaci&amp;oacute;n de documentos, pero a diferencia de &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;text.CountVectorizer&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt; no realiza la divisi&amp;oacute;n de palabras ni ning&amp;uacute;n otro preprocesamiento, excepto la codificaci&amp;oacute;n Unicode a UTF-8; vea &lt;a href=&quot;#hashing-vectorizer&quot;&gt;Vectorizar un corpus de texto grande con el truco de hash&lt;/a&gt; , a continuaci&amp;oacute;n, para un tokenizador / hash combinado.</target>
        </trans-unit>
        <trans-unit id="7bb8776f4e73559816ad10cf154be669f7df47cf" translate="yes" xml:space="preserve">
          <source>Feature importances with forests of trees</source>
          <target state="translated">Presenta las importaciones con los bosques de árboles</target>
        </trans-unit>
        <trans-unit id="561e2555a0c1659bb31a6148c1abe250489ca708" translate="yes" xml:space="preserve">
          <source>Feature mappings for the samples in X.</source>
          <target state="translated">Mapeo de características para las muestras en X.</target>
        </trans-unit>
        <trans-unit id="b96ff43d78f2a1f5a4e1d891c2cc36e4c380c32b" translate="yes" xml:space="preserve">
          <source>Feature matrix, for use with estimators or further transformers.</source>
          <target state="translated">Matriz de características,para su uso con estimadores o transformadores posteriores.</target>
        </trans-unit>
        <trans-unit id="66d052b121f901629f4ce88124bd7cb2635fe497" translate="yes" xml:space="preserve">
          <source>Feature matrix.</source>
          <target state="translated">Matriz de características.</target>
        </trans-unit>
        <trans-unit id="c4587a739696f602e34f3bba74d9106ccf3fff49" translate="yes" xml:space="preserve">
          <source>Feature names corresponding to the indices in &lt;code&gt;features&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="975d25794ea94f600d607356398714d33388d870" translate="yes" xml:space="preserve">
          <source>Feature names of type byte string are used as-is. Unicode strings are converted to UTF-8 first, but no Unicode normalization is done. Feature values must be (finite) numbers.</source>
          <target state="translated">Los nombres de las características del tipo cadena de bytes se utilizan tal cual.Las cadenas Unicode se convierten primero a UTF-8,pero no se hace ninguna normalización Unicode.Los valores de las características deben ser números (finitos).</target>
        </trans-unit>
        <trans-unit id="9509bbe5f8817e2ca0562d6baac6f0ad1787e1d6" translate="yes" xml:space="preserve">
          <source>Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.</source>
          <target state="translated">Clasificación de características con eliminación recursiva de características y selección validada cruzada del mejor número de características.</target>
        </trans-unit>
        <trans-unit id="f706081a8ad25e74e9a88b9e19a48d5a46a52012" translate="yes" xml:space="preserve">
          <source>Feature ranking with recursive feature elimination.</source>
          <target state="translated">Clasificación de características con eliminación de características recursivas.</target>
        </trans-unit>
        <trans-unit id="7f17a87c5de04e39e04129e9c0737edfeafaee92" translate="yes" xml:space="preserve">
          <source>Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.</source>
          <target state="translated">El escalado de características a través de la estandarización (o normalización de puntaje Z)puede ser un importante paso de preprocesamiento para muchos algoritmos de aprendizaje de máquinas.La normalización implica reescalar las características de manera que tengan las propiedades de una distribución normal estándar con una media de cero y una desviación estándar de uno.</target>
        </trans-unit>
        <trans-unit id="3ac99400fe4169f40a977ea7007419c98db61771" translate="yes" xml:space="preserve">
          <source>Feature scores between 0 and 1 for all values of the regularization parameter. The reference article suggests &lt;code&gt;scores_&lt;/code&gt; is the max of &lt;code&gt;all_scores_&lt;/code&gt;.</source>
          <target state="translated">Las caracter&amp;iacute;sticas tienen puntuaciones entre 0 y 1 para todos los valores del par&amp;aacute;metro de regularizaci&amp;oacute;n. El art&amp;iacute;culo de referencia sugiere que &lt;code&gt;scores_&lt;/code&gt; es el m&amp;aacute;ximo de &lt;code&gt;all_scores_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="aa75d746547685bebfdc2a7e366361e478fa27f0" translate="yes" xml:space="preserve">
          <source>Feature scores between 0 and 1.</source>
          <target state="translated">Las puntuaciones de las características entre 0 y 1.</target>
        </trans-unit>
        <trans-unit id="afb880f1a910829f871ab84d5509ba79bf00b111" translate="yes" xml:space="preserve">
          <source>Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">La selecci&amp;oacute;n de caracter&amp;iacute;sticas se usa generalmente como un paso de procesamiento previo antes de realizar el aprendizaje real. La forma recomendada de hacer esto en scikit-learn es usar un &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="5a515063a6bfb2324e04a5cb7dd1fe0bf9c9b6c2" translate="yes" xml:space="preserve">
          <source>Feature selection mode.</source>
          <target state="translated">Modo de selección de características.</target>
        </trans-unit>
        <trans-unit id="c9b5854c848ad0aec62ba5b490b140e3c7a1bb7d" translate="yes" xml:space="preserve">
          <source>Feature selection using SelectFromModel and LassoCV</source>
          <target state="translated">Selección de características usando SelectFromModel y LassoCV</target>
        </trans-unit>
        <trans-unit id="d13bfd588897e09202c5ea9fa5c11da3f65c72ad" translate="yes" xml:space="preserve">
          <source>Feature selection with sparse data</source>
          <target state="translated">Selección de características con datos escasos</target>
        </trans-unit>
        <trans-unit id="8a9e4457b64c8313be96ab86a00a0aa32201d58b" translate="yes" xml:space="preserve">
          <source>Feature selector that removes all low-variance features.</source>
          <target state="translated">Selector de características que elimina todas las características de baja variación.</target>
        </trans-unit>
        <trans-unit id="e1b0ac4f2f2d7f5b2a31bb18d47b68f8979e7db4" translate="yes" xml:space="preserve">
          <source>Feature transformations with ensembles of trees</source>
          <target state="translated">Las transformaciones de las características con los conjuntos de árboles</target>
        </trans-unit>
        <trans-unit id="9a6ded29908e935164e678c6c6abd840802b0c72" translate="yes" xml:space="preserve">
          <source>Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices.</source>
          <target state="translated">Los valores de las características inferiores o iguales a éste se sustituyen por 0,y los superiores por 1.El umbral no puede ser inferior a 0 para las operaciones en matrices dispersas.</target>
        </trans-unit>
        <trans-unit id="0bf4742e81f7f65623aaef302013934b401b3eb5" translate="yes" xml:space="preserve">
          <source>Feature values in training data (also required for prediction)</source>
          <target state="translated">Valores de las características en los datos de entrenamiento (también requeridos para la predicción)</target>
        </trans-unit>
        <trans-unit id="3252ec9f22752fab5d0e2a9197cc3c5847a58e00" translate="yes" xml:space="preserve">
          <source>Feature vectors or other representations of training data (also required for prediction).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ed37149ccf99f3ffe537e50f5c181c973cd258e" translate="yes" xml:space="preserve">
          <source>Feature vectors or other representations of training data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7599b9ef6a2c582b4507f1b87929e181ce07276" translate="yes" xml:space="preserve">
          <source>Feature vectors; always 2-d.</source>
          <target state="translated">Vectores de características;siempre 2-d.</target>
        </trans-unit>
        <trans-unit id="8425332e36244b1448079ae72c3edeebff1ff3df" translate="yes" xml:space="preserve">
          <source>Feature-wise means</source>
          <target state="translated">En cuanto a las características,significa</target>
        </trans-unit>
        <trans-unit id="4dfce3abcd43918524b8cd8b96300d3efd01d1fb" translate="yes" xml:space="preserve">
          <source>Feature-wise transformation of the data.</source>
          <target state="translated">Transformación de los datos en función de las características.</target>
        </trans-unit>
        <trans-unit id="5ce13d491431cc736ca395a68aa77c25024d518b" translate="yes" xml:space="preserve">
          <source>Feature-wise variances</source>
          <target state="translated">Variaciones de las características</target>
        </trans-unit>
        <trans-unit id="8c7ad0b456fa9bf3ae09e270340fb831f15c3f18" translate="yes" xml:space="preserve">
          <source>FeatureHasher and DictVectorizer Comparison</source>
          <target state="translated">Comparación de FeatureHasher y DictVectorizer</target>
        </trans-unit>
        <trans-unit id="fc338f87a058158eb824b53705961801516a9460" translate="yes" xml:space="preserve">
          <source>Features</source>
          <target state="translated">Features</target>
        </trans-unit>
        <trans-unit id="7b144c2dee4c701c6fc61ec2c427f38a9b6c6529" translate="yes" xml:space="preserve">
          <source>Features 1 and 2 of the diabetes-dataset are fitted and plotted below. It illustrates that although feature 2 has a strong coefficient on the full model, it does not give us much regarding &lt;code&gt;y&lt;/code&gt; when compared to just feature 1</source>
          <target state="translated">Las caracter&amp;iacute;sticas 1 y 2 del conjunto de datos de diabetes se ajustan y se representan a continuaci&amp;oacute;n. Ilustra que aunque la caracter&amp;iacute;stica 2 tiene un fuerte coeficiente en el modelo completo, no nos da mucho con respecto a &lt;code&gt;y&lt;/code&gt; en comparaci&amp;oacute;n con solo la caracter&amp;iacute;stica 1</target>
        </trans-unit>
        <trans-unit id="27bf161ea6af475b73d710eebe7f66e368f22dbd" translate="yes" xml:space="preserve">
          <source>Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.</source>
          <target state="translated">Los rasgos se calculan a partir de una imagen digitalizada de un aspirado de aguja fina (FNA)de una masa mamaria.Describen las características de los núcleos celulares presentes en la imagen.</target>
        </trans-unit>
        <trans-unit id="7dc73780307913b147af45ec74f5c7491dc77d15" translate="yes" xml:space="preserve">
          <source>Features got by optimizing the Huber loss.</source>
          <target state="translated">Características obtenidas al optimizar la pérdida de Huber.</target>
        </trans-unit>
        <trans-unit id="0a06ba45f44f1716120206612e20d59fa0c2d9b4" translate="yes" xml:space="preserve">
          <source>Features that are deemed of &lt;strong&gt;low importance for a bad model&lt;/strong&gt; (low cross-validation score) could be &lt;strong&gt;very important for a good model&lt;/strong&gt;. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but &lt;strong&gt;how important this feature is for a particular model&lt;/strong&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="285c7b098890786c959265d1fe4e4933df08392b" translate="yes" xml:space="preserve">
          <source>Features that do not occur in a sample (mapping) will have a zero value in the resulting array/matrix.</source>
          <target state="translated">Las características que no se produzcan en una muestra (mapeo)tendrán un valor cero en la matriz/matriz resultante.</target>
        </trans-unit>
        <trans-unit id="e9ec5e9ca7278a6df71a586ca47e81c93d4d7336" translate="yes" xml:space="preserve">
          <source>Features which contain all missing values at &lt;code&gt;fit&lt;/code&gt; are discarded upon &lt;code&gt;transform&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71724a67f7aa53e34d06b737e24512de12116bad" translate="yes" xml:space="preserve">
          <source>Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples.</source>
          <target state="translated">Las características con una variación del conjunto de entrenamiento inferior a este umbral serán eliminadas.El valor predeterminado es mantener todas las características con una varianza distinta de cero,es decir,eliminar las características que tienen el mismo valor en todas las muestras.</target>
        </trans-unit>
        <trans-unit id="c6023b69815535fce0c52cfebfa0b836b623efcf" translate="yes" xml:space="preserve">
          <source>Ferri, C&amp;egrave;sar &amp;amp; Hernandez-Orallo, Jose &amp;amp; Modroiu, R. (2009). &lt;a href=&quot;https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf&quot;&gt;An Experimental Comparison of Performance Measures for Classification.&lt;/a&gt; Pattern Recognition Letters. 30. 27-38.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7609c48291c2fa7cc57f1dc4a85a3683c0749b4f" translate="yes" xml:space="preserve">
          <source>Fetch an mldata.org data set</source>
          <target state="translated">Obtener un conjunto de datos de mldata.org</target>
        </trans-unit>
        <trans-unit id="2b29d907c2ea7e85ef894ade96f5a9788aa198c9" translate="yes" xml:space="preserve">
          <source>Fetch dataset from openml by name or dataset id.</source>
          <target state="translated">Obtener el conjunto de datos de openml por nombre o id del conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="76c8d1e0086c4e650fc514d650a93664c1ba4aa6" translate="yes" xml:space="preserve">
          <source>Fevotte, C., &amp;amp; Idier, J. (2011). Algorithms for nonnegative matrix factorization with the beta-divergence. Neural Computation, 23(9).</source>
          <target state="translated">Fevotte, C. y Idier, J. (2011). Algoritmos para la factorizaci&amp;oacute;n matricial no negativa con la divergencia beta. Computaci&amp;oacute;n neuronal, 23 (9).</target>
        </trans-unit>
        <trans-unit id="07da69c9120fa48d1a8831ddfdb2e22d5f38a14c" translate="yes" xml:space="preserve">
          <source>Few clusters, even cluster size, non-flat geometry</source>
          <target state="translated">Pocos cúmulos,incluso el tamaño de los cúmulos,la geometría no plana</target>
        </trans-unit>
        <trans-unit id="ba82bf0fe9bdb74f2de9056ff08a208a0201f2e5" translate="yes" xml:space="preserve">
          <source>Field &lt;code&gt;support_vectors_&lt;/code&gt; is now empty, only indices of support vectors are stored in &lt;code&gt;support_&lt;/code&gt;</source>
          <target state="translated">El campo &lt;code&gt;support_vectors_&lt;/code&gt; ahora est&amp;aacute; vac&amp;iacute;o, solo los &amp;iacute;ndices de los vectores de soporte se almacenan en &lt;code&gt;support_&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e6b7eea98ef60c86409d4e4dd54b3796ed56341b" translate="yes" xml:space="preserve">
          <source>Figure containing partial dependence plots.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf29ad109219929951ed38da3e672e781ba3fade" translate="yes" xml:space="preserve">
          <source>Figure containing the confusion matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65bafae58e0d2ce7524ef55e10fcc5ed53a13aa4" translate="yes" xml:space="preserve">
          <source>Figure containing the curve.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63c41242bc3de30c7fa64bf90cc3aa34e5d08243" translate="yes" xml:space="preserve">
          <source>Filter: Select the p-values corresponding to Family-wise error rate</source>
          <target state="translated">Filtro:Selecciona los valores p correspondientes a la tasa de error de la familia</target>
        </trans-unit>
        <trans-unit id="d601967c960c718fae8a27ae10382e904525e519" translate="yes" xml:space="preserve">
          <source>Filter: Select the p-values for an estimated false discovery rate</source>
          <target state="translated">Filtro:Selecciona los valores p para una tasa estimada de falsos descubrimientos</target>
        </trans-unit>
        <trans-unit id="f9d7c51c4e21ffa778934e88c4ada6f78e753e59" translate="yes" xml:space="preserve">
          <source>Filter: Select the pvalues below alpha based on a FPR test.</source>
          <target state="translated">Filtro:Selecciona los valores p por debajo del alfa en base a una prueba de FPR.</target>
        </trans-unit>
        <trans-unit id="fa74263b1f44e0e368feef4eccc429e274602a1d" translate="yes" xml:space="preserve">
          <source>Final perplexity score on training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e832a73dedeb0259ac793f9ac425f8e8ee172602" translate="yes" xml:space="preserve">
          <source>Finally it is possible to discover the main topics of a corpus by relaxing the hard assignment constraint of clustering, for instance by using &lt;a href=&quot;decomposition#nmf&quot;&gt;Non-negative matrix factorization (NMF or NNMF)&lt;/a&gt;:</source>
          <target state="translated">Finalmente, es posible descubrir los temas principales de un corpus relajando la restricci&amp;oacute;n de asignaci&amp;oacute;n estricta de la agrupaci&amp;oacute;n, por ejemplo, utilizando la &lt;a href=&quot;decomposition#nmf&quot;&gt;factorizaci&amp;oacute;n de matriz no negativa (NMF o NNMF)&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="3879f3b348e8db024fb12750b7a6d38d6bbf41c3" translate="yes" xml:space="preserve">
          <source>Finally one can also observe that for some intermediate values of &lt;code&gt;gamma&lt;/code&gt; we get equally performing models when &lt;code&gt;C&lt;/code&gt; becomes very large: it is not necessary to regularize by enforcing a larger margin. The radius of the RBF kernel alone acts as a good structural regularizer. In practice though it might still be interesting to simplify the decision function with a lower value of &lt;code&gt;C&lt;/code&gt; so as to favor models that use less memory and that are faster to predict.</source>
          <target state="translated">Finalmente, tambi&amp;eacute;n se puede observar que para algunos valores intermedios de &lt;code&gt;gamma&lt;/code&gt; obtenemos modelos de igual rendimiento cuando &lt;code&gt;C&lt;/code&gt; se vuelve muy grande: no es necesario regularizar aplicando un margen mayor. El radio del n&amp;uacute;cleo RBF por s&amp;iacute; solo act&amp;uacute;a como un buen regularizador estructural. En la pr&amp;aacute;ctica, sin embargo, podr&amp;iacute;a ser interesante simplificar la funci&amp;oacute;n de decisi&amp;oacute;n con un valor m&amp;aacute;s bajo de &lt;code&gt;C&lt;/code&gt; para favorecer los modelos que usan menos memoria y que son m&amp;aacute;s r&amp;aacute;pidos de predecir.</target>
        </trans-unit>
        <trans-unit id="023b5a681b65ffd6304d113bea1d054693e2974b" translate="yes" xml:space="preserve">
          <source>Finally one should highlight that the Compound Poisson Gamma model that is directly fit on the pure premium is operationally simpler to develop and maintain as it consists in a single scikit-learn estimator instead of a pair of models, each with its own set of hyperparameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="132cf9317ef371444593d6b5ea568f7dd88cd93f" translate="yes" xml:space="preserve">
          <source>Finally we are going to visualize the score:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c7dc3c27466a01b4ea9d811b8dcf76d3f27d658" translate="yes" xml:space="preserve">
          <source>Finally we will plot the selected two features from the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="968616cc558da3fce60b0fa70563b382cf880868" translate="yes" xml:space="preserve">
          <source>Finally, &lt;a href=&quot;#dummy-estimators&quot;&gt;Dummy estimators&lt;/a&gt; are useful to get a baseline value of those metrics for random predictions.</source>
          <target state="translated">Por &amp;uacute;ltimo, los &lt;a href=&quot;#dummy-estimators&quot;&gt;estimadores ficticios&lt;/a&gt; son &amp;uacute;tiles para obtener un valor de referencia de esas m&amp;eacute;tricas para predicciones aleatorias.</target>
        </trans-unit>
        <trans-unit id="f6d5bddccec3511395edb0437e03e1a9559b2146" translate="yes" xml:space="preserve">
          <source>Finally, as we will see next, computing partial dependence plots tree-based models is also orders of magnitude faster making it cheap to compute partial dependence plots for pairs of interacting features:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bac8fe99c0a1d10b0495b7da851a20ddd76d68ac" translate="yes" xml:space="preserve">
          <source>Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the &lt;code&gt;partial_fit&lt;/code&gt; API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called &amp;ldquo;online learning&amp;rdquo;) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f153367033a26a7113472eddd66aa5516b2f784" translate="yes" xml:space="preserve">
          <source>Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the &lt;code&gt;partial_fit&lt;/code&gt; API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called &amp;ldquo;online learning&amp;rdquo;) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">Finalmente, para 3. tenemos varias opciones dentro de scikit-learn. Aunque no todos los algoritmos pueden aprender de forma incremental (es decir, sin ver todas las instancias a la vez), todos los estimadores que implementan la API de &lt;code&gt;partial_fit&lt;/code&gt; parcial son candidatos. En realidad, la capacidad de aprender de forma incremental a partir de un minigolote de instancias (a veces llamado &quot;aprendizaje en l&amp;iacute;nea&quot;) es clave para el aprendizaje fuera del n&amp;uacute;cleo, ya que garantiza que en un momento dado habr&amp;aacute; solo una peque&amp;ntilde;a cantidad de instancias en el memoria principal. Elegir un buen tama&amp;ntilde;o para el mini-lote que equilibre la relevancia y la huella de memoria podr&amp;iacute;a implicar algunos ajustes &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2ea772d9a2706e63fd635b713dfc5a265ad9f7ca" translate="yes" xml:space="preserve">
          <source>Finally, for the last data set, it is hard to say that one sample is more abnormal than another sample as they are uniformly distributed in a hypercube. Except for the &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; which overfits a little, all estimators present decent solutions for this situation. In such a case, it would be wise to look more closely at the scores of abnormality of the samples as a good estimator should assign similar scores to all the samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02251ecbb666e2a1b9f431635bb03c5b54eb0a09" translate="yes" xml:space="preserve">
          <source>Finally, for the last data set, it is hard to say that one sample is more abnormal than another sample as they are uniformly distributed in a hypercube. Except for the &lt;code&gt;svm.OneClassSVM&lt;/code&gt; which overfits a little, all estimators present decent solutions for this situation. In such a case, it would be wise to look more closely at the scores of abnormality of the samples as a good estimator should assign similar scores to all the samples.</source>
          <target state="translated">Finalmente, para el &amp;uacute;ltimo conjunto de datos, es dif&amp;iacute;cil decir que una muestra es m&amp;aacute;s anormal que otra muestra, ya que est&amp;aacute;n distribuidas uniformemente en un hipercubo. Excepto por el &lt;code&gt;svm.OneClassSVM&lt;/code&gt; que se sobreajusta un poco, todos los estimadores presentan soluciones decentes para esta situaci&amp;oacute;n. En tal caso, ser&amp;iacute;a prudente observar m&amp;aacute;s de cerca las puntuaciones de anomal&amp;iacute;a de las muestras, ya que un buen estimador deber&amp;iacute;a asignar puntuaciones similares a todas las muestras.</target>
        </trans-unit>
        <trans-unit id="ced44a1e1c24bc2a905bacd41c580fdb8b398c7b" translate="yes" xml:space="preserve">
          <source>Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the &lt;code&gt;toarray&lt;/code&gt; method of sparse matrices is another option.</source>
          <target state="translated">Por &amp;uacute;ltimo, si se espera que los datos centrados sean lo suficientemente peque&amp;ntilde;os, otra opci&amp;oacute;n es convertir expl&amp;iacute;citamente la entrada en una matriz utilizando el m&amp;eacute;todo &lt;code&gt;toarray&lt;/code&gt; de matrices dispersas.</target>
        </trans-unit>
        <trans-unit id="3e454243da6a98ce545408a4ef381fa38d1b1d45" translate="yes" xml:space="preserve">
          <source>Finally, many parts of the implementation of &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; are parallelized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b957bcce911d726f82907b30c1a02070fcadf91" translate="yes" xml:space="preserve">
          <source>Finally, note that parameters of the models have been here handpicked but that in practice they need to be adjusted. In the absence of labelled data, the problem is completely unsupervised so model selection can be a challenge.</source>
          <target state="translated">Por último,observe que los parámetros de los modelos han sido aquí seleccionados a mano pero que en la práctica deben ser ajustados.En ausencia de datos etiquetados,el problema está completamente sin supervisión,por lo que la selección de los modelos puede ser un desafío.</target>
        </trans-unit>
        <trans-unit id="657149d8f47b73d795165e7f97ca51bcb04565c9" translate="yes" xml:space="preserve">
          <source>Finally, the precomputation can be performed by custom estimators to use different implementations, such as approximate nearest neighbors methods, or implementation with special data types. The precomputed neighbors &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;sparse graph&lt;/a&gt; needs to be formatted as in &lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt;&lt;code&gt;radius_neighbors_graph&lt;/code&gt;&lt;/a&gt; output:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d0d4410d99fc712aa6cde836179153d36f8849c" translate="yes" xml:space="preserve">
          <source>Finally, the preprocessing pipeline is integrated in a full prediction pipeline using &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;, together with a simple classification model.</source>
          <target state="translated">Finalmente, la canalizaci&amp;oacute;n de preprocesamiento se integra en una canalizaci&amp;oacute;n de predicci&amp;oacute;n completa utilizando &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; , junto con un modelo de clasificaci&amp;oacute;n simple.</target>
        </trans-unit>
        <trans-unit id="88dd3e3888504f660d94f91abf7147882c40ab2a" translate="yes" xml:space="preserve">
          <source>Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions through the &lt;code&gt;n_jobs&lt;/code&gt; parameter. If &lt;code&gt;n_jobs=k&lt;/code&gt; then computations are partitioned into &lt;code&gt;k&lt;/code&gt; jobs, and run on &lt;code&gt;k&lt;/code&gt; cores of the machine. If &lt;code&gt;n_jobs=-1&lt;/code&gt; then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using &lt;code&gt;k&lt;/code&gt; jobs will unfortunately not be &lt;code&gt;k&lt;/code&gt; times as fast). Significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets).</source>
          <target state="translated">Finalmente, este m&amp;oacute;dulo tambi&amp;eacute;n presenta la construcci&amp;oacute;n paralela de los &amp;aacute;rboles y el c&amp;aacute;lculo paralelo de las predicciones a trav&amp;eacute;s del par&amp;aacute;metro &lt;code&gt;n_jobs&lt;/code&gt; . Si &lt;code&gt;n_jobs=k&lt;/code&gt; , los c&amp;aacute;lculos se dividen en &lt;code&gt;k&lt;/code&gt; trabajos y se ejecutan en &lt;code&gt;k&lt;/code&gt; n&amp;uacute;cleos de la m&amp;aacute;quina. Si &lt;code&gt;n_jobs=-1&lt;/code&gt; , se utilizan todos los n&amp;uacute;cleos disponibles en la m&amp;aacute;quina. Tenga en cuenta que debido a la sobrecarga de comunicaci&amp;oacute;n entre procesos, es posible que la aceleraci&amp;oacute;n no sea lineal (es decir, el uso de &lt;code&gt;k&lt;/code&gt; trabajos desafortunadamente no ser&amp;aacute; &lt;code&gt;k&lt;/code&gt; veces m&amp;aacute;s r&amp;aacute;pido). Sin embargo, a&amp;uacute;n se puede lograr una aceleraci&amp;oacute;n significativa cuando se construye una gran cantidad de &amp;aacute;rboles o cuando la construcci&amp;oacute;n de un solo &amp;aacute;rbol requiere una buena cantidad de tiempo (por ejemplo, en grandes conjuntos de datos).</target>
        </trans-unit>
        <trans-unit id="ac4279287fdb399e92fa8f401fc1a571b7df3622" translate="yes" xml:space="preserve">
          <source>Finally, we can compare the two models using a plot of cumulated claims: for each model, the policyholders are ranked from safest to riskiest and the fraction of observed total cumulated claims is plotted on the y axis. This plot is often called the ordered Lorenz curve of the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="abb5440e13032f9c21233aa604593a79c839b358" translate="yes" xml:space="preserve">
          <source>Finally, we fit our pipeline on the training data and use it to predict topics for &lt;code&gt;X_test&lt;/code&gt;. Performance metrics of our pipeline are then printed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f50b898f991b3ec8dadce88abad6749cb3fd8140" translate="yes" xml:space="preserve">
          <source>Finally, we have a full-fledged example of &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;. It is aimed at providing a starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed above.</source>
          <target state="translated">Por &amp;uacute;ltimo, tenemos un ejemplo completo de &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;clasificaci&amp;oacute;n de documentos de texto fuera del n&amp;uacute;cleo&lt;/a&gt; . Su objetivo es proporcionar un punto de partida para las personas que desean construir sistemas de aprendizaje fuera del n&amp;uacute;cleo y demuestra la mayor&amp;iacute;a de las nociones discutidas anteriormente.</target>
        </trans-unit>
        <trans-unit id="67b1b42b3e66107ae3b505ef41aac41ee3327ff3" translate="yes" xml:space="preserve">
          <source>Finally, we will consider a non-linear model, namely Gradient Boosting Regression Trees. Tree-based models do not require the categorical data to be one-hot encoded: instead, we can encode each category label with an arbitrary integer using &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;OrdinalEncoder&lt;/code&gt;&lt;/a&gt;. With this encoding, the trees will treat the categorical features as ordered features, which might not be always a desired behavior. However this effect is limited for deep enough trees which are able to recover the categorical nature of the features. The main advantage of the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;OrdinalEncoder&lt;/code&gt;&lt;/a&gt; over the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;OneHotEncoder&lt;/code&gt;&lt;/a&gt; is that it will make training faster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15e943be721eab8a2c2612f45c392677826a2d6c" translate="yes" xml:space="preserve">
          <source>Finally, we will plot the predictions made by all models for comparison.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9d6cf373d4d749e0936e6f64e1c533e8fd8ee41" translate="yes" xml:space="preserve">
          <source>Finally, we will visualize the 20 predictions. The red stars show the average prediction made by &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f642cdc0abe545be7ddd1dd012ea6198c659512" translate="yes" xml:space="preserve">
          <source>Finally, we will visualize the results. To do that we will first compute the test set deviance and then plot it against boosting iterations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a40631d3e46670650e2553ca58294d12cd46b8ee" translate="yes" xml:space="preserve">
          <source>Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#lg2012&quot; id=&quot;id4&quot;&gt;[LG2012]&lt;/a&gt;.</source>
          <target state="translated">Finalmente, cuando los estimadores base se construyen sobre subconjuntos de muestras y caracter&amp;iacute;sticas, entonces el m&amp;eacute;todo se conoce como Parches aleatorios &lt;a href=&quot;#lg2012&quot; id=&quot;id4&quot;&gt;[LG2012]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7185a548932a7d63b1a08d591559f9d8821b9404" translate="yes" xml:space="preserve">
          <source>Find a &amp;lsquo;safe&amp;rsquo; number of components to randomly project to</source>
          <target state="translated">Encuentre un n&amp;uacute;mero 'seguro' de componentes para proyectar al azar</target>
        </trans-unit>
        <trans-unit id="e40e118fb652d4dd06f307746b620728b2345e85" translate="yes" xml:space="preserve">
          <source>Find a good set of parameters using grid search.</source>
          <target state="translated">Encuentra un buen conjunto de parámetros usando la búsqueda en cuadrícula.</target>
        </trans-unit>
        <trans-unit id="022a840c39dad4bac6c36ba2a156531a2e97ca25" translate="yes" xml:space="preserve">
          <source>Find importance of the features</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ff57da6a2f73ea1d0dfc969be6516b83c61b137" translate="yes" xml:space="preserve">
          <source>Find out what the actual encoding of the text is. The file might come with a header or README that tells you the encoding, or there might be some standard encoding you can assume based on where the text comes from.</source>
          <target state="translated">Averigua cuál es la codificación real del texto.El archivo puede venir con un encabezado o un LÉAME que te diga la codificación,o puede haber alguna codificación estándar que puedas asumir basada en la procedencia del texto.</target>
        </trans-unit>
        <trans-unit id="c317277c718283dc0dcc7baf2d812226e24cc453" translate="yes" xml:space="preserve">
          <source>Find the minimum value of an array over positive values</source>
          <target state="translated">Encontrar el valor mínimo de una matriz sobre los valores positivos</target>
        </trans-unit>
        <trans-unit id="d95b4144f33d10b131b37a33f2e7ad7cc55860b7" translate="yes" xml:space="preserve">
          <source>Find the optimal separating hyperplane using an SVC for classes that are unbalanced.</source>
          <target state="translated">Encuentra el hiperplano de separación óptimo usando un SVC para las clases que están desequilibradas.</target>
        </trans-unit>
        <trans-unit id="1c17584736b51c43c88ae3dfa46ea2817cd1a339" translate="yes" xml:space="preserve">
          <source>Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.</source>
          <target state="translated">Encuentra dos matrices no negativas (W,H)cuyo producto se aproxima a la matriz no negativa X.Esta factorización puede utilizarse,por ejemplo,para la reducción de la dimensionalidad,la separación de fuentes o la extracción de temas.</target>
        </trans-unit>
        <trans-unit id="4daca12ed4dca9ca21325e05dde77793902b89f7" translate="yes" xml:space="preserve">
          <source>Finding a reasonable regularization parameter \(\alpha\) is best done using &lt;code&gt;GridSearchCV&lt;/code&gt;, usually in the range &lt;code&gt;10.0 ** -np.arange(1, 7)&lt;/code&gt;.</source>
          <target state="translated">Encontrar un par&amp;aacute;metro de regularizaci&amp;oacute;n razonable \ (\ alpha \) se realiza mejor utilizando &lt;code&gt;GridSearchCV&lt;/code&gt; , generalmente en el rango &lt;code&gt;10.0 ** -np.arange(1, 7)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d0410c73baea9b092d4edb9802ae4c10c09f4696" translate="yes" xml:space="preserve">
          <source>Finding a reasonable regularization term \(\alpha\) is best done using &lt;code&gt;GridSearchCV&lt;/code&gt;, usually in the range &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt;.</source>
          <target state="translated">Encontrar un t&amp;eacute;rmino de regularizaci&amp;oacute;n razonable \ (\ alpha \) se hace mejor usando &lt;code&gt;GridSearchCV&lt;/code&gt; , generalmente en el rango &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="37e657d78a9dafeda1b142a17aaad5659fab59b7" translate="yes" xml:space="preserve">
          <source>Finding a reasonable regularization term \(\alpha\) is best done using automatic hyper-parameter search, e.g. &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt;, usually in the range &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c5a640c918941c9e6a6ecbfe984c3f4bfbed0d7" translate="yes" xml:space="preserve">
          <source>Finding help</source>
          <target state="translated">Encontrar ayuda</target>
        </trans-unit>
        <trans-unit id="71cc727880d53ab3c238ec955595a0ded28610b7" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</source>
          <target state="translated">Encontrar la estructura con aleatoriedad:Algoritmos estocásticos para construir descomposiciones matriciales aproximadas Halko,et al.,2009 (arXiv:909)http://arxiv.org/pdf/0909.4061</target>
        </trans-unit>
        <trans-unit id="de5cdfe8ddd63c8c4fb1f7f388d421a16cbbf828" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50ccb6d4c8a3bfa0951293912c0c5132106a8c89" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 &lt;a href=&quot;http://arxiv.org/abs/arXiv:0909.4061&quot;&gt;http://arxiv.org/abs/arXiv:0909.4061&lt;/a&gt;</source>
          <target state="translated">Encontrar estructura con aleatoriedad: algoritmos estoc&amp;aacute;sticos para construir descomposiciones matriciales aproximadas Halko, et al., 2009 &lt;a href=&quot;http://arxiv.org/abs/arXiv:0909.4061&quot;&gt;http://arxiv.org/abs/arXiv:0909.4061&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="09110546ddb637f3471f1e7efbbe318b3963484e" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;https://arxiv.org/abs/0909.4061&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26becafaa69af10fe9097be8fd6bf298839f5095" translate="yes" xml:space="preserve">
          <source>Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.</source>
          <target state="translated">Encuentra un diccionario (un conjunto de átomos)que se puede utilizar mejor para representar los datos utilizando un código escaso.</target>
        </trans-unit>
        <trans-unit id="bdd5a22a4b66cac66c670f6f1297e40cb6d2aae2" translate="yes" xml:space="preserve">
          <source>Finds a sparse representation of data against a fixed, precomputed dictionary.</source>
          <target state="translated">Encuentra una escasa representación de datos contra un diccionario fijo precalculado.</target>
        </trans-unit>
        <trans-unit id="a5dbd88babc750758342dfe447e24f6c260c3097" translate="yes" xml:space="preserve">
          <source>Finds core samples of high density and expands clusters from them.</source>
          <target state="translated">Encuentra muestras de núcleo de alta densidad y expande los cúmulos a partir de ellas.</target>
        </trans-unit>
        <trans-unit id="e0fb0de42fddb601e63b5dedf2b446e9b6fdd66e" translate="yes" xml:space="preserve">
          <source>Finds core samples of high density and expands clusters from them. This example uses data that is generated so that the clusters have different densities. The &lt;a href=&quot;../../modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS&quot;&gt;&lt;code&gt;sklearn.cluster.OPTICS&lt;/code&gt;&lt;/a&gt; is first used with its Xi cluster detection method, and then setting specific thresholds on the reachability, which corresponds to &lt;a href=&quot;../../modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;sklearn.cluster.DBSCAN&lt;/code&gt;&lt;/a&gt;. We can see that the different clusters of OPTICS&amp;rsquo;s Xi method can be recovered with different choices of thresholds in DBSCAN.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0ac333cc4bd6a4e85a5691bb991f9cd4f114c12" translate="yes" xml:space="preserve">
          <source>Finds the K-neighbors of a point.</source>
          <target state="translated">Encuentra los K-vecinos de un punto.</target>
        </trans-unit>
        <trans-unit id="e7def6740e556221f1aa42003fab4e2365f3483e" translate="yes" xml:space="preserve">
          <source>Finds the K-neighbors of a point. Returns indices of and distances to the neighbors of each point.</source>
          <target state="translated">Encuentra los K-vecinos de un punto.Devuelve índices y distancias a los vecinos de cada punto.</target>
        </trans-unit>
        <trans-unit id="011362db8e96e3edcb95f8fcdf5f1e0d92d5e3e2" translate="yes" xml:space="preserve">
          <source>Finds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving:</source>
          <target state="translated">Encuentra el mejor diccionario y el correspondiente código escaso para aproximarse a la matriz de datos X mediante la resolución:</target>
        </trans-unit>
        <trans-unit id="b6f70d43205a979d509f018cfbe88e93d812b3ec" translate="yes" xml:space="preserve">
          <source>Finds the neighbors within a given radius of a point or points.</source>
          <target state="translated">Encuentra a los vecinos dentro de un radio dado de un punto o puntos.</target>
        </trans-unit>
        <trans-unit id="dd8ddfd214a6d88017dd4006d52b1774d72e0be7" translate="yes" xml:space="preserve">
          <source>Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.</source>
          <target state="translated">Encuentra el conjunto de componentes escasos que pueden reconstruir los datos de forma óptima.La cantidad de dispersión es controlable por el coeficiente de la penalización L1,dado por el parámetro alfa.</target>
        </trans-unit>
        <trans-unit id="9ce397e0f2c9f33bd1911b665e99384e26321100" translate="yes" xml:space="preserve">
          <source>Finite Gaussian mixture fit with EM.</source>
          <target state="translated">La mezcla gaussiana finita encaja con el EM.</target>
        </trans-unit>
        <trans-unit id="fd9ae1ff1bfcac14b05b92d277d3f6074f078a31" translate="yes" xml:space="preserve">
          <source>First 10 columns are numeric predictive values</source>
          <target state="translated">Las primeras 10 columnas son valores numéricos de predicción</target>
        </trans-unit>
        <trans-unit id="30250bc7520fcdac140dbcf0c3820bc484ca88d7" translate="yes" xml:space="preserve">
          <source>First example</source>
          <target state="translated">Primer ejemplo</target>
        </trans-unit>
        <trans-unit id="7874170ffefbed5d0d1c4372827ea2aeb194c3ba" translate="yes" xml:space="preserve">
          <source>First fit an ensemble of trees (totally random trees, a random forest, or gradient boosted trees) on the training set. Then each leaf of each tree in the ensemble is assigned a fixed arbitrary feature index in a new feature space. These leaf indices are then encoded in a one-hot fashion.</source>
          <target state="translated">Primero,coloque un conjunto de árboles (árboles totalmente al azar,un bosque al azar,o árboles impulsados por gradiente)en el set de entrenamiento.Luego se asigna a cada hoja de cada árbol del conjunto un índice de característica arbitraria fija en un nuevo espacio de característica.Estos índices de hojas se codifican entonces de una manera muy precisa.</target>
        </trans-unit>
        <trans-unit id="c9c47ba90ccfdfceca4eb5b8199939d35b815802" translate="yes" xml:space="preserve">
          <source>First note that the K means \(\mu_k\) are vectors in \(\mathcal{R}^d\), and they lie in an affine subspace \(H\) of dimension at least \(K - 1\) (2 points lie on a line, 3 points lie on a plane, etc).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b7d36dc535373f63b2b8c97af1496035ebdccf8" translate="yes" xml:space="preserve">
          <source>First of all, we can take a look to the values of the coefficients of the regressor we have fitted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af1667b67f8a74831d9cbccc10fec01c4bb8c75d" translate="yes" xml:space="preserve">
          <source>First we check which value of \(\alpha\) has been selected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31c524afbf18465a1ae6f80767e0e696d536b875" translate="yes" xml:space="preserve">
          <source>First we create a data set of 9 samples from 3 classes, and plot the points in the original space. For this example, we focus on the classification of point no. 3. The thickness of a link between point no. 3 and another point is proportional to their distance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66d3e97d3998fff9804546e983728130f9a1d48d" translate="yes" xml:space="preserve">
          <source>First we download the two datasets. Diabetes dataset is shipped with scikit-learn. It has 442 entries, each with 10 features. California Housing dataset is much larger with 20640 entries and 8 features. It needs to be downloaded. We will only use the first 400 entries for the sake of speeding up the calculations but feel free to use the whole dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2e4b6f9c83c90558d9df87e3572e1d8ff690d21" translate="yes" xml:space="preserve">
          <source>First we need to load the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d180e35a78f80e45c6959bdc2213230723a6ec9" translate="yes" xml:space="preserve">
          <source>First we verify which value of \(\alpha\) has been selected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da6f0be97a3f9bc23cb9968f6dd764558635185b" translate="yes" xml:space="preserve">
          <source>First, let&amp;rsquo;s get some insights by looking at the variable distributions and at the pairwise relationships between them. Only numerical variables will be used. In the following plot, each dot represents a sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c794cffa5e7e2eb61589bb3349c992e76e5c222" translate="yes" xml:space="preserve">
          <source>First, let&amp;rsquo;s load the diabetes dataset which is available from within sklearn. Then, we will look what features are collected for the diabates patients:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cac3b241d42030e20b4b1109edf6cf6d51db7345" translate="yes" xml:space="preserve">
          <source>First, the precomputed graph can be re-used multiple times, for instance while varying a parameter of the estimator. This can be done manually by the user, or using the caching properties of the scikit-learn pipeline:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97be8aa0250f0cdac7301eb79bf3b2a478185ccc" translate="yes" xml:space="preserve">
          <source>First, three examplary classifiers are initialized (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;GaussianNB&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt;) and used to initialize a soft-voting &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; with weights &lt;code&gt;[1, 1, 5]&lt;/code&gt;, which means that the predicted probabilities of the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; count 5 times as much as the weights of the other classifiers when the averaged probability is calculated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16a8cd51addf2278ba02a88d4c3026fabfe281d9" translate="yes" xml:space="preserve">
          <source>First, three examplary classifiers are initialized (&lt;code&gt;LogisticRegression&lt;/code&gt;, &lt;code&gt;GaussianNB&lt;/code&gt;, and &lt;code&gt;RandomForestClassifier&lt;/code&gt;) and used to initialize a soft-voting &lt;code&gt;VotingClassifier&lt;/code&gt; with weights &lt;code&gt;[1, 1, 5]&lt;/code&gt;, which means that the predicted probabilities of the &lt;code&gt;RandomForestClassifier&lt;/code&gt; count 5 times as much as the weights of the other classifiers when the averaged probability is calculated.</source>
          <target state="translated">Primero, se inicializan tres clasificadores de ejemplo ( &lt;code&gt;LogisticRegression&lt;/code&gt; , &lt;code&gt;GaussianNB&lt;/code&gt; y &lt;code&gt;RandomForestClassifier&lt;/code&gt; ) y se utilizan para inicializar un &lt;code&gt;VotingClassifier&lt;/code&gt; de votaci&amp;oacute;n suave con pesos &lt;code&gt;[1, 1, 5]&lt;/code&gt; , lo que significa que las probabilidades predichas del &lt;code&gt;RandomForestClassifier&lt;/code&gt; cuentan 5 veces m&amp;aacute;s que el ponderaciones de los otros clasificadores cuando se calcula la probabilidad media.</target>
        </trans-unit>
        <trans-unit id="f50c60bee958db3de20cd6c08f2c3fe9b2fb6e88" translate="yes" xml:space="preserve">
          <source>First, three exemplary classifiers are initialized (&lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;) and used to initialize a soft-voting &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; with weights &lt;code&gt;[2,
1, 2]&lt;/code&gt;, which means that the predicted probabilities of the &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; each count 2 times as much as the weights of the &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; classifier when the averaged probability is calculated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ea180eeb1f820255090eaa2e746ff697919b622" translate="yes" xml:space="preserve">
          <source>First, three exemplary classifiers are initialized (&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;, &lt;code&gt;KNeighborsClassifier&lt;/code&gt;, and &lt;code&gt;SVC&lt;/code&gt;) and used to initialize a soft-voting &lt;code&gt;VotingClassifier&lt;/code&gt; with weights &lt;code&gt;[2, 1, 2]&lt;/code&gt;, which means that the predicted probabilities of the &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; and &lt;code&gt;SVC&lt;/code&gt; count 5 times as much as the weights of the &lt;code&gt;KNeighborsClassifier&lt;/code&gt; classifier when the averaged probability is calculated.</source>
          <target state="translated">Primero, se inicializan tres clasificadores ejemplares ( &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; , &lt;code&gt;KNeighborsClassifier&lt;/code&gt; y &lt;code&gt;SVC&lt;/code&gt; ) y se utilizan para inicializar un &lt;code&gt;VotingClassifier&lt;/code&gt; de votaci&amp;oacute;n suave con pesos &lt;code&gt;[2, 1, 2]&lt;/code&gt; , lo que significa que las probabilidades predichas de &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; y &lt;code&gt;SVC&lt;/code&gt; cuentan 5 veces m&amp;aacute;s como los pesos del clasificador &lt;code&gt;KNeighborsClassifier&lt;/code&gt; cuando se calcula la probabilidad promediada.</target>
        </trans-unit>
        <trans-unit id="5fdc5502e4f55e2ed52afffb452568aea16c1218" translate="yes" xml:space="preserve">
          <source>First, we fit the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef9c4b59f96926f6f0e5f166376f8408083d95db" translate="yes" xml:space="preserve">
          <source>First, we load the wine dataset and convert it to a binary classification problem. Then, we train a support vector classifier on a training dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d07cc681f72e7314cfb570248652c85d831875b" translate="yes" xml:space="preserve">
          <source>First, we must understand the structure of our data. It has 100 randomly generated input datapoints, 3 classes split unevenly across datapoints, and 10 &amp;ldquo;groups&amp;rdquo; split evenly across datapoints.</source>
          <target state="translated">Primero, debemos comprender la estructura de nuestros datos. Tiene 100 puntos de datos de entrada generados aleatoriamente, 3 clases divididas de manera desigual entre los puntos de datos y 10 &quot;grupos&quot; divididos uniformemente entre los puntos de datos.</target>
        </trans-unit>
        <trans-unit id="cc0ba427a614a821f465ac72c31717d2cbd7abed" translate="yes" xml:space="preserve">
          <source>First, we train a decision tree and a multi-layer perceptron on the diabetes dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f123dfa67a1788d2150ed770bdb351ff3e4fb8cc" translate="yes" xml:space="preserve">
          <source>First, we train a random forest on the breast cancer dataset and evaluate its accuracy on a test set:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b8f70566f670b04c1ac522dc7b0f129462badb9" translate="yes" xml:space="preserve">
          <source>First, we want to estimate the score on the original data:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fab091b4d5d2bc522f0ed86af86bd56d5c7a2d19" translate="yes" xml:space="preserve">
          <source>First, we will load the diabetes dataset and initiate a gradient boosting regressor, a random forest regressor and a linear regression. Next, we will use the 3 regressors to build the voting regressor:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5918b4ffaa61bc5b2d97e424d8740b3562535f07" translate="yes" xml:space="preserve">
          <source>First, we would like a transformer that extracts the subject and body of each post. Since this is a stateless transformation (does not require state information from training data), we can define a function that performs the data transformation then use &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt;&lt;code&gt;FunctionTransformer&lt;/code&gt;&lt;/a&gt; to create a scikit-learn transformer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b01058ecfb06b2978590da1239a0a81618b1465b" translate="yes" xml:space="preserve">
          <source>Fisher transformation. Wikipedia. &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_transformation&quot;&gt;https://en.wikipedia.org/wiki/Fisher_transformation&lt;/a&gt;</source>
          <target state="translated">Transformaci&amp;oacute;n de Fisher. Wikipedia. &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_transformation&quot;&gt;https://en.wikipedia.org/wiki/Fisher_transformation&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="75ec02df98bcfaf21ee3dd0abe872b47c664ba1c" translate="yes" xml:space="preserve">
          <source>Fisher, R.A. &amp;ldquo;The use of multiple measurements in taxonomic problems&amp;rdquo; Annual Eugenics, 7, Part II, 179-188 (1936); also in &amp;ldquo;Contributions to Mathematical Statistics&amp;rdquo; (John Wiley, NY, 1950).</source>
          <target state="translated">Fisher, RA &amp;ldquo;El uso de mediciones m&amp;uacute;ltiples en problemas taxon&amp;oacute;micos&amp;rdquo; Eugenesia anual, 7, Parte II, 179-188 (1936); tambi&amp;eacute;n en &quot;Contribuciones a la estad&amp;iacute;stica matem&amp;aacute;tica&quot; (John Wiley, NY, 1950).</target>
        </trans-unit>
        <trans-unit id="4d49737491d2fdff73fcfce78bfa3496db9aead4" translate="yes" xml:space="preserve">
          <source>Fit Gaussian Naive Bayes according to X, y</source>
          <target state="translated">Encaja con el ingenuo gaussiano Bayes según X,y</target>
        </trans-unit>
        <trans-unit id="5697d81dd45cef7c7ddb8bf2ab425085bcc0125d" translate="yes" xml:space="preserve">
          <source>Fit Gaussian process classification model</source>
          <target state="translated">Ajustar el modelo de clasificación del proceso Gaussiano</target>
        </trans-unit>
        <trans-unit id="87cac59f89b2fa13ce5de11220e3eeec4bd8c5cf" translate="yes" xml:space="preserve">
          <source>Fit Gaussian process regression model.</source>
          <target state="translated">Ajustar el modelo de regresión del proceso Gaussiano.</target>
        </trans-unit>
        <trans-unit id="8eb1d823143ba03b3eb9b823b5dab8b0cf44511a" translate="yes" xml:space="preserve">
          <source>Fit Kernel Ridge regression model</source>
          <target state="translated">Ajustar el modelo de regresión de Kernel Ridge</target>
        </trans-unit>
        <trans-unit id="62df27bb0a01d202786ab5c8f2652f91558551a4" translate="yes" xml:space="preserve">
          <source>Fit KernelCenterer</source>
          <target state="translated">Fit KernelCenter</target>
        </trans-unit>
        <trans-unit id="f97a5239f74bba940e2a0890df2d8120afcfa03c" translate="yes" xml:space="preserve">
          <source>Fit LSI model on training data X.</source>
          <target state="translated">Ajustar el modelo LSI a los datos de entrenamiento X.</target>
        </trans-unit>
        <trans-unit id="c2f5d79cd7108f6536b92a8ab72454a376b81e9e" translate="yes" xml:space="preserve">
          <source>Fit LSI model to X and perform dimensionality reduction on X.</source>
          <target state="translated">Ajustar el modelo de LSI a X y realizar la reducción de la dimensionalidad en X.</target>
        </trans-unit>
        <trans-unit id="f79f31a7bd81775029053b8e229553093e280715" translate="yes" xml:space="preserve">
          <source>Fit LinearDiscriminantAnalysis model according to the given</source>
          <target state="translated">Ajustar el modelo de Análisis Discriminatorio Lineal de acuerdo con el</target>
        </trans-unit>
        <trans-unit id="5a914df7976c81fad1a03ac324a4632a19d9e602" translate="yes" xml:space="preserve">
          <source>Fit LinearDiscriminantAnalysis model according to the given training data and parameters.</source>
          <target state="translated">Ajustar el modelo de análisis discriminante lineal de acuerdo con los datos y parámetros de entrenamiento dados.</target>
        </trans-unit>
        <trans-unit id="cb91a62a58b24a52b6e32fdd160e489a55cfbac1" translate="yes" xml:space="preserve">
          <source>Fit MultiTaskElasticNet model with coordinate descent</source>
          <target state="translated">Ajustar el modelo de MultiTaskElasticNet con descenso de coordenadas</target>
        </trans-unit>
        <trans-unit id="07e793af99a651a8f0a47822a080e4cf194bac9b" translate="yes" xml:space="preserve">
          <source>Fit Naive Bayes classifier according to X, y</source>
          <target state="translated">Ajustar el ingenuo clasificador de Bayes según X,y</target>
        </trans-unit>
        <trans-unit id="afc10844f54e485a835b4e52b36e0ebefe70c8de" translate="yes" xml:space="preserve">
          <source>Fit OneHotEncoder to X, then transform X.</source>
          <target state="translated">Ajustar el OneHotEncoder a X,y luego transformar X.</target>
        </trans-unit>
        <trans-unit id="6cbc8c577214883d5f9bdf864d2a7d76e95b5a74" translate="yes" xml:space="preserve">
          <source>Fit OneHotEncoder to X.</source>
          <target state="translated">Ajustar el OneHotEncoder a X.</target>
        </trans-unit>
        <trans-unit id="7a3e15c6e4bf063bec4aa435d4748fb868286ed7" translate="yes" xml:space="preserve">
          <source>Fit Ridge and HuberRegressor on a dataset with outliers.</source>
          <target state="translated">Poner Ridge y HuberRegressor en un conjunto de datos con valores atípicos.</target>
        </trans-unit>
        <trans-unit id="9157b13407894777185a7ac8935a66fc19422ca1" translate="yes" xml:space="preserve">
          <source>Fit Ridge classifier model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e744f084cdfc69f98e0c6bdc1ef85d9a5fdf52b4" translate="yes" xml:space="preserve">
          <source>Fit Ridge classifier with cv.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f8e3b0b4710ae51d90dcd69d91e33d123946d3a" translate="yes" xml:space="preserve">
          <source>Fit Ridge regression model</source>
          <target state="translated">Modelo de regresión de Fit Ridge</target>
        </trans-unit>
        <trans-unit id="47da48f063cc42a3f008d1e6c416d90a29ea0591" translate="yes" xml:space="preserve">
          <source>Fit Ridge regression model with cv.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e3e7ebdde04653523f01d0d804ccc0c533f3460" translate="yes" xml:space="preserve">
          <source>Fit Ridge regression model.</source>
          <target state="translated">Modelo de regresión de Fit Ridge.</target>
        </trans-unit>
        <trans-unit id="74fbf563cb041ef637760be7f01def498fc1300e" translate="yes" xml:space="preserve">
          <source>Fit X into an embedded space and return that transformed output.</source>
          <target state="translated">Encajar X en un espacio incrustado y devolver esa salida transformada.</target>
        </trans-unit>
        <trans-unit id="97ca0bc1677dbeaefd600411062ca557eacc6754" translate="yes" xml:space="preserve">
          <source>Fit X into an embedded space.</source>
          <target state="translated">Encajar X en un espacio incrustado.</target>
        </trans-unit>
        <trans-unit id="9b21c283121f9ec299302b1b3016dbcdb3391466" translate="yes" xml:space="preserve">
          <source>Fit a Bayesian ridge model and optimize the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).</source>
          <target state="translated">Ajustar un modelo de cresta Bayesiana y optimizar los parámetros de regularización lambda (precisión de los pesos)y alfa (precisión del ruido).</target>
        </trans-unit>
        <trans-unit id="dd6f625705b26516fc692deb07fa9f7046f6035b" translate="yes" xml:space="preserve">
          <source>Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dac39fc69b5b06ff2bcc2ce8d7b8984bce5d49fc" translate="yes" xml:space="preserve">
          <source>Fit a Generalized Linear Model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bec80135f416f6c22f260fc3b1ac04b4cff59ba" translate="yes" xml:space="preserve">
          <source>Fit a model to the random subset (&lt;code&gt;base_estimator.fit&lt;/code&gt;) and check whether the estimated model is valid (see &lt;code&gt;is_model_valid&lt;/code&gt;).</source>
          <target state="translated">Ajuste un modelo al subconjunto aleatorio ( &lt;code&gt;base_estimator.fit&lt;/code&gt; ) y verifique si el modelo estimado es v&amp;aacute;lido (consulte &lt;code&gt;is_model_valid&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="ebd8c04ad061ef008e394e38c512b2d8b58287fa" translate="yes" xml:space="preserve">
          <source>Fit a semi-supervised label propagation model based</source>
          <target state="translated">Ajustar un modelo de propagación de etiquetas semi-supervisado basado en</target>
        </trans-unit>
        <trans-unit id="deba03b3f9d00ab1e647b43aa626a17c324cec03" translate="yes" xml:space="preserve">
          <source>Fit all the transforms one after the other and transform the data, then fit the transformed data using the final estimator.</source>
          <target state="translated">Ajustar todas las transformaciones una tras otra y transformar los datos,luego ajustar los datos transformados usando el estimador final.</target>
        </trans-unit>
        <trans-unit id="114f8ea42731308cb5abe991bcd972d570944b6a" translate="yes" xml:space="preserve">
          <source>Fit all transformers using X.</source>
          <target state="translated">Encaja en todos los transformadores usando X.</target>
        </trans-unit>
        <trans-unit id="246e8826a6bd7a4f56f132c591c766db9ce2d6b0" translate="yes" xml:space="preserve">
          <source>Fit all transformers, transform the data and concatenate results.</source>
          <target state="translated">Ajustar todos los transformadores,transformar los datos y concatenar los resultados.</target>
        </trans-unit>
        <trans-unit id="922528f3171c2d46b470779ae89446b3062cf8a4" translate="yes" xml:space="preserve">
          <source>Fit estimator and transform dataset.</source>
          <target state="translated">Ajustar el estimador y transformar el conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="54af94e49eb61f7670851d6ef57ae7193fdff2da" translate="yes" xml:space="preserve">
          <source>Fit estimator to data.</source>
          <target state="translated">Ajustar el estimador a los datos.</target>
        </trans-unit>
        <trans-unit id="85fa5fe8b9795ca2b319f392f4a4756a8b584c81" translate="yes" xml:space="preserve">
          <source>Fit estimator using RANSAC algorithm.</source>
          <target state="translated">Estimador de ajuste usando el algoritmo RANSAC.</target>
        </trans-unit>
        <trans-unit id="820a840249b34711a41e746dbc8ea00e0bef6043" translate="yes" xml:space="preserve">
          <source>Fit estimator.</source>
          <target state="translated">Estimador de ajuste.</target>
        </trans-unit>
        <trans-unit id="c7654cec30bb319e2781e513c9f9ca93708de9b2" translate="yes" xml:space="preserve">
          <source>Fit is on grid of alphas and best alpha estimated by cross-validation.</source>
          <target state="translated">El ajuste está en una cuadrícula de alfas y el mejor alfa estimado por validación cruzada.</target>
        </trans-unit>
        <trans-unit id="b8cbf7d1420444207c14a8b65b5fcf478860f130" translate="yes" xml:space="preserve">
          <source>Fit label binarizer</source>
          <target state="translated">Binarizador de la etiqueta de ajuste</target>
        </trans-unit>
        <trans-unit id="e7ecedc7848a8ef424a3d78854bd272e8154c780" translate="yes" xml:space="preserve">
          <source>Fit label binarizer and transform multi-class labels to binary labels.</source>
          <target state="translated">Ajustar el binarizador de etiquetas y transformar las etiquetas multiclase en etiquetas binarias.</target>
        </trans-unit>
        <trans-unit id="c75cef45e92464b24b2d996a50f9aed44b9ef31e" translate="yes" xml:space="preserve">
          <source>Fit label encoder</source>
          <target state="translated">Ajustar el codificador de etiquetas</target>
        </trans-unit>
        <trans-unit id="5de88deba19907fdf6f6eb3678730f7aacb3c312" translate="yes" xml:space="preserve">
          <source>Fit label encoder and return encoded labels</source>
          <target state="translated">Ajustar el codificador de etiquetas y devolver las etiquetas codificadas</target>
        </trans-unit>
        <trans-unit id="cf615a74e4f9177f1a29f39484c75ecef454ecbe" translate="yes" xml:space="preserve">
          <source>Fit linear model with Passive Aggressive algorithm.</source>
          <target state="translated">Ajustar el modelo lineal con el algoritmo Pasivo Agresivo.</target>
        </trans-unit>
        <trans-unit id="def5054532c40485109a181ee65c06aba2df1b53" translate="yes" xml:space="preserve">
          <source>Fit linear model with Stochastic Gradient Descent.</source>
          <target state="translated">Ajustar el modelo lineal con el descenso de gradiente estocástico.</target>
        </trans-unit>
        <trans-unit id="80dff5f041e36b3407c4f4e7528431510ae562cc" translate="yes" xml:space="preserve">
          <source>Fit linear model with coordinate descent</source>
          <target state="translated">Ajustar el modelo lineal con el descenso de coordenadas</target>
        </trans-unit>
        <trans-unit id="4a42bd8bc00e1e2eb46153b9c0e14ea2a3d30f42" translate="yes" xml:space="preserve">
          <source>Fit linear model.</source>
          <target state="translated">Ajustar el modelo lineal.</target>
        </trans-unit>
        <trans-unit id="596c5db56f9a987de5f7691971fcf687d77673ec" translate="yes" xml:space="preserve">
          <source>Fit model to data.</source>
          <target state="translated">Ajustar el modelo a los datos.</target>
        </trans-unit>
        <trans-unit id="7bad9b91d21434b63dea0481c80eaf84a7cbcb25" translate="yes" xml:space="preserve">
          <source>Fit model with coordinate descent.</source>
          <target state="translated">Modelo de ajuste con descenso de coordenadas.</target>
        </trans-unit>
        <trans-unit id="8c69e1745d795a8f39d4e8e6661ab7afc8b0bcff" translate="yes" xml:space="preserve">
          <source>Fit regression model</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dddcb769b9b69c71c174f417fcd2a333b19f55fd" translate="yes" xml:space="preserve">
          <source>Fit regression model with Bayesian Ridge Regression.</source>
          <target state="translated">Modelo de regresión de ajuste con la Regresión de la Cresta Bayesiana.</target>
        </trans-unit>
        <trans-unit id="dda67be7d4db5d3187deb44446a3404222c185a6" translate="yes" xml:space="preserve">
          <source>Fit the ARDRegression model according to the given training data and parameters.</source>
          <target state="translated">Ajustar el modelo de ARDRegresión según los datos y parámetros de entrenamiento dados.</target>
        </trans-unit>
        <trans-unit id="b5a680830417593aa73dd7de7fcef69c6ed24e1b" translate="yes" xml:space="preserve">
          <source>Fit the EllipticEnvelope model.</source>
          <target state="translated">Encaja con el modelo de envoltura elíptica.</target>
        </trans-unit>
        <trans-unit id="b52ba2a596ae6c033bcc7f3cf20ecd5c58d00c2f" translate="yes" xml:space="preserve">
          <source>Fit the FactorAnalysis model to X using EM</source>
          <target state="translated">Ajustar el modelo de FactorAnalysis a X usando EM</target>
        </trans-unit>
        <trans-unit id="d35f22f8e1c64965b364c3173e1ec356759343b4" translate="yes" xml:space="preserve">
          <source>Fit the FactorAnalysis model to X using SVD based approach</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6613ce48c67ddab345a10c60b764cb90b5d43667" translate="yes" xml:space="preserve">
          <source>Fit the Kernel Density model on the data.</source>
          <target state="translated">Ajustar el modelo de Densidad del Núcleo en los datos.</target>
        </trans-unit>
        <trans-unit id="18de2bdae7f6b4aabf66891dec1f775662960310" translate="yes" xml:space="preserve">
          <source>Fit the LSH forest on the data.</source>
          <target state="translated">Ajustar el bosque LSH a los datos.</target>
        </trans-unit>
        <trans-unit id="12a1c0d798db89b07ae159bda7700f6de8a414b1" translate="yes" xml:space="preserve">
          <source>Fit the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e835684e570bb3989f14c8abec4c77362174b02" translate="yes" xml:space="preserve">
          <source>Fit the NearestCentroid model according to the given training data.</source>
          <target state="translated">Ajustar el modelo del CENTROIDE MÁS CERCANO de acuerdo con los datos de entrenamiento dados.</target>
        </trans-unit>
        <trans-unit id="faef2120c4a0d719909dbc882126884ed71710cf" translate="yes" xml:space="preserve">
          <source>Fit the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f68da32a4110559ba3f4b03666c6374bbe1b67bc" translate="yes" xml:space="preserve">
          <source>Fit the OrdinalEncoder to X.</source>
          <target state="translated">Ajustar el Codificador Ordinario a X.</target>
        </trans-unit>
        <trans-unit id="9efb3f60cfc062ba641dafe57455afe5be189469" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and automatically tune the number of selected</source>
          <target state="translated">Ajusta el modelo RFE y sintoniza automáticamente el número de</target>
        </trans-unit>
        <trans-unit id="a1f31e1eab4986ee8b13885092428f58b3a203e9" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and automatically tune the number of selected features.</source>
          <target state="translated">Ajusta el modelo RFE y sintoniza automáticamente el número de características seleccionadas.</target>
        </trans-unit>
        <trans-unit id="9813edcd03393c0187279f71f7b7e90f266a7ca4" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and then the underlying estimator on the selected</source>
          <target state="translated">Ajustar el modelo RFE y luego el estimador subyacente en el seleccionado</target>
        </trans-unit>
        <trans-unit id="2e888db086bdb6861e2b6c27ddb4959896a661ee" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and then the underlying estimator on the selected features.</source>
          <target state="translated">Ajustar el modelo RFE y luego el estimador subyacente en las características seleccionadas.</target>
        </trans-unit>
        <trans-unit id="3e8e54a78ebc1bc30d8f6371731aa34e571e1cb7" translate="yes" xml:space="preserve">
          <source>Fit the SVM model according to the given training data.</source>
          <target state="translated">Ajustar el modelo SVM de acuerdo con los datos de entrenamiento dados.</target>
        </trans-unit>
        <trans-unit id="e094bb2f16b3f77c4f0ded4998dd5acef469f424" translate="yes" xml:space="preserve">
          <source>Fit the SelectFromModel meta-transformer only once.</source>
          <target state="translated">Encaja el meta-transformador de SelectFromModel sólo una vez.</target>
        </trans-unit>
        <trans-unit id="4f2b5174d874ffda16493a72eb6b7d9e49692dfa" translate="yes" xml:space="preserve">
          <source>Fit the SelectFromModel meta-transformer.</source>
          <target state="translated">Encaja el meta-transformador de SelectFromModel.</target>
        </trans-unit>
        <trans-unit id="48df38063d696d0aeb8b0988a010338565cac757" translate="yes" xml:space="preserve">
          <source>Fit the calibrated model</source>
          <target state="translated">Encaja en el modelo calibrado</target>
        </trans-unit>
        <trans-unit id="c57e47c49821e6802e2a740c9051d8c66c744ebb" translate="yes" xml:space="preserve">
          <source>Fit the clustering from features or affinity matrix, and return cluster labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9687ab7d6ecd0d65550bf660d242c20bd8d898b" translate="yes" xml:space="preserve">
          <source>Fit the clustering from features, or affinity matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="548ac10033a88a7f2f90c29b16d9b1eac7c9baf8" translate="yes" xml:space="preserve">
          <source>Fit the data from X, and returns the embedded coordinates</source>
          <target state="translated">Ajusta los datos de X,y devuelve las coordenadas incrustadas</target>
        </trans-unit>
        <trans-unit id="08758549856b6a0e509ce1e713cae02d47e4bedc" translate="yes" xml:space="preserve">
          <source>Fit the estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="855887273459477b0845aa915278722b2bfdc95e" translate="yes" xml:space="preserve">
          <source>Fit the estimators.</source>
          <target state="translated">Ajustar los estimadores.</target>
        </trans-unit>
        <trans-unit id="78a5f0fa7f9e8e8876e6f8fdd879d2a72169691e" translate="yes" xml:space="preserve">
          <source>Fit the gradient boosting model.</source>
          <target state="translated">Encaja en el modelo de aumento de gradiente.</target>
        </trans-unit>
        <trans-unit id="59e531372ee14a17e072823da12da9c216d03637" translate="yes" xml:space="preserve">
          <source>Fit the hierarchical clustering from features or distance matrix, and return cluster labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f5e34ffa1bdd98c15becac765f254aeb2f0a093" translate="yes" xml:space="preserve">
          <source>Fit the hierarchical clustering from features, or distance matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6951b3b9c1e77870a95debb989a3776ac4c5d6a1" translate="yes" xml:space="preserve">
          <source>Fit the hierarchical clustering on the data</source>
          <target state="translated">Ajustar la agrupación jerárquica en los datos</target>
        </trans-unit>
        <trans-unit id="5b5563854bcbf3fe8e972427b6550f84e6f5e44a" translate="yes" xml:space="preserve">
          <source>Fit the imputer on X.</source>
          <target state="translated">Coloca el imputador en X.</target>
        </trans-unit>
        <trans-unit id="f79ad350ada74918a25b6a18b9c98a44219aea81" translate="yes" xml:space="preserve">
          <source>Fit the label sets binarizer and transform the given label sets</source>
          <target state="translated">Ajustar el binarizador de conjuntos de etiquetas y transformar los conjuntos de etiquetas dados</target>
        </trans-unit>
        <trans-unit id="93ac61c2b8893a8a8dc44af4d06cf2252851b7b2" translate="yes" xml:space="preserve">
          <source>Fit the label sets binarizer, storing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aff4cb7658d810f05c33be44cd22feded7d4bab1" translate="yes" xml:space="preserve">
          <source>Fit the label sets binarizer, storing &lt;code&gt;classes_&lt;/code&gt;</source>
          <target state="translated">Ajuste el binarizador de conjuntos de etiquetas, almacenando &lt;code&gt;classes_&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="8fe48671e323549fd93560341a4a3c7b625e4c7d" translate="yes" xml:space="preserve">
          <source>Fit the model</source>
          <target state="translated">Encaja con el modelo</target>
        </trans-unit>
        <trans-unit id="4a0fb954f184570eb54f89782d45ff6b62de8f93" translate="yes" xml:space="preserve">
          <source>Fit the model according to the given training data and parameters.</source>
          <target state="translated">Ajustar el modelo según los datos y parámetros de entrenamiento dados.</target>
        </trans-unit>
        <trans-unit id="226810036d48f43519ac6362c835a5ebb75ac273" translate="yes" xml:space="preserve">
          <source>Fit the model according to the given training data.</source>
          <target state="translated">Ajustar el modelo según los datos de entrenamiento dados.</target>
        </trans-unit>
        <trans-unit id="12dc3b0f35bd8c187f3d6ad72da28e55416ad4ec" translate="yes" xml:space="preserve">
          <source>Fit the model and recover the sources from X.</source>
          <target state="translated">Ajustar el modelo y recuperar las fuentes de X.</target>
        </trans-unit>
        <trans-unit id="71e9ee1734e2bbf9ac4fe07ac1b6c03c04237b08" translate="yes" xml:space="preserve">
          <source>Fit the model and transform with the final estimator</source>
          <target state="translated">Ajustar el modelo y transformarlo con el estimador final</target>
        </trans-unit>
        <trans-unit id="6b8aa0f161bb8a77dc4775ed1ee1fcfc2c406c82" translate="yes" xml:space="preserve">
          <source>Fit the model from data in X and transform X.</source>
          <target state="translated">Ajustar el modelo a partir de los datos en X y transformar X.</target>
        </trans-unit>
        <trans-unit id="0aa179622924cc08ee70a31764d01ab49bbf6bcd" translate="yes" xml:space="preserve">
          <source>Fit the model from data in X.</source>
          <target state="translated">Ajustar el modelo a partir de los datos en X.</target>
        </trans-unit>
        <trans-unit id="09a244ac4f08853db4f27fa0f56ca2d8ae157c91" translate="yes" xml:space="preserve">
          <source>Fit the model to X.</source>
          <target state="translated">Ajustar el modelo a X.</target>
        </trans-unit>
        <trans-unit id="d8fc33348e2baabb167cc4df9e594abe384c1eb2" translate="yes" xml:space="preserve">
          <source>Fit the model to data matrix X and target y.</source>
          <target state="translated">Ajustar el modelo a la matriz de datos X y al objetivo y.</target>
        </trans-unit>
        <trans-unit id="a08fe1d5397b99194d7ea288b13912038b863a22" translate="yes" xml:space="preserve">
          <source>Fit the model to data matrix X and target(s) y.</source>
          <target state="translated">Ajustar el modelo a la matriz de datos X y al objetivo o los objetivos y.</target>
        </trans-unit>
        <trans-unit id="5260da8ec9e88ebba64d6137961aeb8e84b7f391" translate="yes" xml:space="preserve">
          <source>Fit the model to data matrix X and targets Y.</source>
          <target state="translated">Ajustar el modelo a la matriz de datos X y a los objetivos Y.</target>
        </trans-unit>
        <trans-unit id="45ba17915e2e3031d1582da2d277da7f4931a4e1" translate="yes" xml:space="preserve">
          <source>Fit the model to data.</source>
          <target state="translated">Ajustar el modelo a los datos.</target>
        </trans-unit>
        <trans-unit id="afd434d49f80c8d01132ca406d4c79b805654e96" translate="yes" xml:space="preserve">
          <source>Fit the model to data. Fit a separate model for each output variable.</source>
          <target state="translated">Ajustar el modelo a los datos.Ajustar un modelo separado para cada variable de salida.</target>
        </trans-unit>
        <trans-unit id="cfdc299b35aa8a48c0af501e27ae7cce65e8f528" translate="yes" xml:space="preserve">
          <source>Fit the model to the data X which should contain a partial segment of the data.</source>
          <target state="translated">Ajustar el modelo a los datos X que deben contener un segmento parcial de los datos.</target>
        </trans-unit>
        <trans-unit id="b93160749fdae6a7fa042ecb31b3787d54faa056" translate="yes" xml:space="preserve">
          <source>Fit the model to the data X.</source>
          <target state="translated">Ajustar el modelo a los datos X.</target>
        </trans-unit>
        <trans-unit id="22b95bc22aacaf67cf593094b402a5ffdf9c9f1a" translate="yes" xml:space="preserve">
          <source>Fit the model using X as training data</source>
          <target state="translated">Ajustar el modelo usando X como datos de entrenamiento</target>
        </trans-unit>
        <trans-unit id="c8f2bc6ec471b46a95d660913e3db5351ccdd413" translate="yes" xml:space="preserve">
          <source>Fit the model using X as training data and y as target values</source>
          <target state="translated">Ajustar el modelo usando X como datos de entrenamiento e y como valores objetivo</target>
        </trans-unit>
        <trans-unit id="67d84010e7f659892bf337ed29c5877ed8354203" translate="yes" xml:space="preserve">
          <source>Fit the model using X as training data.</source>
          <target state="translated">Ajustar el modelo usando X como datos de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="3b6c346e2454cc9e9a61a017fe7c431d552c8c5b" translate="yes" xml:space="preserve">
          <source>Fit the model using X, y as training data.</source>
          <target state="translated">Ajustar el modelo usando X,y como datos de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="5c7f073bf35ac36015511e86a701acdf9d1f18c6" translate="yes" xml:space="preserve">
          <source>Fit the model with X and apply the dimensionality reduction on X.</source>
          <target state="translated">Ajustar el modelo con X y aplicar la reducción de la dimensionalidad en X.</target>
        </trans-unit>
        <trans-unit id="ea7ce544f3a486e6f2a49538bf4fe64bf5a8afde" translate="yes" xml:space="preserve">
          <source>Fit the model with X, using minibatches of size batch_size.</source>
          <target state="translated">Ajustar el modelo con X,usando minibatches de tamaño batch_size.</target>
        </trans-unit>
        <trans-unit id="e7967bf329b3c1f757d75d92e374951929321ba7" translate="yes" xml:space="preserve">
          <source>Fit the model with X.</source>
          <target state="translated">Ponerle una X a la modelo.</target>
        </trans-unit>
        <trans-unit id="a5abbc09518b549e779e08db2e6e49f0ba32533d" translate="yes" xml:space="preserve">
          <source>Fit the random classifier.</source>
          <target state="translated">Encaja en el clasificador aleatorio.</target>
        </trans-unit>
        <trans-unit id="30f7575e3bac8aaa2852370b548ad3f04b290586" translate="yes" xml:space="preserve">
          <source>Fit the random regressor.</source>
          <target state="translated">Ajustar el regresor aleatorio.</target>
        </trans-unit>
        <trans-unit id="4fae2d49ee8a5e8d8ea52343db3e2b05ff45988e" translate="yes" xml:space="preserve">
          <source>Fit the ridge classifier.</source>
          <target state="translated">Encaja con el clasificador de cresta.</target>
        </trans-unit>
        <trans-unit id="1e4446f620ec40ca22b46eba2b6f8ce6be3c57e2" translate="yes" xml:space="preserve">
          <source>Fit the shrunk covariance model according to the given training data and parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b96765d688b574c756064537ec2686c8ac36571" translate="yes" xml:space="preserve">
          <source>Fit the transformer on X.</source>
          <target state="translated">Ponga el transformador en X.</target>
        </trans-unit>
        <trans-unit id="acd485cd941415835a11d2180278c2146ce5888c" translate="yes" xml:space="preserve">
          <source>Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)</source>
          <target state="translated">Ajustar los pesos de un modelo de regresión,usando un previo ARD.Se supone que los pesos del modelo de regresión están en las distribuciones gaussianas.También se estiman los parámetros lambda (precisiones de las distribuciones de los pesos)y alfa (precisión de la distribución del ruido).La estimación se realiza mediante un procedimiento iterativo (Maximización de la evidencia)</target>
        </trans-unit>
        <trans-unit id="ea18404b90123396c3f41537d11afad54c507780" translate="yes" xml:space="preserve">
          <source>Fit to data, then transform it.</source>
          <target state="translated">Ajustar a los datos,luego transformarlos.</target>
        </trans-unit>
        <trans-unit id="c2cf341635a3875675d46dd618b9a1757d9a6c7c" translate="yes" xml:space="preserve">
          <source>Fit transformer by checking X.</source>
          <target state="translated">Ajustar el transformador comprobando X.</target>
        </trans-unit>
        <trans-unit id="eb75d7eb91b3dadf245e1b3bf8f4c37a27824085" translate="yes" xml:space="preserve">
          <source>Fit underlying estimators.</source>
          <target state="translated">Ajustar los estimadores subyacentes.</target>
        </trans-unit>
        <trans-unit id="c093c0cee7f3b9fa93ffb32acb026baea322889f" translate="yes" xml:space="preserve">
          <source>Fits a Minimum Covariance Determinant with the FastMCD algorithm.</source>
          <target state="translated">Se ajusta a un determinante de covarianza mínima con el algoritmo FastMCD.</target>
        </trans-unit>
        <trans-unit id="f30506afc59d08eae550fdeb02ae856731ea996b" translate="yes" xml:space="preserve">
          <source>Fits all the transforms one after the other and transforms the data, then uses fit_transform on transformed data with the final estimator.</source>
          <target state="translated">Encaja todas las transformaciones una tras otra y transforma los datos,luego usa fit_transform sobre los datos transformados con el estimador final.</target>
        </trans-unit>
        <trans-unit id="c5c1cc9c0352ec3e2d5fcc0df63b71ce152f382f" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso covariance model to X.</source>
          <target state="translated">Encaja con el modelo de covarianza de GraphicalLasso en X.</target>
        </trans-unit>
        <trans-unit id="268ae9ae0a9043d80b2e575c7e344f83f78e662d" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso model to X.</source>
          <target state="translated">Encaja con el modelo de GraphicalLasso en X.</target>
        </trans-unit>
        <trans-unit id="643e04850030e1e1aeeaf619a88e7dab7e6a06be" translate="yes" xml:space="preserve">
          <source>Fits the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">Se ajusta al modelo de covarianza encogida de Ledoit-Wolf según los datos y parámetros de entrenamiento dados.</target>
        </trans-unit>
        <trans-unit id="60a2c06b8221e361c36d5fdce8ee644d8456bfce" translate="yes" xml:space="preserve">
          <source>Fits the Maximum Likelihood Estimator covariance model according to the given training data and parameters.</source>
          <target state="translated">Se ajusta al modelo de covarianza del Estimador de Máxima Verosimilitud según los datos y parámetros de capacitación dados.</target>
        </trans-unit>
        <trans-unit id="9b00c787fd8f13356df0bf86c478f2b7848ac4d7" translate="yes" xml:space="preserve">
          <source>Fits the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.</source>
          <target state="translated">Se ajusta al modelo de covarianza de encogimiento aproximado de Oracle según los datos y parámetros de entrenamiento dados.</target>
        </trans-unit>
        <trans-unit id="2fb3d8548147a6429a7eeed9e3fbe0456049bc8c" translate="yes" xml:space="preserve">
          <source>Fits the estimator.</source>
          <target state="translated">Encaja con el estimador.</target>
        </trans-unit>
        <trans-unit id="3cea2475743e34d34ded723467b2a735e1203cfc" translate="yes" xml:space="preserve">
          <source>Fits the imputer on X and return self.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bb471485459c492057aae581eaa0cf9c4592a1d" translate="yes" xml:space="preserve">
          <source>Fits the imputer on X and return the transformed X.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c13921c2e1a959ce7d05648d804f502aded299cf" translate="yes" xml:space="preserve">
          <source>Fits the model to the training set X and returns the labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98f6beb1ac87a74b86b8c1fedd5ae0ed0ac89461" translate="yes" xml:space="preserve">
          <source>Fits the shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">Se ajusta al modelo de covarianza encogida según los datos y parámetros de entrenamiento dados.</target>
        </trans-unit>
        <trans-unit id="f072640da94e1ad56e67373f3632aeaebdd8b854" translate="yes" xml:space="preserve">
          <source>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</source>
          <target state="translated">Ajusta el transformador a X e y con parámetros opcionales fit_params y devuelve una versión transformada de X.</target>
        </trans-unit>
        <trans-unit id="06f59e2a32c90e6aa8aff9dfb100b6f03f2af9ff" translate="yes" xml:space="preserve">
          <source>Fitted classifier or a fitted &lt;a href=&quot;sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; in which the last estimator is a classifier.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0dea44d64bdac3b47bf2c46576cad767f74d843" translate="yes" xml:space="preserve">
          <source>Fitted estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd568de48dadd27cd4e3ca2395da082642e8f8ec" translate="yes" xml:space="preserve">
          <source>Fitted regressor.</source>
          <target state="translated">Un regresor encajado.</target>
        </trans-unit>
        <trans-unit id="f162f6bcf340dc521df64a1aae70440e61c389db" translate="yes" xml:space="preserve">
          <source>Fitted scaler.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bccb0ee0062e477c480530940a0aca37551f6020" translate="yes" xml:space="preserve">
          <source>Fitted vectorizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14c6da7cd39661e1b0c6468a3c6a5907d4f99a9c" translate="yes" xml:space="preserve">
          <source>Fitting transformers may be computationally expensive. With its &lt;code&gt;memory&lt;/code&gt; parameter set, &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; will cache each transformer after calling &lt;code&gt;fit&lt;/code&gt;. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration.</source>
          <target state="translated">La instalaci&amp;oacute;n de transformadores puede resultar computacionalmente costosa. Con su conjunto de par&amp;aacute;metros de &lt;code&gt;memory&lt;/code&gt; , &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; almacenar&amp;aacute; en cach&amp;eacute; cada transformador despu&amp;eacute;s de llamar a &lt;code&gt;fit&lt;/code&gt; . Esta funci&amp;oacute;n se utiliza para evitar calcular los transformadores de ajuste dentro de una tuber&amp;iacute;a si los par&amp;aacute;metros y los datos de entrada son id&amp;eacute;nticos. Un ejemplo t&amp;iacute;pico es el caso de una b&amp;uacute;squeda de red en la que los transformadores pueden instalarse una sola vez y reutilizarse para cada configuraci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="01fe05c223cb56d84d085e38ca62de1932a87e50" translate="yes" xml:space="preserve">
          <source>Flag indicating if the cross-validation values corresponding to each alpha should be stored in the &lt;code&gt;cv_values_&lt;/code&gt; attribute (see below). This flag is only compatible with &lt;code&gt;cv=None&lt;/code&gt; (i.e. using Generalized Cross-Validation).</source>
          <target state="translated">Bandera que indica si los valores de validaci&amp;oacute;n cruzada correspondientes a cada alfa deben almacenarse en el atributo &lt;code&gt;cv_values_&lt;/code&gt; (ver m&amp;aacute;s abajo). Esta bandera solo es compatible con &lt;code&gt;cv=None&lt;/code&gt; (es decir, con validaci&amp;oacute;n cruzada generalizada).</target>
        </trans-unit>
        <trans-unit id="1f498759924682e2363e94f7b83282b97429fcf5" translate="yes" xml:space="preserve">
          <source>Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are:</source>
          <target state="translated">Bandera que indica la estrategia a utilizar cuando se realiza la validación cruzada generalizada.Las opciones son:</target>
        </trans-unit>
        <trans-unit id="3407c4421a1f6ede0cab565dc5123546e65ddde6" translate="yes" xml:space="preserve">
          <source>Flat geometry, good for density estimation</source>
          <target state="translated">Geometría plana,buena para la estimación de la densidad</target>
        </trans-unit>
        <trans-unit id="748a38982c93bb25fbfeb18b34277c35439ac98c" translate="yes" xml:space="preserve">
          <source>Flavanoids</source>
          <target state="translated">Flavanoids</target>
        </trans-unit>
        <trans-unit id="f55beb472c3b08362b7861294963760ddd037d08" translate="yes" xml:space="preserve">
          <source>Flavanoids:</source>
          <target state="translated">Flavanoids:</target>
        </trans-unit>
        <trans-unit id="1bf94453d6aa9e9092828eefafa6394692100339" translate="yes" xml:space="preserve">
          <source>Flexible pickling control for the communication to and from the worker processes.</source>
          <target state="translated">Control flexible del decapado para la comunicación con los procesos de los trabajadores.</target>
        </trans-unit>
        <trans-unit id="2d83a2dbf42ef510856c4fe5eb69b3efa4599763" translate="yes" xml:space="preserve">
          <source>Flow Chart</source>
          <target state="translated">Gráfico de flujo</target>
        </trans-unit>
        <trans-unit id="87698cca8f914c77b735bad53fe489d2af135e70" translate="yes" xml:space="preserve">
          <source>Folder to be used by the pool for memmapping large arrays for sharing memory with worker processes. If None, this will try in order:</source>
          <target state="translated">Carpeta para ser usada por el grupo para hacer mapas de grandes matrices para compartir la memoria con los procesos de los trabajadores.Si no hay ninguna,esto se intentará en orden:</target>
        </trans-unit>
        <trans-unit id="c09a9bf27e9aabfc7b35dc309da81eb816c5e989" translate="yes" xml:space="preserve">
          <source>Following special cases exist,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="313fc38f449c398563f152e4416c92af47202923" translate="yes" xml:space="preserve">
          <source>Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</source>
          <target state="translated">Sigue el Algoritmo 4.3 de Hallazgo de la estructura con aleatoriedad:Algoritmos estocásticos para la construcción de descomposiciones matriciales aproximadas Halko,et al.,2009 (arXiv:909)http://arxiv.org/pdf/0909.4061</target>
        </trans-unit>
        <trans-unit id="061a7a165d75c1efa21f2a2a8a850415a95a5911" translate="yes" xml:space="preserve">
          <source>Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec0c3b76630fd745381cc215a284820af75a683a" translate="yes" xml:space="preserve">
          <source>Footnotes</source>
          <target state="translated">Footnotes</target>
        </trans-unit>
        <trans-unit id="871453ce5a358112246d8fa103eff55b515e95a2" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;one-vs-rest&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;(n_classes, n_features)&lt;/code&gt; and &lt;code&gt;(n_classes,)&lt;/code&gt; respectively. Each row of the coefficients corresponds to one of the &lt;code&gt;n_classes&lt;/code&gt; &amp;ldquo;one-vs-rest&amp;rdquo; classifiers and similar for the intercepts, in the order of the &amp;ldquo;one&amp;rdquo; class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41e2c901c13d4daacf7c9adad4d559c077a8e51e" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;one-vs-rest&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class, n_features]&lt;/code&gt; and &lt;code&gt;[n_class]&lt;/code&gt; respectively. Each row of the coefficients corresponds to one of the &lt;code&gt;n_class&lt;/code&gt; many &amp;ldquo;one-vs-rest&amp;rdquo; classifiers and similar for the intercepts, in the order of the &amp;ldquo;one&amp;rdquo; class.</source>
          <target state="translated">Para &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; &amp;ldquo;one-vs-rest&amp;rdquo;, los atributos &lt;code&gt;coef_&lt;/code&gt; e &lt;code&gt;intercept_&lt;/code&gt; tienen la forma &lt;code&gt;[n_class, n_features]&lt;/code&gt; y &lt;code&gt;[n_class]&lt;/code&gt; respectivamente. Cada fila de los coeficientes corresponde a uno de los muchos &lt;code&gt;n_class&lt;/code&gt; &quot;uno contra resto&quot; de n_class y similares para las intersecciones, en el orden de la clase &quot;uno&quot;.</target>
        </trans-unit>
        <trans-unit id="c6cc35fe8de003f58f84a7c02bbc9d4b652dc227" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;pairwise&amp;rdquo; metrics, between &lt;em&gt;samples&lt;/em&gt; and not estimators or predictions, see the &lt;a href=&quot;metrics#metrics&quot;&gt;Pairwise metrics, Affinities and Kernels&lt;/a&gt; section.</source>
          <target state="translated">Para conocer las m&amp;eacute;tricas &quot;por pares&quot;, entre &lt;em&gt;muestras&lt;/em&gt; y no estimadores o predicciones, consulte la secci&amp;oacute;n &lt;a href=&quot;metrics#metrics&quot;&gt;M&amp;eacute;tricas&lt;/a&gt; por pares , afinidades y n&amp;uacute;cleos .</target>
        </trans-unit>
        <trans-unit id="935f9d2961eec1b64a8d3f62208c3a16e0e7ef63" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the &lt;code&gt;n_estimators&lt;/code&gt; parameter of &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt;.</source>
          <target state="translated">Para &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; de &amp;aacute;rboles (por ejemplo, RandomForest, GBT, ExtraTrees, etc.), el n&amp;uacute;mero de &amp;aacute;rboles y su profundidad juegan el papel m&amp;aacute;s importante. La latencia y el rendimiento deben escalar linealmente con el n&amp;uacute;mero de &amp;aacute;rboles. En este caso utilizamos directamente la &lt;code&gt;n_estimators&lt;/code&gt; par&amp;aacute;metro de &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c2e53d45d0579b4b39658069206cb04a03ac3808" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge &amp;amp; RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression&amp;hellip;) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent.</source>
          <target state="translated">Para &lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; &lt;/a&gt; (por ejemplo, Lasso, ElasticNet, SGDClassifier / Regressor, Ridge &amp;amp; RidgeClassifier, PassiveAggressiveClassifier / Regressor, LinearSVC, LogisticRegression ...) la funci&amp;oacute;n de decisi&amp;oacute;n que se aplica en el tiempo de predicci&amp;oacute;n es la misma (un producto escalar), por lo que la latencia debe ser equivalente .</target>
        </trans-unit>
        <trans-unit id="b1a84d28126765870388f5c6f8149674d42fd858" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt;, note that the output of the &lt;code&gt;estimators&lt;/code&gt; is controlled by the parameter &lt;code&gt;stack_method&lt;/code&gt; and it is called by each estimator. This parameter is either a string, being estimator method names, or &lt;code&gt;'auto'&lt;/code&gt; which will automatically identify an available method depending on the availability, tested in the order of preference: &lt;code&gt;predict_proba&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5bf64c1d60fc523cf9cf35c71f5018653089c51" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt;, when using &lt;code&gt;stack_method_='predict_proba'&lt;/code&gt;, the first column is dropped when the problem is a binary classification problem. Indeed, both probability columns predicted by each estimator are perfectly collinear.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d0c4acadc1a3e244ad80f139e9b2149c5787665" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;) any input passed as a numpy array will be copied and converted to the &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input, we suggest to use the &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; class instead. The objective function can be configured to be almost the same as the &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8582a7ae6ed830b76bb2d9fd21363d4d3995f59c" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input we suggest to use the &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; class instead. The objective function can be configured to be almost the same as the &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">Para &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; (y &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; ), cualquier entrada pasada como una matriz num&amp;eacute;rica se copiar&amp;aacute; y convertir&amp;aacute; a la representaci&amp;oacute;n de datos dispersos internos liblinear (flotantes de doble precisi&amp;oacute;n e &amp;iacute;ndices int32 de componentes distintos de cero). Si desea ajustar un clasificador lineal a gran escala sin copiar una matriz de doble precisi&amp;oacute;n contigua C densa y numpy como entrada, le sugerimos que use la clase &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; en su&lt;/a&gt; lugar. La funci&amp;oacute;n objetivo se puede configurar para que sea casi igual que el modelo &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2643eb1343d14a8cde9753d0546e2bd56743d7c1" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, the &amp;lsquo;recursion&amp;rsquo; method (used by default) will not account for the &lt;code&gt;init&lt;/code&gt; predictor of the boosting process. In practice, this will produce the same values as &amp;lsquo;brute&amp;rsquo; up to a constant offset in the target response, provided that &lt;code&gt;init&lt;/code&gt; is a constant estimator (which is the default). However, if &lt;code&gt;init&lt;/code&gt; is not a constant estimator, the partial dependence values are incorrect for &amp;lsquo;recursion&amp;rsquo; because the offset will be sample-dependent. It is preferable to use the &amp;lsquo;brute&amp;rsquo; method. Note that this only applies to &lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, not to &lt;a href=&quot;sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9c95b352fd6ddba1a98d2caa12d747735ff41c3" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;0 &amp;lt; power &amp;lt; 1&lt;/code&gt;, no distribution exists.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e56c72d645a0047396221ed2eb5407914a9b6bf6" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;make_classification&lt;/code&gt;, three binary and two multi-class classification datasets are generated, with different numbers of informative features and clusters per class.</source>
          <target state="translated">Para &lt;code&gt;make_classification&lt;/code&gt; , se generan tres conjuntos de datos de clasificaci&amp;oacute;n binarios y dos de clases m&amp;uacute;ltiples, con diferentes n&amp;uacute;meros de caracter&amp;iacute;sticas informativas y grupos por clase.</target>
        </trans-unit>
        <trans-unit id="dda7c631740b122861937f9ebbfdde73fd44b016" translate="yes" xml:space="preserve">
          <source>For Gaussian distributed data, the distance of an observation \(x_i\) to the mode of the distribution can be computed using its Mahalanobis distance: \(d_{(\mu,\Sigma)}(x_i)^2 = (x_i - \mu)'\Sigma^{-1}(x_i - \mu)\) where \(\mu\) and \(\Sigma\) are the location and the covariance of the underlying Gaussian distribution.</source>
          <target state="translated">Para los datos distribuidos de Gauss,la distancia de una observación \N al modo de distribución puede ser calculada usando su distancia de Mahalanobis:\N(d_{(\Nmu,\NSigma)}(x_i)^2=(x_i-\Nmu)'\NSigma^{\N-1}(x_i-\N-sigma)}donde \N(\Nmu)y \N(\Nsigma)son la ubicación y la covarianza de la distribución gaussiana subyacente.</target>
        </trans-unit>
        <trans-unit id="3fb903a20f5aad7e20f9123d2edfa2a0638dc6bc" translate="yes" xml:space="preserve">
          <source>For \(k\) clusters, the Calinski-Harabaz score \(s\) is given as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:</source>
          <target state="translated">Para los cúmulos,el puntaje de Calinski-Harabaz se da como la relación entre la media de dispersión entre los cúmulos y la dispersión dentro del cúmulo:</target>
        </trans-unit>
        <trans-unit id="d27bcc7c91650762beefd01dc3089ec6978d5c87" translate="yes" xml:space="preserve">
          <source>For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.</source>
          <target state="translated">Para un modelo de clasificación,se devuelve la clase prevista para cada muestra en X.Para un modelo de regresión,se devuelve el valor predicho basado en X.</target>
        </trans-unit>
        <trans-unit id="e6fd66f776dfd09a091bc857e8c9d10d50ac3ba8" translate="yes" xml:space="preserve">
          <source>For a comparison of the different scalers, transformers, and normalizers, see &lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples/preprocessing/plot_all_scaling.py&lt;/a&gt;.</source>
          <target state="translated">Para una comparaci&amp;oacute;n de los diferentes escaladores, transformadores y normalizadores, consulte &lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples / preprocessing / plot_all_scaling.py&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="33f57a2a03940da66a0808ac0a9d7e45bc98afe2" translate="yes" xml:space="preserve">
          <source>For a complete probabilistic model we also need a prior distribution for the latent variable \(h\). The most straightforward assumption (based on the nice properties of the Gaussian distribution) is \(h \sim \mathcal{N}(0, \mathbf{I})\). This yields a Gaussian as the marginal distribution of \(x\):</source>
          <target state="translated">Para un modelo probabilístico completo también necesitamos una distribución previa de la variable latente (h).La suposición más directa (basada en las bonitas propiedades de la distribución Gaussiana)es la de que Esto produce un Gaussiano como la distribución marginal de la distribución:</target>
        </trans-unit>
        <trans-unit id="7e3b25cfbbacb17bf9ce066c3983b6610e3fab10" translate="yes" xml:space="preserve">
          <source>For a constant learning rate use &lt;code&gt;learning_rate='constant'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the learning rate.</source>
          <target state="translated">Para una tasa de aprendizaje constante, use &lt;code&gt;learning_rate='constant'&lt;/code&gt; y use &lt;code&gt;eta0&lt;/code&gt; para especificar la tasa de aprendizaje.</target>
        </trans-unit>
        <trans-unit id="76a9227cf28fa05c9bb19673e15a2774476a035c" translate="yes" xml:space="preserve">
          <source>For a description of the implementation and details of the algorithms used, please refer to</source>
          <target state="translated">Para una descripción de la implementación y los detalles de los algoritmos utilizados,por favor consulte</target>
        </trans-unit>
        <trans-unit id="ccaff5036b34bf3986d666eb2f98e4ef943f3dfd" translate="yes" xml:space="preserve">
          <source>For a discussion and comparison of these algorithms, see the &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;manifold module page&lt;/a&gt;</source>
          <target state="translated">Para una discusi&amp;oacute;n y comparaci&amp;oacute;n de estos algoritmos, vea la &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;p&amp;aacute;gina del m&amp;oacute;dulo m&amp;uacute;ltiple&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7b7d7f03d534f8340da15e8579a79cb680de77bd" translate="yes" xml:space="preserve">
          <source>For a document generated from multiple topics, all topics are weighted equally in generating its bag of words.</source>
          <target state="translated">Para un documento generado a partir de múltiples temas,todos los temas se ponderan por igual en la generación de su bolsa de palabras.</target>
        </trans-unit>
        <trans-unit id="b87483db50bfd800e7f61326ccd19592abcc3547" translate="yes" xml:space="preserve">
          <source>For a few of the best biclusters, its most common document categories and its ten most important words get printed. The best biclusters are determined by their normalized cut. The best words are determined by comparing their sums inside and outside the bicluster.</source>
          <target state="translated">Se imprimen sus categorías de documentos más comunes y sus diez palabras más importantes.Los mejores bíceps están determinados por su corte normalizado.Las mejores palabras se determinan comparando sus sumas dentro y fuera del bíceps.</target>
        </trans-unit>
        <trans-unit id="859f5c51d38da3ad244e41ebf28b507a4a99bc62" translate="yes" xml:space="preserve">
          <source>For a full code example that demonstrates using a &lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt;&lt;code&gt;FunctionTransformer&lt;/code&gt;&lt;/a&gt; to do custom feature selection, see &lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;Using FunctionTransformer to select columns&lt;/a&gt;</source>
          <target state="translated">Para obtener un ejemplo de c&amp;oacute;digo completo que demuestra el uso de &lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt; &lt;code&gt;FunctionTransformer&lt;/code&gt; &lt;/a&gt; para realizar una selecci&amp;oacute;n de caracter&amp;iacute;sticas personalizadas, consulte &lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;Uso de FunctionTransformer para seleccionar columnas&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a29c02b5f33160de772eacbd74337a53f6625181" translate="yes" xml:space="preserve">
          <source>For a full-fledged example of out-of-core scaling in a text classification task see &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;.</source>
          <target state="translated">Para obtener un ejemplo completo de escalado fuera del n&amp;uacute;cleo en una tarea de clasificaci&amp;oacute;n de texto, consulte Clasificaci&amp;oacute;n &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;fuera del n&amp;uacute;cleo de documentos de texto&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="d78aafa74d01fdbbdb67982f22aabb8fb92e6131" translate="yes" xml:space="preserve">
          <source>For a given value of &lt;code&gt;n_components&lt;/code&gt;&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is often less accurate as &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is cheaper to compute, though, making use of larger feature spaces more efficient.</source>
          <target state="translated">Para un valor dado de &lt;code&gt;n_components&lt;/code&gt; ,&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; suele ser menos preciso que &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; embargo, RBFSampler es m&amp;aacute;s econ&amp;oacute;mico de calcular, lo que hace que el uso de espacios de caracter&amp;iacute;sticas m&amp;aacute;s grandes sea m&amp;aacute;s eficiente.</target>
        </trans-unit>
        <trans-unit id="46cec00c813e8ea8e2f5bd58264262a28ac481cf" translate="yes" xml:space="preserve">
          <source>For a good choice of alpha, the &lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be &amp;ldquo;sufficiently large&amp;rdquo;, or L1 models will perform at random, where &amp;ldquo;sufficiently large&amp;rdquo; depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated.</source>
          <target state="translated">Para una buena elecci&amp;oacute;n de alfa, el &lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; puede recuperar completamente el conjunto exacto de variables distintas de cero utilizando solo unas pocas observaciones, siempre que se cumplan ciertas condiciones espec&amp;iacute;ficas. En particular, el n&amp;uacute;mero de muestras debe ser &quot;suficientemente grande&quot;, o los modelos L1 funcionar&amp;aacute;n al azar, donde &quot;suficientemente grande&quot; depende del n&amp;uacute;mero de coeficientes distintos de cero, el logaritmo del n&amp;uacute;mero de caracter&amp;iacute;sticas, la cantidad de ruido, el valor absoluto m&amp;aacute;s peque&amp;ntilde;o de los coeficientes distintos de cero, y la estructura de la matriz de dise&amp;ntilde;o X. Adem&amp;aacute;s, la matriz de dise&amp;ntilde;o debe mostrar ciertas propiedades espec&amp;iacute;ficas, como no estar demasiado correlacionada.</target>
        </trans-unit>
        <trans-unit id="283fe9d87c4a4faac62d4b9cee8a3089a1cb638f" translate="yes" xml:space="preserve">
          <source>For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number.</source>
          <target state="translated">Para un problema de clasificación multi-etiqueta con clases N,a los clasificadores binarios N se les asigna un número entero entre 0 y N-1.Estos números enteros definen el orden de los modelos en la cadena.Cada clasificador se ajusta entonces a los datos de formación disponibles más las etiquetas verdaderas de las clases a cuyos modelos se les asignó un número inferior.</target>
        </trans-unit>
        <trans-unit id="73e6ca6403df9166903acb4326e48adf2d2e8f55" translate="yes" xml:space="preserve">
          <source>For a multi_class problem, if multi_class is set to be &amp;ldquo;multinomial&amp;rdquo; the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.</source>
          <target state="translated">Para un problema multi_class, si multi_class se establece como &amp;ldquo;multinomial&amp;rdquo;, la funci&amp;oacute;n softmax se usa para encontrar la probabilidad predicha de cada clase. De lo contrario, utilice un enfoque de uno contra el resto, es decir, calcule la probabilidad de cada clase asumiendo que es positiva utilizando la funci&amp;oacute;n log&amp;iacute;stica. y normalizar estos valores en todas las clases.</target>
        </trans-unit>
        <trans-unit id="642c44d27e63bf2bd3e040832cd67d4f4b97be3d" translate="yes" xml:space="preserve">
          <source>For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.</source>
          <target state="translated">Para un problema de clases múltiples,los hiperparámetros de cada clase se calculan usando las mejores puntuaciones obtenidas al hacer un descanso de uno contra uno en paralelo en todos los pliegues y clases.Por lo tanto,esta no es la verdadera pérdida multinomial.</target>
        </trans-unit>
        <trans-unit id="3c102da8b9e1a48c3e9639790d9170902789d884" translate="yes" xml:space="preserve">
          <source>For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated.</source>
          <target state="translated">Para un nuevo punto que entra en la raíz,se fusiona con el subclúster más cercano y se actualizan la suma lineal,la suma cuadrada y el número de muestras de ese subclúster.Esto se hace de forma recursiva hasta que las propiedades del nodo de la hoja se actualizan.</target>
        </trans-unit>
        <trans-unit id="14657655d860195eca6994d15ac16d17936616a4" translate="yes" xml:space="preserve">
          <source>For a one-class model, +1 or -1 is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49c44661048e1cd2101f0d885d98fc8331d85aee" translate="yes" xml:space="preserve">
          <source>For a set of data \(E\) of size \(n_E\) which has been clustered into \(k\) clusters, the Calinski-Harabasz score \(s\) is defined as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d36945b7ba93218440d9a3fb3620ffe9bc7ebec1" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to a sphere dataset, see &lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;Manifold Learning methods on a severed sphere&lt;/a&gt;</source>
          <target state="translated">Para ver un ejemplo similar, donde los m&amp;eacute;todos se aplican a un dataset de esfera, consulte &lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;M&amp;eacute;todos de aprendizaje m&amp;uacute;ltiple en una esfera cortada&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="945c76e7faeb6fe6d013381d6851dfb972b0f8ae" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to the S-curve dataset, see &lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;Comparison of Manifold Learning methods&lt;/a&gt;</source>
          <target state="translated">Para ver un ejemplo similar, donde los m&amp;eacute;todos se aplican al conjunto de datos de la curva S, consulte &lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;Comparaci&amp;oacute;n de m&amp;eacute;todos de aprendizaje m&amp;uacute;ltiple&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5693da1a0a426bf5e6f357791de67cea3dcb709e" translate="yes" xml:space="preserve">
          <source>For an adaptively decreasing learning rate, use &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6.</source>
          <target state="translated">Para una tasa de aprendizaje decreciente adaptativa, use &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; y use &lt;code&gt;eta0&lt;/code&gt; para especificar la tasa de aprendizaje inicial. Cuando se alcanza el criterio de parada, la tasa de aprendizaje se divide por 5 y el algoritmo no se detiene. El algoritmo se detiene cuando la tasa de aprendizaje desciende por debajo de 1e-6.</target>
        </trans-unit>
        <trans-unit id="289eda38dfb97e5735805aabc918de776eb35066" translate="yes" xml:space="preserve">
          <source>For an estimator to be effective, you need the distance between neighboring points to be less than some value \(d\), which depends on the problem. In one dimension, this requires on average \(n \sim 1/d\) points. In the context of the above \(k\)-NN example, if the data is described by just one feature with values ranging from 0 to 1 and with \(n\) training observations, then new data will be no further away than \(1/n\). Therefore, the nearest neighbor decision rule will be efficient as soon as \(1/n\) is small compared to the scale of between-class feature variations.</source>
          <target state="translated">Para que un estimador sea efectivo,necesitas que la distancia entre los puntos vecinos sea menor que algún valor,lo cual depende del problema.En una dimensión,esto requiere un promedio de 1/d de puntos.En el contexto del ejemplo anterior \(k)-NN,si los datos se describen por una sola característica con valores que van de 0 a 1 y con \N observaciones de entrenamiento,entonces los nuevos datos no estarán más lejos que \N(1/n).Por lo tanto,la regla de la decisión del vecino más cercano será eficiente tan pronto como \ ~ (1/n)es pequeña en comparación con la escala de las variaciones de las características entre las clases.</target>
        </trans-unit>
        <trans-unit id="cc92ccd33be99f33389b25ab23e30ca5c4b36d12" translate="yes" xml:space="preserve">
          <source>For an example of using this dataset with scikit-learn, see &lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples/applications/plot_species_distribution_modeling.py&lt;/a&gt;.</source>
          <target state="translated">Para ver un ejemplo del uso de este conjunto de datos con scikit-learn, consulte &lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples / applications / plot_species_distribution_modeling.py&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="84cc57ff6bd3680e44c549367baf76fa37633107" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples/cluster/plot_affinity_propagation.py&lt;/a&gt;.</source>
          <target state="translated">Para ver un ejemplo, consulte &lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples / cluster / plot_affinity_propagation.py&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="550540d863fd72ef27788284d554fe3cf91d4d31" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples/cluster/plot_dbscan.py&lt;/a&gt;.</source>
          <target state="translated">Para ver un ejemplo, consulte &lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples / cluster / plot_dbscan.py&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5a609c98d64d09ee64c2c225998c2e63797d885b" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples/cluster/plot_mean_shift.py&lt;/a&gt;.</source>
          <target state="translated">Para ver un ejemplo, consulte &lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples / cluster / plot_mean_shift.py&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0c7aeec7cbc3121a908ff21339ef38d8e3682ea4" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples/linear_model/plot_ard.py&lt;/a&gt;.</source>
          <target state="translated">Para ver un ejemplo, consulte &lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples / linear_model / plot_ard.py&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="8606b3a4ce46a8dc7d8f3fc386175108176c1a2f" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples/linear_model/plot_bayesian_ridge.py&lt;/a&gt;.</source>
          <target state="translated">Para ver un ejemplo, consulte &lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples / linear_model / plot_bayesian_ridge.py&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="1a253fcf6bd3baedce8e1812aafc9e481fd05853" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples/linear_model/plot_lasso_coordinate_descent_path.py&lt;/a&gt;.</source>
          <target state="translated">Para ver un ejemplo, consulte &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples / linear_model / plot_lasso_coordinate_descent_path.py&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="dd893fae3c875581ed42afc5493c8a73ae57ea3a" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples/linear_model/plot_lasso_model_selection.py&lt;/a&gt;.</source>
          <target state="translated">Para ver un ejemplo, consulte &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples / linear_model / plot_lasso_model_selection.py&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7185c2666c1903f0809fab3c9082ec1324c6a9c7" translate="yes" xml:space="preserve">
          <source>For an introduction to Unicode and character encodings in general, see Joel Spolsky&amp;rsquo;s &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;Absolute Minimum Every Software Developer Must Know About Unicode&lt;/a&gt;.</source>
          <target state="translated">Para obtener una introducci&amp;oacute;n a Unicode y las codificaciones de caracteres en general, consulte &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;M&amp;iacute;nimo absoluto que todo desarrollador de software debe saber sobre Unicode de&lt;/a&gt; Joel Spolsky .</target>
        </trans-unit>
        <trans-unit id="28d13aab6e72bcde5f37b4278086b064720820de" translate="yes" xml:space="preserve">
          <source>For an introduction to Unicode and character encodings in general, see Joel Spolsky&amp;rsquo;s &lt;a href=&quot;https://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;Absolute Minimum Every Software Developer Must Know About Unicode&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a88e794655306a2809e5d03a41b7ab87506c6aa" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 (inlier) or -1 (outlier) is returned.</source>
          <target state="translated">Para un modelo de una clase,se devuelve +1 (atípico)o -1 (atípico).</target>
        </trans-unit>
        <trans-unit id="6b041f627b95dafb713c53f369d3fb59c9505ee0" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 or -1 is returned.</source>
          <target state="translated">Para un modelo de una clase,se devuelve +1 o -1.</target>
        </trans-unit>
        <trans-unit id="90c9667034ee59a28024b8500c3a1c0772f75e0b" translate="yes" xml:space="preserve">
          <source>For an overview of available strategies in scikit-learn, see also the &lt;a href=&quot;computing#scaling-strategies&quot;&gt;out-of-core learning&lt;/a&gt; documentation.</source>
          <target state="translated">Para obtener una descripci&amp;oacute;n general de las estrategias disponibles en scikit-learn, consulte tambi&amp;eacute;n la documentaci&amp;oacute;n de &lt;a href=&quot;computing#scaling-strategies&quot;&gt;aprendizaje fuera del n&amp;uacute;cleo&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="bd83f9999f935e2f6fce3641b48e5b3393b97a71" translate="yes" xml:space="preserve">
          <source>For binary classification with a true label \(y \in \{0,1\}\) and a probability estimate \(p = \operatorname{Pr}(y = 1)\), the log loss per sample is the negative log-likelihood of the classifier given the true label:</source>
          <target state="translated">Para la clasificación binaria con una etiqueta verdadera \ ~ (y \ ~ en 0,1)y una estimación de probabilidad \ ~ (p=\ ~ nombre del operador (Pr)(y=1)),la pérdida de registro por muestra es la logaritmo negativo \ ~ probabilidad del clasificador dado la etiqueta verdadera:</target>
        </trans-unit>
        <trans-unit id="23a4f6b8b8e57d58b02ac23ef6f32b82b6049d45" translate="yes" xml:space="preserve">
          <source>For binary classification, \(f(x)\) passes through the logistic function \(g(z)=1/(1+e^{-z})\) to obtain output values between zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the rest to the negative class.</source>
          <target state="translated">Para la clasificación binaria,\(f(x)\)pasa a través de la función logística \N \N (g(z)=1/(1+e^{-z})\N para obtener valores de salida entre cero y uno.Un umbral,fijado en 0,5,asignaría muestras de salidas mayores o iguales a 0,5 a la clase positiva,y el resto a la clase negativa.</target>
        </trans-unit>
        <trans-unit id="883efcc2dc17e184b74392564fb44d2a6f9c0bf6" translate="yes" xml:space="preserve">
          <source>For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:</source>
          <target state="translated">Para los problemas binarios,podemos obtener recuentos de verdaderos negativos,falsos positivos,falsos negativos y verdaderos positivos de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="7f954d6786e07c3ef37ff8e0245acb91efbb4b2a" translate="yes" xml:space="preserve">
          <source>For classification with &lt;code&gt;loss='deviance'&lt;/code&gt; the target response is logit(p).</source>
          <target state="translated">Para la clasificaci&amp;oacute;n con &lt;code&gt;loss='deviance'&lt;/code&gt; la respuesta objetivo es logit (p).</target>
        </trans-unit>
        <trans-unit id="4118e1de638d62fd337275c2f8d28f38f1e40db3" translate="yes" xml:space="preserve">
          <source>For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Para la clasificaci&amp;oacute;n con una p&amp;eacute;rdida log&amp;iacute;stica, otra variante de SGD con una estrategia de promediado est&amp;aacute; disponible con el algoritmo de gradiente medio estoc&amp;aacute;stico (SAG), disponible como solucionador en &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7a750c34f262db1f2c0c844eb9774104416e6f5c" translate="yes" xml:space="preserve">
          <source>For classification you can think of it as the regression score before the link function.</source>
          <target state="translated">Para la clasificación puedes pensar en ello como la puntuación de regresión antes de la función de enlace.</target>
        </trans-unit>
        <trans-unit id="6a3d9b2c887776af95639587c4c76f62b0d1c8f1" translate="yes" xml:space="preserve">
          <source>For classification, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt;&lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='hinge'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_hinge'&lt;/code&gt; (PA-II). For regression, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt;&lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; (PA-II).</source>
          <target state="translated">Para la clasificaci&amp;oacute;n, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt; &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; &lt;/a&gt; se puede utilizar con &lt;code&gt;loss='hinge'&lt;/code&gt; (PA-I) o &lt;code&gt;loss='squared_hinge'&lt;/code&gt; (PA-II). Para la regresi&amp;oacute;n, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt; &lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt; &lt;/a&gt; se puede utilizar con &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; (PA-I) o &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; (PA-II).</target>
        </trans-unit>
        <trans-unit id="27898fb8346ff80dce087f616900965fd4196842" translate="yes" xml:space="preserve">
          <source>For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first &lt;code&gt;partial_fit&lt;/code&gt; call using the &lt;code&gt;classes=&lt;/code&gt; parameter.</source>
          <target state="translated">Para la clasificaci&amp;oacute;n, algo importante a tener en cuenta es que, aunque una rutina de extracci&amp;oacute;n de caracter&amp;iacute;sticas sin estado puede hacer frente a atributos nuevos / no vistos, el alumno incremental por s&amp;iacute; mismo puede ser incapaz de hacer frente a clases de objetivos nuevos / no vistos. En este caso, debe pasar todas las clases posibles a la primera llamada &lt;code&gt;partial_fit&lt;/code&gt; usando el par&amp;aacute;metro &lt;code&gt;classes=&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fda4c776ec3f2170d3e99301b50afa3144298d48" translate="yes" xml:space="preserve">
          <source>For classification, as in the labeling &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;iris&lt;/a&gt; task, linear regression is not the right approach as it will give too much weight to data far from the decision frontier. A linear approach is to fit a sigmoid function or &lt;strong&gt;logistic&lt;/strong&gt; function:</source>
          <target state="translated">Para la clasificaci&amp;oacute;n, como en la tarea de etiquetado del &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;iris&lt;/a&gt; , la regresi&amp;oacute;n lineal no es el enfoque correcto, ya que dar&amp;aacute; demasiado peso a los datos alejados de la frontera de decisi&amp;oacute;n. Un enfoque lineal es ajustar una funci&amp;oacute;n sigmoidea o una funci&amp;oacute;n &lt;strong&gt;log&amp;iacute;stica&lt;/strong&gt; :</target>
        </trans-unit>
        <trans-unit id="cda870024c615048609a38e867ca66fe1aaab764" translate="yes" xml:space="preserve">
          <source>For classification, the target response may be the probability of a class (the positive class for binary classification), or the decision function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="049512f622ab4a1900a694aa9ec5fcf56244d47a" translate="yes" xml:space="preserve">
          <source>For classification: &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt;&lt;code&gt;f_classif&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Para clasificaci&amp;oacute;n: &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt; &lt;code&gt;f_classif&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2b5c234d472222c920c562abfd3ef4ec80e6cfee" translate="yes" xml:space="preserve">
          <source>For comparison, a quantized image using a random codebook (colors picked up randomly) is also shown.</source>
          <target state="translated">Para la comparación,también se muestra una imagen cuantizada usando un libro de códigos aleatorios (colores recogidos al azar).</target>
        </trans-unit>
        <trans-unit id="3be5878f7d3ebd4495804d5a6a55058ed71eceb1" translate="yes" xml:space="preserve">
          <source>For comparison, the documents are also clustered using MiniBatchKMeans. The document clusters derived from the biclusters achieve a better V-measure than clusters found by MiniBatchKMeans.</source>
          <target state="translated">Para la comparación,los documentos también se agrupan usando MiniBatchKMeans.Los clusters de documentos derivados de los biclusters logran una mejor medida de V que los clusters encontrados por MiniBatchKMeans.</target>
        </trans-unit>
        <trans-unit id="9da7ddf96abc62edc6f61720c2e84583fe9d6b7c" translate="yes" xml:space="preserve">
          <source>For comparison, we also add the output from &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt;. It can force any arbitrary distribution into a gaussian, provided that there are enough training samples (thousands). Because it is a non-parametric method, it is harder to interpret than the parametric ones (Box-Cox and Yeo-Johnson).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d421941474530194b10c6be8d7b89e2eb530191" translate="yes" xml:space="preserve">
          <source>For comparison, we also add the output from &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt;. It can force any arbitrary distribution into a gaussian, provided that there are enough training samples (thousands). Because it is a non-parametric method, it is harder to interpret than the parametric ones (Box-Cox and Yeo-Johnson).</source>
          <target state="translated">A modo de comparaci&amp;oacute;n, tambi&amp;eacute;n agregamos la salida del &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt; . Puede forzar cualquier distribuci&amp;oacute;n arbitraria en un gaussiano, siempre que haya suficientes muestras de entrenamiento (miles). Debido a que es un m&amp;eacute;todo no param&amp;eacute;trico, es m&amp;aacute;s dif&amp;iacute;cil de interpretar que los param&amp;eacute;tricos (Box-Cox y Yeo-Johnson).</target>
        </trans-unit>
        <trans-unit id="3fedc899dde91ba75e541f7b8d87d7a3665ca193" translate="yes" xml:space="preserve">
          <source>For compatibility, user code relying on this method should wrap its calls in &lt;code&gt;np.asarray&lt;/code&gt; to avoid type issues.</source>
          <target state="translated">Por compatibilidad, el c&amp;oacute;digo de usuario que se basa en este m&amp;eacute;todo debe envolver sus llamadas en &lt;code&gt;np.asarray&lt;/code&gt; para evitar problemas de tipo.</target>
        </trans-unit>
        <trans-unit id="6f31aee2196032d492cf3c67f7f43ab3f6981996" translate="yes" xml:space="preserve">
          <source>For continuous parameters, such as &lt;code&gt;C&lt;/code&gt; above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing &lt;code&gt;n_iter&lt;/code&gt; will always lead to a finer search.</source>
          <target state="translated">Para los par&amp;aacute;metros continuos, como &lt;code&gt;C&lt;/code&gt; anterior, es importante especificar una distribuci&amp;oacute;n continua para aprovechar al m&amp;aacute;ximo la aleatorizaci&amp;oacute;n. De esta manera, aumentar &lt;code&gt;n_iter&lt;/code&gt; siempre conducir&amp;aacute; a una b&amp;uacute;squeda m&amp;aacute;s fina.</target>
        </trans-unit>
        <trans-unit id="4288bdf523218f188616117af5e7ab510c1e81a7" translate="yes" xml:space="preserve">
          <source>For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms give roughly the same results. They differ with regards to their execution speed and sources of numerical errors.</source>
          <target state="translated">Para la validación cruzada,usamos 20 veces con 2 algoritmos para calcular el camino de Lasso:descenso coordinado,como lo implementa la clase LassoCV,y Lars (regresión del ángulo menor)como lo implementa la clase LassoLarsCV.Ambos algoritmos dan aproximadamente los mismos resultados.Se diferencian en cuanto a su velocidad de ejecución y a las fuentes de errores numéricos.</target>
        </trans-unit>
        <trans-unit id="c8cbf2457961ac615bed8ee5d0896fee418cd0e6" translate="yes" xml:space="preserve">
          <source>For custom messages if &amp;ldquo;%(name)s&amp;rdquo; is present in the message string, it is substituted for the estimator name.</source>
          <target state="translated">Para mensajes personalizados, si &quot;% (nombre) s&quot; est&amp;aacute; presente en la cadena del mensaje, se sustituye por el nombre del estimador.</target>
        </trans-unit>
        <trans-unit id="67e0a596cb4bf62edc844af1488251663768a571" translate="yes" xml:space="preserve">
          <source>For details on the precise mathematical formulation of the provided kernel functions and how &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;coef0&lt;/code&gt; and &lt;code&gt;degree&lt;/code&gt; affect each other, see the corresponding section in the narrative documentation: &lt;a href=&quot;../svm#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt;.</source>
          <target state="translated">Para obtener detalles sobre la formulaci&amp;oacute;n matem&amp;aacute;tica precisa de las funciones del n&amp;uacute;cleo proporcionadas y c&amp;oacute;mo &lt;code&gt;gamma&lt;/code&gt; , &lt;code&gt;coef0&lt;/code&gt; y &lt;code&gt;degree&lt;/code&gt; afectan entre s&amp;iacute;, consulte la secci&amp;oacute;n correspondiente en la documentaci&amp;oacute;n narrativa: &lt;a href=&quot;../svm#svm-kernels&quot;&gt;Funciones del n&amp;uacute;cleo&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="885e0e96d0439bc0a3432bfe3535963e5dbd93a5" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape (n_features, n_k), where &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; It is the rotation of the Gaussian distribution, i.e. its principal axis. It corresponds to &lt;code&gt;V&lt;/code&gt;, the matrix of eigenvectors coming from the SVD of &lt;code&gt;Xk = U S Vt&lt;/code&gt; where &lt;code&gt;Xk&lt;/code&gt; is the centered matrix of samples from class k.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4ed0f1e2f8642affc7014ca7518ae7bfac1b31c" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_features, n_k], with &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; It is the rotation of the Gaussian distribution, i.e. its principal axis.</source>
          <target state="translated">Para cada clase k una matriz de forma [n_features, n_k], con &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; Es la rotaci&amp;oacute;n de la distribuci&amp;oacute;n gaussiana, es decir, su eje principal.</target>
        </trans-unit>
        <trans-unit id="a5ae820e12dddee4457f060f6b705b304ca3a1e3" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_k]. It contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system.</source>
          <target state="translated">Para cada clase k una matriz de forma [n_k].Contiene la escala de las distribuciones gaussianas a lo largo de sus ejes principales,es decir,la varianza en el sistema de coordenadas giradas.</target>
        </trans-unit>
        <trans-unit id="bb1ef71f090300eb5b67a4f2bd338bada7005d30" translate="yes" xml:space="preserve">
          <source>For each class of models we make the model complexity vary through the choice of relevant model parameters and measure the influence on both computational performance (latency) and predictive power (MSE or Hamming Loss).</source>
          <target state="translated">Para cada clase de modelos hacemos que la complejidad del modelo varíe a través de la elección de los parámetros relevantes del modelo y medimos la influencia tanto en el rendimiento computacional (latencia)como en la potencia de predicción (MSE o Hamming Loss).</target>
        </trans-unit>
        <trans-unit id="21f3c14e6817e5773b09bd85149800940dffa132" translate="yes" xml:space="preserve">
          <source>For each class, contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system. It corresponds to &lt;code&gt;S^2 /
(n_samples - 1)&lt;/code&gt;, where &lt;code&gt;S&lt;/code&gt; is the diagonal matrix of singular values from the SVD of &lt;code&gt;Xk&lt;/code&gt;, where &lt;code&gt;Xk&lt;/code&gt; is the centered matrix of samples from class k.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93f968263bf15ebdfa5ca6b61b7631f18bb2dedd" translate="yes" xml:space="preserve">
          <source>For each class, gives the covariance matrix estimated using the samples of that class. The estimations are unbiased. Only present if &lt;code&gt;store_covariance&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51af5a1cbe5b213e1b6f97e9040e40d195d22222" translate="yes" xml:space="preserve">
          <source>For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</source>
          <target state="translated">Para cada componente k, encuentre los pesos u, v que maximizan la corr. M&amp;aacute;x. (Xk u, Yk v), tales que &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0acc93a412fe0bc296f4de29ad2df21b9415e5fb" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimize:</source>
          <target state="translated">Para cada componente k,encuentra los pesos u,v que optimizan:</target>
        </trans-unit>
        <trans-unit id="34ffb7458447c8e84aeb0c0dcae78f73f1c9783e" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimizes: &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt;, such that &lt;code&gt;|u| = 1&lt;/code&gt;</source>
          <target state="translated">Para cada componente k, encuentre los pesos u, v que optimicen: &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt; , tales que &lt;code&gt;|u| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f9db939b51ba3d78630796e1c0444915001bc251" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator.</source>
          <target state="translated">Para cada punto de datos x en X y para cada árbol del conjunto,devolver el índice de la hoja x termina en cada estimador.</target>
        </trans-unit>
        <trans-unit id="4571d700205de7840eb7f685393b9c35a449521a" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. In the case of binary classification n_classes is 1.</source>
          <target state="translated">Para cada punto de datos x en X y para cada árbol del conjunto,devolver el índice de la hoja x termina en cada estimador.En el caso de la clasificación binaria n_clases es 1.</target>
        </trans-unit>
        <trans-unit id="44ac20da24a40ac490697d0897d69873261c6900" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.</source>
          <target state="translated">Para cada punto de datos x en X y para cada árbol del bosque,devuelve el índice de la hoja en la que x termina.</target>
        </trans-unit>
        <trans-unit id="d82941d49be2d46b8acb0305feded896c81f7a70" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt;, possibly with gaps in the numbering.</source>
          <target state="translated">Para cada punto de datos x en X, devuelve el &amp;iacute;ndice de la hoja en la que termina x. Las hojas est&amp;aacute;n numeradas dentro de &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt; , posiblemente con espacios en la numeraci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="7b85d3d77de0d2c34f470b25ce92cd73fbf8293c" translate="yes" xml:space="preserve">
          <source>For each dataset, 15% of samples are generated as random uniform noise. This proportion is the value given to the nu parameter of the OneClassSVM and the contamination parameter of the other outlier detection algorithms. Decision boundaries between inliers and outliers are displayed in black except for Local Outlier Factor (LOF) as it has no predict method to be applied on new data when it is used for outlier detection.</source>
          <target state="translated">Para cada conjunto de datos,el 15% de las muestras se generan como ruido uniforme aleatorio.Esta proporción es el valor dado al parámetro nu del OneClassSVM y al parámetro de contaminación de los otros algoritmos de detección de valores atípicos.Los límites de decisión entre los valores atípicos y los valores atípicos se muestran en negro,excepto para el Factor Atípico Local (LOF),ya que no tiene un método de predicción que se aplique a los nuevos datos cuando se utiliza para la detección de valores atípicos.</target>
        </trans-unit>
        <trans-unit id="894d665cd020f2e7e68923ae49f3798173d1a959" translate="yes" xml:space="preserve">
          <source>For each document &lt;code&gt;#i&lt;/code&gt;, count the number of occurrences of each word &lt;code&gt;w&lt;/code&gt; and store it in &lt;code&gt;X[i, j]&lt;/code&gt; as the value of feature &lt;code&gt;#j&lt;/code&gt; where &lt;code&gt;j&lt;/code&gt; is the index of word &lt;code&gt;w&lt;/code&gt; in the dictionary.</source>
          <target state="translated">Para cada documento &lt;code&gt;#i&lt;/code&gt; , cuente el n&amp;uacute;mero de ocurrencias de cada palabra &lt;code&gt;w&lt;/code&gt; y gu&amp;aacute;rdelo en &lt;code&gt;X[i, j]&lt;/code&gt; como el valor de la caracter&amp;iacute;stica &lt;code&gt;#j&lt;/code&gt; donde &lt;code&gt;j&lt;/code&gt; es el &amp;iacute;ndice de la palabra &lt;code&gt;w&lt;/code&gt; en el diccionario.</target>
        </trans-unit>
        <trans-unit id="086df8ffb1b70734dd37b88cb050e6a142a873b8" translate="yes" xml:space="preserve">
          <source>For each document \(d \in D\), draw the topic proportions \(\theta_d \sim \mathrm{Dirichlet}(\alpha)\). \(\alpha\) corresponds to &lt;code&gt;doc_topic_prior&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8df1081a030b15d2d8afea6cd05ff167080d1cd" translate="yes" xml:space="preserve">
          <source>For each document \(d\), draw \(\theta_d \sim \mathrm{Dirichlet}(\alpha), \: d=1...D\)</source>
          <target state="translated">Para cada documento,dibuja un dibujo de un simbolo de matemáticas.</target>
        </trans-unit>
        <trans-unit id="6eb16a2ee99fe65f3a14878a1546f940e1e0bd2a" translate="yes" xml:space="preserve">
          <source>For each feature \(i\) in the training set \(X\), &lt;a href=&quot;generated/sklearn.naive_bayes.categoricalnb#sklearn.naive_bayes.CategoricalNB&quot;&gt;&lt;code&gt;CategoricalNB&lt;/code&gt;&lt;/a&gt; estimates a categorical distribution for each feature i of X conditioned on the class y. The index set of the samples is defined as \(J = \{ 1, \dots, m \}\), with \(m\) as the number of samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9366e353cbe2cfb64854c09c8a8adb8df13a11b1" translate="yes" xml:space="preserve">
          <source>For each feature \(j\) (column of \(D\)):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d88ce97fd881b7b3225357f99ece9e603e7378b5" translate="yes" xml:space="preserve">
          <source>For each observation, tells whether or not (+1 or -1) it should be considered as an inlier according to the fitted model.</source>
          <target state="translated">Para cada observación,dice si debe ser considerada o no (+1 o -1)como un inlier según el modelo ajustado.</target>
        </trans-unit>
        <trans-unit id="ceeb3b0129a3e252c3947f10f0ffdea6343158bd" translate="yes" xml:space="preserve">
          <source>For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples.</source>
          <target state="translated">Para cada par de características del iris,el árbol de decisión aprende límites de decisión hechos de combinaciones de reglas de umbral simples inferidas de las muestras de entrenamiento.</target>
        </trans-unit>
        <trans-unit id="63ed3328d4a6b49da02c095fda15b05ff5f99ccf" translate="yes" xml:space="preserve">
          <source>For each repetition \(k\) in \({1, ..., K}\):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebc579ef4368e5d7216210ea27aaa16d7216a1cd" translate="yes" xml:space="preserve">
          <source>For each sample, the generative process is:</source>
          <target state="translated">Para cada muestra,el proceso generativo es:</target>
        </trans-unit>
        <trans-unit id="d05dbfe66f302738b6b31b31becd47c7eec37764" translate="yes" xml:space="preserve">
          <source>For each topic \(k \in K\), draw \(\beta_k \sim \mathrm{Dirichlet}(\eta)\). This provides a distribution over the words, i.e. the probability of a word appearing in topic \(k\). \(\eta\) corresponds to &lt;code&gt;topic_word_prior&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="884540e9625ad90cb4316f6468437987a56adbd4" translate="yes" xml:space="preserve">
          <source>For each topic \(k\), draw \(\beta_k \sim \mathrm{Dirichlet}(\eta),\: k =1...K\)</source>
          <target state="translated">Para cada tema,dibuja un dibujo de Kim Dirichlet,k =1...K)</target>
        </trans-unit>
        <trans-unit id="ebc60a8584824b9b236f9d24a5aa9b01b10427f0" translate="yes" xml:space="preserve">
          <source>For each value of &lt;code&gt;n_components&lt;/code&gt;, we plot:</source>
          <target state="translated">Para cada valor de &lt;code&gt;n_components&lt;/code&gt; , trazamos :</target>
        </trans-unit>
        <trans-unit id="6ad9736839514923dd21aa4c5541f4da9ec0e497" translate="yes" xml:space="preserve">
          <source>For each value of the &amp;lsquo;target&amp;rsquo; features in the &lt;code&gt;grid&lt;/code&gt; the partial dependence function need to marginalize the predictions of a tree over all possible values of the &amp;lsquo;complement&amp;rsquo; features. In decision trees this function can be evaluated efficiently without reference to the training data. For each grid point a weighted tree traversal is performed: if a split node involves a &amp;lsquo;target&amp;rsquo; feature, the corresponding left or right branch is followed, otherwise both branches are followed, each branch is weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all visited leaves. For tree ensembles the results of each individual tree are again averaged.</source>
          <target state="translated">Para cada valor de las caracter&amp;iacute;sticas 'objetivo' en la &lt;code&gt;grid&lt;/code&gt; la funci&amp;oacute;n de dependencia parcial necesita marginar las predicciones de un &amp;aacute;rbol sobre todos los valores posibles de las caracter&amp;iacute;sticas 'complementarias'. En los &amp;aacute;rboles de decisi&amp;oacute;n, esta funci&amp;oacute;n se puede evaluar de manera eficiente sin referencia a los datos de entrenamiento. Para cada punto de la cuadr&amp;iacute;cula, se realiza un recorrido de &amp;aacute;rbol ponderado: si un nodo dividido implica una caracter&amp;iacute;stica 'objetivo', se sigue la rama izquierda o derecha correspondiente; de ​​lo contrario, se siguen ambas ramas, cada rama se pondera por la fracci&amp;oacute;n de muestras de entrenamiento que ingresaron en ese punto. rama. Finalmente, la dependencia parcial viene dada por un promedio ponderado de todas las hojas visitadas. Para los conjuntos de &amp;aacute;rboles, los resultados de cada &amp;aacute;rbol individual se vuelven a promediar.</target>
        </trans-unit>
        <trans-unit id="719d4a30c8dd97a24789edd5bdd1f3a14cdc8a6c" translate="yes" xml:space="preserve">
          <source>For each word \(i\) in document \(d\):</source>
          <target state="translated">Por cada palabra del documento:</target>
        </trans-unit>
        <trans-unit id="d13eb916a8aa10457ca38f56195d8da451f22b82" translate="yes" xml:space="preserve">
          <source>For efficiency reasons, the euclidean distance between a pair of row vector x and y is computed as:</source>
          <target state="translated">Por razones de eficiencia,la distancia euclidiana entre un par de hileras de vectores x e y se calcula como:</target>
        </trans-unit>
        <trans-unit id="9aa1f675d2607b44cb2ebebaba9400cb8fdf4c6d" translate="yes" xml:space="preserve">
          <source>For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values.</source>
          <target state="translated">Para evaluar múltiples métricas,o bien dar una lista de cadenas (únicas)o un dictado con nombres como claves y llamables como valores.</target>
        </trans-unit>
        <trans-unit id="091a4026feae279bd855844156c1035b747b54da" translate="yes" xml:space="preserve">
          <source>For example &lt;code&gt;average_precision&lt;/code&gt; or the area under the roc curve can not be computed using discrete predictions alone.</source>
          <target state="translated">Por ejemplo, &lt;code&gt;average_precision&lt;/code&gt; o el &amp;aacute;rea bajo la curva roc no se pueden calcular utilizando predicciones discretas solamente.</target>
        </trans-unit>
        <trans-unit id="d73a71b2256308d0d5e12341f8e7d64f3e849f98" translate="yes" xml:space="preserve">
          <source>For example try instead of the &lt;code&gt;SVC&lt;/code&gt;:</source>
          <target state="translated">Por ejemplo, intente en lugar del &lt;code&gt;SVC&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="541edf61321b8728dd0c7cafa11d713cadb8fb1e" translate="yes" xml:space="preserve">
          <source>For example, a less computationally intensive alternative to &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; would be &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt;.</source>
          <target state="translated">Por ejemplo, una alternativa menos computacionalmente intensiva a &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; ser&amp;iacute;a &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="65cec63d764ed66c423ae4b0636bcfb02973f7ba" translate="yes" xml:space="preserve">
          <source>For example, a simple linear regression can be extended by constructing &lt;strong&gt;polynomial features&lt;/strong&gt; from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:</source>
          <target state="translated">Por ejemplo, una regresi&amp;oacute;n lineal simple puede extenderse construyendo &lt;strong&gt;caracter&amp;iacute;sticas polin&amp;oacute;micas a&lt;/strong&gt; partir de los coeficientes. En el caso de regresi&amp;oacute;n lineal est&amp;aacute;ndar, es posible que tenga un modelo que se ve as&amp;iacute; para datos bidimensionales:</target>
        </trans-unit>
        <trans-unit id="8a4469c2cb53cff6560ff1210e7df829ee4e673a" translate="yes" xml:space="preserve">
          <source>For example, classification of the properties &amp;ldquo;type of fruit&amp;rdquo; and &amp;ldquo;colour&amp;rdquo; for a set of images of fruit. The property &amp;ldquo;type of fruit&amp;rdquo; has the possible classes: &amp;ldquo;apple&amp;rdquo;, &amp;ldquo;pear&amp;rdquo; and &amp;ldquo;orange&amp;rdquo;. The property &amp;ldquo;colour&amp;rdquo; has the possible classes: &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo; and &amp;ldquo;orange&amp;rdquo;. Each sample is an image of a fruit, a label is output for both properties and each label is one of the possible classes of the corresponding property.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="794a77a9ad99cd614e0490463df18e6667fa3c9c" translate="yes" xml:space="preserve">
          <source>For example, classification using features extracted from a set of images of fruit, where each image may either be of an orange, an apple, or a pear. Each image is one sample and is labelled as one of the 3 possible classes. Multiclass classification makes the assumption that each sample is assigned to one and only one label - one sample cannot, for example, be both a pear and an apple.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f87725ef6020293ce39f30b54128c30f50570f0e" translate="yes" xml:space="preserve">
          <source>For example, if each point is just a single number (8 bytes), then an effective \(k\)-NN estimator in a paltry \(p \sim 20\) dimensions would require more training data than the current estimated size of the entire internet (&amp;plusmn;1000 Exabytes or so).</source>
          <target state="translated">Por ejemplo, si cada punto es solo un n&amp;uacute;mero (8 bytes), entonces un estimador \ (k \) - NN efectivo en unas dimensiones insignificantes \ (p \ sim 20 \) requerir&amp;iacute;a m&amp;aacute;s datos de entrenamiento que el tama&amp;ntilde;o estimado actual de todo Internet (&amp;plusmn; 1000 Exabytes m&amp;aacute;s o menos).</target>
        </trans-unit>
        <trans-unit id="5e44134c443036a12804aff41c3842c8f74c39ce" translate="yes" xml:space="preserve">
          <source>For example, in random projection, this warning is raised when the number of components, which quantifies the dimensionality of the target projection space, is higher than the number of features, which quantifies the dimensionality of the original source space, to imply that the dimensionality of the problem will not be reduced.</source>
          <target state="translated">Por ejemplo,en la proyección aleatoria,esta advertencia se plantea cuando el número de componentes,que cuantifica la dimensionalidad del espacio de proyección del objetivo,es mayor que el número de características,que cuantifica la dimensionalidad del espacio fuente original,para implicar que la dimensionalidad del problema no se reducirá.</target>
        </trans-unit>
        <trans-unit id="4bb7f297d1cf6895bcaff7553e1e2a2edd1164e9" translate="yes" xml:space="preserve">
          <source>For example, in the cases of multiple experiments, &lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt;&lt;code&gt;LeaveOneGroupOut&lt;/code&gt;&lt;/a&gt; can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one:</source>
          <target state="translated">Por ejemplo, en los casos de m&amp;uacute;ltiples experimentos, &lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt; &lt;code&gt;LeaveOneGroupOut&lt;/code&gt; &lt;/a&gt; se puede utilizar para crear una validaci&amp;oacute;n cruzada basada en los diferentes experimentos: creamos un conjunto de entrenamiento usando las muestras de todos los experimentos excepto uno:</target>
        </trans-unit>
        <trans-unit id="9dc98c27045ddaa13bc1efc30f0c70951a11ebf1" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:</source>
          <target state="translated">Por ejemplo, veamos los resultados de un clasificador multinomial Naive Bayes, que es r&amp;aacute;pido de entrenar y logra una puntuaci&amp;oacute;n F decente:</target>
        </trans-unit>
        <trans-unit id="85d6aa34dc31a086da5a0b5a46fa6367e968ccaf" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s say we&amp;rsquo;re dealing with a corpus of two documents: &lt;code&gt;['words', 'wprds']&lt;/code&gt;. The second document contains a misspelling of the word &amp;lsquo;words&amp;rsquo;. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:</source>
          <target state="translated">Por ejemplo, digamos que estamos tratando con un corpus de dos documentos: &lt;code&gt;['words', 'wprds']&lt;/code&gt; . El segundo documento contiene un error ortogr&amp;aacute;fico de la palabra &quot;palabras&quot;. Una simple representaci&amp;oacute;n de una bolsa de palabras considerar&amp;iacute;a estos dos documentos muy distintos, que difieren en las dos caracter&amp;iacute;sticas posibles. Sin embargo, una representaci&amp;oacute;n de caracteres de 2 gramos encontrar&amp;iacute;a los documentos coincidentes en 4 de las 8 caracter&amp;iacute;sticas, lo que puede ayudar al clasificador preferido a decidir mejor:</target>
        </trans-unit>
        <trans-unit id="a387381a97998b7238fd2bc704165170ce740115" translate="yes" xml:space="preserve">
          <source>For example, prediction of both wind speed and wind direction, in degrees, using data obtained at a certain location. Each sample would be data obtained at one location and both wind speed and direction would be output for each sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0256c851cb28fec27d5dcefe2d74c45aece1b249" translate="yes" xml:space="preserve">
          <source>For example, prediction of the topics relevant to a text document or video. The document or video may be about one of &amp;lsquo;religion&amp;rsquo;, &amp;lsquo;politics&amp;rsquo;, &amp;lsquo;finance&amp;rsquo; or &amp;lsquo;education&amp;rsquo;, several of the topic classes or all of the topic classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a34fc3c98b3c662bed7829df5d3e68b291f14f22" translate="yes" xml:space="preserve">
          <source>For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word &amp;lsquo;sat&amp;rsquo; in the sentence &amp;lsquo;The cat sat on the mat.&amp;rsquo;:</source>
          <target state="translated">Por ejemplo, suponga que tenemos un primer algoritmo que extrae etiquetas de parte del habla (PoS) que queremos usar como etiquetas complementarias para entrenar un clasificador de secuencia (por ejemplo, un chunker). El siguiente dict podr&amp;iacute;a ser una ventana de caracter&amp;iacute;sticas extra&amp;iacute;das alrededor de la palabra 'sentado' en la oraci&amp;oacute;n 'El gato se sent&amp;oacute; en la estera':</target>
        </trans-unit>
        <trans-unit id="5b46799167b8bb61c43e949f07a2335c6f7cd1fa" translate="yes" xml:space="preserve">
          <source>For example, the distance between &lt;code&gt;[3, na, na, 6]&lt;/code&gt; and &lt;code&gt;[1, na, 4, 5]&lt;/code&gt; is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="875d6b4f0ebd92b4525ff9ff007e35a4ef09b992" translate="yes" xml:space="preserve">
          <source>For example, the following snippet uses &lt;code&gt;chardet&lt;/code&gt; (not shipped with scikit-learn, must be installed separately) to figure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not shown here.</source>
          <target state="translated">Por ejemplo, el siguiente fragmento usa &lt;code&gt;chardet&lt;/code&gt; (no se env&amp;iacute;a con scikit-learn, debe instalarse por separado) para averiguar la codificaci&amp;oacute;n de tres textos. Luego vectoriza los textos e imprime el vocabulario aprendido. La salida no se muestra aqu&amp;iacute;.</target>
        </trans-unit>
        <trans-unit id="a5b55b26c17fcbb577abf03053fdea355f9d1a85" translate="yes" xml:space="preserve">
          <source>For example, this warning may occur when the user</source>
          <target state="translated">Por ejemplo,esta advertencia puede ocurrir cuando el usuario</target>
        </trans-unit>
        <trans-unit id="fa61683485e98ae724ab66c0cc502f9a28b6f341" translate="yes" xml:space="preserve">
          <source>For example, to download a dataset of gene expressions in mice brains:</source>
          <target state="translated">Por ejemplo,para descargar un conjunto de datos de expresiones genéticas en cerebros de ratones:</target>
        </trans-unit>
        <trans-unit id="0fb165a7680e316154f88ea288da16adb651003b" translate="yes" xml:space="preserve">
          <source>For example, to use &lt;code&gt;n_jobs&lt;/code&gt; greater than 1 in the example below, &lt;code&gt;custom_scoring_function&lt;/code&gt; function is saved in a user-created module (&lt;code&gt;custom_scorer_module.py&lt;/code&gt;) and imported:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bea8752fb35944e93422e8c1c3a159c63e531901" translate="yes" xml:space="preserve">
          <source>For example, we can compute the tf-idf of the first term in the first document in the &lt;code&gt;counts&lt;/code&gt; array as follows:</source>
          <target state="translated">Por ejemplo, podemos calcular el tf-idf del primer t&amp;eacute;rmino en el primer documento en la matriz de &lt;code&gt;counts&lt;/code&gt; siguiente manera:</target>
        </trans-unit>
        <trans-unit id="8e5e851ba9e40a81086c0f41a19109e3eeaaee54" translate="yes" xml:space="preserve">
          <source>For example, when dealing with boolean features, \(x_i^n = x_i\) for all \(n\) and is therefore useless; but \(x_i x_j\) represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier:</source>
          <target state="translated">Por ejemplo,cuando se trata de características booleanas,\N \N \N â??x_i^n=x_i\N para todos y por lo tanto es inútil;pero \N \N \N â??x_i x_jâ?)representa la conjunción de dos booleanos.De esta manera,podemos resolver el problema de XOR con un clasificador lineal:</target>
        </trans-unit>
        <trans-unit id="dfe2d757676022996b9aa748350ec295d5357a7e" translate="yes" xml:space="preserve">
          <source>For example, when using a validation set, set the &lt;code&gt;test_fold&lt;/code&gt; to 0 for all samples that are part of the validation set, and to -1 for all other samples.</source>
          <target state="translated">Por ejemplo, cuando utilice un conjunto de validaci&amp;oacute;n, establezca &lt;code&gt;test_fold&lt;/code&gt; en 0 para todas las muestras que forman parte del conjunto de validaci&amp;oacute;n y en -1 para todas las dem&amp;aacute;s muestras.</target>
        </trans-unit>
        <trans-unit id="98a47ab600b3d6adb5169d4d316e6fcca6b662b8" translate="yes" xml:space="preserve">
          <source>For examples on how it is to be used refer to the sections below.</source>
          <target state="translated">En las secciones que figuran a continuación se dan ejemplos de su utilización.</target>
        </trans-unit>
        <trans-unit id="717701439dd44ea572c3fc98c89180aa00806795" translate="yes" xml:space="preserve">
          <source>For further details on bias-variance decomposition, see section 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fd7dfc113fdc87e0b1f446fd7d77e9da35e9457" translate="yes" xml:space="preserve">
          <source>For further details on bias-variance decomposition, see section 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">Para obtener m&amp;aacute;s detalles sobre la descomposici&amp;oacute;n de la varianza-sesgo, consulte la secci&amp;oacute;n 7.3 de &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2f1f137cd461ac2e31c6cc087610e0dfea901ed1" translate="yes" xml:space="preserve">
          <source>For further details, &amp;ldquo;How to Use t-SNE Effectively&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt; provides a good discussion of the effects of various parameters, as well as interactive plots to explore those effects.</source>
          <target state="translated">Para obtener m&amp;aacute;s detalles, &quot;C&amp;oacute;mo usar t-SNE de manera efectiva&quot; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt; proporciona una buena discusi&amp;oacute;n de los efectos de varios par&amp;aacute;metros, as&amp;iacute; como gr&amp;aacute;ficos interactivos para explorar esos efectos.</target>
        </trans-unit>
        <trans-unit id="0482a5c448a6c0f244b48ce337c6748a1bc4ca45" translate="yes" xml:space="preserve">
          <source>For further details, &amp;ldquo;How to Use t-SNE Effectively&amp;rdquo; &lt;a href=&quot;https://distill.pub/2016/misread-tsne/&quot;&gt;https://distill.pub/2016/misread-tsne/&lt;/a&gt; provides a good discussion of the effects of various parameters, as well as interactive plots to explore those effects.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f1bbe6e8000be72ab1e63dd45896ee0e4b6eb7a" translate="yes" xml:space="preserve">
          <source>For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (&lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;) can perform effective non-linear feature extraction.</source>
          <target state="translated">Para datos de imagen en escala de grises donde los valores de p&amp;iacute;xeles se pueden interpretar como grados de negrura sobre un fondo blanco, como el reconocimiento de d&amp;iacute;gitos escritos a mano, el modelo de m&amp;aacute;quina Bernoulli Restricted Boltzmann ( &lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; ) puede realizar una extracci&amp;oacute;n de caracter&amp;iacute;sticas no lineal efectiva.</target>
        </trans-unit>
        <trans-unit id="0f8240ee0365c34e0de439585e58b6dfcdcc2ed3" translate="yes" xml:space="preserve">
          <source>For high-dimensional datasets with many collinear features, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; is most often preferable. However, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt;&lt;code&gt;LassoLarsCV&lt;/code&gt;&lt;/a&gt; has the advantage of exploring more relevant values of &lt;code&gt;alpha&lt;/code&gt; parameter, and if the number of samples is very small compared to the number of features, it is often faster than &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="866314f9db098f25a642d6cb6f4a133adac68ce1" translate="yes" xml:space="preserve">
          <source>For high-dimensional datasets with many collinear regressors, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; is most often preferable. However, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt;&lt;code&gt;LassoLarsCV&lt;/code&gt;&lt;/a&gt; has the advantage of exploring more relevant values of &lt;code&gt;alpha&lt;/code&gt; parameter, and if the number of samples is very small compared to the number of features, it is often faster than &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Para conjuntos de datos de alta dimensi&amp;oacute;n con muchos regresores colineales, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt; es a menudo preferible. Sin embargo, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt; &lt;code&gt;LassoLarsCV&lt;/code&gt; &lt;/a&gt; tiene la ventaja de explorar valores m&amp;aacute;s relevantes del par&amp;aacute;metro &lt;code&gt;alpha&lt;/code&gt; , y si el n&amp;uacute;mero de muestras es muy peque&amp;ntilde;o en comparaci&amp;oacute;n con el n&amp;uacute;mero de caracter&amp;iacute;sticas, a menudo es m&amp;aacute;s r&amp;aacute;pido que &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="aff7e1aca1f3054316321a609c5b24bbfe0a02a6" translate="yes" xml:space="preserve">
          <source>For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.</source>
          <target state="translated">Para información sobre las rutinas de preprocesamiento del NIST,véase M.D.Garris,J.L.Blue,G.T.Candela,D.L.Dimmick,J.Geist,P.J.Grother,S.A.Janet,y C.L.Wilson,NIST Form-Based Handprint Recognition System,NISTIR 5469,1994.</target>
        </trans-unit>
        <trans-unit id="ee57e485cfe4c61d12541e3ea6e169aab79014e8" translate="yes" xml:space="preserve">
          <source>For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.</source>
          <target state="translated">Por ejemplo,una colección de 10.000 documentos de texto corto (como los correos electrónicos)utilizará un vocabulario con un tamaño del orden de 100.000 palabras únicas en total,mientras que cada documento utilizará de 100 a 1000 palabras únicas individualmente.</target>
        </trans-unit>
        <trans-unit id="4e9f2f3fee78ca296cddfe5ea5b4e060f79a0a4e" translate="yes" xml:space="preserve">
          <source>For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">Por ejemplo,muchos elementos utilizados en la función objetiva de un algoritmo de aprendizaje (como el núcleo RBF de las máquinas vectoriales de apoyo o los regularizadores L1 y L2 de los modelos lineales)suponen que todas las características están centradas en torno a 0 y tienen una varianza en el mismo orden.Si una característica tiene una varianza de órdenes de magnitud mayores que otras,podría dominar la función objetiva y hacer que el estimador no pueda aprender de otras características correctamente como se espera.</target>
        </trans-unit>
        <trans-unit id="9c54ff6618aa4505fc044efee7a1b7aa2d457afd" translate="yes" xml:space="preserve">
          <source>For instance the below given table</source>
          <target state="translated">Por ejemplo,el siguiente cuadro</target>
        </trans-unit>
        <trans-unit id="61fc71afaf6946d5d1cad0702c1f4030326fcb83" translate="yes" xml:space="preserve">
          <source>For instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.</source>
          <target state="translated">Por ejemplo,los grupos podrían ser el año de la recogida de las muestras y así permitir la validación cruzada contra las divisiones basadas en el tiempo.</target>
        </trans-unit>
        <trans-unit id="c007161505dacd37b43b63ce0c84bfbd3ce25160" translate="yes" xml:space="preserve">
          <source>For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below.</source>
          <target state="translated">Por ejemplo,suponiendo que los datos anteriores estén distribuidos en Gauss,estimará la ubicación y covarianza anterior de manera sólida (es decir,sin dejarse influir por los valores atípicos).Las distancias de Mahalanobis obtenidas a partir de esta estimación se utilizan para derivar una medida de los valores atípicos.Esta estrategia se ilustra a continuación.</target>
        </trans-unit>
        <trans-unit id="7519828cefe279ed0b1f593fd20db7243c914832" translate="yes" xml:space="preserve">
          <source>For instance, given a matrix of shape &lt;code&gt;(10, 10)&lt;/code&gt;, one possible bicluster with three rows and two columns induces a submatrix of shape &lt;code&gt;(3, 2)&lt;/code&gt;:</source>
          <target state="translated">Por ejemplo, dada una matriz de forma &lt;code&gt;(10, 10)&lt;/code&gt; , un posible bicluster con tres filas y dos columnas induce una submatriz de forma &lt;code&gt;(3, 2)&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="66b8e52235d720becae09b00e19696b279a13527" translate="yes" xml:space="preserve">
          <source>For instance, if \(p\) singular vectors were calculated, the \(q\) best are found as described, where \(q&amp;lt;p\). Let \(U\) be the matrix with columns the \(q\) best left singular vectors, and similarly \(V\) for the right. To partition the rows, the rows of \(A\) are projected to a \(q\) dimensional space: \(A * V\). Treating the \(m\) rows of this \(m \times q\) matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to \(A^{\top} * U\) and clustering this \(n \times q\) matrix yields the column labels.</source>
          <target state="translated">Por ejemplo, si se calcularon \ (p \) vectores singulares, los \ (q \) mejores se encuentran como se describe, donde \ (q &amp;lt;p \). Sea \ (U \) la matriz con columnas los \ (q \) mejores vectores singulares izquierdos, y de manera similar \ (V \) para el derecho. Para dividir las filas, las filas de \ (A \) se proyectan en un espacio dimensional \ (q \): \ (A * V \). Si se tratan las filas \ (m \) de esta matriz \ (m \ times q \) como muestras y se agrupan con k-medias, se obtienen las etiquetas de las filas. De manera similar, proyectar las columnas a \ (A ^ {\ top} * U \) y agrupar esta matriz \ (n \ times q \) produce las etiquetas de columna.</target>
        </trans-unit>
        <trans-unit id="040e034a022032a75dc3b85f4ce9de7cff88a6fc" translate="yes" xml:space="preserve">
          <source>For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time.</source>
          <target state="translated">Por ejemplo,si trabajamos con imágenes de nivel de gris de 64x64 píxeles para el reconocimiento facial,la dimensionalidad de los datos es de 4096 y es lento entrenar una máquina de vector de soporte de RBF en datos tan amplios.Además sabemos que la dimensionalidad intrínseca de los datos es mucho menor que 4096 ya que todas las imágenes de rostros humanos se parecen un poco.Las muestras se encuentran en un múltiplo de dimensiones mucho más bajas (digamos alrededor de 200,por ejemplo).El algoritmo de PCA puede utilizarse para transformar linealmente los datos,reduciendo la dimensionalidad y conservando al mismo tiempo la mayor parte de la varianza explicada.</target>
        </trans-unit>
        <trans-unit id="f676ab37a8d5ec2f850de1fcd3ee779d6ce55a52" translate="yes" xml:space="preserve">
          <source>For instance, in the case of the digits dataset, &lt;code&gt;digits.data&lt;/code&gt; gives access to the features that can be used to classify the digits samples:</source>
          <target state="translated">Por ejemplo, en el caso del conjunto de datos de d&amp;iacute;gitos, &lt;code&gt;digits.data&lt;/code&gt; da acceso a las caracter&amp;iacute;sticas que se pueden usar para clasificar las muestras de d&amp;iacute;gitos:</target>
        </trans-unit>
        <trans-unit id="0b72faa109feccaf14265d5a54672e188bc4b73a" translate="yes" xml:space="preserve">
          <source>For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.</source>
          <target state="translated">Por ejemplo,en el siguiente ejemplo,los árboles de decisión aprenden de los datos para aproximarse a una curva sinusoidal con un conjunto de reglas de decisión de &quot;si luego no&quot;.Cuanto más profundo es el árbol,más complejas son las reglas de decisión y más ajustado es el modelo.</target>
        </trans-unit>
        <trans-unit id="a42ba327206d2f6a371d1c7b16518c8946340f21" translate="yes" xml:space="preserve">
          <source>For instance, let&amp;rsquo;s compare the two predictions 1.0 and 100 that are both 50% of their corresponding true value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb4b6181584be99d136bb8fd3d82dd09da37eec0" translate="yes" xml:space="preserve">
          <source>For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">Por ejemplo,muchos elementos utilizados en la función objetiva de un algoritmo de aprendizaje (como el núcleo RBF de las máquinas vectoriales de apoyo o los regularizadores l1 y l2 de los modelos lineales)suponen que todas las características están centradas en torno a cero y tienen una varianza en el mismo orden.Si una característica tiene una varianza de órdenes de magnitud mayores que otras,podría dominar la función objetiva y hacer que el estimador no pueda aprender de otras características correctamente como se espera.</target>
        </trans-unit>
        <trans-unit id="2500a85d8a54066d44afc291418c2bdbdb2d8331" translate="yes" xml:space="preserve">
          <source>For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size \(n_{samples} = 400\) and \(n_{features} = 64 \times 64 = 4096\), the computation time is less than 1s:</source>
          <target state="translated">Por ejemplo,a continuación se muestran 16 retratos de muestra (centrados en torno a 0,0)del conjunto de datos de Olivetti.A la derecha están los primeros 16 vectores singulares remodelados como retratos.Dado que sólo requerimos los 16 vectores singulares superiores de un conjunto de datos con tamaño \(n_{muestras}=400\)y \(n_{características}=64 \ veces 64=4096\),el tiempo de cálculo es inferior a 1s:</target>
        </trans-unit>
        <trans-unit id="8f189274df939cb66095eb188cc3a399c86cb294" translate="yes" xml:space="preserve">
          <source>For instance, we can perform a \(\chi^2\) test to the samples to retrieve only the two best features as follows:</source>
          <target state="translated">Por ejemplo,podemos realizar una prueba a las muestras para recuperar sólo las dos mejores características de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="translated">Por ejemplo:</target>
        </trans-unit>
        <trans-unit id="c017c696c4eba476debcc2637355a4ef09f7c0a3" translate="yes" xml:space="preserve">
          <source>For int/None inputs, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="560fc78b966a9c766c4d5505bc2466cc24122eb7" translate="yes" xml:space="preserve">
          <source>For int/None inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21205df9d4ba13a75af14823666b84f64bd04084" translate="yes" xml:space="preserve">
          <source>For integer/None inputs &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">Para entradas enteras / Ninguna, se utiliza &lt;code&gt;KFold&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f4cdf9352c6e062816193041b97f5514c42b421e" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">Para entradas enteras / Ninguna, se utiliza &lt;code&gt;KFold&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f206c091dc56a7e693c1c1efe6b0899c57cec04a" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used, else, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">Para entradas enteras / Ninguna, si &lt;code&gt;y&lt;/code&gt; es binario o multiclase, se usa &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt; , de lo contrario, se usa &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="84373ac49af10a751441a8470e060e3de62490b1" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">Para entradas enteras / Ninguna, si &lt;code&gt;y&lt;/code&gt; es binario o multiclase, se usa &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt; . Si &lt;code&gt;y&lt;/code&gt; no es ni binario ni multiclase, se utiliza &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="8d4fea32021fed22e35126e0e01d0c620369dc78" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If the estimator is a classifier or if &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">Para entradas enteras / Ninguna, si &lt;code&gt;y&lt;/code&gt; es binario o multiclase, se usa &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt; . Si el estimador es un clasificador o si &lt;code&gt;y&lt;/code&gt; no es binario ni multiclase, se utiliza &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="68ad85210cd514ab63c161f2689020aa738ee186" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if classifier is True and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">Para entradas enteras / Ninguna, si el clasificador es Verdadero e &lt;code&gt;y&lt;/code&gt; es binario o multiclase, se usa &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; . En todos los dem&amp;aacute;s casos, se utiliza &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="821cadb32f750528bd31875526972b81201437b9" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">Para entradas de n&amp;uacute;mero entero / Ninguno, si el estimador es un clasificador y &lt;code&gt;y&lt;/code&gt; es binario o multiclase, se utiliza &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; . En todos los dem&amp;aacute;s casos, se utiliza &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="ec46cd7e35a119deeb6479ced2aa91071495e898" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, &lt;code&gt;StratifiedKFold&lt;/code&gt; is used. In all other cases, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfc9bcb00c8530f8c99a68584e1330a4da8fc56a" translate="yes" xml:space="preserve">
          <source>For intermediate values, we can see on the second plot that good models can be found on a diagonal of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. Smooth models (lower &lt;code&gt;gamma&lt;/code&gt; values) can be made more complex by increasing the importance of classifying each point correctly (larger &lt;code&gt;C&lt;/code&gt; values) hence the diagonal of good performing models.</source>
          <target state="translated">Para valores intermedios, podemos ver en el segundo gr&amp;aacute;fico que se pueden encontrar buenos modelos en una diagonal de &lt;code&gt;C&lt;/code&gt; y &lt;code&gt;gamma&lt;/code&gt; . Los modelos suaves ( valores &lt;code&gt;gamma&lt;/code&gt; m&amp;aacute;s bajos) se pueden hacer m&amp;aacute;s complejos aumentando la importancia de clasificar cada punto correctamente ( valores &lt;code&gt;C&lt;/code&gt; m&amp;aacute;s altos), por lo tanto, la diagonal de los modelos de buen rendimiento.</target>
        </trans-unit>
        <trans-unit id="eb96f16ecd15a6be088f1dc93fa28ca4ca7ecca5" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is (n_samples_test, n_samples_train).</source>
          <target state="translated">Para kernel = &amp;rdquo;precalculado&amp;rdquo;, la forma esperada de X es (n_samples_test, n_samples_train).</target>
        </trans-unit>
        <trans-unit id="93c16e02e4641d6fe7bdb8a83439c237817b53cd" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is [n_samples_test, n_samples_train]</source>
          <target state="translated">Para kernel = &quot;precalculado&quot;, la forma esperada de X es [n_samples_test, n_samples_train]</target>
        </trans-unit>
        <trans-unit id="9cb299cfc771ccbc3a241c16ffeed37fabbcff7c" translate="yes" xml:space="preserve">
          <source>For large dataset, you may also consider using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with &amp;lsquo;log&amp;rsquo; loss.</source>
          <target state="translated">Para conjuntos de datos grandes, tambi&amp;eacute;n puede considerar usar &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; con p&amp;eacute;rdida de 'registro'.</target>
        </trans-unit>
        <trans-unit id="3b24421d17b9758d0c0e99f847aff320121bf350" translate="yes" xml:space="preserve">
          <source>For large datasets, similar (but not identical) results can be obtained via &lt;a href=&quot;https://hdbscan.readthedocs.io&quot;&gt;HDBSCAN&lt;/a&gt;. The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain &lt;em&gt;n&lt;/em&gt; (as opposed to &lt;em&gt;n^2&lt;/em&gt;) memory scaling; however, tuning of the &lt;code&gt;max_eps&lt;/code&gt; parameter will likely need to be used to give a solution in a reasonable amount of wall time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98187e1f181515ca77d41de7fa27ac44b69c7c11" translate="yes" xml:space="preserve">
          <source>For many estimators, including the SVMs, having datasets with unit standard deviation for each feature is important to get good prediction.</source>
          <target state="translated">Para muchos estimadores,incluidos los SVM,tener conjuntos de datos con desviación estándar unitaria para cada característica es importante para obtener una buena predicción.</target>
        </trans-unit>
        <trans-unit id="f1359c1e0656157adbc7e3ee11ae253cb961bb70" translate="yes" xml:space="preserve">
          <source>For mono-output tasks it is:</source>
          <target state="translated">Para las tareas de salida única lo es:</target>
        </trans-unit>
        <trans-unit id="c24592da8118b35d1dd067bf2a75576669aef344" translate="yes" xml:space="preserve">
          <source>For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression,&amp;rdquo; Annals of Statistics (with discussion), 407-499. (&lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;)</source>
          <target state="translated">Para obtener m&amp;aacute;s informaci&amp;oacute;n, consulte: Bradley Efron, Trevor Hastie, Iain Johnstone y Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression&amp;rdquo;, Annals of Statistics (con discusi&amp;oacute;n), 407-499. ( &lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="d5e463f9f0eea6808a42462cec939570f71a16a6" translate="yes" xml:space="preserve">
          <source>For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression,&amp;rdquo; Annals of Statistics (with discussion), 407-499. (&lt;a href=&quot;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4bc4c3f735998ec8c1614ec6127a37e3e7a02d8" translate="yes" xml:space="preserve">
          <source>For more information, see &lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;.</source>
          <target state="translated">Para obtener m&amp;aacute;s informaci&amp;oacute;n, consulte &lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;Agrupaci&amp;oacute;n jer&amp;aacute;rquica&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="b089e1ecd97ba592f22037a3c3fabf2924387c39" translate="yes" xml:space="preserve">
          <source>For more on usage see the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">Para obtener m&amp;aacute;s informaci&amp;oacute;n sobre el uso, consulte la &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;Gu&amp;iacute;a del usuario&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2e682df49d58d058f1f4b4c26ca6fb15a2f979d8" translate="yes" xml:space="preserve">
          <source>For multi-class classification, &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt;&lt;code&gt;AdaBoostClassifier&lt;/code&gt;&lt;/a&gt; implements AdaBoost-SAMME and AdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt;.</source>
          <target state="translated">Para la clasificaci&amp;oacute;n de clases m&amp;uacute;ltiples, &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt; &lt;code&gt;AdaBoostClassifier&lt;/code&gt; &lt;/a&gt; implementa AdaBoost-SAMME y AdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="416ab9ed1829c79f2f091ddf9b13cfb5bb7486ce" translate="yes" xml:space="preserve">
          <source>For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.</source>
          <target state="translated">Para la clasificación multiclase,los clasificadores n_clase se entrenan en un enfoque de uno contra todos.Concretamente,esto se implementa aprovechando el apoyo de respuesta multivariante en Ridge.</target>
        </trans-unit>
        <trans-unit id="f04898b2d6925a5dec9ef02339647623f6f19f8d" translate="yes" xml:space="preserve">
          <source>For multi-class classification, you need to set the class label for which the PDPs should be created via the &lt;code&gt;target&lt;/code&gt; argument:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65042013a5d26811a6a7088f4c47e70c6ddb0074" translate="yes" xml:space="preserve">
          <source>For multi-class models, you need to set the class label for which the PDPs should be created via the &lt;code&gt;label&lt;/code&gt; argument:</source>
          <target state="translated">Para modelos de varias clases, debe establecer la etiqueta de clase para la que se deben crear los PDP a trav&amp;eacute;s del argumento de &lt;code&gt;label&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="ccc2264ef7a998ec0c6ed9c92245080e0f0807c7" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, the scores for all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at the keys ending with that scorer&amp;rsquo;s name (&lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt;) instead of &lt;code&gt;'_score'&lt;/code&gt; shown above. (&amp;lsquo;split0_test_precision&amp;rsquo;, &amp;lsquo;mean_train_precision&amp;rsquo; etc.)</source>
          <target state="translated">Para la evaluaci&amp;oacute;n &lt;code&gt;cv_results_&lt;/code&gt; , las puntuaciones de todos los anotadores est&amp;aacute;n disponibles en el dictado cv_results_ en las claves que terminan con el nombre de ese anotador ( &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; ) en lugar de &lt;code&gt;'_score'&lt;/code&gt; que se muestra arriba. ('split0_test_precision', 'mean_train_precision', etc.)</target>
        </trans-unit>
        <trans-unit id="6cd27769ef18013ec211b9a824f9bced7dd1ce74" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute holds the validated &lt;code&gt;scoring&lt;/code&gt; dict which maps the scorer key to the scorer callable.</source>
          <target state="translated">Para la evaluaci&amp;oacute;n multim&amp;eacute;trica, este atributo contiene el dictado de &lt;code&gt;scoring&lt;/code&gt; validado que asigna la clave del anotador al anotador que se puede llamar.</target>
        </trans-unit>
        <trans-unit id="39c0f65b87a914c1f3244978c44bbc4d0d1190be" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">Para la evaluaci&amp;oacute;n multim&amp;eacute;trica, este atributo est&amp;aacute; presente solo si se especifica &lt;code&gt;refit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="dd788cb84c37fa5cbd110321907573fb764ddce9" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is not available if &lt;code&gt;refit&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;. See &lt;code&gt;refit&lt;/code&gt; parameter for more information.</source>
          <target state="translated">Para la evaluaci&amp;oacute;n multim&amp;eacute;trica, esto no est&amp;aacute; disponible si &lt;code&gt;refit&lt;/code&gt; es &lt;code&gt;False&lt;/code&gt; . Consulte el par&amp;aacute;metro de &lt;code&gt;refit&lt;/code&gt; para obtener m&amp;aacute;s informaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="4367b150d838b05eaff7170a1426f1dc4e6edc76" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">Para la evaluaci&amp;oacute;n multim&amp;eacute;trica, esto solo est&amp;aacute; presente si se especifica &lt;code&gt;refit&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d328aa31182c6b57c5d921c1da1c7333c03486fe" translate="yes" xml:space="preserve">
          <source>For multi-output tasks it is:</source>
          <target state="translated">Para tareas con múltiples salidas lo es:</target>
        </trans-unit>
        <trans-unit id="22c12fa701004eab18f67dae2fb9f6cb3b68f428" translate="yes" xml:space="preserve">
          <source>For multi-output, the weights of each column of y will be multiplied.</source>
          <target state="translated">Para la salida múltiple,se multiplicarán los pesos de cada columna de y.</target>
        </trans-unit>
        <trans-unit id="77f8b599f18368a52e2142c2cada7c61eef9fbca" translate="yes" xml:space="preserve">
          <source>For multiclass classification with a &amp;ldquo;negative class&amp;rdquo;, it is possible to exclude some labels:</source>
          <target state="translated">Para la clasificaci&amp;oacute;n multiclase con una &quot;clase negativa&quot;, es posible excluir algunas etiquetas:</target>
        </trans-unit>
        <trans-unit id="dedd3af803ae0305b6a99c040827201493989ff1" translate="yes" xml:space="preserve">
          <source>For multiclass classification, K trees (for K classes) are built at each of the \(M\) iterations. The probability that \(x_i\) belongs to class k is modeled as a softmax of the \(F_{M,k}(x_i)\) values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="393c73f8bfb73747a6a85987ccf51d73c8d3636f" translate="yes" xml:space="preserve">
          <source>For multiclass problems, only &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; handle multinomial loss; &amp;lsquo;liblinear&amp;rsquo; is limited to one-versus-rest schemes.</source>
          <target state="translated">Para problemas multiclase, solo 'newton-cg', 'sag', 'saga' y 'lbfgs' manejan la p&amp;eacute;rdida multinomial; 'liblinear' se limita a esquemas uno versus resto.</target>
        </trans-unit>
        <trans-unit id="de227a68bb98af9d7f682ac4556427f028c04d66" translate="yes" xml:space="preserve">
          <source>For multiple labels per instance, use &lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">Para varias etiquetas por instancia, use &lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="36264d57db60ea0d287310ae67879ef2207922af" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a &lt;code&gt;str&lt;/code&gt; denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="088c3cd08ec3b30c1e8d705dc9f773b25266ffd1" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer is used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">Para la evaluación de métricas múltiples,ésta debe ser una cadena que denote que el anotador se utiliza para encontrar los mejores parámetros para reajustar el estimador al final.</target>
        </trans-unit>
        <trans-unit id="ab406c3bb6ddaeec6408e58ba4985d8a5097ee33" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">Para la evaluación de métricas múltiples,ésta debe ser una cadena que denote el calificador que se utilizaría para encontrar los mejores parámetros para reajustar el estimador al final.</target>
        </trans-unit>
        <trans-unit id="f895ac59b8264ca94c275f903e2d6c6c438b4c9c" translate="yes" xml:space="preserve">
          <source>For multiplicative-update (&amp;lsquo;mu&amp;rsquo;) solver, the Frobenius norm (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss, by changing the beta_loss parameter.</source>
          <target state="translated">Para el solucionador de actualizaci&amp;oacute;n multiplicativa ('mu'), la norma de Frobenius (0.5 * || X - WH || _Fro ^ 2) se puede cambiar a otra p&amp;eacute;rdida de divergencia beta, cambiando el par&amp;aacute;metro beta_loss.</target>
        </trans-unit>
        <trans-unit id="4d0bed9bc5aa3b36bb0ba6ad6bc592a5bb3e78af" translate="yes" xml:space="preserve">
          <source>For n_components == &amp;lsquo;mle&amp;rsquo;, this class uses the method of &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt;</source>
          <target state="translated">Para n_components == 'mle', esta clase utiliza el m&amp;eacute;todo de &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="bd307d11754e6296ef567a0e60c357948ac810da" translate="yes" xml:space="preserve">
          <source>For n_components == &amp;lsquo;mle&amp;rsquo;, this class uses the method of &lt;em&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/em&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f27cc961ae174b8e96b15764c2907159174c2c2" translate="yes" xml:space="preserve">
          <source>For non-sparse models, i.e. when there are not many zeros in &lt;code&gt;coef_&lt;/code&gt;, this may actually &lt;em&gt;increase&lt;/em&gt; memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt;, must be more than 50% for this to provide significant benefits.</source>
          <target state="translated">Para modelos no dispersos, es decir, cuando no hay muchos ceros en &lt;code&gt;coef_&lt;/code&gt; , esto puede &lt;em&gt;aumentar el&lt;/em&gt; uso de memoria, as&amp;iacute; que use este m&amp;eacute;todo con cuidado. Una regla general es que el n&amp;uacute;mero de elementos cero, que se puede calcular con &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt; , debe ser superior al 50% para que esto proporcione beneficios significativos.</target>
        </trans-unit>
        <trans-unit id="ffa9a81349b558a734852a9e93a9e01e72e14f97" translate="yes" xml:space="preserve">
          <source>For normalized mutual information and adjusted mutual information, the normalizing value is typically some &lt;em&gt;generalized&lt;/em&gt; mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides &amp;ldquo;qualitatively similar behaviours&amp;rdquo; &lt;a href=&quot;#yat2016&quot; id=&quot;id14&quot;&gt;[YAT2016]&lt;/a&gt;. In our implementation, this is controlled by the &lt;code&gt;average_method&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e62cf2ed735d43186d3b55660d3dd2856258814f" translate="yes" xml:space="preserve">
          <source>For normalized mutual information and adjusted mutual information, the normalizing value is typically some &lt;em&gt;generalized&lt;/em&gt; mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides &amp;ldquo;qualitatively similar behaviours&amp;rdquo; [YAT2016]. In our implementation, this is controlled by the &lt;code&gt;average_method&lt;/code&gt; parameter.</source>
          <target state="translated">Para la informaci&amp;oacute;n mutua normalizada y la informaci&amp;oacute;n mutua ajustada, el valor de normalizaci&amp;oacute;n suele ser una media &lt;em&gt;generalizada&lt;/em&gt; de las entrop&amp;iacute;as de cada agrupaci&amp;oacute;n. Existen varios medios generalizados y no existen reglas firmes para preferir uno sobre los otros. La decisi&amp;oacute;n se toma en gran medida campo por campo; por ejemplo, en la detecci&amp;oacute;n de comunidades, la media aritm&amp;eacute;tica es la m&amp;aacute;s com&amp;uacute;n. Cada m&amp;eacute;todo de normalizaci&amp;oacute;n proporciona &quot;comportamientos cualitativamente similares&quot; [YAT2016]. En nuestra implementaci&amp;oacute;n, esto est&amp;aacute; controlado por el par&amp;aacute;metro &lt;code&gt;average_method&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8d22dd702102471017e490a2b7bc8b22b9add5e5" translate="yes" xml:space="preserve">
          <source>For now &amp;ldquo;auto&amp;rdquo; (kept for backward compatibiliy) chooses &amp;ldquo;elkan&amp;rdquo; but it might change in the future for a better heuristic.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5573de0c3d8eae2871980794db1007675aa56eed" translate="yes" xml:space="preserve">
          <source>For now, we will consider the estimator as a black box:</source>
          <target state="translated">Por ahora,consideraremos el estimador como una caja negra:</target>
        </trans-unit>
        <trans-unit id="436dc589cc421427188d0cca81de34e1e73f3c7d" translate="yes" xml:space="preserve">
          <source>For one sample, given the vector of continuous ground-truth values for each target \(y \in \mathbb{R}^{M}\), where \(M\) is the number of outputs, and the prediction \(\hat{y}\), which induces the ranking function \(f\), the DCG score is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09d6f289aa89a27e5d33a2c2e001f7d32a001ce5" translate="yes" xml:space="preserve">
          <source>For our dataset, again the model is not very predictive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7f8c21b68cc173c251c1992bff906fb9b13f276" translate="yes" xml:space="preserve">
          <source>For parameter estimation, the posterior distribution is:</source>
          <target state="translated">Para la estimación de los parámetros,la distribución posterior es:</target>
        </trans-unit>
        <trans-unit id="acaedbca04fb48c0d8436452cabe74f03629d275" translate="yes" xml:space="preserve">
          <source>For regression the default learning rate schedule is inverse scaling (&lt;code&gt;learning_rate='invscaling'&lt;/code&gt;), given by</source>
          <target state="translated">Para la regresi&amp;oacute;n, el programa de tasa de aprendizaje predeterminado es la escala inversa ( &lt;code&gt;learning_rate='invscaling'&lt;/code&gt; ), dada por</target>
        </trans-unit>
        <trans-unit id="17d6bf85a6ee02e9c1e3f5f799f4a791f96ca437" translate="yes" xml:space="preserve">
          <source>For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Para la regresi&amp;oacute;n con una p&amp;eacute;rdida al cuadrado y una penalizaci&amp;oacute;n de 12, est&amp;aacute; disponible otra variante de SGD con una estrategia de promediado con el algoritmo de gradiente medio estoc&amp;aacute;stico (SAG), disponible como solucionador en &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="82e8631b28597b123d5803439222a08ee2e59047" translate="yes" xml:space="preserve">
          <source>For regression, &lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt;&lt;code&gt;AdaBoostRegressor&lt;/code&gt;&lt;/a&gt; implements AdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt;.</source>
          <target state="translated">Para la regresi&amp;oacute;n, &lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt; &lt;code&gt;AdaBoostRegressor&lt;/code&gt; &lt;/a&gt; implementa AdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="faab950ebeb88e87f11e22c6efcd943439e07aea" translate="yes" xml:space="preserve">
          <source>For regression, MLP uses the Square Error loss function; written as,</source>
          <target state="translated">Para la regresión,el MLP utiliza la función de pérdida del error cuadrado;escrito como,</target>
        </trans-unit>
        <trans-unit id="a8c842b7da02e24dac30b073413aa7112e52aecd" translate="yes" xml:space="preserve">
          <source>For regression: &lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt;&lt;code&gt;f_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Para la regresi&amp;oacute;n: &lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt; &lt;code&gt;f_regression&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e8aa16ccbf6b94ae32b7c5b78e608f800f0eb6cd" translate="yes" xml:space="preserve">
          <source>For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance. Going forward, np.ndarray returns an np.ndarray, as expected.</source>
          <target state="translated">Para las versiones 0.14.1 y anteriores de scikit-learn,return_as=np.ndarray se manejaba devolviendo una instancia densa de np.matrix.Siguiendo adelante,np.ndarray devuelve un np.ndarray,como se esperaba.</target>
        </trans-unit>
        <trans-unit id="2410f1ccaa1a03065aaeec2b709967381feb9cea" translate="yes" xml:space="preserve">
          <source>For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping:</source>
          <target state="translated">Para transformaciones simples,en lugar de un objeto Transformador,se puede pasar un par de funciones que definen la transformación y su mapeo inverso:</target>
        </trans-unit>
        <trans-unit id="2f72f7e3c1f68f97fc714ad1c06f3f5738fb15a6" translate="yes" xml:space="preserve">
          <source>For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples.</source>
          <target state="translated">Para simplificar,la ecuación anterior está escrita para un solo ejemplo de entrenamiento.El gradiente con respecto a los pesos está formado por dos términos correspondientes a los anteriores.Normalmente se conocen como el gradiente positivo y el gradiente negativo,debido a sus respectivos signos.En esta aplicación,los gradientes se estiman sobre mini lotes de muestras.</target>
        </trans-unit>
        <trans-unit id="27c46746207f2a31ae25da1632ef3ccf3ef87e4d" translate="yes" xml:space="preserve">
          <source>For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</source>
          <target state="translated">Para la evaluaci&amp;oacute;n de m&amp;eacute;trica &amp;uacute;nica, donde el par&amp;aacute;metro de puntuaci&amp;oacute;n es una cadena, invocable o None, las claves ser&amp;aacute;n: &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a0a8bb77034843b440cd9ae1f7c55bd3aa47ece1" translate="yes" xml:space="preserve">
          <source>For small data sets (\(N\) less than 30 or so), \(\log(N)\) is comparable to \(N\), and brute force algorithms can be more efficient than a tree-based approach. Both &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; address this through providing a &lt;em&gt;leaf size&lt;/em&gt; parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small \(N\).</source>
          <target state="translated">Para conjuntos de datos peque&amp;ntilde;os (\ (N \) menos de 30 aproximadamente), \ (\ log (N) \) es comparable a \ (N \), y los algoritmos de fuerza bruta pueden ser m&amp;aacute;s eficientes que un enfoque basado en &amp;aacute;rboles. Tanto &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; como &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; abordan esto proporcionando un par&amp;aacute;metro de &lt;em&gt;tama&amp;ntilde;o de hoja&lt;/em&gt; : esto controla el n&amp;uacute;mero de muestras en las que una consulta cambia a fuerza bruta. Esto permite que ambos algoritmos se acerquen a la eficiencia de un c&amp;aacute;lculo de fuerza bruta para \ (N \) peque&amp;ntilde;o.</target>
        </trans-unit>
        <trans-unit id="4638d963661692a289a12b8cac2d92a9d2c758fa" translate="yes" xml:space="preserve">
          <source>For small datasets, &amp;lsquo;liblinear&amp;rsquo; is a good choice, whereas &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; are faster for large ones.</source>
          <target state="translated">Para conjuntos de datos peque&amp;ntilde;os, 'liblinear' es una buena opci&amp;oacute;n, mientras que 'sag' y 'saga' son m&amp;aacute;s r&amp;aacute;pidos para los grandes.</target>
        </trans-unit>
        <trans-unit id="dffce5e2239efe7c22f00e78e9cfce148c8ba698" translate="yes" xml:space="preserve">
          <source>For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.</source>
          <target state="translated">En algunas aplicaciones,la cantidad de ejemplos,características (o ambas)y/o la velocidad a la que deben procesarse son un reto para los enfoques tradicionales.En estos casos,scikit-learn tiene una serie de opciones que puede considerar para hacer que su sistema escale.</target>
        </trans-unit>
        <trans-unit id="d0fa030cdd6e029de147eaddac9b22a69b1ced78" translate="yes" xml:space="preserve">
          <source>For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).</source>
          <target state="translated">Para algunas aplicaciones,el rendimiento (principalmente la latencia y el rendimiento en el tiempo de predicción)de los estimadores es crucial.También puede ser interesante considerar el rendimiento de la capacitación,pero a menudo es menos importante en una instalación de producción (donde a menudo tiene lugar fuera de línea).</target>
        </trans-unit>
        <trans-unit id="7c42c316b39e1433fd7efc7ca18424a81519f374" translate="yes" xml:space="preserve">
          <source>For some business applications, we are interested in the ability of the model to rank the riskiest from the safest policyholders, irrespective of the absolute value of the prediction. In this case, the model evaluation would cast the problem as a ranking problem rather than a regression problem.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7400fa7073eb75f62370e5aadbb0f2aef8d5fc81" translate="yes" xml:space="preserve">
          <source>For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using &lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt;&lt;code&gt;PredefinedSplit&lt;/code&gt;&lt;/a&gt; it is possible to use these folds e.g. when searching for hyperparameters.</source>
          <target state="translated">Para algunos conjuntos de datos, ya existe una divisi&amp;oacute;n predefinida de los datos en pliegues de capacitaci&amp;oacute;n y validaci&amp;oacute;n o en varios pliegues de validaci&amp;oacute;n cruzada. Con &lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt; &lt;code&gt;PredefinedSplit&lt;/code&gt; &lt;/a&gt; es posible utilizar estos pliegues, por ejemplo, al buscar hiperpar&amp;aacute;metros.</target>
        </trans-unit>
        <trans-unit id="694369dc091b24e34e3df33be9eef2601d0ef6d1" translate="yes" xml:space="preserve">
          <source>For some losses, e.g. the least absolute deviation (LAD) where the gradients are \(\pm 1\), the values predicted by a fitted \(h_m\) are not accurate enough: the tree can only output integer values. As a result, the leaves values of the tree \(h_m\) are modified once the tree is fitted, such that the leaves values minimize the loss \(L_m\). The update is loss-dependent: for the LAD loss, the value of a leaf is updated to the median of the samples in that leaf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c50bf24a893de08f1d0809fe397202f1a031fb85" translate="yes" xml:space="preserve">
          <source>For some miscellaneous data such as images, videos, and audio, you may wish to refer to:</source>
          <target state="translated">Para algunos datos misceláneos como imágenes,videos y audio,puede que desee consultar:</target>
        </trans-unit>
        <trans-unit id="303cfe0bd7811405e77868ed843c4904556ebde5" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">Para una entrada dispersa, los datos se &lt;strong&gt;convierten a la representaci&amp;oacute;n de filas dispersas comprimidas&lt;/strong&gt; (consulte &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; ) antes de ser alimentados a rutinas Cython eficientes. Para evitar copias de memoria innecesarias, se recomienda elegir la representaci&amp;oacute;n CSR en sentido ascendente.</target>
        </trans-unit>
        <trans-unit id="44ae99861a7942ab4350b3f16d27ddb33207e51f" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">Para una entrada dispersa, los datos se &lt;strong&gt;convierten a la representaci&amp;oacute;n de filas dispersas comprimidas&lt;/strong&gt; (consulte &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; ). Para evitar copias de memoria innecesarias, se recomienda elegir la representaci&amp;oacute;n CSR en sentido ascendente.</target>
        </trans-unit>
        <trans-unit id="a6ddfb481ebd700db5464677bebe705030e704c9" translate="yes" xml:space="preserve">
          <source>For speed and space efficiency reasons &lt;code&gt;scikit-learn&lt;/code&gt; loads the target attribute as an array of integers that corresponds to the index of the category name in the &lt;code&gt;target_names&lt;/code&gt; list. The category integer id of each sample is stored in the &lt;code&gt;target&lt;/code&gt; attribute:</source>
          <target state="translated">Por razones de velocidad y eficiencia de espacio, &lt;code&gt;scikit-learn&lt;/code&gt; carga el atributo de destino como una matriz de n&amp;uacute;meros enteros que corresponde al &amp;iacute;ndice del nombre de la categor&amp;iacute;a en la lista de &lt;code&gt;target_names&lt;/code&gt; . El ID de n&amp;uacute;mero entero de categor&amp;iacute;a de cada muestra se almacena en el atributo de &lt;code&gt;target&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="7ace947ef3298ab26b0edee5253deecf977a3b02" translate="yes" xml:space="preserve">
          <source>For speed, all real work is done at the C level in function copy_predict (libsvm_helper.c).</source>
          <target state="translated">Para la velocidad,todo el trabajo real se hace en el nivel C en la función copy_predict (libsvm_helper.c).</target>
        </trans-unit>
        <trans-unit id="1c3a0f29bcc1c543ffc020478b77d5b706223ce4" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit domain-specific stratification of the dataset.</source>
          <target state="translated">Para dividir los datos según una estratificación explícita de dominio específico del conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="0adf7a63adc917db1adffd6d4cf61e05de34a6e7" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit, domain-specific stratification of the dataset.</source>
          <target state="translated">Para dividir los datos según una estratificación explícita y específica del dominio del conjunto de datos.</target>
        </trans-unit>
        <trans-unit id="ab4a742934d8510715858d09854f728742beaaec" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;arpack&amp;rsquo;, refer to &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt;.</source>
          <target state="translated">Para svd_solver == 'arpack', consulte &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="25caaa7ea914cf58c60f262a55964ee17d21e090" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;randomized&amp;rsquo;, see: &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; and also &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</source>
          <target state="translated">Para svd_solver == 'randomized', consulte: &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; y tambi&amp;eacute;n &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c853f7e3d4c36b0f8076402fc9583125688d75a1" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;randomized&amp;rsquo;, see: &lt;em&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/em&gt; and also &lt;em&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/em&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7341be727234aad9fa4e331ba9d61b6fce122ff" translate="yes" xml:space="preserve">
          <source>For the &amp;lsquo;liblinear&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">Para los solucionadores 'liblinear', 'sag' y 'lbfgs', establezca verbose en cualquier n&amp;uacute;mero positivo para verbosity.</target>
        </trans-unit>
        <trans-unit id="e0f6f55d7894824895d15fbf0dd2debb3188c403" translate="yes" xml:space="preserve">
          <source>For the &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the &lt;code&gt;nu&lt;/code&gt; parameter of &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt;&lt;/a&gt; was used to influence the number of support vectors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a081b6c50a07cf5457333031806f4c48e54ea42e" translate="yes" xml:space="preserve">
          <source>For the &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the &lt;code&gt;nu&lt;/code&gt; parameter of &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; was used to influence the number of support vectors.</source>
          <target state="translated">Para la familia de algoritmos &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; &lt;/a&gt; con un kernel no lineal, la latencia est&amp;aacute; ligada al n&amp;uacute;mero de vectores de soporte (cuantos menos, m&amp;aacute;s r&amp;aacute;pido). La latencia y el rendimiento deber&amp;iacute;an crecer (asint&amp;oacute;ticamente) linealmente con el n&amp;uacute;mero de vectores de soporte en un modelo SVC o SVR. El kernel tambi&amp;eacute;n influir&amp;aacute; en la latencia, ya que se utiliza para calcular la proyecci&amp;oacute;n del vector de entrada una vez por vector de soporte. En el siguiente gr&amp;aacute;fico, se us&amp;oacute; el par&amp;aacute;metro &lt;code&gt;nu&lt;/code&gt; de &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; para influir en el n&amp;uacute;mero de vectores de soporte.</target>
        </trans-unit>
        <trans-unit id="49dfac47eea992144c43ccc29f61713fabd0e5ae" translate="yes" xml:space="preserve">
          <source>For the &lt;code&gt;l2&lt;/code&gt; penalty case, the best result comes from the case where &lt;code&gt;C&lt;/code&gt; is not scaled.</source>
          <target state="translated">Para el caso de penalizaci&amp;oacute;n de &lt;code&gt;l2&lt;/code&gt; , el mejor resultado proviene del caso en el que &lt;code&gt;C&lt;/code&gt; no est&amp;aacute; escalado.</target>
        </trans-unit>
        <trans-unit id="b64515e541acdc2619277e9fede8598b267eca89" translate="yes" xml:space="preserve">
          <source>For the coefficient analysis, scaling is not needed this time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73523e969cb224887344b79725c4bd74783daf1f" translate="yes" xml:space="preserve">
          <source>For the grid of &lt;code&gt;Cs&lt;/code&gt; values and &lt;code&gt;l1_ratios&lt;/code&gt; values, the best hyperparameter is selected by the cross-validator &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt;, but it can be changed using the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt; parameter. The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers can warm-start the coefficients (see &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;Glossary&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9f81a56586341e43516abb99b238b1b5d6587c8" translate="yes" xml:space="preserve">
          <source>For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data.</source>
          <target state="translated">Para la cuadrícula de valores de Cs (que por defecto se establecen en diez valores en una escala logarítmica entre 1e-4 y 1e4),el mejor hiperparámetro es seleccionado por el validador cruzado StratifiedKFold,pero puede ser cambiado usando el parámetro cv.En el caso de los solucionadores de newton-cg y lbfgs,hacemos un arranque en caliente a lo largo del camino,es decir,suponemos que los coeficientes iniciales del ajuste actual son los coeficientes obtenidos tras la convergencia en el ajuste anterior,por lo que se supone que es más rápido para los datos densos de alta dimensión.</target>
        </trans-unit>
        <trans-unit id="a2e465567e94e51f5ccdac035f7ba7d95a9eeb84" translate="yes" xml:space="preserve">
          <source>For the lbfgs solver set verbose to any positive number for verbosity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93f0b6841feed67e5fc00af0443562656921cce7" translate="yes" xml:space="preserve">
          <source>For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">Para los solucionadores liblineares y lbfgs pongan verboso a cualquier número positivo para la verbosidad.</target>
        </trans-unit>
        <trans-unit id="181a8f355e9076e45b2bb33dd2324e0459310c3b" translate="yes" xml:space="preserve">
          <source>For the linear case, the algorithm used in &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; by the &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; implementation is much more efficient than its &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;-based &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; counterpart and can scale almost linearly to millions of samples and/or features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c4a7d0fb25ecd0231acfef000eb4ebb4024b077" translate="yes" xml:space="preserve">
          <source>For the most common use cases, you can designate a scorer object with the &lt;code&gt;scoring&lt;/code&gt; parameter; the table below shows all possible values. All scorer objects follow the convention that &lt;strong&gt;higher return values are better than lower return values&lt;/strong&gt;. Thus metrics which measure the distance between the model and the data, like &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;metrics.mean_squared_error&lt;/code&gt;&lt;/a&gt;, are available as neg_mean_squared_error which return the negated value of the metric.</source>
          <target state="translated">Para los casos de uso m&amp;aacute;s comunes, puede designar un objeto de &lt;code&gt;scoring&lt;/code&gt; con el par&amp;aacute;metro de puntuaci&amp;oacute;n ; la siguiente tabla muestra todos los valores posibles. Todos los objetos de puntuaci&amp;oacute;n siguen la convenci&amp;oacute;n de que &lt;strong&gt;los valores de retorno m&amp;aacute;s altos son mejores que los valores de retorno m&amp;aacute;s bajos&lt;/strong&gt; . Por lo tanto, las m&amp;eacute;tricas que miden la distancia entre el modelo y los datos, como &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;metrics.mean_squared_error&lt;/code&gt; &lt;/a&gt; , est&amp;aacute;n disponibles como neg_mean_squared_error que devuelven el valor negado de la m&amp;eacute;trica.</target>
        </trans-unit>
        <trans-unit id="d789fec5338f15a1f2a15098b90094ce0875aa16" translate="yes" xml:space="preserve">
          <source>For the naive Bayes, both the validation score and the training score converge to a value that is quite low with increasing size of the training set. Thus, we will probably not benefit much from more training data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c5d7d9872e083b1cf44dccc9ef51ebf6d1fc473" translate="yes" xml:space="preserve">
          <source>For the rationale behind the names &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;, i.e. naive Bayes as a linear classifier, see J. Rennie et al. (2003), Tackling the poor assumptions of naive Bayes text classifiers, ICML.</source>
          <target state="translated">Para conocer el fundamento de los nombres &lt;code&gt;coef_&lt;/code&gt; e &lt;code&gt;intercept_&lt;/code&gt; , es decir, Bayes ingenuo como clasificador lineal, v&amp;eacute;ase J. Rennie et al. (2003), Abordando las malas suposiciones de los clasificadores de texto ingenuos de Bayes, ICML.</target>
        </trans-unit>
        <trans-unit id="1a1a7cda3c63aae62fc5939e46e880b718f959b0" translate="yes" xml:space="preserve">
          <source>For the remainder of this example, we remove the last element in &lt;code&gt;clfs&lt;/code&gt; and &lt;code&gt;ccp_alphas&lt;/code&gt;, because it is the trivial tree with only one node. Here we show that the number of nodes and tree depth decreases as alpha increases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08233f540a36ed45603c5cb01e2e4f593cd79c27" translate="yes" xml:space="preserve">
          <source>For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can be used:</source>
          <target state="translated">Para la simple tarea de encontrar los vecinos m&amp;aacute;s cercanos entre dos conjuntos de datos, se pueden usar los algoritmos no supervisados ​​dentro de &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="a0a1bdaba8df32e218fd25f1e35b96e97a68bb33" translate="yes" xml:space="preserve">
          <source>For this data, we might want to encode the &lt;code&gt;'city'&lt;/code&gt; column as a categorical variable using &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; but apply a &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;'title'&lt;/code&gt; column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say &lt;code&gt;'city_category'&lt;/code&gt; and &lt;code&gt;'title_bow'&lt;/code&gt;. By default, the remaining rating columns are ignored (&lt;code&gt;remainder='drop'&lt;/code&gt;):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="daed0c58d42835f25cc91f4ef37c8c2918d442fd" translate="yes" xml:space="preserve">
          <source>For this data, we might want to encode the &lt;code&gt;'city'&lt;/code&gt; column as a categorical variable, but apply a &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;'title'&lt;/code&gt; column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say &lt;code&gt;'city_category'&lt;/code&gt; and &lt;code&gt;'title_bow'&lt;/code&gt;. By default, the remaining rating columns are ignored (&lt;code&gt;remainder='drop'&lt;/code&gt;):</source>
          <target state="translated">Para estos datos, es posible que queramos codificar la columna &lt;code&gt;'city'&lt;/code&gt; como una variable categ&amp;oacute;rica, pero aplicar &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; a la columna &lt;code&gt;'title'&lt;/code&gt; . Como podr&amp;iacute;amos usar varios m&amp;eacute;todos de extracci&amp;oacute;n de caracter&amp;iacute;sticas en la misma columna, le damos a cada transformador un nombre &amp;uacute;nico, digamos &lt;code&gt;'city_category'&lt;/code&gt; y &lt;code&gt;'title_bow'&lt;/code&gt; . De forma predeterminada, las columnas de calificaci&amp;oacute;n restantes se ignoran ( &lt;code&gt;remainder='drop'&lt;/code&gt; ):</target>
        </trans-unit>
        <trans-unit id="2e47bbc09921a29cf4a007e2d92242f5a8a9f3d8" translate="yes" xml:space="preserve">
          <source>For this example we will use the &lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;yeast&lt;/a&gt; dataset which contains 2417 datapoints each with 103 features and 14 possible labels. Each data point has at least one label. As a baseline we first train a logistic regression classifier for each of the 14 labels. To evaluate the performance of these classifiers we predict on a held-out test set and calculate the &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard similarity score&lt;/a&gt;.</source>
          <target state="translated">Para este ejemplo, utilizaremos el conjunto de datos de &lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;levadura&lt;/a&gt; que contiene 2417 puntos de datos, cada uno con 103 caracter&amp;iacute;sticas y 14 etiquetas posibles. Cada punto de datos tiene al menos una etiqueta. Como l&amp;iacute;nea de base, primero entrenamos un clasificador de regresi&amp;oacute;n log&amp;iacute;stica para cada una de las 14 etiquetas. Para evaluar el rendimiento de estos clasificadores, predecimos en un conjunto de pruebas retenido y calculamos la &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;puntuaci&amp;oacute;n de similitud de jaccard&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="ccd3bf49df3c855961b0d8c881b499df98dfa5eb" translate="yes" xml:space="preserve">
          <source>For this example we will use the &lt;a href=&quot;https://www.openml.org/d/40597&quot;&gt;yeast&lt;/a&gt; dataset which contains 2417 datapoints each with 103 features and 14 possible labels. Each data point has at least one label. As a baseline we first train a logistic regression classifier for each of the 14 labels. To evaluate the performance of these classifiers we predict on a held-out test set and calculate the &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard score&lt;/a&gt; for each sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50c8fe3ea12a1b37824eecf96be6564acb86b697" translate="yes" xml:space="preserve">
          <source>For this example, the impurity-based and permutation methods identify the same 2 strongly predictive features but not in the same order. The third most predictive feature, &amp;ldquo;bp&amp;rdquo;, is also the same for the 2 methods. The remaining features are less predictive and the error bars of the permutation plot show that they overlap with 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a29265ef3f92733677c4dbea10c38e7e64e8fed9" translate="yes" xml:space="preserve">
          <source>For this example, we load a blood transfusion service center data set from &lt;code&gt;OpenML &amp;lt;https://www.openml.org/d/1464&amp;gt;&lt;/code&gt;. This is a binary classification problem where the target is whether an individual donated blood. Then the data is split into a train and test dataset and a logistic regression is fitted wtih the train dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ab77ac3ad8536d0d4bc113a409f055607cd6e01" translate="yes" xml:space="preserve">
          <source>For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems. It is best to try several random seeds in order to check results.</source>
          <target state="translated">Para este método,M puede ser una matriz densa,una matriz escasa o un operador lineal general.Advertencia:ARPACK puede ser inestable para algunos problemas.Es mejor probar varias semillas al azar para comprobar los resultados.</target>
        </trans-unit>
        <trans-unit id="08c8de94919880222510697f43131d88196a6fc1" translate="yes" xml:space="preserve">
          <source>For this particular pattern of missing values we see that &lt;a href=&quot;../../modules/generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt;&lt;code&gt;sklearn.ensemble.ExtraTreesRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt;&lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt;&lt;/a&gt; give the best results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="201d282b655d8d026b78eb9f9255553e50678277" translate="yes" xml:space="preserve">
          <source>For this purpose, the estimators use a &amp;lsquo;connectivity&amp;rsquo; matrix, giving which samples are connected.</source>
          <target state="translated">Para este prop&amp;oacute;sito, los estimadores utilizan una matriz de 'conectividad', dando qu&amp;eacute; muestras est&amp;aacute;n conectadas.</target>
        </trans-unit>
        <trans-unit id="764ea2ccb7951b3db73f825ee916559c0e4bce1d" translate="yes" xml:space="preserve">
          <source>For this reason, the functions that load 20 Newsgroups data provide a parameter called &lt;strong&gt;remove&lt;/strong&gt;, telling it what kinds of information to strip out of each file. &lt;strong&gt;remove&lt;/strong&gt; should be a tuple containing any subset of &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt;, telling it to remove headers, signature blocks, and quotation blocks respectively.</source>
          <target state="translated">Por esta raz&amp;oacute;n, las funciones que cargan datos de 20 grupos de noticias proporcionan un par&amp;aacute;metro llamado &lt;strong&gt;eliminar&lt;/strong&gt; , que le indica qu&amp;eacute; tipo de informaci&amp;oacute;n eliminar de cada archivo. &lt;strong&gt;remove&lt;/strong&gt; debe ser una tupla que contenga cualquier subconjunto de &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt; , indic&amp;aacute;ndole que elimine encabezados, bloques de firmas y bloques de citas respectivamente.</target>
        </trans-unit>
        <trans-unit id="60d82b78a5294ae2dc0ada0318b904b11e85c403" translate="yes" xml:space="preserve">
          <source>For two clusters, SpectralClustering solves a convex relaxation of the &lt;a href=&quot;https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;normalised cuts&lt;/a&gt; problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images, where graph vertices are pixels, and weights of the edges of the similarity graph are computed using a function of a gradient of the image.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2d2e6058597b408c702846b2d537e901630ce3a" translate="yes" xml:space="preserve">
          <source>For two clusters, it solves a convex relaxation of the &lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;normalised cuts&lt;/a&gt; problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images: graph vertices are pixels, and edges of the similarity graph are a function of the gradient of the image.</source>
          <target state="translated">Para dos grupos, resuelve una relajaci&amp;oacute;n convexa del problema de &lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;cortes normalizados&lt;/a&gt; en el gr&amp;aacute;fico de similitud: cortar el gr&amp;aacute;fico en dos para que el peso de los bordes cortados sea peque&amp;ntilde;o en comparaci&amp;oacute;n con los pesos de los bordes dentro de cada grupo. Este criterio es especialmente interesante cuando se trabaja con im&amp;aacute;genes: los v&amp;eacute;rtices del gr&amp;aacute;fico son p&amp;iacute;xeles y los bordes del gr&amp;aacute;fico de similitud son una funci&amp;oacute;n del gradiente de la imagen.</target>
        </trans-unit>
        <trans-unit id="4092abbbb6ead577ab2b40e6704455f3cb4d3df5" translate="yes" xml:space="preserve">
          <source>For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt; entry on imputation.</source>
          <target state="translated">Por diversas razones, muchos conjuntos de datos del mundo real contienen valores perdidos, a menudo codificados como espacios en blanco, NaN u otros marcadores de posici&amp;oacute;n. Sin embargo, estos conjuntos de datos son incompatibles con los estimadores de scikit-learn, que asumen que todos los valores de una matriz son num&amp;eacute;ricos y que todos tienen y tienen significado. Una estrategia b&amp;aacute;sica para utilizar conjuntos de datos incompletos es descartar filas y / o columnas completas que contienen valores faltantes. Sin embargo, esto tiene el precio de perder datos que pueden ser valiosos (aunque est&amp;eacute;n incompletos). Una mejor estrategia es imputar los valores perdidos, es decir, inferirlos de la parte conocida de los datos. Consulte la entrada &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glosario de t&amp;eacute;rminos comunes y elementos de API&lt;/a&gt; sobre imputaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="e2303c3ef75d0f928a4dd5ec4517d95f82d7d02b" translate="yes" xml:space="preserve">
          <source>For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt; entry on imputation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f12e7919ffa1c3009c9eefff46504b7c0642e13" translate="yes" xml:space="preserve">
          <source>For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints.</source>
          <target state="translated">Para la visualización (que es el principal caso de uso del t-SNE),se recomienda encarecidamente utilizar el método Barnes-Hut.El método exacto de t-SNE es útil para comprobar las propiedades teóricas de la incrustación posiblemente en un espacio dimensional más alto,pero limitado a pequeños conjuntos de datos debido a las limitaciones de los cálculos.</target>
        </trans-unit>
        <trans-unit id="e7a5b4b1244321faa67509dff73df9a23d7da1b3" translate="yes" xml:space="preserve">
          <source>For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the bicluster contiguous.</source>
          <target state="translated">A efectos de visualización,dado un bicluster,las filas y columnas de la matriz de datos pueden reordenarse para que el bicluster sea contiguo.</target>
        </trans-unit>
        <trans-unit id="d62d3122e2e4eef979e7c46fd629936aec0233be" translate="yes" xml:space="preserve">
          <source>For visualization purposes, we need to lay out the different symbols on a 2D canvas. For this we use &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold learning&lt;/a&gt; techniques to retrieve 2D embedding.</source>
          <target state="translated">Para fines de visualizaci&amp;oacute;n, necesitamos dise&amp;ntilde;ar los diferentes s&amp;iacute;mbolos en un lienzo 2D. Para ello, utilizamos t&amp;eacute;cnicas de &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;aprendizaje m&amp;uacute;ltiples&lt;/a&gt; para recuperar la incrustaci&amp;oacute;n 2D.</target>
        </trans-unit>
        <trans-unit id="c502fd7960fae5affa9295a7a329adeddad6ab37" translate="yes" xml:space="preserve">
          <source>Force row-by-row generation by reducing &lt;code&gt;working_memory&lt;/code&gt;:</source>
          <target state="translated">Forzar la generaci&amp;oacute;n fila por fila reduciendo &lt;code&gt;working_memory&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="4bb98e5d778957b0dd66fa6aed87be22d170768c" translate="yes" xml:space="preserve">
          <source>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</source>
          <target state="translated">Forina,M.et al,PARVUS-Un paquete extensible para la exploración,clasificación y correlación de datos.Instituto de Análisis y Tecnologías Farmacéuticas y Alimenticias,Via Brigata Salerno,16147 Génova,Italia.</target>
        </trans-unit>
        <trans-unit id="35705e005c1f18ed14dab92df9e1435742858283" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the average precision is defined as</source>
          <target state="translated">Formalmente,dada una matriz indicadora binaria de las etiquetas de la verdad del suelo (y en la izquierda,1 derecha,1 derecha)y la puntuación asociada a cada etiqueta (en la derecha,1 derecha),la precisión media se define como</target>
        </trans-unit>
        <trans-unit id="4f395914b8fb9e643646835cc07cbc38c9742edc" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the coverage is defined as</source>
          <target state="translated">Formalmente,dada una matriz indicadora binaria de las etiquetas de la verdad del suelo (y en la izquierda,1 derecha,1 derecha)y la puntuación asociada a cada etiqueta (en la derecha,1 derecha,1 derecha),la cobertura se define como</target>
        </trans-unit>
        <trans-unit id="c175d46f254d733413b5b0ee831c9d600136a7b6" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the ranking loss is defined as</source>
          <target state="translated">Formalmente,dada una matriz indicadora binaria de las etiquetas de verdad del suelo (y en la izquierda,1 derecha,1 derecha)y la puntuación asociada a cada etiqueta (en la derecha,1 derecha),la pérdida de clasificación se define como</target>
        </trans-unit>
        <trans-unit id="c45b835372dc3641934de6a0e36f8d5df72bc091" translate="yes" xml:space="preserve">
          <source>Format specification for values in confusion matrix. If &lt;code&gt;None&lt;/code&gt;, the format specification is &amp;lsquo;d&amp;rsquo; or &amp;lsquo;.2g&amp;rsquo; whichever is shorter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47fb5045fef615598469a37da8a59110352753ff" translate="yes" xml:space="preserve">
          <source>Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.</source>
          <target state="translated">Forma una matriz de afinidad dada por la función especificada y aplica la descomposición espectral al correspondiente gráfico laplaciano.La transformación resultante viene dada por el valor de los vectores propios de cada punto de datos.</target>
        </trans-unit>
        <trans-unit id="638babaaa209a18fe959f40f19725a01af068351" translate="yes" xml:space="preserve">
          <source>Fortunately, &lt;strong&gt;most values in X will be zeros&lt;/strong&gt; since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically &lt;strong&gt;high-dimensional sparse datasets&lt;/strong&gt;. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.</source>
          <target state="translated">Afortunadamente, la &lt;strong&gt;mayor&amp;iacute;a de los valores en X ser&amp;aacute;n ceros,&lt;/strong&gt; ya que para un documento dado se usar&amp;aacute;n menos de unos pocos miles de palabras distintas. Por esta raz&amp;oacute;n, decimos que las bolsas de palabras son t&amp;iacute;picamente &lt;strong&gt;conjuntos de datos dispersos de alta dimensi&amp;oacute;n&lt;/strong&gt; . Podemos ahorrar mucha memoria almacenando solo las partes distintas de cero de los vectores de caracter&amp;iacute;sticas en la memoria.</target>
        </trans-unit>
        <trans-unit id="e399cc71dd11205217c77d4c7f1a914e719b462c" translate="yes" xml:space="preserve">
          <source>Frequency model &amp;ndash; Poisson distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="659b18cdaec75234c8e955e09af5dc004ab6498a" translate="yes" xml:space="preserve">
          <source>Frequently asked questions about the project and contributing.</source>
          <target state="translated">Preguntas frecuentes sobre el proyecto y la contribución.</target>
        </trans-unit>
        <trans-unit id="c378e5372fbcc6968de3f23916b1cc385b9617be" translate="yes" xml:space="preserve">
          <source>Friedman et al, &lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Sparse inverse covariance estimation with the graphical lasso&amp;rdquo;&lt;/a&gt;, Biostatistics 9, pp 432, 2008</source>
          <target state="translated">Friedman et al, &lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Estimaci&amp;oacute;n de covarianza inversa dispersa con el lazo gr&amp;aacute;fico&amp;rdquo;&lt;/a&gt; , Bioestad&amp;iacute;stica 9, p&amp;aacute;gs. 432, 2008</target>
        </trans-unit>
        <trans-unit id="563c92aa7d72e63e351914d407eb4e8a1bed4188" translate="yes" xml:space="preserve">
          <source>Friedman et al, &lt;a href=&quot;https://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Sparse inverse covariance estimation with the graphical lasso&amp;rdquo;&lt;/a&gt;, Biostatistics 9, pp 432, 2008</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8438e27109208985a133518d65493568dedc6924" translate="yes" xml:space="preserve">
          <source>Friedman, &amp;ldquo;Stochastic Gradient Boosting&amp;rdquo;, 1999</source>
          <target state="translated">Friedman, &quot;Impulso de gradiente estoc&amp;aacute;stico&quot;, 1999</target>
        </trans-unit>
        <trans-unit id="9fcad16d5a3614a8ac9a3dd3615a46004936d92d" translate="yes" xml:space="preserve">
          <source>Friedman, Stochastic Gradient Boosting, 1999</source>
          <target state="translated">Friedman,Stochastic Gradient Boosting,1999</target>
        </trans-unit>
        <trans-unit id="910211d4464f03643fe19d7b724011f366eafec5" translate="yes" xml:space="preserve">
          <source>Friedmann, Jerome H., 2007, &lt;a href=&quot;https://statweb.stanford.edu/~jhf/ftp/stobst.pdf&quot;&gt;&amp;ldquo;Stochastic Gradient Boosting&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d93c5ad51427861e9927c0f93ba75f21fa7b3769" translate="yes" xml:space="preserve">
          <source>Frobenius norm of the matrix difference, or beta-divergence, between the training data &lt;code&gt;X&lt;/code&gt; and the reconstructed data &lt;code&gt;WH&lt;/code&gt; from the fitted model.</source>
          <target state="translated">Norma de Frobenius de la diferencia de matriz, o divergencia beta, entre los datos de entrenamiento &lt;code&gt;X&lt;/code&gt; y los datos reconstruidos &lt;code&gt;WH&lt;/code&gt; del modelo ajustado.</target>
        </trans-unit>
        <trans-unit id="c51f06e97e3098b2be5170a19f440afe2d031cf5" translate="yes" xml:space="preserve">
          <source>From features with fewest missing values to most.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42591b8a9cd574128a8414edcfb65dcf2e26248d" translate="yes" xml:space="preserve">
          <source>From features with most missing values to fewest.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a2bd03e6f160e636919837a5a755bde731a1eeb" translate="yes" xml:space="preserve">
          <source>From images</source>
          <target state="translated">De las imágenes</target>
        </trans-unit>
        <trans-unit id="5ff0ffd1e24dbd90ba4e307313dc3fed8b0cd6c4" translate="yes" xml:space="preserve">
          <source>From occurrences to frequencies</source>
          <target state="translated">De las ocurrencias a las frecuencias</target>
        </trans-unit>
        <trans-unit id="4a6ea847ae49dd26abc66504268644d690f3206b" translate="yes" xml:space="preserve">
          <source>From scikit-learn: [&amp;lsquo;cityblock&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;, &amp;lsquo;euclidean&amp;rsquo;, &amp;lsquo;l1&amp;rsquo;, &amp;lsquo;l2&amp;rsquo;, &amp;lsquo;manhattan&amp;rsquo;]. These metrics support sparse matrix inputs.</source>
          <target state="translated">De scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']. Estas m&amp;eacute;tricas admiten entradas de matriz dispersas.</target>
        </trans-unit>
        <trans-unit id="e036581a0079e3a00bbe6bddf83817f4c4f30e31" translate="yes" xml:space="preserve">
          <source>From scikit-learn: [&amp;lsquo;cityblock&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;, &amp;lsquo;euclidean&amp;rsquo;, &amp;lsquo;l1&amp;rsquo;, &amp;lsquo;l2&amp;rsquo;, &amp;lsquo;manhattan&amp;rsquo;]. These metrics support sparse matrix inputs. [&amp;lsquo;nan_euclidean&amp;rsquo;] but it does not yet support sparse matrices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d8d962b98fbbe50de709ee2f1e71db53d579c2e" translate="yes" xml:space="preserve">
          <source>From scipy.spatial.distance: [&amp;lsquo;braycurtis&amp;rsquo;, &amp;lsquo;canberra&amp;rsquo;, &amp;lsquo;chebyshev&amp;rsquo;, &amp;lsquo;correlation&amp;rsquo;, &amp;lsquo;dice&amp;rsquo;, &amp;lsquo;hamming&amp;rsquo;, &amp;lsquo;jaccard&amp;rsquo;, &amp;lsquo;kulsinski&amp;rsquo;, &amp;lsquo;mahalanobis&amp;rsquo;, &amp;lsquo;minkowski&amp;rsquo;, &amp;lsquo;rogerstanimoto&amp;rsquo;, &amp;lsquo;russellrao&amp;rsquo;, &amp;lsquo;seuclidean&amp;rsquo;, &amp;lsquo;sokalmichener&amp;rsquo;, &amp;lsquo;sokalsneath&amp;rsquo;, &amp;lsquo;sqeuclidean&amp;rsquo;, &amp;lsquo;yule&amp;rsquo;] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.</source>
          <target state="translated">De scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'minkowski', 'rogerstanimoto ',' russellrao ',' seuclidean ',' sokalmichener ',' sokalsneath ',' sqeuclidean ',' yule '] Consulte la documentaci&amp;oacute;n de scipy.spatial.distance para obtener detalles sobre estas m&amp;eacute;tricas. Estas m&amp;eacute;tricas no admiten entradas matriciales dispersas.</target>
        </trans-unit>
        <trans-unit id="d3990f36d057d6745fedc272447b2563e02193f7" translate="yes" xml:space="preserve">
          <source>From text</source>
          <target state="translated">Del texto</target>
        </trans-unit>
        <trans-unit id="b8b69c633940e44df1e4f7cf5b91c3ed0ce89b65" translate="yes" xml:space="preserve">
          <source>From the Wikipedia page for Discounted Cumulative Gain:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eea6e746f349894e725f780b7f19777ab0b59905" translate="yes" xml:space="preserve">
          <source>From the above formula, it is clear that LDA has a linear decision surface. In the case of QDA, there are no assumptions on the covariance matrices \(\Sigma_k\) of the Gaussians, leading to quadratic decision surfaces. See &lt;a href=&quot;#id5&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13bce2493b501286a428cebcb4e0bc57e6083c63" translate="yes" xml:space="preserve">
          <source>From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.</source>
          <target state="translated">Desde el punto de vista de la implementación,esto es simplemente cuadrados mínimos ordinarios (scipy.linalg.lstsq)envueltos como un objeto predictor.</target>
        </trans-unit>
        <trans-unit id="704f1c6d00ec317ee90c0b6672883e3dd205ae68" translate="yes" xml:space="preserve">
          <source>From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly 50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the &lt;a href=&quot;../../modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method of the online KMeans object, MiniBatchKMeans.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae8e3bdf9c1967ed71af43f356a6c2d5d1712708" translate="yes" xml:space="preserve">
          <source>From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly 50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the &lt;code&gt;partial_fit&lt;/code&gt; method of the online KMeans object, MiniBatchKMeans.</source>
          <target state="translated">Desde el punto de vista de la programaci&amp;oacute;n, es interesante porque muestra c&amp;oacute;mo utilizar la API en l&amp;iacute;nea de scikit-learn para procesar un conjunto de datos muy grande por fragmentos. La forma en que procedemos es que cargamos una imagen a la vez y extraemos al azar 50 parches de esta imagen. Una vez que hemos acumulado 500 de estos parches (usando 10 im&amp;aacute;genes), se corre el &lt;code&gt;partial_fit&lt;/code&gt; m&amp;eacute;todo del objeto KMeans en l&amp;iacute;nea, MiniBatchKMeans.</target>
        </trans-unit>
        <trans-unit id="f1e410ad1472b42cb42cc98962428637290b6706" translate="yes" xml:space="preserve">
          <source>Function</source>
          <target state="translated">Function</target>
        </trans-unit>
        <trans-unit id="9f410a9e5384dfe1720c4cd228fe7bb63965656b" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">Funci&amp;oacute;n que toma dos matrices X e y, y devuelve un par de matrices (puntuaciones, valores p) o una &amp;uacute;nica matriz con puntuaciones. El valor predeterminado es f_classif (ver m&amp;aacute;s abajo &quot;Ver tambi&amp;eacute;n&quot;). La funci&amp;oacute;n predeterminada solo funciona con tareas de clasificaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="a8696032e0adf35ffec7c9da28cd036adeb91c99" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">Funci&amp;oacute;n que toma dos matrices X e y, y devuelve un par de matrices (puntuaciones, pvalores). El valor predeterminado es f_classif (ver m&amp;aacute;s abajo &quot;Ver tambi&amp;eacute;n&quot;). La funci&amp;oacute;n predeterminada solo funciona con tareas de clasificaci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="acf1f055cd0885a9fc7d245efda7d1c727fca691" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes &amp;lsquo;percentile&amp;rsquo; or &amp;lsquo;kbest&amp;rsquo; it can return a single array scores.</source>
          <target state="translated">Funci&amp;oacute;n que toma dos matrices X e y, y devuelve un par de matrices (puntuaciones, pvalores). Para los modos 'percentil' o 'kbest', puede devolver una &amp;uacute;nica puntuaci&amp;oacute;n de matriz.</target>
        </trans-unit>
        <trans-unit id="0c64f21c81859fb42c302c0d2cd301e40332c2c7" translate="yes" xml:space="preserve">
          <source>Function to apply to &lt;code&gt;y&lt;/code&gt; before passing to &lt;code&gt;fit&lt;/code&gt;. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt;. The function needs to return a 2-dimensional array. If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, the function used will be the identity function.</source>
          <target state="translated">Funci&amp;oacute;n para aplicar &lt;code&gt;y&lt;/code&gt; antes de pasar a &lt;code&gt;fit&lt;/code&gt; . No se puede configurar al mismo tiempo que el &lt;code&gt;transformer&lt;/code&gt; . La funci&amp;oacute;n debe devolver una matriz bidimensional. Si &lt;code&gt;func&lt;/code&gt; es &lt;code&gt;None&lt;/code&gt; , la funci&amp;oacute;n utilizada ser&amp;aacute; la funci&amp;oacute;n de identidad.</target>
        </trans-unit>
        <trans-unit id="f712e33ad68950dd5132b77ad3129994bf2cbbce" translate="yes" xml:space="preserve">
          <source>Function to apply to the prediction of the regressor. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt; as well. The function needs to return a 2-dimensional array. The inverse function is used to return predictions to the same space of the original training labels.</source>
          <target state="translated">Funci&amp;oacute;n a aplicar a la predicci&amp;oacute;n del regresor. No se puede configurar al mismo tiempo que el &lt;code&gt;transformer&lt;/code&gt; . La funci&amp;oacute;n debe devolver una matriz bidimensional. La funci&amp;oacute;n inversa se usa para devolver predicciones al mismo espacio de las etiquetas de entrenamiento originales.</target>
        </trans-unit>
        <trans-unit id="2b961dea1dc0c60ddf9a2c8e9d090f6f7d082483" translate="yes" xml:space="preserve">
          <source>Functions</source>
          <target state="translated">Functions</target>
        </trans-unit>
        <trans-unit id="c216053588b385d3de175b467017426b8b421912" translate="yes" xml:space="preserve">
          <source>Further discussion on the importance of centering and scaling data is available on this FAQ: &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;Should I normalize/standardize/rescale the data?&lt;/a&gt;</source>
          <target state="translated">M&amp;aacute;s informaci&amp;oacute;n sobre la importancia de centrar y escalar los datos est&amp;aacute; disponible en esta pregunta frecuente: &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;&amp;iquest;Deber&amp;iacute;a normalizar / estandarizar / cambiar la escala de los datos?&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="606b0f774f1d5e4151969dbb768df62ebca8a20e" translate="yes" xml:space="preserve">
          <source>Further removes the linear correlation across features with &amp;lsquo;whiten=True&amp;rsquo;.</source>
          <target state="translated">Elimina a&amp;uacute;n m&amp;aacute;s la correlaci&amp;oacute;n lineal entre caracter&amp;iacute;sticas con 'blanquear = Verdadero'.</target>
        </trans-unit>
        <trans-unit id="ec9ba56eabfa3f70786eb84612f0623df80dfc4d" translate="yes" xml:space="preserve">
          <source>Further, the model supports &lt;a href=&quot;multiclass#multiclass&quot;&gt;multi-label classification&lt;/a&gt; in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to &lt;code&gt;0.5&lt;/code&gt; are rounded to &lt;code&gt;1&lt;/code&gt;, otherwise to &lt;code&gt;0&lt;/code&gt;. For a predicted output of a sample, the indices where the value is &lt;code&gt;1&lt;/code&gt; represents the assigned classes of that sample:</source>
          <target state="translated">Adem&amp;aacute;s, el modelo admite la &lt;a href=&quot;multiclass#multiclass&quot;&gt;clasificaci&amp;oacute;n de m&amp;uacute;ltiples etiquetas&lt;/a&gt; en la que una muestra puede pertenecer a m&amp;aacute;s de una clase. Para cada clase, la salida sin procesar pasa por la funci&amp;oacute;n log&amp;iacute;stica. Los valores mayores o iguales a &lt;code&gt;0.5&lt;/code&gt; se redondean a &lt;code&gt;1&lt;/code&gt; , de lo contrario a &lt;code&gt;0&lt;/code&gt; . Para una salida prevista de una muestra, los &amp;iacute;ndices donde el valor es &lt;code&gt;1&lt;/code&gt; representan las clases asignadas de esa muestra:</target>
        </trans-unit>
        <trans-unit id="cc118108875cca01a2724ce6e20debf4e124a846" translate="yes" xml:space="preserve">
          <source>Furthermore, &lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt;&lt;code&gt;adjusted_rand_score&lt;/code&gt;&lt;/a&gt; is &lt;strong&gt;symmetric&lt;/strong&gt;: swapping the argument does not change the score. It can thus be used as a &lt;strong&gt;consensus measure&lt;/strong&gt;:</source>
          <target state="translated">Adem&amp;aacute;s, la puntuaci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt; &lt;code&gt;adjusted_rand_score&lt;/code&gt; &lt;/a&gt; la puntuaci&amp;oacute;n es &lt;strong&gt;sim&amp;eacute;trica&lt;/strong&gt; : cambiar el argumento no cambia la puntuaci&amp;oacute;n. Por tanto, puede utilizarse como &lt;strong&gt;medida de consenso&lt;/strong&gt; :</target>
        </trans-unit>
        <trans-unit id="f148ccef557451e9c13ec83bfface59d01588547" translate="yes" xml:space="preserve">
          <source>Furthermore, impurity-based feature importance for trees are &lt;strong&gt;strongly biased&lt;/strong&gt; and &lt;strong&gt;favor high cardinality features&lt;/strong&gt; (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7218de362b7befd5a71b1a5a01365e3552aa1087" translate="yes" xml:space="preserve">
          <source>Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples.</source>
          <target state="translated">Además,también muestra la evolución del rendimiento de los diferentes algoritmos con el número de ejemplos procesados.</target>
        </trans-unit>
        <trans-unit id="17753e7322d4f150d032ddf1f2dbdf4fe6d38592" translate="yes" xml:space="preserve">
          <source>Furthermore, the default parameter &lt;code&gt;smooth_idf=True&lt;/code&gt; adds &amp;ldquo;1&amp;rdquo; to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:</source>
          <target state="translated">Adem&amp;aacute;s, el par&amp;aacute;metro predeterminado &lt;code&gt;smooth_idf=True&lt;/code&gt; agrega &quot;1&quot; al numerador y al denominador como si se viera un documento adicional que contiene todos los t&amp;eacute;rminos de la colecci&amp;oacute;n exactamente una vez, lo que evita las divisiones cero:</target>
        </trans-unit>
        <trans-unit id="2e93583dd7fd8dcf1f0371a9818f0db1fd3c80a7" translate="yes" xml:space="preserve">
          <source>Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR as follows:</source>
          <target state="translated">Además,las fórmulas utilizadas para calcular tf y idf dependen de los ajustes de los parámetros que corresponden a la notación SMART utilizada en el IR de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="f576c03970023ff0ede275b819158dc78e4bd414" translate="yes" xml:space="preserve">
          <source>Furthermore, the impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset: the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a28e3ed3e4426c3b74ba7f5c5c797d3018edc64c" translate="yes" xml:space="preserve">
          <source>Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size &lt;code&gt;max_features&lt;/code&gt;. (See the &lt;a href=&quot;#random-forest-parameters&quot;&gt;parameter tuning guidelines&lt;/a&gt; for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b6ca190d547b1e777d8fa3e93274ce6ad7c42b4" translate="yes" xml:space="preserve">
          <source>G. Brier, &lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;Verification of forecasts expressed in terms of probability&lt;/a&gt;, Monthly weather review 78.1 (1950)</source>
          <target state="translated">G. Brier, &lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;Verificaci&amp;oacute;n de pron&amp;oacute;sticos expresados ​​en t&amp;eacute;rminos de probabilidad&lt;/a&gt; , Revisi&amp;oacute;n meteorol&amp;oacute;gica mensual 78.1 (1950)</target>
        </trans-unit>
        <trans-unit id="8ccf25498da17f5ff69133909511a6d98d2976f3" translate="yes" xml:space="preserve">
          <source>G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert, &amp;ldquo;Regularization in regression: comparing Bayesian and frequentist methods in a poorly informative situation&amp;rdquo;, 2009.</source>
          <target state="translated">G. Celeux, M. El Anbari, J.-M. Marin, CP Robert, &amp;ldquo;Regularizaci&amp;oacute;n en regresi&amp;oacute;n: comparaci&amp;oacute;n de m&amp;eacute;todos bayesianos y frecuentistas en una situaci&amp;oacute;n poco informativa&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="623c67fa2cbbcf37d713401c722eceec0b680645" translate="yes" xml:space="preserve">
          <source>G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5, Section 5.4.4, pp. 252-253.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="755f0c9208b383f3b380dd0d2b1a156d6d5865c4" translate="yes" xml:space="preserve">
          <source>G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, Springer 2013.</source>
          <target state="translated">G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;Introducci&amp;oacute;n al aprendizaje estad&amp;iacute;stico&lt;/a&gt; , Springer 2013.</target>
        </trans-unit>
        <trans-unit id="3cb2eea23f004b3e761d528ddc6b2e3e79458d85" translate="yes" xml:space="preserve">
          <source>G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;https://www-bcf.usc.edu/~gareth/ISL/&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, Springer 2013.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28ef1689ee2219c624cfde5d7c88afdcae0138ec" translate="yes" xml:space="preserve">
          <source>G. Louppe and P. Geurts, &amp;ldquo;Ensembles on Random Patches&amp;rdquo;, Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</source>
          <target state="translated">G. Louppe y P. Geurts, &quot;Ensambles on Random Patches&quot;, Aprendizaje autom&amp;aacute;tico y descubrimiento de conocimientos en bases de datos, 346-361, 2012.</target>
        </trans-unit>
        <trans-unit id="c722e87d5d9d7dbc54dd2b811a759cc621efb047" translate="yes" xml:space="preserve">
          <source>G. Louppe, &amp;ldquo;Understanding Random Forests: From Theory to Practice&amp;rdquo;, PhD Thesis, U. of Liege, 2014.</source>
          <target state="translated">G. Louppe, &quot;Comprensi&amp;oacute;n de los bosques aleatorios: de la teor&amp;iacute;a a la pr&amp;aacute;ctica&quot;, Tesis doctoral, U. de Lieja, 2014.</target>
        </trans-unit>
        <trans-unit id="80030b72580197a6197c00418acf9f08511c2c49" translate="yes" xml:space="preserve">
          <source>G. Ridgeway, &amp;ldquo;Generalized Boosted Models: A guide to the gbm package&amp;rdquo;, 2007</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8ed6bad205ec1f52f0b48e7f8377435663ec074" translate="yes" xml:space="preserve">
          <source>G.E.P. Box and D.R. Cox, &amp;ldquo;An Analysis of Transformations&amp;rdquo;, Journal of the Royal Statistical Society B, 26, 211-252 (1964).</source>
          <target state="translated">GEP Box y DR Cox, &quot;Un an&amp;aacute;lisis de transformaciones&quot;, Revista de la Royal Statistical Society B, 26, 211-252 (1964).</target>
        </trans-unit>
        <trans-unit id="83c6052410f7be2971558c8f2b162b661b4a734b" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion. Regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8cb867b444fe174ab482df0a111ed147a9ceddf" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage &lt;code&gt;n_classes_&lt;/code&gt; regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</source>
          <target state="translated">GB construye un modelo aditivo de manera progresiva por etapas; permite la optimizaci&amp;oacute;n de funciones de p&amp;eacute;rdida diferenciables arbitrarias. En cada etapa, &lt;code&gt;n_classes_&lt;/code&gt; &amp;aacute;rboles de regresi&amp;oacute;n se ajustan al gradiente negativo de la funci&amp;oacute;n de p&amp;eacute;rdida de desviaci&amp;oacute;n binomial o multinomial. La clasificaci&amp;oacute;n binaria es un caso especial en el que solo se induce un &amp;uacute;nico &amp;aacute;rbol de regresi&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="80f39c4fc4a6461ea00d5d7be636d9c6f77055de" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.</source>
          <target state="translated">GB construye un modelo aditivo de forma progresiva;permite la optimización de funciones de pérdida diferenciables arbitrarias.En cada etapa se ajusta un árbol de regresión en el gradiente negativo de la función de pérdida dada.</target>
        </trans-unit>
        <trans-unit id="2c1af0078ebec6d87c6fe14b52a6ca7ecb93e0e6" translate="yes" xml:space="preserve">
          <source>GBRT considers additive models of the following form:</source>
          <target state="translated">La GBRT considera modelos aditivos de la siguiente forma:</target>
        </trans-unit>
        <trans-unit id="af90cc1188550654bd22990a09c9155ebaa04680" translate="yes" xml:space="preserve">
          <source>GBRT regressors are additive models whose prediction \(y_i\) for a given input \(x_i\) is of the following form:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="348ddf733ebe39c89fe60cc4aea0def489f0df0c" translate="yes" xml:space="preserve">
          <source>GMM covariances</source>
          <target state="translated">Covarianzas del GMM</target>
        </trans-unit>
        <trans-unit id="89a541e422be32f4e38c95b70a35778f6b3b29a5" translate="yes" xml:space="preserve">
          <source>G[i,j] gives the shortest distance from point i to point j along the graph.</source>
          <target state="translated">G[i,j]da la distancia más corta del punto i al punto j a lo largo del gráfico.</target>
        </trans-unit>
        <trans-unit id="dc7da4ca9757d9015c0ba1d2228560006792966e" translate="yes" xml:space="preserve">
          <source>Gallery generated by Sphinx-Gallery</source>
          <target state="translated">Galería generada por Sphinx-Gallery</target>
        </trans-unit>
        <trans-unit id="cba508b12182b68f501d6af46c4f03f8fc5d2473" translate="yes" xml:space="preserve">
          <source>Gamma</source>
          <target state="translated">Gamma</target>
        </trans-unit>
        <trans-unit id="927ef8e7f274c04e9c8836a36352812cc37bb26c" translate="yes" xml:space="preserve">
          <source>Gamma deviance is equivalent to the Tweedie deviance with the power parameter &lt;code&gt;power=2&lt;/code&gt;. It is invariant to scaling of the target variable, and measures relative errors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24f0f86d8b8da4a3eb66c5315b49fb7db14a0fa6" translate="yes" xml:space="preserve">
          <source>Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.</source>
          <target state="translated">Parámetro gamma para los núcleos RBF,laplaciano,polinomio,chi2 exponencial y sigmoide.La interpretación del valor por defecto se deja en manos del núcleo;véase la documentación de sklearn.metrics.pairwise.Ignorado por otros núcleos.</target>
        </trans-unit>
        <trans-unit id="8abb933fe9bd6d8a92eb104bdc2fd613c351d44f" translate="yes" xml:space="preserve">
          <source>Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other kernels. 0.1 by default.</source>
          <target state="translated">Parámetro gamma en los núcleos rbf,poly y sigmoidales.Ignorado por otros núcleos.0.1 por defecto.</target>
        </trans-unit>
        <trans-unit id="86050f4573c138fca290821e4b579d6d320e40d1" translate="yes" xml:space="preserve">
          <source>Gates, G.W. (1972) &amp;ldquo;The Reduced Nearest Neighbor Rule&amp;rdquo;. IEEE Transactions on Information Theory, May 1972, 431-433.</source>
          <target state="translated">Gates, GW (1972) &quot;La regla del vecino m&amp;aacute;s cercano reducido&quot;. IEEE Transactions on Information Theory, mayo de 1972, 431-433.</target>
        </trans-unit>
        <trans-unit id="46a57bcdd34ea523f3417e94b431a41097b638e9" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Ellipsoids</source>
          <target state="translated">Modelo de mezcla gaussiana de elipsoides</target>
        </trans-unit>
        <trans-unit id="2f22bd1dad8340bd3d8973db40a56083b791482c" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Selection</source>
          <target state="translated">Selección del modelo de mezcla gaussiana</target>
        </trans-unit>
        <trans-unit id="7ada59d703243073c5122ce20c200108df4cf582" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Sine Curve</source>
          <target state="translated">Curva sinusoidal modelo de mezcla gaussiana</target>
        </trans-unit>
        <trans-unit id="b8fb995e81cb89650fea0baec9d6ae8f98a9538f" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Models</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="662a25df4ddd527b4e6e6b4415fd19857fcb55fc" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture.</source>
          <target state="translated">Mezcla Gaussiana.</target>
        </trans-unit>
        <trans-unit id="52d32c3ce740bd6bf6fa9b8c9a00c471e2b8ab61" translate="yes" xml:space="preserve">
          <source>Gaussian Naive Bayes (GaussianNB)</source>
          <target state="translated">Gaussian Naive Bayes (GaussianNB)</target>
        </trans-unit>
        <trans-unit id="d854cefb413c2902ad18940be1c741ae3117e7e6" translate="yes" xml:space="preserve">
          <source>Gaussian Process for Machine Learning</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e71cc209c706f89187660af28df9dbd656b7dfb" translate="yes" xml:space="preserve">
          <source>Gaussian Processes regression: basic introductory example</source>
          <target state="translated">Regresión de los Procesos Gaussianos:ejemplo básico de introducción</target>
        </trans-unit>
        <trans-unit id="7c9060d2e2a8ab44211d4b8690374c1230f1b7f2" translate="yes" xml:space="preserve">
          <source>Gaussian kernel (&lt;code&gt;kernel = 'gaussian'&lt;/code&gt;)</source>
          <target state="translated">Kernel gaussiano ( &lt;code&gt;kernel = 'gaussian'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="16bd9bbb5a5342036acd14278f2e03ad41c57f6a" translate="yes" xml:space="preserve">
          <source>Gaussian mixture model fit with a variational inference.</source>
          <target state="translated">El modelo de mezcla gaussiano encaja con una inferencia variacional.</target>
        </trans-unit>
        <trans-unit id="c4278ff51902cddfc2c28028add69085822b616d" translate="yes" xml:space="preserve">
          <source>Gaussian mixture models, useful for clustering, are described in &lt;a href=&quot;mixture#mixture&quot;&gt;another chapter of the documentation&lt;/a&gt; dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component.</source>
          <target state="translated">Los modelos de mezcla gaussianos, &amp;uacute;tiles para la agrupaci&amp;oacute;n, se describen en &lt;a href=&quot;mixture#mixture&quot;&gt;otro cap&amp;iacute;tulo de la documentaci&amp;oacute;n&lt;/a&gt; dedicado a los modelos de mezcla. KMeans puede verse como un caso especial de modelo de mezcla gaussiana con igual covarianza por componente.</target>
        </trans-unit>
        <trans-unit id="52102b8851b98924c7d8b1f347902fc1a6a2f6c4" translate="yes" xml:space="preserve">
          <source>Gaussian mixtures</source>
          <target state="translated">Mezclas gaussianas</target>
        </trans-unit>
        <trans-unit id="fb2ed046d4b5b73ab490df316744dbd7803b27c6" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) based on Laplace approximation.</source>
          <target state="translated">Clasificación del proceso Gaussiano (GPC)basada en la aproximación de Laplace.</target>
        </trans-unit>
        <trans-unit id="6022eb0f0e245ca9c1dcd7d4b4311ff01e4db354" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) on iris dataset</source>
          <target state="translated">Clasificación del proceso Gaussiano (GPC)en el conjunto de datos del iris</target>
        </trans-unit>
        <trans-unit id="21a63bbdb2d774ad21ffa6c87b635dc00ddbdcbd" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) on Mauna Loa CO2 data.</source>
          <target state="translated">Regresión del proceso Gaussiano (GPR)en los datos de CO2 de Mauna Loa.</target>
        </trans-unit>
        <trans-unit id="0c7b8e025d47923893c509b893c584646dec60f9" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) with noise-level estimation</source>
          <target state="translated">Regresión del proceso Gaussiano (GPR)con estimación del nivel de ruido</target>
        </trans-unit>
        <trans-unit id="e020234a1ce464bccd79fd7ca6cd9571320c3263" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR).</source>
          <target state="translated">Regresión del proceso Gaussiano (GPR).</target>
        </trans-unit>
        <trans-unit id="81d5ab12411c6f249a3ae9ff3884b17e3d00399b" translate="yes" xml:space="preserve">
          <source>Gaussian processes on discrete data structures</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3eef2758f8f04922436ba69e73f365c3b677d080" translate="yes" xml:space="preserve">
          <source>GaussianNaiveBayes tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features.</source>
          <target state="translated">GaussianNaiveBayes tiende a empujar las probabilidades a 0 o 1 (note los recuentos en los histogramas).Esto se debe principalmente a que asume que las características son condicionalmente independientes dada la clase,lo que no es el caso en este conjunto de datos que contiene 2 características redundantes.</target>
        </trans-unit>
        <trans-unit id="9ee50bfb8852bcfbfa07c7c7a246c842043563a2" translate="yes" xml:space="preserve">
          <source>General KDD structure :</source>
          <target state="translated">Estructura general de KDD:</target>
        </trans-unit>
        <trans-unit id="082e84b7a80940ab38b8fafffced3896fbf61a5f" translate="yes" xml:space="preserve">
          <source>General examples about classification algorithms.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="340183f53d5a585fe2f90b1573169f80622dc9bd" translate="yes" xml:space="preserve">
          <source>General-purpose, even cluster size, flat geometry, not too many clusters</source>
          <target state="translated">De uso general,tamaño de cúmulo uniforme,geometría plana,no demasiados cúmulos</target>
        </trans-unit>
        <trans-unit id="5a99200d3c187d0fcefb7b4df6803366dc2748df" translate="yes" xml:space="preserve">
          <source>Generalized Linear Model with a Gamma distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ed4c66ad535ba7380d74741129413d4f8c145bc" translate="yes" xml:space="preserve">
          <source>Generalized Linear Model with a Poisson distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d682c681b1fa1783371317722a00ec63b80aa77c" translate="yes" xml:space="preserve">
          <source>Generalized Linear Model with a Tweedie distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b17d9222a12b9513aac695dd37d7bdc64c218d77" translate="yes" xml:space="preserve">
          <source>Generalized Linear Models</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bcbd479b250088e4214e337494acb2a2758516bc" translate="yes" xml:space="preserve">
          <source>Generalized Linear Models (GLM) extend linear models in two ways &lt;a href=&quot;#id33&quot; id=&quot;id31&quot;&gt;10&lt;/a&gt;. First, the predicted values \(\hat{y}\) are linked to a linear combination of the input variables \(X\) via an inverse link function \(h\) as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="050c76b497e038e0c5a06ed24dce47a6958dfb02" translate="yes" xml:space="preserve">
          <source>Generalized Linear Models, and Poisson loss for gradient boosting</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="194ee7e5ec30d094070f5f72a72c8597376dc276" translate="yes" xml:space="preserve">
          <source>Generalized linear models (GLM) for regression</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a807e718c7c2444084ecd599b5293f02618f18b0" translate="yes" xml:space="preserve">
          <source>Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models.</source>
          <target state="translated">En términos generales,cuando la complejidad del modelo aumenta,se supone que la potencia de predicción y la latencia aumentan.El aumento de la potencia de predicción suele ser interesante,pero para muchas aplicaciones sería mejor no aumentar demasiado la latencia de predicción.Ahora revisaremos esta idea para diferentes familias de modelos supervisados.</target>
        </trans-unit>
        <trans-unit id="af9887d0c879889fc0d4b97d28831fef1da0e335" translate="yes" xml:space="preserve">
          <source>Generate a distance matrix chunk by chunk with optional reduction</source>
          <target state="translated">Generar una matriz de distancia pedazo a pedazo con reducción opcional</target>
        </trans-unit>
        <trans-unit id="06c2a79c89c40ddc99e314455bfeabb348baaefc" translate="yes" xml:space="preserve">
          <source>Generate a mostly low rank matrix with bell-shaped singular values</source>
          <target state="translated">Generar una matriz de bajo rango con valores singulares en forma de campana</target>
        </trans-unit>
        <trans-unit id="c1825817fcf44112a4d64fe6f2acf131fceae396" translate="yes" xml:space="preserve">
          <source>Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</source>
          <target state="translated">Generar una nueva matriz de características que consista en todas las combinaciones polinómicas de las características con grado menor o igual al grado especificado.Por ejemplo,si una muestra de entrada es bidimensional y de la forma [a,b],los rasgos del polinomio de grado 2 son [1,a,b,a^2,ab,b^2].</target>
        </trans-unit>
        <trans-unit id="138afdc51f7a90d9b74b5dc5c84735ab7ad5ab97" translate="yes" xml:space="preserve">
          <source>Generate a random multilabel classification problem.</source>
          <target state="translated">Generar un problema de clasificación aleatoria de múltiples etiquetas.</target>
        </trans-unit>
        <trans-unit id="6e53d56707f7eb93fc64a285e9e5b0c1571546a7" translate="yes" xml:space="preserve">
          <source>Generate a random n-class classification problem.</source>
          <target state="translated">Generar un problema de clasificación aleatoria de n clases.</target>
        </trans-unit>
        <trans-unit id="45b70aa4bfe7b5254dd4845949fd163391dae828" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem with sparse uncorrelated design</source>
          <target state="translated">Generar un problema de regresión aleatoria con un diseño disperso no correlacionado</target>
        </trans-unit>
        <trans-unit id="097811da2f026de1c67525043ab17d6d057450a6" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem.</source>
          <target state="translated">Generar un problema de regresión aleatoria.</target>
        </trans-unit>
        <trans-unit id="90aba5bbbbad8863550c06ced91ee520b1c0caff" translate="yes" xml:space="preserve">
          <source>Generate a random symmetric, positive-definite matrix.</source>
          <target state="translated">Generar una matriz simétrica aleatoria,positiva-definida.</target>
        </trans-unit>
        <trans-unit id="b303920886f4c442ac72ea67b8bd3cb1b7460430" translate="yes" xml:space="preserve">
          <source>Generate a signal as a sparse combination of dictionary elements.</source>
          <target state="translated">Generar una señal como una combinación dispersa de elementos de diccionario.</target>
        </trans-unit>
        <trans-unit id="035b22a208f9d34d7467f70a3f8e5a4c27edb9b2" translate="yes" xml:space="preserve">
          <source>Generate a sparse random projection matrix</source>
          <target state="translated">Generar una matriz de proyección aleatoria dispersa</target>
        </trans-unit>
        <trans-unit id="4dc557ac054fd2b6925cea078345560226a5469c" translate="yes" xml:space="preserve">
          <source>Generate a sparse symmetric definite positive matrix.</source>
          <target state="translated">Generar una matriz positiva definida simétrica y dispersa.</target>
        </trans-unit>
        <trans-unit id="2b6ed08a20bd86f602cf70906530ae751a13aa6a" translate="yes" xml:space="preserve">
          <source>Generate a swiss roll dataset.</source>
          <target state="translated">Generar un conjunto de datos de rollos suizos.</target>
        </trans-unit>
        <trans-unit id="2f7e815b3b193bc1cd3e7e4a28307316625909c7" translate="yes" xml:space="preserve">
          <source>Generate an S curve dataset.</source>
          <target state="translated">Generar un conjunto de datos de la curva S.</target>
        </trans-unit>
        <trans-unit id="a97cf86ca659bda28267893fc11990f8622b62e7" translate="yes" xml:space="preserve">
          <source>Generate an array with block checkerboard structure for biclustering.</source>
          <target state="translated">Generar una matriz con estructura de tablero de bloques para el biclustering.</target>
        </trans-unit>
        <trans-unit id="a9f13a8783d09446e6122b3e3234e1d6fcb95591" translate="yes" xml:space="preserve">
          <source>Generate an array with constant block diagonal structure for biclustering.</source>
          <target state="translated">Generar una matriz con una estructura diagonal de bloque constante para el biclustering.</target>
        </trans-unit>
        <trans-unit id="afeaee3f091598162e7eb33b08779a77e0e748f4" translate="yes" xml:space="preserve">
          <source>Generate cross-validated estimates for each input data point</source>
          <target state="translated">Generar estimaciones validadas cruzadas para cada punto de datos de entrada</target>
        </trans-unit>
        <trans-unit id="99b9ba538a40d50737f63d924a3c7ce27d75993f" translate="yes" xml:space="preserve">
          <source>Generate datasets. We choose the size big enough to see the scalability of the algorithms, but not too big to avoid too long running times</source>
          <target state="translated">Generar conjuntos de datos.Elegimos el tamaño lo suficientemente grande para ver la escalabilidad de los algoritmos,pero no demasiado grande para evitar tiempos de ejecución demasiado largos</target>
        </trans-unit>
        <trans-unit id="c00dd920cc2725de42546dcb337634c4ac897029" translate="yes" xml:space="preserve">
          <source>Generate indices to split data into training and test set.</source>
          <target state="translated">Generar índices para dividir los datos en el entrenamiento y el conjunto de pruebas.</target>
        </trans-unit>
        <trans-unit id="5107cc8a6ff57cac684ccce1f62420eaa4260507" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian and label samples by quantile</source>
          <target state="translated">Generar Gauss isotrópico y etiquetar las muestras por cuantiles</target>
        </trans-unit>
        <trans-unit id="8e89de3bc63d92fa78eda36337c27db80aab71fe" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian blobs for clustering.</source>
          <target state="translated">Generar glóbulos gausianos isotrópicos para la agrupación.</target>
        </trans-unit>
        <trans-unit id="37d03dbfefb10390fe483e5ed2d7b03c5a459fa1" translate="yes" xml:space="preserve">
          <source>Generate missing values indicator for X.</source>
          <target state="translated">Generar el indicador de valores perdidos para X.</target>
        </trans-unit>
        <trans-unit id="462cab2784077aa54955d18bb40a9de12e6edf3c" translate="yes" xml:space="preserve">
          <source>Generate polynomial and interaction features.</source>
          <target state="translated">Generar características polinómicas y de interacción.</target>
        </trans-unit>
        <trans-unit id="d1bba874447d3710a4261bda204e3775c6148149" translate="yes" xml:space="preserve">
          <source>Generate random samples from the fitted Gaussian distribution.</source>
          <target state="translated">Generar muestras aleatorias de la distribución gaussiana ajustada.</target>
        </trans-unit>
        <trans-unit id="ce67c2d91c83a1d56ab9a9ee35d822063af6506a" translate="yes" xml:space="preserve">
          <source>Generate random samples from the model.</source>
          <target state="translated">Generar muestras aleatorias del modelo.</target>
        </trans-unit>
        <trans-unit id="077b466863e9f097ef6d30c373ea7fea91f90736" translate="yes" xml:space="preserve">
          <source>Generate test sets such that all contain the same distribution of classes, or as close as possible.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7afab3e6555db4edb28194e580e8ac980040a53c" translate="yes" xml:space="preserve">
          <source>Generate test sets where the smallest and largest differ by at most one sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4defed702b6f02ff908f1cd9f8b411c35ee40dd" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #1&amp;rdquo; regression problem</source>
          <target state="translated">Genere el problema de regresi&amp;oacute;n &quot;Friedman # 1&quot;</target>
        </trans-unit>
        <trans-unit id="75088d435099809ee2a5f0ec830b6e2b26fb0500" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #2&amp;rdquo; regression problem</source>
          <target state="translated">Genere el problema de regresi&amp;oacute;n &quot;Friedman # 2&quot;</target>
        </trans-unit>
        <trans-unit id="18ca02f4b303dec3c31289cd6db22246b19d8adb" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #3&amp;rdquo; regression problem</source>
          <target state="translated">Genere el problema de regresi&amp;oacute;n &quot;Friedman # 3&quot;</target>
        </trans-unit>
        <trans-unit id="1526c84b2e9b495f9ed3216009ebf8b31d461518" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al.</source>
          <target state="translated">Genera datos para la clasificación binaria utilizada en Hastie et al.</target>
        </trans-unit>
        <trans-unit id="c2cb269fed6a06711794c0a014b9a89e92300ddb" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al. 2009, Example 10.2.</source>
          <target state="translated">Genera datos para la clasificación binaria utilizada en Hastie et al.2009,Ejemplo 10.2.</target>
        </trans-unit>
        <trans-unit id="fbfd61fc35f16aea2f376426724b313bf45b644a" translate="yes" xml:space="preserve">
          <source>Generates indices to split data into training and test set.</source>
          <target state="translated">Genera índices para dividir los datos en el entrenamiento y el conjunto de pruebas.</target>
        </trans-unit>
        <trans-unit id="9a963ad633fdf36ff4f1d429308e1f3d90a2ceea" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on predefined splits.</source>
          <target state="translated">Genera índices de tren/prueba basados en divisiones predefinidas.</target>
        </trans-unit>
        <trans-unit id="4678269441c5cad2dec162c29e80b19e70944794" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on random permutation.</source>
          <target state="translated">Genera índices de tren/prueba basados en una permutación aleatoria.</target>
        </trans-unit>
        <trans-unit id="025efadf6f18cb5d61732c8188dd311431f2fe8b" translate="yes" xml:space="preserve">
          <source>Generator on parameters sampled from given distributions.</source>
          <target state="translated">Generador sobre los parámetros muestreados de las distribuciones dadas.</target>
        </trans-unit>
        <trans-unit id="6d76c76581c79bfcc7307e6698c50d3852025179" translate="yes" xml:space="preserve">
          <source>Generator that yields (estimator, check) tuples. Returned when &lt;code&gt;generate_only=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9008c79b50b6e856f48dd8a1acb75bd481c83565" translate="yes" xml:space="preserve">
          <source>Generator to create n_packs slices going up to n.</source>
          <target state="translated">Generador para crear rebanadas de n_packs que van hasta n.</target>
        </trans-unit>
        <trans-unit id="6a34af9aa1c17133e53bdde13fa952c7bcbcf3f6" translate="yes" xml:space="preserve">
          <source>Geometry (metric used)</source>
          <target state="translated">Geometría (métrica utilizada)</target>
        </trans-unit>
        <trans-unit id="e5f048789e3e59e8993091df470af502112331aa" translate="yes" xml:space="preserve">
          <source>George W Bush</source>
          <target state="translated">George W.Bush</target>
        </trans-unit>
        <trans-unit id="b583db923d23716d80d92ca8bb6a609aa1f738a2" translate="yes" xml:space="preserve">
          <source>Gerhard Schroeder</source>
          <target state="translated">Gerhard Schroeder</target>
        </trans-unit>
        <trans-unit id="33868dad5f60b783d41cfb7c4e686fd5af82ea02" translate="yes" xml:space="preserve">
          <source>Get a list of all estimators from sklearn.</source>
          <target state="translated">Consigue una lista de todos los estimadores de Sklearn.</target>
        </trans-unit>
        <trans-unit id="c89b4f911ae16fa0b7caa09ce0c140306df6a7bd" translate="yes" xml:space="preserve">
          <source>Get a mask, or integer index, of the features selected</source>
          <target state="translated">Obtener una máscara,o índice entero,de las características seleccionadas</target>
        </trans-unit>
        <trans-unit id="72908cf84377de645c7534a22afeddeeaba91d9d" translate="yes" xml:space="preserve">
          <source>Get a scorer from string</source>
          <target state="translated">Consigue un anotador de cuerda</target>
        </trans-unit>
        <trans-unit id="892dda63b5e110479cdb36e62f1ec2fd9807071b" translate="yes" xml:space="preserve">
          <source>Get a scorer from string.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9077d712fdd7764174aa9d64af32e58e63a20fd2" translate="yes" xml:space="preserve">
          <source>Get data and node arrays.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45a250b2600ca82b0e59f392e6c981ee3cc2728d" translate="yes" xml:space="preserve">
          <source>Get feature names from all transformers.</source>
          <target state="translated">Consigue los nombres de las características de todos los transformadores.</target>
        </trans-unit>
        <trans-unit id="29212f8ab4fb514c62f70555a85d5fbb976ec617" translate="yes" xml:space="preserve">
          <source>Get number of calls.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4be0c520942fc8926cfd53e42cd4ae1d1cc70df9" translate="yes" xml:space="preserve">
          <source>Get parameters for this estimator.</source>
          <target state="translated">Obtener los parámetros para este estimador.</target>
        </trans-unit>
        <trans-unit id="fe15f50ace10fe1b8c70139542f4a1796682abb3" translate="yes" xml:space="preserve">
          <source>Get parameters of this kernel.</source>
          <target state="translated">Obtener los parámetros de este núcleo.</target>
        </trans-unit>
        <trans-unit id="1314abe875bac1db97b1a7155d7b4a8c13c230ee" translate="yes" xml:space="preserve">
          <source>Get predictions from each split of cross-validation for diagnostic purposes.</source>
          <target state="translated">Obtener predicciones de cada división de la validación cruzada con fines de diagnóstico.</target>
        </trans-unit>
        <trans-unit id="dd0a065fc935a1fd709e1a1d7d55ca6c3433dca5" translate="yes" xml:space="preserve">
          <source>Get the given distance metric from the string identifier.</source>
          <target state="translated">Obtener la métrica de distancia dada del identificador de la cadena.</target>
        </trans-unit>
        <trans-unit id="d1f7f6e0092ef00a29852a7f857762682fb899cd" translate="yes" xml:space="preserve">
          <source>Get the parameters of an estimator from the ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df2089c702273c8bc78b6842775813fe9702ad55" translate="yes" xml:space="preserve">
          <source>Get the parameters of the VotingClassifier</source>
          <target state="translated">Obtener los parámetros del VotingClassifier</target>
        </trans-unit>
        <trans-unit id="44fa9d84cdb2287aa5766955eab26611c0998b04" translate="yes" xml:space="preserve">
          <source>Get tree status.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f6030226293d5ed7b4d4b045e215d6de20db61c" translate="yes" xml:space="preserve">
          <source>Getter for the precision matrix.</source>
          <target state="translated">Trae la matriz de precisión.</target>
        </trans-unit>
        <trans-unit id="24670d1cd19283e4b5f2e1096ab423493375ec8f" translate="yes" xml:space="preserve">
          <source>Gibbs sampling from visible and hidden layers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53379a8bafa1cbd8bc5da14050f14d3817f01039" translate="yes" xml:space="preserve">
          <source>Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the &amp;lsquo;directions of covariance&amp;rsquo;, i.e. the components of each datasets that explain the most shared variance between both datasets. This is apparent on the &lt;strong&gt;scatterplot matrix&lt;/strong&gt; display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the first diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different components is weak: the point cloud is very spherical.</source>
          <target state="translated">Dados 2 conjuntos de datos bidimensionales covariantes multivariados, X e Y, PLS extrae las 'direcciones de covarianza', es decir, los componentes de cada conjunto de datos que explican la varianza m&amp;aacute;s compartida entre ambos conjuntos de datos. Esto es evidente en la pantalla de la &lt;strong&gt;matriz de gr&amp;aacute;ficos de dispersi&amp;oacute;n&lt;/strong&gt; : los componentes 1 en el conjunto de datos X y el conjunto de datos Y est&amp;aacute;n correlacionados al m&amp;aacute;ximo (los puntos se encuentran alrededor de la primera diagonal). Esto tambi&amp;eacute;n es cierto para los componentes 2 en ambos conjuntos de datos, sin embargo, la correlaci&amp;oacute;n entre conjuntos de datos para diferentes componentes es d&amp;eacute;bil: la nube de puntos es muy esf&amp;eacute;rica.</target>
        </trans-unit>
        <trans-unit id="16179644ab5a4c2a1f730ff634ab3d4d3a869791" translate="yes" xml:space="preserve">
          <source>Given a candidate centroid \(x_i\) for iteration \(t\), the candidate is updated according to the following equation:</source>
          <target state="translated">Si se le da un centroide candidato para la iteración,el candidato se actualiza de acuerdo con la siguiente ecuación:</target>
        </trans-unit>
        <trans-unit id="4368fa47ed8eb35b757e7b3d5aaf6d7ee1cd4ff6" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to a binary one-hot encoding.</source>
          <target state="translated">Dado un conjunto de datos con dos características,dejamos que el codificador encuentre los valores únicos por característica y transforme los datos en una codificación binaria de un solo disparo.</target>
        </trans-unit>
        <trans-unit id="a65060cb3a96ad97e8800308b9076a9a49180060" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding.</source>
          <target state="translated">Dado un conjunto de datos con dos características,dejamos que el codificador encuentre los valores únicos por característica y transforme los datos en una codificación ordinal.</target>
        </trans-unit>
        <trans-unit id="6cc0cdb4252ae3fe585bd759a612161dfe7c6d85" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^n\) and \(y_i \in \{0, 1\}\), a one hidden layer one hidden neuron MLP learns the function \(f(x) = W_2 g(W_1^T x + b_1) + b_2\) where \(W_1 \in \mathbf{R}^m\) and \(W_2, b_1, b_2 \in \mathbf{R}\) are model parameters. \(W_1, W_2\) represent the weights of the input layer and hidden layer, respectively; and \(b_1, b_2\) represent the bias added to the hidden layer and the output layer, respectively. \(g(\cdot) : R \rightarrow R\) is the activation function, set by default as the hyperbolic tan. It is given as,</source>
          <target state="translated">Dando un conjunto de ejemplos de entrenamiento \ ~ (x_1,y_1),(x_2,y_2),\ ~ puntos,(x_n,y_n)\ ~ donde \ ~ (x_i \ ~ en \ ~ Mathbf {R}}y \ ~ (y_i \ ~ en \ ~ 1),una capa oculta una neurona oculta MLP aprende la función \(f(x)=W_2 g(W_1^T x+b_1)+b_2\)donde \(W_1 \Nen \Nmathbf{R}^m})y \N(W_2,b_1,b_2 \Nen \Nmathbf{R})son parámetros modelo.\(W_1,W_2\)representan los pesos de la capa de entrada y la capa oculta,respectivamente;y \(b_1,b_2\)representan el sesgo añadido a la capa oculta y la capa de salida,respectivamente.\(g(\cdot):R \N-estrecha derecha R\N es la función de activación,establecida por defecto como el bronceado hiperbólico.Se da como,</target>
        </trans-unit>
        <trans-unit id="c774496cc27fa29850b7be4385a4e837807fe19c" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \mathcal{R}\) (\(y_i \in {-1, 1}\) for classification), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions for binary classification, we simply look at the sign of \(f(x)\). To find the model parameters, we minimize the regularized training error given by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99b85508f1069fad6e9945b3624fea4140b5fbae" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \{-1,1\}\), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions, we simply look at the sign of \(f(x)\). A common choice to find the model parameters is by minimizing the regularized training error given by</source>
          <target state="translated">Dando un conjunto de ejemplos de entrenamiento \ ~ (x_1,y_1),\ ~ puntos,(x_n,y_n)\ ~ donde \ ~ (x_i \ ~ en \ ~ mathbf {R}}y \ ~ (y_i \ ~ en \ ~-1,1),nuestro objetivo es aprender una función de puntuación lineal (f(x)=w^T x+b)con los parámetros del modelo (w en matemáticas)e interceptar (b en matemáticas).Para hacer predicciones,simplemente miramos el signo de la f(x)²).Una elección común para encontrar los parámetros del modelo es minimizar el error de entrenamiento regularizado dado por</target>
        </trans-unit>
        <trans-unit id="f05ffd1dc56829aeb2ce3b1aa47183d5a5a71272" translate="yes" xml:space="preserve">
          <source>Given an exception, a callable to raise the exception, and a message string, tests that the correct exception is raised and that the message is a substring of the error thrown. Used to test that the specific message thrown during an exception is correct.</source>
          <target state="translated">Dada una excepción,una llamada para elevar la excepción,y una cadena de mensajes,prueba que la excepción correcta es elevada y que el mensaje es una subcadena del error lanzado.Se usa para probar que el mensaje específico lanzado durante una excepción es correcto.</target>
        </trans-unit>
        <trans-unit id="d5588778e54082615cf481fafbc7dcf0b337d76d" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (&lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt;&lt;code&gt;RFE&lt;/code&gt;&lt;/a&gt;) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">Dado un estimador externo que asigna ponderaciones a las caracter&amp;iacute;sticas (por ejemplo, los coeficientes de un modelo lineal), la eliminaci&amp;oacute;n de caracter&amp;iacute;sticas recursivas ( &lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt; &lt;code&gt;RFE&lt;/code&gt; &lt;/a&gt; ) es seleccionar caracter&amp;iacute;sticas considerando recursivamente conjuntos de caracter&amp;iacute;sticas cada vez m&amp;aacute;s peque&amp;ntilde;os. Primero, el estimador se entrena en el conjunto inicial de caracter&amp;iacute;sticas y la importancia de cada caracter&amp;iacute;stica se obtiene a trav&amp;eacute;s de un atributo &lt;code&gt;coef_&lt;/code&gt; o mediante un atributo &lt;code&gt;feature_importances_&lt;/code&gt; . Luego, las caracter&amp;iacute;sticas menos importantes se eliminan del conjunto actual de caracter&amp;iacute;sticas. Ese procedimiento se repite de forma recursiva en el conjunto eliminado hasta que finalmente se alcanza el n&amp;uacute;mero deseado de caracter&amp;iacute;sticas para seleccionar.</target>
        </trans-unit>
        <trans-unit id="0a3e62329db7e0582a525546102e4bc5a3e414ee" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">Dado un estimador externo que asigna pesos a las caracter&amp;iacute;sticas (por ejemplo, los coeficientes de un modelo lineal), el objetivo de la eliminaci&amp;oacute;n de caracter&amp;iacute;sticas recursivas (RFE) es seleccionar caracter&amp;iacute;sticas considerando recursivamente conjuntos de caracter&amp;iacute;sticas cada vez m&amp;aacute;s peque&amp;ntilde;os. Primero, el estimador se entrena en el conjunto inicial de caracter&amp;iacute;sticas y la importancia de cada caracter&amp;iacute;stica se obtiene a trav&amp;eacute;s de un atributo &lt;code&gt;coef_&lt;/code&gt; o mediante un atributo &lt;code&gt;feature_importances_&lt;/code&gt; . Luego, las caracter&amp;iacute;sticas menos importantes se eliminan del conjunto de caracter&amp;iacute;sticas actual. Ese procedimiento se repite de forma recursiva en el conjunto podado hasta que finalmente se alcanza el n&amp;uacute;mero deseado de caracter&amp;iacute;sticas para seleccionar.</target>
        </trans-unit>
        <trans-unit id="2e4a90e9413cabdb8d0d79c137af8efe3fbd16ef" translate="yes" xml:space="preserve">
          <source>Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the &lt;code&gt;init='k-means++'&lt;/code&gt; parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.</source>
          <target state="translated">Con el tiempo suficiente, las K-medias siempre converger&amp;aacute;n, sin embargo, esto puede ser un m&amp;iacute;nimo local. Esto depende en gran medida de la inicializaci&amp;oacute;n de los centroides. Como resultado, el c&amp;aacute;lculo a menudo se realiza varias veces, con diferentes inicializaciones de los centroides. Un m&amp;eacute;todo para ayudar a abordar este problema es el esquema de inicializaci&amp;oacute;n k-means ++, que se ha implementado en scikit-learn (use el par&amp;aacute;metro &lt;code&gt;init='k-means++'&lt;/code&gt; ). Esto inicializa los centroides para que est&amp;eacute;n (generalmente) distantes entre s&amp;iacute;, lo que conduce a resultados demostrablemente mejores que la inicializaci&amp;oacute;n aleatoria, como se muestra en la referencia.</target>
        </trans-unit>
        <trans-unit id="74d4aecb20e2cdcd5c8865136aad914eecac7d61" translate="yes" xml:space="preserve">
          <source>Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a &lt;strong&gt;clustering task&lt;/strong&gt;: split the observations into well-separated group called &lt;em&gt;clusters&lt;/em&gt;.</source>
          <target state="translated">Dado el conjunto de datos del iris, si supi&amp;eacute;ramos que hay 3 tipos de iris, pero no tuvi&amp;eacute;ramos acceso a un tax&amp;oacute;nomo para etiquetarlos: podr&amp;iacute;amos intentar una &lt;strong&gt;tarea de agrupamiento&lt;/strong&gt; : dividir las observaciones en grupos bien separados llamados &lt;em&gt;grupos&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="7ffdaa4cdda4b54b62086a7f5ac68bd7ea3b5908" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;Mutual Information&lt;/strong&gt; is a function that measures the &lt;strong&gt;agreement&lt;/strong&gt; of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, &lt;strong&gt;Normalized Mutual Information (NMI)&lt;/strong&gt; and &lt;strong&gt;Adjusted Mutual Information (AMI)&lt;/strong&gt;. NMI is often used in the literature, while AMI was proposed more recently and is &lt;strong&gt;normalized against chance&lt;/strong&gt;:</source>
          <target state="translated">Dado el conocimiento de las asignaciones de la clase de verdad &lt;code&gt;labels_true&lt;/code&gt; y nuestras asignaciones de algoritmo de agrupamiento de las mismas muestras &lt;code&gt;labels_pred&lt;/code&gt; , la &lt;strong&gt;Informaci&amp;oacute;n Mutua&lt;/strong&gt; es una funci&amp;oacute;n que mide la &lt;strong&gt;concordancia&lt;/strong&gt; de las dos asignaciones, ignorando las permutaciones. Hay dos versiones normalizadas diferentes de esta medida disponibles, &lt;strong&gt;Informaci&amp;oacute;n mutua normalizada (NMI)&lt;/strong&gt; e &lt;strong&gt;Informaci&amp;oacute;n mutua ajustada (AMI)&lt;/strong&gt; . El NMI se usa a menudo en la literatura, mientras que el IAM se propuso m&amp;aacute;s recientemente y se &lt;strong&gt;normaliza frente al azar&lt;/strong&gt; :</target>
        </trans-unit>
        <trans-unit id="943836cb04e0640667940c68f56d5deeb3e35898" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;adjusted Rand index&lt;/strong&gt; is a function that measures the &lt;strong&gt;similarity&lt;/strong&gt; of the two assignments, ignoring permutations and &lt;strong&gt;with chance normalization&lt;/strong&gt;:</source>
          <target state="translated">Dado el conocimiento de las asignaciones de la clase de verdad &lt;code&gt;labels_true&lt;/code&gt; y nuestras asignaciones de algoritmo de agrupamiento de las mismas muestras &lt;code&gt;labels_pred&lt;/code&gt; , el &lt;strong&gt;&amp;iacute;ndice Rand ajustado&lt;/strong&gt; es una funci&amp;oacute;n que mide la &lt;strong&gt;similitud&lt;/strong&gt; de las dos asignaciones, ignorando las permutaciones y &lt;strong&gt;con normalizaci&amp;oacute;n al azar&lt;/strong&gt; :</target>
        </trans-unit>
        <trans-unit id="3a989bbd6a98db5dab53799fee5637e2080ce141" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.</source>
          <target state="translated">Dado el conocimiento de las asignaciones de clases de verdad de las muestras,es posible definir alguna métrica intuitiva utilizando el análisis de entropía condicional.</target>
        </trans-unit>
        <trans-unit id="4d7a7b1af5c7c7276434270fce7100038c705add" translate="yes" xml:space="preserve">
          <source>Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered.</source>
          <target state="translated">Dados estos vectores singulares,se clasifican de acuerdo a lo que se puede aproximar mejor por un vector constante a trozos.Las aproximaciones para cada vector se encuentran usando medios k unidimensionales y se califican usando la distancia euclidiana.Se seleccionan algunos subconjuntos del mejor vector singular izquierdo y derecho.A continuación,los datos se proyectan a este mejor subconjunto de vectores singulares y se agrupan.</target>
        </trans-unit>
        <trans-unit id="21675a464e2ca3b8f99eef191d00e106aa21c0dd" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in R^n\), i=1,&amp;hellip;, l and a label vector \(y \in R^l\), a decision tree recursively partitions the space such that the samples with the same labels are grouped together.</source>
          <target state="translated">Dados los vectores de entrenamiento \ (x_i \ in R ^ n \), i = 1,&amp;hellip;, ly un vector de etiqueta \ (y \ in R ^ l \), un &amp;aacute;rbol de decisi&amp;oacute;n divide recursivamente el espacio de tal manera que las muestras con el mismo las etiquetas est&amp;aacute;n agrupadas.</target>
        </trans-unit>
        <trans-unit id="02fd4db44c84fce9026584422f7727ba079bc40a" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, and a vector \(y \in \mathbb{R}^n\)\(\varepsilon\)-SVR solves the following primal problem:</source>
          <target state="translated">Dados los vectores de entrenamiento \ (x_i \ in \ mathbb {R} ^ p \), i = 1,&amp;hellip;, n, y un vector \ (y \ in \ mathbb {R} ^ n \) \ (\ varepsilon \) - SVR resuelve el siguiente problema primario:</target>
        </trans-unit>
        <trans-unit id="70e397398a5003e0a6b00de067e9804bfe571e70" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, in two classes, and a vector \(y \in \{1, -1\}^n\), SVC solves the following primal problem:</source>
          <target state="translated">Dados los vectores de entrenamiento \ (x_i \ in \ mathbb {R} ^ p \), i = 1,&amp;hellip;, n, en dos clases y un vector \ (y \ in \ {1, -1 \} ^ n \) , SVC resuelve el siguiente problema primario:</target>
        </trans-unit>
        <trans-unit id="e43c2f871d10fa4875c4f15e109aeb5faf94fb18" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, in two classes, and a vector \(y \in \{1, -1\}^n\), our goal is to find \(w \in \mathbb{R}^p\) and \(b \in \mathbb{R}\) such that the prediction given by \(\text{sign} (w^T\phi(x) + b)\) is correct for most samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e44bf83eca8aa1cc0c5bdaa89da0afa702f51625" translate="yes" xml:space="preserve">
          <source>Gives the number of (complex) sampling points.</source>
          <target state="translated">Da el número de puntos de muestreo (complejos).</target>
        </trans-unit>
        <trans-unit id="ac4e9c94eac5d688eef08c9122f5d38187b8f922" translate="yes" xml:space="preserve">
          <source>Global min and max average predictions, such that all plots will have the same scale and y limits. &lt;code&gt;pdp_lim[1]&lt;/code&gt; is the global min and max for single partial dependence curves. &lt;code&gt;pdp_lim[2]&lt;/code&gt; is the global min and max for two-way partial dependence curves.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f36c7685daa8ebc7e1344aa0d6e3a7d679decebf" translate="yes" xml:space="preserve">
          <source>Global structure is not explicitly preserved. This is problem is mitigated by initializing points with PCA (using &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt;).</source>
          <target state="translated">La estructura global no se conserva expl&amp;iacute;citamente. Este problema se mitiga inicializando puntos con PCA (usando &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="01649050ef673ff19d8d011219103526d0d7370d" translate="yes" xml:space="preserve">
          <source>Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using &lt;code&gt;init='pca'&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="178c27bf7200da0534de904ea7e6ca7da842dbb5" translate="yes" xml:space="preserve">
          <source>Glorot, Xavier, and Yoshua Bengio. &amp;ldquo;Understanding the difficulty of</source>
          <target state="translated">Glorot, Xavier y Yoshua Bengio. &quot;Comprender la dificultad de</target>
        </trans-unit>
        <trans-unit id="7427cf697be16a4ec1d916910128a59d920125e7" translate="yes" xml:space="preserve">
          <source>Glossary</source>
          <target state="translated">Glossary</target>
        </trans-unit>
        <trans-unit id="f7c22aaad44fb28f4ee8f06d6d4f4f14ac9ce899" translate="yes" xml:space="preserve">
          <source>Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,</source>
          <target state="translated">Golub y C.Van Loan.Matrix Computations,tercera edición,capítulo 5,</target>
        </trans-unit>
        <trans-unit id="1de5b736be2f9def46d07ed88549feeeea5a97b0" translate="yes" xml:space="preserve">
          <source>Gorodkin, (2004). Comparing two K-category assignments by a K-category correlation coefficient</source>
          <target state="translated">Gorodkin,(2004).Comparando dos asignaciones de la categoría K mediante un coeficiente de correlación de la categoría K</target>
        </trans-unit>
        <trans-unit id="46268d41f41f8e1954ca3d54fd29ddb1959ea6db" translate="yes" xml:space="preserve">
          <source>Gradient Boosting Out-of-Bag estimates</source>
          <target state="translated">Estimaciones del aumento de la graduación fuera de la bolsa</target>
        </trans-unit>
        <trans-unit id="ff01958eb0f121764b2210794dcbe435f29dcc7a" translate="yes" xml:space="preserve">
          <source>Gradient Boosting Regression Trees for Poisson regression</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9396c57fff04d750ce06a05cfd3c756b4f971532" translate="yes" xml:space="preserve">
          <source>Gradient Boosting also gives the possibility to fit the trees with a Poisson loss (with an implicit log-link function) instead of the default least-squares loss. Here we only fit trees with the Poisson loss to keep this example concise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e3d95a92c5a33953c001956fd3fd6ac3b1082fa" translate="yes" xml:space="preserve">
          <source>Gradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent direction is the negative gradient of the loss function evaluated at the current model \(F_{m-1}\) which can be calculated for any differentiable loss function:</source>
          <target state="translated">Gradient Boosting intenta resolver este problema de minimización numéricamente a través del descenso más pronunciado:La dirección de descenso más pronunciado es el gradiente negativo de la función de pérdida evaluada en el modelo actual \(F_{m-1}\)que puede ser calculado para cualquier función de pérdida diferenciable:</target>
        </trans-unit>
        <trans-unit id="be45c92854a0f55592d6c3c1c28201cf75d59d94" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for classification.</source>
          <target state="translated">Gradient Boosting para la clasificación.</target>
        </trans-unit>
        <trans-unit id="65fd480d2da13d80eb18643fd08c31b9e5239c9a" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for regression.</source>
          <target state="translated">Impulso de gradiente para la regresión.</target>
        </trans-unit>
        <trans-unit id="23dcf8253cdacbdd915f0e5e69e684c3457ad1df" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regression</source>
          <target state="translated">Gradiente Impulsando la regresión</target>
        </trans-unit>
        <trans-unit id="33b1659de13c2a7e036f71b3c26eda1d552a4b1c" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regularization</source>
          <target state="translated">Gradiente Impulsar la regularización</target>
        </trans-unit>
        <trans-unit id="a558a9ccdbbb397deb97e7223684a95578fb2ba7" translate="yes" xml:space="preserve">
          <source>Gradient boosting for classification is very similar to the regression case. However, the sum of the trees \(F_M(x_i) = \sum_m h_m(x_i)\) is not homogeneous to a prediction: it cannot be a class, since the trees predict continuous values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf9557c4e6e59de44aebd2e8b07221ff19e46958" translate="yes" xml:space="preserve">
          <source>Gradient boosting is an ensembling technique where several weak learners (regression trees) are combined to yield a powerful single model, in an iterative fashion.</source>
          <target state="translated">El aumento de gradiente es una técnica de ensamblaje en la que se combinan varios aprendices débiles (árboles de regresión)para obtener un poderoso modelo único,de manera iterativa.</target>
        </trans-unit>
        <trans-unit id="692996b3838fb57cde4b100ea1ec7f66fff47afe" translate="yes" xml:space="preserve">
          <source>Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when &lt;code&gt;eval_gradient&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4611f197e5e7430aa271445ae503720ad1cf3d4" translate="yes" xml:space="preserve">
          <source>Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True.</source>
          <target state="translated">Gradiente de la probabilidad logarítmica marginal con respecto a los hiperparámetros del núcleo en la posición theta.Sólo se devuelve cuando eval_gradient es True.</target>
        </trans-unit>
        <trans-unit id="e64c4914bb8a27678b7a6969455bd717adc62d09" translate="yes" xml:space="preserve">
          <source>GradientBoostingRegressor</source>
          <target state="translated">GradientBoostingRegressor</target>
        </trans-unit>
        <trans-unit id="e2fb5831cb5dd547c1703af3319394a3f8535468" translate="yes" xml:space="preserve">
          <source>Gram = np.dot(X.T * X).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f77edae6db0cdcd4449adeeb038c653af7406ea3" translate="yes" xml:space="preserve">
          <source>Gram Orthogonal Matching Pursuit (OMP)</source>
          <target state="translated">La persecución ortogonal de la Gram Orthogonal Matching Pursuit (OMP)</target>
        </trans-unit>
        <trans-unit id="10ef9123115df39a65f62ffa3d9d0e10899ca7cd" translate="yes" xml:space="preserve">
          <source>Gram matrix of the input data: X.T * X</source>
          <target state="translated">Matriz de gramas de los datos de entrada:X.T*X</target>
        </trans-unit>
        <trans-unit id="a83784084519ce853a92535121a74c85019c19b0" translate="yes" xml:space="preserve">
          <source>Graph distance (e.g. nearest-neighbor graph)</source>
          <target state="translated">Gráfico de distancia (por ejemplo,el gráfico del vecino más cercano)</target>
        </trans-unit>
        <trans-unit id="8d5c9a04db77341319c1b38643f0d38066fc8710" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel connections</source>
          <target state="translated">Gráfico de las conexiones píxel a píxel</target>
        </trans-unit>
        <trans-unit id="1b6f746d097f9fe3740f364d944363a7e3d991f9" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel gradient connections</source>
          <target state="translated">Gráfico de las conexiones de gradiente píxel a píxel</target>
        </trans-unit>
        <trans-unit id="a291a5c559f789dd92fe83af065257b966fe9953" translate="yes" xml:space="preserve">
          <source>Graph where A[i, j] is assigned the weight of edge that connects i to j. The matrix is of CSR format.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="933bf21afdd55a0d2283845fed0e7bbdd1f5db49" translate="yes" xml:space="preserve">
          <source>Green</source>
          <target state="translated">Green</target>
        </trans-unit>
        <trans-unit id="9786dcbe8afbab8ac93bdfcd6653b6cd7aa7993b" translate="yes" xml:space="preserve">
          <source>Grid of Cs used for cross-validation.</source>
          <target state="translated">Cuadrícula de Cs utilizada para la validación cruzada.</target>
        </trans-unit>
        <trans-unit id="5bd85812ea7e2436359885d902fd71d10cd1c2d9" translate="yes" xml:space="preserve">
          <source>Grid of parameters with a discrete number of values for each.</source>
          <target state="translated">Cuadrícula de parámetros con un número discreto de valores para cada uno.</target>
        </trans-unit>
        <trans-unit id="4a6f9190abeab5c3ccde3d9c276bc4db019e7d38" translate="yes" xml:space="preserve">
          <source>Grid search can also be performed on the different preprocessing steps defined in the &lt;code&gt;ColumnTransformer&lt;/code&gt; object, together with the classifier&amp;rsquo;s hyperparameters as part of the &lt;code&gt;Pipeline&lt;/code&gt;. We will search for both the imputer strategy of the numeric preprocessing and the regularization parameter of the logistic regression using &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">La b&amp;uacute;squeda de cuadr&amp;iacute;cula tambi&amp;eacute;n se puede realizar en los diferentes pasos de preprocesamiento definidos en el objeto &lt;code&gt;ColumnTransformer&lt;/code&gt; , junto con los hiperpar&amp;aacute;metros del clasificador como parte de la &lt;code&gt;Pipeline&lt;/code&gt; . Buscaremos tanto la estrategia de imputaci&amp;oacute;n del preprocesamiento num&amp;eacute;rico como el par&amp;aacute;metro de regularizaci&amp;oacute;n de la regresi&amp;oacute;n log&amp;iacute;stica utilizando &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="71a1782f5aa6d2b7cc26a083f91eef66c1cf3aff" translate="yes" xml:space="preserve">
          <source>Grid-search</source>
          <target state="translated">Grid-search</target>
        </trans-unit>
        <trans-unit id="ed926e289de9aa5047e6b09f7b537df04bde4bbf" translate="yes" xml:space="preserve">
          <source>Grid-search and cross-validated estimators</source>
          <target state="translated">Búsqueda en cuadrículas y estimadores validados cruzados</target>
        </trans-unit>
        <trans-unit id="64ba146c44fdd8e95f622a314398320f76845aed" translate="yes" xml:space="preserve">
          <source>GridSearchCV implements a &amp;ldquo;fit&amp;rdquo; and a &amp;ldquo;score&amp;rdquo; method. It also implements &amp;ldquo;predict&amp;rdquo;, &amp;ldquo;predict_proba&amp;rdquo;, &amp;ldquo;decision_function&amp;rdquo;, &amp;ldquo;transform&amp;rdquo; and &amp;ldquo;inverse_transform&amp;rdquo; if they are implemented in the estimator used.</source>
          <target state="translated">GridSearchCV implementa un m&amp;eacute;todo de &quot;ajuste&quot; y &quot;puntuaci&amp;oacute;n&quot;. Tambi&amp;eacute;n implementa &amp;ldquo;predict&amp;rdquo;, &amp;ldquo;predict_proba&amp;rdquo;, &amp;ldquo;decision_function&amp;rdquo;, &amp;ldquo;transform&amp;rdquo; y &amp;ldquo;inverse_transform&amp;rdquo; si est&amp;aacute;n implementados en el estimador utilizado.</target>
        </trans-unit>
        <trans-unit id="2e6f2bdd92d1c5e33352841cf6b10ed864b19fa7" translate="yes" xml:space="preserve">
          <source>Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification: An Overview. International Journal of Data Warehousing &amp;amp; Mining, 3(3), 1-13, July-September 2007.</source>
          <target state="translated">Grigorios Tsoumakas, Ioannis Katakis. Clasificaci&amp;oacute;n de etiquetas m&amp;uacute;ltiples: descripci&amp;oacute;n general. Revista Internacional de Almacenamiento de Datos y Miner&amp;iacute;a, 3 (3), 1-13, julio-septiembre de 2007.</target>
        </trans-unit>
        <trans-unit id="796325c68f51f69a2afcc84a9fb61fa1d8420435" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels for n_samples samples.</source>
          <target state="translated">Etiquetas de verdad (correctas)para n_muestras de muestras.</target>
        </trans-unit>
        <trans-unit id="740dd68aa13d511b42941c79135a52aa5a0f5bc4" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels.</source>
          <target state="translated">Etiquetas de la verdad del suelo (correcto).</target>
        </trans-unit>
        <trans-unit id="cf154969e860842a471602bf65b740057751e47b" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values.</source>
          <target state="translated">Valores del objetivo de la verdad de la tierra (correctos).</target>
        </trans-unit>
        <trans-unit id="b29893e6134ceb0ae63100b750ea64cd6227af16" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values. Requires y_true &amp;gt; 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8dcf4476037b86142a6c723cc6d58c74ce6fa30f" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values. Requires y_true &amp;gt;= 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="691f624e8ff75b4d50175631f407699ccfb7e35d" translate="yes" xml:space="preserve">
          <source>Ground truth class labels to be used as a reference</source>
          <target state="translated">Las etiquetas de clase de verdad de la tierra se usarán como referencia</target>
        </trans-unit>
        <trans-unit id="2859baca63ac3255284d20bc28f887a4c54fefb4" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set.</source>
          <target state="translated">Etiquetas de grupo para las muestras utilizadas al dividir el conjunto de datos en tren/juego de pruebas.</target>
        </trans-unit>
        <trans-unit id="da0d044e30ddccc2bad0f6e17da06a788ea2385a" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set. Only used in conjunction with a &amp;ldquo;Group&amp;rdquo; &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt; instance (e.g., &lt;a href=&quot;sklearn.model_selection.groupkfold#sklearn.model_selection.GroupKFold&quot;&gt;&lt;code&gt;GroupKFold&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5ee660cf40b3d432d2833cbe2c4255cb71b873d" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set. This &amp;lsquo;groups&amp;rsquo; parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.</source>
          <target state="translated">Agrupe las etiquetas para las muestras utilizadas al dividir el conjunto de datos en conjunto de tren / prueba. Este par&amp;aacute;metro de 'grupos' siempre debe especificarse para calcular el n&amp;uacute;mero de divisiones, aunque los dem&amp;aacute;s par&amp;aacute;metros pueden omitirse.</target>
        </trans-unit>
        <trans-unit id="2fe58cc1aca321453c1632eb3218b2ee2034ed27" translate="yes" xml:space="preserve">
          <source>Grow a tree with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">&lt;code&gt;max_leaf_nodes&lt;/code&gt; crecer un &amp;aacute;rbol con max_leaf_nodes de la mejor manera primero. Los mejores nodos se definen como una reducci&amp;oacute;n relativa de la impureza. Si es Ninguno, entonces un n&amp;uacute;mero ilimitado de nodos hoja.</target>
        </trans-unit>
        <trans-unit id="9f319cd9d13cdc03649579ff252c6b96c720508d" translate="yes" xml:space="preserve">
          <source>Grow trees with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">&lt;code&gt;max_leaf_nodes&lt;/code&gt; &amp;aacute;rboles con max_leaf_nodes de la mejor manera. Los mejores nodos se definen como una reducci&amp;oacute;n relativa de la impureza. Si es Ninguno, entonces un n&amp;uacute;mero ilimitado de nodos hoja.</target>
        </trans-unit>
        <trans-unit id="bf073fae640ded81eeb7a4cee70faff4a623c16c" translate="yes" xml:space="preserve">
          <source>Guide</source>
          <target state="translated">Guide</target>
        </trans-unit>
        <trans-unit id="1fd932db6b504d046b60a30c3273eb39ba2ac7a5" translate="yes" xml:space="preserve">
          <source>Guyon, I., Weston, J., Barnhill, S., &amp;amp; Vapnik, V., &amp;ldquo;Gene selection for cancer classification using support vector machines&amp;rdquo;, Mach. Learn., 46(1-3), 389&amp;ndash;422, 2002.</source>
          <target state="translated">Guyon, I., Weston, J., Barnhill, S. y Vapnik, V., &quot;Selecci&amp;oacute;n de genes para la clasificaci&amp;oacute;n del c&amp;aacute;ncer mediante m&amp;aacute;quinas de vectores de apoyo&quot;, Mach. Learn., 46 (1-3), 389&amp;ndash;422, 2002.</target>
        </trans-unit>
        <trans-unit id="dd4d457c816b0cb358c91f5b8813986bac26cb3d" translate="yes" xml:space="preserve">
          <source>H. Zhang (2004). &lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;The optimality of Naive Bayes.&lt;/a&gt; Proc. FLAIRS.</source>
          <target state="translated">H. Zhang (2004). &lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;La optimalidad de Naive Bayes. &lt;/a&gt;Proc. FLAIRS.</target>
        </trans-unit>
        <trans-unit id="ce3bde746d403806636c8d155597ef91ca7e1f03" translate="yes" xml:space="preserve">
          <source>H. Zhang (2004). &lt;a href=&quot;https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;The optimality of Naive Bayes.&lt;/a&gt; Proc. FLAIRS.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de489f31c1f185d4a81f0399ead4e066a95b91be" translate="yes" xml:space="preserve">
          <source>HTML representation of &lt;code&gt;Pipeline&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d930a6037b9120a42017959402d9dc27dd6bf69c" translate="yes" xml:space="preserve">
          <source>HTML representation of estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5b6915b0e377ea69d7b62d27d3f027cc63657d7" translate="yes" xml:space="preserve">
          <source>Hagai Attias. (2000). &amp;ldquo;A Variational Bayesian Framework for Graphical Models&amp;rdquo;. In Advances in Neural Information Processing Systems 12.</source>
          <target state="translated">Hagai Attias. (2000). &amp;ldquo;Un marco bayesiano variacional para modelos gr&amp;aacute;ficos&amp;rdquo;. Avances en los sistemas de procesamiento de informaci&amp;oacute;n neuronal 12.</target>
        </trans-unit>
        <trans-unit id="8446ed65374f4c03b547ffebe7ab69437207be78" translate="yes" xml:space="preserve">
          <source>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo; Journal of Intelligent Information Systems, 17(2-3), 107-145. &lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi:10.1023/A:1012801612483&lt;/a&gt;.</source>
          <target state="translated">Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). Revista de sistemas de informaci&amp;oacute;n inteligentes sobre t&amp;eacute;cnicas de validaci&amp;oacute;n de agrupaciones, 17 (2-3), 107-145. &lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi: 10.1023 / A: 1012801612483&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="959a6f4c185bd74a74d43205ed3cc9281eca4d45" translate="yes" xml:space="preserve">
          <source>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo; Journal of Intelligent Information Systems, 17(2-3), 107-145. &lt;a href=&quot;https://doi.org/10.1023/A:1012801612483&quot;&gt;doi:10.1023/A:1012801612483&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57fe625410e680c160d128700bfb1af1b965809e" translate="yes" xml:space="preserve">
          <source>HammingDistance</source>
          <target state="translated">HammingDistance</target>
        </trans-unit>
        <trans-unit id="9757089e5251a143827d61ff72e389c7fd386869" translate="yes" xml:space="preserve">
          <source>Hand, D.J. and Till, R.J., (2001). &lt;a href=&quot;http://link.springer.com/article/10.1023/A:1010920819831&quot;&gt;A simple generalisation of the area under the ROC curve for multiple class classification problems.&lt;/a&gt; Machine learning, 45(2), pp.171-186.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88b6ef37ba2f9ba619bbf453c13dce1665119f21" translate="yes" xml:space="preserve">
          <source>Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dee17735ec3038cb9f5dda5413031eefdf59071a" translate="yes" xml:space="preserve">
          <source>Handle or name of the output file. If &lt;code&gt;None&lt;/code&gt;, the result is returned as a string.</source>
          <target state="translated">Identificador o nombre del archivo de salida. Si es &lt;code&gt;None&lt;/code&gt; , el resultado se devuelve como una cadena.</target>
        </trans-unit>
        <trans-unit id="528b68d16981ccbe32f7d51bc822d75e077c8b80" translate="yes" xml:space="preserve">
          <source>Handling Multicollinear Features</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="077bb86f8a4736a0992a0b108c1d4b8e9298e04f" translate="yes" xml:space="preserve">
          <source>Hard constraint to select the backend. If set to &amp;lsquo;sharedmem&amp;rsquo;, the selected backend will be single-host and thread-based even if the user asked for a non-thread based backend with parallel_backend.</source>
          <target state="translated">Restricci&amp;oacute;n dura para seleccionar el backend. Si se establece en 'sharedmem', el backend seleccionado ser&amp;aacute; de un solo host y estar&amp;aacute; basado en subprocesos incluso si el usuario solicit&amp;oacute; un backend no basado en subprocesos con paralelo_backend.</target>
        </trans-unit>
        <trans-unit id="9b9156693e970a15a3c18a9425374c7bf2903574" translate="yes" xml:space="preserve">
          <source>Hard limit on iterations within solver, or -1 for no limit.</source>
          <target state="translated">Límite duro en las iteraciones dentro del solucionador,o -1 para ningún límite.</target>
        </trans-unit>
        <trans-unit id="73dd008516fbc283773051e5943e3b658488b1b1" translate="yes" xml:space="preserve">
          <source>Harrison, D. and Rubinfeld, D.L.</source>
          <target state="translated">Harrison,D.y Rubinfeld,D.L.</target>
        </trans-unit>
        <trans-unit id="c23f4e8aad7e2235e0ebdbc3c9d2bf9b602d6e3d" translate="yes" xml:space="preserve">
          <source>Hash function g(p,x) for a tree is an array of 32 randomly generated float arrays with the same dimension as the data set. This array is stored in GaussianRandomProjectionHash object and can be obtained from &lt;code&gt;components_&lt;/code&gt; attribute.</source>
          <target state="translated">La funci&amp;oacute;n hash g (p, x) para un &amp;aacute;rbol es una matriz de 32 matrices flotantes generadas aleatoriamente con la misma dimensi&amp;oacute;n que el conjunto de datos. Esta matriz se almacena en el objeto GaussianRandomProjectionHash y se puede obtener del atributo &lt;code&gt;components_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8b5d87d4a16c0b826cb8988befd77a0e52c765c1" translate="yes" xml:space="preserve">
          <source>Hashing feature transformation using Totally Random Trees</source>
          <target state="translated">Transformación de la característica Hashing usando Árboles Totalmente Aleatorios</target>
        </trans-unit>
        <trans-unit id="717a562588a8bf4bd25fb65069c4d3192c7a16dc" translate="yes" xml:space="preserve">
          <source>HashingVectorizer does not provide IDF weighting as this is a stateless model (the fit method does nothing). When IDF weighting is needed it can be added by pipelining its output to a TfidfTransformer instance.</source>
          <target state="translated">HashingVectorizer no proporciona la ponderación de la FID,ya que se trata de un modelo sin estado (el método de ajuste no hace nada).Cuando se necesita la ponderación de la FID,se puede añadir canalizando su salida a una instancia del Transformador Tfidf.</target>
        </trans-unit>
        <trans-unit id="d06cc92706967f16b8b9c95848cf5aff7ec1c456" translate="yes" xml:space="preserve">
          <source>HashingVectorizer hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space.</source>
          <target state="translated">HashingVectorizer envía los sucesos de las palabras a un espacio dimensional fijo,posiblemente con colisiones.Los vectores de recuento de palabras se normalizan entonces para que cada uno tenga una norma l2 igual a una (proyectada a la bola unidad euclidiana),lo que parece ser importante para que los vectores k funcionen en un espacio dimensional elevado.</target>
        </trans-unit>
        <trans-unit id="28041ffc119d6685560d28cedcd34e917cd495e5" translate="yes" xml:space="preserve">
          <source>Hastie, R. Tibshirani and J. Friedman, &amp;ldquo;Elements of Statistical Learning Ed. 2&amp;rdquo;, Springer, 2009.</source>
          <target state="translated">Hastie, R. Tibshirani y J. Friedman, &amp;ldquo;Elements of Statistical Learning Ed. 2 &amp;rdquo;, Springer, 2009.</target>
        </trans-unit>
        <trans-unit id="9803f456bd9f04731b6843d220c5ae89fa289aa2" translate="yes" xml:space="preserve">
          <source>Haussler, D. (1999). Convolution kernels on discrete structures (Vol. 646). Technical report, Department of Computer Science, University of California at Santa Cruz.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8dd0d155e19e8a71f19b1bbe40cdccabf151805" translate="yes" xml:space="preserve">
          <source>Have a look at the &lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;Hashing Vectorizer&lt;/a&gt; as a memory efficient alternative to &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Eche un vistazo al &lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;Hashing Vectorizer&lt;/a&gt; como una alternativa eficiente en memoria a &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="a899619755f5d06da20b9b2964b88739a1ab106e" translate="yes" xml:space="preserve">
          <source>Have a look at using &lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core Classification&lt;/a&gt; to learn from data that would not fit into the computer main memory.</source>
          <target state="translated">Eche un vistazo a la &lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;clasificaci&amp;oacute;n de fuera del n&amp;uacute;cleo&lt;/a&gt; para aprender de los datos que no cabr&amp;iacute;an en la memoria principal de la computadora.</target>
        </trans-unit>
        <trans-unit id="27a688f240efc4c1f8111e73298dc1d5dd7e9964" translate="yes" xml:space="preserve">
          <source>HaversineDistance</source>
          <target state="translated">HaversineDistance</target>
        </trans-unit>
        <trans-unit id="260c7f8bcac0cff0858b268328a3c57270e6d05b" translate="yes" xml:space="preserve">
          <source>He, Kaiming, et al. &amp;ldquo;Delving deep into rectifiers: Surpassing human-level</source>
          <target state="translated">&amp;Eacute;l, Kaiming y col. &quot;Profundizando en los rectificadores: superando el nivel humano</target>
        </trans-unit>
        <trans-unit id="2f8a00b4f7c2990e23253c9271642cb45a1f2224" translate="yes" xml:space="preserve">
          <source>Helper class for readable parallel mapping.</source>
          <target state="translated">Clase de ayuda para la cartografía paralela legible.</target>
        </trans-unit>
        <trans-unit id="15e3ecfce92d858c5fac5d21e2153dba45c36e72" translate="yes" xml:space="preserve">
          <source>Helper function to test the message raised in an exception.</source>
          <target state="translated">Función de ayuda para probar el mensaje planteado en una excepción.</target>
        </trans-unit>
        <trans-unit id="e22b8152bb5ec7ad5480951d5d1692b1809abba4" translate="yes" xml:space="preserve">
          <source>Hence using random projections on the digits dataset which only has 64 features in the input space does not make sense: it does not allow for dimensionality reduction in this case.</source>
          <target state="translated">Por lo tanto,el uso de proyecciones aleatorias en el conjunto de datos de los dígitos que sólo tiene 64 características en el espacio de entrada no tiene sentido:no permite la reducción de la dimensionalidad en este caso.</target>
        </trans-unit>
        <trans-unit id="858c4ba42a503184b8af0061cb8145e1add6548c" translate="yes" xml:space="preserve">
          <source>Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:</source>
          <target state="translated">Por lo tanto,las palabras que no se vieron en el corpus de entrenamiento serán completamente ignoradas en futuras llamadas al método de transformación:</target>
        </trans-unit>
        <trans-unit id="3fea43b2d3bbf05cef0fdbdec4ca7b01a2de9eb5" translate="yes" xml:space="preserve">
          <source>Hence, the None case results in:</source>
          <target state="translated">Por lo tanto,el caso None resulta:</target>
        </trans-unit>
        <trans-unit id="4fffc6a6ec537bad9c19e154df4fbf4c1dee1839" translate="yes" xml:space="preserve">
          <source>Here &lt;code&gt;func&lt;/code&gt; is a function which takes two one-dimensional numpy arrays, and returns a distance. Note that in order to be used within the BallTree, the distance must be a true metric: i.e. it must satisfy the following properties</source>
          <target state="translated">Aqu&amp;iacute; &lt;code&gt;func&lt;/code&gt; es una funci&amp;oacute;n que toma dos matrices num&amp;eacute;ricas unidimensionales y devuelve una distancia. Tenga en cuenta que para que se utilice dentro de BallTree, la distancia debe ser una m&amp;eacute;trica verdadera: es decir, debe satisfacer las siguientes propiedades</target>
        </trans-unit>
        <trans-unit id="8918252717f29fe05952e0490941948a7c1afcd2" translate="yes" xml:space="preserve">
          <source>Here a sine function is fit with a polynomial of order 3, for values close to zero.</source>
          <target state="translated">Aquí una función sinusoidal se ajusta con un polinomio de orden 3,para valores cercanos a cero.</target>
        </trans-unit>
        <trans-unit id="dd2684d229285b3b1a04454d9cd068cd1e054408" translate="yes" xml:space="preserve">
          <source>Here a small example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a binary class problem:</source>
          <target state="translated">Aqu&amp;iacute; un peque&amp;ntilde;o ejemplo que demuestra el uso de la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; con un clasificador svm en un problema de clase binaria:</target>
        </trans-unit>
        <trans-unit id="5113795d86cf9b1916006aecdb0ceee73192da33" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the Gaussian random projection transformer:</source>
          <target state="translated">Aquí un pequeño extracto que ilustra cómo utilizar el transformador de proyección aleatoria gaussiano:</target>
        </trans-unit>
        <trans-unit id="d838251a264bfc0a86e50e7c2f5d9ef6f54d10aa" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the sparse random projection transformer:</source>
          <target state="translated">Aquí un pequeño extracto que ilustra cómo usar el transformador de proyección aleatoria dispersa:</target>
        </trans-unit>
        <trans-unit id="32f51b9dd909238771016da8eae995fa183bb752" translate="yes" xml:space="preserve">
          <source>Here are a few suggestions to help further your scikit-learn intuition upon the completion of this tutorial:</source>
          <target state="translated">Aquí hay algunas sugerencias para ayudar a fomentar su intuición de aprendizaje de la ciencia al completar este tutorial:</target>
        </trans-unit>
        <trans-unit id="3dda6ea8d57e795e10f1bb02b3e190ba1eee1ee3" translate="yes" xml:space="preserve">
          <source>Here are some examples demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function to calculate recall (or sensitivity), specificity, fall out and miss rate for each class in a problem with multilabel indicator matrix input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d2ed57227d29b030e11d07a6ad14619156d6baa" translate="yes" xml:space="preserve">
          <source>Here are some recommended ways to load standard columnar data into a format usable by scikit-learn:</source>
          <target state="translated">A continuación se recomiendan algunas formas de cargar los datos de las columnas estándar en un formato utilizable por scikit-learn:</target>
        </trans-unit>
        <trans-unit id="6caa2e3f5fa319efda163f3ada59f70b9af4251d" translate="yes" xml:space="preserve">
          <source>Here are some small examples in binary classification:</source>
          <target state="translated">He aquí algunos pequeños ejemplos en la clasificación binaria:</target>
        </trans-unit>
        <trans-unit id="cad58f968788a0c8b830200526f46c2e8380af6d" translate="yes" xml:space="preserve">
          <source>Here is a list of incremental estimators for different tasks:</source>
          <target state="translated">Aquí hay una lista de estimadores incrementales para diferentes tareas:</target>
        </trans-unit>
        <trans-unit id="e5cc3ef05cd44a377ff0113c5a0144a6cd05b3f4" translate="yes" xml:space="preserve">
          <source>Here is a sample output of a run on a quad-core machine:</source>
          <target state="translated">Aquí hay una muestra de una corrida en una máquina de cuatro núcleos:</target>
        </trans-unit>
        <trans-unit id="00dac27806e77f637d738445566eb627365e0881" translate="yes" xml:space="preserve">
          <source>Here is a sketch of a system designed to achieve this goal:</source>
          <target state="translated">Aquí hay un bosquejo de un sistema diseñado para lograr este objetivo:</target>
        </trans-unit>
        <trans-unit id="c595619fa24f58ee3930e8429960f874f9b329e7" translate="yes" xml:space="preserve">
          <source>Here is a small example illustrating the usage of the &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">Aqu&amp;iacute; hay un peque&amp;ntilde;o ejemplo que ilustra el uso de la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="423aaa3f8753fc630af578bc1fbb46728b5a06e5" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">A continuaci&amp;oacute;n, se muestra un peque&amp;ntilde;o ejemplo del uso de la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="1875c837ecf1df623da5e8546dcb195bb9e83c64" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.max_error#sklearn.metrics.max_error&quot;&gt;&lt;code&gt;max_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb41f576a02130e8636700bae5b58c941781076b" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">Aqu&amp;iacute; hay un peque&amp;ntilde;o ejemplo del uso de la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="7c0ba7d72bd4599fa8b6676ec86f846e3705f7da" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">Aqu&amp;iacute; hay un peque&amp;ntilde;o ejemplo del uso de la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="0be0450f469be9534c036908ab2afdbd59b24548" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">Aqu&amp;iacute; hay un peque&amp;ntilde;o ejemplo de uso de la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="f8da86e09b21d704ee9aa6f7fcb4b0cf6258a18d" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">Aqu&amp;iacute; hay un peque&amp;ntilde;o ejemplo de uso de la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="e0060b4a19332fa9cdf176d47debc4e3de22af1f" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">Aqu&amp;iacute; hay un peque&amp;ntilde;o ejemplo de uso de la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="2121874e07dc9ac1fb205417370f94e728d5e5e6" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function:</source>
          <target state="translated">Aquí hay un pequeño ejemplo del uso de esta función:</target>
        </trans-unit>
        <trans-unit id="f03ea6f9a5b7db0b84376e166dbfe9d87d690fa9" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function::</source>
          <target state="translated">Aquí hay un pequeño ejemplo del uso de esta función::</target>
        </trans-unit>
        <trans-unit id="da9291cb119f102218681b72119ede84a1e93115" translate="yes" xml:space="preserve">
          <source>Here is a usage example:</source>
          <target state="translated">Aquí hay un ejemplo de uso:</target>
        </trans-unit>
        <trans-unit id="ec46f6fe41e667dcb81fcf9a89e2aaf0a6763af5" translate="yes" xml:space="preserve">
          <source>Here is a visual representation of such a confusion matrix (this figure comes from the &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;Confusion matrix&lt;/a&gt; example):</source>
          <target state="translated">Aqu&amp;iacute; hay una representaci&amp;oacute;n visual de dicha matriz de confusi&amp;oacute;n (esta figura proviene del ejemplo de la &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;matriz de confusi&amp;oacute;n&lt;/a&gt; ):</target>
        </trans-unit>
        <trans-unit id="876abdb2188ee5022ae84c77928e2082f05a478c" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior.</source>
          <target state="translated">Aquí hay una visualización del comportamiento de validación cruzada.</target>
        </trans-unit>
        <trans-unit id="d5a8fd11bd11ae3f4eb764b39ba1acfee92579af" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">A continuaci&amp;oacute;n se muestra una visualizaci&amp;oacute;n del comportamiento de validaci&amp;oacute;n cruzada. Tenga en cuenta que &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; no se ve afectado por clases o grupos.</target>
        </trans-unit>
        <trans-unit id="e10cd61d7e44ad9e6bb0d4cec30745248d4c4e93" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">A continuaci&amp;oacute;n se muestra una visualizaci&amp;oacute;n del comportamiento de validaci&amp;oacute;n cruzada. Tenga en cuenta que &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; &lt;/a&gt; no se ve afectado por clases o grupos.</target>
        </trans-unit>
        <trans-unit id="0b166c480658b240c273df0a43ce9ffa8405561c" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a multiclass problem:</source>
          <target state="translated">Aqu&amp;iacute; hay un ejemplo que demuestra el uso de la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; con un clasificador svm en un problema multiclase:</target>
        </trans-unit>
        <trans-unit id="01baca5f1060ff615b57707b27b89349c3831f22" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function with &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;multiclass&lt;/a&gt; input:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac06b69a6bbd9ae081c446d18662dd3517d588fd" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function with &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel-indicator-matrix&quot;&gt;multilabel indicator matrix&lt;/a&gt; input:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58bc8e3595ba3624485387b6def524d2d31bae65" translate="yes" xml:space="preserve">
          <source>Here is an example of &lt;code&gt;cross_validate&lt;/code&gt; using a single metric:</source>
          <target state="translated">A continuaci&amp;oacute;n, se muestra un ejemplo de &lt;code&gt;cross_validate&lt;/code&gt; con una &amp;uacute;nica m&amp;eacute;trica:</target>
        </trans-unit>
        <trans-unit id="f7e50cdf4078c7823c206e72ce0bf5486f1e2a9f" translate="yes" xml:space="preserve">
          <source>Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:</source>
          <target state="translated">He aquí un ejemplo de la aplicación de esta idea a datos unidimensionales,utilizando características polinómicas de diversos grados:</target>
        </trans-unit>
        <trans-unit id="8375acd14d3c16b75f14ad4cf9799bf09154cba1" translate="yes" xml:space="preserve">
          <source>Here is an example of building custom scorers, and of using the &lt;code&gt;greater_is_better&lt;/code&gt; parameter:</source>
          <target state="translated">A continuaci&amp;oacute;n, se muestra un ejemplo de &lt;code&gt;greater_is_better&lt;/code&gt; crear marcadores personalizados y de utilizar el par&amp;aacute;metro mayor_es_better :</target>
        </trans-unit>
        <trans-unit id="3be41bccb12847b90804b0be88468f33593d4dc5" translate="yes" xml:space="preserve">
          <source>Here is an example of stratified 3-fold cross-validation on a dataset with 50 samples from two unbalanced classes. We show the number of samples in each class and compare with &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="120ffb4ca9b2da814644df8eb634b8cef584b2a0" translate="yes" xml:space="preserve">
          <source>Here is an example to scale a toy data matrix to the &lt;code&gt;[0, 1]&lt;/code&gt; range:</source>
          <target state="translated">A continuaci&amp;oacute;n, se muestra un ejemplo para escalar una matriz de datos de juguete al rango &lt;code&gt;[0, 1]&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="04b3257d3ad37f9ca0ccbd79a367325c6e1ed5f4" translate="yes" xml:space="preserve">
          <source>Here is an example using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; with the &lt;code&gt;elasticnet&lt;/code&gt; penalty. The regularization strength is globally controlled by the &lt;code&gt;alpha&lt;/code&gt; parameter. With a sufficiently high &lt;code&gt;alpha&lt;/code&gt;, one can then increase the &lt;code&gt;l1_ratio&lt;/code&gt; parameter of &lt;code&gt;elasticnet&lt;/code&gt; to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f89ae42a83e786b17cd6f1b83024799754e5687" translate="yes" xml:space="preserve">
          <source>Here is an example using &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; with the &lt;code&gt;elasticnet&lt;/code&gt; penalty. The regularization strength is globally controlled by the &lt;code&gt;alpha&lt;/code&gt; parameter. With a sufficiently high &lt;code&gt;alpha&lt;/code&gt;, one can then increase the &lt;code&gt;l1_ratio&lt;/code&gt; parameter of &lt;code&gt;elasticnet&lt;/code&gt; to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.</source>
          <target state="translated">Aqu&amp;iacute; hay un ejemplo que usa &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; con la penalizaci&amp;oacute;n de &lt;code&gt;elasticnet&lt;/code&gt; . La intensidad de la regularizaci&amp;oacute;n est&amp;aacute; controlada globalmente por el par&amp;aacute;metro &lt;code&gt;alpha&lt;/code&gt; . Con una suficientemente alta &lt;code&gt;alpha&lt;/code&gt; , se puede entonces aumentar la &lt;code&gt;l1_ratio&lt;/code&gt; par&amp;aacute;metro de &lt;code&gt;elasticnet&lt;/code&gt; para hacer cumplir varios niveles de escasez en los coeficientes del modelo. Aqu&amp;iacute;, una mayor dispersi&amp;oacute;n se interpreta como una menor complejidad del modelo, ya que necesitamos menos coeficientes para describirlo completamente. Por supuesto, la escasez influye a su vez en el tiempo de predicci&amp;oacute;n, ya que el producto punto escaso lleva un tiempo aproximadamente proporcional al n&amp;uacute;mero de coeficientes distintos de cero.</target>
        </trans-unit>
        <trans-unit id="540ee2aaf7182c6dfc449b18e5accb694e3b0894" translate="yes" xml:space="preserve">
          <source>Here is an example:</source>
          <target state="translated">Aquí hay un ejemplo:</target>
        </trans-unit>
        <trans-unit id="a1ce1cc95adf7777aaf8483ebc72e46f7e0c5dd5" translate="yes" xml:space="preserve">
          <source>Here is how to use the toy data from the previous example with this scaler:</source>
          <target state="translated">Aquí está cómo usar los datos de los juguetes del ejemplo anterior con este escalador:</target>
        </trans-unit>
        <trans-unit id="da00252cb105e8e07c4719d8131543c2597c6b64" translate="yes" xml:space="preserve">
          <source>Here is sample code that illustrates the use of the &lt;code&gt;sparsify()&lt;/code&gt; method:</source>
          <target state="translated">Aqu&amp;iacute; hay un c&amp;oacute;digo de muestra que ilustra el uso del m&amp;eacute;todo &lt;code&gt;sparsify()&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="b1b76d97b9ed98e3661e06b53d247e6f552362c3" translate="yes" xml:space="preserve">
          <source>Here is sample code to test the sparsity of your input:</source>
          <target state="translated">Aquí está el código de muestra para probar la escasez de su entrada:</target>
        </trans-unit>
        <trans-unit id="7a4f1fdf399f62578619e41a5fba4a345597683a" translate="yes" xml:space="preserve">
          <source>Here is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:</source>
          <target state="translated">A continuación figura la lista de modelos que se benefician del Criterio de Información Akaike (AIC)o del Criterio de Información Bayesiana (BIC)para la selección automatizada de modelos:</target>
        </trans-unit>
        <trans-unit id="24d46233c5b1cf5947d798926d1e317b272fc656" translate="yes" xml:space="preserve">
          <source>Here is the list of such models:</source>
          <target state="translated">Aquí está la lista de tales modelos:</target>
        </trans-unit>
        <trans-unit id="757c8807092bec583b3c00400f122f638dd1b02a" translate="yes" xml:space="preserve">
          <source>Here one can observe that the train accuracy is very high (the forest model has enough capacity to completely memorize the training set) but it can still generalize well enough to the test set thanks to the built-in bagging of random forests.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15f3441e24a8e858a11c375d5c2fee3bc8aa09ba" translate="yes" xml:space="preserve">
          <source>Here our goal goal is to predict the expected value, i.e. the mean, of the total claim amount per exposure unit also referred to as the pure premium.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="785dc58756e0e128c51d97262d985997dfc75263" translate="yes" xml:space="preserve">
          <source>Here the &lt;code&gt;transform&lt;/code&gt; operation returns \(LX^T\), therefore its time complexity equals &lt;code&gt;n_components * n_features * n_samples_test&lt;/code&gt;. There is no added space complexity in the operation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8029b08717fd12d596415c9951c99ad442611512" translate="yes" xml:space="preserve">
          <source>Here the computation is achieved thanks to Martinsson&amp;rsquo;s Randomized SVD algorithm implemented in scikit-learn.</source>
          <target state="translated">Aqu&amp;iacute; el c&amp;aacute;lculo se logra gracias al algoritmo SVD aleatorio de Martinsson implementado en scikit-learn.</target>
        </trans-unit>
        <trans-unit id="baa6fd34087f3f3b80a068e5198c152eb2224084" translate="yes" xml:space="preserve">
          <source>Here the results are not as good as they could be as our choice for the regularization parameter C was not the best. In real life applications this parameter is usually chosen using &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">Aqu&amp;iacute; los resultados no son tan buenos como podr&amp;iacute;an ser ya que nuestra elecci&amp;oacute;n para el par&amp;aacute;metro de regularizaci&amp;oacute;n C no fue la mejor. En aplicaciones de la vida real, este par&amp;aacute;metro generalmente se elige mediante el &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;ajuste de los hiperpar&amp;aacute;metros de un estimador&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e33a1b9fd8981a72cf8c17c638c489933e2535f4" translate="yes" xml:space="preserve">
          <source>Here we choose the SAGA solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.</source>
          <target state="translated">Aquí elegimos el solucionador SAGA porque puede optimizar eficientemente para la pérdida de la Regresión Logística con una penalización de l1 inducida por la escasez.</target>
        </trans-unit>
        <trans-unit id="15d9d9f74de48f0968b756664a2f90e7435e3e3c" translate="yes" xml:space="preserve">
          <source>Here we choose the liblinear solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4668460d1dfffc9a12dd3f0ca645c8016c22599" translate="yes" xml:space="preserve">
          <source>Here we compare 3 approaches:</source>
          <target state="translated">Aquí comparamos 3 enfoques:</target>
        </trans-unit>
        <trans-unit id="3aadfee7aa5bef8aeacf179790398b01b017dc93" translate="yes" xml:space="preserve">
          <source>Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on &lt;em&gt;clusterings with an infinite, unbounded, number of partitions&lt;/em&gt;. Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model.</source>
          <target state="translated">Aqu&amp;iacute; describimos algoritmos de inferencia variacional en la mezcla de procesos de Dirichlet. El proceso de Dirichlet es una distribuci&amp;oacute;n de probabilidad previa en &lt;em&gt;agrupaciones con un n&amp;uacute;mero infinito e ilimitado de particiones&lt;/em&gt; . Las t&amp;eacute;cnicas de variaci&amp;oacute;n nos permiten incorporar esta estructura previa en modelos de mezcla gaussiana casi sin penalizaci&amp;oacute;n en el tiempo de inferencia, en comparaci&amp;oacute;n con un modelo de mezcla gaussiano finito.</target>
        </trans-unit>
        <trans-unit id="19eba1946aa4b2b6c504a0a7a0b9c334a19205ae" translate="yes" xml:space="preserve">
          <source>Here we fit a multinomial logistic regression with L1 penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the l1-penalty. Test accuracy reaches &amp;gt; 0.8, while weight vectors remains &lt;em&gt;sparse&lt;/em&gt; and therefore more easily &lt;em&gt;interpretable&lt;/em&gt;.</source>
          <target state="translated">Aqu&amp;iacute; ajustamos una regresi&amp;oacute;n log&amp;iacute;stica multinomial con penalizaci&amp;oacute;n L1 en un subconjunto de la tarea de clasificaci&amp;oacute;n de d&amp;iacute;gitos MNIST. Usamos el algoritmo SAGA para este prop&amp;oacute;sito: este es un solucionador que es r&amp;aacute;pido cuando el n&amp;uacute;mero de muestras es significativamente mayor que el n&amp;uacute;mero de caracter&amp;iacute;sticas y es capaz de optimizar con precisi&amp;oacute;n funciones objetivas que no son uniformes, como es el caso de la penalizaci&amp;oacute;n l1. La precisi&amp;oacute;n de la prueba alcanza&amp;gt; 0,8, mientras que los vectores de peso siguen siendo &lt;em&gt;escasos&lt;/em&gt; y, por lo tanto, m&amp;aacute;s f&amp;aacute;ciles de &lt;em&gt;interpretar&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="bc0bdb5bd44175832d7fcee805ba26710508e043" translate="yes" xml:space="preserve">
          <source>Here we have used &lt;code&gt;kernel='gaussian'&lt;/code&gt;, as seen above. Mathematically, a kernel is a positive function \(K(x;h)\) which is controlled by the bandwidth parameter \(h\). Given this kernel form, the density estimate at a point \(y\) within a group of points \(x_i; i=1\cdots N\) is given by:</source>
          <target state="translated">Aqu&amp;iacute; hemos usado &lt;code&gt;kernel='gaussian'&lt;/code&gt; , como se vio arriba. Matem&amp;aacute;ticamente, un n&amp;uacute;cleo es una funci&amp;oacute;n positiva \ (K (x; h) \) que est&amp;aacute; controlada por el par&amp;aacute;metro de ancho de banda \ (h \). Dada esta forma de n&amp;uacute;cleo, la estimaci&amp;oacute;n de densidad en un punto \ (y \) dentro de un grupo de puntos \ (x_i; i = 1 \ cdots N \) viene dada por:</target>
        </trans-unit>
        <trans-unit id="9bc4f467db4b070f940a4ae98fc40a9d9951075c" translate="yes" xml:space="preserve">
          <source>Here we simulate independent sources using a highly non-Gaussian process, 2 student T with a low number of degrees of freedom (top left figure). We mix them to create observations (top right figure). In this raw observation space, directions identified by PCA are represented by orange vectors. We represent the signal in the PCA space, after whitening by the variance corresponding to the PCA vectors (lower left). Running ICA corresponds to finding a rotation in this space to identify the directions of largest non-Gaussianity (lower right).</source>
          <target state="translated">Aquí simulamos fuentes independientes usando un proceso altamente no-gausiano,2 estudiantes T con un bajo número de grados de libertad (figura superior izquierda).Los mezclamos para crear observaciones (figura superior derecha).En este espacio de observación en bruto,las direcciones identificadas por PCA están representadas por vectores naranja.Representamos la señal en el espacio de PCA,después de blanquearla por la varianza correspondiente a los vectores de PCA (abajo a la izquierda).Ejecutar el PCA corresponde a encontrar una rotación en este espacio para identificar las direcciones de mayor no gaussianismo (abajo a la derecha).</target>
        </trans-unit>
        <trans-unit id="72e463d9d9e7402af61cb976f70a41a655f66554" translate="yes" xml:space="preserve">
          <source>Here we use the caching property of pipelines to cache the nearest neighbors graph between multiple fits of KNeighborsClassifier. The first call is slow since it computes the neighbors graph, while subsequent call are faster as they do not need to recompute the graph. Here the durations are small since the dataset is small, but the gain can be more substantial when the dataset grows larger, or when the grid of parameter to search is large.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4efad2f65eb490631f07741acecbb3c6dbc45f0c" translate="yes" xml:space="preserve">
          <source>Here we use the l1 sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing l2 penalty instead.</source>
          <target state="translated">Aquí usamos la sparsidad l1 que reduce a cero los pesos de las características no informativas.Esto es bueno si el objetivo es extraer el vocabulario fuertemente discriminatorio de cada clase.Si el objetivo es obtener la mejor precisión de predicción,es mejor usar la penalización de l2 no inductora de la dispersión.</target>
        </trans-unit>
        <trans-unit id="0013bebf729b11b2c5dfc0313efdfb116ab3b0c5" translate="yes" xml:space="preserve">
          <source>Here we want to model the frequency &lt;code&gt;y = ClaimNb / Exposure&lt;/code&gt; conditionally on &lt;code&gt;X&lt;/code&gt; via a (scaled) Poisson distribution, and use &lt;code&gt;Exposure&lt;/code&gt; as &lt;code&gt;sample_weight&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="034b3c08e48de8757b6f1f86830d0869f87a8e39" translate="yes" xml:space="preserve">
          <source>Here, &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; is the parameter name of the nested estimator, in this case &lt;code&gt;base_estimator&lt;/code&gt;. If the meta-estimator is constructed as a collection of estimators as in &lt;code&gt;pipeline.Pipeline&lt;/code&gt;, then &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; refers to the name of the estimator, see &lt;a href=&quot;compose#pipeline-nested-parameters&quot;&gt;Nested parameters&lt;/a&gt;. In practice, there can be several levels of nesting:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0260adbe3be54cb933a36e08a92f87d76459f0fc" translate="yes" xml:space="preserve">
          <source>Here, \(\alpha \geq 0\) is a complexity parameter that controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="translated">Aquí,\ ~-es un parámetro de complejidad que controla la cantidad de la contracción:cuanto mayor sea el valor de \ ~-mayor es la cantidad de la contracción y por lo tanto los coeficientes se vuelven más robustos a la colinealidad.</target>
        </trans-unit>
        <trans-unit id="9412689bc5806775f9bf0419d0db25d5c39e9741" translate="yes" xml:space="preserve">
          <source>Here, the classifier is &lt;code&gt;fit()&lt;/code&gt; on a 2d binary label representation of &lt;code&gt;y&lt;/code&gt;, using the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt;&lt;code&gt;LabelBinarizer&lt;/code&gt;&lt;/a&gt;. In this case &lt;code&gt;predict()&lt;/code&gt; returns a 2d array representing the corresponding multilabel predictions.</source>
          <target state="translated">Aqu&amp;iacute;, el clasificador se &lt;code&gt;fit()&lt;/code&gt; en una representaci&amp;oacute;n de etiqueta binaria 2d de &lt;code&gt;y&lt;/code&gt; , usando &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt; &lt;code&gt;LabelBinarizer&lt;/code&gt; &lt;/a&gt; . En este caso, &lt;code&gt;predict()&lt;/code&gt; devuelve una matriz 2d que representa las predicciones de m&amp;uacute;ltiples etiquetas correspondientes.</target>
        </trans-unit>
        <trans-unit id="e42ec4b790491f01a91defa6334fdc833c4f6019" translate="yes" xml:space="preserve">
          <source>Here, the default kernel &lt;code&gt;rbf&lt;/code&gt; is first changed to &lt;code&gt;linear&lt;/code&gt; via &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt;&lt;code&gt;SVC.set_params()&lt;/code&gt;&lt;/a&gt; after the estimator has been constructed, and changed back to &lt;code&gt;rbf&lt;/code&gt; to refit the estimator and to make a second prediction.</source>
          <target state="translated">Aqu&amp;iacute;, el &lt;code&gt;rbf&lt;/code&gt; del kernel predeterminado se cambia primero a &lt;code&gt;linear&lt;/code&gt; trav&amp;eacute;s de &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt; &lt;code&gt;SVC.set_params()&lt;/code&gt; &lt;/a&gt; despu&amp;eacute;s de que se ha construido el estimador, y se cambia de nuevo a &lt;code&gt;rbf&lt;/code&gt; para reajustar el estimador y hacer una segunda predicci&amp;oacute;n.</target>
        </trans-unit>
        <trans-unit id="379b23b33ba6458bba2f568a4c2b136ad7c827a5" translate="yes" xml:space="preserve">
          <source>Here, the first &lt;code&gt;predict()&lt;/code&gt; returns an integer array, since &lt;code&gt;iris.target&lt;/code&gt; (an integer array) was used in &lt;code&gt;fit&lt;/code&gt;. The second &lt;code&gt;predict()&lt;/code&gt; returns a string array, since &lt;code&gt;iris.target_names&lt;/code&gt; was for fitting.</source>
          <target state="translated">Aqu&amp;iacute;, el primer &lt;code&gt;predict()&lt;/code&gt; devuelve una matriz de enteros, ya que &lt;code&gt;iris.target&lt;/code&gt; (una matriz de enteros) se us&amp;oacute; en &lt;code&gt;fit&lt;/code&gt; . El segundo &lt;code&gt;predict()&lt;/code&gt; devuelve una matriz de cadenas, ya que &lt;code&gt;iris.target_names&lt;/code&gt; fue para ajustar.</target>
        </trans-unit>
        <trans-unit id="21a97ae1e0557499a4ed4420c47199de8f6f0cde" translate="yes" xml:space="preserve">
          <source>Here, the number of samples is slightly larger than the number of dimensions, thus the empirical covariance is still invertible. However, as the observations are strongly correlated, the empirical covariance matrix is ill-conditioned and as a result its inverse &amp;ndash;the empirical precision matrix&amp;ndash; is very far from the ground truth.</source>
          <target state="translated">Aqu&amp;iacute;, el n&amp;uacute;mero de muestras es ligeramente mayor que el n&amp;uacute;mero de dimensiones, por lo que la covarianza emp&amp;iacute;rica sigue siendo invertible. Sin embargo, como las observaciones est&amp;aacute;n fuertemente correlacionadas, la matriz de covarianza emp&amp;iacute;rica est&amp;aacute; mal condicionada y, como resultado, su inversa, la matriz de precisi&amp;oacute;n emp&amp;iacute;rica, est&amp;aacute; muy lejos de la verdad fundamental.</target>
        </trans-unit>
        <trans-unit id="2c6a31e993187ebfe932ff15824a46e0c83fd078" translate="yes" xml:space="preserve">
          <source>Here, the predicted class label is 2, since it has the highest average probability.</source>
          <target state="translated">Aquí,la etiqueta de clase pronosticada es 2,ya que tiene la probabilidad media más alta.</target>
        </trans-unit>
        <trans-unit id="2ac251de8487bd51d665dc484131dc49cb351bf8" translate="yes" xml:space="preserve">
          <source>Here, the scores for the test data call for caution as they are significantly worse than for the training data indicating an overfit despite the strong regularization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4ef491b953b35546ea7174e484d0bf1fa4f7b9b" translate="yes" xml:space="preserve">
          <source>Here, we are penalizing samples whose prediction is at least \(\varepsilon\) away from their true target. These samples penalize the objective by \(\zeta_i\) or \(\zeta_i^*\), depending on whether their predictions lie above or below the \(\varepsilon\) tube.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3799f87043c16356967f90e71e555e0bd9e10d17" translate="yes" xml:space="preserve">
          <source>Here, we combine 3 learners (linear and non-linear) and use a ridge regressor to combine their outputs together.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a1c9125aac07396a0a83a0358b1364478df4bbe" translate="yes" xml:space="preserve">
          <source>Here, we plot the partial dependence curves for a single feature, &amp;ldquo;age&amp;rdquo;, on the same axes. In this case, &lt;code&gt;tree_disp.axes_&lt;/code&gt; is passed into the second plot function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69bdd751bbdbd7bb5746658b628d98b5240af47b" translate="yes" xml:space="preserve">
          <source>Here, we used the default hyperparameters for the gradient boosting model without any preprocessing as tree-based models are naturally robust to monotonic transformations of numerical features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="264995c0dc7ac7309d4709ed0ce1258e4439b015" translate="yes" xml:space="preserve">
          <source>Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, &lt;code&gt;sklearn&lt;/code&gt; implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, with the keyword &lt;code&gt;method = 'hessian'&lt;/code&gt;. It requires &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt;.</source>
          <target state="translated">El mapeo propio de Hessian (tambi&amp;eacute;n conocido como LLE: HLLE basado en Hessian) es otro m&amp;eacute;todo para resolver el problema de regularizaci&amp;oacute;n de LLE. Gira en torno a una forma cuadr&amp;aacute;tica basada en arpillera en cada vecindario que se utiliza para recuperar la estructura lineal local. Aunque otras implementaciones notan su escaso escalado con el tama&amp;ntilde;o de los datos, &lt;code&gt;sklearn&lt;/code&gt; implementa algunas mejoras algor&amp;iacute;tmicas que hacen que su costo sea comparable al de otras variantes de LLE para una dimensi&amp;oacute;n de salida peque&amp;ntilde;a. HLLE puede realizarse con la funci&amp;oacute;n &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt; o su contraparte orientada a objetos &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; , con la palabra clave &lt;code&gt;method = 'hessian'&lt;/code&gt; . Requiere &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4b22ade72c6627b1b562254b0df9c6b4d814d7ac" translate="yes" xml:space="preserve">
          <source>Hidden Activation sampled from the model distribution, where batch_size in the number of examples per minibatch and n_components is the number of hidden units.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afac02e66e409c4004e2cc2adafb5b5e842109eb" translate="yes" xml:space="preserve">
          <source>Hierarchical agglomerative clustering: Ward</source>
          <target state="translated">Agrupación aglomerada jerárquica:Ward</target>
        </trans-unit>
        <trans-unit id="09f7d65b121068e93f6d1d655d20b242aded6b7b" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia page&lt;/a&gt; for more details.</source>
          <target state="translated">La agrupaci&amp;oacute;n en cl&amp;uacute;steres jer&amp;aacute;rquica es una familia general de algoritmos de agrupaci&amp;oacute;n en cl&amp;uacute;steres que construyen cl&amp;uacute;steres anidados fusion&amp;aacute;ndolos o dividi&amp;eacute;ndolos sucesivamente. Esta jerarqu&amp;iacute;a de grupos se representa como un &amp;aacute;rbol (o dendrograma). La ra&amp;iacute;z del &amp;aacute;rbol es el grupo &amp;uacute;nico que re&amp;uacute;ne todas las muestras, siendo las hojas los grupos con una sola muestra. Consulte la &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;p&amp;aacute;gina de Wikipedia&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="dbe40063cd6e20f1519715e45afa5ca7ed6442de" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering: structured vs unstructured ward</source>
          <target state="translated">Agrupación jerárquica:sala estructurada vs.no estructurada</target>
        </trans-unit>
        <trans-unit id="645ba4388b8ba9172558b321f188082e4d2fd9ef" translate="yes" xml:space="preserve">
          <source>High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.</source>
          <target state="translated">Los conjuntos de datos de alta dimensión pueden ser muy difíciles de visualizar.Mientras que los datos en dos o tres dimensiones pueden ser trazados para mostrar la estructura inherente de los datos,los trazados equivalentes de alta dimensión son mucho menos intuitivos.Para ayudar a la visualización de la estructura de un conjunto de datos,la dimensión debe ser reducida de alguna manera.</target>
        </trans-unit>
        <trans-unit id="769f5c4cc60f755dfe8d93fd7b194f0c3a9b7156" translate="yes" xml:space="preserve">
          <source>Hinge (soft-margin): equivalent to Support Vector Classification. \(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75c18021e736bcb99a099c597122a642054caa8c" translate="yes" xml:space="preserve">
          <source>Hinge: (soft-margin) Support Vector Machines.</source>
          <target state="translated">Bisagra:(soft-margin)Máquinas Vectoriales de Apoyo.</target>
        </trans-unit>
        <trans-unit id="a319ae13863bb8d6da087a8b6e0305de9278e27f" translate="yes" xml:space="preserve">
          <source>Hinton, Geoffrey E.</source>
          <target state="translated">Hinton,Geoffrey E.</target>
        </trans-unit>
        <trans-unit id="04790fed22fa2f8c9274791cf963a575a884ed64" translate="yes" xml:space="preserve">
          <source>Hispanic</source>
          <target state="translated">Hispanic</target>
        </trans-unit>
        <trans-unit id="d6215d83f5c22f8bb7c1095fa0298c0dd2d51f9d" translate="yes" xml:space="preserve">
          <source>Histogram-based Gradient Boosting Classification Tree.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="772913778b88215dfb5f0c612cecbcae2c4b1a8f" translate="yes" xml:space="preserve">
          <source>Histogram-based Gradient Boosting Regression Tree.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8cd925555526484cde7ba65174f600e33250f6b" translate="yes" xml:space="preserve">
          <source>Hochreiter, Bodenhofer, et. al., 2010. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;FABIA: factor analysis for bicluster acquisition&lt;/a&gt;.</source>
          <target state="translated">Hochreiter, Bodenhofer y col. al., 2010. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;FABIA: an&amp;aacute;lisis factorial para la adquisici&amp;oacute;n de bicluster&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="cbd8a9628e732e150b1ccedb68c9c6ad4fe27c2c" translate="yes" xml:space="preserve">
          <source>Holds arrays of shape (n_classes, n_categories of respective feature) for each feature. Each array provides the empirical log probability of categories given the respective feature and class, &lt;code&gt;P(x_i|y)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0265e0dec8e2ff0952abf481f5094201e14a30d8" translate="yes" xml:space="preserve">
          <source>Holds arrays of shape (n_classes, n_categories of respective feature) for each feature. Each array provides the number of samples encountered for each class and category of the specific feature.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a5c3ac7ac58dcde82acc59c23c0dce0d31966d1" translate="yes" xml:space="preserve">
          <source>Holds the label for each class.</source>
          <target state="translated">Lleva la etiqueta de cada clase.</target>
        </trans-unit>
        <trans-unit id="4b7dceb5fe5f7e92199815a2b66fef8fdf05dc27" translate="yes" xml:space="preserve">
          <source>Homogeneity and completeness scores are formally given by:</source>
          <target state="translated">Los puntajes de homogeneidad e integridad son formalmente dados por:</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
