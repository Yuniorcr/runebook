<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="a5d4ce8f33e2308a786a65897ca2b53b361ed230" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.gcd#torch.gcd&quot;&gt;&lt;code&gt;torch.gcd()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47e2863133f746b04949765203596816d2a569fd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc3905a5552f33afb11f303bb93b1f92da393e87" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0f396c7008d561476ffb59f26b17018bc84fa73" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ger#torch.ger&quot;&gt;&lt;code&gt;torch.ger()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29a71ad0e5b1620b27e713b79b570ee4a9882fbd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.greater#torch.greater&quot;&gt;&lt;code&gt;torch.greater()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2aa63e3e1cb9df927bb8622309466c088861e7c1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.greater_equal#torch.greater_equal&quot;&gt;&lt;code&gt;torch.greater_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22d16c3e505067a772ddee47e68d0c5e9efdf1ea" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f797b96d9a976433ee5e82f85c212bf9cc6c07aa" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.heaviside#torch.heaviside&quot;&gt;&lt;code&gt;torch.heaviside()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4c88cbc91c3b48e1204ec2fc29b766aaa6e3c59" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.histc#torch.histc&quot;&gt;&lt;code&gt;torch.histc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f24053f673f97bf0cebaec0ddd1ab96fa9d420f2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.hypot#torch.hypot&quot;&gt;&lt;code&gt;torch.hypot()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a27de95d9ce95538faa35bfb5a5044535b471e7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.i0#torch.i0&quot;&gt;&lt;code&gt;torch.i0()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55baf0c2297883aeca1f8032e140f1dce62c7f54" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;torch.ifft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dbd53ee94265f26254d53b09c2158dbdc7c06b75" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.index_select#torch.index_select&quot;&gt;&lt;code&gt;torch.index_select()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91cb515b356d2c52d4004829619ffcf0284a140c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.inverse#torch.inverse&quot;&gt;&lt;code&gt;torch.inverse()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1921c02b416a3c35ca9a7616e742bedc37f85261" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.irfft#torch.irfft&quot;&gt;&lt;code&gt;torch.irfft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a61d72d732d7b3d2bfcfab031758abf3bd3f2bf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isclose#torch.isclose&quot;&gt;&lt;code&gt;torch.isclose()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d43646aed73f0e63bb4282297ddeb0953661084" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isfinite#torch.isfinite&quot;&gt;&lt;code&gt;torch.isfinite()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f01c4969936eb94939bfba08517f9dc5d3c9b65" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isinf#torch.isinf&quot;&gt;&lt;code&gt;torch.isinf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c15049f87e6457b76bc4c833aad4e86fc330a9a4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isnan#torch.isnan&quot;&gt;&lt;code&gt;torch.isnan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0c99b8516b26410ede0975c9d5e8303e6a56d72" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isneginf#torch.isneginf&quot;&gt;&lt;code&gt;torch.isneginf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c45c9bc98276b87a137a720f4ab4cbfc8bccf9d9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isposinf#torch.isposinf&quot;&gt;&lt;code&gt;torch.isposinf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bff4ba1100e12cf9a28654669a565681a03dd913" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isreal#torch.isreal&quot;&gt;&lt;code&gt;torch.isreal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcf71e098da4b8c28d983651980d01f5b1a6049b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.istft#torch.istft&quot;&gt;&lt;code&gt;torch.istft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f606579981b49ec0a8206d550a759168fd92964" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.kthvalue#torch.kthvalue&quot;&gt;&lt;code&gt;torch.kthvalue()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d8ba07cef0e3cd2ee953932e7401cfd0248e444" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lcm#torch.lcm&quot;&gt;&lt;code&gt;torch.lcm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6699a864923ddee28ba56b38069aaab083349df" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32c678d238915a61f701966540f011fe2fc9cbc1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lerp#torch.lerp&quot;&gt;&lt;code&gt;torch.lerp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1d6db8a3f3362c5bfd61e84e72244e0ee6bf8e9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.less#torch.less&quot;&gt;&lt;code&gt;torch.less()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8eea9085bfaff6760dd93361241db5a26d61c2c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.less_equal#torch.less_equal&quot;&gt;&lt;code&gt;torch.less_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cd2d05d4cdb6e340422b7b49a0f18ef4af47558" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lgamma#torch.lgamma&quot;&gt;&lt;code&gt;torch.lgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7caa6f91635bb60ab80ed9ca2541683e873e5da4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.log#torch.log&quot;&gt;&lt;code&gt;torch.log()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73a23f7411ea79d1413155727db906f0562b2082" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.log10#torch.log10&quot;&gt;&lt;code&gt;torch.log10()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8dc5bc392cc9737d55c7a288ffae377a0f80b8e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.log1p#torch.log1p&quot;&gt;&lt;code&gt;torch.log1p()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b442c9296b5b51dc12a6d62a9e946bae4e4cace4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.log2#torch.log2&quot;&gt;&lt;code&gt;torch.log2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82580a23e0c28a78662f5e93ee563089d4043db8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logaddexp#torch.logaddexp&quot;&gt;&lt;code&gt;torch.logaddexp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7856ea9cc5c5b34b3fad25b3ceb90619e2bcf1a1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logaddexp2#torch.logaddexp2&quot;&gt;&lt;code&gt;torch.logaddexp2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d244fcdaec339fa07bf0c8b02d8e5206a3756511" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logcumsumexp#torch.logcumsumexp&quot;&gt;&lt;code&gt;torch.logcumsumexp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e64767e412c54452d0e93a4095d3fb5a34e4a87d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logdet#torch.logdet&quot;&gt;&lt;code&gt;torch.logdet()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dae45426090891d1953ee3ad38c045f8a0fceedc" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logical_and#torch.logical_and&quot;&gt;&lt;code&gt;torch.logical_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d20544f0f1149d37dc67d5b4c2ec2fcf87602aaf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logical_not#torch.logical_not&quot;&gt;&lt;code&gt;torch.logical_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f1dd2cde87434261bce6932056bcd2beea83bc0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logical_or#torch.logical_or&quot;&gt;&lt;code&gt;torch.logical_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64c77ca5d503f49a2495909f4b01e01b4e55a15f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logical_xor#torch.logical_xor&quot;&gt;&lt;code&gt;torch.logical_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9fec990c19512cd7cbc5c7730d590537da8787a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logit#torch.logit&quot;&gt;&lt;code&gt;torch.logit()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7682310645eb1b3d8a10df47deccb4b498fc4809" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logsumexp#torch.logsumexp&quot;&gt;&lt;code&gt;torch.logsumexp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f577c3338ec6045533458e8c6be36ac9572aed9e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lstsq#torch.lstsq&quot;&gt;&lt;code&gt;torch.lstsq()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e411f1ff349b11bfd836c08df5971026b855e59a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="578639cb9fc16b15aa6b7e1967466fe9b09ff42d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lu#torch.lu&quot;&gt;&lt;code&gt;torch.lu()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c96a4514c11416811fe9f9b86d4dd2e1d0e4e1cd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lu_solve#torch.lu_solve&quot;&gt;&lt;code&gt;torch.lu_solve()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c6b2883c1c8d2d0de37ea2c95e5b1fa94069b4d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.masked_select#torch.masked_select&quot;&gt;&lt;code&gt;torch.masked_select()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4dc4224536b289cdb91dd6eb3b784c8632062f7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.matmul#torch.matmul&quot;&gt;&lt;code&gt;torch.matmul()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18b30626e1ae4fc77dd2baf1eea08d60901e4a44" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.matrix_exp#torch.matrix_exp&quot;&gt;&lt;code&gt;torch.matrix_exp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ceb6fc4ba32f1c6513652ba41e7ed5365209b9d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.matrix_power#torch.matrix_power&quot;&gt;&lt;code&gt;torch.matrix_power()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3d3a41458bccec6e7e87d98e4ad75ec5c4237fb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.max#torch.max&quot;&gt;&lt;code&gt;torch.max()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="689831f5600395b5cce9e382345788eac42fd2b3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.maximum#torch.maximum&quot;&gt;&lt;code&gt;torch.maximum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75cde60dd542a6062dc76781c6f95cef4726b299" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt;&lt;code&gt;torch.mean()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="106c7cf7d53f462e1efa14592e3a206e193353ed" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.median#torch.median&quot;&gt;&lt;code&gt;torch.median()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45e4b29591dc1fd5fe60cf3f1a2528339eb79d7a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.min#torch.min&quot;&gt;&lt;code&gt;torch.min()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71ff8dd28d5ec4bba7fa90d123381bf318d57316" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.minimum#torch.minimum&quot;&gt;&lt;code&gt;torch.minimum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f88fe431125507817f53eadcab5b7cf0486d3c3a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mm#torch.mm&quot;&gt;&lt;code&gt;torch.mm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89a5d4426d8c3c83f76b02d2a04a8fa74cd9c984" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mode#torch.mode&quot;&gt;&lt;code&gt;torch.mode()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="480d2c05ebbc86790a491b5029a1882724ab97f9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.movedim#torch.movedim&quot;&gt;&lt;code&gt;torch.movedim()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e47d65e128408a5146c2909f05667f8df18f4de" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="346834db1e953738900b156dccde72730c321179" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.multinomial#torch.multinomial&quot;&gt;&lt;code&gt;torch.multinomial()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2abc253dc35a3f16344ec6f7c0d4b4f3f6f389c0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.multiply#torch.multiply&quot;&gt;&lt;code&gt;torch.multiply()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd83a3f33cb0434d70d1281178c10f4d4186288c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mv#torch.mv&quot;&gt;&lt;code&gt;torch.mv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7ada1c677f5eab787ec7f543df6913243851e42" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mvlgamma#torch.mvlgamma&quot;&gt;&lt;code&gt;torch.mvlgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96ec4016de71e2ca1618c82409f4cbcb9c612e86" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nanquantile#torch.nanquantile&quot;&gt;&lt;code&gt;torch.nanquantile()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eda1f1134fb78245db2a5b46d9c8dcff19e60476" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nansum#torch.nansum&quot;&gt;&lt;code&gt;torch.nansum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2aaeb0cd2a053ef536d53c256571aadbdbf969ca" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.narrow#torch.narrow&quot;&gt;&lt;code&gt;torch.narrow()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32b6bd21755f0280c169e12a964b6d0d8f9f944d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef00c00cfc229955de1af61e47329e20da955299" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c59bed38407778d4b3b65157e276bcba44fa481e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.negative#torch.negative&quot;&gt;&lt;code&gt;torch.negative()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab18f54431cb99ac38d5579314003713c8aef19f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nextafter#torch.nextafter&quot;&gt;&lt;code&gt;torch.nextafter()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="211dc318b0ac18e597f00d3389f89e2fa69af8fd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d&quot;&gt;&lt;code&gt;AdaptiveAvgPool1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c297648804959d7e566eb6462302a062556e0a26" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d&quot;&gt;&lt;code&gt;AdaptiveAvgPool2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e3e0fe3b85f801c746e8e3435b99b12af2758cb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d&quot;&gt;&lt;code&gt;AdaptiveAvgPool3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcb4c522bcd653244a2b0e212d2a6c1dec943575" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d&quot;&gt;&lt;code&gt;AdaptiveMaxPool1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9197dd28b388a9ad018e4140bcb03f05bef22fce" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d&quot;&gt;&lt;code&gt;AdaptiveMaxPool2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="671fa746ed1d96f6e5c409933c093bf281ece232" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d&quot;&gt;&lt;code&gt;AdaptiveMaxPool3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06fcc12676bdce703006b9934a46a5d88c4ea69e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.alphadropout#torch.nn.AlphaDropout&quot;&gt;&lt;code&gt;AlphaDropout&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="299a82bba9fc9f3c48bfd00d90ea771941bb3509" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.avgpool1d#torch.nn.AvgPool1d&quot;&gt;&lt;code&gt;AvgPool1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e431c47012b760f030d5cd2cfeebdfaaa73e3a4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.avgpool2d#torch.nn.AvgPool2d&quot;&gt;&lt;code&gt;AvgPool2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d5e8ddb26e145fc6c6ccd8ea827c9249bd3e2c2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.avgpool3d#torch.nn.AvgPool3d&quot;&gt;&lt;code&gt;AvgPool3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2f2febc074ac5ab0c26eb12dab1477a725d346c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d&quot;&gt;&lt;code&gt;BatchNorm1d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt;&lt;code&gt;BatchNorm3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49181f1a5d0f855f350c509add5501b487b8de9a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.bceloss#torch.nn.BCELoss&quot;&gt;&lt;code&gt;BCELoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9b2acc69b3141658463c5b48ed119f653d953fc" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss&quot;&gt;&lt;code&gt;BCEWithLogitsLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e05c2119de76670ff15f68392d6126a185752ff" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.celu#torch.nn.CELU&quot;&gt;&lt;code&gt;CELU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46a2836ec87b23dd081f1f557c9d269b806abf39" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d&quot;&gt;&lt;code&gt;torch.nn.ConstantPad2d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d&quot;&gt;&lt;code&gt;torch.nn.ReflectionPad2d&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d&quot;&gt;&lt;code&gt;torch.nn.ReplicationPad2d&lt;/code&gt;&lt;/a&gt; for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Replicate padding is implemented for padding the last 3 dimensions of 5D input tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor. Reflect padding is only implemented for padding the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92e4bf1e1580e8ab1c867f9318919922f5d69097" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb3d45384af0f65dcc256fd4e526a226237028c3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; for other attributes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d1d57fe3bdff062ac6991806c22d029393fd38a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d165cb34672d664f9dd17ceb8fa4d5cf8092ba2a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; for other attributes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62b640305a360b502c1c6f7e2a7cc59219562d5b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="827a3d0d159d0163f089c1b8b07697a3b4d204b1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; for other attributes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c6215c6031dbc3057e738f03ebc6ccb657c175c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d&quot;&gt;&lt;code&gt;ConvTranspose1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92a07f307c29825975eb91630c55dc6f1ef73449" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d&quot;&gt;&lt;code&gt;ConvTranspose2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26d37ceb58b322f40fdbdfcf86ed7b27d34f3ea4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d&quot;&gt;&lt;code&gt;ConvTranspose3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ecbef13440cb7e3c72978fb3f953d0ea4a05dec" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss&quot;&gt;&lt;code&gt;CosineEmbeddingLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07cebd07469d5659a87059395414105b3f0a54c1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss&quot;&gt;&lt;code&gt;CrossEntropyLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed9307322caa5510dafcec4ab6a6439e46cca0ac" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.ctcloss#torch.nn.CTCLoss&quot;&gt;&lt;code&gt;CTCLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90c67074e8fb1e5903e2b6ceed293a19589c2504" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.dropout#torch.nn.Dropout&quot;&gt;&lt;code&gt;Dropout&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b5e6bf0d87dc5eedbd14a70bf17165953b58726" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.dropout2d#torch.nn.Dropout2d&quot;&gt;&lt;code&gt;Dropout2d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9181ef4cdcda4e21d4c1d578bf817479c3e73473" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.dropout3d#torch.nn.Dropout3d&quot;&gt;&lt;code&gt;Dropout3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9638040580d53d7a62b5948dfef978c66524b9c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt;&lt;code&gt;ELU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9516b3d99ccc49e5403a54c4cd4186fa2ebff368" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.embedding#torch.nn.Embedding&quot;&gt;&lt;code&gt;torch.nn.Embedding&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0ef85f5e373bd3344bdb431e1a684be4a2c111f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag&quot;&gt;&lt;code&gt;torch.nn.EmbeddingBag&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87b8eae451b415ea1f7e39fa3cca366d80225aa8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.fold#torch.nn.Fold&quot;&gt;&lt;code&gt;torch.nn.Fold&lt;/code&gt;&lt;/a&gt; for details</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3f647c2cb1152f6be057eaec2f333adc27b7848" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.hardshrink#torch.nn.Hardshrink&quot;&gt;&lt;code&gt;Hardshrink&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e54bc653f24897904f35291d48d7a4d3e1b15dd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid&quot;&gt;&lt;code&gt;Hardsigmoid&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d8089dc9fcfe5932254ec4b47d12c1cbe02d107" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt;&lt;code&gt;Hardswish&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79e7f193eb69518b7c2bc43975107d6c9f8a543d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss&quot;&gt;&lt;code&gt;HingeEmbeddingLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a07f37d494d50f907a7a0be59a0004e665fc42f6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt;&lt;code&gt;InstanceNorm1d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt;&lt;code&gt;InstanceNorm2d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt;&lt;code&gt;InstanceNorm3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc584ec8326a5572f69ad2a713d36f3294777bbf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.kldivloss#torch.nn.KLDivLoss&quot;&gt;&lt;code&gt;KLDivLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82170e2aa45b79e0abc6968e9d7a135946023f5a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.l1loss#torch.nn.L1Loss&quot;&gt;&lt;code&gt;L1Loss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d19657b2b5b68544134330ee366c7bfea1457eb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt;&lt;code&gt;LayerNorm&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2bef43e82a39e507cbf89ef28c4b26fd78760fc7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.leakyrelu#torch.nn.LeakyReLU&quot;&gt;&lt;code&gt;LeakyReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2764126c51f5530622c4b4e091ebc79776ade07c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm&quot;&gt;&lt;code&gt;LocalResponseNorm&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e31635636f40648e7e735d06a4392b7cebd2b258" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.logsigmoid#torch.nn.LogSigmoid&quot;&gt;&lt;code&gt;LogSigmoid&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="412a8150cd54d51d6530e2d245b63346bf33130a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt;&lt;code&gt;LogSoftmax&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f565430e626357bd4afefcb8e556e2cb25e9480" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.lppool1d#torch.nn.LPPool1d&quot;&gt;&lt;code&gt;LPPool1d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35b0452fd507c2aa91e1da43aa355009dc166d13" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.lppool2d#torch.nn.LPPool2d&quot;&gt;&lt;code&gt;LPPool2d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cbda61d0da4455a2876aa2b87f22199f2f1cdb3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss&quot;&gt;&lt;code&gt;MarginRankingLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="898e6a57e299ae9b57ccd5993abb1ef8bd55140b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt;&lt;code&gt;MaxPool1d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0dc529ce91ee71820fbf1ed9bed0661ddc036f6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt;&lt;code&gt;MaxPool2d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6be4f2f66ae6f0a242da37cad7e7f70f4a8e328" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt;&lt;code&gt;MaxPool3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5de0ae10e2d087fd9d49043502233c22cd7d1ed" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d&quot;&gt;&lt;code&gt;MaxUnpool1d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06bc4bd975e3b0441df74b337a9e03841cdc0854" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d&quot;&gt;&lt;code&gt;MaxUnpool2d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be3c23907ec483dcbebdd45f57208e45ff569446" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d&quot;&gt;&lt;code&gt;MaxUnpool3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="787cf804e853557c82df78674a398c819b20b9c8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.mseloss#torch.nn.MSELoss&quot;&gt;&lt;code&gt;MSELoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01976be184fbec1b413106ad9e281456840b581b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss&quot;&gt;&lt;code&gt;MultiLabelMarginLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7aa5a41923f45ec7053a3fcb00c51f4757953233" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss&quot;&gt;&lt;code&gt;MultiLabelSoftMarginLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1b726bdc0737f6680b51d100e20056a744ff1e6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss&quot;&gt;&lt;code&gt;MultiMarginLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="729c99354cfb624e609e3f72dffceaad9cd0847c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;NLLLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ee9817c275d3255a1242d0c82f69825ae4ec439" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance&quot;&gt;&lt;code&gt;torch.nn.PairwiseDistance&lt;/code&gt;&lt;/a&gt; for details</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="100c5c298a4adf254a044824d7e8f1c566ed72d4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle&quot;&gt;&lt;code&gt;PixelShuffle&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62b7addc0514a1405a79ca49e242813d820bc663" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss&quot;&gt;&lt;code&gt;PoissonNLLLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50b2df5a36c2945c6e24d5c71f419748722a3887" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.prelu#torch.nn.PReLU&quot;&gt;&lt;code&gt;PReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5978d8e05fcfe3f5b05baa41628058a1104f3a4e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.relu6#torch.nn.ReLU6&quot;&gt;&lt;code&gt;ReLU6&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f52859f67be89a0431749df85025dfa79a6a4c5" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.rrelu#torch.nn.RReLU&quot;&gt;&lt;code&gt;RReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="852d4ca82259db7394eed56f5e5c7736dab64bcf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.selu#torch.nn.SELU&quot;&gt;&lt;code&gt;SELU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13f47034bf13462a634f77a02e2bd8afda50269b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.sigmoid#torch.nn.Sigmoid&quot;&gt;&lt;code&gt;Sigmoid&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="568ac6254f20261c85c9d016122513b35b9c681e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.silu#torch.nn.SiLU&quot;&gt;&lt;code&gt;SiLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd919bcb7cd8684a3301f3776235810cef864477" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss&quot;&gt;&lt;code&gt;SmoothL1Loss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aec2f6e607f94f542eec97b84c6a6b548899b489" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss&quot;&gt;&lt;code&gt;SoftMarginLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c76fd9ca241b6dc4f7f601122680872738372dd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softmax#torch.nn.Softmax&quot;&gt;&lt;code&gt;Softmax&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6858b7405b4fb52e94900af334a9f8d7d36cdcef" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softmin#torch.nn.Softmin&quot;&gt;&lt;code&gt;Softmin&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3078ba39159561c5fdab61a5073beb5e6d358df8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softplus#torch.nn.Softplus&quot;&gt;&lt;code&gt;Softplus&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="169ff197b89985d58ad2ce1e9b54fb8b6c45231c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softshrink#torch.nn.Softshrink&quot;&gt;&lt;code&gt;Softshrink&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdc172b1efd10ce5727c58366e9d3635597d6ae5" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softsign#torch.nn.Softsign&quot;&gt;&lt;code&gt;Softsign&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29958016ecc903cab7fa7360e164fabacfbf9cca" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.tanh#torch.nn.Tanh&quot;&gt;&lt;code&gt;Tanh&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a254e517f98f621d085febfde8c38a66e6d36e75" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.tanhshrink#torch.nn.Tanhshrink&quot;&gt;&lt;code&gt;Tanhshrink&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35902a0c2cfa9e585fec0c3e3666693fec9dd4a9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.threshold#torch.nn.Threshold&quot;&gt;&lt;code&gt;Threshold&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ce6b129bc423f40aba54b0c2ce12c00beddaaaf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss&quot;&gt;&lt;code&gt;TripletMarginLoss&lt;/code&gt;&lt;/a&gt; for details</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3fff4676883a1b11b9e2b38b57ad823ba8354c4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss&quot;&gt;&lt;code&gt;TripletMarginWithDistanceLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6ce3d99c47d2a3129d44cea56b1d906e3f87b1a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.unfold#torch.nn.Unfold&quot;&gt;&lt;code&gt;torch.nn.Unfold&lt;/code&gt;&lt;/a&gt; for details</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9788bdbd224f6b9c0bd977e955f549886daf1bcf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nonzero#torch.nonzero&quot;&gt;&lt;code&gt;torch.nonzero()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2369f591ef4e6e16ec27b6f775edd376eeeaebaa" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.norm#torch.norm&quot;&gt;&lt;code&gt;torch.norm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56e7e5c8709e5c8e2f93b31866c6deb6c856253e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.not_equal#torch.not_equal&quot;&gt;&lt;code&gt;torch.not_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b9ce8340dc0317b49aade691cfc70799f4e8a00" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.numel#torch.numel&quot;&gt;&lt;code&gt;torch.numel()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4622b4f7c7ee0a0d13c8074a55141e4ff3229c8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.orgqr#torch.orgqr&quot;&gt;&lt;code&gt;torch.orgqr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1edcc9509ea3d17ee6a647709ab6bf5071894e7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ormqr#torch.ormqr&quot;&gt;&lt;code&gt;torch.ormqr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ec140159032ba8d0085d266f44eb6db3c1a6341" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87e19d2ae8154d45a564a41febb06c16a86567e9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.pinverse#torch.pinverse&quot;&gt;&lt;code&gt;torch.pinverse()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7324cc21e0ad12841d92431a5e1818fae17672c9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.polygamma#torch.polygamma&quot;&gt;&lt;code&gt;torch.polygamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60b1f75853d0d1cd9cd5f0d2d089d7ee6125b0df" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.pow#torch.pow&quot;&gt;&lt;code&gt;torch.pow()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b8e7716730933beddce93ceeb9531967a7b8f33" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.prod#torch.prod&quot;&gt;&lt;code&gt;torch.prod()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b444d47f2174704778737bc705448fc08b1ec182" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.qr#torch.qr&quot;&gt;&lt;code&gt;torch.qr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c72437f61c64c17a7e0b66bf3ffe01e9afc41be" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32fb1a354f8c66ea4a0848048572ffcf87e84e6f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.rad2deg#torch.rad2deg&quot;&gt;&lt;code&gt;torch.rad2deg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc4ab991cb3b46b0132c1ba3ff2b91cc0500fdf4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.reciprocal#torch.reciprocal&quot;&gt;&lt;code&gt;torch.reciprocal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfe7f00978e3613a7a554678c8956f4cb13251a3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.remainder#torch.remainder&quot;&gt;&lt;code&gt;torch.remainder()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47bdb2571c3f363d76dceb91442f7ba7bc5f47c8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.renorm#torch.renorm&quot;&gt;&lt;code&gt;torch.renorm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="392bb5e2b00d5ad1124b111f4a08963b839d220b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.repeat_interleave#torch.repeat_interleave&quot;&gt;&lt;code&gt;torch.repeat_interleave()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d77b448fa6a312c6a3af7702ed4407f9a96ffff" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt;&lt;code&gt;torch.reshape()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64c78ad14ae1246bf4029f36289f782990d17cc2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;torch.rfft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="edc899a0ae7fde4858b6a50cce775b0372b7653c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.roll#torch.roll&quot;&gt;&lt;code&gt;torch.roll()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7815d433685f994781f0429772db78da8710e65c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.rot90#torch.rot90&quot;&gt;&lt;code&gt;torch.rot90()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0947a333f5346f54b736a4223ca159d84235f8b5" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.round#torch.round&quot;&gt;&lt;code&gt;torch.round()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3f439c8c8008d964e1bcaba78d49e70b3cb5ee0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.rsqrt#torch.rsqrt&quot;&gt;&lt;code&gt;torch.rsqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f1a0ad46bcdd4a06fd3e3e914c5990d45babeaf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sigmoid#torch.sigmoid&quot;&gt;&lt;code&gt;torch.sigmoid()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d03b7125b49df42263bd14ed0db4426d77994382" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sign#torch.sign&quot;&gt;&lt;code&gt;torch.sign()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0966c7edc653b2d2d7858c36c7f0e9e1d3aea910" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.signbit#torch.signbit&quot;&gt;&lt;code&gt;torch.signbit()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="241d2b853c66fa249e8726729a4ac39a10b2e697" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sin#torch.sin&quot;&gt;&lt;code&gt;torch.sin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89a061ad1e618c20982077437a8a87568c9307c4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sinh#torch.sinh&quot;&gt;&lt;code&gt;torch.sinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a499f1efa1ffa37cdc83e453614c6da1a94917dc" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.slogdet#torch.slogdet&quot;&gt;&lt;code&gt;torch.slogdet()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b94d091f48db8b1ed862846a551122f839916ce3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.solve#torch.solve&quot;&gt;&lt;code&gt;torch.solve()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7969c92902441e24ef604b1fb7e01e1633b60a0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sort#torch.sort&quot;&gt;&lt;code&gt;torch.sort()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec7801816e74ec7a3d900fbf20c476c205206e5a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.split#torch.split&quot;&gt;&lt;code&gt;torch.split()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39e5cf9eb721bd472b296db97e93a6ca162d7a75" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sqrt#torch.sqrt&quot;&gt;&lt;code&gt;torch.sqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e20283dbffe987cb9aa4c639a5d348b483031bd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.square#torch.square&quot;&gt;&lt;code&gt;torch.square()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4537022c26f4e739fc2002bc8948554d909b1ec0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b79329d6e4ba347ba65f0ebd150eb9025549e650" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.std#torch.std&quot;&gt;&lt;code&gt;torch.std()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8072306c8cd74c97cd95675a2ff3d6b6cdfaa602" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18ce7dc42c983592bcaecd4a3fc5855fdfc8d9ab" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af5eb1862625e7193f970d356cf7c80cbe6b8474" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.subtract#torch.subtract&quot;&gt;&lt;code&gt;torch.subtract()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66057444812a0310739be9c55bf3b8e63f04f931" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sum#torch.sum&quot;&gt;&lt;code&gt;torch.sum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bcf75355dab89c53d2da96f041aaf2f0acf7b6d2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.svd#torch.svd&quot;&gt;&lt;code&gt;torch.svd()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dfa85538b9855279675cb6cbe04a9c51aa801f91" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.symeig#torch.symeig&quot;&gt;&lt;code&gt;torch.symeig()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c1d54d9efffe6c88816f66acc03d8d8dfbcb987" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.t#torch.t&quot;&gt;&lt;code&gt;torch.t()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed0de6f96e833610021fd49c3f7874b4e1addb21" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.take#torch.take&quot;&gt;&lt;code&gt;torch.take()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99e605d53360cc080a69f4f5d980d95d7aa826ad" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.tan#torch.tan&quot;&gt;&lt;code&gt;torch.tan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e96f7967fc1e38f0303eea058aa618aef4fac2ef" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.tanh#torch.tanh&quot;&gt;&lt;code&gt;torch.tanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="777022f41fcab6d3a8b8bc1104eab2f51f7b9171" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.topk#torch.topk&quot;&gt;&lt;code&gt;torch.topk()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c49ff88894110989a51d17dc1b799b6a1d234266" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.trace#torch.trace&quot;&gt;&lt;code&gt;torch.trace()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4dd4e10bc975adcbd79cbfba2536b23c1ca9fe5d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.transpose#torch.transpose&quot;&gt;&lt;code&gt;torch.transpose()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2bade69272d3a8a621109ac0c3cebe47f8592ccf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.triangular_solve#torch.triangular_solve&quot;&gt;&lt;code&gt;torch.triangular_solve()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f87e190394989d10b94b8ed6f12d07725726bba2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.tril#torch.tril&quot;&gt;&lt;code&gt;torch.tril()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d172b311d0d769ff2c0ed9dd6ac700f12b7519eb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.triu#torch.triu&quot;&gt;&lt;code&gt;torch.triu()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7d12fec787dfa2633076e0c7bc951ad201eab36" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.true_divide#torch.true_divide&quot;&gt;&lt;code&gt;torch.true_divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e33491c2346b21653dcb58517e74623e00bc1cf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b23641ab143374e2f32432ecf86e4e46888de3d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.unbind#torch.unbind&quot;&gt;&lt;code&gt;torch.unbind()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19b180c6110b55208ea1c8b65bc50f246a5ff942" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.unique#torch.unique&quot;&gt;&lt;code&gt;torch.unique()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54a43b1289dd07e0a5b73c772257b5c512989147" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd1faf4696524ab565ebe96977ff8f6d3996368c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.unsqueeze#torch.unsqueeze&quot;&gt;&lt;code&gt;torch.unsqueeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4056678fae5769fa6b3f1cdb6156ce7a2397a43" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.var#torch.var&quot;&gt;&lt;code&gt;torch.var()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="916b508c9cb38c716d16bc8c991ff57f921295f9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.vdot#torch.vdot&quot;&gt;&lt;code&gt;torch.vdot()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b5ecc20fdf55d0eb2ca422795c8c3780a985baa" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1602.07868&quot;&gt;https://arxiv.org/abs/1602.07868&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b9412e01247af2b574511c1ccac61f158442f0f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;&gt;Gaussian Error Linear Units (GELUs)&lt;/a&gt; where the SiLU (Sigmoid Linear Unit) was originally coined, and see &lt;a href=&quot;https://arxiv.org/abs/1702.03118&quot;&gt;Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1710.05941v1&quot;&gt;Swish: a Self-Gated Activation Function&lt;/a&gt; where the SiLU was experimented with later.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14f3ddc89b63e2e7ace4c43afa51eb155bb8b073" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;&gt;Gaussian Error Linear Units (GELUs)&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd1bf1046086c393add977bf1d0da3c9f0372ead" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1612.08083&quot;&gt;Language Modeling with Gated Convolutional Networks&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db07a2a989e739b25605b6e4d0716eb84e5dae0a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1802.05957&quot;&gt;Spectral Normalization for Generative Adversarial Networks&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a6dff82856c75a203e206e75be968948b922059" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/test/test_cpp_extensions.py&quot;&gt;the tests&lt;/a&gt; for good examples of using this function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04429ddfa22962a6b5135b0b2014ba16416b5a3a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-management&quot;&gt;Memory management&lt;/a&gt; for more details about GPU memory management.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17c133c8d2a0e8bbfe7a62feb9aac8d574c244ed" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACK documentation for geqrf&lt;/a&gt; for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21fd55c2238e14aca1f89595f1ab24153a2317e2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;jit_unsupported#jit-unsupported&quot;&gt;TorchScript Unsupported Pytorch Constructs&lt;/a&gt; for a list of unsupported PyTorch functions and modules.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="867beb050ab38870ddc832eb3ecef0bd481ecc8d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors operator coverage&lt;/a&gt; for a full list of the supported torch and tensor operations. We do not yet support the following that is not covered by the link:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1babcf3a21d12db47391affdb4c96e1dd62be4d6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;nn.functional#torch.nn.functional.hardshrink&quot;&gt;&lt;code&gt;torch.nn.functional.hardshrink()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1db234df13e3a4d61ca75e6b5a3105bfac530a9e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;nn.functional#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt; for implementation details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b45b38436f82adea06622961c718c298fe2b5459" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.jit.save#torch.jit.save&quot;&gt;&lt;code&gt;torch.jit.save&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="906e8a62d5d0063bfb8aed76d72645065a77e807" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt;&lt;code&gt;torch.jit.trace&lt;/code&gt;&lt;/a&gt; for more information on tracing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="777f4ad759227cc1de547b64fab825a56ffc273e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.maximum#torch.maximum&quot;&gt;&lt;code&gt;torch.maximum()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee5434d3ddff1ae2016c291b61569fbc56d53430" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.minimum#torch.minimum&quot;&gt;&lt;code&gt;torch.minimum()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0be965581dce920964b073625fcdd3abb3431e9c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt; for details on conjugate symmetry.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72e0d158f46758840a4164068e9e82210cb8c350" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;AdaptiveAvgPool2d&lt;/code&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9940c6d70818d17787385090e557dd2c785cc737" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;AvgPool2d&lt;/code&gt; for details and output shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f13aa780c2216fa1121984d0068e8f5c43a24fb7" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;FeatureAlphaDropout&lt;/code&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d51e9069af161e98bae58fb7454beeacc6c7dd49" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;MaxPool2d&lt;/code&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe5ba62143f025d44dc65bfc7175cc40e26dc5a1" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;detect_anomaly&lt;/code&gt; above for details of the anomaly detection behaviour.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8c535644744dd427c75006813886670b5ecbefc" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;torch.sgn()&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d8243a2c0e464492c9d563c4f92c56ae3421bcc" translate="yes" xml:space="preserve">
          <source>See also</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf84761b907cce8756615b37c03c0de477c6797c" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.bernoulli&quot;&gt;&lt;code&gt;bernoulli()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.bernoulli#torch.bernoulli&quot;&gt;&lt;code&gt;torch.bernoulli()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f81c151298858c7ededc5c4b44ed15fc0e739e1" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.dense_dim&quot;&gt;&lt;code&gt;Tensor.dense_dim()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a95f7e982847b8fe1f244ef9a89f823deb4ea379" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.indices&quot;&gt;&lt;code&gt;Tensor.indices()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5404c54976e3ca510ce12e37b5127349f618c641" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.sparse_dim&quot;&gt;&lt;code&gt;Tensor.sparse_dim()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54ff5675a07df4058683c73c3542b2001d40f9cf" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.values&quot;&gt;&lt;code&gt;Tensor.values()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7cb8cf286ca9cd05efe1662c9601fe839849b83" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;https://en.wikipedia.org/wiki/One-hot&quot;&gt;One-hot on Wikipedia&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b57d4272104e4b0612e05446cf15e6f3bd6c435" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss&quot;&gt;&lt;code&gt;TripletMarginLoss&lt;/code&gt;&lt;/a&gt;, which computes the triplet loss for input tensors using the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="338b9a67a86b6e51ea1d6e3591262fbb1f90e795" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss&quot;&gt;&lt;code&gt;TripletMarginWithDistanceLoss&lt;/code&gt;&lt;/a&gt;, which computes the triplet margin loss for input tensors using a custom distance function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be5e1942829e9e077f50ab2375a2b7c88388f6d1" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;torch.nonzero#torch.nonzero&quot;&gt;&lt;code&gt;torch.nonzero()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3944875c16171bc90184fc5b9486bebdfeaab438" translate="yes" xml:space="preserve">
          <source>See also: &lt;a href=&quot;../distributed#distributed-basics&quot;&gt;Basics&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-nn-ddp-instead&quot;&gt;Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel&lt;/a&gt;. The same constraints on input as in &lt;a href=&quot;torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;torch.nn.DataParallel&lt;/code&gt;&lt;/a&gt; apply.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57828c6fcf489090610d5b402c9d6f3f4fda9c73" translate="yes" xml:space="preserve">
          <source>See also: &lt;a href=&quot;generated/torch.multinomial#torch.multinomial&quot;&gt;&lt;code&gt;torch.multinomial()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7eb108ecfdf49da8d98dacb8dde1315fef208724" translate="yes" xml:space="preserve">
          <source>See also: &lt;code&gt;saving-loading-tensors&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8334cf7461d5baada537c0ffa23cfd90187f5e4" translate="yes" xml:space="preserve">
          <source>See also: &lt;code&gt;torch.distributions.Categorical()&lt;/code&gt; for specifications of &lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6eaee759e08dae96ac3e7296ea90bee9503c008c" translate="yes" xml:space="preserve">
          <source>See below for examples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f51e613b80a9cad0a8955652d10c4dc58a70617" translate="yes" xml:space="preserve">
          <source>See below for more details on the two behaviors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ddb6a37aae13cb0b24c2efb3a4dab06796b836f" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html&quot;&gt;cuDNN 8 Release Notes&lt;/a&gt; for more information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8eeaf16a4b3b3eff601df5e984656d59eacad769" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-examples&quot;&gt;Automatic Mixed Precision examples&lt;/a&gt; for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f064248b1c5b1747c5f3357f6ac931ae0fbb78e9" translate="yes" xml:space="preserve">
          <source>See the Note on extending the autograd engine for more details on how to use this class: &lt;a href=&quot;https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd&quot;&gt;https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3f25968fb38aca8940c3474e1a28527b6999cfd" translate="yes" xml:space="preserve">
          <source>See the example below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81019ba57a197ef19b2f977fef5caa9379d67190" translate="yes" xml:space="preserve">
          <source>See: &lt;a href=&quot;https://arxiv.org/pdf/1505.00853.pdf&quot;&gt;https://arxiv.org/pdf/1505.00853.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0155b122fd91b7c13b804a1099ae2d088140dfb" translate="yes" xml:space="preserve">
          <source>Semantic Segmentation</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c75579406c3ae78822e3ffcc9e4cc60ff251dd2d" translate="yes" xml:space="preserve">
          <source>Sender rank -1, if not part of the group</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17a32bd6e1dfb35f3140b1db64642f975712c5d9" translate="yes" xml:space="preserve">
          <source>Sends a tensor asynchronously.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c841aadee573fffcf4298f38f9bffb444801dab" translate="yes" xml:space="preserve">
          <source>Sends a tensor synchronously.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0edc0112db95758e96c3f9613104687a95e00afc" translate="yes" xml:space="preserve">
          <source>Sequential</source>
          <target state="translated">Sequential</target>
        </trans-unit>
        <trans-unit id="e7d74cd050fb66a4fa3c59e667aee9ae0ee10227" translate="yes" xml:space="preserve">
          <source>Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/a&gt; manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89282363da677473acdbf45f0831bde9f4d3b6c4" translate="yes" xml:space="preserve">
          <source>Serialization</source>
          <target state="translated">Serialization</target>
        </trans-unit>
        <trans-unit id="086bcd13e430cfb2d3607712e21df8c908da9204" translate="yes" xml:space="preserve">
          <source>Serialization semantics</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49d31523ecd7343c1a446e5158bebd854caf4035" translate="yes" xml:space="preserve">
          <source>Set options for printing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da649b091863ea75252b37be60743b57866836c1" translate="yes" xml:space="preserve">
          <source>Set options for printing. Items shamelessly taken from NumPy</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f8fce6ab85b47c6992c9a753d4c65a4df880bed" translate="yes" xml:space="preserve">
          <source>Set the extra representation of the module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd4c737e6e6f369967a01656e378372e2b057847" translate="yes" xml:space="preserve">
          <source>Set the learning rate of each parameter group using a cosine annealing schedule, where</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb1d30c28b5d6e9bf862978770dcfc4108953994" translate="yes" xml:space="preserve">
          <source>Set the result for this &lt;code&gt;Future&lt;/code&gt;, which will mark this &lt;code&gt;Future&lt;/code&gt; as completed and trigger all attached callbacks. Note that a &lt;code&gt;Future&lt;/code&gt; cannot be marked completed twice.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d01a2557701031cde3d1b8c62a166dbe983b48e" translate="yes" xml:space="preserve">
          <source>Set your device to local rank using either</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f9ed8e87d1736694183486ee8b4775a705262ed" translate="yes" xml:space="preserve">
          <source>Sets gradients of all model parameters to zero. See similar function under &lt;a href=&quot;../optim#torch.optim.Optimizer&quot;&gt;&lt;code&gt;torch.optim.Optimizer&lt;/code&gt;&lt;/a&gt; for more context.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d404cad224614646e396f2671954c99061ff4ca5" translate="yes" xml:space="preserve">
          <source>Sets the Generator state.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9111f7925b4f9859890d95207d49fc1a4c287f5d" translate="yes" xml:space="preserve">
          <source>Sets the current device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="599ca5f4725e8b4ae5c8a8c258bedbaa0ae4f7ca" translate="yes" xml:space="preserve">
          <source>Sets the default &lt;code&gt;torch.Tensor&lt;/code&gt; type to floating point tensor type &lt;code&gt;t&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="017579c4ce17f464667bcc54bc1a6e8591f854a8" translate="yes" xml:space="preserve">
          <source>Sets the default &lt;code&gt;torch.Tensor&lt;/code&gt; type to floating point tensor type &lt;code&gt;t&lt;/code&gt;. This type will also be used as default floating point type for type inference in &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0194ba9213175f7befa0a141d39b82b09c76ae20" translate="yes" xml:space="preserve">
          <source>Sets the default floating point dtype to &lt;code&gt;d&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="552e257ff3a4ad554910ae569da22027db68a506" translate="yes" xml:space="preserve">
          <source>Sets the default floating point dtype to &lt;code&gt;d&lt;/code&gt;. This dtype is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="135e0facc67febcd6e6046179fa5ad6d5c7dfcb0" translate="yes" xml:space="preserve">
          <source>Sets the gradients of all optimized &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; s to zero.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00a00aa79d426ae2b3c0c06d23ade776e4f0d645" translate="yes" xml:space="preserve">
          <source>Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper &lt;a href=&quot;https://arxiv.org/abs/1506.01186&quot;&gt;Cyclical Learning Rates for Training Neural Networks&lt;/a&gt;. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="665434656af0e1a615df6cc92c6ecf52d30fbd49" translate="yes" xml:space="preserve">
          <source>Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper &lt;a href=&quot;https://arxiv.org/abs/1708.07120&quot;&gt;Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03d13473984ea489da3481909e502cdf0dc06a49" translate="yes" xml:space="preserve">
          <source>Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03f12972542800069e5267c12cb43edb6f65202d" translate="yes" xml:space="preserve">
          <source>Sets the module in evaluation mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf6241075b09cc953edd0e9568c4cd5f5c88fe95" translate="yes" xml:space="preserve">
          <source>Sets the module in training mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="850e5cb239a66b02ec6c1c59e0300349c7abf33d" translate="yes" xml:space="preserve">
          <source>Sets the number of threads used for interop parallelism (e.g.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be8644f2249be95fe3ec1aaa39ce51051699b0f6" translate="yes" xml:space="preserve">
          <source>Sets the number of threads used for interop parallelism (e.g. in JIT interpreter) on CPU.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf85722cd322795a8d83eb653874b27e33c5355c" translate="yes" xml:space="preserve">
          <source>Sets the number of threads used for intraop parallelism on CPU.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d725baae9b356dc9c5ed1aec81420b19033493b" translate="yes" xml:space="preserve">
          <source>Sets the random number generator state of all devices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d3f74d91be66fe1c0dcf86ea77ec056f2a793db" translate="yes" xml:space="preserve">
          <source>Sets the random number generator state of the specified GPU.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97ab6a55939177c2033f769ee905c9c0419e2d5d" translate="yes" xml:space="preserve">
          <source>Sets the random number generator state.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba56ea18f53dc2a28624660de79c9c1b82cce789" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers for the current GPU. It&amp;rsquo;s safe to call this function if CUDA is not available; in that case, it is silently ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="673ea6e5e56cccf50f89c4dcf1d57a9a886467da" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers on all GPUs. It&amp;rsquo;s safe to call this function if CUDA is not available; in that case, it is silently ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45382a699be8898ec2b2618f2defa7c97e636940" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers to a non-deterministic random number.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7abac63ee55826e63cd8690359844f301cb36545" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8cf00796144a8bf6bcaeca0922d063540aa0555" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers to a random number for the current GPU. It&amp;rsquo;s safe to call this function if CUDA is not available; in that case, it is silently ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29567c11f6e9503a646d0f30b8e295f5e5873021" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers to a random number on all GPUs. It&amp;rsquo;s safe to call this function if CUDA is not available; in that case, it is silently ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82d234e8fcede91a904aeab469b5635a53af28cb" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90135ab68f94754348a83b2e9a672e458fc27687" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers. Returns a &lt;code&gt;torch.Generator&lt;/code&gt; object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec7c3df03637450d6927de0719c85e704fa2352b" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers. Returns a &lt;code&gt;torch.Generator&lt;/code&gt; object. It is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits. Avoid having many 0 bits in the seed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f7618c30e9d279fa17b2c4bdef606d408ddac63" translate="yes" xml:space="preserve">
          <source>Sets the store&amp;rsquo;s default timeout. This timeout is used during initialization and in &lt;code&gt;wait()&lt;/code&gt; and &lt;code&gt;get()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="403c562468c4aed511a578286850fe13a038a13a" translate="yes" xml:space="preserve">
          <source>Sets the strategy for sharing CPU tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fdbbd0a94629feee695b40b70eaf4cd936d83f2" translate="yes" xml:space="preserve">
          <source>Sets the underlying storage, size, and strides. If &lt;code&gt;source&lt;/code&gt; is a tensor, &lt;code&gt;self&lt;/code&gt; tensor will share the same storage and have the same size and strides as &lt;code&gt;source&lt;/code&gt;. Changes to elements in one tensor will be reflected in the other.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0bcd71b693d5fcc4b6051caa2db49bd3be5ace5c" translate="yes" xml:space="preserve">
          <source>Sets whether PyTorch operations must use &amp;ldquo;deterministic&amp;rdquo; algorithms.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e8c135ec7a3af0dd0ffe1fe388882479c98805f" translate="yes" xml:space="preserve">
          <source>Sets whether PyTorch operations must use &amp;ldquo;deterministic&amp;rdquo; algorithms. That is, algorithms which, given the same input, and when run on the same software and hardware, always produce the same output. When True, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw a :class:RuntimeError when called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8145a6b5d6477fdcd3f4ae8c4f2e54d4aea089cc" translate="yes" xml:space="preserve">
          <source>Sets whether to materialize output grad tensors. Default is true.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f66b4449eb38b9763427d77c0a12d7806005f6b" translate="yes" xml:space="preserve">
          <source>Setting &lt;code&gt;return_complex&lt;/code&gt; explicitly will be required in a future PyTorch release. Set it to False to preserve the current behavior or True to return a complex output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50bf9fd4e5b67c599d58e411062b855bf1acfd69" translate="yes" xml:space="preserve">
          <source>Setting the argument &lt;code&gt;num_workers&lt;/code&gt; as a positive integer will turn on multi-process data loading with the specified number of loader worker processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cee805f166fb37a702f66bc6ef5dafcc4636a19c" translate="yes" xml:space="preserve">
          <source>Setting the environment variable &lt;code&gt;PYTORCH_JIT=0&lt;/code&gt; will disable all script and tracing annotations. If there is hard-to-debug error in one of your TorchScript models, you can use this flag to force everything to run using native Python. Since TorchScript (scripting and tracing) is disabled with this flag, you can use tools like &lt;code&gt;pdb&lt;/code&gt; to debug the model code. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14edb2c4ec235464bf2c2b8b3e2ce45c04e83a80" translate="yes" xml:space="preserve">
          <source>Shape:</source>
          <target state="translated">Shape:</target>
        </trans-unit>
        <trans-unit id="f03987f78c9ae43379f61461ae0064f0d5607ece" translate="yes" xml:space="preserve">
          <source>Shared file system, &lt;code&gt;init_method=&quot;file://////{machine_name}/{share_folder_name}/some_file&quot;&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="931e6a72bc22b9c481f30ac82befe23983a1c7aa" translate="yes" xml:space="preserve">
          <source>Shared file-system initialization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e27a0384e6eefcbae31d9ee40a6788234f717210" translate="yes" xml:space="preserve">
          <source>Sharing CUDA tensors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1970dc9e6498a19bd933b2843bc57e2084c8cc4" translate="yes" xml:space="preserve">
          <source>Sharing CUDA tensors between processes is supported only in Python 3, using a &lt;code&gt;spawn&lt;/code&gt; or &lt;code&gt;forkserver&lt;/code&gt; start methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ca95d11aaeb17762db51cad4433dcf3e2614300" translate="yes" xml:space="preserve">
          <source>Sharing strategies</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe846e8cff1382c632019e0d7be670d40fca4949" translate="yes" xml:space="preserve">
          <source>Short-time Fourier transform (STFT).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba0ad92a63c363484c20c75c1280fe0b91b17b2c" translate="yes" xml:space="preserve">
          <source>Should be overridden by all subclasses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a37ae8778cd1020f5c30499fa7ea9a6181211f61" translate="yes" xml:space="preserve">
          <source>Show the docstring of entrypoint &lt;code&gt;model&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7504f2364dcd93257f97f33b05a38d51614bd7fa" translate="yes" xml:space="preserve">
          <source>ShuffleNet V2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f25ac460d78b88009ddd1aa94f79739535ca8b5" translate="yes" xml:space="preserve">
          <source>ShuffleNet v2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2cac7f225122d558eb3c0591bdc2ae980f1b0b1" translate="yes" xml:space="preserve">
          <source>SiLU</source>
          <target state="translated">SiLU</target>
        </trans-unit>
        <trans-unit id="40cbe2acb9d6a109dfc4fb28692090f839183dea" translate="yes" xml:space="preserve">
          <source>Sigmoid</source>
          <target state="translated">Sigmoid</target>
        </trans-unit>
        <trans-unit id="5b7359698c3ea62e0fe6f50fc802ad6bb7e51d4a" translate="yes" xml:space="preserve">
          <source>Similar to &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt;, with FakeQuantize modules initialized to default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d16e4a78bd1e379e62b250638417164ecf26a8e" translate="yes" xml:space="preserve">
          <source>Similar to &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt;&lt;code&gt;Linear&lt;/code&gt;&lt;/a&gt;, attributes will be randomly initialized at module creation time and will be overwritten later</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb2775c2f284c5797e72dc4b9b56c7b83a742605" translate="yes" xml:space="preserve">
          <source>Similar to &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt;&lt;code&gt;torch.nn.Linear&lt;/code&gt;&lt;/a&gt;, attributes will be randomly initialized at module creation time and will be overwritten later</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c8d9b9716eb5e6f4a29b36c1d72924a394fd995" translate="yes" xml:space="preserve">
          <source>Similar to &lt;code&gt;torch.nn.Conv2d&lt;/code&gt;, with FakeQuantize modules initialized to default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7172942fcac61201e57b0f8f8151549517ac9d8" translate="yes" xml:space="preserve">
          <source>Similar to &lt;code&gt;torch.nn.Linear&lt;/code&gt;, with FakeQuantize modules initialized to default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb262cc11e95ba6ae1040b6fdf77b3aabdee1e50" translate="yes" xml:space="preserve">
          <source>Similar to &lt;code&gt;torch.nn.intrinsic.LinearReLU&lt;/code&gt;, with FakeQuantize modules initialized to default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="088b19f4865d3ae00ef9ed3545c87c4c39a038f5" translate="yes" xml:space="preserve">
          <source>Similar to the function above, but the means and standard deviations are shared among all drawn elements. The resulting tensor has size given by &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="144ca09471d4e86a370dcd5e499624c907232699" translate="yes" xml:space="preserve">
          <source>Similar to the function above, but the means are shared among all drawn elements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7b2ad151fdc2daf79483df18949d1a193fac72e" translate="yes" xml:space="preserve">
          <source>Similar to the function above, but the standard-deviations are shared among all drawn elements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35e4478885a3d3de0a23fd41d27cf9e8b3fd2526" translate="yes" xml:space="preserve">
          <source>Similarly, a variable is not allowed to be used if it is only &lt;em&gt;defined&lt;/em&gt; along some paths through the function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ffdace347411406995002bee149d93b4c8d5097" translate="yes" xml:space="preserve">
          <source>Similarly, if you directly pass in a &lt;code&gt;store&lt;/code&gt; argument, it must be a &lt;code&gt;FileStore&lt;/code&gt; instance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9be5c47bd8d9e298dbec515a79abc57fb63bd041" translate="yes" xml:space="preserve">
          <source>Similarly, the directions can be separated in the packed case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02a822df6b8073596762077fbb97fadeba9b1cf8" translate="yes" xml:space="preserve">
          <source>Simple Assignments</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5faf41d66cbc35e2d464a122a6e9520effb835d" translate="yes" xml:space="preserve">
          <source>Simple end to end example</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="605569f81b2cecc08a1e8d975481f24ac3831063" translate="yes" xml:space="preserve">
          <source>Simple example, using &lt;a href=&quot;#torch.cuda.amp.GradScaler.unscale_&quot;&gt;&lt;code&gt;unscale_()&lt;/code&gt;&lt;/a&gt; to enable clipping of unscaled gradients:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f6256b1a68c342ed7c517637989544d7f9e875b" translate="yes" xml:space="preserve">
          <source>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9d02a1900249db21bb923f01100461e477f4a2a" translate="yes" xml:space="preserve">
          <source>Simulate the quantize and dequantize operations in training time. The output of this module is given by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d60674760461d218ee6d7f02184c2ead492c87d0" translate="yes" xml:space="preserve">
          <source>Since &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;stft()&lt;/code&gt;&lt;/a&gt; discards elements at the end of the signal if they do not fit in a frame, &lt;code&gt;istft&lt;/code&gt; may return a shorter signal than the original signal (can occur if &lt;code&gt;center&lt;/code&gt; is False since the signal isn&amp;rsquo;t padded).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b57ff46579e1243d4e5f8600a0a9241de5ac5737" translate="yes" xml:space="preserve">
          <source>Since SparseTensor._indices() is always a 2D tensor, the smallest sparse_dim = 1. Therefore, representation of a SparseTensor of sparse_dim = 0 is simply a dense tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e55bc14205c3b61b63018bcb70eae3ef124b8ad" translate="yes" xml:space="preserve">
          <source>Since eigenvalues and eigenvectors might be complex, backward pass is supported only for &lt;a href=&quot;torch.symeig#torch.symeig&quot;&gt;&lt;code&gt;torch.symeig()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cc899b4bce705836850ca654589c887f5fd6278" translate="yes" xml:space="preserve">
          <source>Since global structured pruning doesn&amp;rsquo;t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67bd75f4f8766a48c7b96bda1b055c053f15aefe" translate="yes" xml:space="preserve">
          <source>Since the input matrix &lt;code&gt;input&lt;/code&gt; is supposed to be symmetric, only the upper triangular portion is used by default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a190d7c173d1800801bb5236c42d4c18532c731" translate="yes" xml:space="preserve">
          <source>Since workers rely on Python &lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt;&lt;code&gt;multiprocessing&lt;/code&gt;&lt;/a&gt;, worker launch behavior is different on Windows compared to Unix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da8a1ea02e7c7de523fde6a9a1ed49139987cfb8" translate="yes" xml:space="preserve">
          <source>Single- and Multi-process Data Loading</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21a1b9cf02c75eb1f1e22759af089802524d644f" translate="yes" xml:space="preserve">
          <source>Single-Node multi-process distributed training</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d2ab9ec1f053a991bd56f9ddd052665f52de541" translate="yes" xml:space="preserve">
          <source>Single-process data loading (default)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8df21c36569ccf0ecf10173294bd3bcb1ac958d" translate="yes" xml:space="preserve">
          <source>Slices the &lt;code&gt;self&lt;/code&gt; tensor along the selected dimension at the given index. This function returns a view of the original tensor with the given dimension removed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1388fcaebbfe7bba9af50141f343d91cd5010970" translate="yes" xml:space="preserve">
          <source>SmoothL1Loss</source>
          <target state="translated">SmoothL1Loss</target>
        </trans-unit>
        <trans-unit id="cf592edcfbdc177c81b474cbd6b83f6e29d0f7cc" translate="yes" xml:space="preserve">
          <source>So, it is recommended to always pass the signal length &lt;code&gt;n&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec836ce94eae96d1e5118d150d4218a7f447e2e4" translate="yes" xml:space="preserve">
          <source>So, it is recommended to always pass the signal shape &lt;code&gt;s&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fdb4ccd6a62db91e946fc4d593f43a0eb0c2b97" translate="yes" xml:space="preserve">
          <source>SobolEngine</source>
          <target state="translated">SobolEngine</target>
        </trans-unit>
        <trans-unit id="5780e85860e3ef851155673787aae34395fdcb14" translate="yes" xml:space="preserve">
          <source>SoftMarginLoss</source>
          <target state="translated">SoftMarginLoss</target>
        </trans-unit>
        <trans-unit id="bace6f3ab809aca48a2c72d1f9a972334f596086" translate="yes" xml:space="preserve">
          <source>SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4db38f97f455fc816850ffbe57a25de4b9b124c8" translate="yes" xml:space="preserve">
          <source>SoftShrinkage</source>
          <target state="translated">SoftShrinkage</target>
        </trans-unit>
        <trans-unit id="a2852636b9c2b8ea655bec2aa6ef18c0c81bce34" translate="yes" xml:space="preserve">
          <source>SoftSign</source>
          <target state="translated">SoftSign</target>
        </trans-unit>
        <trans-unit id="06a2c3db74f0f43566dbc7efd5bedd81dfa46888" translate="yes" xml:space="preserve">
          <source>Softmax</source>
          <target state="translated">Softmax</target>
        </trans-unit>
        <trans-unit id="38b81da3b6ab3b9be6104d3e4c422885eca96901" translate="yes" xml:space="preserve">
          <source>Softmax is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0444b47cdcbeed809978f61fc222d1069a142d9c" translate="yes" xml:space="preserve">
          <source>Softmax2d</source>
          <target state="translated">Softmax2d</target>
        </trans-unit>
        <trans-unit id="29b7eb80efd3b1d65f2835df414e4222cab21ee1" translate="yes" xml:space="preserve">
          <source>Softmin</source>
          <target state="translated">Softmin</target>
        </trans-unit>
        <trans-unit id="af86abb7b472b1e8a9af01b4180064b037b78bf8" translate="yes" xml:space="preserve">
          <source>Softmin is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90ba42633b716cf810a9a32d70553b14c097c177" translate="yes" xml:space="preserve">
          <source>Softplus</source>
          <target state="translated">Softplus</target>
        </trans-unit>
        <trans-unit id="7bb40d05bf436ff810963d7ee11a5bc95644a2f9" translate="yes" xml:space="preserve">
          <source>Softshrink</source>
          <target state="translated">Softshrink</target>
        </trans-unit>
        <trans-unit id="2b10804bb8f17cdf24b14774ca1d70aa3ba213d6" translate="yes" xml:space="preserve">
          <source>Softsign</source>
          <target state="translated">Softsign</target>
        </trans-unit>
        <trans-unit id="73a65eb2cc67f9395fdddf23ed853dd71a26bf17" translate="yes" xml:space="preserve">
          <source>Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab02cfb3b2fcd0c1b4621d3c7cc39a469375c3ec" translate="yes" xml:space="preserve">
          <source>Solves a system of equations with a triangular coefficient matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5773ea0d4a0b0567ea2d9a3b38579ffd7e09e6ff" translate="yes" xml:space="preserve">
          <source>Some functions (for example, &lt;a href=&quot;https://docs.python.org/3/library/functions.html#zip&quot;&gt;&lt;code&gt;zip&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://docs.python.org/3/library/functions.html#enumerate&quot;&gt;&lt;code&gt;enumerate&lt;/code&gt;&lt;/a&gt;) can only operate on iterable types. Iterable types in TorchScript include &lt;code&gt;Tensor&lt;/code&gt;s, lists, tuples, dictionaries, strings, &lt;a href=&quot;generated/torch.nn.modulelist#torch.nn.ModuleList&quot;&gt;&lt;code&gt;torch.nn.ModuleList&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.moduledict#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;torch.nn.ModuleDict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbbdc83ebf4a1410688a2167e77428899715f68d" translate="yes" xml:space="preserve">
          <source>Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5a5860f652baaa71e759f95e6546fb5a30df1fa" translate="yes" xml:space="preserve">
          <source>Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, use &lt;code&gt;model.train()&lt;/code&gt; or &lt;code&gt;model.eval()&lt;/code&gt; as appropriate. See &lt;a href=&quot;../generated/torch.nn.module#torch.nn.Module.train&quot;&gt;&lt;code&gt;train()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../generated/torch.nn.module#torch.nn.Module.eval&quot;&gt;&lt;code&gt;eval()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b9a08f765d29fc2a0904578fe32193591816e48" translate="yes" xml:space="preserve">
          <source>Some ops not listed here (e.g., binary ops like &lt;code&gt;add&lt;/code&gt;) natively promote inputs without autocasting&amp;rsquo;s intervention. If inputs are a mixture of &lt;code&gt;float16&lt;/code&gt; and &lt;code&gt;float32&lt;/code&gt;, these ops run in &lt;code&gt;float32&lt;/code&gt; and produce &lt;code&gt;float32&lt;/code&gt; output, regardless of whether autocast is enabled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72e33a4ea3969c9936cfbf0b72aacf8cc360fedb" translate="yes" xml:space="preserve">
          <source>Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae4df2df1ef3b4db7bd9154e0c9f55f5890f0c7a" translate="yes" xml:space="preserve">
          <source>Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as &lt;code&gt;float32&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78039ee874a9a5bb0eb3d37a1f02f096ee61bda5" translate="yes" xml:space="preserve">
          <source>Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as &lt;code&gt;float32&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba80efc106bf3fdda88df0da58461971d3468eca" translate="yes" xml:space="preserve">
          <source>Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important at the expense of range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4633684bcdc43c53e1cfb801845c2325140c26e1" translate="yes" xml:space="preserve">
          <source>Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3900f0b870b5719788e2d6646aafed691f38941e" translate="yes" xml:space="preserve">
          <source>Sorts the elements of the &lt;code&gt;input&lt;/code&gt; tensor along a given dimension in ascending order by value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a085f98a6ae4bb57c97870bfd419ca83f17e143" translate="yes" xml:space="preserve">
          <source>Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to &lt;code&gt;cpp_sources&lt;/code&gt; are first concatenated into a single &lt;code&gt;.cpp&lt;/code&gt; file. This file is then prepended with &lt;code&gt;#include
&amp;lt;torch/extension.h&amp;gt;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38317d08ad25b33e290bef6bc76c581c87c8620f" translate="yes" xml:space="preserve">
          <source>Sparse Layers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e3084d96416ea178eaf45c018b3d6a666f048ab" translate="yes" xml:space="preserve">
          <source>Sparse functions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca9492470122675a4d013eba5c404de6d984fba7" translate="yes" xml:space="preserve">
          <source>SparseTensor has the following invariants:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad7f19daae9a456b6857c9e6506b86f3b2ab9c8d" translate="yes" xml:space="preserve">
          <source>SparseTensor._indices().shape = (sparse_dim, nnz)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="018f02659b563c79f98ce5dc06e6c8ead57c92f3" translate="yes" xml:space="preserve">
          <source>SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ca5e4706f1046fd29382939145a442a4ce0eeb1" translate="yes" xml:space="preserve">
          <source>Spawn utility</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="762ee18552488ab27f7d553d5bb224a9478ed209" translate="yes" xml:space="preserve">
          <source>Spawning a number of subprocesses to perform some function can be done by creating &lt;code&gt;Process&lt;/code&gt; instances and calling &lt;code&gt;join&lt;/code&gt; to wait for their completion. This approach works fine when dealing with a single subprocess but presents potential issues when dealing with multiple processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62f14e7f69f0c928f3521a52d4471e369748ed2e" translate="yes" xml:space="preserve">
          <source>Spawning subprocesses</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c66cb2e22db24d665900e10960b23a14a873c2d2" translate="yes" xml:space="preserve">
          <source>Spawns &lt;code&gt;nprocs&lt;/code&gt; processes that run &lt;code&gt;fn&lt;/code&gt; with &lt;code&gt;args&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2019fa476b609b73dfcc70b82b7767a098a7dbd" translate="yes" xml:space="preserve">
          <source>Specifically, in the forward pass, &lt;code&gt;function&lt;/code&gt; will run in &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/a&gt; manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the &lt;code&gt;function&lt;/code&gt; parameter. In the backwards pass, the saved inputs and &lt;code&gt;function&lt;/code&gt; is retrieved, and the forward pass is computed on &lt;code&gt;function&lt;/code&gt; again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fb7ff759e9a0b4f8cf21fa63ff4032c4ca275d1" translate="yes" xml:space="preserve">
          <source>Specify &lt;code&gt;init_method&lt;/code&gt; (a URL string) which indicates where/how to discover peers. Optionally specify &lt;code&gt;rank&lt;/code&gt; and &lt;code&gt;world_size&lt;/code&gt;, or encode all required parameters in the URL and omit them.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaa6b0a968f0861818a7c60f3b1e30636bcfe285" translate="yes" xml:space="preserve">
          <source>Specify &lt;code&gt;store&lt;/code&gt;, &lt;code&gt;rank&lt;/code&gt;, and &lt;code&gt;world_size&lt;/code&gt; explicitly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="729466c776095dc7f1dabe6031157b446a5540f2" translate="yes" xml:space="preserve">
          <source>Spectral Ops</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2edd2d0e89ea6a8fb45d2a4b97947c90f7836312" translate="yes" xml:space="preserve">
          <source>Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7da9517b7a5b4892d14a9dd4e7bfc3bd2215ddfe" translate="yes" xml:space="preserve">
          <source>Splits a tensor into a specific number of chunks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15060a11c482f2d0d29c0ea280b19cfd35770c5a" translate="yes" xml:space="preserve">
          <source>Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e2be38fc2259b1dc48a946fa30b9de28430d8f6" translate="yes" xml:space="preserve">
          <source>Splits the tensor into chunks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a4c888de6c076a07c14ac6f6526916b6b64d375" translate="yes" xml:space="preserve">
          <source>Splits the tensor into chunks. Each chunk is a view of the original tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bb9feffd05f6ed4721f33c4d4bcd3050fc0acce" translate="yes" xml:space="preserve">
          <source>SqueezeNet</source>
          <target state="translated">SqueezeNet</target>
        </trans-unit>
        <trans-unit id="8e3b43524d2794940e994fbdb21d126a6c23a892" translate="yes" xml:space="preserve">
          <source>SqueezeNet 1.0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83d7283f405d03fc00d84f279e5aa4a9ad100122" translate="yes" xml:space="preserve">
          <source>SqueezeNet 1.1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8ffcf5b4a1dac24c72399125fdd5187787b0df5" translate="yes" xml:space="preserve">
          <source>SqueezeNet 1.1 model from the &lt;a href=&quot;https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1&quot;&gt;official SqueezeNet repo&lt;/a&gt;. SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters than SqueezeNet 1.0, without sacrificing accuracy.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fe53423313b926c6b9706c2a81b42a519942948" translate="yes" xml:space="preserve">
          <source>SqueezeNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;&amp;ldquo;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &amp;lt;0.5MB model size&amp;rdquo;&lt;/a&gt; paper.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d061769f4cfb05ab146ba458ef5636f865a72a82" translate="yes" xml:space="preserve">
          <source>Stack tensors in sequence depthwise (along third axis).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46e9e1e8c6989a91e6a1a0548e571225be43cbbf" translate="yes" xml:space="preserve">
          <source>Stack tensors in sequence horizontally (column wise).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd5277dc6eddb37c391be9a3629b9809479f61ec" translate="yes" xml:space="preserve">
          <source>Stack tensors in sequence vertically (row wise).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7911bbae7aa66ed740b8a6ebf74df4fbffeb0338" translate="yes" xml:space="preserve">
          <source>State collector class for float operations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5653cebc057d4791ce07031ad9286e729de6d691" translate="yes" xml:space="preserve">
          <source>Statements</source>
          <target state="translated">Statements</target>
        </trans-unit>
        <trans-unit id="19abb71cdc8040d0d69069e805acbb331fb10491" translate="yes" xml:space="preserve">
          <source>Step between two slices is given by &lt;code&gt;step&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14f4390fa64c3d61f311b3d728a02a53224437ed" translate="yes" xml:space="preserve">
          <source>Step could be called after every batch update</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="800f66cc7b49914c33ebebf4faf976e88aaac6fe" translate="yes" xml:space="preserve">
          <source>Stochastic Weight Averaging</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a93d86688c50b7db5df06afe98e12c7eb843b764" translate="yes" xml:space="preserve">
          <source>Stores names for each of this tensor&amp;rsquo;s dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a96f51db31dc0b324375deea02a8cc1a060aba6" translate="yes" xml:space="preserve">
          <source>Strategy management</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0f4aaebf484b2f55d024a76952eb003a0d189f2" translate="yes" xml:space="preserve">
          <source>Streams and events</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3036c584463ccfc0fa687352917db10956640ab" translate="yes" xml:space="preserve">
          <source>Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c7ec55740528fa4d2910cfad7b5e1cae6e7da08" translate="yes" xml:space="preserve">
          <source>Stride is the jump necessary to go from one element to the next one in the specified dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt;. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaae0d5e6cf55aab12950842f08c8cf9355bcef4" translate="yes" xml:space="preserve">
          <source>StudentT</source>
          <target state="translated">StudentT</target>
        </trans-unit>
        <trans-unit id="8234b01c9735ef16f8e122a86ac6b13bee795314" translate="yes" xml:space="preserve">
          <source>Submodules assigned in this way will be registered, and will have their parameters converted too when you call &lt;a href=&quot;#torch.nn.Module.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82a36013ba03c5404268ec4092f5e0e3210ace06" translate="yes" xml:space="preserve">
          <source>Subscripts and Slicing</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a822d1e4dad94b5aa9ace547ed1affaadebbf09a" translate="yes" xml:space="preserve">
          <source>Subset of a dataset at specified indices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae017f86a36a573c987d535dde17fe71055f3774" translate="yes" xml:space="preserve">
          <source>Subsystems</source>
          <target state="translated">Subsystems</target>
        </trans-unit>
        <trans-unit id="fb5f075e6679b4e2fffcc3ea921e3be776764746" translate="yes" xml:space="preserve">
          <source>Subtracts &lt;code&gt;other&lt;/code&gt;, scaled by &lt;code&gt;alpha&lt;/code&gt;, from &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61228272a4a7f9d4dd0cc2a12ad63a807870bdc6" translate="yes" xml:space="preserve">
          <source>Sum &lt;code&gt;this&lt;/code&gt; tensor to &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; must be broadcastable to &lt;code&gt;this&lt;/code&gt; tensor size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bbc7a7ed0ef03a5bd96699fe5d02cbbd0777b92" translate="yes" xml:space="preserve">
          <source>Sums tensors from multiple GPUs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2a1c18ad4d1dc4a9dbab6435947b2aca87f5bcb" translate="yes" xml:space="preserve">
          <source>SuperResolution</source>
          <target state="translated">SuperResolution</target>
        </trans-unit>
        <trans-unit id="fe6c7185d6335e53d7798880272887b9e595250b" translate="yes" xml:space="preserve">
          <source>Supported constant Python types are</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c9970da0829cd2d77d97494b716ecac47fb4272" translate="yes" xml:space="preserve">
          <source>Supported inputs are dense, sparse, and batches of dense matrices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a72acca9638828e6fcd59dd9d5146b8ffaac77cb" translate="yes" xml:space="preserve">
          <source>Supported operators</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2212b71de8fd24dec99bf8599144400e7f7e95d" translate="yes" xml:space="preserve">
          <source>Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd&amp;rsquo;s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you&amp;rsquo;re operating under heavy memory pressure, you might never need to use them.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d88be7203e6e438cd32536d295a5ed587dd4a76" translate="yes" xml:space="preserve">
          <source>Supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcasting to a common shape&lt;/a&gt;, &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;type promotion&lt;/a&gt;, and integer, float, and complex inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12a17f409a6267d331bbba640e585b7dceceb728" translate="yes" xml:space="preserve">
          <source>Supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcasting to a common shape&lt;/a&gt;, &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;type promotion&lt;/a&gt;, and integer, float, and complex inputs. Always promotes integer types to the default scalar type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c515e39b10e558f93fb5480918ab7b6f80dd1a7c" translate="yes" xml:space="preserve">
          <source>Supports broadcasting to a common shape, type promotion, and integer and float inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="776e5bf297164498f6e5ef24869ccff8d9d007bf" translate="yes" xml:space="preserve">
          <source>Swaps the module if it has a quantized counterpart and it has an &lt;code&gt;observer&lt;/code&gt; attached.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b5fa2f5e7f20411e614b5e4239c9394a99b2a3f" translate="yes" xml:space="preserve">
          <source>Symbolic functions should be implemented in Python. All of these functions interact with Python methods which are implemented via C++-Python bindings, but intuitively the interface they provide looks like this:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd8b7a9758c34d0bf6248878c673e14fd8037607" translate="yes" xml:space="preserve">
          <source>SyncBatchNorm</source>
          <target state="translated">SyncBatchNorm</target>
        </trans-unit>
        <trans-unit id="1dc553edb93ee2799123f5bdebb8f2d61aa9771e" translate="yes" xml:space="preserve">
          <source>Synchronizes all processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e32e1c40401ff0c5bb5e89fba4cab573a0ddefc7" translate="yes" xml:space="preserve">
          <source>Synchronizes with another stream.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68f18accfbfac81fef2a2bc99a682b9a686206f1" translate="yes" xml:space="preserve">
          <source>Synchronous and asynchronous collective operations</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2c53d66948214258a26ca9ca845d7ac0c17f8e7" translate="yes" xml:space="preserve">
          <source>T</source>
          <target state="translated">T</target>
        </trans-unit>
        <trans-unit id="a55d4194696253efb0ce8eaeea351140c88a65da" translate="yes" xml:space="preserve">
          <source>T = \text{input length}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26aecc925a999c69c46a16510ab79a1145705de8" translate="yes" xml:space="preserve">
          <source>T+X</source>
          <target state="translated">T+X</target>
        </trans-unit>
        <trans-unit id="a614078b58d53b0cc0d0d780950744eb6f8ae5a6" translate="yes" xml:space="preserve">
          <source>TCP initialization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae0cb888e5b3c4b34090835d26990b761f9663d6" translate="yes" xml:space="preserve">
          <source>T_{cur}</source>
          <target state="translated">T_{cur}</target>
        </trans-unit>
        <trans-unit id="728548804a0da830440ca32e7488e5d512871f39" translate="yes" xml:space="preserve">
          <source>T_{cur}=0</source>
          <target state="translated">T_{cur}=0</target>
        </trans-unit>
        <trans-unit id="3f5463cfd9bd0758a96ea8db117479b2314fff2b" translate="yes" xml:space="preserve">
          <source>T_{cur}=T_{i}</source>
          <target state="translated">T_{cur}=T_{i}</target>
        </trans-unit>
        <trans-unit id="97af24691624857deb13ae1ab91e6164cd26f2be" translate="yes" xml:space="preserve">
          <source>T_{i}</source>
          <target state="translated">T_{i}</target>
        </trans-unit>
        <trans-unit id="e80c75f58bfc00b9ae732035ef70cb00333f4a1c" translate="yes" xml:space="preserve">
          <source>Take in and process masked source/target sequences.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="945a1e1f48dcf43605fff2c03f1828ea5952eb79" translate="yes" xml:space="preserve">
          <source>Take the instruction &lt;code&gt;%rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10&lt;/code&gt; for example.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e94daa7e8a28b957c758019bcfa728a98c04545f" translate="yes" xml:space="preserve">
          <source>Takes LongTensor with index values of shape &lt;code&gt;(*)&lt;/code&gt; and returns a tensor of shape &lt;code&gt;(*, num_classes)&lt;/code&gt; that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d3331f3bb652437bd8045860b4a682532d6d90a" translate="yes" xml:space="preserve">
          <source>Takes the inverse of the square matrix &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="537dfa1e7ebc0ec78b53314930b233f489d3f1c5" translate="yes" xml:space="preserve">
          <source>Takes the inverse of the square matrix &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; can be batches of 2D square tensors, in which case this function would return a tensor composed of individual inverses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0ad657474446cfbcb32a233330ba630a430b967" translate="yes" xml:space="preserve">
          <source>Takes the power of each element in &lt;code&gt;input&lt;/code&gt; with &lt;code&gt;exponent&lt;/code&gt; and returns a tensor with the result.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9d3154243fedb184083b9dcec0135d9da418872" translate="yes" xml:space="preserve">
          <source>Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ffacc7470da0fe6c6a8dae8d1ef37e2319cadeb" translate="yes" xml:space="preserve">
          <source>Taking an optimization step</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd4e923c14f3ac2ab68ed029691caa5081fb04f7" translate="yes" xml:space="preserve">
          <source>Taking care of batch normalization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19bff9fbbbadd7b339e0ee0ae1715d62fea9cae0" translate="yes" xml:space="preserve">
          <source>Tanh</source>
          <target state="translated">Tanh</target>
        </trans-unit>
        <trans-unit id="fa7acfd0630e85ee187c290a39277eaf9f5359da" translate="yes" xml:space="preserve">
          <source>Tanhshrink</source>
          <target state="translated">Tanhshrink</target>
        </trans-unit>
        <trans-unit id="652ac2cbbafccc62d55637f20bfa949ef565ffbd" translate="yes" xml:space="preserve">
          <source>Target:</source>
          <target state="translated">Target:</target>
        </trans-unit>
        <trans-unit id="3032d3d269195493b60b2b5232d8a35ccf1cf179" translate="yes" xml:space="preserve">
          <source>Target_lengths: Tuple or tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cce3202a79fe1345bd15442aee9a15d0666fb25f" translate="yes" xml:space="preserve">
          <source>Targets: Tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="680cb2d3fe397d144ed6bcd0531961c3a05ca6c0" translate="yes" xml:space="preserve">
          <source>Tensor</source>
          <target state="translated">Tensor</target>
        </trans-unit>
        <trans-unit id="7d374fa02b51c2a0bd017c85ffdbaad8aa59c748" translate="yes" xml:space="preserve">
          <source>Tensor Attributes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47b8aa0b5113cdb5b15d1487280d5b3f9b245998" translate="yes" xml:space="preserve">
          <source>Tensor Views</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7ecaeb20f986ea84c795703082ccf954c1b6a60" translate="yes" xml:space="preserve">
          <source>Tensor autograd functions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d5ce15cb1a252917b40c643e7a2eb6d5463df19" translate="yes" xml:space="preserve">
          <source>Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44e4866b0b5a5220d0057d20a02b4c4e9ccd49d7" translate="yes" xml:space="preserve">
          <source>Tensor iterating over dimension 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="968efb5c209c67b8c5e5cb36ab67b3d4543380ee" translate="yes" xml:space="preserve">
          <source>Tensor of shape batch_shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f4edfd7fe26e893eda2475a21249dda3127731b" translate="yes" xml:space="preserve">
          <source>Tensor of size &lt;code&gt;T x B x *&lt;/code&gt; if &lt;code&gt;batch_first&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;. Tensor of size &lt;code&gt;B x T x *&lt;/code&gt; otherwise</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f64e01f2c0bfc00c617fa5cfe062d17dd7881fa" translate="yes" xml:space="preserve">
          <source>TensorPipe Backend</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fa30b4e43ae7ab6c13e5a62747d73bd4f6a50ff" translate="yes" xml:space="preserve">
          <source>Tensors</source>
          <target state="translated">Tensors</target>
        </trans-unit>
        <trans-unit id="3f3793b3bb16961a91ba80319c433489d751557f" translate="yes" xml:space="preserve">
          <source>Tensors may not have two named dimensions with the same name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a4ff50c90d4ae49393e7ddb58e4182969af3c75" translate="yes" xml:space="preserve">
          <source>Ternary Expressions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3aa34ed971590db58293e74b01ccf27489043c07" translate="yes" xml:space="preserve">
          <source>Tests if each element of &lt;code&gt;input&lt;/code&gt; has its sign bit set (is less than zero) or not.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5de15b26e6a795a7480af2f5882e1811d35f284" translate="yes" xml:space="preserve">
          <source>Tests if each element of &lt;code&gt;input&lt;/code&gt; is infinite (positive or negative infinity) or not.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0285b21693b5a12974f50c902b26299554e6465" translate="yes" xml:space="preserve">
          <source>Tests if each element of &lt;code&gt;input&lt;/code&gt; is negative infinity or not.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69536d45bf03029aac4041aa441f6044c68432e4" translate="yes" xml:space="preserve">
          <source>Tests if each element of &lt;code&gt;input&lt;/code&gt; is positive infinity or not.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93ef0dd827103681fcee453b78be2ff14e1a261d" translate="yes" xml:space="preserve">
          <source>The</source>
          <target state="translated">The</target>
        </trans-unit>
        <trans-unit id="2312d5bb8d4ccdb5a8d40c96ff98fa570d836574" translate="yes" xml:space="preserve">
          <source>The 1-dimensional dot product version of this function does not support an &lt;code&gt;out&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0763c643a0c9e64fc2e1987a16313347a5b7157" translate="yes" xml:space="preserve">
          <source>The 1.6 release of PyTorch switched &lt;code&gt;torch.save&lt;/code&gt; to use a new zipfile-based file format. &lt;code&gt;torch.load&lt;/code&gt; still retains the ability to load files in the old format. If for any reason you want &lt;code&gt;torch.save&lt;/code&gt; to use the old format, pass the kwarg &lt;code&gt;_use_new_zipfile_serialization=False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="539ba0cb4053409de4820d1ae27b5ebc56acc073" translate="yes" xml:space="preserve">
          <source>The 1cycle learning rate policy changes the learning rate after every batch. &lt;code&gt;step&lt;/code&gt; should be called after a batch has been used for training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a532bc29eaf3d25b0af52da2857f32bdfeed457d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt;th dimension of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; must have the same size as the length of &lt;code&gt;index&lt;/code&gt; (which must be a vector), and all other dimensions must match &lt;code&gt;self&lt;/code&gt;, or an error will be raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbb84ae310dddae342bffdaeaa243633f2a53249" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.quasirandom.SobolEngine&quot;&gt;&lt;code&gt;torch.quasirandom.SobolEngine&lt;/code&gt;&lt;/a&gt; is an engine for generating (scrambled) Sobol sequences. Sobol sequences are an example of low discrepancy quasi-random sequences.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2e5c436ae6a7b841286261fcf013451bf51db2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; argument in functions can generally be substituted with a string. This allows for fast prototyping of code.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f98c90dfa427c3d98ac8ee0cbb86e967d2daadab" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; contains a device type (&lt;code&gt;'cpu'&lt;/code&gt; or &lt;code&gt;'cuda'&lt;/code&gt;) and optional device ordinal for the device type. If the device ordinal is not present, this object will always represent the current device for the device type, even after &lt;a href=&quot;cuda#torch.cuda.set_device&quot;&gt;&lt;code&gt;torch.cuda.set_device()&lt;/code&gt;&lt;/a&gt; is called; e.g., a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; constructed with device &lt;code&gt;'cuda'&lt;/code&gt; is equivalent to &lt;code&gt;'cuda:X'&lt;/code&gt; where X is the result of &lt;a href=&quot;cuda#torch.cuda.current_device&quot;&gt;&lt;code&gt;torch.cuda.current_device()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb07fddc9a5d6fb2f9f535ebf9278f5523df3bef" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36716fd31e682e7596ade3d0fc033b9418b25151" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; annotation&amp;rsquo;s behavior changes in PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function or method callable from code that is exported. To get this functionality back, use &lt;code&gt;@torch.jit.unused()&lt;/code&gt;. &lt;code&gt;@torch.jit.ignore&lt;/code&gt; is now equivalent to &lt;code&gt;@torch.jit.ignore(drop=False)&lt;/code&gt;. See &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="629d16d00f1d4af587a0e984e39f346b9c762a9d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine&quot;&gt;&lt;code&gt;torch.quasirandom.SobolEngine&lt;/code&gt;&lt;/a&gt; is an engine for generating (scrambled) Sobol sequences.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b29b06dcd5a85e7f1ec30bf4cac512a58a2b2613" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback-Leibler_divergence&quot;&gt;Kullback-Leibler divergence Loss&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f5b85ebb4e0453454d4b99cfd4ca555eb171ffc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;multiprocessing#multiprocessing-doc&quot;&gt;Multiprocessing package - torch.multiprocessing&lt;/a&gt; package also provides a &lt;code&gt;spawn&lt;/code&gt; function in &lt;a href=&quot;multiprocessing#torch.multiprocessing.spawn&quot;&gt;&lt;code&gt;torch.multiprocessing.spawn()&lt;/code&gt;&lt;/a&gt;. This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e1098da0df12b2fcc4a3fa80eabfd00f062a3e8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; is a tensor with the mean of each output element&amp;rsquo;s normal distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="235814295d4fe94a904ed9a1a782c8eebf3b58c5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt; is a tensor with the standard deviation of each output element&amp;rsquo;s normal distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fc3044dc4c4ace9a26403a7a793e46904198525" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;@torch.jit.script&lt;/code&gt; decorator will construct a &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; by compiling the body of the function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8a401f5a86feecc05362977ef403b66b5efedab" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;@torch.jit.script_method&lt;/code&gt; decorator</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9011f0cf08e406be11c8a00cbbbc2003dbdb187f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;Final&lt;/code&gt; type constructor can be used to mark members as &lt;code&gt;constant&lt;/code&gt;. If members are not marked constant, they will be copied to the resulting &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; as an attribute. Using &lt;code&gt;Final&lt;/code&gt; opens opportunities for optimization if the value is known to be fixed and gives additional type safety.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94d670ae23dc45d9238050515c460edb409b464f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;MixtureSameFamily&lt;/code&gt; distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a &lt;code&gt;Categorical&lt;/code&gt; &amp;ldquo;selecting distribution&amp;rdquo; (over &lt;code&gt;k&lt;/code&gt; component) and a component distribution, i.e., a &lt;code&gt;Distribution&lt;/code&gt; with a rightmost batch shape (equal to &lt;code&gt;[k]&lt;/code&gt;) which indexes each (batch of) component.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="681a06025ef48d63b48a8f0ade5d434c9ed8e4d8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;None&lt;/code&gt; check must be within the if-statement&amp;rsquo;s condition; assigning a &lt;code&gt;None&lt;/code&gt; check to a variable and using it in the if-statement&amp;rsquo;s condition will not refine the types of variables in the check. Only local variables will be refined, an attribute like &lt;code&gt;self.x&lt;/code&gt; will not and must assigned to a local variable to be refined.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c57db6f6aeb13e4c4808482871b866cd32a08ce8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;SummaryWriter&lt;/code&gt; class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="965753cbd79a45739bc9ad1bc6ad8aab3c55f336" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;__constants__&lt;/code&gt; array</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f242923c2659bf7bc5a464857040425ba312d13f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;__len__()&lt;/code&gt; method isn&amp;rsquo;t strictly required by &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;, but is expected in any calculation involving the length of a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eba7c266175279603efbf76b860806dfd23d6385" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;batch_size&lt;/code&gt; and &lt;code&gt;drop_last&lt;/code&gt; arguments essentially are used to construct a &lt;code&gt;batch_sampler&lt;/code&gt; from &lt;code&gt;sampler&lt;/code&gt;. For map-style datasets, the &lt;code&gt;sampler&lt;/code&gt; is either provided by user or constructed based on the &lt;code&gt;shuffle&lt;/code&gt; argument. For iterable-style datasets, the &lt;code&gt;sampler&lt;/code&gt; is a dummy infinite one. See &lt;a href=&quot;#data-loading-order-and-sampler&quot;&gt;this section&lt;/a&gt; on more details on samplers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a26eee2b4f8c5fa56dc0e1b6d51769e2674252ff" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;biject_to()&lt;/code&gt; registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained &lt;code&gt;.support&lt;/code&gt; are propagated in an unconstrained space, and algorithms are typically rotation invariant.:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d56e7b5edb09cb0ed4ea4637fa714f0dc8503dd" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;biject_to&lt;/code&gt; and &lt;code&gt;transform_to&lt;/code&gt; objects can be extended by user-defined constraints and transforms using their &lt;code&gt;.register()&lt;/code&gt; method either as a function on singleton constraints:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44a5102b3b32f58d5c9c3eb95af7ab62d791ac38" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;callable&lt;/code&gt; should have the signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4cd295b30506f6711ba85f58a42cc9115e197d2" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;delete_key&lt;/code&gt; API is only supported by the &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;. Using this API with the &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt; will result in an exception.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79ae973263b72787a35d19a21f1d5ebc7a491f12" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;distributions&lt;/code&gt; package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the &lt;a href=&quot;https://arxiv.org/abs/1711.10604&quot;&gt;TensorFlow Distributions&lt;/a&gt; package.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e75f8845977efa943a8e24244efa0158e93fd55" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of &lt;code&gt;grad_input&lt;/code&gt; in subsequent computations. &lt;code&gt;grad_input&lt;/code&gt; will only correspond to the inputs given as positional arguments.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c418e4ba558348daf084f57cfc5ba8f635661eb0" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;gradient_as_bucket_view&lt;/code&gt; mode does not yet work with Automatic Mixed Precision (AMP). AMP maintains stashed gradients that are used for unscaling gradients. With &lt;code&gt;gradient_as_bucket_view=True&lt;/code&gt;, these stashed gradients will point to communication buckets in the first iteration. In the next iteration, the communication buckets are mutated and thus these stashed gradients will be unexpectedly mutated as well, which might lead to wrong results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fd870aae016b2c00935c2fbc6c52d9270bae088" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;input&lt;/code&gt; given through a forward call is expected to contain log-probabilities of each class. &lt;code&gt;input&lt;/code&gt; has to be a Tensor of size either</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60f14a9d856acc735af342914f0ff8281b2528f6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;input&lt;/code&gt; is expected to contain raw, unnormalized scores for each class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6624e1abebb827c38014a043499dfb4fc7c338b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;input&lt;/code&gt; tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in &lt;code&gt;input&lt;/code&gt; have to be in the range:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb4a0ff705e3f1a46ebc8daed33e4bce289527c8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;mask&lt;/code&gt; operates on the &lt;code&gt;self&lt;/code&gt; tensor, not on the given &lt;code&gt;source&lt;/code&gt; tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad5a92992bc736ff5322b797aa0cf7673e44901d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;n_fft&lt;/code&gt;, &lt;code&gt;hop_length&lt;/code&gt;, &lt;code&gt;win_length&lt;/code&gt; are all the same which prevents the calculation of right padding. These additional values could be zeros or a reflection of the signal so providing &lt;code&gt;length&lt;/code&gt; could be useful. If &lt;code&gt;length&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; then padding will be aggressively removed (some loss of signal).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0ace9222bb0e305cbf096e49fb16c8bd736635b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;num_keys&lt;/code&gt; API is only supported by the &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;. Using this API with the &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt; will result in an exception.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bd6f3dbf9c9f10c4e3b624b6d33b29b1f55e1ac" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;padding&lt;/code&gt; argument effectively adds &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; amount of zero padding to both sizes of the input. This is set so that when a &lt;a href=&quot;torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; and a &lt;a href=&quot;#torch.nn.ConvTranspose1d&quot;&gt;&lt;code&gt;ConvTranspose1d&lt;/code&gt;&lt;/a&gt; are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when &lt;code&gt;stride &amp;gt; 1&lt;/code&gt;, &lt;a href=&quot;torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; maps multiple input shapes to the same output shape. &lt;code&gt;output_padding&lt;/code&gt; is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that &lt;code&gt;output_padding&lt;/code&gt; is only used to find output shape, but does not actually add zero-padding to output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5440e08e1ef68605bef85def7ab3c93778452300" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;padding&lt;/code&gt; argument effectively adds &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; amount of zero padding to both sizes of the input. This is set so that when a &lt;a href=&quot;torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; and a &lt;a href=&quot;#torch.nn.ConvTranspose2d&quot;&gt;&lt;code&gt;ConvTranspose2d&lt;/code&gt;&lt;/a&gt; are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when &lt;code&gt;stride &amp;gt; 1&lt;/code&gt;, &lt;a href=&quot;torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; maps multiple input shapes to the same output shape. &lt;code&gt;output_padding&lt;/code&gt; is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that &lt;code&gt;output_padding&lt;/code&gt; is only used to find output shape, but does not actually add zero-padding to output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="624a36cdb030302abd92f10a870506676fb58830" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;padding&lt;/code&gt; argument effectively adds &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; amount of zero padding to both sizes of the input. This is set so that when a &lt;a href=&quot;torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; and a &lt;a href=&quot;#torch.nn.ConvTranspose3d&quot;&gt;&lt;code&gt;ConvTranspose3d&lt;/code&gt;&lt;/a&gt; are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when &lt;code&gt;stride &amp;gt; 1&lt;/code&gt;, &lt;a href=&quot;torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; maps multiple input shapes to the same output shape. &lt;code&gt;output_padding&lt;/code&gt; is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that &lt;code&gt;output_padding&lt;/code&gt; is only used to find output shape, but does not actually add zero-padding to output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac2a8aee0768d2dee229b10ae182a5bbe1013563" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; and &lt;code&gt;dilation&lt;/code&gt; arguments specify how the sliding blocks are retrieved.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a86ce4705a9d8461e1968c13331ef942f95a281" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remote&lt;/code&gt; API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the &lt;a href=&quot;#torch.distributed.rpc.RRef.confirmed_by_owner&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.confirmed_by_owner()&lt;/code&gt;&lt;/a&gt; API.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d9f2b4532f5542083c83b1d9834f5f944e782fe" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;rpc_async&lt;/code&gt; API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; completes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3596722c9e70073957aeadba20c33b436e025428" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;spawn&lt;/code&gt; function below addresses these concerns and takes care of error propagation, out of order termination, and will actively terminate processes upon detecting an error in one of them.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a7cdd56e507990bc132c8770c06d1f57641cbc2" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;src&lt;/code&gt; tensor must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the &lt;code&gt;self&lt;/code&gt; tensor. It may be of a different data type or reside on a different device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17a168b9f78b1ca48e7a2120cd057ee2f894411e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;target&lt;/code&gt; that this loss expects should be a class index in the range</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1785ac75c5e40c0724177cfc6462e25d84d9b18" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.distributed&lt;/code&gt; package also provides a launch utility in &lt;code&gt;torch.distributed.launch&lt;/code&gt;. This helper utility can be used to launch multiple processes per node for distributed training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72f50afcb54f84e9f4aac792b1e96785232d670d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.distributed&lt;/code&gt; package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel()&lt;/code&gt;&lt;/a&gt; builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by &lt;a href=&quot;multiprocessing&quot;&gt;Multiprocessing package - torch.multiprocessing&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;torch.nn.DataParallel()&lt;/code&gt;&lt;/a&gt; in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1ce33b04ab70eff6a70899c7f72e87b970b8ffd" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.futures&lt;/code&gt; package is a &lt;strong&gt;Prototype&lt;/strong&gt; feature and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ae3c23dcd7aaa1724a7bbfd85a5d075a7d8913b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.jit.Attribute&lt;/code&gt; wrapper class</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd6ab112f221ddabf781ca4bc22d2c58fdee3a44" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.jit.annotate&lt;/code&gt; function</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="addf386bae846dc433dd4589519a4e51ea1626e9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.layout&lt;/code&gt; class is in beta and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69af7e2fc3132014bb438bfe3f8c1bbddaee2ce2" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.nn.Parameter&lt;/code&gt; wrapper and &lt;code&gt;register_buffer&lt;/code&gt; can be used to assign tensors to a module. Other values assigned to a module that is compiled will be added to the compiled module if their types can be inferred. All &lt;a href=&quot;#types&quot;&gt;types&lt;/a&gt; available in TorchScript can be used as module attributes. Tensor attributes are semantically the same as buffers. The type of empty lists and dictionaries and &lt;code&gt;None&lt;/code&gt; values cannot be inferred and must be specified via &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations. If a type cannot be inferred and is not explicilty annotated, it will not be added as an attribute to the resulting &lt;code&gt;ScriptModule&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd928d04d4959114feb5d458396348030d86627d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;transform_to()&lt;/code&gt; registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution&amp;rsquo;s &lt;code&gt;.arg_constraints&lt;/code&gt; dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cee34c614aefb2bdef2c172499b9302c8a00b05" translate="yes" xml:space="preserve">
          <source>The API is 100% compatible with the original module - it&amp;rsquo;s enough to change &lt;code&gt;import multiprocessing&lt;/code&gt; to &lt;code&gt;import torch.multiprocessing&lt;/code&gt; to have all the tensors sent through the queues or shared via other mechanisms, moved to shared memory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc46cba00894331ffa02531912b869b31d53f5a1" translate="yes" xml:space="preserve">
          <source>The Connectionist Temporal Classification loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68d2c2ff975df64fa09115de4df3b92bf60905da" translate="yes" xml:space="preserve">
          <source>The FFT of a real signal is Hermitian-symmetric, &lt;code&gt;X[i] = conj(X[-i])&lt;/code&gt; so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="554abbeeb98f672613a7ec23a19dc9e48ad28309" translate="yes" xml:space="preserve">
          <source>The FFT of a real signal is Hermitian-symmetric, &lt;code&gt;X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])&lt;/code&gt; so the full &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt; output contains redundant information. &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt; instead omits the negative frequencies in the last dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e13169ea1b51d49a8b92e863c0bf3126ef56123" translate="yes" xml:space="preserve">
          <source>The Fourier domain representation of any real signal satisfies the Hermitian property: &lt;code&gt;X[i] = conj(X[-i])&lt;/code&gt;. This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt; returns the more compact one-sided representation where only the positive frequencies are returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9fe021ce10d1cbbed575b9284c6f59da576001a" translate="yes" xml:space="preserve">
          <source>The Fourier domain representation of any real signal satisfies the Hermitian property: &lt;code&gt;X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])&lt;/code&gt;. This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt; returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ad0a191a9fee1e60e241897ad6f1ce3ecbb965d" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler divergence loss measure</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d60b9fe818a7402cdbe31175747d27cd34ed024b" translate="yes" xml:space="preserve">
          <source>The Nesterov version is analogously modified.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79342dbd5370ca7ac1b96027ceb5844371ecbc95" translate="yes" xml:space="preserve">
          <source>The ONNX exporter can be both &lt;em&gt;trace-based&lt;/em&gt; and &lt;em&gt;script-based&lt;/em&gt; exporter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c11a05ce23a6c40119d293ee087abf34796175d7" translate="yes" xml:space="preserve">
          <source>The ONNX graph C++ definition is in &lt;code&gt;torch/csrc/jit/ir/ir.h&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a8c3a370e79a6afd160a058933e30e88f62a863" translate="yes" xml:space="preserve">
          <source>The Process Group Backend will be deprecated soon, we recommend using the TensorPipe Backend instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c611b23c7d2afdcd9cdf21ee774e5b2b21bffaa1" translate="yes" xml:space="preserve">
          <source>The Process Group agent instantiates a process group from the &lt;a href=&quot;distributed#module-torch.distributed&quot;&gt;&lt;code&gt;distributed&lt;/code&gt;&lt;/a&gt; module and utilizes its point-to-point communication capabilities to send RPC messages. Internally, the process group uses &lt;a href=&quot;https://github.com/facebookincubator/gloo/&quot;&gt;the Gloo library&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a48f9bb3889d63ba5122c625f2532d0293bf1e9f" translate="yes" xml:space="preserve">
          <source>The RPC module can leverage different backends to perform the communication between the nodes. The backend to be used can be specified in the &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; function, by passing a certain value of the &lt;a href=&quot;#torch.distributed.rpc.BackendType&quot;&gt;&lt;code&gt;BackendType&lt;/code&gt;&lt;/a&gt; enum. Regardless of what backend is used, the rest of the RPC API won&amp;rsquo;t change. Each backend also defines its own subclass of the &lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;&lt;code&gt;RpcBackendOptions&lt;/code&gt;&lt;/a&gt; class, an instance of which can also be passed to &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; to configure the backend&amp;rsquo;s behavior.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14ee5344d06a238323bb82692b3ba5195a694523" translate="yes" xml:space="preserve">
          <source>The RPC package also provides decorators which allow applications to specify how a given function should be treated on the callee side.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d97e45dd1ab295237092f3a7f655e6e94838339b" translate="yes" xml:space="preserve">
          <source>The RPC tutorials introduce users to the RPC framework, provide several example applications using &lt;a href=&quot;#distributed-rpc-framework&quot;&gt;torch.distributed.rpc&lt;/a&gt; APIs, and demonstrate how to use &lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;the profiler&lt;/a&gt; to profile RPC-based workloads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1362003f90d57874aad22424c41e2c89849947f1" translate="yes" xml:space="preserve">
          <source>The RRef design note covers the design of the &lt;a href=&quot;#rref&quot;&gt;RRef&lt;/a&gt; (Remote REFerence) protocol used to refer to values on remote workers by the framework.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e195940e5abd6ebbfad00ac76917c59c49c9651e" translate="yes" xml:space="preserve">
          <source>The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time. The interface of this function is modeled after the &lt;a href=&quot;https://librosa.org/doc/latest/generated/librosa.stft.html&quot;&gt;librosa&lt;/a&gt; stft function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56e279d91a2e6e157edb87095248831fbd79e513" translate="yes" xml:space="preserve">
          <source>The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e88b194ff4e6feceb2bfaaa1ec38472db3a49b02" translate="yes" xml:space="preserve">
          <source>The TensorPipe agent, which is the default, leverages &lt;a href=&quot;https://github.com/pytorch/tensorpipe&quot;&gt;the TensorPipe library&lt;/a&gt;, which provides a natively point-to-point communication primitive specifically suited for machine learning that fundamentally addresses some of the limitations of Gloo. Compared to Gloo, it has the advantage of being asynchronous, which allows a large number of transfers to occur simultaneously, each at their own speed, without blocking each other. It will only open pipes between pairs of nodes when needed, on demand, and when one node fails only its incident pipes will be closed, while all other ones will keep working as normal. In addition, it is able to support multiple different transports (TCP, of course, but also shared memory, NVLink, InfiniBand, &amp;hellip;) and can automatically detect their availability and negotiate the best transport to use for each pipe.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d362a2db0f7abde4a8479b2b7feada60ead120fd" translate="yes" xml:space="preserve">
          <source>The TensorPipe backend has been introduced in PyTorch v1.6 and is being actively developed. At the moment, it only supports CPU tensors, with GPU support coming soon. It comes with a TCP-based transport, just like Gloo. It is also able to automatically chunk and multiplex large tensors over multiple sockets and threads in order to achieve very high bandwidths. The agent will be able to pick the best transport on its own, with no intervention required.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe34bfd2269a025104eb8b4a08ae444d0c47f764" translate="yes" xml:space="preserve">
          <source>The TorchScript compiler needs to know the types of &lt;code&gt;module attributes&lt;/code&gt;. Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="108ed42a68e399da0a184bd6d4468cb53c249801" translate="yes" xml:space="preserve">
          <source>The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with &lt;code&gt;requires_grad&lt;/code&gt; set to &lt;code&gt;True&lt;/code&gt;. Below please find a quick guide on what has changed:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d526d524819b552e08647a3450928e08c0be85f0" translate="yes" xml:space="preserve">
          <source>The accuracies of the pre-trained models evaluated on COCO val2017 are as follows</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd6ae7d26b925d9ddede1463aabdca8219c04ca1" translate="yes" xml:space="preserve">
          <source>The algorithm used for interpolation is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50402614be52e24dbcb07aaaf1f84990228308aa" translate="yes" xml:space="preserve">
          <source>The algorithm used for upsampling is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9dbbacc61ab643f749eabb1318de7d252379ec8c" translate="yes" xml:space="preserve">
          <source>The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bdd6673496c6fd17d43bfb63694d67b1a23e7074" translate="yes" xml:space="preserve">
          <source>The approximate decimal resolution of this type, i.e., &lt;code&gt;10**-precision&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11e64a47029713e51445813986a3824ac2548079" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2720b921eb619ce89ed4f91beabc1047581eca82" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eff44d81484d8a3edd9c1b438e09ac7f19f85991" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5cfcdff9c158bfd56a616516f4454be040717f37" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d114863c5d0c0b07094ca0c20e0cd672c5d3f900" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19b889e5a4344dcdd3129928b40b0f46befb2501" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54707c812db6f29dfe9835066cfce0408bd514d9" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;. However, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this instead returns the results multiplied by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bf649a8dfc260fcc9e9737948bc65802a397444" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;. Similar to &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by multiplying it with</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcab133c96e453facb95db0b7ee0dc8c72d699d6" translate="yes" xml:space="preserve">
          <source>The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects &lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;torch.nn.DataParallel&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt; when used with more than one GPU per process (see &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-multigpu&quot;&gt;Working with Multiple GPUs&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf4c102338f979040fb6ff061754881df7061681" translate="yes" xml:space="preserve">
          <source>The backend of the given process group as a lower case string.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e19263dbfb8f739b55183f8eb835a3eac6ee2887" translate="yes" xml:space="preserve">
          <source>The backend options class for &lt;code&gt;ProcessGroupAgent&lt;/code&gt;, which is derived from &lt;code&gt;RpcBackendOptions&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ced0f1f27d8dec6f68219ca478db12401a16eb9d" translate="yes" xml:space="preserve">
          <source>The backend options for &lt;code&gt;TensorPipeAgent&lt;/code&gt;, derived from &lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;&lt;code&gt;RpcBackendOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="502ce5e4564798f5189d3ade0d0a77ccd7a4b686" translate="yes" xml:space="preserve">
          <source>The backward method does not support sparse and complex inputs. It works only when &lt;code&gt;B&lt;/code&gt; is not provided (i.e. &lt;code&gt;B == None&lt;/code&gt;). We are actively working on extensions, and the details of the algorithms are going to be published promptly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55caae9ff73829418d2cf1f4b542673bb0f801f6" translate="yes" xml:space="preserve">
          <source>The backward passes of &lt;a href=&quot;nn.functional#torch.nn.functional.binary_cross_entropy&quot;&gt;&lt;code&gt;torch.nn.functional.binary_cross_entropy()&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/torch.nn.bceloss#torch.nn.BCELoss&quot;&gt;&lt;code&gt;torch.nn.BCELoss&lt;/code&gt;&lt;/a&gt;, which wraps it) can produce gradients that aren&amp;rsquo;t representable in &lt;code&gt;float16&lt;/code&gt;. In autocast-enabled regions, the forward input may be &lt;code&gt;float16&lt;/code&gt;, which means the backward gradient must be representable in &lt;code&gt;float16&lt;/code&gt; (autocasting &lt;code&gt;float16&lt;/code&gt; forward inputs to &lt;code&gt;float32&lt;/code&gt; doesn&amp;rsquo;t help, because that cast must be reversed in backward). Therefore, &lt;code&gt;binary_cross_entropy&lt;/code&gt; and &lt;code&gt;BCELoss&lt;/code&gt; raise an error in autocast-enabled regions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="977219b95a63d49a9a83d568761b5dea94f71247" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used locally.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23b97210326ef320e2d1633f1e69b26cc12a15dc" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1822f0d582e28f7680fb7e40c87465297eaceb93" translate="yes" xml:space="preserve">
          <source>The behavior depends on the dimensionality of the tensors as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f9d3a99640c5733b646d772d2660cd0aaa3e59f" translate="yes" xml:space="preserve">
          <source>The behavior of the model changes depending if it is in training or evaluation mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="671bda2b5e08e92d9c6a9bee65efdef513a603fd" translate="yes" xml:space="preserve">
          <source>The boolean argument &lt;code&gt;eigenvectors&lt;/code&gt; defines computation of both eigenvectors and eigenvalues or eigenvalues only.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a803e781482443f16488bb7ddd92d4c8599d93d8" translate="yes" xml:space="preserve">
          <source>The boolean option &lt;code&gt;sorted&lt;/code&gt; if &lt;code&gt;True&lt;/code&gt;, will make sure that the returned &lt;code&gt;k&lt;/code&gt; elements are themselves sorted</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7230c4dcc10bf6edcaa500d739b7ece219bab321" translate="yes" xml:space="preserve">
          <source>The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="750a4192b0cf8d28e81899b152edaaa9a146f0c9" translate="yes" xml:space="preserve">
          <source>The case when</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40367078c772777ce16b0a9ed2c98fb39bfc9303" translate="yes" xml:space="preserve">
          <source>The centered version first appears in &lt;a href=&quot;https://arxiv.org/pdf/1308.0850v5.pdf&quot;&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cae8914c78d70dfbfebc7267cb87ec8d54626689" translate="yes" xml:space="preserve">
          <source>The check between numerical and analytical gradients uses &lt;a href=&quot;generated/torch.allclose#torch.allclose&quot;&gt;&lt;code&gt;allclose()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dfe4da87960d20e13c57512ebae8f18f5cb935c" translate="yes" xml:space="preserve">
          <source>The checkpoint can be later loaded and inspected under &lt;code&gt;chrome://tracing&lt;/code&gt; URL.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a15a1c2561662d55e6adec5b59058d1e70d3911" translate="yes" xml:space="preserve">
          <source>The columns of the output matrix are elementwise powers of the input vector</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21a375cae9200e37dd0602ab95738f8e9f8a4429" translate="yes" xml:space="preserve">
          <source>The computation for determinant and inverse of covariance matrix is avoided when &lt;code&gt;cov_factor.shape[1] &amp;lt;&amp;lt; cov_factor.shape[0]&lt;/code&gt; thanks to &lt;a href=&quot;https://en.wikipedia.org/wiki/Woodbury_matrix_identity&quot;&gt;Woodbury matrix identity&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_determinant_lemma&quot;&gt;matrix determinant lemma&lt;/a&gt;. Thanks to these formulas, we just need to compute the determinant and inverse of the small size &amp;ldquo;capacitance&amp;rdquo; matrix:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce6592c2eac0498a5cc7f983f7cee34a07c28d68" translate="yes" xml:space="preserve">
          <source>The constructor of &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; can be called without argument, in which case the class is created for the pytorch default dtype (as returned by &lt;a href=&quot;generated/torch.get_default_dtype#torch.get_default_dtype&quot;&gt;&lt;code&gt;torch.get_default_dtype()&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4ae76d927f18889b6dd2cecc1dcf754fe0ab549" translate="yes" xml:space="preserve">
          <source>The contents of a tensor can be accessed and modified using Python&amp;rsquo;s indexing and slicing notation:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c51b27c3ede61443058e137258f2eb8e4fb6bb72" translate="yes" xml:space="preserve">
          <source>The context can be used to retrieve tensors saved during the forward pass. It also has an attribute &lt;code&gt;ctx.needs_input_grad&lt;/code&gt; as a tuple of booleans representing whether each input needs gradient. E.g., &lt;a href=&quot;#torch.autograd.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; will have &lt;code&gt;ctx.needs_input_grad[0] = True&lt;/code&gt; if the first input to &lt;a href=&quot;#torch.autograd.Function.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; needs gradient computated w.r.t. the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d18c64aa54a70cac0b6ada6c6ee8aec0ef1f0917" translate="yes" xml:space="preserve">
          <source>The context can be used to store tensors that can be then retrieved during the backward pass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68f6bd6745cd407cbd762333af1947bd80a473cd" translate="yes" xml:space="preserve">
          <source>The context managers &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.enable_grad#torch.enable_grad&quot;&gt;&lt;code&gt;torch.enable_grad()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt;&lt;code&gt;torch.set_grad_enabled()&lt;/code&gt;&lt;/a&gt; are helpful for locally disabling and enabling gradient computation. See &lt;a href=&quot;autograd#locally-disable-grad&quot;&gt;Locally disabling gradient computation&lt;/a&gt; for more details on their usage. These context managers are thread local, so they won&amp;rsquo;t work if you send work to another thread using the &lt;code&gt;threading&lt;/code&gt; module, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ca40d5babc478c27564467be749f0f09c316678" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;n&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length &lt;code&gt;n&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15f3e882aab04b4ad9da40b79636691c5dd75e80" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;s&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape &lt;code&gt;s&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6651537fc75b50914667cdbdb45bc29550b4b400" translate="yes" xml:space="preserve">
          <source>The corresponding quantized module of &lt;code&gt;mod&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2f2b30024847cce229c4bda1e1b5ab964a9eaf4" translate="yes" xml:space="preserve">
          <source>The criterion only considers a contiguous block of non-negative targets that starts at the front.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e11b9195cb3521aafd237a69ce124b8d788f6886" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2bf74abb2f8760663c776314dd933141deb8a40" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41f3fc6117e6365cbfdd81a5124ee1886a82a466" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;code&gt;Module&lt;/code&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;code&gt;Module&lt;/code&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c92e6421025a94114fff69d31efad27a7d187bee" translate="yes" xml:space="preserve">
          <source>The default behavior (letting &lt;code&gt;.grad&lt;/code&gt;s be &lt;code&gt;None&lt;/code&gt; before the first &lt;code&gt;backward()&lt;/code&gt;, such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to &lt;code&gt;model.zero_grad()&lt;/code&gt; or &lt;code&gt;optimizer.zero_grad()&lt;/code&gt; will not affect &lt;code&gt;.grad&lt;/code&gt; layouts.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fcbf7f37e10aae7ed7b4feee2a1e7bd2fefc4c8" translate="yes" xml:space="preserve">
          <source>The default floating point dtype is initially &lt;code&gt;torch.float32&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fed344676665a74468953d310a0df7afafaa24e3" translate="yes" xml:space="preserve">
          <source>The default floating point tensor type is initially &lt;code&gt;torch.FloatTensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c712a8331c0e06f199c3cd7a844a65de36cd9e42" translate="yes" xml:space="preserve">
          <source>The default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a &lt;code&gt;collate_fn&lt;/code&gt; that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a &lt;code&gt;pin_memory()&lt;/code&gt; method on your custom type(s).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0dec0a713cccbf6f2a025c718d1183b7f17ecc48" translate="yes" xml:space="preserve">
          <source>The default values are designed for &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;grad_outputs&lt;/code&gt; of double precision. This check will likely fail if they are of less precision, e.g., &lt;code&gt;FloatTensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e78814d77915278208123d7230c026ea150426b" translate="yes" xml:space="preserve">
          <source>The default values are designed for &lt;code&gt;input&lt;/code&gt; of double precision. This check will likely fail if &lt;code&gt;input&lt;/code&gt; is of less precision, e.g., &lt;code&gt;FloatTensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a80bd362bedb3fcb6747d9fd75b4961eb0cdb1f6" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68e62e57e835a4d91bc43ca48c95e3efa63535c9" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1984f4c44a0a732d169d5d6a5c5f08cad523bc50" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to a combination of &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6def4e7a63f51a47c9f6b51fb1bb8c54875ce717" translate="yes" xml:space="preserve">
          <source>The distance swap is described in detail in the paper &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;Learning shallow convolutional feature descriptors with triplet losses&lt;/a&gt; by V. Balntas, E. Riba et al.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0ab87bc03e5a5294329af8c45b84a5da57986c5" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework makes it easy to run functions remotely, supports referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backward and update parameters across RPC boundaries. These features can be categorized into four sets of APIs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58cfec2b510135f93e0701fc204e248929b783f2" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework provides mechanisms for multi-machine model training through a set of primitives to allow for remote communication, and a higher-level API to automatically differentiate models split across several machines.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10060f95e0764042598d146fc06132ab3dfae090" translate="yes" xml:space="preserve">
          <source>The distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dafbad2c73a3de536f4d90e5d5fa2fe3c91b9950" translate="yes" xml:space="preserve">
          <source>The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed pacakge in &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; (by explicitly creating the store as an alternative to specifying &lt;code&gt;init_method&lt;/code&gt;.) There are 3 choices for Key-Value Stores: &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0856f15ea144836b8b8d8118fc0a896821ee6c9d" translate="yes" xml:space="preserve">
          <source>The distribution is supported in [0, 1] and parameterized by &amp;lsquo;probs&amp;rsquo; (in (0,1)) or &amp;lsquo;logits&amp;rsquo; (real-valued). Note that, unlike the Bernoulli, &amp;lsquo;probs&amp;rsquo; does not correspond to a probability and &amp;lsquo;logits&amp;rsquo; does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6db33b80979f7c8f4114d1e3c176aabac7ef279b" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77ad1382d4823bfe1261c0964c88050a7ad3d117" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the divisor &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c8c32198d8c41dc6ec53302edb97fbbfa15ce0d" translate="yes" xml:space="preserve">
          <source>The division by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4d19d774aea3434987d4b4cece2b5f99cec9991" translate="yes" xml:space="preserve">
          <source>The dlpack shares the tensors memory. Note that each dlpack can only be consumed once.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a37e21cd1a669630ae636a9ca39104d114477d0e" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic cosine is &lt;code&gt;[1, inf)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for &lt;code&gt;+ INF&lt;/code&gt; for which the output is mapped to &lt;code&gt;+ INF&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9eae4d9c836ec727b1fd15ef73922995c7e82e1" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic tangent is &lt;code&gt;(-1, 1)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for the values &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;-1&lt;/code&gt; for which the output is mapped to &lt;code&gt;+/-INF&lt;/code&gt; respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e24f7a4d115e62a42f56d551f1451d37a5e25266" translate="yes" xml:space="preserve">
          <source>The dynamic control flow is captured correctly. We can verify in backends with different loop range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a418864112d90d162f2d2d68264a78b5661b0c49" translate="yes" xml:space="preserve">
          <source>The eigenvalues are returned in ascending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70c72529d7b55dc5ab2b127843a9a42c7179e611" translate="yes" xml:space="preserve">
          <source>The elements are sorted into equal width bins between &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;. If &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt; are both zero, the minimum and maximum values of the data are used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef67a3a1e1ceed752fed1303fe693fb7bb4731f9" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;Backend.UNDEFINED&lt;/code&gt; is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0ed2b956d8f2913276ea868cc2cc1c195e28811" translate="yes" xml:space="preserve">
          <source>The example script above produces the graph:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8850a69d0e77cfdf6128ac9dcc486c371b38e541" translate="yes" xml:space="preserve">
          <source>The export fails because PyTorch does not support exporting &lt;code&gt;elu&lt;/code&gt; operator. We find &lt;code&gt;virtual Tensor elu(const Tensor &amp;amp; input, Scalar alpha, bool inplace) const override;&lt;/code&gt; in &lt;code&gt;VariableType.h&lt;/code&gt;. This means &lt;code&gt;elu&lt;/code&gt; is an ATen operator. We check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;, and confirm that &lt;code&gt;Elu&lt;/code&gt; is standardized in ONNX. We add the following lines to &lt;code&gt;symbolic_opset9.py&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eaaaa8926b11cbb1c679608a1678277381f47dca" translate="yes" xml:space="preserve">
          <source>The fact that gradients need to be computed for a Tensor do not mean that the &lt;a href=&quot;#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; attribute will be populated, see &lt;a href=&quot;#torch.Tensor.is_leaf&quot;&gt;&lt;code&gt;is_leaf&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9a2121d113f583a300c4eeb7ced28dd018af26f" translate="yes" xml:space="preserve">
          <source>The fact that gradients need to be computed for a Tensor do not mean that the &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; attribute will be populated, see &lt;a href=&quot;autograd#torch.Tensor.is_leaf&quot;&gt;&lt;code&gt;is_leaf&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="221c8b91c0e460724cf31fb2005c0c9e54071204" translate="yes" xml:space="preserve">
          <source>The first call to add for a given &lt;code&gt;key&lt;/code&gt; creates a counter associated with &lt;code&gt;key&lt;/code&gt; in the store, initialized to &lt;code&gt;amount&lt;/code&gt;. Subsequent calls to add with the same &lt;code&gt;key&lt;/code&gt; increment the counter by the specified &lt;code&gt;amount&lt;/code&gt;. Calling &lt;code&gt;add()&lt;/code&gt; with a key that has already been set in the store by &lt;code&gt;set()&lt;/code&gt; will result in an exception.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d04a7a0c088c359fab554cd1a6049882b564ff2e" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26a3da676366966fcc7fbfc997f2f64e6ee31c9d" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in &lt;code&gt;VariableType.h&lt;/code&gt;, because dispatch is done with keyword arguments.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5964988cdbc48443556dc471facbf25fa24c6e63" translate="yes" xml:space="preserve">
          <source>The following APIs allow users to remotely execute functions as well as create references (RRefs) to remote data objects. In these APIs, when passing a &lt;code&gt;Tensor&lt;/code&gt; as an argument or a return value, the destination worker will try to create a &lt;code&gt;Tensor&lt;/code&gt; with the same meta (i.e., shape, stride, etc.). We intentionally disallow transmitting CUDA tensors because it might crash if the device lists on source and destination workers do not match. In such cases, applications can always explicitly move the input tensors to CPU on the caller and move it to the desired devices on the callee if necessary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0331e44de903db51dab5949d62e369ce763fc069" translate="yes" xml:space="preserve">
          <source>The following Python Expressions are supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a58ae690d15d348f9e4dcb69eeff8937464b7b91" translate="yes" xml:space="preserve">
          <source>The following constraints are implemented:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f123fb71a763a700ae8133c3cf978d6e15341a63" translate="yes" xml:space="preserve">
          <source>The following factory functions support named tensors:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08607d57d85e6e823913a7de503035f5f9376e6f" translate="yes" xml:space="preserve">
          <source>The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a &lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;torch.nn.Module&lt;/code&gt;&lt;/a&gt;, as a function, or as a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="690193766b4768be386db05b742730b7dbdf34e9" translate="yes" xml:space="preserve">
          <source>The following methods are unique to &lt;a href=&quot;#torch.BoolTensor&quot;&gt;&lt;code&gt;torch.BoolTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fcc9561d17b3f2c0c3db2c65c4b45e1fd39309f" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will act deterministically when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c856d87fb36615ab5d3e65d2705a558ef7fc7ab" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will throw a &lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt;&lt;code&gt;RuntimeError&lt;/code&gt;&lt;/a&gt; when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10cc3c798c883edec11b18c9b813ac5a197463fe" translate="yes" xml:space="preserve">
          <source>The following operators are supported:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="116feb31ceb0c75370eb04add5159eb16382e56b" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in PyTorch 1.8. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.fftn&quot;&gt;&lt;code&gt;torch.fft.fftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18bbed5d4e21fbcafc8624ec507383dc653a9737" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.ifft&quot;&gt;&lt;code&gt;torch.ifft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.ifftn&quot;&gt;&lt;code&gt;torch.fft.ifftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1a40cf129fc842a7fe39cfe2023111384b05a6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;torch.irfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.irfft&quot;&gt;&lt;code&gt;torch.fft.irfft()&lt;/code&gt;&lt;/a&gt; for one-sided input, or &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; for two-sided input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0db51d208a07883c44d7be8c9786454460ee522" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.rfft&quot;&gt;&lt;code&gt;torch.rfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.rfft&quot;&gt;&lt;code&gt;torch.fft.rfft()&lt;/code&gt;&lt;/a&gt; for one-sided output, or &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; for two-sided output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10b975abfe7a8de3222390632564c8c39557329e" translate="yes" xml:space="preserve">
          <source>The function is called as &lt;code&gt;fn(i, *args)&lt;/code&gt;, where &lt;code&gt;i&lt;/code&gt; is the process index and &lt;code&gt;args&lt;/code&gt; is the passed through tuple of arguments.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78c07c2cfdea987b1b930691f54fd99adb7a7932" translate="yes" xml:space="preserve">
          <source>The function is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81e6543a378b1e27df259854cebe693b6ae023b6" translate="yes" xml:space="preserve">
          <source>The gated linear unit. Computes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2ac4a47b4d32b1f56737b1e1c139ea56803b6cc" translate="yes" xml:space="preserve">
          <source>The graph is differentiated using the chain rule. If any of &lt;code&gt;tensors&lt;/code&gt; are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying &lt;code&gt;grad_tensors&lt;/code&gt;. It should be a sequence of matching length, that contains the &amp;ldquo;vector&amp;rdquo; in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (&lt;code&gt;None&lt;/code&gt; is an acceptable value for all tensors that don&amp;rsquo;t need gradient tensors).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c30476379351f558b5c5fb5c388947e81c5eddf9" translate="yes" xml:space="preserve">
          <source>The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying &lt;code&gt;gradient&lt;/code&gt;. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e635ba31c5601250c0b5605418f5a52e38b9a9ea" translate="yes" xml:space="preserve">
          <source>The histogram is computed continuously, and the ranges per bin change with every new tensor observed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fce725259bdabadb8950b8c6e8b1cc9bdca5a28" translate="yes" xml:space="preserve">
          <source>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of &lt;a href=&quot;#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63a016154b7848c498c0189a691b12aee6391726" translate="yes" xml:space="preserve">
          <source>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3b8c649dc7d5a1913425cba9308cbaac351ab8a" translate="yes" xml:space="preserve">
          <source>The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91b8909b6b0301d53bc827787fcd07621d744139" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; has computed an output. It should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea6266d19428a156d329cc252aacfffb34135bcf" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;code&gt;forward()&lt;/code&gt; has computed an output. It should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3027e825f33c77d15aef5c4a000dacdcd543639b" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is invoked. It should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="658103272c0bdd847853bfee8c1acd84bd1873a4" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;code&gt;forward()&lt;/code&gt; is invoked. It should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7317d93b9ecea6baa18606aa1761b42a13618f5" translate="yes" xml:space="preserve">
          <source>The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47b9323acd4c956c6840fecee45ba64f27f04f5f" translate="yes" xml:space="preserve">
          <source>The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute &amp;ndash; that is, contain a small number of assigned labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53da9964154c69f0225ac83d2e63e3e638d7b44d" translate="yes" xml:space="preserve">
          <source>The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd1e3da6ac802ac499dbc929d3c01e7d8d1e592e" translate="yes" xml:space="preserve">
          <source>The implementation is based on the Algorithm 5.1 from Halko et al, 2009.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b39161781b1b7b35ca70ad2826501e304e14740d" translate="yes" xml:space="preserve">
          <source>The implementation is based on: Bader, P.; Blanes, S.; Casas, F. Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation. Mathematics 2019, 7, 1174.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9e5eb34b5f671210df55afa7cf37ff9e10daa69" translate="yes" xml:space="preserve">
          <source>The implementation of SGD with Momentum/Nesterov subtly differs from Sutskever et. al. and implementations in some other frameworks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9d89b5b4a75085b7eb81dcbff4669f9ff584606" translate="yes" xml:space="preserve">
          <source>The implementation of SVD on CPU uses the LAPACK routine &lt;code&gt;?gesdd&lt;/code&gt; (a divide-and-conquer algorithm) instead of &lt;code&gt;?gesvd&lt;/code&gt; for speed. Analogously, the SVD on GPU uses the MAGMA routine &lt;code&gt;gesdd&lt;/code&gt; as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c05db592ff41dfc6119d9765d1f98df7bbeac593" translate="yes" xml:space="preserve">
          <source>The implementations of the models for object detection, instance segmentation and keypoint detection are efficient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ee40d087e274de07c646cdfb7f973b9dab8f9cf" translate="yes" xml:space="preserve">
          <source>The inferred dtype for python floats in &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c9a1b1a8c22e228f94e607d9ba23a7a9895221c" translate="yes" xml:space="preserve">
          <source>The input &lt;code&gt;window_length&lt;/code&gt; is a positive integer controlling the returned window size. &lt;code&gt;periodic&lt;/code&gt; flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;. Therefore, if &lt;code&gt;periodic&lt;/code&gt; is true, the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dffb8d48e1eff38cec4958715ef4e4bea17673f9" translate="yes" xml:space="preserve">
          <source>The input channels are separated into &lt;code&gt;num_groups&lt;/code&gt; groups, each containing &lt;code&gt;num_channels / num_groups&lt;/code&gt; channels. The mean and standard-deviation are calculated separately over the each group.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24d9bce9a9af270abcd63358a34e29c31cf80c6f" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="501f21d86b3222c931438d1d37f49ef014fe1070" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d84cfbddddc9e1c2d0a053623e51f4c9b23ed343" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;code&gt;forward()&lt;/code&gt; is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0bbaa069be68586598b46c411d0f2665ffc0e9d" translate="yes" xml:space="preserve">
          <source>The input data is assumed to be of the form &lt;code&gt;minibatch x channels x [optional depth] x [optional height] x width&lt;/code&gt;. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="855506544cad1eeac05a21efea97a3853e2dceb9" translate="yes" xml:space="preserve">
          <source>The input dimensions are interpreted in the form: &lt;code&gt;mini-batch x channels x [optional depth] x [optional height] x width&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff7b890633934634eb44c66753eb478acaced44e" translate="yes" xml:space="preserve">
          <source>The input is assumed to be a low-rank matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa72bb59a173177382b9660fee81f62bda47bdae" translate="yes" xml:space="preserve">
          <source>The input quantization parameters are propagated to the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c8e7facaf470e54c3381c46dad3dd11548e58de" translate="yes" xml:space="preserve">
          <source>The input quantization parameters propagate to the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c93ac513b2302a4f83a43405b9a47bcb2309e67" translate="yes" xml:space="preserve">
          <source>The input to the model is expected to be a list of tensors, each of shape &lt;code&gt;[C, H, W]&lt;/code&gt;, one for each image, and should be in &lt;code&gt;0-1&lt;/code&gt; range. Different images can have different sizes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33129e152019831a4d6451e1dcb0878f56c1bef2" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.&lt;/code&gt; prefix for some operations. See example usage below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="351e07f8a31af7d82fc6045df4c793836de62f62" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.ops.quantized&lt;/code&gt; prefix. See example usage below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c3dcf3c5f75afd5957f352f9917b83b90088988" translate="yes" xml:space="preserve">
          <source>The interface for specifying operator definitions is a Prototype feature; adventurous users should note that the APIs will probably change in a future interface.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bb29d8ff742c3f78ac5a2da01f22239940307d9" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ff786d8f2651e07ce1136605dc8809d77fc8a6a" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd26976415ff02dd321f8aabd58b78a55df77549" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.irfft#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51e2eb96b40c3aad396917c7dd98de9a6585beea" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f2e4deb4f5b0c2c822ac4f786f365c2a6921f20" translate="yes" xml:space="preserve">
          <source>The jvp is currently computed by using the backward of the backward (sometimes called the double backwards trick) as we don&amp;rsquo;t have support for forward mode AD in PyTorch at the moment.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b9e7887dd437999f146f02a0f60028f96d84bf3" translate="yes" xml:space="preserve">
          <source>The largest difference between TorchScript and the full Python language is that TorchScript only supports a small set of types that are needed to express neural net models. In particular, TorchScript supports:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b1d6d882f09df5877abb8ef200d99a4552edbcc" translate="yes" xml:space="preserve">
          <source>The largest representable number.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63b23adb215af0fe9eaa51ef9153973e0c53546f" translate="yes" xml:space="preserve">
          <source>The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08f3358598c31282b2114da38bcad5d74e704334" translate="yes" xml:space="preserve">
          <source>The locations are used in the order of</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ebd3a640891ee47a9a38fbd56c89724e57e2af2" translate="yes" xml:space="preserve">
          <source>The loss can be described as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f21bd0e4e809cda5ba13223c1dfb3494e56f4f3e" translate="yes" xml:space="preserve">
          <source>The loss function for</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86081d5fc32148129e542f3dc9192fb2857da205" translate="yes" xml:space="preserve">
          <source>The loss function for each pair of samples in the mini-batch is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb7d036d43546cb725a6bf9bf29c5cc2076ac418" translate="yes" xml:space="preserve">
          <source>The loss function for each sample in the mini-batch is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48f8488012ce60a4d13e1aebd7fa0ba10061eaf3" translate="yes" xml:space="preserve">
          <source>The loss function for each sample is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c43bc97db5e0527c42aa84dc0f06c5fb6d69ddbc" translate="yes" xml:space="preserve">
          <source>The loss function then becomes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dbd64b8b1e6f8a768dc1cc623e35d46784438d27" translate="yes" xml:space="preserve">
          <source>The losses are averaged across observations for each minibatch. If the &lt;code&gt;weight&lt;/code&gt; argument is specified then this is a weighted average:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="187e00ce183312dd132d6826870b8e0983ae12a8" translate="yes" xml:space="preserve">
          <source>The lower triangular part of the matrix is defined as the elements on and below the diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cba1d04dc7071338e4468867933f6f030f4f19fd" translate="yes" xml:space="preserve">
          <source>The machine with rank 0 will be used to set up all connections.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6b0320b620362b6deb5bb70025cb38b632f6a57" translate="yes" xml:space="preserve">
          <source>The main trick for &lt;code&gt;hard&lt;/code&gt; is to do &lt;code&gt;y_hard - y_soft.detach() + y_soft&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61caa3335965d2f3a59a97a54b15a39417eaac38" translate="yes" xml:space="preserve">
          <source>The max-pooling operation is applied in</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d65dca627767b86d15d82ece9fa3008acf19101f" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3242427612d3b3c3e0cb9845d6bf7db1596d5b9" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over the mini-batches and</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5661df9ad4405c387ea95d88a69a9fe1650b1552" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29fab6215a0696acbd3e84d8cb442b324881c861" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by &lt;code&gt;normalized_shape&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a930aa85422123ed7ce795a3af91990ae6ee8f7" translate="yes" xml:space="preserve">
          <source>The mean operation still operates over all the elements, and divides by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="619af861d215de1b87a6226cb50c28f6e623bb30" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements in the dimension &lt;code&gt;dim&lt;/code&gt;. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d7cf56bfc5cedceb3ca00313d8c3d414937caef" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3568ef228089a375953fda3fd0c97aca0ace05a3" translate="yes" xml:space="preserve">
          <source>The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f15c86c9dfb63d5c112adb09bde9cb61a9fe3f2e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="065007e637ab5dd9f6a9a41f5c632d9aa1aa3f5e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the mask loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0c3a4815182bd38a59e8d48bd1acda3676df3d3" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25fbe42f5bebb67d721733ebaa470925353914ab" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6df7f86985b4db8720a0303772bb804adf7b58b" translate="yes" xml:space="preserve">
          <source>The model will be attached with observer or fake quant modules, and qconfig will be propagated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32c7b059b3336b886e2737e75a7a90bf14890a11" translate="yes" xml:space="preserve">
          <source>The models expect a list of &lt;code&gt;Tensor[C, H, W]&lt;/code&gt;, in the range &lt;code&gt;0-1&lt;/code&gt;. The models internally resize the images so that they have a minimum size of &lt;code&gt;800&lt;/code&gt;. This option can be changed by passing the option &lt;code&gt;min_size&lt;/code&gt; to the constructor of the models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8cd112b9466880b251d727f36c9b5a8099f6121" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for detection:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fd7064c0a6dbdb00e5c05b4bcf7bb996635bfb3" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for image classification:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a82a240d4b1d0867d7d72a1a3f555d48973b4c7" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for semantic segmentation:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18bc003911e615e5dba9603961f512f27acb297a" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cf46d13a21b259a13e6ce62948cca48ddbc78f6" translate="yes" xml:space="preserve">
          <source>The modes available for resizing are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only), &lt;code&gt;area&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28d8cfd32d3249e2763e181ffc749144e0ea24a2" translate="yes" xml:space="preserve">
          <source>The modes available for upsampling are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18b2caf95e73a1c791145d0b79ff330ee08dd6df" translate="yes" xml:space="preserve">
          <source>The module can be accessed as an attribute using the given name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4279942bb80ec4b3ccf751b2de250bbb42e94404" translate="yes" xml:space="preserve">
          <source>The module is mainly for debug and records the tensor values during runtime.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbcdfca1ae0201a3feb424a59be4618b6c74eaeb" translate="yes" xml:space="preserve">
          <source>The module records the running histogram of tensor values along with min/max values. &lt;code&gt;calculate_qparams&lt;/code&gt; will calculate scale and zero_point.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00f10b867e105cafb015ef514a5b43106e8a7867" translate="yes" xml:space="preserve">
          <source>The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8d325f814f94943441bf4453b4ce6fd4bb34555" translate="yes" xml:space="preserve">
          <source>The most important argument of &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; constructor is &lt;code&gt;dataset&lt;/code&gt;, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85f2f05d4d25b4d7e3119d033b3d3b1eb5b1a272" translate="yes" xml:space="preserve">
          <source>The moving average min/max is computed as follows</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1664b05e14f7fa9291ef99d0be2e3353189a5613" translate="yes" xml:space="preserve">
          <source>The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f00cd6c1142679bd0fe1c3ebd4ca78600af96a2c" translate="yes" xml:space="preserve">
          <source>The name of the worker.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fba68059efcba7726021d64196e7ad3e2e4fcb96" translate="yes" xml:space="preserve">
          <source>The named tensor API is a prototype feature and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed535f6a7c3020c739db9c59caec32bca959d97b" translate="yes" xml:space="preserve">
          <source>The named tensor API is experimental and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36555b023721ec5738f0c183a135318d3c9a4466" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80880349d7cc0a615947c8d7ecadbb60eab216c2" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss. It is useful to train a classification problem with &lt;code&gt;C&lt;/code&gt; classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07e33a3c723549700a6c484c42061c0b83f6423c" translate="yes" xml:space="preserve">
          <source>The new backend derives from &lt;code&gt;c10d.ProcessGroup&lt;/code&gt; and registers the backend name and the instantiating interface through &lt;code&gt;torch.distributed.Backend.register_backend()&lt;/code&gt; when imported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ef80d57536dd4f889b111944ebd026c0916a56b" translate="yes" xml:space="preserve">
          <source>The new usage looks like this:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfa45b6345caf33366ff2d44c58d6e6325263da6" translate="yes" xml:space="preserve">
          <source>The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="760d37d3c11fd75fa728a8e00ba020ac1e72a10c" translate="yes" xml:space="preserve">
          <source>The normalization parameters are different from the image classification ones, and correspond to the mean and std from Kinetics-400.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b4b699c449424f5a0ee9c963faafecf0fd0d061" translate="yes" xml:space="preserve">
          <source>The number of bins (size 1) is one larger than the largest value in &lt;code&gt;input&lt;/code&gt; unless &lt;code&gt;input&lt;/code&gt; is empty, in which case the result is a tensor of size 0. If &lt;code&gt;minlength&lt;/code&gt; is specified, the number of bins is at least &lt;code&gt;minlength&lt;/code&gt; and if &lt;code&gt;input&lt;/code&gt; is empty, then the result is tensor of size &lt;code&gt;minlength&lt;/code&gt; filled with zeros. If &lt;code&gt;n&lt;/code&gt; is the value at position &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;out[n] += weights[i]&lt;/code&gt; if &lt;code&gt;weights&lt;/code&gt; is specified else &lt;code&gt;out[n] += 1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="792cc1a58f0a3a46d19118877b6d13c493cb98ec" translate="yes" xml:space="preserve">
          <source>The number of bits occupied by the type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="280e9a9c6268d27aec18f9552a0a0e023b3a5d6c" translate="yes" xml:space="preserve">
          <source>The number of keys present in the store.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb1db44aaa6eaf0c2b9f7e2d3594ec089fe36151" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by &lt;code&gt;TensorPipeAgent&lt;/code&gt; to execute requests.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20a68b367a08bf827b8cb29b9a7a5775403402ae" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by ProcessGroupAgent.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25f85e0ff2381c99b81c5c68406ed2a516dd1318" translate="yes" xml:space="preserve">
          <source>The numerical properties of a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; can be accessed through either the &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; or the &lt;a href=&quot;#torch.torch.iinfo&quot;&gt;&lt;code&gt;torch.iinfo&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8e759a2b7197a6d6914a25b2e4bccfa94711180" translate="yes" xml:space="preserve">
          <source>The operation applied is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b61e3d8bf7f65d3c8a372be2ffe4aa181113744" translate="yes" xml:space="preserve">
          <source>The operation is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="416029866582b442a7cfd38c944f5ccc41de6ea3" translate="yes" xml:space="preserve">
          <source>The operator set above is sufficient to export the following models:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97549a235d386a02f6c874f871b2a3fc368e849a" translate="yes" xml:space="preserve">
          <source>The order of norm. inf refers to &lt;code&gt;float('inf')&lt;/code&gt;, numpy&amp;rsquo;s &lt;code&gt;inf&lt;/code&gt; object, or any equivalent object. The following norms can be calculated:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="587438e00b276c5c07c33c3b833d1847c428edb1" translate="yes" xml:space="preserve">
          <source>The original &lt;code&gt;module&lt;/code&gt; with the converted &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layers. If the original &lt;code&gt;module&lt;/code&gt; is a &lt;code&gt;BatchNorm*D&lt;/code&gt; layer, a new &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layer object will be returned instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="016c290cf6497163981ebcd1b0530771762971db" translate="yes" xml:space="preserve">
          <source>The original Adam algorithm was proposed in &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;. The AdamW variant was proposed in &lt;a href=&quot;https://arxiv.org/abs/1711.05101&quot;&gt;Decoupled Weight Decay Regularization&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c43e721e428fefa0921064ef9d974f9d9c3710a" translate="yes" xml:space="preserve">
          <source>The original module with the spectral norm hook</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c8ee61f168965198d32cd38694cc41b385bdcc7" translate="yes" xml:space="preserve">
          <source>The original module with the weight norm hook</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ad8002d2700baeb804ee6b717ba471c2750abdc" translate="yes" xml:space="preserve">
          <source>The other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the &lt;code&gt;rsample()&lt;/code&gt; method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7c71aa7150538e239f1ed9763931d65c1aab1d1" translate="yes" xml:space="preserve">
          <source>The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d13bc6b68893874e15ef23c3e8bf09f29542e58" translate="yes" xml:space="preserve">
          <source>The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd1aa83704336b7c60e0ac15205527d8852065d5" translate="yes" xml:space="preserve">
          <source>The output of the &lt;code&gt;model&lt;/code&gt; callable when called with the given &lt;code&gt;*args&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2624c3f93c8a5ba7f8db76475b9b2160784ad8be" translate="yes" xml:space="preserve">
          <source>The output size is H, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaed500780776d71aa7aa3beac0d23a2ad0bee52" translate="yes" xml:space="preserve">
          <source>The output tuple size must match the outputs of &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69fc4cd3bd84d50752da22ec0fa2da75a028b6c8" translate="yes" xml:space="preserve">
          <source>The package needs to be initialized using the &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; function before calling any other methods. This blocks until all processes have joined.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a24470d7f3dafd97610b1a97cfc4e625080f5d77" translate="yes" xml:space="preserve">
          <source>The padding size by which to pad some dimensions of &lt;code&gt;input&lt;/code&gt; are described starting from the last dimension and moving forward.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3d894892755f58e334d42f2a6bb03ad3c9c1029" translate="yes" xml:space="preserve">
          <source>The parallelized &lt;code&gt;module&lt;/code&gt; must have its parameters and buffers on &lt;code&gt;device_ids[0]&lt;/code&gt; before running this &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt; module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="611179cf7c23e5fa87e86dc65fc6c2d7edcfcdbf" translate="yes" xml:space="preserve">
          <source>The parameter can be accessed as an attribute using given name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3673b833c2a753da5c92e7b2bd7867575cba0695" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; can either be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb60e522db72c109467c75d771e518ca64062c0c" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can each be an &lt;code&gt;int&lt;/code&gt; or a one-element tuple.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df694fbe631671fb53535c127328a8f9b5ccfdfe" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can either be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d35449e64c1f14944b10572266aa02084f0787f" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt; can either be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16e542ef2d9e6480cf0213002d643e04ad39afd9" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;output_padding&lt;/code&gt; can either be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9df0d4116a648b1a73d8fd1ad856ffa0c5b125d9" translate="yes" xml:space="preserve">
          <source>The parameters represented by a single vector</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16c253cfff05379e83f73283d75913a9d3bc5eb0" translate="yes" xml:space="preserve">
          <source>The pivots returned by the function are 1-indexed. If &lt;code&gt;pivot&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the returned pivots is a tensor filled with zeros of the appropriate size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4600b83baa50f6eee75af80c6cdb275baf5a4820" translate="yes" xml:space="preserve">
          <source>The pre-trained models for detection, instance segmentation and keypoint detection are initialized with the classification models in torchvision.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bacc13981715b20b015df75a9953a0a7c1d88d4c" translate="yes" xml:space="preserve">
          <source>The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in &lt;code&gt;references/segmentation/coco_utils.py&lt;/code&gt;. The classes that the pre-trained model outputs are the following, in order:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f5bcce5c35434b7def7a2c2abecc7855a7ff6ae" translate="yes" xml:space="preserve">
          <source>The process for obtaining the values of &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;std&lt;/code&gt; is roughly equivalent to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2eeaf80bb67fd9965166125db5e39a93b156744c" translate="yes" xml:space="preserve">
          <source>The provided mean is the circular one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a5bce0d740da57838aed1d4d747fbb1351683c7" translate="yes" xml:space="preserve">
          <source>The provided variance is the circular one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb1f1b1b62d82d262fa3ee527ff2235c6fe33cf0" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse is not necessarily a continuous function in the elements of the matrix &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/0117004&quot;&gt;[1]&lt;/a&gt;. Therefore, derivatives are not always existent, and exist for a constant rank only &lt;a href=&quot;https://www.jstor.org/stable/2156365&quot;&gt;[2]&lt;/a&gt;. However, this method is backprop-able due to the implementation by using SVD results, and could be unstable. Double-backward will also be unstable due to the usage of SVD internally. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df88bf331dfcd873fd61e2a961e2a4565dca71ab" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of &lt;code&gt;input&lt;/code&gt; of dimensions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="801cd7b0289a0cab3f1164523d810bc285cdd10c" translate="yes" xml:space="preserve">
          <source>The published models should be at least in a branch/tag. It can&amp;rsquo;t be a random commit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5002f22d2309c888525404ef88b8a18252cac236" translate="yes" xml:space="preserve">
          <source>The quantization parameters are computed the same way as in &lt;code&gt;MinMaxObserver&lt;/code&gt;, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37425d4454f834297b034206170c35abd72a213b" translate="yes" xml:space="preserve">
          <source>The quantization parameters are computed the same way as in &lt;code&gt;MovingAverageMinMaxObserver&lt;/code&gt;, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="346d775a68239ce50fc9c648f8e7ac9c30934f97" translate="yes" xml:space="preserve">
          <source>The range of the linear region</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a67bc4760995b69a50ba7bb512777bd75a29786d" translate="yes" xml:space="preserve">
          <source>The rank of the process group -1, if not part of the group</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1640b5e94a24bbde9b57f55563952bd38f53f1c7" translate="yes" xml:space="preserve">
          <source>The real-to-complex Fourier transform results follow conjugate symmetry:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3326dbe4c730be437e49c0a1205659d772531b4" translate="yes" xml:space="preserve">
          <source>The regular implementation uses the (more common in PyTorch) &lt;code&gt;torch.long&lt;/code&gt; dtype.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f42ad6925b2f350139e05854ee56333bfbf81bfa" translate="yes" xml:space="preserve">
          <source>The relation of &lt;code&gt;(U, S, V)&lt;/code&gt; to PCA is as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="946fd419edbe7367b996132ceeb5715b2129c56a" translate="yes" xml:space="preserve">
          <source>The rest of this section concerns the case with &lt;a href=&quot;#map-style-datasets&quot;&gt;map-style datasets&lt;/a&gt;. &lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;&lt;code&gt;torch.utils.data.Sampler&lt;/code&gt;&lt;/a&gt; classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a &lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;&lt;code&gt;Sampler&lt;/code&gt;&lt;/a&gt; could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0dee4722eb608774d930f39be3f46f18bcb0b430" translate="yes" xml:space="preserve">
          <source>The result will never require gradient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b0575a2f172061283c10c05f0db13db17166b95" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;alexnet.onnx&lt;/code&gt; is a binary protobuf file which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The keyword argument &lt;code&gt;verbose=True&lt;/code&gt; causes the exporter to print out a human-readable representation of the network:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c02f08f3d1024ae629cdcc31caf60626bc7a365" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;out&lt;/code&gt; tensor shares it&amp;rsquo;s underlying storage with the &lt;code&gt;input&lt;/code&gt; tensor, so changing the content of one would change the content of the other.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6758477f8561982cc2515cdbf897a47792390375" translate="yes" xml:space="preserve">
          <source>The resulting recording of &lt;code&gt;nn.Module.forward&lt;/code&gt; or &lt;code&gt;nn.Module&lt;/code&gt; produces &lt;code&gt;ScriptModule&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="094611e344ec0108ea463a9054c21e208ca824dd" translate="yes" xml:space="preserve">
          <source>The resulting recording of a standalone function produces &lt;code&gt;ScriptFunction&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e1976b95811fe3c223eccaa3dc30d2bb2ed6c6f" translate="yes" xml:space="preserve">
          <source>The return value of this function is a dictionary of statistics, each of which is a non-negative integer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14a7d916a182796f96dea780975d504da8d70e49" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object can come from &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; constructor. The example below shows directly using the &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; returned by &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35586b86275e1afc45c84727656b22c56c0dfc4c" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;out&lt;/code&gt; tensor only has values 0 or 1 and is of the same shape as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c1ca22658d19c73ead093d5a482f3f0748946cd" translate="yes" xml:space="preserve">
          <source>The returned Tensor&amp;rsquo;s data will be of size &lt;code&gt;T x B x *&lt;/code&gt;, where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence and &lt;code&gt;B&lt;/code&gt; is the batch size. If &lt;code&gt;batch_first&lt;/code&gt; is True, the data will be transposed into &lt;code&gt;B x T x *&lt;/code&gt; format.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a86d643d348528641013d71d94cfc9c2436b5f5a" translate="yes" xml:space="preserve">
          <source>The returned matrices will always be transposed, irrespective of the strides of the input matrices. That is, they will have stride &lt;code&gt;(1, m)&lt;/code&gt; instead of &lt;code&gt;(m, 1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e874ec4d7da2420c99c0303e4f7a958324a670ae" translate="yes" xml:space="preserve">
          <source>The returned tensor and &lt;code&gt;ndarray&lt;/code&gt; share the same memory. Modifications to the tensor will be reflected in the &lt;code&gt;ndarray&lt;/code&gt; and vice versa. The returned tensor is not resizable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93c4d85268a064ece907fbc8a5535a6f396d9a4e" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52f5aa4074a60eef379e1658572f1bb6509ef7da" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor. If &lt;code&gt;out&lt;/code&gt; has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f46100c8ee7a830bab0c89e9873612006ce16b25" translate="yes" xml:space="preserve">
          <source>The returned tensor has the same number of dimensions as the original tensor (&lt;code&gt;input&lt;/code&gt;). The &lt;code&gt;dim&lt;/code&gt;th dimension has the same size as the length of &lt;code&gt;index&lt;/code&gt;; other dimensions have the same size as in the original tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c72a07e12ac6ea766a45eac39941a4c32ee2a049" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e400aae54e1364ef7f0ae0e4394533a68ef83da8" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same underlying data with this tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="172826726b64a692f9e7f61dbdcbf5a0b8b36f39" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28c770021d54b212e8c62be1e09939a0369312ae" translate="yes" xml:space="preserve">
          <source>The rows of &lt;code&gt;input&lt;/code&gt; do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67ad32a77054c34abcbd23ca0d1823d1b895c487" translate="yes" xml:space="preserve">
          <source>The running minimum/maximum</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a731d56c62ca577d4cbb49839358e51e33b8ca80" translate="yes" xml:space="preserve">
          <source>The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. &amp;ldquo;Efficient simulation of the von Mises distribution.&amp;rdquo; Applied Statistics (1979): 152-157.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a088efcbb868e84088d5a0662d97d3ed3613744" translate="yes" xml:space="preserve">
          <source>The scale</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="425de0e2446cb018a534e98b7c1b54edd4cbbc42" translate="yes" xml:space="preserve">
          <source>The scale and zero point are computed as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e92d877ce880170691af2f314602d7f6a781ea7" translate="yes" xml:space="preserve">
          <source>The scale and zero point are then computed as in &lt;code&gt;MinMaxObserver&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a5fb7f0444cbf25c6894566e5c0f368d5dfcf33" translate="yes" xml:space="preserve">
          <source>The search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1eff8f651f1c1e81084414db9ad8e86ff7d386f5" translate="yes" xml:space="preserve">
          <source>The second argument can be a number or a tensor whose shape is &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the first argument.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8fc5e267a8ddd57a603f273d62b726d831286e07" translate="yes" xml:space="preserve">
          <source>The sections below describe in details the effects and usages of these options.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35bad0572344e9ca340bcfea7b7e08eb1fb48094" translate="yes" xml:space="preserve">
          <source>The shape of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44ad26e34a9ec24516da93c641da630606e76045" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt; don&amp;rsquo;t need to match, but the total number of elements in each tensor need to be the same.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ab58315d98cc62bd1c43b5269219135f914a361" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="472d04a7b4ef19856846b867c2431ae86dfeee09" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4bb8d54bf4e66fa1d51318971fbd84c5dd471f64" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c2b5afb57ab65d651f6e8673ddc0bb16bc251d9" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;. If &lt;code&gt;weight&lt;/code&gt; is a tensor, then the shapes of &lt;code&gt;weight&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30ae8ec04282dfa89f820e2638cc76a4c5308051" translate="yes" xml:space="preserve">
          <source>The shapes of the &lt;code&gt;mask&lt;/code&gt; tensor and the &lt;code&gt;input&lt;/code&gt; tensor don&amp;rsquo;t need to match, but they must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80ed96242b937c96d491a6608007e2c80ef6fbfe" translate="yes" xml:space="preserve">
          <source>The singular values are returned in descending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86c71484bcb812cb0f2e7260fb264f82086dbacb" translate="yes" xml:space="preserve">
          <source>The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for &lt;code&gt;offset&lt;/code&gt; other than</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0739eb19e36226e98c11fd96e8291d8a209e3a7e" translate="yes" xml:space="preserve">
          <source>The smallest positive representable number.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf0617c3e175f01c2b40e71995108022f6b64f95" translate="yes" xml:space="preserve">
          <source>The smallest representable number (typically &lt;code&gt;-max&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27d91cd31d6556c601166e6b57bbeb8fc2cd13fe" translate="yes" xml:space="preserve">
          <source>The smallest representable number such that &lt;code&gt;1.0 + eps != 1.0&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff2ebfdbbf9420dca19c2e2c489305049c84e69e" translate="yes" xml:space="preserve">
          <source>The smallest representable number.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e4db462bf55509b809c59ea2298379ae2dba77f" translate="yes" xml:space="preserve">
          <source>The sources in &lt;code&gt;cuda_sources&lt;/code&gt; are concatenated into a separate &lt;code&gt;.cu&lt;/code&gt; file and prepended with &lt;code&gt;torch/types.h&lt;/code&gt;, &lt;code&gt;cuda.h&lt;/code&gt; and &lt;code&gt;cuda_runtime.h&lt;/code&gt; includes. The &lt;code&gt;.cpp&lt;/code&gt; and &lt;code&gt;.cu&lt;/code&gt; files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in &lt;code&gt;cuda_sources&lt;/code&gt; per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the &lt;code&gt;cpp_sources&lt;/code&gt; (and include its name in &lt;code&gt;functions&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f06fdc0c2350c66f3a9d82727c760926c8016f15" translate="yes" xml:space="preserve">
          <source>The stashing logic saves and restores the RNG state for the current device and the device of all cuda Tensor arguments to the &lt;code&gt;run_fn&lt;/code&gt;. However, the logic has no way to anticipate if the user will move Tensors to a new device within the &lt;code&gt;run_fn&lt;/code&gt; itself. Therefore, if you move Tensors to a new device (&amp;ldquo;new&amp;rdquo; meaning not belonging to the set of [current device + devices of Tensor arguments]) within &lt;code&gt;run_fn&lt;/code&gt;, deterministic output compared to non-checkpointed passes is never guaranteed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7f3ab8a2df042d6e3025c18a145e6e14f77af25" translate="yes" xml:space="preserve">
          <source>The sum operation still operates over all the elements, and divides by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2531e155fe9e77a5cfae31abf7b6f59ffb9fcaa" translate="yes" xml:space="preserve">
          <source>The support of third-party backend is experimental and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2873a12c84aba239b7842369e12bb64236d9f288" translate="yes" xml:space="preserve">
          <source>The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc114e497003f25e6fca05f82fcba607f7d78078" translate="yes" xml:space="preserve">
          <source>The tensors &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab7ff2ed427b3e162ddc9c1bc3618ade11c4bc8e" translate="yes" xml:space="preserve">
          <source>The torch package contains data structures for multi-dimensional tensors and mathematical operations over these are defined. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="437652102ffae6443117ed89a3eae4a8f5b41da3" translate="yes" xml:space="preserve">
          <source>The tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89850de90f112bfc18c8cdc61311b3872ebf2872" translate="yes" xml:space="preserve">
          <source>The tracer records the example inputs shape in the graph. In case the model should accept inputs of dynamic shape, you can utilize the parameter &lt;code&gt;dynamic_axes&lt;/code&gt; in export api.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da3842947e505833ecf56e0071c7d1669aedc528" translate="yes" xml:space="preserve">
          <source>The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2328cc72d77d5be4ac31362bf776708e1354f25" translate="yes" xml:space="preserve">
          <source>The unreduced (i.e. with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) loss can be described as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83a2adc9d7ff925b785017e994bf1c2dfff219e3" translate="yes" xml:space="preserve">
          <source>The unreduced loss (i.e., with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) can be described as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55b435ee6855f1f093ca66cf72abe8bd2c30eb03" translate="yes" xml:space="preserve">
          <source>The upper triangular part of the matrix is defined as the elements on and above the diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04027d302afa8c6fa85d808794bef7b659457976" translate="yes" xml:space="preserve">
          <source>The use of &lt;code&gt;collate_fn&lt;/code&gt; is slightly different when automatic batching is enabled or disabled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ff79d6204f8beff86c1883967aad41f978dae9c" translate="yes" xml:space="preserve">
          <source>The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8baea02cc4125823d2a9295d67b336747f5d060" translate="yes" xml:space="preserve">
          <source>The value held by this &lt;code&gt;Future&lt;/code&gt;. If the function (callback or RPC) creating the value has thrown an error, this &lt;code&gt;wait&lt;/code&gt; method will also throw an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c32d99e8bca557bc1750470ea69ae3bfb345050" translate="yes" xml:space="preserve">
          <source>The values of this class are lowercase strings, e.g., &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;. They can be accessed as attributes, e.g., &lt;code&gt;Backend.NCCL&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d09c2230d5453858141229e7b0087efe234f737" translate="yes" xml:space="preserve">
          <source>The values of this class can be accessed as attributes, e.g., &lt;code&gt;ReduceOp.SUM&lt;/code&gt;. They are used in specifying strategies for reduction collectives, e.g., &lt;a href=&quot;#torch.distributed.reduce&quot;&gt;&lt;code&gt;reduce()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt;&lt;code&gt;all_reduce_multigpu()&lt;/code&gt;&lt;/a&gt;, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="539dcafbce59fbba03d94008a7b26e37638c0f1b" translate="yes" xml:space="preserve">
          <source>The world size of the process group -1, if not part of the group</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5d66dffec3f1eab9a22e5606deb2eaebb37f157" translate="yes" xml:space="preserve">
          <source>Then &lt;code&gt;dynamic axes&lt;/code&gt; can be defined either as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cacf045e7460da89c00e2f44ce242ff34d557d4d" translate="yes" xml:space="preserve">
          <source>Then for any (supported) &lt;code&gt;input&lt;/code&gt; tensor the following equality holds:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61c6ce8b4e502ae0f9cf574180a84ce71379a45a" translate="yes" xml:space="preserve">
          <source>Then run the following code in two different processes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ce92152c376045e006c7df7e500a93b25016945" translate="yes" xml:space="preserve">
          <source>Then, you can run:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9157fc1a5d0f6b3459cf4d13b451acd4e1ae988a" translate="yes" xml:space="preserve">
          <source>There are 2 main ways to initialize a process group:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="989aa62e15f82179c46dd7bca86e4177e7b7bf45" translate="yes" xml:space="preserve">
          <source>There are a few main ways to create a tensor, depending on your use case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="448eaecf17e019efecc19b1280b7ffb9043292f0" translate="yes" xml:space="preserve">
          <source>There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a50a4f02b41f0e4e7768bdf9e0d8e8e8f9ca103" translate="yes" xml:space="preserve">
          <source>There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94f93e5cb61707d9a9f9f8c1e8bae87549a0661b" translate="yes" xml:space="preserve">
          <source>There are more examples in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;symbolic_opset9.py&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset10.py&quot;&gt;symbolic_opset10.py&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7b2c38d0b8818a513a12fc7c7e90b9479638b6e" translate="yes" xml:space="preserve">
          <source>There are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b0666c05baae6bc9dd7801107606ece55efb258" translate="yes" xml:space="preserve">
          <source>There are two main usages:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b78c278fc5d3de5d25f299a98858079b45aa6eb" translate="yes" xml:space="preserve">
          <source>There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired &lt;code&gt;world_size&lt;/code&gt;. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2b97adc02fc81457ca80fb753c694becd93e274" translate="yes" xml:space="preserve">
          <source>There is a subtlety in using the &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; pattern in a &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; wrapped in &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;My recurrent network doesn&amp;rsquo;t work with data parallelism&lt;/a&gt; section in FAQ for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bedff7a4f5803a09c69737d429b263e4a4e07c0" translate="yes" xml:space="preserve">
          <source>Therefore, indexing &lt;code&gt;output&lt;/code&gt; at the last dimension (column dimension) gives all values within a certain block.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="220cff043cb62e0e4cb5366907ebd80ead3bf366" translate="yes" xml:space="preserve">
          <source>Therefore, to invert an &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;normalized&lt;/code&gt; and &lt;code&gt;onesided&lt;/code&gt; arguments should be set identically for &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, and preferably a &lt;code&gt;signal_sizes&lt;/code&gt; is given to avoid size mismatch. See the example below for a case of size mismatch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11de0478b61c8d1fd83658744b437989c835c987" translate="yes" xml:space="preserve">
          <source>These are the basic building block for graphs</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ca4969bd4e4273b5349ab2f675d16c1de44f187" translate="yes" xml:space="preserve">
          <source>These backends include:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0e365abd4e5be3113ea52d047c2f616e603945c" translate="yes" xml:space="preserve">
          <source>These ops don&amp;rsquo;t require a particular dtype for stability, but take multiple inputs and require that the inputs&amp;rsquo; dtypes match. If all of the inputs are &lt;code&gt;float16&lt;/code&gt;, the op runs in &lt;code&gt;float16&lt;/code&gt;. If any of the inputs is &lt;code&gt;float32&lt;/code&gt;, autocast casts all inputs to &lt;code&gt;float32&lt;/code&gt; and runs the op in &lt;code&gt;float32&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="150d61b0973e4711130a967de9e4414841dbc8c3" translate="yes" xml:space="preserve">
          <source>These options are configured by the constructor arguments of a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;, which has signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dd026a9fc19a074410394c15ee56c930e5686d0" translate="yes" xml:space="preserve">
          <source>These types and features from the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module are unavailble in TorchScript.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aca9bd729c973c4d3bc4eb7f3f8d4e953c82edfd" translate="yes" xml:space="preserve">
          <source>These unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5042b3fea81612a742984a1fb55be5b675720283" translate="yes" xml:space="preserve">
          <source>Third-party backends</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9ab4d0d07c5bfd671637ba9723d67603253bcae" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;momentum&lt;/code&gt; argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d057f12e63c19fa40f83b9e371a09dbaf3641ca" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;setuptools.build_ext&lt;/code&gt; subclass takes care of passing the minimum required compiler flags (e.g. &lt;code&gt;-std=c++14&lt;/code&gt;) as well as mixed C++/CUDA compilation (and support for CUDA files in general).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48b0bfc66718d9ed75062a9f5d57a687fd7749c9" translate="yes" xml:space="preserve">
          <source>This API is in beta and may change in the near future.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53d04b24e9fbd24d72acb9f431a964889b4c7cf6" translate="yes" xml:space="preserve">
          <source>This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c994160251b3f47ef8b0dd185e96a7b1ad0ee446" translate="yes" xml:space="preserve">
          <source>This API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don&amp;rsquo;t have requires_grad set, you can use a lambda to capture them. For example, for a function &lt;code&gt;f&lt;/code&gt; that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as &lt;code&gt;f(input, constant, flag=flag)&lt;/code&gt; you can use it as &lt;code&gt;functional.jacobian(lambda x: f(x, constant, flag=flag), input)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33a32d5fcbf1c45d06b8358ae2591cd5047cff16" translate="yes" xml:space="preserve">
          <source>This allows better BC support for &lt;a href=&quot;#torch.nn.Module.load_state_dict&quot;&gt;&lt;code&gt;load_state_dict()&lt;/code&gt;&lt;/a&gt;. In &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt;, the version number will be saved as in the attribute &lt;code&gt;_metadata&lt;/code&gt; of the returned state dict, and thus pickled. &lt;code&gt;_metadata&lt;/code&gt; is a dictionary with keys that follow the naming convention of state dict. See &lt;code&gt;_load_from_state_dict&lt;/code&gt; on how to use this information in loading.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09b607c5ebd52b97aa3c6cf95a474cc71af1ac47" translate="yes" xml:space="preserve">
          <source>This allows for different samples to have variable amounts of target classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc869bc2254cbb18e88bb0a8c7e186c575d69c2e" translate="yes" xml:space="preserve">
          <source>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd79f292616deed21e20e4449558742671a5f03e" translate="yes" xml:space="preserve">
          <source>This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set &lt;code&gt;use_external_data_format&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; to successfully export such models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37b077e70d126ef1c4f52c0e784fa4698cbe6a7d" translate="yes" xml:space="preserve">
          <source>This attribute is &lt;code&gt;None&lt;/code&gt; by default and becomes a Tensor the first time a call to &lt;a href=&quot;#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; computes gradients for &lt;code&gt;self&lt;/code&gt;. The attribute will then contain the gradients computed and future calls to &lt;a href=&quot;#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; will accumulate (add) gradients into it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9aed446ae3aa4dc28d8d65d1c929ff54cb2a74f" translate="yes" xml:space="preserve">
          <source>This attribute is &lt;code&gt;None&lt;/code&gt; by default and becomes a Tensor the first time a call to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; computes gradients for &lt;code&gt;self&lt;/code&gt;. The attribute will then contain the gradients computed and future calls to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; will accumulate (add) gradients into it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27eaef81d3fd1f9e704b27f64a092d8671cfb020" translate="yes" xml:space="preserve">
          <source>This can be called as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90fa23d66ed39d5f59f1b4a16b52b28d18188c72" translate="yes" xml:space="preserve">
          <source>This can be useful to display periodically during training, or when handling out-of-memory exceptions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c207d7ed4d817a917f51b1a12e087c64e12c094" translate="yes" xml:space="preserve">
          <source>This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the &lt;a href=&quot;#torch.optim.Optimizer&quot;&gt;&lt;code&gt;Optimizer&lt;/code&gt;&lt;/a&gt; as training progresses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36c9a312367d269d922b679dae091acec9f12a5e" translate="yes" xml:space="preserve">
          <source>This can be useful when there is a need to create classes with the same constructor arguments, but different instances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5eeeb2d9e42f39a239e76c72a9314e3136e5540" translate="yes" xml:space="preserve">
          <source>This can then be visualized with TensorBoard, which should be installable and runnable with:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2306bf42c44895d1fc76396938e439c137c2dffb" translate="yes" xml:space="preserve">
          <source>This class can be directly called to parse the string, e.g., &lt;code&gt;Backend(backend_str)&lt;/code&gt; will check if &lt;code&gt;backend_str&lt;/code&gt; is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., &lt;code&gt;Backend(&quot;GLOO&quot;)&lt;/code&gt; returns &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ef03a595a40d9415bbad765363aad2d964d5b16" translate="yes" xml:space="preserve">
          <source>This class does not provide a &lt;code&gt;forward&lt;/code&gt; hook. Instead, you must use one of the underlying functions (e.g. &lt;code&gt;add&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c851561f849ef1eb6b901d2edf564052e34c6c02" translate="yes" xml:space="preserve">
          <source>This class has three built-in policies, as put forth in the paper:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ddd34a2492252eba3a31d1accbd978513965aba" translate="yes" xml:space="preserve">
          <source>This class is an intermediary between the &lt;code&gt;Distribution&lt;/code&gt; class and distributions which belong to an exponential family mainly to check the correctness of the &lt;code&gt;.entropy()&lt;/code&gt; and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc115d83b6af36150f881200c064c4d238e34462" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c13ac423de7a3c762fee4c3a9510054ffdd5d384" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;. It is equivalent to &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d90500fc17196cb70e7a700371ed43a1a87eeb32" translate="yes" xml:space="preserve">
          <source>This class is useful to assemble different existing dataset streams. The chainning operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af734602c7eaf1931d607624127baafea2243648" translate="yes" xml:space="preserve">
          <source>This class is useful to assemble different existing datasets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f63c41dd97936df60be517dad06b50e25bfe14d0" translate="yes" xml:space="preserve">
          <source>This class uses &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; in order to retrieve the gradients for specific parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1862d470ef8a5cd214866c60dc3b2e18ed9571b7" translate="yes" xml:space="preserve">
          <source>This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43ff3e72065cb78dedbfc0cb5795ef6d313a86b8" translate="yes" xml:space="preserve">
          <source>This composition also works for &lt;code&gt;nn.Module&lt;/code&gt;s as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d56b8d3c3a6cd08249ac0400c9bd4fa666010028" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given &lt;code&gt;module&lt;/code&gt; by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e99fa6e39b3592c4eea5075ef0e0720a6948ff1" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c09652531ac8f4b111870bcb71b813e31279457c" translate="yes" xml:space="preserve">
          <source>This context manager is thread local; it will not affect computation in other threads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26ef7031b1f75f10e86902c0f567ce05a4ea44b5" translate="yes" xml:space="preserve">
          <source>This context manager will keep track of already-joined DDP processes, and &amp;ldquo;shadow&amp;rdquo; the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09d1e555fd19e686f48ac7306fa2525b334ba962" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt;&lt;code&gt;nn.LogSoftmax()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;nn.NLLLoss()&lt;/code&gt;&lt;/a&gt; in one single class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="925d7f6dcda18d5ae787b2ea0d4e4cf02702d590" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;log_softmax&lt;/code&gt; and &lt;code&gt;nll_loss&lt;/code&gt; in a single function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7ea31e3bc10ddc7b5db55c0a7c9d23739bfa2db" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; and &lt;code&gt;nn.NLLLoss()&lt;/code&gt; in one single class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a017ebd876454e242cede3ce02b356f5f295daa" translate="yes" xml:space="preserve">
          <source>This criterion expects a &lt;code&gt;target&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; of the same size as the &lt;code&gt;input&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c10376150f504915e5bd847798be0f3a52dc669c" translate="yes" xml:space="preserve">
          <source>This criterion expects a class index in the range</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c181079125dc2e25b97bd2d3da70b20b81a75a7" translate="yes" xml:space="preserve">
          <source>This decorator also works with RRef helpers, i.e., . &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_sync&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_sync()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_async&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_async()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.rpc.RRef.remote&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.remote()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba581390e34d5e57c67e068b92dac8e4709b3ad6" translate="yes" xml:space="preserve">
          <source>This decorator indicates that a method on an &lt;code&gt;nn.Module&lt;/code&gt; is used as an entry point into a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and should be compiled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="174b3d4b823448bf6302b0b53449be4c6d5ad12e" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d10437bc80b062c46cb5ff8be0bb49322ddb59f" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function. This allows you to leave code in your model that is not yet TorchScript compatible. If called from TorchScript, ignored functions will dispatch the call to the Python interpreter. Models with ignored functions cannot be exported; use &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1508df53ef2be98f34c189e45c94ca9319ce75f2" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75d6e9d0a82393986fc62b4441accfe0e14ee3a9" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8952c5fbe2a12cf21923d9bc22bc2c081674eb1e" translate="yes" xml:space="preserve">
          <source>This defines</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04bcd4a4cb1d0ee1f357d9037605979a3155b988" translate="yes" xml:space="preserve">
          <source>This depends on the &lt;code&gt;spawn&lt;/code&gt; start method in Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; package.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e40f466c8c675241a4989cdf735bce5f4ddce6bc" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?orgqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-orgqr&quot;&gt;LAPACK documentation for orgqr&lt;/a&gt; for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fb3c5a1f24bdd83af6fd14c07d4d7ef76ec7527" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?ormqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-ormqr&quot;&gt;LAPACK documentation for ormqr&lt;/a&gt; for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a44ae2c253871828f29057a792173301af10f61" translate="yes" xml:space="preserve">
          <source>This does two things: - Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function. - Any backward computation that generate &amp;ldquo;nan&amp;rdquo; value will raise an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7d3fb60afba5b578d1ed91b3b2d20aba1664bb0" translate="yes" xml:space="preserve">
          <source>This error usually means that the method you are tracing uses a module&amp;rsquo;s parameters and you are passing the module&amp;rsquo;s method instead of the module instance (e.g. &lt;code&gt;my_module_instance.forward&lt;/code&gt; vs &lt;code&gt;my_module_instance&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c753cc91ef9f274f764a4ae41747b732c7fe2261" translate="yes" xml:space="preserve">
          <source>This feature is in beta, and its design and implementation may change in the future.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7614e828d8ac51033f6b1d436da1e7ec19a65d5" translate="yes" xml:space="preserve">
          <source>This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; object by accessing its &lt;code&gt;.data&lt;/code&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5cfef34a5b604499ec2d99f480d0835ab4472fc" translate="yes" xml:space="preserve">
          <source>This function accumulates gradients in the leaves - you might need to zero &lt;code&gt;.grad&lt;/code&gt; attributes or set them to &lt;code&gt;None&lt;/code&gt; before calling it. See &lt;a href=&quot;#default-grad-layouts&quot;&gt;Default gradient layouts&lt;/a&gt; for details on the memory layout of accumulated gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97ac98bd75da52df5620120c1ca469a7f25204b3" translate="yes" xml:space="preserve">
          <source>This function accumulates gradients in the leaves - you might need to zero &lt;code&gt;.grad&lt;/code&gt; attributes or set them to &lt;code&gt;None&lt;/code&gt; before calling it. See &lt;a href=&quot;autograd#default-grad-layouts&quot;&gt;Default gradient layouts&lt;/a&gt; for details on the memory layout of accumulated gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5919d90354daacc619c8dc367427cc669c661c6e" translate="yes" xml:space="preserve">
          <source>This function behaves exactly like &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;, but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of &lt;a href=&quot;#torch.utils.cpp_extension.load_inline&quot;&gt;&lt;code&gt;load_inline()&lt;/code&gt;&lt;/a&gt; is identical to &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4742c80d8253ffcd737e46fcd5dffcfb774bfb9a" translate="yes" xml:space="preserve">
          <source>This function calculates all eigenvalues (and vectors) of &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e52ac392528ad63a53ea2de90d126a45274fb82" translate="yes" xml:space="preserve">
          <source>This function can be called in an interleaved way.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f591ac1d01d2706090dd5158a48cfafbf7078fcd" translate="yes" xml:space="preserve">
          <source>This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the &lt;code&gt;ord&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a99e80c93a86e3189bf95a219e4fc70666128f91" translate="yes" xml:space="preserve">
          <source>This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2742ba18db0e477006e0d4a879a88ee8aaa73a1c" translate="yes" xml:space="preserve">
          <source>This function checks if all &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; satisfy the condition:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63c734a4df7eea8aea1bf4116fd041c2a840b41a" translate="yes" xml:space="preserve">
          <source>This function checks that backpropagating through the gradients computed to the given &lt;code&gt;grad_outputs&lt;/code&gt; are correct.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ad58a0a805b7dcf7c19d64d7ab1a5b0774c3bf4" translate="yes" xml:space="preserve">
          <source>This function does exact same thing as &lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt;&lt;code&gt;torch.addmm()&lt;/code&gt;&lt;/a&gt; in the forward, except that it supports backward for sparse matrix &lt;code&gt;mat1&lt;/code&gt;. &lt;code&gt;mat1&lt;/code&gt; need to have &lt;code&gt;sparse_dim = 2&lt;/code&gt;. Note that the gradients of &lt;code&gt;mat1&lt;/code&gt; is a coalesced sparse tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b687a9d19b48f08afdcc0c1bcf3778a2f67002f3" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6eaaf4ac25e2420bf8da7dbd7b25891750193e61" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;. For broadcasting matrix products, see &lt;a href=&quot;torch.matmul#torch.matmul&quot;&gt;&lt;code&gt;torch.matmul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1273ec3288d9debd903d3f9e04111f9b0d5a8d6b" translate="yes" xml:space="preserve">
          <source>This function does not check if the factorization was successful or not if &lt;code&gt;get_infos&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; since the status of the factorization is present in the third element of the return tuple.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68c688ded2e6ac8d7e4721168f69f94608270e24" translate="yes" xml:space="preserve">
          <source>This function does not optimize the given expression, so a different formula for the same computation may run faster or consume less memory. Projects like opt_einsum (&lt;a href=&quot;https://optimized-einsum.readthedocs.io/en/stable/&quot;&gt;https://optimized-einsum.readthedocs.io/en/stable/&lt;/a&gt;) can optimize the formula for you.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f44824ec132a2e35c26075b94e4bfe941180a2db" translate="yes" xml:space="preserve">
          <source>This function doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba69951c62e61a879aa86b6caaae011ee277fff1" translate="yes" xml:space="preserve">
          <source>This function eagerly initializes CUDA.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12a2d17eb1ae0addb52aeb2b46ee9bc80c204ede" translate="yes" xml:space="preserve">
          <source>This function insert observer module to all leaf child module that has a valid qconfig attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c40534c7583c126c2b6a9ea9f9ce426b4120e2b" translate="yes" xml:space="preserve">
          <source>This function is a front-end to the following LOBPCG algorithms selectable via &lt;code&gt;method&lt;/code&gt; argument:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="773eb7977e3d589f5a2a946752611ffc03872d99" translate="yes" xml:space="preserve">
          <source>This function is deprecated and may be removed in a future release. It can be implemented using &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; as &lt;code&gt;alpha * torch.outer(vec1, vec2) + beta * input&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is not zero, and as &lt;code&gt;alpha * torch.outer(vec1, vec2)&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is zero.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecdb962fda9e357b596ac207e636ee312e395193" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future PyTorch release. Use &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32ba15aae127598880c645601e7ae7f8189c6c82" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future release because its behavior is inconsistent with Python&amp;rsquo;s range builtin. Instead, use &lt;a href=&quot;torch.arange#torch.arange&quot;&gt;&lt;code&gt;torch.arange()&lt;/code&gt;&lt;/a&gt;, which produces values in [start, end).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd06f9dfe663c3caf3762160bc99450c126a39a9" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1632348f060a8b22bffa765800a936ba8981c455" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e11fc8a8842739d9e68d3ea9f36d243b26e33ee2" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="409b1beff6de19c9e0fc8534a682b00b07c2667b" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f088ca0f34a9882f6e67d133eea7956f99a05c61" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a1e1948ca54d505e36aa98bcd4ccf28bf3f09403" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6111b83606e5f76d2d19a4fd63d2df2a457f3738" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique#torch.unique&quot;&gt;&lt;code&gt;torch.unique()&lt;/code&gt;&lt;/a&gt; in the sense that this function only eliminates consecutive duplicate values. This semantics is similar to &lt;code&gt;std::unique&lt;/code&gt; in C++.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c84bc16a342354e3160aa386d7ad206e77f3ce71" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt; in the sense that this function also eliminates non-consecutive duplicate values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42a1f69345a02122fc091a770868079bb8638971" translate="yes" xml:space="preserve">
          <source>This function is differentiable, so gradients will flow back from the result of this operation to &lt;code&gt;input&lt;/code&gt;. To create a tensor without an autograd relationship to &lt;code&gt;input&lt;/code&gt; see &lt;a href=&quot;../autograd#torch.Tensor.detach&quot;&gt;&lt;code&gt;detach()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fb9aa5da82ff7de2350851193131992c19c52a8" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.cdist(input,&amp;rsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc680c0e90287ae914b5b4470b9652c693f1647a" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.pdist(input, &amp;lsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff6a78f486235a2c4ee2bb0612d674661defb9c5" translate="yes" xml:space="preserve">
          <source>This function is here for legacy reasons, may be removed from nn.Functional in the future.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ffbcd1ffd822d93e92f3e882d1d47ebf27a02de" translate="yes" xml:space="preserve">
          <source>This function is implemented only for nonnegative integers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4af64360b4b0bf60d1db0c5268a216c0039ae031" translate="yes" xml:space="preserve">
          <source>This function is more accurate than &lt;a href=&quot;torch.log#torch.log&quot;&gt;&lt;code&gt;torch.log()&lt;/code&gt;&lt;/a&gt; for small values of &lt;code&gt;input&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9a118660ffcea349bb4668844a1f15351a99bbb" translate="yes" xml:space="preserve">
          <source>This function is not defined for &lt;code&gt;torch.cuda.Tensor&lt;/code&gt; yet.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69c6cf6408f0b3c6058b2d09b95171f65a52d054" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt;&lt;code&gt;affine_grid()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bda1093b38d267fe7d744a3f96bb51ca111b18d" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff271211b68caf9a357a20a5285a2d8ec4fb66ee" translate="yes" xml:space="preserve">
          <source>This function is significantly slower than &lt;code&gt;vhp&lt;/code&gt; due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18b3cca742284c69f10463b509a01d9fefa6450b" translate="yes" xml:space="preserve">
          <source>This function is to be overridden by all subclasses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83aed61e5e3f24098a9d4e00feeea5da8441abc7" translate="yes" xml:space="preserve">
          <source>This function now calls &lt;code&gt;reset_peak_memory_stats()&lt;/code&gt;, which resets /all/ peak memory stats.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d26ea75bdd03743d020bb8fd3f0b4f05e5fca44d" translate="yes" xml:space="preserve">
          <source>This function only works with CPU tensors and should not be used in code sections that require high performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ed8f6139c0d2640189a9321e1da51e7b538d228" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;max(dim=0)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49eba71d4c9b31d15ee67772326e88d1462e9393" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;median(dim=0)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9251c4cd19a437c8f31a2c44be4bdf2e2282d65" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;min(dim=0)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d3e2fb10adcbb7f259d94a424b7b3b695d7efa3" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6dd319ec8d2bd7cb7695497b2e2c0c7da18093d6" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="464a58780f1e7ad5a78398d14f9e91d688f64e19" translate="yes" xml:space="preserve">
          <source>This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a52b7a21105c565c661abb413198331f8bc8a45e" translate="yes" xml:space="preserve">
          <source>This function returns a Tensor of size &lt;code&gt;T x B x *&lt;/code&gt; or &lt;code&gt;B x T x *&lt;/code&gt; where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c177e0164ff9028b4584fc5df4d1e3daee9e253a" translate="yes" xml:space="preserve">
          <source>This function returns a handle with a method &lt;code&gt;handle.remove()&lt;/code&gt; that removes the hook from the module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0496aaa431bc80a1684ef66937522b9d485ea0d" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the nearly optimal approximation of a singular value decomposition of a centered matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eda5e94a6c116342560b85fa8d7ac690f3e68684" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the singular value decomposition of a input real matrix or batches of real matrices &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aeb212c2592eaa5e981d2676890741ba536e05d" translate="yes" xml:space="preserve">
          <source>This function returns eigenvalues and eigenvectors of a real symmetric matrix &lt;code&gt;input&lt;/code&gt; or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f955323e5a9f295f213a25f7ecc7dec98b5caa2" translate="yes" xml:space="preserve">
          <source>This function returns the solution to the system of linear equations represented by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e819ccb2e6403dc786009cdee93abaa1f0eeb18b" translate="yes" xml:space="preserve">
          <source>This function returns without waiting for &lt;code&gt;event&lt;/code&gt;: only future operations are affected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf880005934990f2038c1e6b5eb0e99c0dad47ad" translate="yes" xml:space="preserve">
          <source>This function returns without waiting for currently enqueued kernels in &lt;a href=&quot;#torch.cuda.stream&quot;&gt;&lt;code&gt;stream&lt;/code&gt;&lt;/a&gt;: only future operations are affected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d7a84c0650abce4d3f9c668ace3ede1befabda4" translate="yes" xml:space="preserve">
          <source>This function&amp;rsquo;s name is a misnomer. It actually rounds the quotient towards zero instead of taking its floor. This behavior will be deprecated in a future PyTorch release.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c29a81af2b62737c72764612d782901c3f2eb4d7" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;a href=&quot;torch.nn.dropout#torch.nn.Dropout&quot;&gt;&lt;code&gt;Dropout&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a56c6998fe86297bbe85815fb835af9e00568e26" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;code&gt;Dropout&lt;/code&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f8efe32a81509d5ad4a6e1e586b778004252c39" translate="yes" xml:space="preserve">
          <source>This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16cf55bdf2b28322a9b3129eb62ae4e3fc4f60f8" translate="yes" xml:space="preserve">
          <source>This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 1111. It uses direction numbers to generate these sequences, and these numbers have been adapted from &lt;a href=&quot;https://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111&quot;&gt;here&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="695271e751e01b7a3e177cda22c6d8e9f50c985c" translate="yes" xml:space="preserve">
          <source>This implementation uses polar coordinates. The &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="218178424bb17bc6ad1e0c3396e3e36ddf25ff43" translate="yes" xml:space="preserve">
          <source>This implementation was adapted from the github repo: &lt;a href=&quot;https://github.com/bckenstler/CLR&quot;&gt;bckenstler/CLR&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5d17292f2576cd2e36da2cca0252f9780e064b5" translate="yes" xml:space="preserve">
          <source>This invariant is maintained throughout &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; class, and all functions that construct a &lt;code&gt;:class:PackedSequence&lt;/code&gt; in PyTorch (i.e., they only pass in tensors conforming to this constraint).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0dcba97fb0267a928459453464ca8f56762a0535" translate="yes" xml:space="preserve">
          <source>This is TorchScript&amp;rsquo;s compilation of the code for the &lt;code&gt;forward&lt;/code&gt; method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78aa499d132d9b73782bab5e2924798d784eafb1" translate="yes" xml:space="preserve">
          <source>This is a &lt;strong&gt;Prototype&lt;/strong&gt; function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="554d0b95763033953c158f9820e38365e62ff2d9" translate="yes" xml:space="preserve">
          <source>This is a generalized version of &lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt;&lt;code&gt;torch.hann_window()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27da13545c38e0ad916bfb253add6c85dbfec69d" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="baa9ff1557d00c12989b3535b16426973504c66e" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in &lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACK documentation for geqrf&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bed0909b657a7fcd560ee9c527be9120da241148" translate="yes" xml:space="preserve">
          <source>This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt;, which checks for contiguity, or &lt;a href=&quot;#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, which copies data if needed. To change the size in-place with custom strides, see &lt;a href=&quot;#torch.Tensor.set_&quot;&gt;&lt;code&gt;set_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3b5e88eb67e346fdbdf0577eada21e1b383ed17" translate="yes" xml:space="preserve">
          <source>This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5b9f918ec25aa03605059dcc957950d2df96994" translate="yes" xml:space="preserve">
          <source>This is a no-op if the tensor is already of the correct type. This is equivalent to &lt;code&gt;self.type(tensor.type())&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be9417a8a097120c57731208ecd0b2e0d9e0a45b" translate="yes" xml:space="preserve">
          <source>This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="584f2eb806dc59e0bb28f52080bdd509e138734b" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3187ce0a4f646a0f02690921fbe6d7114d95c7e3" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a2d1424088d04fbdd7c029c2c5a864c5f132531" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee8eabed86139ec26051c16d9d19973c0fd6b09c" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12efc861b006501b2b4700d386ad4f25b9ff0f7e" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98a705e3678fe480b570b51f1f965a2f3d037e83" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01f4b9cc31c6559249deff015369111df9aa6f57" translate="yes" xml:space="preserve">
          <source>This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. &lt;code&gt;backward()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd838137a8dd7616f8151e3b75e72da4794d6fa0" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57134b77fa81442b10e02b41eaf3243756f7f5ee" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist. If all values in a reduced row are &lt;code&gt;NaN&lt;/code&gt; then the quantiles for that reduction will be &lt;code&gt;NaN&lt;/code&gt;. See the documentation for &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae0e7bea21fe5aaecb615e30a54297c8a770a6eb" translate="yes" xml:space="preserve">
          <source>This is a very memory intensive optimizer (it requires additional &lt;code&gt;param_bytes * (history_size + 1)&lt;/code&gt; bytes). If it doesn&amp;rsquo;t fit in memory try reducing the history size, or use a different algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="994f42290c63351a1c873f9744cae43cb48288cc" translate="yes" xml:space="preserve">
          <source>This is a wrapper around &lt;code&gt;cudaEventSynchronize()&lt;/code&gt;: see &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html&quot;&gt;CUDA Event documentation&lt;/a&gt; for more info.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d81e1e16ab1c2e5ec9a42a6d43cc837303bcaf0" translate="yes" xml:space="preserve">
          <source>This is a wrapper around &lt;code&gt;cudaStreamSynchronize()&lt;/code&gt;: see &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html&quot;&gt;CUDA Stream documentation&lt;/a&gt; for more info.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13f3d83bdd2dfc81633c08aeb7934d3136f118c5" translate="yes" xml:space="preserve">
          <source>This is a wrapper around &lt;code&gt;cudaStreamWaitEvent()&lt;/code&gt;: see &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html&quot;&gt;CUDA Stream documentation&lt;/a&gt; for more info.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55c07d098e34194632b960dfc4bd298727835fac" translate="yes" xml:space="preserve">
          <source>This is always &lt;code&gt;True&lt;/code&gt; for CUDA tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afdc3cee342017a6e1562fe587b2298feb148739" translate="yes" xml:space="preserve">
          <source>This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16c88bb46cbf5ef3f448ca4fe755467e88b5e578" translate="yes" xml:space="preserve">
          <source>This is different from &lt;a href=&quot;../tensors#torch.Tensor.repeat&quot;&gt;&lt;code&gt;torch.Tensor.repeat()&lt;/code&gt;&lt;/a&gt; but similar to &lt;code&gt;numpy.repeat&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db053cd2631236023d48272222af395e026941e8" translate="yes" xml:space="preserve">
          <source>This is equivalent to &lt;code&gt;self.log_pob(input).argmax(dim=1)&lt;/code&gt;, but is more efficient in some cases.</source>
          <target state="new"/>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
