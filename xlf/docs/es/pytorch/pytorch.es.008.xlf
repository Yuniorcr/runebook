<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="14ee5344d06a238323bb82692b3ba5195a694523" translate="yes" xml:space="preserve">
          <source>The RPC package also provides decorators which allow applications to specify how a given function should be treated on the callee side.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d97e45dd1ab295237092f3a7f655e6e94838339b" translate="yes" xml:space="preserve">
          <source>The RPC tutorials introduce users to the RPC framework, provide several example applications using &lt;a href=&quot;#distributed-rpc-framework&quot;&gt;torch.distributed.rpc&lt;/a&gt; APIs, and demonstrate how to use &lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;the profiler&lt;/a&gt; to profile RPC-based workloads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1362003f90d57874aad22424c41e2c89849947f1" translate="yes" xml:space="preserve">
          <source>The RRef design note covers the design of the &lt;a href=&quot;#rref&quot;&gt;RRef&lt;/a&gt; (Remote REFerence) protocol used to refer to values on remote workers by the framework.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e195940e5abd6ebbfad00ac76917c59c49c9651e" translate="yes" xml:space="preserve">
          <source>The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time. The interface of this function is modeled after the &lt;a href=&quot;https://librosa.org/doc/latest/generated/librosa.stft.html&quot;&gt;librosa&lt;/a&gt; stft function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56e279d91a2e6e157edb87095248831fbd79e513" translate="yes" xml:space="preserve">
          <source>The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e88b194ff4e6feceb2bfaaa1ec38472db3a49b02" translate="yes" xml:space="preserve">
          <source>The TensorPipe agent, which is the default, leverages &lt;a href=&quot;https://github.com/pytorch/tensorpipe&quot;&gt;the TensorPipe library&lt;/a&gt;, which provides a natively point-to-point communication primitive specifically suited for machine learning that fundamentally addresses some of the limitations of Gloo. Compared to Gloo, it has the advantage of being asynchronous, which allows a large number of transfers to occur simultaneously, each at their own speed, without blocking each other. It will only open pipes between pairs of nodes when needed, on demand, and when one node fails only its incident pipes will be closed, while all other ones will keep working as normal. In addition, it is able to support multiple different transports (TCP, of course, but also shared memory, NVLink, InfiniBand, &amp;hellip;) and can automatically detect their availability and negotiate the best transport to use for each pipe.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d362a2db0f7abde4a8479b2b7feada60ead120fd" translate="yes" xml:space="preserve">
          <source>The TensorPipe backend has been introduced in PyTorch v1.6 and is being actively developed. At the moment, it only supports CPU tensors, with GPU support coming soon. It comes with a TCP-based transport, just like Gloo. It is also able to automatically chunk and multiplex large tensors over multiple sockets and threads in order to achieve very high bandwidths. The agent will be able to pick the best transport on its own, with no intervention required.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe34bfd2269a025104eb8b4a08ae444d0c47f764" translate="yes" xml:space="preserve">
          <source>The TorchScript compiler needs to know the types of &lt;code&gt;module attributes&lt;/code&gt;. Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d526d524819b552e08647a3450928e08c0be85f0" translate="yes" xml:space="preserve">
          <source>The accuracies of the pre-trained models evaluated on COCO val2017 are as follows</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd6ae7d26b925d9ddede1463aabdca8219c04ca1" translate="yes" xml:space="preserve">
          <source>The algorithm used for interpolation is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50402614be52e24dbcb07aaaf1f84990228308aa" translate="yes" xml:space="preserve">
          <source>The algorithm used for upsampling is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9dbbacc61ab643f749eabb1318de7d252379ec8c" translate="yes" xml:space="preserve">
          <source>The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bdd6673496c6fd17d43bfb63694d67b1a23e7074" translate="yes" xml:space="preserve">
          <source>The approximate decimal resolution of this type, i.e., &lt;code&gt;10**-precision&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11e64a47029713e51445813986a3824ac2548079" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2720b921eb619ce89ed4f91beabc1047581eca82" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eff44d81484d8a3edd9c1b438e09ac7f19f85991" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5cfcdff9c158bfd56a616516f4454be040717f37" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d114863c5d0c0b07094ca0c20e0cd672c5d3f900" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19b889e5a4344dcdd3129928b40b0f46befb2501" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54707c812db6f29dfe9835066cfce0408bd514d9" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;. However, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this instead returns the results multiplied by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bf649a8dfc260fcc9e9737948bc65802a397444" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;. Similar to &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by multiplying it with</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf4c102338f979040fb6ff061754881df7061681" translate="yes" xml:space="preserve">
          <source>The backend of the given process group as a lower case string.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e19263dbfb8f739b55183f8eb835a3eac6ee2887" translate="yes" xml:space="preserve">
          <source>The backend options class for &lt;code&gt;ProcessGroupAgent&lt;/code&gt;, which is derived from &lt;code&gt;RpcBackendOptions&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ced0f1f27d8dec6f68219ca478db12401a16eb9d" translate="yes" xml:space="preserve">
          <source>The backend options for &lt;code&gt;TensorPipeAgent&lt;/code&gt;, derived from &lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;&lt;code&gt;RpcBackendOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="502ce5e4564798f5189d3ade0d0a77ccd7a4b686" translate="yes" xml:space="preserve">
          <source>The backward method does not support sparse and complex inputs. It works only when &lt;code&gt;B&lt;/code&gt; is not provided (i.e. &lt;code&gt;B == None&lt;/code&gt;). We are actively working on extensions, and the details of the algorithms are going to be published promptly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="977219b95a63d49a9a83d568761b5dea94f71247" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used locally.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23b97210326ef320e2d1633f1e69b26cc12a15dc" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1822f0d582e28f7680fb7e40c87465297eaceb93" translate="yes" xml:space="preserve">
          <source>The behavior depends on the dimensionality of the tensors as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f9d3a99640c5733b646d772d2660cd0aaa3e59f" translate="yes" xml:space="preserve">
          <source>The behavior of the model changes depending if it is in training or evaluation mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="671bda2b5e08e92d9c6a9bee65efdef513a603fd" translate="yes" xml:space="preserve">
          <source>The boolean argument &lt;code&gt;eigenvectors&lt;/code&gt; defines computation of both eigenvectors and eigenvalues or eigenvalues only.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a803e781482443f16488bb7ddd92d4c8599d93d8" translate="yes" xml:space="preserve">
          <source>The boolean option &lt;code&gt;sorted&lt;/code&gt; if &lt;code&gt;True&lt;/code&gt;, will make sure that the returned &lt;code&gt;k&lt;/code&gt; elements are themselves sorted</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7230c4dcc10bf6edcaa500d739b7ece219bab321" translate="yes" xml:space="preserve">
          <source>The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="750a4192b0cf8d28e81899b152edaaa9a146f0c9" translate="yes" xml:space="preserve">
          <source>The case when</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a15a1c2561662d55e6adec5b59058d1e70d3911" translate="yes" xml:space="preserve">
          <source>The columns of the output matrix are elementwise powers of the input vector</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce6592c2eac0498a5cc7f983f7cee34a07c28d68" translate="yes" xml:space="preserve">
          <source>The constructor of &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; can be called without argument, in which case the class is created for the pytorch default dtype (as returned by &lt;a href=&quot;generated/torch.get_default_dtype#torch.get_default_dtype&quot;&gt;&lt;code&gt;torch.get_default_dtype()&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4ae76d927f18889b6dd2cecc1dcf754fe0ab549" translate="yes" xml:space="preserve">
          <source>The contents of a tensor can be accessed and modified using Python&amp;rsquo;s indexing and slicing notation:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68f6bd6745cd407cbd762333af1947bd80a473cd" translate="yes" xml:space="preserve">
          <source>The context managers &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.enable_grad#torch.enable_grad&quot;&gt;&lt;code&gt;torch.enable_grad()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt;&lt;code&gt;torch.set_grad_enabled()&lt;/code&gt;&lt;/a&gt; are helpful for locally disabling and enabling gradient computation. See &lt;a href=&quot;autograd#locally-disable-grad&quot;&gt;Locally disabling gradient computation&lt;/a&gt; for more details on their usage. These context managers are thread local, so they won&amp;rsquo;t work if you send work to another thread using the &lt;code&gt;threading&lt;/code&gt; module, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ca40d5babc478c27564467be749f0f09c316678" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;n&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length &lt;code&gt;n&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15f3e882aab04b4ad9da40b79636691c5dd75e80" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;s&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape &lt;code&gt;s&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2f2b30024847cce229c4bda1e1b5ab964a9eaf4" translate="yes" xml:space="preserve">
          <source>The criterion only considers a contiguous block of non-negative targets that starts at the front.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e11b9195cb3521aafd237a69ce124b8d788f6886" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2bf74abb2f8760663c776314dd933141deb8a40" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41f3fc6117e6365cbfdd81a5124ee1886a82a466" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;code&gt;Module&lt;/code&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;code&gt;Module&lt;/code&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fcbf7f37e10aae7ed7b4feee2a1e7bd2fefc4c8" translate="yes" xml:space="preserve">
          <source>The default floating point dtype is initially &lt;code&gt;torch.float32&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fed344676665a74468953d310a0df7afafaa24e3" translate="yes" xml:space="preserve">
          <source>The default floating point tensor type is initially &lt;code&gt;torch.FloatTensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a80bd362bedb3fcb6747d9fd75b4961eb0cdb1f6" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68e62e57e835a4d91bc43ca48c95e3efa63535c9" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1984f4c44a0a732d169d5d6a5c5f08cad523bc50" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to a combination of &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6def4e7a63f51a47c9f6b51fb1bb8c54875ce717" translate="yes" xml:space="preserve">
          <source>The distance swap is described in detail in the paper &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;Learning shallow convolutional feature descriptors with triplet losses&lt;/a&gt; by V. Balntas, E. Riba et al.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0ab87bc03e5a5294329af8c45b84a5da57986c5" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework makes it easy to run functions remotely, supports referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backward and update parameters across RPC boundaries. These features can be categorized into four sets of APIs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58cfec2b510135f93e0701fc204e248929b783f2" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework provides mechanisms for multi-machine model training through a set of primitives to allow for remote communication, and a higher-level API to automatically differentiate models split across several machines.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10060f95e0764042598d146fc06132ab3dfae090" translate="yes" xml:space="preserve">
          <source>The distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dafbad2c73a3de536f4d90e5d5fa2fe3c91b9950" translate="yes" xml:space="preserve">
          <source>The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed pacakge in &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; (by explicitly creating the store as an alternative to specifying &lt;code&gt;init_method&lt;/code&gt;.) There are 3 choices for Key-Value Stores: &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6db33b80979f7c8f4114d1e3c176aabac7ef279b" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77ad1382d4823bfe1261c0964c88050a7ad3d117" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the divisor &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c8c32198d8c41dc6ec53302edb97fbbfa15ce0d" translate="yes" xml:space="preserve">
          <source>The division by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4d19d774aea3434987d4b4cece2b5f99cec9991" translate="yes" xml:space="preserve">
          <source>The dlpack shares the tensors memory. Note that each dlpack can only be consumed once.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a37e21cd1a669630ae636a9ca39104d114477d0e" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic cosine is &lt;code&gt;[1, inf)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for &lt;code&gt;+ INF&lt;/code&gt; for which the output is mapped to &lt;code&gt;+ INF&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9eae4d9c836ec727b1fd15ef73922995c7e82e1" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic tangent is &lt;code&gt;(-1, 1)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for the values &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;-1&lt;/code&gt; for which the output is mapped to &lt;code&gt;+/-INF&lt;/code&gt; respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e24f7a4d115e62a42f56d551f1451d37a5e25266" translate="yes" xml:space="preserve">
          <source>The dynamic control flow is captured correctly. We can verify in backends with different loop range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a418864112d90d162f2d2d68264a78b5661b0c49" translate="yes" xml:space="preserve">
          <source>The eigenvalues are returned in ascending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70c72529d7b55dc5ab2b127843a9a42c7179e611" translate="yes" xml:space="preserve">
          <source>The elements are sorted into equal width bins between &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;. If &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt; are both zero, the minimum and maximum values of the data are used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef67a3a1e1ceed752fed1303fe693fb7bb4731f9" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;Backend.UNDEFINED&lt;/code&gt; is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0ed2b956d8f2913276ea868cc2cc1c195e28811" translate="yes" xml:space="preserve">
          <source>The example script above produces the graph:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8850a69d0e77cfdf6128ac9dcc486c371b38e541" translate="yes" xml:space="preserve">
          <source>The export fails because PyTorch does not support exporting &lt;code&gt;elu&lt;/code&gt; operator. We find &lt;code&gt;virtual Tensor elu(const Tensor &amp;amp; input, Scalar alpha, bool inplace) const override;&lt;/code&gt; in &lt;code&gt;VariableType.h&lt;/code&gt;. This means &lt;code&gt;elu&lt;/code&gt; is an ATen operator. We check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;, and confirm that &lt;code&gt;Elu&lt;/code&gt; is standardized in ONNX. We add the following lines to &lt;code&gt;symbolic_opset9.py&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9a2121d113f583a300c4eeb7ced28dd018af26f" translate="yes" xml:space="preserve">
          <source>The fact that gradients need to be computed for a Tensor do not mean that the &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; attribute will be populated, see &lt;a href=&quot;autograd#torch.Tensor.is_leaf&quot;&gt;&lt;code&gt;is_leaf&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="221c8b91c0e460724cf31fb2005c0c9e54071204" translate="yes" xml:space="preserve">
          <source>The first call to add for a given &lt;code&gt;key&lt;/code&gt; creates a counter associated with &lt;code&gt;key&lt;/code&gt; in the store, initialized to &lt;code&gt;amount&lt;/code&gt;. Subsequent calls to add with the same &lt;code&gt;key&lt;/code&gt; increment the counter by the specified &lt;code&gt;amount&lt;/code&gt;. Calling &lt;code&gt;add()&lt;/code&gt; with a key that has already been set in the store by &lt;code&gt;set()&lt;/code&gt; will result in an exception.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d04a7a0c088c359fab554cd1a6049882b564ff2e" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26a3da676366966fcc7fbfc997f2f64e6ee31c9d" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in &lt;code&gt;VariableType.h&lt;/code&gt;, because dispatch is done with keyword arguments.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5964988cdbc48443556dc471facbf25fa24c6e63" translate="yes" xml:space="preserve">
          <source>The following APIs allow users to remotely execute functions as well as create references (RRefs) to remote data objects. In these APIs, when passing a &lt;code&gt;Tensor&lt;/code&gt; as an argument or a return value, the destination worker will try to create a &lt;code&gt;Tensor&lt;/code&gt; with the same meta (i.e., shape, stride, etc.). We intentionally disallow transmitting CUDA tensors because it might crash if the device lists on source and destination workers do not match. In such cases, applications can always explicitly move the input tensors to CPU on the caller and move it to the desired devices on the callee if necessary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0331e44de903db51dab5949d62e369ce763fc069" translate="yes" xml:space="preserve">
          <source>The following Python Expressions are supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f123fb71a763a700ae8133c3cf978d6e15341a63" translate="yes" xml:space="preserve">
          <source>The following factory functions support named tensors:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="690193766b4768be386db05b742730b7dbdf34e9" translate="yes" xml:space="preserve">
          <source>The following methods are unique to &lt;a href=&quot;#torch.BoolTensor&quot;&gt;&lt;code&gt;torch.BoolTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fcc9561d17b3f2c0c3db2c65c4b45e1fd39309f" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will act deterministically when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c856d87fb36615ab5d3e65d2705a558ef7fc7ab" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will throw a &lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt;&lt;code&gt;RuntimeError&lt;/code&gt;&lt;/a&gt; when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10cc3c798c883edec11b18c9b813ac5a197463fe" translate="yes" xml:space="preserve">
          <source>The following operators are supported:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="116feb31ceb0c75370eb04add5159eb16382e56b" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in PyTorch 1.8. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.fftn&quot;&gt;&lt;code&gt;torch.fft.fftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18bbed5d4e21fbcafc8624ec507383dc653a9737" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.ifft&quot;&gt;&lt;code&gt;torch.ifft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.ifftn&quot;&gt;&lt;code&gt;torch.fft.ifftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1a40cf129fc842a7fe39cfe2023111384b05a6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;torch.irfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.irfft&quot;&gt;&lt;code&gt;torch.fft.irfft()&lt;/code&gt;&lt;/a&gt; for one-sided input, or &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; for two-sided input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0db51d208a07883c44d7be8c9786454460ee522" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.rfft&quot;&gt;&lt;code&gt;torch.rfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.rfft&quot;&gt;&lt;code&gt;torch.fft.rfft()&lt;/code&gt;&lt;/a&gt; for one-sided output, or &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; for two-sided output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78c07c2cfdea987b1b930691f54fd99adb7a7932" translate="yes" xml:space="preserve">
          <source>The function is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81e6543a378b1e27df259854cebe693b6ae023b6" translate="yes" xml:space="preserve">
          <source>The gated linear unit. Computes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c30476379351f558b5c5fb5c388947e81c5eddf9" translate="yes" xml:space="preserve">
          <source>The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying &lt;code&gt;gradient&lt;/code&gt;. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63a016154b7848c498c0189a691b12aee6391726" translate="yes" xml:space="preserve">
          <source>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3b8c649dc7d5a1913425cba9308cbaac351ab8a" translate="yes" xml:space="preserve">
          <source>The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91b8909b6b0301d53bc827787fcd07621d744139" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; has computed an output. It should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea6266d19428a156d329cc252aacfffb34135bcf" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;code&gt;forward()&lt;/code&gt; has computed an output. It should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3027e825f33c77d15aef5c4a000dacdcd543639b" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is invoked. It should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="658103272c0bdd847853bfee8c1acd84bd1873a4" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;code&gt;forward()&lt;/code&gt; is invoked. It should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7317d93b9ecea6baa18606aa1761b42a13618f5" translate="yes" xml:space="preserve">
          <source>The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47b9323acd4c956c6840fecee45ba64f27f04f5f" translate="yes" xml:space="preserve">
          <source>The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute &amp;ndash; that is, contain a small number of assigned labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd1e3da6ac802ac499dbc929d3c01e7d8d1e592e" translate="yes" xml:space="preserve">
          <source>The implementation is based on the Algorithm 5.1 from Halko et al, 2009.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b39161781b1b7b35ca70ad2826501e304e14740d" translate="yes" xml:space="preserve">
          <source>The implementation is based on: Bader, P.; Blanes, S.; Casas, F. Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation. Mathematics 2019, 7, 1174.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9d89b5b4a75085b7eb81dcbff4669f9ff584606" translate="yes" xml:space="preserve">
          <source>The implementation of SVD on CPU uses the LAPACK routine &lt;code&gt;?gesdd&lt;/code&gt; (a divide-and-conquer algorithm) instead of &lt;code&gt;?gesvd&lt;/code&gt; for speed. Analogously, the SVD on GPU uses the MAGMA routine &lt;code&gt;gesdd&lt;/code&gt; as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c05db592ff41dfc6119d9765d1f98df7bbeac593" translate="yes" xml:space="preserve">
          <source>The implementations of the models for object detection, instance segmentation and keypoint detection are efficient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ee40d087e274de07c646cdfb7f973b9dab8f9cf" translate="yes" xml:space="preserve">
          <source>The inferred dtype for python floats in &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c9a1b1a8c22e228f94e607d9ba23a7a9895221c" translate="yes" xml:space="preserve">
          <source>The input &lt;code&gt;window_length&lt;/code&gt; is a positive integer controlling the returned window size. &lt;code&gt;periodic&lt;/code&gt; flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;. Therefore, if &lt;code&gt;periodic&lt;/code&gt; is true, the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dffb8d48e1eff38cec4958715ef4e4bea17673f9" translate="yes" xml:space="preserve">
          <source>The input channels are separated into &lt;code&gt;num_groups&lt;/code&gt; groups, each containing &lt;code&gt;num_channels / num_groups&lt;/code&gt; channels. The mean and standard-deviation are calculated separately over the each group.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24d9bce9a9af270abcd63358a34e29c31cf80c6f" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="501f21d86b3222c931438d1d37f49ef014fe1070" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d84cfbddddc9e1c2d0a053623e51f4c9b23ed343" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;code&gt;forward()&lt;/code&gt; is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0bbaa069be68586598b46c411d0f2665ffc0e9d" translate="yes" xml:space="preserve">
          <source>The input data is assumed to be of the form &lt;code&gt;minibatch x channels x [optional depth] x [optional height] x width&lt;/code&gt;. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="855506544cad1eeac05a21efea97a3853e2dceb9" translate="yes" xml:space="preserve">
          <source>The input dimensions are interpreted in the form: &lt;code&gt;mini-batch x channels x [optional depth] x [optional height] x width&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff7b890633934634eb44c66753eb478acaced44e" translate="yes" xml:space="preserve">
          <source>The input is assumed to be a low-rank matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa72bb59a173177382b9660fee81f62bda47bdae" translate="yes" xml:space="preserve">
          <source>The input quantization parameters are propagated to the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c8e7facaf470e54c3381c46dad3dd11548e58de" translate="yes" xml:space="preserve">
          <source>The input quantization parameters propagate to the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c93ac513b2302a4f83a43405b9a47bcb2309e67" translate="yes" xml:space="preserve">
          <source>The input to the model is expected to be a list of tensors, each of shape &lt;code&gt;[C, H, W]&lt;/code&gt;, one for each image, and should be in &lt;code&gt;0-1&lt;/code&gt; range. Different images can have different sizes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33129e152019831a4d6451e1dcb0878f56c1bef2" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.&lt;/code&gt; prefix for some operations. See example usage below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="351e07f8a31af7d82fc6045df4c793836de62f62" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.ops.quantized&lt;/code&gt; prefix. See example usage below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c3dcf3c5f75afd5957f352f9917b83b90088988" translate="yes" xml:space="preserve">
          <source>The interface for specifying operator definitions is a Prototype feature; adventurous users should note that the APIs will probably change in a future interface.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bb29d8ff742c3f78ac5a2da01f22239940307d9" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ff786d8f2651e07ce1136605dc8809d77fc8a6a" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd26976415ff02dd321f8aabd58b78a55df77549" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.irfft#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51e2eb96b40c3aad396917c7dd98de9a6585beea" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b9e7887dd437999f146f02a0f60028f96d84bf3" translate="yes" xml:space="preserve">
          <source>The largest difference between TorchScript and the full Python language is that TorchScript only supports a small set of types that are needed to express neural net models. In particular, TorchScript supports:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b1d6d882f09df5877abb8ef200d99a4552edbcc" translate="yes" xml:space="preserve">
          <source>The largest representable number.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63b23adb215af0fe9eaa51ef9153973e0c53546f" translate="yes" xml:space="preserve">
          <source>The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08f3358598c31282b2114da38bcad5d74e704334" translate="yes" xml:space="preserve">
          <source>The locations are used in the order of</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ebd3a640891ee47a9a38fbd56c89724e57e2af2" translate="yes" xml:space="preserve">
          <source>The loss can be described as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f21bd0e4e809cda5ba13223c1dfb3494e56f4f3e" translate="yes" xml:space="preserve">
          <source>The loss function for</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86081d5fc32148129e542f3dc9192fb2857da205" translate="yes" xml:space="preserve">
          <source>The loss function for each pair of samples in the mini-batch is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb7d036d43546cb725a6bf9bf29c5cc2076ac418" translate="yes" xml:space="preserve">
          <source>The loss function for each sample in the mini-batch is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48f8488012ce60a4d13e1aebd7fa0ba10061eaf3" translate="yes" xml:space="preserve">
          <source>The loss function for each sample is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c43bc97db5e0527c42aa84dc0f06c5fb6d69ddbc" translate="yes" xml:space="preserve">
          <source>The loss function then becomes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dbd64b8b1e6f8a768dc1cc623e35d46784438d27" translate="yes" xml:space="preserve">
          <source>The losses are averaged across observations for each minibatch. If the &lt;code&gt;weight&lt;/code&gt; argument is specified then this is a weighted average:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="187e00ce183312dd132d6826870b8e0983ae12a8" translate="yes" xml:space="preserve">
          <source>The lower triangular part of the matrix is defined as the elements on and below the diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cba1d04dc7071338e4468867933f6f030f4f19fd" translate="yes" xml:space="preserve">
          <source>The machine with rank 0 will be used to set up all connections.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6b0320b620362b6deb5bb70025cb38b632f6a57" translate="yes" xml:space="preserve">
          <source>The main trick for &lt;code&gt;hard&lt;/code&gt; is to do &lt;code&gt;y_hard - y_soft.detach() + y_soft&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61caa3335965d2f3a59a97a54b15a39417eaac38" translate="yes" xml:space="preserve">
          <source>The max-pooling operation is applied in</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d65dca627767b86d15d82ece9fa3008acf19101f" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3242427612d3b3c3e0cb9845d6bf7db1596d5b9" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over the mini-batches and</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5661df9ad4405c387ea95d88a69a9fe1650b1552" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29fab6215a0696acbd3e84d8cb442b324881c861" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by &lt;code&gt;normalized_shape&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a930aa85422123ed7ce795a3af91990ae6ee8f7" translate="yes" xml:space="preserve">
          <source>The mean operation still operates over all the elements, and divides by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="619af861d215de1b87a6226cb50c28f6e623bb30" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements in the dimension &lt;code&gt;dim&lt;/code&gt;. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d7cf56bfc5cedceb3ca00313d8c3d414937caef" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3568ef228089a375953fda3fd0c97aca0ace05a3" translate="yes" xml:space="preserve">
          <source>The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f15c86c9dfb63d5c112adb09bde9cb61a9fe3f2e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="065007e637ab5dd9f6a9a41f5c632d9aa1aa3f5e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the mask loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0c3a4815182bd38a59e8d48bd1acda3676df3d3" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25fbe42f5bebb67d721733ebaa470925353914ab" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32c7b059b3336b886e2737e75a7a90bf14890a11" translate="yes" xml:space="preserve">
          <source>The models expect a list of &lt;code&gt;Tensor[C, H, W]&lt;/code&gt;, in the range &lt;code&gt;0-1&lt;/code&gt;. The models internally resize the images so that they have a minimum size of &lt;code&gt;800&lt;/code&gt;. This option can be changed by passing the option &lt;code&gt;min_size&lt;/code&gt; to the constructor of the models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8cd112b9466880b251d727f36c9b5a8099f6121" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for detection:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fd7064c0a6dbdb00e5c05b4bcf7bb996635bfb3" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for image classification:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a82a240d4b1d0867d7d72a1a3f555d48973b4c7" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for semantic segmentation:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18bc003911e615e5dba9603961f512f27acb297a" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cf46d13a21b259a13e6ce62948cca48ddbc78f6" translate="yes" xml:space="preserve">
          <source>The modes available for resizing are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only), &lt;code&gt;area&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28d8cfd32d3249e2763e181ffc749144e0ea24a2" translate="yes" xml:space="preserve">
          <source>The modes available for upsampling are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18b2caf95e73a1c791145d0b79ff330ee08dd6df" translate="yes" xml:space="preserve">
          <source>The module can be accessed as an attribute using the given name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00f10b867e105cafb015ef514a5b43106e8a7867" translate="yes" xml:space="preserve">
          <source>The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f00cd6c1142679bd0fe1c3ebd4ca78600af96a2c" translate="yes" xml:space="preserve">
          <source>The name of the worker.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fba68059efcba7726021d64196e7ad3e2e4fcb96" translate="yes" xml:space="preserve">
          <source>The named tensor API is a prototype feature and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed535f6a7c3020c739db9c59caec32bca959d97b" translate="yes" xml:space="preserve">
          <source>The named tensor API is experimental and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36555b023721ec5738f0c183a135318d3c9a4466" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80880349d7cc0a615947c8d7ecadbb60eab216c2" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss. It is useful to train a classification problem with &lt;code&gt;C&lt;/code&gt; classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07e33a3c723549700a6c484c42061c0b83f6423c" translate="yes" xml:space="preserve">
          <source>The new backend derives from &lt;code&gt;c10d.ProcessGroup&lt;/code&gt; and registers the backend name and the instantiating interface through &lt;code&gt;torch.distributed.Backend.register_backend()&lt;/code&gt; when imported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ef80d57536dd4f889b111944ebd026c0916a56b" translate="yes" xml:space="preserve">
          <source>The new usage looks like this:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfa45b6345caf33366ff2d44c58d6e6325263da6" translate="yes" xml:space="preserve">
          <source>The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="760d37d3c11fd75fa728a8e00ba020ac1e72a10c" translate="yes" xml:space="preserve">
          <source>The normalization parameters are different from the image classification ones, and correspond to the mean and std from Kinetics-400.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b4b699c449424f5a0ee9c963faafecf0fd0d061" translate="yes" xml:space="preserve">
          <source>The number of bins (size 1) is one larger than the largest value in &lt;code&gt;input&lt;/code&gt; unless &lt;code&gt;input&lt;/code&gt; is empty, in which case the result is a tensor of size 0. If &lt;code&gt;minlength&lt;/code&gt; is specified, the number of bins is at least &lt;code&gt;minlength&lt;/code&gt; and if &lt;code&gt;input&lt;/code&gt; is empty, then the result is tensor of size &lt;code&gt;minlength&lt;/code&gt; filled with zeros. If &lt;code&gt;n&lt;/code&gt; is the value at position &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;out[n] += weights[i]&lt;/code&gt; if &lt;code&gt;weights&lt;/code&gt; is specified else &lt;code&gt;out[n] += 1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="792cc1a58f0a3a46d19118877b6d13c493cb98ec" translate="yes" xml:space="preserve">
          <source>The number of bits occupied by the type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="280e9a9c6268d27aec18f9552a0a0e023b3a5d6c" translate="yes" xml:space="preserve">
          <source>The number of keys present in the store.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb1db44aaa6eaf0c2b9f7e2d3594ec089fe36151" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by &lt;code&gt;TensorPipeAgent&lt;/code&gt; to execute requests.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20a68b367a08bf827b8cb29b9a7a5775403402ae" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by ProcessGroupAgent.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25f85e0ff2381c99b81c5c68406ed2a516dd1318" translate="yes" xml:space="preserve">
          <source>The numerical properties of a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; can be accessed through either the &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; or the &lt;a href=&quot;#torch.torch.iinfo&quot;&gt;&lt;code&gt;torch.iinfo&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8e759a2b7197a6d6914a25b2e4bccfa94711180" translate="yes" xml:space="preserve">
          <source>The operation applied is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b61e3d8bf7f65d3c8a372be2ffe4aa181113744" translate="yes" xml:space="preserve">
          <source>The operation is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="416029866582b442a7cfd38c944f5ccc41de6ea3" translate="yes" xml:space="preserve">
          <source>The operator set above is sufficient to export the following models:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97549a235d386a02f6c874f871b2a3fc368e849a" translate="yes" xml:space="preserve">
          <source>The order of norm. inf refers to &lt;code&gt;float('inf')&lt;/code&gt;, numpy&amp;rsquo;s &lt;code&gt;inf&lt;/code&gt; object, or any equivalent object. The following norms can be calculated:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="587438e00b276c5c07c33c3b833d1847c428edb1" translate="yes" xml:space="preserve">
          <source>The original &lt;code&gt;module&lt;/code&gt; with the converted &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layers. If the original &lt;code&gt;module&lt;/code&gt; is a &lt;code&gt;BatchNorm*D&lt;/code&gt; layer, a new &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layer object will be returned instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c43e721e428fefa0921064ef9d974f9d9c3710a" translate="yes" xml:space="preserve">
          <source>The original module with the spectral norm hook</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c8ee61f168965198d32cd38694cc41b385bdcc7" translate="yes" xml:space="preserve">
          <source>The original module with the weight norm hook</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7c71aa7150538e239f1ed9763931d65c1aab1d1" translate="yes" xml:space="preserve">
          <source>The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d13bc6b68893874e15ef23c3e8bf09f29542e58" translate="yes" xml:space="preserve">
          <source>The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd1aa83704336b7c60e0ac15205527d8852065d5" translate="yes" xml:space="preserve">
          <source>The output of the &lt;code&gt;model&lt;/code&gt; callable when called with the given &lt;code&gt;*args&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2624c3f93c8a5ba7f8db76475b9b2160784ad8be" translate="yes" xml:space="preserve">
          <source>The output size is H, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaed500780776d71aa7aa3beac0d23a2ad0bee52" translate="yes" xml:space="preserve">
          <source>The output tuple size must match the outputs of &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69fc4cd3bd84d50752da22ec0fa2da75a028b6c8" translate="yes" xml:space="preserve">
          <source>The package needs to be initialized using the &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; function before calling any other methods. This blocks until all processes have joined.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a24470d7f3dafd97610b1a97cfc4e625080f5d77" translate="yes" xml:space="preserve">
          <source>The padding size by which to pad some dimensions of &lt;code&gt;input&lt;/code&gt; are described starting from the last dimension and moving forward.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3d894892755f58e334d42f2a6bb03ad3c9c1029" translate="yes" xml:space="preserve">
          <source>The parallelized &lt;code&gt;module&lt;/code&gt; must have its parameters and buffers on &lt;code&gt;device_ids[0]&lt;/code&gt; before running this &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt; module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="611179cf7c23e5fa87e86dc65fc6c2d7edcfcdbf" translate="yes" xml:space="preserve">
          <source>The parameter can be accessed as an attribute using given name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3673b833c2a753da5c92e7b2bd7867575cba0695" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; can either be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb60e522db72c109467c75d771e518ca64062c0c" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can each be an &lt;code&gt;int&lt;/code&gt; or a one-element tuple.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df694fbe631671fb53535c127328a8f9b5ccfdfe" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can either be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d35449e64c1f14944b10572266aa02084f0787f" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt; can either be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16e542ef2d9e6480cf0213002d643e04ad39afd9" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;output_padding&lt;/code&gt; can either be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9df0d4116a648b1a73d8fd1ad856ffa0c5b125d9" translate="yes" xml:space="preserve">
          <source>The parameters represented by a single vector</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16c253cfff05379e83f73283d75913a9d3bc5eb0" translate="yes" xml:space="preserve">
          <source>The pivots returned by the function are 1-indexed. If &lt;code&gt;pivot&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the returned pivots is a tensor filled with zeros of the appropriate size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4600b83baa50f6eee75af80c6cdb275baf5a4820" translate="yes" xml:space="preserve">
          <source>The pre-trained models for detection, instance segmentation and keypoint detection are initialized with the classification models in torchvision.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bacc13981715b20b015df75a9953a0a7c1d88d4c" translate="yes" xml:space="preserve">
          <source>The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in &lt;code&gt;references/segmentation/coco_utils.py&lt;/code&gt;. The classes that the pre-trained model outputs are the following, in order:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f5bcce5c35434b7def7a2c2abecc7855a7ff6ae" translate="yes" xml:space="preserve">
          <source>The process for obtaining the values of &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;std&lt;/code&gt; is roughly equivalent to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb1f1b1b62d82d262fa3ee527ff2235c6fe33cf0" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse is not necessarily a continuous function in the elements of the matrix &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/0117004&quot;&gt;[1]&lt;/a&gt;. Therefore, derivatives are not always existent, and exist for a constant rank only &lt;a href=&quot;https://www.jstor.org/stable/2156365&quot;&gt;[2]&lt;/a&gt;. However, this method is backprop-able due to the implementation by using SVD results, and could be unstable. Double-backward will also be unstable due to the usage of SVD internally. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df88bf331dfcd873fd61e2a961e2a4565dca71ab" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of &lt;code&gt;input&lt;/code&gt; of dimensions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="801cd7b0289a0cab3f1164523d810bc285cdd10c" translate="yes" xml:space="preserve">
          <source>The published models should be at least in a branch/tag. It can&amp;rsquo;t be a random commit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="346d775a68239ce50fc9c648f8e7ac9c30934f97" translate="yes" xml:space="preserve">
          <source>The range of the linear region</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a67bc4760995b69a50ba7bb512777bd75a29786d" translate="yes" xml:space="preserve">
          <source>The rank of the process group -1, if not part of the group</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1640b5e94a24bbde9b57f55563952bd38f53f1c7" translate="yes" xml:space="preserve">
          <source>The real-to-complex Fourier transform results follow conjugate symmetry:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3326dbe4c730be437e49c0a1205659d772531b4" translate="yes" xml:space="preserve">
          <source>The regular implementation uses the (more common in PyTorch) &lt;code&gt;torch.long&lt;/code&gt; dtype.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f42ad6925b2f350139e05854ee56333bfbf81bfa" translate="yes" xml:space="preserve">
          <source>The relation of &lt;code&gt;(U, S, V)&lt;/code&gt; to PCA is as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0dee4722eb608774d930f39be3f46f18bcb0b430" translate="yes" xml:space="preserve">
          <source>The result will never require gradient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b0575a2f172061283c10c05f0db13db17166b95" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;alexnet.onnx&lt;/code&gt; is a binary protobuf file which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The keyword argument &lt;code&gt;verbose=True&lt;/code&gt; causes the exporter to print out a human-readable representation of the network:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c02f08f3d1024ae629cdcc31caf60626bc7a365" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;out&lt;/code&gt; tensor shares it&amp;rsquo;s underlying storage with the &lt;code&gt;input&lt;/code&gt; tensor, so changing the content of one would change the content of the other.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6758477f8561982cc2515cdbf897a47792390375" translate="yes" xml:space="preserve">
          <source>The resulting recording of &lt;code&gt;nn.Module.forward&lt;/code&gt; or &lt;code&gt;nn.Module&lt;/code&gt; produces &lt;code&gt;ScriptModule&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="094611e344ec0108ea463a9054c21e208ca824dd" translate="yes" xml:space="preserve">
          <source>The resulting recording of a standalone function produces &lt;code&gt;ScriptFunction&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14a7d916a182796f96dea780975d504da8d70e49" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object can come from &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; constructor. The example below shows directly using the &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; returned by &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35586b86275e1afc45c84727656b22c56c0dfc4c" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;out&lt;/code&gt; tensor only has values 0 or 1 and is of the same shape as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c1ca22658d19c73ead093d5a482f3f0748946cd" translate="yes" xml:space="preserve">
          <source>The returned Tensor&amp;rsquo;s data will be of size &lt;code&gt;T x B x *&lt;/code&gt;, where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence and &lt;code&gt;B&lt;/code&gt; is the batch size. If &lt;code&gt;batch_first&lt;/code&gt; is True, the data will be transposed into &lt;code&gt;B x T x *&lt;/code&gt; format.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a86d643d348528641013d71d94cfc9c2436b5f5a" translate="yes" xml:space="preserve">
          <source>The returned matrices will always be transposed, irrespective of the strides of the input matrices. That is, they will have stride &lt;code&gt;(1, m)&lt;/code&gt; instead of &lt;code&gt;(m, 1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e874ec4d7da2420c99c0303e4f7a958324a670ae" translate="yes" xml:space="preserve">
          <source>The returned tensor and &lt;code&gt;ndarray&lt;/code&gt; share the same memory. Modifications to the tensor will be reflected in the &lt;code&gt;ndarray&lt;/code&gt; and vice versa. The returned tensor is not resizable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93c4d85268a064ece907fbc8a5535a6f396d9a4e" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52f5aa4074a60eef379e1658572f1bb6509ef7da" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor. If &lt;code&gt;out&lt;/code&gt; has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f46100c8ee7a830bab0c89e9873612006ce16b25" translate="yes" xml:space="preserve">
          <source>The returned tensor has the same number of dimensions as the original tensor (&lt;code&gt;input&lt;/code&gt;). The &lt;code&gt;dim&lt;/code&gt;th dimension has the same size as the length of &lt;code&gt;index&lt;/code&gt;; other dimensions have the same size as in the original tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c72a07e12ac6ea766a45eac39941a4c32ee2a049" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e400aae54e1364ef7f0ae0e4394533a68ef83da8" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same underlying data with this tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="172826726b64a692f9e7f61dbdcbf5a0b8b36f39" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28c770021d54b212e8c62be1e09939a0369312ae" translate="yes" xml:space="preserve">
          <source>The rows of &lt;code&gt;input&lt;/code&gt; do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1eff8f651f1c1e81084414db9ad8e86ff7d386f5" translate="yes" xml:space="preserve">
          <source>The second argument can be a number or a tensor whose shape is &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the first argument.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35bad0572344e9ca340bcfea7b7e08eb1fb48094" translate="yes" xml:space="preserve">
          <source>The shape of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44ad26e34a9ec24516da93c641da630606e76045" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt; don&amp;rsquo;t need to match, but the total number of elements in each tensor need to be the same.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ab58315d98cc62bd1c43b5269219135f914a361" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="472d04a7b4ef19856846b867c2431ae86dfeee09" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4bb8d54bf4e66fa1d51318971fbd84c5dd471f64" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c2b5afb57ab65d651f6e8673ddc0bb16bc251d9" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;. If &lt;code&gt;weight&lt;/code&gt; is a tensor, then the shapes of &lt;code&gt;weight&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30ae8ec04282dfa89f820e2638cc76a4c5308051" translate="yes" xml:space="preserve">
          <source>The shapes of the &lt;code&gt;mask&lt;/code&gt; tensor and the &lt;code&gt;input&lt;/code&gt; tensor don&amp;rsquo;t need to match, but they must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80ed96242b937c96d491a6608007e2c80ef6fbfe" translate="yes" xml:space="preserve">
          <source>The singular values are returned in descending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86c71484bcb812cb0f2e7260fb264f82086dbacb" translate="yes" xml:space="preserve">
          <source>The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for &lt;code&gt;offset&lt;/code&gt; other than</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0739eb19e36226e98c11fd96e8291d8a209e3a7e" translate="yes" xml:space="preserve">
          <source>The smallest positive representable number.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf0617c3e175f01c2b40e71995108022f6b64f95" translate="yes" xml:space="preserve">
          <source>The smallest representable number (typically &lt;code&gt;-max&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27d91cd31d6556c601166e6b57bbeb8fc2cd13fe" translate="yes" xml:space="preserve">
          <source>The smallest representable number such that &lt;code&gt;1.0 + eps != 1.0&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff2ebfdbbf9420dca19c2e2c489305049c84e69e" translate="yes" xml:space="preserve">
          <source>The smallest representable number.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e4db462bf55509b809c59ea2298379ae2dba77f" translate="yes" xml:space="preserve">
          <source>The sources in &lt;code&gt;cuda_sources&lt;/code&gt; are concatenated into a separate &lt;code&gt;.cu&lt;/code&gt; file and prepended with &lt;code&gt;torch/types.h&lt;/code&gt;, &lt;code&gt;cuda.h&lt;/code&gt; and &lt;code&gt;cuda_runtime.h&lt;/code&gt; includes. The &lt;code&gt;.cpp&lt;/code&gt; and &lt;code&gt;.cu&lt;/code&gt; files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in &lt;code&gt;cuda_sources&lt;/code&gt; per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the &lt;code&gt;cpp_sources&lt;/code&gt; (and include its name in &lt;code&gt;functions&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f06fdc0c2350c66f3a9d82727c760926c8016f15" translate="yes" xml:space="preserve">
          <source>The stashing logic saves and restores the RNG state for the current device and the device of all cuda Tensor arguments to the &lt;code&gt;run_fn&lt;/code&gt;. However, the logic has no way to anticipate if the user will move Tensors to a new device within the &lt;code&gt;run_fn&lt;/code&gt; itself. Therefore, if you move Tensors to a new device (&amp;ldquo;new&amp;rdquo; meaning not belonging to the set of [current device + devices of Tensor arguments]) within &lt;code&gt;run_fn&lt;/code&gt;, deterministic output compared to non-checkpointed passes is never guaranteed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7f3ab8a2df042d6e3025c18a145e6e14f77af25" translate="yes" xml:space="preserve">
          <source>The sum operation still operates over all the elements, and divides by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2531e155fe9e77a5cfae31abf7b6f59ffb9fcaa" translate="yes" xml:space="preserve">
          <source>The support of third-party backend is experimental and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2873a12c84aba239b7842369e12bb64236d9f288" translate="yes" xml:space="preserve">
          <source>The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc114e497003f25e6fca05f82fcba607f7d78078" translate="yes" xml:space="preserve">
          <source>The tensors &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab7ff2ed427b3e162ddc9c1bc3618ade11c4bc8e" translate="yes" xml:space="preserve">
          <source>The torch package contains data structures for multi-dimensional tensors and mathematical operations over these are defined. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="437652102ffae6443117ed89a3eae4a8f5b41da3" translate="yes" xml:space="preserve">
          <source>The tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89850de90f112bfc18c8cdc61311b3872ebf2872" translate="yes" xml:space="preserve">
          <source>The tracer records the example inputs shape in the graph. In case the model should accept inputs of dynamic shape, you can utilize the parameter &lt;code&gt;dynamic_axes&lt;/code&gt; in export api.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2328cc72d77d5be4ac31362bf776708e1354f25" translate="yes" xml:space="preserve">
          <source>The unreduced (i.e. with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) loss can be described as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83a2adc9d7ff925b785017e994bf1c2dfff219e3" translate="yes" xml:space="preserve">
          <source>The unreduced loss (i.e., with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) can be described as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55b435ee6855f1f093ca66cf72abe8bd2c30eb03" translate="yes" xml:space="preserve">
          <source>The upper triangular part of the matrix is defined as the elements on and above the diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ff79d6204f8beff86c1883967aad41f978dae9c" translate="yes" xml:space="preserve">
          <source>The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8baea02cc4125823d2a9295d67b336747f5d060" translate="yes" xml:space="preserve">
          <source>The value held by this &lt;code&gt;Future&lt;/code&gt;. If the function (callback or RPC) creating the value has thrown an error, this &lt;code&gt;wait&lt;/code&gt; method will also throw an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c32d99e8bca557bc1750470ea69ae3bfb345050" translate="yes" xml:space="preserve">
          <source>The values of this class are lowercase strings, e.g., &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;. They can be accessed as attributes, e.g., &lt;code&gt;Backend.NCCL&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d09c2230d5453858141229e7b0087efe234f737" translate="yes" xml:space="preserve">
          <source>The values of this class can be accessed as attributes, e.g., &lt;code&gt;ReduceOp.SUM&lt;/code&gt;. They are used in specifying strategies for reduction collectives, e.g., &lt;a href=&quot;#torch.distributed.reduce&quot;&gt;&lt;code&gt;reduce()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt;&lt;code&gt;all_reduce_multigpu()&lt;/code&gt;&lt;/a&gt;, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="539dcafbce59fbba03d94008a7b26e37638c0f1b" translate="yes" xml:space="preserve">
          <source>The world size of the process group -1, if not part of the group</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5d66dffec3f1eab9a22e5606deb2eaebb37f157" translate="yes" xml:space="preserve">
          <source>Then &lt;code&gt;dynamic axes&lt;/code&gt; can be defined either as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cacf045e7460da89c00e2f44ce242ff34d557d4d" translate="yes" xml:space="preserve">
          <source>Then for any (supported) &lt;code&gt;input&lt;/code&gt; tensor the following equality holds:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61c6ce8b4e502ae0f9cf574180a84ce71379a45a" translate="yes" xml:space="preserve">
          <source>Then run the following code in two different processes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ce92152c376045e006c7df7e500a93b25016945" translate="yes" xml:space="preserve">
          <source>Then, you can run:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9157fc1a5d0f6b3459cf4d13b451acd4e1ae988a" translate="yes" xml:space="preserve">
          <source>There are 2 main ways to initialize a process group:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="989aa62e15f82179c46dd7bca86e4177e7b7bf45" translate="yes" xml:space="preserve">
          <source>There are a few main ways to create a tensor, depending on your use case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="448eaecf17e019efecc19b1280b7ffb9043292f0" translate="yes" xml:space="preserve">
          <source>There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a50a4f02b41f0e4e7768bdf9e0d8e8e8f9ca103" translate="yes" xml:space="preserve">
          <source>There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94f93e5cb61707d9a9f9f8c1e8bae87549a0661b" translate="yes" xml:space="preserve">
          <source>There are more examples in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;symbolic_opset9.py&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset10.py&quot;&gt;symbolic_opset10.py&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7b2c38d0b8818a513a12fc7c7e90b9479638b6e" translate="yes" xml:space="preserve">
          <source>There are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b0666c05baae6bc9dd7801107606ece55efb258" translate="yes" xml:space="preserve">
          <source>There are two main usages:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b78c278fc5d3de5d25f299a98858079b45aa6eb" translate="yes" xml:space="preserve">
          <source>There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired &lt;code&gt;world_size&lt;/code&gt;. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2b97adc02fc81457ca80fb753c694becd93e274" translate="yes" xml:space="preserve">
          <source>There is a subtlety in using the &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; pattern in a &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; wrapped in &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;My recurrent network doesn&amp;rsquo;t work with data parallelism&lt;/a&gt; section in FAQ for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bedff7a4f5803a09c69737d429b263e4a4e07c0" translate="yes" xml:space="preserve">
          <source>Therefore, indexing &lt;code&gt;output&lt;/code&gt; at the last dimension (column dimension) gives all values within a certain block.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="220cff043cb62e0e4cb5366907ebd80ead3bf366" translate="yes" xml:space="preserve">
          <source>Therefore, to invert an &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;normalized&lt;/code&gt; and &lt;code&gt;onesided&lt;/code&gt; arguments should be set identically for &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, and preferably a &lt;code&gt;signal_sizes&lt;/code&gt; is given to avoid size mismatch. See the example below for a case of size mismatch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11de0478b61c8d1fd83658744b437989c835c987" translate="yes" xml:space="preserve">
          <source>These are the basic building block for graphs</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ca4969bd4e4273b5349ab2f675d16c1de44f187" translate="yes" xml:space="preserve">
          <source>These backends include:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dd026a9fc19a074410394c15ee56c930e5686d0" translate="yes" xml:space="preserve">
          <source>These types and features from the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module are unavailble in TorchScript.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aca9bd729c973c4d3bc4eb7f3f8d4e953c82edfd" translate="yes" xml:space="preserve">
          <source>These unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5042b3fea81612a742984a1fb55be5b675720283" translate="yes" xml:space="preserve">
          <source>Third-party backends</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9ab4d0d07c5bfd671637ba9723d67603253bcae" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;momentum&lt;/code&gt; argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d057f12e63c19fa40f83b9e371a09dbaf3641ca" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;setuptools.build_ext&lt;/code&gt; subclass takes care of passing the minimum required compiler flags (e.g. &lt;code&gt;-std=c++14&lt;/code&gt;) as well as mixed C++/CUDA compilation (and support for CUDA files in general).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48b0bfc66718d9ed75062a9f5d57a687fd7749c9" translate="yes" xml:space="preserve">
          <source>This API is in beta and may change in the near future.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33a32d5fcbf1c45d06b8358ae2591cd5047cff16" translate="yes" xml:space="preserve">
          <source>This allows better BC support for &lt;a href=&quot;#torch.nn.Module.load_state_dict&quot;&gt;&lt;code&gt;load_state_dict()&lt;/code&gt;&lt;/a&gt;. In &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt;, the version number will be saved as in the attribute &lt;code&gt;_metadata&lt;/code&gt; of the returned state dict, and thus pickled. &lt;code&gt;_metadata&lt;/code&gt; is a dictionary with keys that follow the naming convention of state dict. See &lt;code&gt;_load_from_state_dict&lt;/code&gt; on how to use this information in loading.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09b607c5ebd52b97aa3c6cf95a474cc71af1ac47" translate="yes" xml:space="preserve">
          <source>This allows for different samples to have variable amounts of target classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc869bc2254cbb18e88bb0a8c7e186c575d69c2e" translate="yes" xml:space="preserve">
          <source>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd79f292616deed21e20e4449558742671a5f03e" translate="yes" xml:space="preserve">
          <source>This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set &lt;code&gt;use_external_data_format&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; to successfully export such models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9aed446ae3aa4dc28d8d65d1c929ff54cb2a74f" translate="yes" xml:space="preserve">
          <source>This attribute is &lt;code&gt;None&lt;/code&gt; by default and becomes a Tensor the first time a call to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; computes gradients for &lt;code&gt;self&lt;/code&gt;. The attribute will then contain the gradients computed and future calls to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; will accumulate (add) gradients into it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27eaef81d3fd1f9e704b27f64a092d8671cfb020" translate="yes" xml:space="preserve">
          <source>This can be called as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5eeeb2d9e42f39a239e76c72a9314e3136e5540" translate="yes" xml:space="preserve">
          <source>This can then be visualized with TensorBoard, which should be installable and runnable with:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2306bf42c44895d1fc76396938e439c137c2dffb" translate="yes" xml:space="preserve">
          <source>This class can be directly called to parse the string, e.g., &lt;code&gt;Backend(backend_str)&lt;/code&gt; will check if &lt;code&gt;backend_str&lt;/code&gt; is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., &lt;code&gt;Backend(&quot;GLOO&quot;)&lt;/code&gt; returns &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ef03a595a40d9415bbad765363aad2d964d5b16" translate="yes" xml:space="preserve">
          <source>This class does not provide a &lt;code&gt;forward&lt;/code&gt; hook. Instead, you must use one of the underlying functions (e.g. &lt;code&gt;add&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc115d83b6af36150f881200c064c4d238e34462" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c13ac423de7a3c762fee4c3a9510054ffdd5d384" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;. It is equivalent to &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f63c41dd97936df60be517dad06b50e25bfe14d0" translate="yes" xml:space="preserve">
          <source>This class uses &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; in order to retrieve the gradients for specific parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1862d470ef8a5cd214866c60dc3b2e18ed9571b7" translate="yes" xml:space="preserve">
          <source>This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43ff3e72065cb78dedbfc0cb5795ef6d313a86b8" translate="yes" xml:space="preserve">
          <source>This composition also works for &lt;code&gt;nn.Module&lt;/code&gt;s as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d56b8d3c3a6cd08249ac0400c9bd4fa666010028" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given &lt;code&gt;module&lt;/code&gt; by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e99fa6e39b3592c4eea5075ef0e0720a6948ff1" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c09652531ac8f4b111870bcb71b813e31279457c" translate="yes" xml:space="preserve">
          <source>This context manager is thread local; it will not affect computation in other threads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26ef7031b1f75f10e86902c0f567ce05a4ea44b5" translate="yes" xml:space="preserve">
          <source>This context manager will keep track of already-joined DDP processes, and &amp;ldquo;shadow&amp;rdquo; the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09d1e555fd19e686f48ac7306fa2525b334ba962" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt;&lt;code&gt;nn.LogSoftmax()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;nn.NLLLoss()&lt;/code&gt;&lt;/a&gt; in one single class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="925d7f6dcda18d5ae787b2ea0d4e4cf02702d590" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;log_softmax&lt;/code&gt; and &lt;code&gt;nll_loss&lt;/code&gt; in a single function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7ea31e3bc10ddc7b5db55c0a7c9d23739bfa2db" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; and &lt;code&gt;nn.NLLLoss()&lt;/code&gt; in one single class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a017ebd876454e242cede3ce02b356f5f295daa" translate="yes" xml:space="preserve">
          <source>This criterion expects a &lt;code&gt;target&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; of the same size as the &lt;code&gt;input&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c10376150f504915e5bd847798be0f3a52dc669c" translate="yes" xml:space="preserve">
          <source>This criterion expects a class index in the range</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c181079125dc2e25b97bd2d3da70b20b81a75a7" translate="yes" xml:space="preserve">
          <source>This decorator also works with RRef helpers, i.e., . &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_sync&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_sync()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_async&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_async()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.rpc.RRef.remote&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.remote()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba581390e34d5e57c67e068b92dac8e4709b3ad6" translate="yes" xml:space="preserve">
          <source>This decorator indicates that a method on an &lt;code&gt;nn.Module&lt;/code&gt; is used as an entry point into a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and should be compiled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="174b3d4b823448bf6302b0b53449be4c6d5ad12e" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d10437bc80b062c46cb5ff8be0bb49322ddb59f" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function. This allows you to leave code in your model that is not yet TorchScript compatible. If called from TorchScript, ignored functions will dispatch the call to the Python interpreter. Models with ignored functions cannot be exported; use &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1508df53ef2be98f34c189e45c94ca9319ce75f2" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75d6e9d0a82393986fc62b4441accfe0e14ee3a9" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8952c5fbe2a12cf21923d9bc22bc2c081674eb1e" translate="yes" xml:space="preserve">
          <source>This defines</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e40f466c8c675241a4989cdf735bce5f4ddce6bc" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?orgqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-orgqr&quot;&gt;LAPACK documentation for orgqr&lt;/a&gt; for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fb3c5a1f24bdd83af6fd14c07d4d7ef76ec7527" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?ormqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-ormqr&quot;&gt;LAPACK documentation for ormqr&lt;/a&gt; for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7d3fb60afba5b578d1ed91b3b2d20aba1664bb0" translate="yes" xml:space="preserve">
          <source>This error usually means that the method you are tracing uses a module&amp;rsquo;s parameters and you are passing the module&amp;rsquo;s method instead of the module instance (e.g. &lt;code&gt;my_module_instance.forward&lt;/code&gt; vs &lt;code&gt;my_module_instance&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c753cc91ef9f274f764a4ae41747b732c7fe2261" translate="yes" xml:space="preserve">
          <source>This feature is in beta, and its design and implementation may change in the future.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7614e828d8ac51033f6b1d436da1e7ec19a65d5" translate="yes" xml:space="preserve">
          <source>This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; object by accessing its &lt;code&gt;.data&lt;/code&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97ac98bd75da52df5620120c1ca469a7f25204b3" translate="yes" xml:space="preserve">
          <source>This function accumulates gradients in the leaves - you might need to zero &lt;code&gt;.grad&lt;/code&gt; attributes or set them to &lt;code&gt;None&lt;/code&gt; before calling it. See &lt;a href=&quot;autograd#default-grad-layouts&quot;&gt;Default gradient layouts&lt;/a&gt; for details on the memory layout of accumulated gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5919d90354daacc619c8dc367427cc669c661c6e" translate="yes" xml:space="preserve">
          <source>This function behaves exactly like &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;, but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of &lt;a href=&quot;#torch.utils.cpp_extension.load_inline&quot;&gt;&lt;code&gt;load_inline()&lt;/code&gt;&lt;/a&gt; is identical to &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4742c80d8253ffcd737e46fcd5dffcfb774bfb9a" translate="yes" xml:space="preserve">
          <source>This function calculates all eigenvalues (and vectors) of &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f591ac1d01d2706090dd5158a48cfafbf7078fcd" translate="yes" xml:space="preserve">
          <source>This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the &lt;code&gt;ord&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a99e80c93a86e3189bf95a219e4fc70666128f91" translate="yes" xml:space="preserve">
          <source>This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2742ba18db0e477006e0d4a879a88ee8aaa73a1c" translate="yes" xml:space="preserve">
          <source>This function checks if all &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; satisfy the condition:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ad58a0a805b7dcf7c19d64d7ab1a5b0774c3bf4" translate="yes" xml:space="preserve">
          <source>This function does exact same thing as &lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt;&lt;code&gt;torch.addmm()&lt;/code&gt;&lt;/a&gt; in the forward, except that it supports backward for sparse matrix &lt;code&gt;mat1&lt;/code&gt;. &lt;code&gt;mat1&lt;/code&gt; need to have &lt;code&gt;sparse_dim = 2&lt;/code&gt;. Note that the gradients of &lt;code&gt;mat1&lt;/code&gt; is a coalesced sparse tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b687a9d19b48f08afdcc0c1bcf3778a2f67002f3" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6eaaf4ac25e2420bf8da7dbd7b25891750193e61" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;. For broadcasting matrix products, see &lt;a href=&quot;torch.matmul#torch.matmul&quot;&gt;&lt;code&gt;torch.matmul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1273ec3288d9debd903d3f9e04111f9b0d5a8d6b" translate="yes" xml:space="preserve">
          <source>This function does not check if the factorization was successful or not if &lt;code&gt;get_infos&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; since the status of the factorization is present in the third element of the return tuple.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68c688ded2e6ac8d7e4721168f69f94608270e24" translate="yes" xml:space="preserve">
          <source>This function does not optimize the given expression, so a different formula for the same computation may run faster or consume less memory. Projects like opt_einsum (&lt;a href=&quot;https://optimized-einsum.readthedocs.io/en/stable/&quot;&gt;https://optimized-einsum.readthedocs.io/en/stable/&lt;/a&gt;) can optimize the formula for you.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f44824ec132a2e35c26075b94e4bfe941180a2db" translate="yes" xml:space="preserve">
          <source>This function doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c40534c7583c126c2b6a9ea9f9ce426b4120e2b" translate="yes" xml:space="preserve">
          <source>This function is a front-end to the following LOBPCG algorithms selectable via &lt;code&gt;method&lt;/code&gt; argument:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="773eb7977e3d589f5a2a946752611ffc03872d99" translate="yes" xml:space="preserve">
          <source>This function is deprecated and may be removed in a future release. It can be implemented using &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; as &lt;code&gt;alpha * torch.outer(vec1, vec2) + beta * input&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is not zero, and as &lt;code&gt;alpha * torch.outer(vec1, vec2)&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is zero.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecdb962fda9e357b596ac207e636ee312e395193" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future PyTorch release. Use &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32ba15aae127598880c645601e7ae7f8189c6c82" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future release because its behavior is inconsistent with Python&amp;rsquo;s range builtin. Instead, use &lt;a href=&quot;torch.arange#torch.arange&quot;&gt;&lt;code&gt;torch.arange()&lt;/code&gt;&lt;/a&gt;, which produces values in [start, end).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd06f9dfe663c3caf3762160bc99450c126a39a9" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1632348f060a8b22bffa765800a936ba8981c455" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e11fc8a8842739d9e68d3ea9f36d243b26e33ee2" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="409b1beff6de19c9e0fc8534a682b00b07c2667b" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f088ca0f34a9882f6e67d133eea7956f99a05c61" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a1e1948ca54d505e36aa98bcd4ccf28bf3f09403" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6111b83606e5f76d2d19a4fd63d2df2a457f3738" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique#torch.unique&quot;&gt;&lt;code&gt;torch.unique()&lt;/code&gt;&lt;/a&gt; in the sense that this function only eliminates consecutive duplicate values. This semantics is similar to &lt;code&gt;std::unique&lt;/code&gt; in C++.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c84bc16a342354e3160aa386d7ad206e77f3ce71" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt; in the sense that this function also eliminates non-consecutive duplicate values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42a1f69345a02122fc091a770868079bb8638971" translate="yes" xml:space="preserve">
          <source>This function is differentiable, so gradients will flow back from the result of this operation to &lt;code&gt;input&lt;/code&gt;. To create a tensor without an autograd relationship to &lt;code&gt;input&lt;/code&gt; see &lt;a href=&quot;../autograd#torch.Tensor.detach&quot;&gt;&lt;code&gt;detach()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fb9aa5da82ff7de2350851193131992c19c52a8" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.cdist(input,&amp;rsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc680c0e90287ae914b5b4470b9652c693f1647a" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.pdist(input, &amp;lsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff6a78f486235a2c4ee2bb0612d674661defb9c5" translate="yes" xml:space="preserve">
          <source>This function is here for legacy reasons, may be removed from nn.Functional in the future.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ffbcd1ffd822d93e92f3e882d1d47ebf27a02de" translate="yes" xml:space="preserve">
          <source>This function is implemented only for nonnegative integers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4af64360b4b0bf60d1db0c5268a216c0039ae031" translate="yes" xml:space="preserve">
          <source>This function is more accurate than &lt;a href=&quot;torch.log#torch.log&quot;&gt;&lt;code&gt;torch.log()&lt;/code&gt;&lt;/a&gt; for small values of &lt;code&gt;input&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9a118660ffcea349bb4668844a1f15351a99bbb" translate="yes" xml:space="preserve">
          <source>This function is not defined for &lt;code&gt;torch.cuda.Tensor&lt;/code&gt; yet.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69c6cf6408f0b3c6058b2d09b95171f65a52d054" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt;&lt;code&gt;affine_grid()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bda1093b38d267fe7d744a3f96bb51ca111b18d" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d26ea75bdd03743d020bb8fd3f0b4f05e5fca44d" translate="yes" xml:space="preserve">
          <source>This function only works with CPU tensors and should not be used in code sections that require high performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ed8f6139c0d2640189a9321e1da51e7b538d228" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;max(dim=0)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49eba71d4c9b31d15ee67772326e88d1462e9393" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;median(dim=0)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9251c4cd19a437c8f31a2c44be4bdf2e2282d65" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;min(dim=0)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d3e2fb10adcbb7f259d94a424b7b3b695d7efa3" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6dd319ec8d2bd7cb7695497b2e2c0c7da18093d6" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="464a58780f1e7ad5a78398d14f9e91d688f64e19" translate="yes" xml:space="preserve">
          <source>This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a52b7a21105c565c661abb413198331f8bc8a45e" translate="yes" xml:space="preserve">
          <source>This function returns a Tensor of size &lt;code&gt;T x B x *&lt;/code&gt; or &lt;code&gt;B x T x *&lt;/code&gt; where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c177e0164ff9028b4584fc5df4d1e3daee9e253a" translate="yes" xml:space="preserve">
          <source>This function returns a handle with a method &lt;code&gt;handle.remove()&lt;/code&gt; that removes the hook from the module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0496aaa431bc80a1684ef66937522b9d485ea0d" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the nearly optimal approximation of a singular value decomposition of a centered matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eda5e94a6c116342560b85fa8d7ac690f3e68684" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the singular value decomposition of a input real matrix or batches of real matrices &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aeb212c2592eaa5e981d2676890741ba536e05d" translate="yes" xml:space="preserve">
          <source>This function returns eigenvalues and eigenvectors of a real symmetric matrix &lt;code&gt;input&lt;/code&gt; or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f955323e5a9f295f213a25f7ecc7dec98b5caa2" translate="yes" xml:space="preserve">
          <source>This function returns the solution to the system of linear equations represented by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d7a84c0650abce4d3f9c668ace3ede1befabda4" translate="yes" xml:space="preserve">
          <source>This function&amp;rsquo;s name is a misnomer. It actually rounds the quotient towards zero instead of taking its floor. This behavior will be deprecated in a future PyTorch release.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c29a81af2b62737c72764612d782901c3f2eb4d7" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;a href=&quot;torch.nn.dropout#torch.nn.Dropout&quot;&gt;&lt;code&gt;Dropout&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a56c6998fe86297bbe85815fb835af9e00568e26" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;code&gt;Dropout&lt;/code&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f8efe32a81509d5ad4a6e1e586b778004252c39" translate="yes" xml:space="preserve">
          <source>This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16cf55bdf2b28322a9b3129eb62ae4e3fc4f60f8" translate="yes" xml:space="preserve">
          <source>This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 1111. It uses direction numbers to generate these sequences, and these numbers have been adapted from &lt;a href=&quot;https://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111&quot;&gt;here&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5d17292f2576cd2e36da2cca0252f9780e064b5" translate="yes" xml:space="preserve">
          <source>This invariant is maintained throughout &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; class, and all functions that construct a &lt;code&gt;:class:PackedSequence&lt;/code&gt; in PyTorch (i.e., they only pass in tensors conforming to this constraint).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0dcba97fb0267a928459453464ca8f56762a0535" translate="yes" xml:space="preserve">
          <source>This is TorchScript&amp;rsquo;s compilation of the code for the &lt;code&gt;forward&lt;/code&gt; method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78aa499d132d9b73782bab5e2924798d784eafb1" translate="yes" xml:space="preserve">
          <source>This is a &lt;strong&gt;Prototype&lt;/strong&gt; function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="554d0b95763033953c158f9820e38365e62ff2d9" translate="yes" xml:space="preserve">
          <source>This is a generalized version of &lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt;&lt;code&gt;torch.hann_window()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27da13545c38e0ad916bfb253add6c85dbfec69d" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="baa9ff1557d00c12989b3535b16426973504c66e" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in &lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACK documentation for geqrf&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bed0909b657a7fcd560ee9c527be9120da241148" translate="yes" xml:space="preserve">
          <source>This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt;, which checks for contiguity, or &lt;a href=&quot;#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, which copies data if needed. To change the size in-place with custom strides, see &lt;a href=&quot;#torch.Tensor.set_&quot;&gt;&lt;code&gt;set_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3b5e88eb67e346fdbdf0577eada21e1b383ed17" translate="yes" xml:space="preserve">
          <source>This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5b9f918ec25aa03605059dcc957950d2df96994" translate="yes" xml:space="preserve">
          <source>This is a no-op if the tensor is already of the correct type. This is equivalent to &lt;code&gt;self.type(tensor.type())&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be9417a8a097120c57731208ecd0b2e0d9e0a45b" translate="yes" xml:space="preserve">
          <source>This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="584f2eb806dc59e0bb28f52080bdd509e138734b" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3187ce0a4f646a0f02690921fbe6d7114d95c7e3" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a2d1424088d04fbdd7c029c2c5a864c5f132531" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee8eabed86139ec26051c16d9d19973c0fd6b09c" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12efc861b006501b2b4700d386ad4f25b9ff0f7e" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98a705e3678fe480b570b51f1f965a2f3d037e83" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd838137a8dd7616f8151e3b75e72da4794d6fa0" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57134b77fa81442b10e02b41eaf3243756f7f5ee" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist. If all values in a reduced row are &lt;code&gt;NaN&lt;/code&gt; then the quantiles for that reduction will be &lt;code&gt;NaN&lt;/code&gt;. See the documentation for &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55c07d098e34194632b960dfc4bd298727835fac" translate="yes" xml:space="preserve">
          <source>This is always &lt;code&gt;True&lt;/code&gt; for CUDA tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16c88bb46cbf5ef3f448ca4fe755467e88b5e578" translate="yes" xml:space="preserve">
          <source>This is different from &lt;a href=&quot;../tensors#torch.Tensor.repeat&quot;&gt;&lt;code&gt;torch.Tensor.repeat()&lt;/code&gt;&lt;/a&gt; but similar to &lt;code&gt;numpy.repeat&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db053cd2631236023d48272222af395e026941e8" translate="yes" xml:space="preserve">
          <source>This is equivalent to &lt;code&gt;self.log_pob(input).argmax(dim=1)&lt;/code&gt;, but is more efficient in some cases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="374b7dab14fec694b4f5430f4c9e29dd3262cee0" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_2d#torch.atleast_2d&quot;&gt;&lt;code&gt;torch.atleast_2d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6ef108509ff368b6991089b8608a37aca821f0d" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5b9e7c481799905ba918331b66fdb7b66a8da08" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_3d#torch.atleast_3d&quot;&gt;&lt;code&gt;torch.atleast_3d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c60beaf37836d9a93c155f39fdfc6ecdf807a30e" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6810c532f9495d317b56fc8164ab42f154df6d1" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;torch.nn.module#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18f227b9fbcb58581a48e62cc2b1e4b0a1736273" translate="yes" xml:space="preserve">
          <source>This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model &lt;em&gt;before&lt;/em&gt; saving it ensures that the tracer has the correct device information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61139b0235e460eab7709588bf0db5fef3c1ed33" translate="yes" xml:space="preserve">
          <source>This is supported for &lt;a href=&quot;#module-attributes&quot;&gt;module attributes&lt;/a&gt; class attribute annotations but not for functions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d93a3f447e3870288983d617bf3d5ca0babcaca" translate="yes" xml:space="preserve">
          <source>This is the default method, meaning that &lt;code&gt;init_method&lt;/code&gt; does not have to be specified (or can be &lt;code&gt;env://&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b58bf6260a34528b9c54af5a74ed650426bd462d" translate="yes" xml:space="preserve">
          <source>This is the functional version of the DataParallel module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f2aa90c9ddd953d8134c3b2013f0dd5a5e04793" translate="yes" xml:space="preserve">
          <source>This is the quantized equivalent of &lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt;&lt;code&gt;ELU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="051cad7dbbb9111a95c9efe4f04cfeb331282a9e" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea4fe276ddec4ebc483841a1170deaf100cb0956" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt;&lt;code&gt;BatchNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5ce0ce0859be0cbdcd66ae4d96e1834e80fb440" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.groupnorm#torch.nn.GroupNorm&quot;&gt;&lt;code&gt;GroupNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fd9da5ac2b1219bc045ac6f1d4efc7962f29b18" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt;&lt;code&gt;Hardswish&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81ee4b5a559a55d776be5e5ad40fea15e08c77c3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt;&lt;code&gt;InstanceNorm1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="306614a03d85542a417f30827501dbc097b0e4e2" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt;&lt;code&gt;InstanceNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c54c49dfa0b1be230f77c6acfc8832f75ff793b" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt;&lt;code&gt;InstanceNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ab22dac0481c7e9d72041caf66a8d6e6bce24a3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt;&lt;code&gt;LayerNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c1757bb112d3facda0c6b8d39d0c0a08d49265a" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;nn.functional#torch.nn.functional.hardswish&quot;&gt;&lt;code&gt;hardswish()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6493b5bad0951f0b43e4846b14cd1a429b62cc1" translate="yes" xml:space="preserve">
          <source>This is the reverse operation of the manner described in &lt;a href=&quot;#torch.Tensor.gather&quot;&gt;&lt;code&gt;gather()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5295faab37629f799f6e6fbb1e9825a33f998b68" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;torch.max()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19c006dca5dfcf6b24613e9eb4ec7d4564e3ffde" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;torch.min()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea4ca9f34dc42157c431ad05a09921e13d264890" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.sort#torch.sort&quot;&gt;&lt;code&gt;torch.sort()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4162fd55661c8ae3715301b7e45fb9d304f83a8" translate="yes" xml:space="preserve">
          <source>This is typically passed to an optimizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8881d58b1f068203ff9ef6d05a7b8193f5aad690" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc05416a2fcfa2651cf566b1804c2fef547cfa8c" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdf0d415e18e8e03494fb14369b3744608b2dde4" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="387b12018432b2012dd14d16de801f3f2b4ba6be" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18ae6606a0b9fa5a1d8375a29d5adf1260f68aa6" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44bcfdd8636d4fef63e74c75c8c5e2e8049a6723" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets &lt;code&gt;t[i]&lt;/code&gt; should be numbers between 0 and 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82ff1110b5f2f18a81804af99bb7a1033487ea4f" translate="yes" xml:space="preserve">
          <source>This is useful for implementing efficient sub-pixel convolution with a stride of</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b3e0c96ed0785872533b5ba8bff1ae227ce271e" translate="yes" xml:space="preserve">
          <source>This layer uses statistics computed from input data in both training and evaluation modes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e61f8043e790e86849febfe55d39af2c5aac3218" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="940b923ef28b8c58cfd52fc019408d07c1f8a33f" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class. This version is more numerically stable than using a plain &lt;code&gt;Sigmoid&lt;/code&gt; followed by a &lt;code&gt;BCELoss&lt;/code&gt; as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b85bf08f0c04a7b3a3773bc9f4c6784cf3720ebd" translate="yes" xml:space="preserve">
          <source>This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version &amp;lt; 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b5f23050e1225367270ed2f80d8587f70aa2118" translate="yes" xml:space="preserve">
          <source>This message indicates to us that the computation differed between when we first traced it and when we traced it with the &lt;code&gt;check_inputs&lt;/code&gt;. Indeed, the loop within the body of &lt;code&gt;loop_in_traced_fn&lt;/code&gt; depends on the shape of the input &lt;code&gt;x&lt;/code&gt;, and thus when we try another &lt;code&gt;x&lt;/code&gt; with a different shape, the trace differs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b6071e9753ecc5843a463c244f78990925791d4" translate="yes" xml:space="preserve">
          <source>This method assumes that the file system supports locking using &lt;code&gt;fcntl&lt;/code&gt; - most local systems and NFS support it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56fec11901471926f8e8051da013976d59b168bf" translate="yes" xml:space="preserve">
          <source>This method can only be called on a coalesced sparse tensor. See &lt;code&gt;Tensor.coalesce()&lt;/code&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab188eb8949a2274bed5b2cb6c64d4adfb8be156" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b6b6de7677cc3f29ad4b4bb71322ca8119f722c" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex inverse discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07d469ff88f699ae509e6d44562a14e49142eb33" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-real inverse discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0df83d3e836abc896e3d3ef88cddff7fe41858c" translate="yes" xml:space="preserve">
          <source>This method computes the real-to-complex discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdf0174c78d429d9d5f3575a0eab21bb4e7cb40a" translate="yes" xml:space="preserve">
          <source>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca8069362a7e2b0181c1b2a91e97eec6d51986a6" translate="yes" xml:space="preserve">
          <source>This method is implemented using the Singular Value Decomposition.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a51d9e13859426129477705a920dac6fd0c48f90" translate="yes" xml:space="preserve">
          <source>This method modifies the module in-place.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44d70c89b433cfca3df93bd2dca81609164685a8" translate="yes" xml:space="preserve">
          <source>This method sets the parameters&amp;rsquo; &lt;code&gt;requires_grad&lt;/code&gt; attributes in-place.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba184c175718a9033afc302007ca57b3b2566fe2" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D complex-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with last dimension of size 2, representing the real and imaginary components of complex numbers, and should have at least &lt;code&gt;signal_ndim + 1&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d0394dd315af559db4d9ad47ffb52f54e5c2212" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D real-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with at least &lt;code&gt;signal_ndim&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75583911adf2f93140c081540537180003c155f0" translate="yes" xml:space="preserve">
          <source>This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36d7188a05dab83ac9defa225d250cd47190ba2f" translate="yes" xml:space="preserve">
          <source>This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57da4e0220bd8e9709ab47673f34882d1449aae9" translate="yes" xml:space="preserve">
          <source>This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX. Exported falls through and exports the operator as is, as custom op. Exporting custom operators enables users to register and implement the operator as part of their runtime backend.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da76922486c33377aa2791af6603164e435bb6ab" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9d848a6942846852bedaf5aea8e5e5bd3995b73" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as regular ONNX operators. This is the default &lt;code&gt;operator_export_type&lt;/code&gt; mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15b7320d744a4575a9e6699705096be13d718c1b" translate="yes" xml:space="preserve">
          <source>This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose &lt;code&gt;torch.memory_format&lt;/code&gt; is &lt;code&gt;torch.contiguous_format&lt;/code&gt; and others whose format is &lt;code&gt;torch.channels_last&lt;/code&gt;. However, corresponding parameters in different processes must have the same strides.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db9962266fa6ecf5ecb4337d45f21434962d46f1" translate="yes" xml:space="preserve">
          <source>This module also contains any parameters that the original module had as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5790600450f4acb47de56be8f01d6569e2e765da" translate="yes" xml:space="preserve">
          <source>This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of &lt;code&gt;fp16&lt;/code&gt; and &lt;code&gt;fp32&lt;/code&gt;, the gradient reduction on these mixed types of parameters will just work fine.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaad9c62985785aa5e132959be1bf299d5f15d45" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56c78b422ae4f13f13cfea0f3331ef6b5469addb" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient &lt;code&gt;allreduce&lt;/code&gt; following the reverse order of the registered parameters of the model. In other words, it is users&amp;rsquo; responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2fc61472d65b92ddda57c12782f0c49ee43ce07" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51bd62541eadcc8d3cc881058d8a71b8bed71b7f" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03c2d1d66e7773d2c26345cff96e3404e927c5ec" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a37a41f99ccf18be289e494dc942de371c38958" translate="yes" xml:space="preserve">
          <source>This module currently does not support custom distributed collective operations in the forward pass, such as &lt;code&gt;SyncBatchNorm&lt;/code&gt; or other custom defined collectives in the model&amp;rsquo;s forward pass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4e13a3367c83e826e07b89f6231cff61b5213ab" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use &lt;code&gt;LogSoftmax&lt;/code&gt; instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b724d7ccdd2d4a2fad1d30f042a783b90f3e1b70" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work with &lt;a href=&quot;../autograd#torch.autograd.grad&quot;&gt;&lt;code&gt;torch.autograd.grad()&lt;/code&gt;&lt;/a&gt; (i.e. it will only work if gradients are to be accumulated in &lt;code&gt;.grad&lt;/code&gt; attributes of parameters).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b31e8020fcc76734bdb0706e9d94b6fab41f09de" translate="yes" xml:space="preserve">
          <source>This module implements the combined (fused) modules conv + relu which can be then quantized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5f06261e4042915626c1433e6142f8f0b72218a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized implementations of fused operations like conv + relu.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe84855e7e341d93fbb671f10f878fe740d68b6a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized versions of the nn layers such as ~`torch.nn.Conv2d` and &lt;code&gt;torch.nn.ReLU&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cc16ec4495bf63da75c9357c640897793fcbd84" translate="yes" xml:space="preserve">
          <source>This module implements the versions of those fused operations needed for quantization aware training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd564c5f343a0b5d3e2281d9491b97ab89a390d6" translate="yes" xml:space="preserve">
          <source>This module implements versions of the key nn modules &lt;strong&gt;Conv2d()&lt;/strong&gt; and &lt;strong&gt;Linear()&lt;/strong&gt; which run in FP32 but with rounding applied to simulate the effect of INT8 quantization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d28957208d801ad4a8275f10517753872c57b97" translate="yes" xml:space="preserve">
          <source>This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a56a23f8f5587064fad32f9351698184c9934864" translate="yes" xml:space="preserve">
          <source>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c517186ab821b45b2c9090ee2862c5050c7fd67" translate="yes" xml:space="preserve">
          <source>This module provides an RPC-based distributed autograd framework that can be used for applications such as model parallel training. In short, applications may send and receive gradient recording tensors over RPC. In the forward pass, we record when gradient recording tensors are sent over RPC and during the backward pass we use this information to perform a distributed backward pass using RPC. For more details see &lt;a href=&quot;rpc/distributed_autograd#distributed-autograd-design&quot;&gt;Distributed Autograd Design&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87af39490036eebaa3381d1990a65c31f6408fae" translate="yes" xml:space="preserve">
          <source>This module returns a &lt;code&gt;NamedTuple&lt;/code&gt; with &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;loss&lt;/code&gt; fields. See further documentation for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59e9905faed8adfff4eeb0ecdbe57c1cac83f3a4" translate="yes" xml:space="preserve">
          <source>This module supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9c9e1a196eef3e1f967008237f4372d9edc34d7" translate="yes" xml:space="preserve">
          <source>This module works only with the multi-process, single-device usage of &lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt;, which means that a single process works on a single GPU.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31368b5a919564a12c569127df416d6acb1973be" translate="yes" xml:space="preserve">
          <source>This op should be disambiguated with &lt;a href=&quot;torch.logsumexp#torch.logsumexp&quot;&gt;&lt;code&gt;torch.logsumexp()&lt;/code&gt;&lt;/a&gt; which performs a reduction on a single tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3256e874629b9e3bf7609aa4774907058a898427" translate="yes" xml:space="preserve">
          <source>This operation is not differentiable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f253d72c955897e96cbe7e655ade3b1e375b9508" translate="yes" xml:space="preserve">
          <source>This operation is useful for explicit broadcasting by names (see examples).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be6c030e3433204ea07b32a8e6ce714fff330e3a" translate="yes" xml:space="preserve">
          <source>This operator supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="836145aa5caebff4eaffcf9dc94e2f473ff1df15" translate="yes" xml:space="preserve">
          <source>This package provides a &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects. Currently, the &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type is primarily used by the &lt;a href=&quot;rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="510d5659f81fbb2c92add3848324ac3e769287d8" translate="yes" xml:space="preserve">
          <source>This requires &lt;code&gt;scipy&lt;/code&gt; to be installed</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d1f42ea5265cdd17159cef860e2867af1a5fed2" translate="yes" xml:space="preserve">
          <source>This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="792e85a6083fd7bff7900d359e6ad50e4d63f9a7" translate="yes" xml:space="preserve">
          <source>This subset is restricted:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f610c707a33d251ea07e96805fc742b6f94fb773" translate="yes" xml:space="preserve">
          <source>This will call &lt;a href=&quot;optim#torch.optim.Optimizer.step&quot;&gt;&lt;code&gt;torch.optim.Optimizer.step()&lt;/code&gt;&lt;/a&gt; on each worker containing parameters to be optimized, and will block until all workers return. The provided &lt;code&gt;context_id&lt;/code&gt; will be used to retrieve the corresponding &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;context&lt;/code&gt;&lt;/a&gt; that contains the gradients that should be applied to the parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c51f7b72279e26fd7cf66d9d61e7f7204bf3b26a" translate="yes" xml:space="preserve">
          <source>Threshold</source>
          <target state="translated">Threshold</target>
        </trans-unit>
        <trans-unit id="4c8543e85c70d9042806658de48a8eae05e8c630" translate="yes" xml:space="preserve">
          <source>Threshold is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2ed3cfe5a92d0e9667b7a33a4666b01b859c7a8" translate="yes" xml:space="preserve">
          <source>Thresholds each element of the input Tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="880f2f41d4783f36e126ec98db6135c252954795" translate="yes" xml:space="preserve">
          <source>To achieve this, developers need to touch the source code of PyTorch. Please follow the &lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot;&gt;instructions&lt;/a&gt; for installing PyTorch from source. If the wanted operator is standardized in ONNX, it should be easy to add support for exporting such operator (adding a symbolic function for the operator). To confirm whether the operator is standardized or not, please check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5aa3a35df66cb08f05248ae2964bf5546e98afe" translate="yes" xml:space="preserve">
          <source>To align a tensor to a specific order, use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42a1cb8b698578c9fe48bbb02d735034e28323f6" translate="yes" xml:space="preserve">
          <source>To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please avoid use of &lt;code&gt;torch.Tensor.item()&lt;/code&gt;. Torch supports implicit cast of single-element tensors to numbers. E.g.:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e33cac23c601d0b5ef35b2617bd1e0b84d5ac6c7" translate="yes" xml:space="preserve">
          <source>To be able to save a module, it must not make any calls to native Python functions. This means that all submodules must be subclasses of &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="517547cd8d7cdf6793257301cea557fd580f4509" translate="yes" xml:space="preserve">
          <source>To change an existing tensor&amp;rsquo;s &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; and/or &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, consider using &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt; method on the tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cccb194237ee7a6be8a8fca8580f1ea27dde7026" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; (and recursively compile anything it calls), add the &lt;a href=&quot;../jit#torch.jit.export&quot;&gt;&lt;code&gt;@torch.jit.export&lt;/code&gt;&lt;/a&gt; decorator to the method. To opt out of compilation use &lt;a href=&quot;torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13132861c718e28cce4e074280b9a030df11d21a" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; that is not called from &lt;code&gt;forward&lt;/code&gt;, add &lt;code&gt;@torch.jit.export&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f8e9d3b1e59225987eeef2681c83b1bcdde57d0" translate="yes" xml:space="preserve">
          <source>To compile the sources, the default system compiler (&lt;code&gt;c++&lt;/code&gt;) is used, which can be overridden by setting the &lt;code&gt;CXX&lt;/code&gt; environment variable. To pass additional arguments to the compilation process, &lt;code&gt;extra_cflags&lt;/code&gt; or &lt;code&gt;extra_ldflags&lt;/code&gt; can be provided. For example, to compile your extension with optimizations, pass &lt;code&gt;extra_cflags=['-O3']&lt;/code&gt;. You can also use &lt;code&gt;extra_cflags&lt;/code&gt; to pass further include directories.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e66cd2fec20ece4ce742fe906879dd31b4bc3e7a" translate="yes" xml:space="preserve">
          <source>To compute log-probabilities for all classes, the &lt;code&gt;log_prob&lt;/code&gt; method can be used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bcee88acc8b939fa9c28961779b7b64da43214ae" translate="yes" xml:space="preserve">
          <source>To create a tensor with pre-existing data, use &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="427cce7803261872f64a384400396e0d1af0bda9" translate="yes" xml:space="preserve">
          <source>To create a tensor with similar type but different size as another tensor, use &lt;code&gt;tensor.new_*&lt;/code&gt; creation ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed5a59600ac4cfd9c95f09892fb740465f6d0959" translate="yes" xml:space="preserve">
          <source>To create a tensor with specific size, use &lt;code&gt;torch.*&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a84b4146d0aefbfce9733ab7943da8a8972fc9bd" translate="yes" xml:space="preserve">
          <source>To create a tensor with the same size (and similar types) as another tensor, use &lt;code&gt;torch.*_like&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e323c9fb04b24bbe21e136c6130c8b1036dfb2e1" translate="yes" xml:space="preserve">
          <source>To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (&amp;ldquo;CPU total time is much greater than CUDA total time&amp;rdquo;). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d6487001340d96dfbd24e0dfe67d91d448bd378" translate="yes" xml:space="preserve">
          <source>To enable &lt;code&gt;backend == Backend.MPI&lt;/code&gt;, PyTorch needs to be built from source on a system that supports MPI.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="928a8a1dba70cf38d7b64d41e9f2de79497ca1b2" translate="yes" xml:space="preserve">
          <source>To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a &lt;code&gt;Future&lt;/code&gt; object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with &lt;code&gt;@staticmethod&lt;/code&gt; or &lt;code&gt;@classmethod&lt;/code&gt;, &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16234789078f98ba40aad60005ffa95f2e9e8ac6" translate="yes" xml:space="preserve">
          <source>To ensure that the correct number of threads is used, set_num_threads must be called before running eager, JIT or autograd code.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cd4366390e742467d09cd376578959df8585e90" translate="yes" xml:space="preserve">
          <source>To export a raw ir.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13a45cf8b5a671e88848c78052e09b8b9f6c109f" translate="yes" xml:space="preserve">
          <source>To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly. In the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64ba89c98e2ef72ba75fa028267bbaa3251bdd0e" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a complex data type, the property &lt;a href=&quot;generated/torch.is_complex#torch.is_complex&quot;&gt;&lt;code&gt;is_complex&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a complex data type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4bde254844ec10d73414a4f9097cbf8e8d5d1c09" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a floating point data type, the property &lt;a href=&quot;generated/torch.is_floating_point#torch.is_floating_point&quot;&gt;&lt;code&gt;is_floating_point&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a floating point data type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48e44e1c5bae8e339aa29cc00850c62cff2a0ea7" translate="yes" xml:space="preserve">
          <source>To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It&amp;rsquo;s also helpful to include a minimal working example.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e370e4d333aa52e5158136385c8ce683abfc7e1" translate="yes" xml:space="preserve">
          <source>To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f1500bbb514e4eeb3d360549b7085361d0903f1" translate="yes" xml:space="preserve">
          <source>To look up what optional arguments this module offers:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="efe5a1ff1c27c51217ed9e88d6d14c167e44ae98" translate="yes" xml:space="preserve">
          <source>To make it easier to understand, here is a small example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4861c71c5547ba6da8095cee191825540e015436" translate="yes" xml:space="preserve">
          <source>To make writing TorchScript more convenient, we allow script code to refer to Python values in the surrounding scope. For instance, any time there is a reference to &lt;code&gt;torch&lt;/code&gt;, the TorchScript compiler is actually resolving it to the &lt;code&gt;torch&lt;/code&gt; Python module when the function is declared. These Python values are not a first class part of TorchScript. Instead they are de-sugared at compile-time into the primitive types that TorchScript supports. This depends on the dynamic type of the Python valued referenced when compilation occurs. This section describes the rules that are used when accessing Python values in TorchScript.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56b4b291b8193f650ab440b4d0108041657026e3" translate="yes" xml:space="preserve">
          <source>To obtain repeatable results, reset the seed for the pseudorandom number generator</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdaaaf6ab8e173c9d7ceb41b8dea6cb46e09c870" translate="yes" xml:space="preserve">
          <source>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff43c0b33f5574b95fded09d54e409788745cf85" translate="yes" xml:space="preserve">
          <source>To run the exported script with &lt;a href=&quot;https://caffe2.ai/&quot;&gt;caffe2&lt;/a&gt;, you will need to install &lt;code&gt;caffe2&lt;/code&gt;: If you don&amp;rsquo;t have one already, Please &lt;a href=&quot;https://caffe2.ai/docs/getting-started.html&quot;&gt;follow the install instructions&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c62b3cc0b531750df7ee34e20beaf808665f78f8" translate="yes" xml:space="preserve">
          <source>To specify the scale, it takes either the &lt;code&gt;size&lt;/code&gt; or the &lt;code&gt;scale_factor&lt;/code&gt; as it&amp;rsquo;s constructor argument.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bc8fb9e3000235214e485d36fb9186012476b04" translate="yes" xml:space="preserve">
          <source>To stop the compiler from compiling a method, add &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;@ignore&lt;/code&gt; leaves the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83e075942db53d26dad4b0cea801b6df02d892d5" translate="yes" xml:space="preserve">
          <source>To take a batch diagonal, pass in dim1=-2, dim2=-1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06d7e723aad8726de16a32b689408fd5f6a39bff" translate="yes" xml:space="preserve">
          <source>To trace a specific method on a module, see &lt;a href=&quot;generated/torch.jit.trace_module#torch.jit.trace_module&quot;&gt;&lt;code&gt;torch.jit.trace_module&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cbbf69a127d5627b468752b233e45c932bef649" translate="yes" xml:space="preserve">
          <source>To use &lt;code&gt;DistributedDataParallel&lt;/code&gt; on a host with N GPUs, you should spawn up &lt;code&gt;N&lt;/code&gt; processes, ensuring that each process exclusively works on a single GPU from 0 to N-1. This can be done by either setting &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; for every process or by calling:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55e1a9559362bb4b36c706d26afd5c7744bfa4b4" translate="yes" xml:space="preserve">
          <source>To use a &lt;code&gt;nn.ModuleList&lt;/code&gt; inside a compiled method, it must be marked constant by adding the name of the attribute to the &lt;code&gt;__constants__&lt;/code&gt; list for the type. For loops over a &lt;code&gt;nn.ModuleList&lt;/code&gt; will unroll the body of the loop at compile time, with each member of the constant module list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="527375a7ef1d0e262542dedd5c0253dc05250f0d" translate="yes" xml:space="preserve">
          <source>To use these functions the torch.fft module must be imported since its name conflicts with the &lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43427ba5c0a00d9ec53c4149e3f5de362381fec9" translate="yes" xml:space="preserve">
          <source>To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6239832323d9bb66e30c43f099010c4cf674fbce" translate="yes" xml:space="preserve">
          <source>To utilize &lt;em&gt;script-based&lt;/em&gt; exporter for capturing the dynamic loop, we can write the loop in script, and call it from the regular nn.Module:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="111ccaae56ca75ec4b6793420aa1331efeccd7da" translate="yes" xml:space="preserve">
          <source>Top-1 error</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1240994c38def01302ee353c1f3e6ab783c6922" translate="yes" xml:space="preserve">
          <source>Top-5 error</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecebe7e89f3537ec9d48a23fefb140aef1ac7e73" translate="yes" xml:space="preserve">
          <source>Torch defines 10 tensor types with CPU and GPU variants which are as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2270c240add89c3f53429a527826adf7f8c68848" translate="yes" xml:space="preserve">
          <source>Torch hub works by importing the package as if it was installed. There&amp;rsquo;re some side effects introduced by importing in Python. For example, you can see new items in Python caches &lt;code&gt;sys.modules&lt;/code&gt; and &lt;code&gt;sys.path_importer_cache&lt;/code&gt; which is normal Python behavior.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e678d1ba0a8c71917974dd6b809d475b07d566ed" translate="yes" xml:space="preserve">
          <source>Torch mobile supports &lt;code&gt;torch.mobile_optimizer.optimize_for_mobile&lt;/code&gt; utility to run a list of optimization pass with modules in eval mode. The method takes the following parameters: a torch.jit.ScriptModule object, a blacklisting optimization set and a preserved method list</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03340697e5da4f35bf9335b934c01e6bb269d07d" translate="yes" xml:space="preserve">
          <source>Torch supports sparse tensors in COO(rdinate) format, which can efficiently store and process tensors for which the majority of elements are zeros.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="efa8e2bf58145beaa70cc1139b800b0cc73441ae" translate="yes" xml:space="preserve">
          <source>TorchElastic</source>
          <target state="translated">TorchElastic</target>
        </trans-unit>
        <trans-unit id="3b90040dd3c7ea550e3ae7ebb31cbbf38c50d775" translate="yes" xml:space="preserve">
          <source>TorchScript</source>
          <target state="translated">TorchScript</target>
        </trans-unit>
        <trans-unit id="604c8e5563e16356b9e1a864727a3495aaa4b8f6" translate="yes" xml:space="preserve">
          <source>TorchScript Classes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d73d728706f6b5bfb8bc6488f8b896532317b47" translate="yes" xml:space="preserve">
          <source>TorchScript Enums</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d3d498afcd6dbfb2767a5e73bfdf3c641826801" translate="yes" xml:space="preserve">
          <source>TorchScript Language</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="655dca0ad6a50c42b950785617112aa26c2b7ea8" translate="yes" xml:space="preserve">
          <source>TorchScript Language Reference</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ca55dbce27b090f25e1beef5f42bc0b74a3e196" translate="yes" xml:space="preserve">
          <source>TorchScript Unsupported Pytorch Constructs</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f37751da08a275fc84f6719af858bb6fca4efcaa" translate="yes" xml:space="preserve">
          <source>TorchScript also has a representation at a lower level than the code pretty- printer, in the form of IR graphs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f6a6dd855f78002eaa186408d37b5a14d38fb6e" translate="yes" xml:space="preserve">
          <source>TorchScript also provides a way to use constants that are defined in Python. These can be used to hard-code hyper-parameters into the function, or to define universal constants. There are two ways of specifying that a Python value should be treated as a constant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="252b708f15ebb50ba55ac0b5f321607abb84f702" translate="yes" xml:space="preserve">
          <source>TorchScript can call Python functions. This functionality is very useful when incrementally converting a model to TorchScript. The model can be moved function-by-function to TorchScript, leaving calls to Python functions in place. This way you can incrementally check the correctness of the model as you go.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e634a60c8e32dbe73734d8f74e5fc62a8bbd892" translate="yes" xml:space="preserve">
          <source>TorchScript can lookup attributes on modules. &lt;code&gt;Builtin functions&lt;/code&gt; like &lt;code&gt;torch.add&lt;/code&gt; are accessed this way. This allows TorchScript to call functions defined in other modules.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05a14206c4e373e5e12c57e024877cf642631ecf" translate="yes" xml:space="preserve">
          <source>TorchScript class support is experimental. Currently it is best suited for simple record-like types (think a &lt;code&gt;NamedTuple&lt;/code&gt; with methods attached).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40d12a53eedc122f7c1c26cc26562fe87abc6002" translate="yes" xml:space="preserve">
          <source>TorchScript classes are statically typed. Members can only be declared by assigning to self in the &lt;code&gt;__init__()&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fd4fa394dbd8f52548eb83a0ab865caf3d46d44" translate="yes" xml:space="preserve">
          <source>TorchScript does not support &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#bytes&quot;&gt;&lt;code&gt;bytes&lt;/code&gt;&lt;/a&gt; so this type is not used</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95d83676fd42ab283c45a040ec05adf9e17c4b2a" translate="yes" xml:space="preserve">
          <source>TorchScript does not support all features and types of the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module. Some of these are more fundamental things that are unlikely to be added in the future while others may be added if there is enough user demand to make it a priority.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="155bc1507dbbed5e44c922d48bd933f969683779" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python that can either be written directly (using the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; decorator) or generated automatically from Python code via tracing. When using tracing, code is automatically converted into this subset of Python by recording only the actual operators on tensors and simply executing and discarding the other surrounding Python code.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e3be4ae1ba105244947e696b16b923455648d05" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full &lt;a href=&quot;jit_language_reference#language-reference&quot;&gt;TorchScript Language Reference&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b80f73ada7844bf2ed31f70d01150d3e0f521094" translate="yes" xml:space="preserve">
          <source>TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3730c4db0b9ee76825d3365cc7e6ee5d81c09f3" translate="yes" xml:space="preserve">
          <source>TorchScript provides a code pretty-printer for all &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; instances. This pretty-printer gives an interpretation of the script method&amp;rsquo;s code as valid Python syntax. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f800d70869c5af16f03f352efad0babdc4a4cee" translate="yes" xml:space="preserve">
          <source>TorchScript support in RPC is a prototype feature and subject to change. Since v1.5.0, &lt;code&gt;torch.distributed.rpc&lt;/code&gt; supports calling TorchScript functions as RPC target functions, and this will help improve parallelism on the callee side as executing TorchScript functions does not require GIL.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d483bee6192edebb2157f5cffc7ebe0cf7bf9b5" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of Python&amp;rsquo;s variable resolution (i.e. scoping) rules. Local variables behave the same as in Python, except for the restriction that a variable must have the same type along all paths through a function. If a variable has a different type on different branches of an if statement, it is an error to use it after the end of the if statement.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="439e6a570c3e92811a193d2bb2eb0c532aac85c2" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the &lt;code&gt;torch&lt;/code&gt; namespace, all functions in &lt;code&gt;torch.nn.functional&lt;/code&gt; and most modules from &lt;code&gt;torch.nn&lt;/code&gt; are supported in TorchScript.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85815975862ed9b632ab9ab2cb6516181e624194" translate="yes" xml:space="preserve">
          <source>TorchScript supports the following types of statements:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="237a371f4a7260a3005de6488e3d8470546b5ec9" translate="yes" xml:space="preserve">
          <source>TorchScript supports the use of most PyTorch functions and many Python built-ins. See &lt;a href=&quot;jit_builtin_functions#builtin-functions&quot;&gt;TorchScript Builtins&lt;/a&gt; for a full reference of supported functions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f626090730ab6e08eee60582294fe0a9f90efe3d" translate="yes" xml:space="preserve">
          <source>TorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee07cfab05a06e606f9327817d635689c5cd9b5d" translate="yes" xml:space="preserve">
          <source>TorchScript will refine the type of a variable of type &lt;code&gt;Optional[T]&lt;/code&gt; when a comparison to &lt;code&gt;None&lt;/code&gt; is made inside the conditional of an if-statement or checked in an &lt;code&gt;assert&lt;/code&gt;. The compiler can reason about multiple &lt;code&gt;None&lt;/code&gt; checks that are combined with &lt;code&gt;and&lt;/code&gt;, &lt;code&gt;or&lt;/code&gt;, and &lt;code&gt;not&lt;/code&gt;. Refinement will also occur for else blocks of if-statements that are not explicitly written.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db3578de30ac281698832393e03259941ecc59a4" translate="yes" xml:space="preserve">
          <source>TorchServe</source>
          <target state="translated">TorchServe</target>
        </trans-unit>
        <trans-unit id="174786ece7d01b50c5a292bbe7c461a354fd217b" translate="yes" xml:space="preserve">
          <source>TorchVision support</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb01e02a87150ef514202bea3337f1bbd7e6a429" translate="yes" xml:space="preserve">
          <source>Total norm of the parameters (viewed as a single vector).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="117b4c950645374477e1f2b2a96517111745fe19" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4cae3f60263770c81b066cebd1a77d6ae0c2a925" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. Tracing is ideal for code that operates only on &lt;code&gt;Tensor&lt;/code&gt;s and lists, dictionaries, and tuples of &lt;code&gt;Tensor&lt;/code&gt;s.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20f26cd87e3024b30b8eeb8b74edbe9cdd81f4b6" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bad49a27d3b91711b9d48661ccd1bb2133d17d3" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. When a module is passed to &lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt;&lt;code&gt;torch.jit.trace&lt;/code&gt;&lt;/a&gt;, only the &lt;code&gt;forward&lt;/code&gt; method is run and traced. With &lt;code&gt;trace_module&lt;/code&gt;, you can specify a dictionary of method names to example inputs to trace (see the &lt;code&gt;inputs&lt;/code&gt;) argument below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d93ef3f27cd83786d560cf0e7159be09c5658df2" translate="yes" xml:space="preserve">
          <source>Traced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fce73bbb5357ac93c61680e5f58be84db277c7b" translate="yes" xml:space="preserve">
          <source>Tracer</source>
          <target state="translated">Tracer</target>
        </trans-unit>
        <trans-unit id="d09bb53da249750b0b3b27f27eabe9dbddadc0ed" translate="yes" xml:space="preserve">
          <source>Tracer Warnings</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf933c3083c5a6cb4cedac1b1e0c22d6553dab1d" translate="yes" xml:space="preserve">
          <source>Tracing Edge Cases</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5153b07a6f011eb8b3585b8b94ddfea62a7a0ce" translate="yes" xml:space="preserve">
          <source>Tracing of control flow that is dependent on inputs (e.g. tensor shapes)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="656eeb39a9810745d15b70b8f4cca3cf416cf8d6" translate="yes" xml:space="preserve">
          <source>Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e793ff03e25bf5dd4f85e642caecdeee487a3a73" translate="yes" xml:space="preserve">
          <source>Tracing only correctly records functions and modules which are not data dependent (e.g., do not have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned &lt;code&gt;ScriptModule&lt;/code&gt; will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1289e51c253ed91b441a90b66b33a5a7cc2af8cf" translate="yes" xml:space="preserve">
          <source>Tracing vs Scripting</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4de8ec680853c82acd4d7ddcd35ef7c092a558c3" translate="yes" xml:space="preserve">
          <source>Tracing will not record any control-flow like if-statements or loops. When this control-flow is constant across your module, this is fine and it often inlines the control-flow decisions. But sometimes the control-flow is actually part of the model itself. For instance, a recurrent network is a loop over the (possibly dynamic) length of an input sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6fe7f5e79177b05f6d251ecb9c162d45455045d" translate="yes" xml:space="preserve">
          <source>Training</source>
          <target state="translated">Training</target>
        </trans-unit>
        <trans-unit id="68c170c0011cf476eed353d994b12887940cfc96" translate="yes" xml:space="preserve">
          <source>Transformer</source>
          <target state="translated">Transformer</target>
        </trans-unit>
        <trans-unit id="6e42fdb55f8e197d60cafca548c1e82579acbea4" translate="yes" xml:space="preserve">
          <source>Transformer Layers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef303bb941bf717e2006e7273f0810b07b78b045" translate="yes" xml:space="preserve">
          <source>TransformerDecoder</source>
          <target state="translated">TransformerDecoder</target>
        </trans-unit>
        <trans-unit id="39490c0e215073daec405a4b72d513bc1f4fb82e" translate="yes" xml:space="preserve">
          <source>TransformerDecoder is a stack of N decoder layers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92f20fa7687824fdbb370a6b475f6eeb6f79194b" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer</source>
          <target state="translated">TransformerDecoderLayer</target>
        </trans-unit>
        <trans-unit id="98ae4e9bf414c9fe8b37e6dbc99426a1c7335c2f" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04a4a3c2fb795f7db70756477e2d518c4a892786" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32d7343edd3b94812b03b1cdf834b1b16cc2a3fc" translate="yes" xml:space="preserve">
          <source>TransformerEncoder</source>
          <target state="translated">TransformerEncoder</target>
        </trans-unit>
        <trans-unit id="933db464a961834a0fb72e3b98e0d537c401c215" translate="yes" xml:space="preserve">
          <source>TransformerEncoder is a stack of N encoder layers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a732f60462f98f99de0f8f387f8c71e6c32aba7" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer</source>
          <target state="translated">TransformerEncoderLayer</target>
        </trans-unit>
        <trans-unit id="7b3ae5e9124dac923ef7ce7bbf1287aa019083f2" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c8dccb3d09a0f9d0d9b17ea9825251d663f04d4" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9cb51a530db6f55c97c4e0720722c1b583dafb7" translate="yes" xml:space="preserve">
          <source>TripletMarginLoss</source>
          <target state="translated">TripletMarginLoss</target>
        </trans-unit>
        <trans-unit id="858db382b48e5cc5988f431f28f1130330c1eaf1" translate="yes" xml:space="preserve">
          <source>TripletMarginWithDistanceLoss</source>
          <target state="translated">TripletMarginWithDistanceLoss</target>
        </trans-unit>
        <trans-unit id="88b33e4e12f75ac8bf792aebde41f1a090f3a612" translate="yes" xml:space="preserve">
          <source>True</source>
          <target state="translated">True</target>
        </trans-unit>
        <trans-unit id="6692b9e66830b65bbf15ff9de3c8b1af8bfbc5eb" translate="yes" xml:space="preserve">
          <source>Tuple Construction</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c809de45543797accbb7ba7433a5877747ac289" translate="yes" xml:space="preserve">
          <source>Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to &lt;code&gt;pack_padded_sequence&lt;/code&gt; or &lt;code&gt;pack_sequence&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="654171647baa6be8557a5d627cf35c7075ebb257" translate="yes" xml:space="preserve">
          <source>Tutorials</source>
          <target state="translated">Tutorials</target>
        </trans-unit>
        <trans-unit id="19fd25590b1f3de52164e96d6a585a4ae3f02132" translate="yes" xml:space="preserve">
          <source>Two names &lt;em&gt;match&lt;/em&gt; if they are equal (string equality) or if at least one is &lt;code&gt;None&lt;/code&gt;. Nones are essentially a special &amp;ldquo;wildcard&amp;rdquo; name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3deb7456519697ecf4eefc455516c969a3681bae" translate="yes" xml:space="preserve">
          <source>Type</source>
          <target state="translated">Type</target>
        </trans-unit>
        <trans-unit id="74a99ad458c9adf9d415d4bdb125da11688a37f7" translate="yes" xml:space="preserve">
          <source>Type Info</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58cf557846d7f65d34e8a92739eef2665164fad4" translate="yes" xml:space="preserve">
          <source>Type aliases</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93b9e289e2842469d001eccf7ad5d79f3c302dc9" translate="yes" xml:space="preserve">
          <source>Types</source>
          <target state="translated">Types</target>
        </trans-unit>
        <trans-unit id="c0d285234eb927c53c2fc1dda528e04061e0d90d" translate="yes" xml:space="preserve">
          <source>Types produced by &lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt;&lt;code&gt;collections.namedtuple&lt;/code&gt;&lt;/a&gt; can be used in TorchScript.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2c7c0caa10a0cca5ea7d69e54018ae0c0389dd6" translate="yes" xml:space="preserve">
          <source>U</source>
          <target state="translated">U</target>
        </trans-unit>
        <trans-unit id="4e5f249d2049283bfab86474c53e19434fabe07e" translate="yes" xml:space="preserve">
          <source>URL specifying how to initialize the process group. Default is &lt;code&gt;env://&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9016840b6ab501762ae42c3bc2d77284127ecd9d" translate="yes" xml:space="preserve">
          <source>Unflatten</source>
          <target state="translated">Unflatten</target>
        </trans-unit>
        <trans-unit id="22753787538a4df3368517dc3a28771d6a6bd1c2" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ffd6987181a5e9a56284c36d90107b63ec0ad279" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape. For use with &lt;code&gt;Sequential&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02d85ea4efaa0d1ca099f19843e9be07e28b954d" translate="yes" xml:space="preserve">
          <source>Unfold</source>
          <target state="translated">Unfold</target>
        </trans-unit>
        <trans-unit id="27c8f884a26740cbb923c350e9fa436fb8314b34" translate="yes" xml:space="preserve">
          <source>Unfortunately, the concrete &lt;code&gt;subset&lt;/code&gt; that was used is lost. For more information see &lt;a href=&quot;https://github.com/pytorch/vision/issues/1439&quot;&gt;this discussion&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/vision/pull/1965&quot;&gt;these experiments&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b69bc99e876e93b97ea0728c1acca97b14bbffef" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand()&lt;/code&gt;&lt;/a&gt;, this function copies the tensor&amp;rsquo;s data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef5392b685d4622304e5dfc150347ec19f0ee0af" translate="yes" xml:space="preserve">
          <source>Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the &lt;code&gt;affine&lt;/code&gt; option, Layer Normalization applies per-element scale and bias with &lt;code&gt;elementwise_affine&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2498c971c849991894b17969e87a3329f48f1d2d" translate="yes" xml:space="preserve">
          <source>Unlike Python, each variable in TorchScript function must have a single static type. This makes it easier to optimize TorchScript functions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d7bdd35d7e8961eff68645ed326f054b5482c52" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5b6f222f736f9cef541160a005848fe4ef77888" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented (however &lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;&gt;&lt;code&gt;typing.Optional&lt;/code&gt;&lt;/a&gt; is supported)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01773bdc2e10d2279820d7115b2522610a09e4f3" translate="yes" xml:space="preserve">
          <source>Unpacks the data and pivots from a LU factorization of a tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7afe79ac1105fe133bbed5120636913c111a929" translate="yes" xml:space="preserve">
          <source>Unsupported Typing Constructs</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2d1fe571301d72d9191816bf953a47985e426ec" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;ModuleDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4eca5c6565cbf1cfd8391f002e24b24767bdf27d" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt;&lt;code&gt;ParameterDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9f4cec65260954e7a90830aa6f73b90e5c8817f" translate="yes" xml:space="preserve">
          <source>Upsample</source>
          <target state="translated">Upsample</target>
        </trans-unit>
        <trans-unit id="0b472c3013e4cd26ac1f5f84d9b57c5b6b4ca455" translate="yes" xml:space="preserve">
          <source>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55dfe2d567c845d264928977c08cd8d377265025" translate="yes" xml:space="preserve">
          <source>Upsamples the input to either the given &lt;code&gt;size&lt;/code&gt; or the given &lt;code&gt;scale_factor&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9b84912595a004dd92417c63046087231bc7d36" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using bilinear upsampling.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a31dc0017d79484c9069d06f3ac78a206e41dbfa" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using nearest neighbours&amp;rsquo; pixel values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fef8a1cac9f9d013e57f5acb6da8a9f36cede8d1" translate="yes" xml:space="preserve">
          <source>UpsamplingBilinear2d</source>
          <target state="translated">UpsamplingBilinear2d</target>
        </trans-unit>
        <trans-unit id="2a7960d23688a71e6707ef6d16f626ed000e779c" translate="yes" xml:space="preserve">
          <source>UpsamplingNearest2d</source>
          <target state="translated">UpsamplingNearest2d</target>
        </trans-unit>
        <trans-unit id="680c6c9f314b68f86ff46d8fdf9a289c7fb0c343" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_as&quot;&gt;&lt;code&gt;align_as()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to align tensor dimensions by name to a specified ordering. This is useful for performing &amp;ldquo;broadcasting by names&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66804d0e4e83078e14bfdbaa37e395675103792c" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to permute large amounts of dimensions without mentioning all of them as in required by &lt;a href=&quot;tensors#torch.Tensor.permute&quot;&gt;&lt;code&gt;permute()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ede9bff88acacf3e23cc669406c39e0d8aba75b8" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.item&quot;&gt;&lt;code&gt;torch.Tensor.item()&lt;/code&gt;&lt;/a&gt; to get a Python number from a tensor containing a single value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e75b6839717cab84ecfa02664778f32a2c198780" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; to access the dimension names of a tensor and &lt;a href=&quot;#torch.Tensor.rename&quot;&gt;&lt;code&gt;rename()&lt;/code&gt;&lt;/a&gt; to rename named dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5d22900f5f0642196ebec69d6c5572874ee7c1b" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;tensors#torch.Tensor.flatten&quot;&gt;&lt;code&gt;flatten()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.Tensor.unflatten&quot;&gt;&lt;code&gt;unflatten()&lt;/code&gt;&lt;/a&gt; to flatten and unflatten dimensions, respectively. These methods are more verbose than &lt;a href=&quot;tensors#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensors#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, but have more semantic meaning to someone reading the code.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="641cc18d21edd9671570b1c6c598d26a3e5396f4" translate="yes" xml:space="preserve">
          <source>Use Gloo, unless you have specific reasons to use MPI.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b4d173cba748d683c0210fec722f5787173d83a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b1bf082bc9bf3ac7914e6187c00d474a91fb73a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it&amp;rsquo;s the only backend that currently supports InfiniBand and GPUDirect.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e1700010ee503b77904278f8d396bfe42da63c7" translate="yes" xml:space="preserve">
          <source>Use external data format</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d0232b01e54b3a2e55b622100adbb68b4edb340" translate="yes" xml:space="preserve">
          <source>Use of Python Values</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73922dca1fc1eb81dc09b31de16429c2bad893e0" translate="yes" xml:space="preserve">
          <source>Use the Gloo backend for distributed &lt;strong&gt;CPU&lt;/strong&gt; training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc3648b93e2309e15d382b23a53006e5ccb31b40" translate="yes" xml:space="preserve">
          <source>Use the NCCL backend for distributed &lt;strong&gt;GPU&lt;/strong&gt; training</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ca66ca0ae6872cefb7cdaba9a1b1d7c4fb946f1" translate="yes" xml:space="preserve">
          <source>Used to infer dtype for python complex numbers. The default complex dtype is set to &lt;code&gt;torch.complex128&lt;/code&gt; if default floating point dtype is &lt;code&gt;torch.float64&lt;/code&gt;, otherwise it&amp;rsquo;s set to &lt;code&gt;torch.complex64&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fac6c061fc6d273aebc0e9ab0c99f04ae81e1097" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Argument</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a20f912ad88b795f826c479ed261f676c30da2f7" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Return Value</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7e43a6c0dede8cc47d9e15900c78d9da28530c0" translate="yes" xml:space="preserve">
          <source>User Share RRef with User</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78d0e495aa5431e6e0a8139c0666804c17a54940" translate="yes" xml:space="preserve">
          <source>User extensions can register their own location tags and tagging and deserialization methods using &lt;code&gt;torch.serialization.register_package()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2628232b96e27d1cfe74641b5e5eef1e4a68d67" translate="yes" xml:space="preserve">
          <source>Users can force a reload by calling &lt;code&gt;hub.load(..., force_reload=True)&lt;/code&gt;. This will delete the existing github folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df3d36e2101db7508ee9c65bba6c79f4162eb560" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;DistributedDataParallel&lt;/code&gt; in conjunction with the &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt; is experimental and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f8c8c4de2dead884cf1cd1b1efe492df15bdbf3" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;torch.jit.trace&lt;/code&gt; and &lt;code&gt;torch.jit.trace_module&lt;/code&gt;, you can turn an existing module or Python function into a TorchScript &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="016141762b3eda50d55b6434765c656b21e2c21e" translate="yes" xml:space="preserve">
          <source>Using GPU tensors as arguments or return values of &lt;code&gt;func&lt;/code&gt; is not supported since we don&amp;rsquo;t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of &lt;code&gt;func&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40f8083d709ef1dfeba3266d73cd40bc2120c4c0" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute matrix norms:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34eeb9d75aa34a25cf342ae4a398eead839c1594" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute vector norms:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="892e242741e1c766ef8440ce7e813adfc429718b" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv2d&lt;/code&gt; modules.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62c50ebd5d624de117b146ade6f9d5575d770fc3" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv3d&lt;/code&gt; modules.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18fdc5ee8b1f8fba8dabaa933373c0483ab7fad7" translate="yes" xml:space="preserve">
          <source>Utilities</source>
          <target state="translated">Utilities</target>
        </trans-unit>
        <trans-unit id="ad96fcc3041d4053b9449ab4a648c6341e2217d7" translate="yes" xml:space="preserve">
          <source>Utility functions in other modules</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c41457151932f1c1abddf366a843f91dd5ea098" translate="yes" xml:space="preserve">
          <source>Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9ee5681d3c59f7541c27a38b67edf46259e187b" translate="yes" xml:space="preserve">
          <source>V</source>
          <target state="translated">V</target>
        </trans-unit>
        <trans-unit id="1eafbd543ea2d8baa3df59414ec62e236836b78c" translate="yes" xml:space="preserve">
          <source>V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses: &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2badedb167ecf8f97a1ffcdd2817401074fa728e" translate="yes" xml:space="preserve">
          <source>VGG</source>
          <target state="translated">VGG</target>
        </trans-unit>
        <trans-unit id="76dc2de0cc7c95272fc67f826f2dc5fa09c8ded2" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) from &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="efac66ad38738b85f5ef46a914f1b2bede87a54e" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ab10add11dcdf5fb182aad6d9f35aa59134c466" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c68abe1f69acd7242c26b04b450c20879de247dc" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7df2c09a84b7ee83b6d8493aa3dccf2e12cc714a" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4979b51da2376b295925fa358ed83201a60846eb" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="734385d469c0b7b16d39310aab3a223999973e9e" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;ldquo;E&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b468de2d615302723a63a19cf8ef07ba2f87d240" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;lsquo;E&amp;rsquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22dca5ed30facb9e021b2a416131b71d9addd5df" translate="yes" xml:space="preserve">
          <source>VGG-11</source>
          <target state="translated">VGG-11</target>
        </trans-unit>
        <trans-unit id="59c5eb8db8735266b63e11d68abfe0513f53e937" translate="yes" xml:space="preserve">
          <source>VGG-11 with batch normalization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="275a0abbe0e323332b36f91fd1e0bbabaa98823b" translate="yes" xml:space="preserve">
          <source>VGG-13</source>
          <target state="translated">VGG-13</target>
        </trans-unit>
        <trans-unit id="00e8568346d410021e9a56f3b1b1f1983ac75021" translate="yes" xml:space="preserve">
          <source>VGG-13 with batch normalization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3fa5851c47708aca43af1f031237ec76652d5fb8" translate="yes" xml:space="preserve">
          <source>VGG-16</source>
          <target state="translated">VGG-16</target>
        </trans-unit>
        <trans-unit id="6794221fe94aa99e89cd4a3fc469cd560c5b1798" translate="yes" xml:space="preserve">
          <source>VGG-16 with batch normalization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e33f8119a3d18ad4bc21aa5913ffff22adbc62fa" translate="yes" xml:space="preserve">
          <source>VGG-19</source>
          <target state="translated">VGG-19</target>
        </trans-unit>
        <trans-unit id="6a06101d542844efc9851734dd33c0a3fcfb9071" translate="yes" xml:space="preserve">
          <source>VGG-19 with batch normalization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08ab4ecc000363002865da057bb07b708354e689" translate="yes" xml:space="preserve">
          <source>Valid operation names:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ab2f6ea14647497320511c9699ad1fa98390d6e" translate="yes" xml:space="preserve">
          <source>Value associated with &lt;code&gt;key&lt;/code&gt; if &lt;code&gt;key&lt;/code&gt; is in the store.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f65e3050b4aa23d2b9ffeafc8ef420283f1bc31" translate="yes" xml:space="preserve">
          <source>Values looked up as attributes of a module are assumed to be constant:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="766a55330fbdeceea3990ed650f19ff9d7ebc2b2" translate="yes" xml:space="preserve">
          <source>Vandermonde matrix. If increasing is False, the first column is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8993fc586517fabcc3d0fce5c92e800dc4e3de15" translate="yes" xml:space="preserve">
          <source>Variable Resolution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac018db1f7b00972061adff843d37497d8ee153c" translate="yes" xml:space="preserve">
          <source>Variables</source>
          <target state="translated">Variables</target>
        </trans-unit>
        <trans-unit id="b1c39119660f33e5be726c1652639e668fcdfcd8" translate="yes" xml:space="preserve">
          <source>Verifies that the given compiler is ABI-compatible with PyTorch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="991eeb7d6acd3a1b90daf8e907ad4a1126fbf67e" translate="yes" xml:space="preserve">
          <source>Via a string and device ordinal:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78f371ae51565c626e223c43a305351baadcdfcb" translate="yes" xml:space="preserve">
          <source>Via a string:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3eb4e2b65c3b6adcd10d477d9e1ded0579505479" translate="yes" xml:space="preserve">
          <source>Video classification</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61ad6c7f7397fbdc6a6513e489917c12a6953f11" translate="yes" xml:space="preserve">
          <source>View this tensor as the same size as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.view_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.view(other.size())&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68b460da28c94fa06be12e6f2242c79c2eca8bd2" translate="yes" xml:space="preserve">
          <source>Vision Layers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cd7ed1352bc6bdd613ec698de479a989aa9357b" translate="yes" xml:space="preserve">
          <source>Vision functions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2415cb7f63df0c9de23362326ad3c37a9adfc96" translate="yes" xml:space="preserve">
          <source>W</source>
          <target state="translated">W</target>
        </trans-unit>
        <trans-unit id="46965f2770b3f862f5982cb5b877918de164026c" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b7347a76af1465d2b26baeed335756321992f36" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06cfb3f342da0b25161bf11f15bf2d81bfb676a7" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="144094cbaa937780b12d5cb8356e1fc4a6c692aa" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23da97705d0e8d5a571139859c1eefc7a25b59f7" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf2c9a7a90367be7032bd4f58614c0c9ad3779f8" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} \times \text{upscale\_factor}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5a97d192d1067a74706f036b87ed2b96b8d5895" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd2204cd0212ab9c7330f7c2e6a0b40cae1ae693" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]} \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="876c8e0e601e7178678d337712548c5cc7269f61" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e0657f1536a7a37c826480069677e9010835607" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a11c0191402c7be2c31a4ea7dcec53e255776ef6" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4191f6bab2de7c1a0f2413fa95cad94e0003ac5" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90b3ee06862d0af7ed835a4f57d3b9839c234435" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02d96942e51e668861c82b6ad6a1888bbaffdfc3" translate="yes" xml:space="preserve">
          <source>Waits for all provided futures to be complete, and returns the list of completed values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a15d8c4d33065db8154fb49b0e2d2a98624af19" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store, and throws an exception if the keys have not been set by the supplied &lt;code&gt;timeout&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24a824e1b1fcd89978ae586133c59626ca034951" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store. If not all keys are set before the &lt;code&gt;timeout&lt;/code&gt; (set during store initialization), then &lt;code&gt;wait&lt;/code&gt; will throw an exception.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9c45563358e813f157ba81b33143542165ba84e" translate="yes" xml:space="preserve">
          <source>Warning</source>
          <target state="translated">Warning</target>
        </trans-unit>
        <trans-unit id="322f5a9cd5158cc9dec2b3e7da850004249c9e4e" translate="yes" xml:space="preserve">
          <source>We accumulate the gradients in the appropriate &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;torch.distributed.autograd.context&lt;/code&gt;&lt;/a&gt; on each of the nodes. The autograd context to be used is looked up given the &lt;code&gt;context_id&lt;/code&gt; that is passed in when &lt;a href=&quot;#torch.distributed.autograd.backward&quot;&gt;&lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt;&lt;/a&gt; is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; API.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db56b28279fe3aea1467a7671b69e8c448f2218c" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt;&lt;code&gt;torch.nn.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f8d51566ad98edb597de3ba663338465830f744" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv2d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e48e5f00566ef4a4b3bcb10eab8d225acffc2e4" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv3d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3fb7d904d476590d98359b7241880b1be2f9a76" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Linear&quot;&gt;&lt;code&gt;torch.nn.quantized.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fb0293b1874dcfe3feb591ccd49a1edaf2d5f3f" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Conv2d&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&quot;&gt;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&lt;/a&gt; for documentation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27cfadeddc9160f1498530cc3eb1f719bfd8434a" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Linear&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt; for documentation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5105f862983064b0cf7594afba8d668c8f56e9ff" translate="yes" xml:space="preserve">
          <source>We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements of a part of a model. Checkout this example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6defebafebb2e25f76bcf411ab1ec455c032f365" translate="yes" xml:space="preserve">
          <source>We also do not support the following subsystems, though some may work out of the box:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3fa9a188f6de578d3bee11e42ad3ca8321060293" translate="yes" xml:space="preserve">
          <source>We can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with &lt;code&gt;torch.cat&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7aac442ad82fddd2074ccf19e4043ee60a7cdfb0" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4304f8e1f80a53c4c4788f88c99ef4dcb6d0bc02" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt;&lt;code&gt;torch.nn.ReLU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94fba3bcda49d77ec84730c68d223d594ab23cb3" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a068c518b2d5e22e14c479569a01c9c9d06bceec" translate="yes" xml:space="preserve">
          <source>We highly recommend taking a look at the original paper for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86ad4838215218ac000a7d87b4b48e01c6961c5f" translate="yes" xml:space="preserve">
          <source>We provide models for action recognition pre-trained on Kinetics-400. They have all been trained with the scripts provided in &lt;code&gt;references/video_classification&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e921c07692e5a21fc62c5f94070c73cf433f0bb7" translate="yes" xml:space="preserve">
          <source>We provide pre-trained models, using the PyTorch &lt;a href=&quot;../model_zoo#module-torch.utils.model_zoo&quot;&gt;&lt;code&gt;torch.utils.model_zoo&lt;/code&gt;&lt;/a&gt;. These can be constructed by passing &lt;code&gt;pretrained=True&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62c9fd377e2aad0f66ac5c26c69e0a5867347f36" translate="yes" xml:space="preserve">
          <source>We provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd73422aa80da87d8ae75185414e27781227d850" translate="yes" xml:space="preserve">
          <source>We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48996658259127412cd98e4a4e7b4a204e6d1b0a" translate="yes" xml:space="preserve">
          <source>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by &lt;code&gt;name&lt;/code&gt; (e.g. &lt;code&gt;'weight'&lt;/code&gt;) with two parameters: one specifying the magnitude (e.g. &lt;code&gt;'weight_g'&lt;/code&gt;) and one specifying the direction (e.g. &lt;code&gt;'weight_v'&lt;/code&gt;). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every &lt;code&gt;forward()&lt;/code&gt; call.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d53879f401fe4a3643952a897298dc95fdaf8d7e" translate="yes" xml:space="preserve">
          <source>Weight:</source>
          <target state="translated">Weight:</target>
        </trans-unit>
        <trans-unit id="d4a914f8dcec6b172849d1c8fb16401fbdbb7604" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when &lt;code&gt;align_corners = False&lt;/code&gt;. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at &lt;code&gt;-1&lt;/code&gt;. From version 1.3.0, under &lt;code&gt;align_corners = True&lt;/code&gt; all grid points along a unit dimension are considered to be at &lt;code&gt;`0&lt;/code&gt; (the center of the input image).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71f1f4c1d858ca7758a6c256f77db87995af50dc" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was &lt;code&gt;align_corners = True&lt;/code&gt;. Since then, the default behavior has been changed to &lt;code&gt;align_corners = False&lt;/code&gt;, in order to bring it in line with the default for &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;interpolate()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19d86416250dfee41099689ccdc474523b9de05a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;compute_uv&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, backward cannot be performed since &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; from the forward pass is required for the backward operation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e150d7511a776c744a811c73ef08944e9bb6434" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;dim&lt;/code&gt; is given, a squeeze operation is done only in the given dimension. If &lt;code&gt;input&lt;/code&gt; is of shape:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1120f7dfd992f23bbea9418ec57a1af2fe9af61" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a scalar value, the operation applied is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66481b64a8fde71d4312aa50639d9ce101db880a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the operation applied is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25f39b6b9d40581f69f611ee72f68c18106be3d1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;exponent&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2903dec45ead9638a2d9f01c40302928711917b1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;groups == in_channels&lt;/code&gt; and &lt;code&gt;out_channels == K * in_channels&lt;/code&gt;, where &lt;code&gt;K&lt;/code&gt; is a positive integer, this operation is also termed in literature as depthwise convolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1b8e1328795e35aa33cb3682bf9e0a128163cf6" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;input&lt;/code&gt; is on CUDA, &lt;a href=&quot;#torch.nonzero&quot;&gt;&lt;code&gt;torch.nonzero()&lt;/code&gt;&lt;/a&gt; causes host-device synchronization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fd8de791261b8fe5e3fd820c55e05ef945d4e88" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;module&lt;/code&gt; returns a scalar (i.e., 0-dimensional tensor) in &lt;code&gt;forward()&lt;/code&gt;, this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14811e48d579473d679dcda9d3180b5365c2423a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shape of &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d6234d9ff0cd3ff058351e570bff6a3e90e8d1e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b23daf961076c6f771f3b98153804afb65cab2e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;size&lt;/code&gt; is given, it is the output size of the image &lt;code&gt;(h, w)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d240a83215b54cebb671916345cdadc2427a5a89" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;some&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, the gradients on &lt;code&gt;U[..., :, min(m, n):]&lt;/code&gt; and &lt;code&gt;V[..., :, min(m, n):]&lt;/code&gt; will be ignored in backward as those vectors can be arbitrary bases of the subspaces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85caf16edf789cb76e0cd678a0b93ec725f0105b" translate="yes" xml:space="preserve">
          <source>When a model is trained on &lt;code&gt;M&lt;/code&gt; nodes with &lt;code&gt;batch=N&lt;/code&gt;, the gradient will be &lt;code&gt;M&lt;/code&gt; times smaller when compared to the same model trained on a single node with &lt;code&gt;batch=M*N&lt;/code&gt; (because the gradients between different nodes are averaged). You should take this into consideration when you want to obtain a mathematically equivalent training process compared to the local training counterpart.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc06fbb059ebdabdac77c068a81b8512376f9ddd" translate="yes" xml:space="preserve">
          <source>When called with &lt;code&gt;dims&lt;/code&gt; of the list form, the given dimensions will be contracted in place of the last</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f0991d5226cadceec16a5e2277a3d90d9a71503" translate="yes" xml:space="preserve">
          <source>When called with a non-negative integer argument &lt;code&gt;dims&lt;/code&gt; =</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6abce014373fbb8ba1a8a6700a958f442096752d" translate="yes" xml:space="preserve">
          <source>When combined with TorchScript decorators, this decorator must be the outmost one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01692078c2de1ab235032349659ea1bad48180a7" translate="yes" xml:space="preserve">
          <source>When combined with static or class method, this decorator must be the inner one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24460dec44dc8e232523a0b3b6cffc083f18ab5b" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.Tensor.new_tensor&quot;&gt;&lt;code&gt;new_tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;tensor.new_tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;tensor.new_tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44313813a50d98d5898f4bddc561c8a3109b1400" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;torch.tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;torch.tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d87a694539e44d9bb0922817749f14565db02a2" translate="yes" xml:space="preserve">
          <source>When drawn without replacement, &lt;code&gt;num_samples&lt;/code&gt; must be lower than number of non-zero elements in &lt;code&gt;input&lt;/code&gt; (or the min number of non-zero elements in each row of &lt;code&gt;input&lt;/code&gt; if it is a matrix).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aeab232209859fdb56ecfdd5e27289109b3779cd" translate="yes" xml:space="preserve">
          <source>When given an image of &lt;code&gt;Channels x Height x Width&lt;/code&gt;, it will apply &lt;code&gt;Softmax&lt;/code&gt; to each location</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9635ed5ff58b5f10f0a861bfe4da315b19a3d9ec" translate="yes" xml:space="preserve">
          <source>When manually importing this backend and invoking &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; with the corresponding backend name, the &lt;code&gt;torch.distributed&lt;/code&gt; package runs on the new backend.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c58952c2db520eb2d0c000e5b5179ffccb22124" translate="yes" xml:space="preserve">
          <source>When passed to the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;torch.jit.script&lt;/code&gt;&lt;/a&gt; function, a &lt;code&gt;torch.nn.Module&lt;/code&gt;&amp;rsquo;s data is copied to a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and the TorchScript compiler compiles the module. The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;, as well as any &lt;code&gt;@torch.jit.export&lt;/code&gt; methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98975218294f0d2cf1d9e027d0c9f18c335eccc0" translate="yes" xml:space="preserve">
          <source>When running on CUDA, &lt;code&gt;row * col&lt;/code&gt; must be less than</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6442e147505eb082b7d915b41a9300ebc0cdaab5" translate="yes" xml:space="preserve">
          <source>When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6134dc991dfe53224afc4b25750c288358d06db2" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;divisor&lt;/code&gt; tensor contains no zero elements, then &lt;code&gt;fold&lt;/code&gt; and &lt;code&gt;unfold&lt;/code&gt; operations are inverses of each other (up to constant divisor).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea57553addc94dbd66c0aab4dca41d371d835f2e" translate="yes" xml:space="preserve">
          <source>When the dtypes of inputs to an arithmetic operation (&lt;code&gt;add&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;) differ, we promote by finding the minimum dtype that satisfies the following rules:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b36d5c15d9cebdd9841a3cbecf930ef5be3e8fb3" translate="yes" xml:space="preserve">
          <source>When the input Tensor is a sparse tensor then the unspecifed values are treated as &lt;code&gt;-inf&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adc8c2e11d80dad257b6a86b35591121148d959c" translate="yes" xml:space="preserve">
          <source>When the shapes do not match, the shape of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; is used as the shape for the returned output tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a635c724cadbffe9cd8e217f6606360c059e6c0" translate="yes" xml:space="preserve">
          <source>When using &lt;a href=&quot;#torch.utils.cpp_extension.BuildExtension&quot;&gt;&lt;code&gt;BuildExtension&lt;/code&gt;&lt;/a&gt;, it is allowed to supply a dictionary for &lt;code&gt;extra_compile_args&lt;/code&gt; (rather than the usual list) that maps from languages (&lt;code&gt;cxx&lt;/code&gt; or &lt;code&gt;nvcc&lt;/code&gt;) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7635dd8102bfabbafa2a3b5c97436e5cb54d9ba1" translate="yes" xml:space="preserve">
          <source>When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;Reproducibility&lt;/a&gt; for background.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79561f3cf985a5e4719b8247ff531adef7e29706" translate="yes" xml:space="preserve">
          <source>When writing TorchScript directly using &lt;code&gt;@torch.jit.script&lt;/code&gt; decorator, the programmer must only use the subset of Python supported in TorchScript. This section documents what is supported in TorchScript as if it were a language reference for a stand alone language. Any features of Python not mentioned in this reference are not part of TorchScript. See &lt;code&gt;Builtin Functions&lt;/code&gt; for a complete reference of available Pytorch tensor methods, modules, and functions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50bf66ea03e8d3f7a8a4f3f33c0d423a99cbb251" translate="yes" xml:space="preserve">
          <source>When you call &lt;a href=&quot;#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt; on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call &lt;code&gt;torch.load(.., map_location='cpu')&lt;/code&gt; and then &lt;code&gt;load_state_dict()&lt;/code&gt; to avoid GPU RAM surge when loading a model checkpoint.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4b9a73359398be175a62d4f2cbf23eca1eb7394" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="540833bd1b024f45b94c5fe10bdcce398da41e2a" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fa609678777cb0f1d2bea5c36804f3e7f699ab3" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="983301015460563605882b2d43f60b6f31ca13f7" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcabf659c01386b96c4252ad5247bb9f50b66574" translate="yes" xml:space="preserve">
          <source>Where are my downloaded models saved?</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="017d4a3464bb8236ca4dfe2a53c2f262d76de8ab" translate="yes" xml:space="preserve">
          <source>Which backend to use?</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3631991286d3139c1de9a41ef4982f8e5d886ce2" translate="yes" xml:space="preserve">
          <source>Which produces:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a68a67a970d91d390715c4a5723442211582200e" translate="yes" xml:space="preserve">
          <source>While Loops</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48f3d5bde3297b3b923d0f7759d754056d7f6c40" translate="yes" xml:space="preserve">
          <source>While it is assumed that &lt;code&gt;A&lt;/code&gt; is symmetric, &lt;code&gt;A.grad&lt;/code&gt; is not. To make sure that &lt;code&gt;A.grad&lt;/code&gt; is symmetric, so that &lt;code&gt;A - t * A.grad&lt;/code&gt; is symmetric in first-order optimization routines, prior to running &lt;code&gt;lobpcg&lt;/code&gt; we do the following symmetrization map: &lt;code&gt;A -&amp;gt; (A + A.t()) / 2&lt;/code&gt;. The map is performed only when the &lt;code&gt;A&lt;/code&gt; requires gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f69597c76f979c6f47613f7588234a4093ae500a" translate="yes" xml:space="preserve">
          <source>While it should always give you a valid decomposition, it may not give you the same one across platforms - it will depend on your LAPACK implementation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90f928cf1ec6eb30bffa5d56a4ee00f33a26ccf6" translate="yes" xml:space="preserve">
          <source>While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adfc4c1cf043279d74d69b96592d62572f9a8648" translate="yes" xml:space="preserve">
          <source>Wide ResNet</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eec7e605ef42b4a1da3f9a4494e1414462efb313" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac86db1353b797ed1c0d73605a56146ab6cb1914" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b631c349f7132ef6c2a8879b09c961b30fce8aba" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="726f31b3e4c15ecbbc898cae3f9e8f73a3460020" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2b826d3f7d8e2201135c671569ea283afb245af" translate="yes" xml:space="preserve">
          <source>Will result in:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8fa871c4385dea4733bc80aea73b5f5364e4ef06" translate="yes" xml:space="preserve">
          <source>Windows FAQ</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ac66d0e68a85c8d292fec91f6cbcf0bd809b4e9" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;bilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05c8bfe5d3e24898d75d1b114e7f0550313f9961" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See below for concrete examples on how this affects the outputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b1242167a707b16c47db2fe5f3c0130b4361833" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="343e3c563441d98e78b2af38a8ee16b2107d4b5b" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;mode='bicubic'&lt;/code&gt;, it&amp;rsquo;s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call &lt;code&gt;result.clamp(min=0, max=255)&lt;/code&gt; if you want to reduce the overshoot when displaying the image.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="670f4308b71dfa44f0ce3d603252c9a80431b7a1" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;padding_idx&lt;/code&gt; set, the embedding vector at &lt;code&gt;padding_idx&lt;/code&gt; is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from &lt;a href=&quot;#torch.nn.Embedding&quot;&gt;&lt;code&gt;Embedding&lt;/code&gt;&lt;/a&gt; is always zero.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="245747fecda85d5b71d626b9e2eb04627690bcb2" translate="yes" xml:space="preserve">
          <source>With &lt;em&gt;trace-based&lt;/em&gt; exporter, we get the result ONNX graph which unrolls the for loop:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19fffb59deac1debbb0ed0b605bbe04aebbddb29" translate="yes" xml:space="preserve">
          <source>With the default arguments it uses the Euclidean norm over vectors along dimension</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67793521614b6b44f958c708ad8988aeadfd2309" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length in the last dimension:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4263d953681d9ae30565f30202d1492e42e9e8ac" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5375f310fdbbbaceba22e923b5eb13a5f74fab7f" translate="yes" xml:space="preserve">
          <source>Wrapper around a &lt;code&gt;torch._C.Future&lt;/code&gt; which encapsulates an asynchronous execution of a callable, e.g. &lt;a href=&quot;rpc#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;. It also exposes a set of APIs to add callback functions and set results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65acb3d4bdbad8b7e153886b6b2199ad76a756c8" translate="yes" xml:space="preserve">
          <source>Wrapper class for quantized operations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a31ffa99fd6d35b029d1f439707a5bd7c9b4da8" translate="yes" xml:space="preserve">
          <source>Writes all values from the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor. For each value in &lt;code&gt;src&lt;/code&gt;, its output index is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5adf3c6baa6091132b20eef0ff93f0984499afa5" translate="yes" xml:space="preserve">
          <source>Writes entries directly to event files in the log_dir to be consumed by TensorBoard.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c032adc1ff629c9b66f22749ad667e6beadf144b" translate="yes" xml:space="preserve">
          <source>X</source>
          <target state="translated">X</target>
        </trans-unit>
        <trans-unit id="29822901b44f81c6464586704f28915142f932bc" translate="yes" xml:space="preserve">
          <source>X (Tensor): tensor of eigenvectors of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="030b21a9a5ef307806a06b78be434c1972173393" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = X^*[N_1 - \omega_1, \dots, N_d - \omega_d],</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed1d6f505827ea7067cb8c300c0904dbfa5fbd7a" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="746d94247ac643f86d1ee17dab642515e2ff9085" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a14f6ead4d7bc2b1f74caf9a0cc9b1f3d45fbb66" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = X[m, \text{n\_fft} - \omega]^*</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e6854a6a1b525f353a271b3c9b646d1b5c28d3f" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = \sum_{k = 0}^{\text{win\_length-1}}% \text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ % \exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14ebad2cda6dcca683250b526859348822d9d5c8" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. E.g.:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5684cedf8d1212771db72a004af08859ca152372" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. ONNX introduced the concept of Sequence in opset 11. Similar to list, Sequence is a data type that contains arbitrary number of Tensors. Associated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc. However, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace add operator. E.g.:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="738a2b66281e5ca4973cbceebc923d1996e03dad" translate="yes" xml:space="preserve">
          <source>Yields</source>
          <target state="translated">Yields</target>
        </trans-unit>
        <trans-unit id="7c19fb2e314a3137e93fb758f531465aacc3456b" translate="yes" xml:space="preserve">
          <source>You can also construct hybrid sparse tensors, where only the first n dimensions are sparse, and the rest of the dimensions are dense.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d617602dcc62272743d7a795be368e5ffbe5433d" translate="yes" xml:space="preserve">
          <source>You can also run the exported model with &lt;a href=&quot;https://github.com/microsoft/onnxruntime&quot;&gt;ONNX Runtime&lt;/a&gt;, you will need to install &lt;code&gt;ONNX Runtime&lt;/code&gt;: please &lt;a href=&quot;https://github.com/microsoft/onnxruntime#installation&quot;&gt;follow these instructions&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8c75fa187b181f202978d8a86ab28cca3e9eb36" translate="yes" xml:space="preserve">
          <source>You can also verify the protobuf using the &lt;a href=&quot;https://github.com/onnx/onnx/&quot;&gt;ONNX&lt;/a&gt; library. You can install &lt;code&gt;ONNX&lt;/code&gt; with conda:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec539497ce517be7d2c9354eae2288ab7aed0f7d" translate="yes" xml:space="preserve">
          <source>You can construct a model with random weights by calling its constructor:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="762890e45c5b225da275ae73984ed756702605c8" translate="yes" xml:space="preserve">
          <source>You should never try to change your model&amp;rsquo;s parameters after wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;. Because, when wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;, the constructor of &lt;code&gt;DistributedDataParallel&lt;/code&gt; will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model&amp;rsquo;s parameters afterwards, gradient redunction functions no longer match the correct set of parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ccd4aee0db1253438f0c3a9d3c74816b28340f08" translate="yes" xml:space="preserve">
          <source>You&amp;rsquo;ll generally want to use &lt;a href=&quot;torch.qr#torch.qr&quot;&gt;&lt;code&gt;torch.qr()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f259809289921b4be91b7ff0e29bbdb10551660f" translate="yes" xml:space="preserve">
          <source>Your models should also subclass this class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93d450611dc79948223d0cdb9f4a99610848c9d6" translate="yes" xml:space="preserve">
          <source>ZeroPad2d</source>
          <target state="translated">ZeroPad2d</target>
        </trans-unit>
        <trans-unit id="90bc5acc0a3b091bfe56eb668e9c84ac53428130" translate="yes" xml:space="preserve">
          <source>[* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1] \times \ldots \times \text{normalized\_shape}[-1]]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b61210c8825bb88613c060ba48aae051333bd30a" translate="yes" xml:space="preserve">
          <source>[-1, 1]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae17aa1eaf46c89eecaf929c74d8fda9a55db49b" translate="yes" xml:space="preserve">
          <source>[0, 1)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c49d95c97e6b97b46f0fa87c4c3d5517b2dd2ac1" translate="yes" xml:space="preserve">
          <source>[0, C-1]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c00ab2adc7412d84a0750550969f7439fdbed02f" translate="yes" xml:space="preserve">
          <source>[0] a pretty-printed representation (as valid Python syntax) of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;code&gt;code&lt;/code&gt;. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant&amp;rsquo;s values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc47f02dcecd076fd210f14fb47e8772c796bc8d" translate="yes" xml:space="preserve">
          <source>[1] D. W. Griffin and J. S. Lim, &amp;ldquo;Signal estimation from modified short-time Fourier transform,&amp;rdquo; IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9390898997c65a41ca007de79555abfa5f0ba5df" translate="yes" xml:space="preserve">
          <source>[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A Robust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/17M1129830&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/17M1129830&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f08253d232026d76249c2db42c2940e24ba3bacf" translate="yes" xml:space="preserve">
          <source>[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2), 517-541. (25 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff64b4c2d43518023be089f31430ef90034f605e" translate="yes" xml:space="preserve">
          <source>[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/S1064827500370883&quot;&gt;https://epubs.siam.org/doi/10.1137/S1064827500370883&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1e9b6692d359e3ebb28163afd3f06ce34d6b2df" translate="yes" xml:space="preserve">
          <source>[[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(&amp;lsquo;A&amp;rsquo;, &amp;lsquo;B1&amp;rsquo;, &amp;lsquo;B2&amp;rsquo;))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38ac8c676253501b3fb6819ff9373ce6cfe5b055" translate="yes" xml:space="preserve">
          <source>\ ^*</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80a1cbf9743b017c73b23f243284395f0c518670" translate="yes" xml:space="preserve">
          <source>\Gamma(\cdot)</source>
          <target state="translated">\Gamma(\cdot)</target>
        </trans-unit>
        <trans-unit id="15e33ea71862e7b6632cf90aebb8ab938c698f9d" translate="yes" xml:space="preserve">
          <source>\Phi(x)</source>
          <target state="translated">\Phi(x)</target>
        </trans-unit>
        <trans-unit id="2a6c118642891e41137cb68e1346d34dabbd128c" translate="yes" xml:space="preserve">
          <source>\Vert x \Vert _p = \left( \sum_{i=1}^n \vert x_i \vert ^ p \right) ^ {1/p}.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7c665b45932a814215e979bc2611080b4948e68" translate="yes" xml:space="preserve">
          <source>\alpha</source>
          <target state="translated">\alpha</target>
        </trans-unit>
        <trans-unit id="1eafe1d2e67a6ccfd6c43fcc6a7aba05feb684c2" translate="yes" xml:space="preserve">
          <source>\alpha = 1.6732632423543772848170429916717</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbe1c5b26860bdd639e58977c2072d9b65aece25" translate="yes" xml:space="preserve">
          <source>\alpha=1.6732632423543772848170429916717</source>
          <target state="translated">\alpha=1.6732632423543772848170429916717</target>
        </trans-unit>
        <trans-unit id="eef3f6c4a900c50c6ccae139e6900e18ddc9222e" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k, \text{stride[1]} \times h + m, \text{stride[2]} \times w + n) \end{aligned}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3444423e4aa6a27dc2d4e17fde867139b4f23a7b" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\ &amp;amp; \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k, \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)} {kD \times kH \times kW} \end{aligned}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b5fbddfbbd75f7006abb89ad6964ec0f13538c7" translate="yes" xml:space="preserve">
          <source>\begin{aligned} out(N_i, C_j, h, w) ={} &amp;amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m, \text{stride[1]} \times w + n) \end{aligned}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ea19e5cef880f790f54322d49c339a67ef7eba6" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \\ i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\ f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\ g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\ o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\ c_t = f_t \odot c_{t-1} + i_t \odot g_t \\ h_t = o_t \odot \tanh(c_t) \\ \end{array}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b6cb34a158b8201fbf48f50465b1ece00a7eb5f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|AX-B\|_2. \end{array}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9312cff9d66b9a6753766d98374f8fbc6cf98e2f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|X\|_2 &amp;amp; \text{subject to} &amp;amp; AX = B. \end{array}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="930f69e97734f3784bc6b17a4919f47bb6fa5660" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\ f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\ g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\ o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\ c' = f * c + i * g \\ h' = o * \tanh(c') \\ \end{array}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6bea3b60c648ba0107ad5a273553cd4d585a9b3" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\ z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\ n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\ h' = (1 - z) * n + z * h \end{array}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e41fd097ab91cc853c05e5d0d9e0d37ff3dd7662" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\ z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\ n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \end{array}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6499d503bfc00cadae1440b191c52a8632e2f8c4" translate="yes" xml:space="preserve">
          <source>\beta</source>
          <target state="translated">\beta</target>
        </trans-unit>
        <trans-unit id="6ceee6706c4f97ef457cd65c6ed19732a4a4c7e0" translate="yes" xml:space="preserve">
          <source>\delta^{(l-1)}_t</source>
          <target state="translated">\delta^{(l-1)}_t</target>
        </trans-unit>
        <trans-unit id="986673ab936df8ce1454d7dc93297bd7bbc51c4b" translate="yes" xml:space="preserve">
          <source>\ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d38ace96b9720e15bef838db0ce85ef5b456092" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n) + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec2ad56a41af8d7cfc9a26f61f048cf6cbd7e78d" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de7b2379237fe388c3459990f0a508ab1bd86588" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6a58890a57b18854cfa1707f659a4fd6ff6e4cb" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left( x_n - y_n \right)^2,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbe073a5e1335cf9e8392124ee3bc939a82d8704" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right|,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="295ee7beff5d61b430ac876642e47ec1ac559795" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';} \\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d5ca23779a198f768fff42eaa5060e03279277c" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c1589dd2fb19090831cbb7f520baedd8d65a3e4" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{`mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{`sum'.} \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ba1942b6f9513bd39cc8cb1f1c52037ed6d49d3" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;amp; \text{if reduction} = \text{'mean';}\\ \sum_{n=1}^N l_n, &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a3a211a3bdfba5d642037c3946f715f2ce73187" translate="yes" xml:space="preserve">
          <source>\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c}) + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c04fbcdd9308d49d4c2300f2ebbb1e83c44e8b83" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target} * \text{input}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd47433a58e73b2601de69c1a9744b288212fb5f" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target}*\text{input}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe43eb66c8bdd27010ca6db1c98281c7d4c4731f" translate="yes" xml:space="preserve">
          <source>\exp^A = \sum_{k=0}^\infty A^k / k!.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14f6d7074093dd4d9ef81de502fd654605e4bb67" translate="yes" xml:space="preserve">
          <source>\forall i = d, \dots, d+k-1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6aed51b4bd21bf9aaad2932aa936dab4bdd62611" translate="yes" xml:space="preserve">
          <source>\frac{1}{1-p}</source>
          <target state="translated">\frac{1}{1-p}</target>
        </trans-unit>
        <trans-unit id="037205eaa442691656469a316bf2dff320c2f572" translate="yes" xml:space="preserve">
          <source>\frac{1}{2} N (N - 1)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64c94d13eeb330b494061e86538db66574ad0f7d" translate="yes" xml:space="preserve">
          <source>\frac{1}{3}</source>
          <target state="translated">\frac{1}{3}</target>
        </trans-unit>
        <trans-unit id="5947a169159fe867f85f3fd8b9690019b48152f5" translate="yes" xml:space="preserve">
          <source>\frac{1}{8}</source>
          <target state="translated">\frac{1}{8}</target>
        </trans-unit>
        <trans-unit id="f81c864ea46e4c42b16663b744993f9011be95f2" translate="yes" xml:space="preserve">
          <source>\frac{300}{100}=3</source>
          <target state="translated">\frac{300}{100}=3</target>
        </trans-unit>
        <trans-unit id="436bcf2181eea8fe79cdb015a5567ca1b8d69652" translate="yes" xml:space="preserve">
          <source>\frac{5}{3}</source>
          <target state="translated">\frac{5}{3}</target>
        </trans-unit>
        <trans-unit id="a9c2bfb5b8138830fd93a3a13faef54bc4dc94a2" translate="yes" xml:space="preserve">
          <source>\frac{m}{2} \leq</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8fb95020958e9f0b58822f35b35fb490054c02e" translate="yes" xml:space="preserve">
          <source>\frac{p - 1}{2}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67833ee2012ec1c6254b6c009dc72bf0dc48aa6d" translate="yes" xml:space="preserve">
          <source>\gamma</source>
          <target state="translated">\gamma</target>
        </trans-unit>
        <trans-unit id="27634ea2c473bc36e59149a7f0c457ffb292325a" translate="yes" xml:space="preserve">
          <source>\hat{x}</source>
          <target state="translated">\hat{x}</target>
        </trans-unit>
        <trans-unit id="b9e47e1f86f68634e0d0a997d4ea3952fae1e892" translate="yes" xml:space="preserve">
          <source>\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="770b7843ffee64a984041915fe8461fd98a76060" translate="yes" xml:space="preserve">
          <source>\in [0, \infty]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b97f26fbeb1b84327c736de515f10980d9c7d68" translate="yes" xml:space="preserve">
          <source>\infty</source>
          <target state="translated">\infty</target>
        </trans-unit>
        <trans-unit id="b237071f96360004cf37b06340000864b59f54c3" translate="yes" xml:space="preserve">
          <source>\int y\,dx</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a10251c74fceb1b1b9e9c45471b613f216beb4a9" translate="yes" xml:space="preserve">
          <source>\int_a^b f = -\int_b^a f</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3931f1ce298c536432fd324b3a1ab4337120689" translate="yes" xml:space="preserve">
          <source>\lambda</source>
          <target state="translated">\lambda</target>
        </trans-unit>
        <trans-unit id="1b47c3b18de49455a713efe91ac03ec27aad59a5" translate="yes" xml:space="preserve">
          <source>\lbrace (i, i) \rbrace</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fa91d11567564172cce99f01e48d1e73a28bc31" translate="yes" xml:space="preserve">
          <source>\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89ca5f286a6835c142dd1390cd67d2698dca7bcb" translate="yes" xml:space="preserve">
          <source>\left[\text{-clip\_value}, \text{clip\_value}\right]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ceb7ce51c6d9da5e34a800614788f55dff712c0" translate="yes" xml:space="preserve">
          <source>\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a74c86baea5a7bb180947e759ccafb91eee50e79" translate="yes" xml:space="preserve">
          <source>\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed56196dc362da2ecbc9244bc832a7311739bb3d" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="de0088faf4547f5a22dd4cd77b23209a7dd8113e" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="88061aa72baf92fe1d129c82b73d16c37a6af8f0" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="f0a9328764eddcc1a62bf8fbb7c249505ed2c539" translate="yes" xml:space="preserve">
          <source>\leq</source>
          <target state="translated">\leq</target>
        </trans-unit>
        <trans-unit id="49abff7af14753dc3cdab27a1bf7f8ed5f2c2fa1" translate="yes" xml:space="preserve">
          <source>\leq 256</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a719b606b8fa813d6dc2397930551b66016965ba" translate="yes" xml:space="preserve">
          <source>\leq S</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7d8821758716417ac6285ab84b0661734c3439c" translate="yes" xml:space="preserve">
          <source>\leq T</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ca5d36772ef139985921c4d545788d48fddcf0c" translate="yes" xml:space="preserve">
          <source>\lfloor \frac{N_d}{2} \rfloor + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc0c94864255e5978415bf4d290dee47e2677194" translate="yes" xml:space="preserve">
          <source>\lfloor\frac{\text{input planes}}{sT}\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9878e94e87e5bd7098242aa36f29f409bc60ed2b" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26c67f72ffeaf83ec8c07ff1c00c57b7b4ce05e6" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \log (x) = -\infty</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d92b1f0baf6824880fb434c0077a618ba265ea33" translate="yes" xml:space="preserve">
          <source>\log (0) = -\infty</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99e33ed0cf12197cde63048ced916ea73cbc9c3e" translate="yes" xml:space="preserve">
          <source>\log(0)</source>
          <target state="translated">\log(0)</target>
        </trans-unit>
        <trans-unit id="9f7859c28e08a9c15994c8896e613dd27080b6a5" translate="yes" xml:space="preserve">
          <source>\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1e3a3daacbe46fc4916de547be4049f250f3288" translate="yes" xml:space="preserve">
          <source>\log(\text{Softmax}(x))</source>
          <target state="translated">\log(\text{Softmax}(x))</target>
        </trans-unit>
        <trans-unit id="322a0b1861bac0e7bc727021b82a0313c2a2b303" translate="yes" xml:space="preserve">
          <source>\log\left(e^x + e^y\right)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8783c75c38eec2ddd68cac88f7dc5ac00e806274" translate="yes" xml:space="preserve">
          <source>\log_2\left(2^x + 2^y\right)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8935b97a0f600209c234abf4d224c5fe967c9fa1" translate="yes" xml:space="preserve">
          <source>\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eafd018e3a8aa85682e4f0b33612ce6efed642ca" translate="yes" xml:space="preserve">
          <source>\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})}, \sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="201a130976ca261c5bc7ef67511ced9f15d7653a" translate="yes" xml:space="preserve">
          <source>\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df511dffcdad49a67cd5945ee0ab7aef8bb4f9fc" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 0.01)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f3ba18912099017c093331676f6188e744efdac" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 1)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50c7a47dd01b89919d5422d0295475337aab6ace" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, \text{std}^2)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92c992916044275832483fbb98179a2d00ca4c1e" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(\text{mean}, \text{std}^2)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87b1ef0bb4d4a0924cc95ef538f83b2bb3e53e42" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\sqrt{k}, \sqrt{k})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7f6ed3e182dc030701ad2deb57bf7caa5b88d6f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\text{bound}, \text{bound})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="325a9ed78efc110eaab4160ef2c97f9ac713df84" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-a, a)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b28ca9812620dfef2c686761b7aa8334acb0312f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(0, 1)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9712b8b025515dc1dea5f46b97d3c4fe634d954" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(\text{lower}, \text{upper})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e216b98083013f9a980a8176c0057ec95ff5e691" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(a, b)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2df7cbd2cee6b04f561cf080750aba56226af4cb" translate="yes" xml:space="preserve">
          <source>\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20a69b6b57674088939e407b3b5098441f752171" translate="yes" xml:space="preserve">
          <source>\mathrm{erfinv}(\mathrm{erf}(x)) = x</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56d825970c3fb1fe7543e6c61686e2830fac7d9f" translate="yes" xml:space="preserve">
          <source>\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e9c0d2c84e11d8338d1dac942d5dede72bd1acf" translate="yes" xml:space="preserve">
          <source>\min(input.size(-1), input.size(-2))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a4e56595df1d02f21e5c3ff7b0e9d80921858ff" translate="yes" xml:space="preserve">
          <source>\mu</source>
          <target state="translated">\mu</target>
        </trans-unit>
        <trans-unit id="c8e2d1a0bf50a27d43ade30cfb048d99feb31ad1" translate="yes" xml:space="preserve">
          <source>\odot</source>
          <target state="translated">\odot</target>
        </trans-unit>
        <trans-unit id="73b077a63e22815fe5c8ee82dab9894be842b19c" translate="yes" xml:space="preserve">
          <source>\omega</source>
          <target state="translated">\omega</target>
        </trans-unit>
        <trans-unit id="72166555a6db55785fc6fbe9a7c5bbe72be28db8" translate="yes" xml:space="preserve">
          <source>\otimes</source>
          <target state="translated">\otimes</target>
        </trans-unit>
        <trans-unit id="6dc2ada78a76bad95d3b921558b1195549985eab" translate="yes" xml:space="preserve">
          <source>\prod(\text{kernel\_size})</source>
          <target state="translated">\prod(\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="02cea321dbc3a3edfc9cc1780dfe84f694c585f0" translate="yes" xml:space="preserve">
          <source>\psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b30c154a70c78f1c5dcecce58dde2e9ff4ec56aa" translate="yes" xml:space="preserve">
          <source>\psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69c15416b63fd933850978302bcda59da879774e" translate="yes" xml:space="preserve">
          <source>\sigma</source>
          <target state="translated">\sigma</target>
        </trans-unit>
        <trans-unit id="bfe16f27ebc966df6f10ba356a1547b6e7242dd7" translate="yes" xml:space="preserve">
          <source>\sqrt{2}</source>
          <target state="translated">\sqrt{2}</target>
        </trans-unit>
        <trans-unit id="358161536f25000be6a773604fcf4a28afd7b7bd" translate="yes" xml:space="preserve">
          <source>\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64126f7d8d3d661d4e29a191268f98bf759903a3" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^K N_i}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="976e9a0789eb95323325b92e6edc3b432c6754b8" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^d N_i}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9312c2748ccad7c25f8d92bc855a5a0b38989a51" translate="yes" xml:space="preserve">
          <source>\star</source>
          <target state="translated">\star</target>
        </trans-unit>
        <trans-unit id="3df82d7a797b96797b79d72f235bc018cb8155c9" translate="yes" xml:space="preserve">
          <source>\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a12be8b8923c0e92cce3f35968e39960bc665833" translate="yes" xml:space="preserve">
          <source>\tanh</source>
          <target state="translated">\tanh</target>
        </trans-unit>
        <trans-unit id="053658991aeb9a94a57c16cbe979538b3eca3b46" translate="yes" xml:space="preserve">
          <source>\texttt{n\_classes}</source>
          <target state="translated">\texttt{n\_classes}</target>
        </trans-unit>
        <trans-unit id="79df4825cae48291f72d91cc9840f2adcb2716c4" translate="yes" xml:space="preserve">
          <source>\texttt{result[i]}</source>
          <target state="translated">\texttt{result[i]}</target>
        </trans-unit>
        <trans-unit id="8a01c6904694a6040cbbe4d31b35e67c5fb8c5bc" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p\_tensor[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p\_tensor[i]})</target>
        </trans-unit>
        <trans-unit id="248f05fb27cede89d993bc3bb9750c4fde2467c2" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p})</target>
        </trans-unit>
        <trans-unit id="15ca5fa99d2411cc7a336f5df78382fd090360c9" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{self[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{self[i]})</target>
        </trans-unit>
        <trans-unit id="f4308a3c6285dab040bdf907914e0906e5918d43" translate="yes" xml:space="preserve">
          <source>\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c5fbaf107a1bd80dcb32b25718623eef6cabd9f" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; 0\\ \alpha * (\exp(x) - 1), &amp;amp; \text{ if } x \leq 0 \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee2c000c326328286929b5abe78d5f28ecc355ca" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ae2cb648d379e90a17c82c0bca8393180166a13" translate="yes" xml:space="preserve">
          <source>\text{GELU}(x) = x * \Phi(x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98d15f980c451db5bf1f8450a31ea12ac80b261b" translate="yes" xml:space="preserve">
          <source>\text{GLU}(a, b) = a \otimes \sigma(b)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b0fd43cdc620c274316d22c100c5a1cedf8758f" translate="yes" xml:space="preserve">
          <source>\text{HardShrink}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="905f89b6d5fe72acb1c8befdbd1fbd394ccc9c79" translate="yes" xml:space="preserve">
          <source>\text{HardTanh}(x) = \begin{cases} 1 &amp;amp; \text{ if } x &amp;gt; 1 \\ -1 &amp;amp; \text{ if } x &amp;lt; -1 \\ x &amp;amp; \text{ otherwise } \\ \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0b2885acfe7c1d78f13cbb375808a8a70622e9c" translate="yes" xml:space="preserve">
          <source>\text{Hardsigmoid}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ 1 &amp;amp; \text{if~} x \ge +3, \\ x / 6 + 1 / 2 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3707baa731a2a1d93fe314a962f04a7440710fa1" translate="yes" xml:space="preserve">
          <source>\text{Hardswish}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ x &amp;amp; \text{if~} x \ge +3, \\ x \cdot (x + 3) /6 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4d15ebefe7344cee7608a90993d80b6ed04f631" translate="yes" xml:space="preserve">
          <source>\text{LeakyRELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ \text{negative\_slope} \times x, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68988527b0b6237352fc1f66e3f8116ea81dd84a" translate="yes" xml:space="preserve">
          <source>\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51a86607b1de32af3fd6faa8dfe21281f15bc36b" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4939da8f3a870de43d45395bdbb72f415fd6a5c" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a58af29fea78e99ffe2b4d0392c259d952558379" translate="yes" xml:space="preserve">
          <source>\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96135669cb29b74a14e76462df05bf97eef0ca84" translate="yes" xml:space="preserve">
          <source>\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ccbab99b824f6a91f1e1bf82a1a0fdaf575b1bb6" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ ax, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="caa4e9686c6c337eb30002e3bdcb6a130c86d148" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b347087764b6a93bb6f3ca448de0809ea32466b" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + a * \min(0,x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ab73665e19333454b7dcdadd14407c6f4eaf162" translate="yes" xml:space="preserve">
          <source>\text{RReLU}(x) = \begin{cases} x &amp;amp; \text{if } x \geq 0 \\ ax &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca059eb696d6eaad86cfafe12af59ddef8aeb677" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(0,x), 6)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be84129fb353e250c9169d66a9b03062f62f4151" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(x_0, x), q(6))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bfab1dc2c67446ab1cfb128a9c2bc89afb1f336" translate="yes" xml:space="preserve">
          <source>\text{ReLU}</source>
          <target state="translated">\text{ReLU}</target>
        </trans-unit>
        <trans-unit id="08bf65cb7cffc00d2f4597ea4d30a22a63976e74" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x) = (x)^+ = \max(0, x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="836fcdcc8abf1c798414718b995cf2a357c6c5a8" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x)= \max(x_0, x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cab6677fddc14bf3ddd26b8fe4fe9e7d772b3259" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0eacb9a4c9b22d795a671d9085d3576f65f88226" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14b5832965c3a723de26aea464be1a7e1207b0e7" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd6633f7f328cd88d5fded389f4a69b2a11f8bed" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa68ac444b17ef606bcb6fc0587fbb0c08130721" translate="yes" xml:space="preserve">
          <source>\text{SoftShrinkage}(x) = \begin{cases} x - \lambda, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x + \lambda, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16ab4540ec544f6557be83896c80fd5ae2348d4a" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{ 1 + |x|}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4e8d4178c0a5a0115f54178dd9083d36fddd624" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{1 + |x|}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2f536bf3a8bd1033dd978de393c848edde7120d" translate="yes" xml:space="preserve">
          <source>\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b81ade15ec338b468467f739fb0a7a9ffbb69c9" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x) = \text{Softmax}(-x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f07f1bdd8c02f9e56e3ec0dac4ae00ebee846aae" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b624f5a535d4c37714f3d81447e39c785f806f7" translate="yes" xml:space="preserve">
          <source>\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0fd3a2a1e0641ec623557f2f43fda5f7c89dc6c" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \tanh(x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="995fcea13ed41fcda67e8f62153baff1abf82a33" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \text{Tanh}(x)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0334b6a3fe1d679df6ff4bfb438db0b64230682" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75a10715f4ce8ac09932b779e24b1a0a5468348f" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f169080e8d1eb07929eead4219c50bde74fb23da" translate="yes" xml:space="preserve">
          <source>\text{batch1} \mathbin{@} \text{batch2}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="229245bb08fb0569d7993b5cea15398607e69900" translate="yes" xml:space="preserve">
          <source>\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e2f5fb306be83947e6aad79d83c918b084bbee4" translate="yes" xml:space="preserve">
          <source>\text{in\_channels}</source>
          <target state="translated">\text{in\_channels}</target>
        </trans-unit>
        <trans-unit id="26c469932ffffddf5378f023f97a627052bcde45" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;gt; \text{other}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c15cd245b2e6c1904e5b0a29dc06bb37cb23bb5e" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;lt; \text{other}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f395bedc77d1db475982b0b075906a247b84c2eb" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target} * \log(\text{input}+\text{eps})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="686d55b3f5e0392f4e8fcfd75b77c03037718ecf" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target}*\log(\text{input}+\text{eps})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6da1a408fc1384eb95e3208a559d3a3e0f03e99f" translate="yes" xml:space="preserve">
          <source>\text{input} = Q R</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4dd474d77793c7a92038a0755b05fe0ba4adf17" translate="yes" xml:space="preserve">
          <source>\text{input} = V \text{diag}(e) V^T</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e721787652f4fc0b6f66950d386917c38d8957f8" translate="yes" xml:space="preserve">
          <source>\text{input} \geq \text{other}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cbe8a936fc7cb3a9fdf09ad329713fbe61317e4" translate="yes" xml:space="preserve">
          <source>\text{input} \leq \text{other}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3901704e8ee508e9522d87ef1f88b52ef027532b" translate="yes" xml:space="preserve">
          <source>\text{input} \neq \text{other}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ccf6648a02673a0bbbd4f96889ba48888a0e40f4" translate="yes" xml:space="preserve">
          <source>\text{input}[i, j]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47eeab2292dd874946bd7e66f27cc66b11eafc5e" translate="yes" xml:space="preserve">
          <source>\text{input}_{i}</source>
          <target state="translated">\text{input}_{i}</target>
        </trans-unit>
        <trans-unit id="dd2a901c455f604c6af98c920667ce74d801ef8c" translate="yes" xml:space="preserve">
          <source>\text{input}_{i} / \text{other}_{i}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c920fc92a2f279db08ab80d5217f4a1714bd40d6" translate="yes" xml:space="preserve">
          <source>\text{i}^{th}</source>
          <target state="translated">\text{i}^{th}</target>
        </trans-unit>
        <trans-unit id="6ca0545c1ccedc4ff77a3d4acb74e3770c0fbbdd" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9229970eeb833df18b71461a132cd3b0ce98ef3a" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15869cb2da85e3f8dc3a924d02c0d13f080d42d1" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size})</source>
          <target state="translated">\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="80b2d9d0a1aff139a212f622b578c27a2cb6c126" translate="yes" xml:space="preserve">
          <source>\text{k}^{th}</source>
          <target state="translated">\text{k}^{th}</target>
        </trans-unit>
        <trans-unit id="0f84b2b9c4507f4c38935799e7145beab872e3ff" translate="yes" xml:space="preserve">
          <source>\text{logcumsumexp}(x)_{ij} = \log \sum\limits_{j=0}^{i} \exp(x_{ij})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4fff2f19ba639fc2ee13a3cda790993ad87fdfe" translate="yes" xml:space="preserve">
          <source>\text{logsumexp}(x)_{i} = \log \sum_j \exp(x_{ij})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e990f63d798552bb3d1da254e1d0c3c11424c266" translate="yes" xml:space="preserve">
          <source>\text{loss} = \frac{\sum^{N}_{i=1} loss(i, class[i])}{\sum^{N}_{i=1} weight[class[i]]}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65ca80a02b47d73c67e5a3e63c9b314787018d8d" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) = -x[class] + \log\left(\sum_j \exp(x[j])\right)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="019a711532dd32707cd8b1b52a85f9dcf92bf993" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ffd0b22341cc6c018e0b5728d553ab798985dbfc" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \begin{cases} 1 - \cos(x_1, x_2), &amp;amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp;amp; \text{if } y = -1 \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c59e75cdc56fc3f83b35cdb9f628a1f6dc39d344" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b06e3acda76e096ac3266b690a1f15f3c86a036c" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="932b5db2aa0a39e3cde91ee0926767654acd252f" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1be1a15f43a5fe29e30cefc8a77697b52665cc11" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be8b3997d164f84206bbdb59fc30b16e4df28e7b" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed773e5223ed4e8e7a2085c415f1e22caa9ee61a" translate="yes" xml:space="preserve">
          <source>\text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a70593a5153f0c40ba1299b72f0b57903179f3e" translate="yes" xml:space="preserve">
          <source>\text{other}_{i}</source>
          <target state="translated">\text{other}_{i}</target>
        </trans-unit>
        <trans-unit id="17f7519844253d6013b5ffbe16cec0f08dcc6ff0" translate="yes" xml:space="preserve">
          <source>\text{out} = -1 \times \text{input}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90a41651feeb08c3eedd09a310ec016d8b14ad0c" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb3213012476cae8440fbe19a6e07cd71a7793bd" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e949532f9deb6bd6d752ab7f572576835541d193" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0358e27d2d15a38fee22f3930f0704830355a8e2" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{abs} \cdot \cos(\text{angle}) + \text{abs} \cdot \sin(\text{angle}) \cdot j</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36eb11cd26daa728869e9dc0ff4a77afceb36308" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{alpha} \times \text{other}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="256d0e45774d81cdb6896827af46bd880155747c" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{other}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b32a02ba7ef69d1c7496b06c7c87e64e5599ad7d" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1} \text{input}(N_i, C_j, \text{stride} \times l + m)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ffc018e43b40a0d8b659cfa4eb86a17bb9a021a" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1329f06a718ae43bb8cf775c59ab239d1fb3b8cd" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43e8084d30aefee2bbf06206dd0a8ab14834163f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \begin{cases} \text{x}_i &amp;amp; \text{if } \text{condition}_i \\ \text{y}_i &amp;amp; \text{otherwise} \\ \end{cases}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="887bfc6ee868fe88195f2b1a697c2be49d6ee9a3" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65617250468b820b83705d0c5a443bdbe573e367" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \frac{\text{input}_i}{\text{other}_i}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07bf2f98a46000c9d46f464978022925a2bd7e3f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3da52d8260a7cb2e43645767f57dc80dcb7dfe10" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62f87f35ab6f8ad57797570d5573c2164ad3b265" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a9589802a74e5f4ebdc7b3bf9edf66a572b8b2d" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \times \text{other}_i</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="598abd50041125894fc9d56c763e555dc53626ff" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{other} \times \text{input}_i</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f19bf9db2f65318c8d3279e96d9b9001602fb09a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{self} ^ {\text{exponent}_i}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7a2515cc1e93998b3a18636546dddfa38daeec0" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cb57186f10cc53449c5a3667b3e9e3171f06840" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ \text{exponent}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc2ce21ca05c23e8481f3ba74137e106e861116a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ {\text{exponent}_i}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="199c9bafa2de3cddd25186555e3341b49eafa3be" translate="yes" xml:space="preserve">
          <source>\text{out}_i \sim \text{Poisson}(\text{input}_i)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec5b93008f32dc9277b22a2479b439d557449a63" translate="yes" xml:space="preserve">
          <source>\text{out}_{i+1} = \text{out}_i + \text{step}.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebe2d70cc579db08dc5e67ffe414031300d09f4c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = I_0(\text{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b1003c5523ecc63d8ba6aacefacf06606afa128" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos(\text{input}_{i})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f24a159b70e10e5357a57a49e58cd32d187c432a" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos^{-1}(\text{input}_{i})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21103602f770a5f7ee7fc1da1e89140f56599227" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh(\text{input}_{i})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84def1c39988847c83528f8fb8f04adcd967d458" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh^{-1}(\text{input}_{i})</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f2819fdd8d29dbcba0965986f63a77a316be87c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}</source>
          <target state="new"/>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
