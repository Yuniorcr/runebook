<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="797ae6ccf0f3fa0fcbeb9b7b388baedb5b006fbf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Distributed Autograd&lt;/strong&gt; stitches together local autograd engines on all the workers involved in the forward pass, and automatically reach out to them during the backward pass to compute gradients. This is especially helpful if the forward pass needs to span multiple machines when conducting, e.g., distributed model parallel training, parameter-server training, etc. With this feature, user code no longer needs to worry about how to send gradients across RPC boundaries and in which order should the local autograd engines be launched, which can become quite complicated where there are nested and inter-dependent RPC calls in the forward pass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1bb7e35d8fadfa0ec492eaeee55b03e9a384fe57" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Distributed Optimizer&lt;/strong&gt;&amp;rsquo;s constructor takes a &lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;&lt;code&gt;Optimizer()&lt;/code&gt;&lt;/a&gt; (e.g., &lt;a href=&quot;optim#torch.optim.SGD&quot;&gt;&lt;code&gt;SGD()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;optim#torch.optim.Adagrad&quot;&gt;&lt;code&gt;Adagrad()&lt;/code&gt;&lt;/a&gt;, etc.) and a list of parameter RRefs, creates an &lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;&lt;code&gt;Optimizer()&lt;/code&gt;&lt;/a&gt; instance on each distinct RRef owner, and updates parameters accordingly when running &lt;code&gt;step()&lt;/code&gt;. When you have distributed forward and backward passes, parameters and gradients will be scattered across multiple workers, and hence it requires an optimizer on each of the involved workers. Distributed Optimizer wraps all those local optimizers into one, and provides a concise constructor and &lt;code&gt;step()&lt;/code&gt; API.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f231e46f6260f0b30e89ac8d9edeea3e85af0b5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Double-backward&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;Double-backward&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="2d5f8bc4ac56cfd97f97d119eb653f063ce9a8c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Dropout removal&lt;/strong&gt; (blacklisting option &lt;code&gt;MobileOptimizerType::REMOVE_DROPOUT&lt;/code&gt;): This optimization pass removes &lt;code&gt;dropout&lt;/code&gt; and &lt;code&gt;dropout_&lt;/code&gt; nodes from this module when training is false.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="223393d322f23e3fe61df9bd2a6b6a80c0693ded" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Forward-backward correlation&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3e372f6656879627d1504382a4b7846f22e788b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;GLOO_SOCKET_IFNAME&lt;/strong&gt;, for example &lt;code&gt;export GLOO_SOCKET_IFNAME=eth0&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f6592a881a64ce2fc84fda7388b35a82d8e09ea" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;How to use this module:&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f89fecb44ae27dd51a780204df3a0f0d43ccc86" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Important Notices:&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9e3d7be29b190f07aadef5e37d2c8e47444424a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Important&lt;/strong&gt;: In contrast to the other models the inception_v3 expects tensors with a size of N x 3 x 299 x 299, so ensure your images are sized accordingly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1df66398ad89ce8863dff1901f021a972d0a77b3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Insert and Fold prepacked ops&lt;/strong&gt; (blacklisting option &lt;code&gt;MobileOptimizerType::INSERT_FOLD_PREPACK_OPS&lt;/code&gt;): This optimization pass rewrites the graph to replace 2D convolutions and linear ops with their prepacked counterparts. Prepacked ops are stateful ops in that, they require some state to be created, such as weight prepacking and use this state, i.e. prepacked weights, during op execution. XNNPACK is one such backend that provides prepacked ops, with kernels optimized for mobile platforms (such as ARM CPUs). Prepacking of weight enables efficient memory access and thus faster kernel execution. At the moment &lt;code&gt;optimize_for_mobile&lt;/code&gt; pass rewrites the graph to replace &lt;code&gt;Conv2D/Linear&lt;/code&gt; with 1) op that pre-packs weight for XNNPACK conv2d/linear ops and 2) op that takes pre-packed weight and activation as input and generates output activations. Since 1 needs to be done only once, we fold the weight pre-packing such that it is done only once at model load time. This pass of the &lt;code&gt;optimize_for_mobile&lt;/code&gt; does 1 and 2 and then folds, i.e. removes, weight pre-packing ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c203022c4b7e41a789c0850a218035678246aafc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;LU_data&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the packed LU factorization data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b68de83e526debfd8f4bba73fb2af28a093a044c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;LU_data&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the pivoted LU factorization of A from &lt;a href=&quot;torch.lu#torch.lu&quot;&gt;&lt;code&gt;torch.lu()&lt;/code&gt;&lt;/a&gt; of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c5a7f1d3cffe65ae015e850f4296ab899102cff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;LU_pivots&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the packed LU factorization pivots</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f3d2fa0664b850596925f91912a8226903a5e6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;LU_pivots&lt;/strong&gt; (&lt;em&gt;IntTensor&lt;/em&gt;) &amp;ndash; the pivots of the LU factorization from &lt;a href=&quot;torch.lu#torch.lu&quot;&gt;&lt;code&gt;torch.lu()&lt;/code&gt;&lt;/a&gt; of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cde0f3c0abde6143690d2cb7605d91fbcc4fe6ec" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;N&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Number of columns in the output. If N is not specified, a square array is returned</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04a56de9a19a514a1fcf10373351c67efd1b49d3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;NCCL_SOCKET_IFNAME&lt;/strong&gt;, for example &lt;code&gt;export NCCL_SOCKET_IFNAME=eth0&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec5bc033e1610e4a75f2543da0a843e4cbbadc81" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Note&lt;/strong&gt; &amp;ndash; if kdim and vdim are None, they will be set to embed_dim such that</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="576aa905938e327a5ed4b8fede98a6c9f70e96e9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Propagate names&lt;/strong&gt;: &lt;em&gt;unify&lt;/em&gt; the names to select which one to propagate. In the case of &lt;code&gt;x + y&lt;/code&gt;, &lt;code&gt;unify('X', None) = 'X'&lt;/code&gt; because &lt;code&gt;'X'&lt;/code&gt; is more specific than &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ee6ef9bb908af00d480a75de89ef5668a859963" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Propagate names&lt;/strong&gt;: name inference propagates names to output tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af407aa1e4067bd772f2d564ff50863c94343551" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;QR&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): the details of the QR factorization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6af63ab37345973eb43a4a5fd4d33de8547a1e3b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ReLU/Hardtanh fusion&lt;/strong&gt;: XNNPACK ops support fusion of clamping. That is clamping of output activation is done as part of the kernel, including for 2D convolution and linear op kernels. Thus clamping effectively comes for free. Thus any op that can be expressed as clamping op, such as &lt;code&gt;ReLU&lt;/code&gt; or &lt;code&gt;hardtanh&lt;/code&gt;, can be fused with previous &lt;code&gt;Conv2D&lt;/code&gt; or &lt;code&gt;linear&lt;/code&gt; op in XNNPACK. This pass rewrites graph by finding &lt;code&gt;ReLU/hardtanh&lt;/code&gt; ops that follow XNNPACK &lt;code&gt;Conv2D/linear&lt;/code&gt; ops, written by the previous pass, and fuses them together.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa93a38017c6f0838a547724d9c4d119cf4afed6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Remote Procedure Call (RPC)&lt;/strong&gt; supports running a function on the specified destination worker with the given arguments and getting the return value back or creating a reference to the return value. There are three main RPC APIs: &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt;&lt;code&gt;rpc_sync()&lt;/code&gt;&lt;/a&gt; (synchronous), &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt; (asynchronous), and &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt;&lt;code&gt;remote()&lt;/code&gt;&lt;/a&gt; (asynchronous and returns a reference to the remote return value). Use the synchronous API if the user code cannot proceed without the return value. Otherwise, use the asynchronous API to get a future, and wait on the future when the return value is needed on the caller. The &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt;&lt;code&gt;remote()&lt;/code&gt;&lt;/a&gt; API is useful when the requirement is to create something remotely but never need to fetch it to the caller. Imagine the case that a driver process is setting up a parameter server and a trainer. The driver can create an embedding table on the parameter server and then share the reference to the embedding table with the trainer, but itself will never use the embedding table locally. In this case, &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt;&lt;code&gt;rpc_sync()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt; are no longer appropriate, as they always imply that the return value will be returned to the caller immediately or in the future.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5314a5e0f78e161be1308e7f0f36425e0e43560f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Remote Reference (RRef)&lt;/strong&gt; serves as a distributed shared pointer to a local or remote object. It can be shared with other workers and reference counting will be handled transparently. Each RRef only has one owner and the object only lives on that owner. Non-owner workers holding RRefs can get copies of the object from the owner by explicitly requesting it. This is useful when a worker needs to access some data object, but itself is neither the creator (the caller of &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt;&lt;code&gt;remote()&lt;/code&gt;&lt;/a&gt;) or the owner of the object. The distributed optimizer, as we will discuss below, is one example of such use cases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ac1d5992a6fca7da1c35caf72d55ee459b83fab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Same dims as t.&lt;/strong&gt; (&lt;em&gt;applied.&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="962e703212f421c013940c950f0179bcfd7e5558" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Scripting a function&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8db690c93c292d8d188cd36e3aadd7bd6d0213c5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Scripting an nn.Module&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a536ecf0100c4415f783288e8b43aa79628874b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;T_0&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of iterations for the first restart.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="174b928ff160a616fa4b90bde1a707315fe9b615" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;T_max&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Maximum number of iterations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e89669f1c5480ecb975d25c72f6bb5f8c0ebc0ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;T_mult&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A factor increases</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf58a23619e37e51e3cff098624c58ed88faeae4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;This should be called at most once, and only from inside the&lt;/strong&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;strong&gt;method.&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a2ef395099e9fb2d32689f2611c9515a234fb1a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;This should be called at most once, only from inside the&lt;/strong&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;strong&gt;method, and all arguments should be inputs.&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="181ffcc34f961e9cbd7992b0842b4c0ee0183d30" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;This should be called at most once, only from inside the&lt;/strong&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;strong&gt;method, and all arguments should be outputs.&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b1571fb48f0f1c3d3c9ec13e2345e4633b9bb38" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;This should be called only from inside the&lt;/strong&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;strong&gt;method&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b683203185b069345fba8db122634cd0d19dfbb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;When automatic batching is disabled&lt;/strong&gt;, &lt;code&gt;collate_fn&lt;/code&gt; is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default &lt;code&gt;collate_fn&lt;/code&gt; simply converts NumPy arrays in PyTorch tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="691205dbb08fc344913e2f7b21842e73d9efda0c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;When automatic batching is disabled&lt;/strong&gt;, the default &lt;code&gt;collate_fn&lt;/code&gt; simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d858374143f1dc8a1f62310d56d15bff7dbf021" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;When automatic batching is enabled&lt;/strong&gt;, &lt;code&gt;collate_fn&lt;/code&gt; is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes behavior of the default &lt;code&gt;collate_fn&lt;/code&gt; in this case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f1531884e296514d90d800035577a8e1accf01f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;When&lt;/strong&gt;&lt;code&gt;as_tuple&lt;/code&gt;&lt;strong&gt;is ``False`` (default)&lt;/strong&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ab18d59c167aab4a342a25877738485f51a7096" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;When&lt;/strong&gt;&lt;code&gt;as_tuple&lt;/code&gt;&lt;strong&gt;is ``True``&lt;/strong&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7078285e2211f504ade0f00ffd2f939c57da6bd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;X&lt;/strong&gt; (&lt;em&gt;tensor&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the input tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1b67d6a132e9c963db47428cb2379e1e546861a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;_extra_files&lt;/strong&gt; &amp;ndash; Map from filename to contents which will be stored as part of &lt;code&gt;f&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a683a25cede5c84fac0359282cc1ba91c60e4ca5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;_extra_files&lt;/strong&gt; (&lt;em&gt;dictionary of filename to content&lt;/em&gt;) &amp;ndash; The extra filenames given in the map would be loaded and their content would be stored in the provided map.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d748636c83b33f2e9575e4d3e73b87eb583016ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;_instance&lt;/strong&gt; &amp;ndash; new instance provided by subclasses that need to override &lt;code&gt;.expand&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e611d50a4e099f661790454d298a621c6db3772" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;a&lt;/strong&gt; &amp;ndash; the lower bound of the uniform distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d6b643d3bc99d8356ba238e8f3bc442ddc42326" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;a&lt;/strong&gt; &amp;ndash; the negative slope of the rectifier used after this layer (only used with &lt;code&gt;'leaky_relu'&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13e08f7f03b31160851ef342f7107a5a0a61cecd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;a&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Left tensor to contract</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8973a7f97fc0beb9adb7e234c6f186d1a27f19c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;abbreviated&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to return an abbreviated summary (default: False).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48c6d9a7b5991de5ceb0f2f0002b9b581b68e5fe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;abs&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The absolute value the complex tensor. Must be float or double.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="769675c7e81eef43f6366734f82732784d9c6c35" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;accumulate&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether to accumulate into self</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06c11a2977c3992817e33b18690cf09337f0cacb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;activation&lt;/strong&gt; &amp;ndash; the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="923b9fe69988df8bf8cfd3d90e121105f87077db" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;activation&lt;/strong&gt; &amp;ndash; the activation function of intermediate layer, relu or gelu (default=relu).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="761a00d8ce0801730293c2567fab0839a725bb91" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;add_bias_kv&lt;/strong&gt; &amp;ndash; add bias to the key and value sequences at dim=0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9907f733a01182c1fbfd28915afc66890b30f3df" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;add_zero_attn&lt;/strong&gt; &amp;ndash; add a new batch of zeros to the key and value sequences at dim=1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25e60582f1deda7c8462bf51f20fc8de0b64b938" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;affine&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c8473b78a9df928d3a9e2a5c428e31960781087" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;affine&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module has learnable affine parameters. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="643d4c7ac1c81e9da7cfa09a73aadce98fb242af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;affine&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="559454a2b31e0657d70c2b51eea7f95541808926" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Geometrically, we consider the pixels of the input and output as squares rather than points. If set to &lt;code&gt;True&lt;/code&gt;, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to &lt;code&gt;False&lt;/code&gt;, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation &lt;em&gt;independent&lt;/em&gt; of input size when &lt;code&gt;scale_factor&lt;/code&gt; is kept the same. This only has an effect when &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;'bilinear'&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36cb44cb5ed893a7388ab1e5bafec71dbcb4bf49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Geometrically, we consider the pixels of the input and output as squares rather than points. If set to &lt;code&gt;True&lt;/code&gt;, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to &lt;code&gt;False&lt;/code&gt;, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation &lt;em&gt;independent&lt;/em&gt; of input size when &lt;code&gt;scale_factor&lt;/code&gt; is kept the same. This only has an effect when &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;'linear'&lt;/code&gt;, &lt;code&gt;'bilinear'&lt;/code&gt;, &lt;code&gt;'bicubic'&lt;/code&gt; or &lt;code&gt;'trilinear'&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cfac0e91a78176be96ddc6ef793ea60d709b2eb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Geometrically, we consider the pixels of the input as squares rather than points. If set to &lt;code&gt;True&lt;/code&gt;, the extrema (&lt;code&gt;-1&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;) are considered as referring to the center points of the input&amp;rsquo;s corner pixels. If set to &lt;code&gt;False&lt;/code&gt;, they are instead considered as referring to the corner points of the input&amp;rsquo;s corner pixels, making the sampling more resolution agnostic. This option parallels the &lt;code&gt;align_corners&lt;/code&gt; option in &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;interpolate()&lt;/code&gt;&lt;/a&gt;, and so whichever option is used here should also be used there to resize the input image before grid sampling. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18bc31d486c6b3f3dc8b60be043591c427b7d925" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, consider &lt;code&gt;-1&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; to refer to the centers of the corner pixels rather than the image corners. Refer to &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; for a more complete description. A grid generated by &lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt;&lt;code&gt;affine_grid()&lt;/code&gt;&lt;/a&gt; should be passed to &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; with the same setting for this option. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7e8f77490dec1d2b29fdbb675f0df9b2418c385" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;'linear'&lt;/code&gt;, &lt;code&gt;'bilinear'&lt;/code&gt;, or &lt;code&gt;'trilinear'&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4550e117269208de394cc0f647e9c83e2e6b506b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;allow_list&lt;/strong&gt; &amp;ndash; list of quantizable modules</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61284d3bd86eb968f4704c5b682d247f9a471d42" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;allow_unused&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;False&lt;/code&gt;, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b7ed549d17ccf06c2a73555f8c110d2b2777c64" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; &amp;ndash; multiplicative factor. Default: 0.0001</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c7e987f8b1b60db336f8816506d789632b3a056" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; &amp;ndash; the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9144dd58af56a7e78f94a1c6d24c0dd775ffa08a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; &amp;ndash; the alpha constant</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d910a02ba4f1d9ff8786a89bc62f31022f71840" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Shape parameter of the distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaed323126d545c766d6b20c8792516dd1012883" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The coefficient</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61214109be5b49c8fe2b57494c2492c111db371d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; power for eta update (default: 0.75)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c0c6834bd8b3a27d9f0cb960885271d801e89bc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; smoothing constant (default: 0.99)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bef472f2822264f8844272a566f6ae7e013eac0f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the scalar multiplier for &lt;code&gt;other&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eafad06cad281ac6182faa727319bec1cdc6e990" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0c3d78d14051f159b2c380fbcdbccf7032cc7c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for &lt;code&gt;batch1 @ batch2&lt;/code&gt; (</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01962c7a9caccb6944e0dd97ce4d52d57152f949" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; the scalar multiplier for &lt;code&gt;other&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65c245be1540fcaab2e6ed5fa6d61bf7293f1117" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;amount&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The quantity by which the counter will be incremented.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a86a8ab17b48cd324b8df6130d66a8bfdd18e3f5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;amount&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; quantity of channels to prune. If &lt;code&gt;float&lt;/code&gt;, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If &lt;code&gt;int&lt;/code&gt;, it represents the absolute number of parameters to prune.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a209b964ca3729de2ffee2abca2f378072e01b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;amount&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; quantity of parameters to prune. If &lt;code&gt;float&lt;/code&gt;, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If &lt;code&gt;int&lt;/code&gt;, it represents the absolute number of parameters to prune.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ce470aadb580314ab69ed9394de23b210bd77c0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;amsgrad&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to use the AMSGrad variant of this algorithm from the paper &lt;a href=&quot;https://openreview.net/forum?id=ryQu7f-RZ&quot;&gt;On the Convergence of Adam and Beyond&lt;/a&gt; (default: False)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="412d2ada083b0f76466ad50fa67e45bff30d28be" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;angle&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The angle of the complex tensor. Must be same dtype as &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe4e9858755e2f94a973c152c11e6edad9884bd7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;anneal_strategy&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; {&amp;lsquo;cos&amp;rsquo;, &amp;lsquo;linear&amp;rsquo;} Specifies the annealing strategy: &amp;ldquo;cos&amp;rdquo; for cosine annealing, &amp;ldquo;linear&amp;rdquo; for linear annealing. Default: &amp;lsquo;cos&amp;rsquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="476bb8fddaba867557b5e140f0e7347d124d9d38" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; Any arguments.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3296f07398c854ae12376a870bec69a75a03688" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; any argument (unused)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58fda4a765460c36ef0809232ca862a52a746df4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; arguments passed on to a subclass of &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cf26c4412d3af55e77c722a46e12a3de8c0a087" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; arguments passed on to a subclass of &lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06452ebbd4e53eb2a57ca1d68b73dffabe3a9e69" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; arguments to pass to the optimizer constructor on each worker.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="527ad6571a32b831a6fa84b982cf26783e444b79" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; tuple containing inputs to the &lt;code&gt;function&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa75d1d4cde2ad42292912372e00091b89a16d9f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; Arguments passed to &lt;code&gt;fn&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbbea77d989716da38960ac97dc3c1b13738a750" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; the argument tuple for the &lt;code&gt;func&lt;/code&gt; invocation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a6ccd94c0969142ac42560e759fc19e3bb7b28e2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; (&lt;em&gt;tuple of arguments&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; the inputs to the model, e.g., such that &lt;code&gt;model(*args)&lt;/code&gt; is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args. If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor. (Note: passing keyword arguments to the model is not currently supported. Give us a shout if you need it.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16fdafe6c67075fc015e420e0bb0f8d630e5f4e5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;as torch.nn.quantized.Conv2d&lt;/strong&gt; (&lt;em&gt;Same&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79c9815e7f5a48dc393a28760411256901b78dc1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;as torch.nn.quantized.Linear&lt;/strong&gt; (&lt;em&gt;Same&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b96cfefc433ac6d54b2e6d75776bf06a2aa2c2ef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;async_op&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Whether this op should be an async op</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ef0166b6388a4f0216cd0af7040aee2dea01134" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;async_op&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Whether this op should be an async op.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c393f84aad8fa71c90593f9038432da8c10f4ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;aten&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset&amp;lt;version&amp;gt;.py are exported as ATen ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1aa89f7b6b3c09ca31403919ee5c7ade6eca50cd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;atol&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; absolute tolerance</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d413d0f3f92358bb21c09129aebaafca7327d9cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;atol&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; absolute tolerance. Default: 1e-08</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3edeccc3509794121a4c8cbb0c9c3947569f0355" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;attn_mask&lt;/strong&gt; &amp;ndash; 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b70ffd8c9aa3ced153863f4307b36931662a4e6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;aux_logits&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, add an auxiliary branch that can improve training. Default: &lt;em&gt;True&lt;/em&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7f0d0ce983d8eb3de6cf4b329489cdf94ceff22" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;aux_logits&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, adds two auxiliary branches that can improve training. Default: &lt;em&gt;False&lt;/em&gt; when pretrained is True otherwise &lt;em&gt;True&lt;/em&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef35e44897a61b67d11a6892ff916c7a17ffe70c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;averaging_constant&lt;/strong&gt; &amp;ndash; Averaging constant for min/max.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7ddd6c4129c6131d8812cc22b9dde600a187a20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;axis&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension on which apply per-channel quantization</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8531d8872ce2920e620f2844e86bbe33088a4efd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;b&lt;/strong&gt; &amp;ndash; the upper bound of the uniform distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8325c83f74f3a97ea28f95055f6ccadb9fe0420" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;b&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Right tensor to contract</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41ffccfc315d025808792ff986c6bf85fc3cfb20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;b&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the RHS tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bedd1fb2f39140f6b07702dfe6d9850270d68d48" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;backend&lt;/strong&gt; &amp;ndash; Device type to use for running the result model (&amp;lsquo;CPU&amp;rsquo;(default) or &amp;lsquo;Vulkan&amp;rsquo;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="857b931bd7b5bfc52eb0de68ac6430dc390f6af7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;backend&lt;/strong&gt; (&lt;a href=&quot;#torch.distributed.rpc.BackendType&quot;&gt;BackendType&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The type of RPC backend implementation. Supported values include &lt;code&gt;BackendType.TENSORPIPE&lt;/code&gt; (the default) and &lt;code&gt;BackendType.PROCESS_GROUP&lt;/code&gt;. See &lt;a href=&quot;#rpc-backends&quot;&gt;Backends&lt;/a&gt; for more information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2deecff901759888bdb71ea34d20b18f5d55de46" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;backend&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;#torch.distributed.Backend&quot;&gt;Backend&lt;/a&gt;) &amp;ndash; The backend to use. Depending on build-time configurations, valid values include &lt;code&gt;mpi&lt;/code&gt;, &lt;code&gt;gloo&lt;/code&gt;, and &lt;code&gt;nccl&lt;/code&gt;. This field should be given as a lowercase string (e.g., &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;), which can also be accessed via &lt;a href=&quot;#torch.distributed.Backend&quot;&gt;&lt;code&gt;Backend&lt;/code&gt;&lt;/a&gt; attributes (e.g., &lt;code&gt;Backend.GLOO&lt;/code&gt;). If using multiple processes per machine with &lt;code&gt;nccl&lt;/code&gt; backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="611838a036cbfc666e9b5fee5ea917b77c462eae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;backend&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;#torch.distributed.Backend&quot;&gt;Backend&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The backend to use. Depending on build-time configurations, valid values are &lt;code&gt;gloo&lt;/code&gt; and &lt;code&gt;nccl&lt;/code&gt;. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;), which can also be accessed via &lt;a href=&quot;#torch.distributed.Backend&quot;&gt;&lt;code&gt;Backend&lt;/code&gt;&lt;/a&gt; attributes (e.g., &lt;code&gt;Backend.GLOO&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd1b20b89d2cc0ec5cb752e68d4f7968fe15164e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;base&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; base of the logarithm function. Default: &lt;code&gt;10.0&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a732d69bc0c02255c6ebdddc229969991439ab4d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;base_distribution&lt;/strong&gt; (&lt;a href=&quot;#torch.distributions.distribution.Distribution&quot;&gt;torch.distributions.distribution.Distribution&lt;/a&gt;) &amp;ndash; a base distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="476a4762eae7b33d8cc5f0ed09807b11d346a607" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;base_lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; Initial learning rate which is the lower boundary in the cycle for each parameter group.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45bfdd8de24999d7830386707112d1d252ec4ede" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;base_momentum&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is &amp;lsquo;base_momentum&amp;rsquo; and learning rate is &amp;lsquo;max_lr&amp;rsquo;. Default: 0.8</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="372f4bc26c77679cdc93407eca357992bf64c343" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;base_momentum&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is &amp;lsquo;base_momentum&amp;rsquo; and learning rate is &amp;lsquo;max_lr&amp;rsquo;. Default: 0.85</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d70c00dc7a0b2333944c557a49e48e57f71ef5e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch1&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first batch of matrices to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6f89ac68899abcdeec75e50915476dc96eff70e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the second batch of matrices to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cf203097252d2e89f058a192c342938374bbb59" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, then the input and output tensors are provided as (batch, seq, feature). Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2bb4ed84deb95504a0fd7b72a8e70cbe91a0c79" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, then the input and output tensors are provided as &lt;code&gt;(batch, seq, feature)&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb58228cb0588bbf40e1aa70f7aa94c4d87966f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the input is expected in &lt;code&gt;B x T x *&lt;/code&gt; format.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97878838dd43a3b22c60de0a798485d3232a77f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the output will be in &lt;code&gt;B x T x *&lt;/code&gt; format.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ed5b9e169e5e04e47a2bda761131d2763152218" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; output will be in &lt;code&gt;B x T x *&lt;/code&gt; if True, or in &lt;code&gt;T x B x *&lt;/code&gt; otherwise</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfc93e33ec4c6682c210e0126029bc47f7bcc35b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_sampler&lt;/strong&gt; (&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;Sampler&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; like &lt;code&gt;sampler&lt;/code&gt;, but returns a batch of indices at a time. Mutually exclusive with &lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;shuffle&lt;/code&gt;, &lt;code&gt;sampler&lt;/code&gt;, and &lt;code&gt;drop_last&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c8f4e7015075fc57b0aafa91c8da1ccf8ea7f6c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_shape&lt;/strong&gt; (&lt;em&gt;torch.Size&lt;/em&gt;) &amp;ndash; the desired expanded size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5f6baf5327e101f2b921c13705c1ef97f75cfc4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Size of mini-batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf77b09b13edd32fcb21c1fcbdf18d8a6268c85e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; how many samples per batch to load (default: &lt;code&gt;1&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a9f6fb92330f139eb9547e65e6c72e563c7f9e2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; &amp;ndash; exponent. Default: 0.75</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef999fec40e863a728ff14d8247d251aee96fa89" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; &amp;ndash; the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cf575cbd58c43c32309649550fcd836702cf2fe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies the threshold at which to change between L1 and L2 loss. This value defaults to 1.0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c471f0e5d8fd5619e33288b2fd6dc3e2c59c406" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The coefficient</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="842828693822b4d9f2aa884edc9269cce4696288" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; shape parameter for the window.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b11887536b01d5ee066bad51f6fc21a475d853cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for &lt;code&gt;input&lt;/code&gt; (</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa1cf47ccd539344b4c2e84e2076311ee96e5b1f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for &lt;code&gt;mat&lt;/code&gt; (</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a24e7da699bda084e89f8e714c763a68a433a209" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;betas&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; coefficients used for computing running averages of gradient and its square</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c878ef998237737758de6fede183a7e8e739ff80" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;betas&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc78c217f09fc1013e5acbefa8a13316ec9a1fca" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; &lt;strong&gt;non-quantized&lt;/strong&gt; bias tensor of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5cf46533d930efcc3fd64418202381a80648d2fc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; If &lt;code&gt;False&lt;/code&gt;, then the layer does not use bias weights &lt;code&gt;b_ih&lt;/code&gt; and &lt;code&gt;b_hh&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a6cf6d24a3d1d4d2e99d86a09905c2dcc844ee4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; If set to &lt;code&gt;False&lt;/code&gt;, the layer will not learn an additive bias. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9166de183aefb5a5420210ed060a3775a566bedc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; If set to False, the layer will not learn an additive bias. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81d19585b9271c2697fbe2d0820cd2cd9a202b52" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; add bias as module parameter. Default: True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1d7f9d42640778760e245fa86bc8ea0c619a3a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; optional bias of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e06c7b1cc26cfb4e243f63f4508fe3ad4aaecc3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; optional bias tensor of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e03a417e197c4f988306955bf8e28f75e1103c81" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, adds a learnable bias to the output. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c804fd0b8536d52a938e789eaf231e2d8488e976" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; None or fp32 bias of type &lt;code&gt;torch.float&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a33731aa2d3126a1f9307fbc74003f4b76cba728" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bidirectional&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, becomes a bidirectional GRU. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4d0b0c0ab949b4fb4278979fa4bb9deb02a9fbe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bidirectional&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, becomes a bidirectional LSTM. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8771da0d807097f9c9e787cd58800e69ca9ba9e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bidirectional&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, becomes a bidirectional RNN. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2bdb7d7d14d67274cdaee544d6984dd9b157097" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bins&lt;/strong&gt; &amp;ndash; Number of bins to use for the histogram</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7eb56a898db0c0dd4ebf0e179f5fd52a33dbbba3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bins&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of histogram bins</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d0079f1170ed85a5972e6fbc1007549726ed554" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bins&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; One of {&amp;lsquo;tensorflow&amp;rsquo;,&amp;rsquo;auto&amp;rsquo;, &amp;lsquo;fd&amp;rsquo;, &amp;hellip;}. This determines how the bins are made. You can find other options in: &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html&quot;&gt;https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be7c2ee257bfd0c3170743cdce62c675328d09dd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;blank&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Blank label. Default</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98d9784c554d3e6070ecb671487ffb5edf2addb7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;blank&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; blank label. Default</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f468597461731cace7790edbc7b181778b52c95e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;blocking&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, &lt;a href=&quot;#torch.cuda.Event.wait&quot;&gt;&lt;code&gt;wait()&lt;/code&gt;&lt;/a&gt; will be blocking (default: &lt;code&gt;False&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="124b59c3f64e2d2878e908f98928989224419b13" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;boundaries&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1-D tensor, must contain a monotonically increasing sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5da31c2835927c49a7bcd45bbe610fbd0fda6d6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;broadcast_buffers&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Flag that enables syncing (broadcasting) buffers of the module at beginning of the &lt;code&gt;forward&lt;/code&gt; function. (default: &lt;code&gt;True&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d5abee92111d698306a568507b4404bca34ef32" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bucket_cap_mb&lt;/strong&gt; &amp;ndash; &lt;code&gt;DistributedDataParallel&lt;/code&gt; will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. &lt;code&gt;bucket_cap_mb&lt;/code&gt; controls the bucket size in MegaBytes (MB). (default: 25)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a33ab96a8acd996a84aba710ffdc9e4c59ed291" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;buffer_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; maximum size of the buffer used for coalescing</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fa537aa1114daae5c6231d8a64de02ef3a1e4f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;build_directory&lt;/strong&gt; &amp;ndash; optional path to use as build workspace.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3141d5b804d7f3a2ff206cba71370ed4fb7cf6b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;c_0&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the initial cell state for each element in the batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cdfc2f84c394419e5070447dbe32bbe5a849b2c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;c_0&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the initial cell state for each element in the batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9b02af500b7cb6986cf95c49ee08bafe6ac6c55" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;c_1&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the next cell state for each element in the batch</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc8b348167534518488e0ae2031d0338daba2877" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;c_n&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the cell state for &lt;code&gt;t = seq_len&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f3d6a6619594dae9f728601472ac26049551428" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cache_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3afc2463b07a6cb3c6c5f91f7a5a9121d9323c6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;callback&lt;/strong&gt; (&lt;code&gt;Callable&lt;/code&gt;) &amp;ndash; a &lt;code&gt;Callable&lt;/code&gt; that takes this &lt;code&gt;Future&lt;/code&gt; as the only argument.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="158dc34f4b85c715ce37b97b602ff17aa31fafed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cast_inputs&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt; or None, optional, default=None) &amp;ndash; If not &lt;code&gt;None&lt;/code&gt;, when &lt;code&gt;forward&lt;/code&gt; runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes &lt;code&gt;forward&lt;/code&gt; with autocast disabled. If &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;forward&lt;/code&gt;&amp;rsquo;s internal ops execute with the current autocast state.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a50ab6a8fd5d6ca7187e241b339e3ce04e06ad53" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8f8c031ecdef7a290512f2d2c0f3fb9843f10ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; when True, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; in the formula to compute the output shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4607006703c79ed95d46d4bdf76b0c53e13b775b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; when True, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; in the formula to compute the output shape. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11fbe0576979d5cf6a08896bdf3211baf5a56d94" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; when True, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; to compute the output shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b293b825d6cac1646ea030a31f09a3ccbb476cad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; when True, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; to compute the output shape. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39ee4edbfa46d12a64c7dc2b9fa7acda768a096e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;center&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether &lt;code&gt;input&lt;/code&gt; was padded on both sides so that the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="554e667848e98458fc2cceae44866e4bc318c449" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;center&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if True, center the input tensor, otherwise, assume that the input is centered.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9f7299f6837ef46cae44a83afb72d7f43c0f17b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;center&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to pad &lt;code&gt;input&lt;/code&gt; on both sides so that the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e60f9d3649e78b261bdfd669e6452599a42743a3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;centered&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, compute the centered RMSProp, the gradient is normalized by an estimation of its variance</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4ea6bf9c1eff8d3014f59d1fd1a0f38423eba97" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ch_axis&lt;/strong&gt; &amp;ndash; Channel axis</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="901e29d2efd439abae8b39739b45f91fde3c2e1f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_hash&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If True, the filename part of the URL should follow the naming convention &lt;code&gt;filename-&amp;lt;sha256&amp;gt;.ext&lt;/code&gt; where &lt;code&gt;&amp;lt;sha256&amp;gt;&lt;/code&gt; is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef5504f62a92974b3ed5764b39be00b40da7ec96" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_inputs&lt;/strong&gt; (&lt;em&gt;list of dicts&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A list of dicts of input arguments that should be used to check the trace against what is expected. Each tuple is equivalent to a set of input arguments that would be specified in &lt;code&gt;inputs&lt;/code&gt;. For best results, pass in a set of checking inputs representative of the space of shapes and types of inputs you expect the network to see. If not specified, the original &lt;code&gt;inputs&lt;/code&gt; are used for checking</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c341222b235229269c7fc1f716c9a876b11bd749" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_inputs&lt;/strong&gt; (&lt;em&gt;list of tuples&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A list of tuples of input arguments that should be used to check the trace against what is expected. Each tuple is equivalent to a set of input arguments that would be specified in &lt;code&gt;example_inputs&lt;/code&gt;. For best results, pass in a set of checking inputs representative of the space of shapes and types of inputs you expect the network to see. If not specified, the original &lt;code&gt;example_inputs&lt;/code&gt; are used for checking</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5be047e7257e74bb986cba2cbddd77287afb54a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_reduction&lt;/strong&gt; &amp;ndash; This argument is deprecated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71817e8aa767be0ecdbf86860b68e14e0b17f973" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_sparse_nnz&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b901bf06f9dd61829b7a11e137cee91046aa7d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_tolerance&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Floating-point comparison tolerance to use in the checker procedure. This can be used to relax the checker strictness in the event that results diverge numerically for a known reason, such as operator fusion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d4b51194efc0c3988804d99693826828fb517f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_trace&lt;/strong&gt; (&lt;code&gt;bool&lt;/code&gt;, optional) &amp;ndash; Check if the same inputs run through traced code produce the same outputs. Default: &lt;code&gt;True&lt;/code&gt;. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12c8d96002100fa26d3949cd31bda14104f0723b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_undefined_grad&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;options&lt;/em&gt;) &amp;ndash; if True, check if undefined output grads are supported and treated as zeros</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2a343bee8eb1432764513dd55cca2fafc51d5a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;chunk_sizes&lt;/strong&gt; (&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; sizes of chunks to be placed on each device. It should match &lt;code&gt;devices&lt;/code&gt; in length and sums to &lt;code&gt;tensor.size(dim)&lt;/code&gt;. If not specified, &lt;code&gt;tensor&lt;/code&gt; will be divided into equal chunks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b22f20f0f0764fbf8ab51098534c41733587e08" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;chunks&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of chunks to return</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="768622c9f916e93f3f54009c2cae67ec11e32f7c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;clip_value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; maximum allowed value of the gradients. The gradients are clipped in the range</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="469c227d8924eacf04bdee3cc8cf8084affddf1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;close&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Flag to automatically close the figure</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b862e2504262c8b48874fae9e7b1554d76bea967" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;closure&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;) &amp;ndash; A closure that reevaluates the model and returns the loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dfbd8eaebd57f4eb3b5ef5282a6d03a48f71f6f4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;closure&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;) &amp;ndash; A closure that reevaluates the model and returns the loss. Optional for most optimizers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3398e0f638605e68fc3a97d79281029232e5345" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;closure&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A closure that reevaluates the model and returns the loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf87633936d5aa46fae578ecedd6b3692d516d64" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;col&lt;/strong&gt; (&lt;code&gt;int&lt;/code&gt;) &amp;ndash; number of columns in the 2-D matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc7488927fd2aeb973d91ad0ef3a00f2bc4856a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;collate_fn&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a942bb7a89546ea8db2e80c5772f25d6c0dc388d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;colors&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Colors for each vertex</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af25ed4733fad11d3e062d79140e6b8c23a8a955" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;comment&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Comment log_dir suffix appended to the default &lt;code&gt;log_dir&lt;/code&gt;. If &lt;code&gt;log_dir&lt;/code&gt; is assigned, this argument has no effect.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88100fd7e4f3c66560dd9b530a7164b8a95efdd5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;compiler&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The compiler executable name to check (e.g. &lt;code&gt;g++&lt;/code&gt;). Must be executable in a shell process.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce333629967f987f8531534ac06dca279c4f44ff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;component_distribution&lt;/strong&gt; &amp;ndash; &lt;code&gt;torch.distributions.Distribution&lt;/code&gt;-like instance. Right-most batch dimension indexes component.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1808bd911df345d583c080a56c28591067ce66ed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;compute_mode&lt;/strong&gt; &amp;ndash; &amp;lsquo;use_mm_for_euclid_dist_if_necessary&amp;rsquo; - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P &amp;gt; 25 or R &amp;gt; 25 &amp;lsquo;use_mm_for_euclid_dist&amp;rsquo; - will always use matrix multiplication approach to calculate euclidean distance (p = 2) &amp;lsquo;donot_use_mm_for_euclid_dist&amp;rsquo; - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b7321414a4a3b3cf3f67b84bd0c573e753c0ef0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;compute_uv&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; option whether to compute &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; or not</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29c6932263dd36cb2b649a7c4c8e4f226a6f5bc4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;concentration0&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 2nd concentration parameter of the distribution (often referred to as beta)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a9195b109082c62a5572bf77fe482e738e28674" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;concentration1&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1st concentration parameter of the distribution (often referred to as alpha)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5a7d26c8f7b3912f3a21aa42dedc38581ee8d66" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;concentration&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Concentration parameter of distribution (k/shape).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="984ded62b32cbb90dd847390d944952e565ae495" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;concentration&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; shape parameter of the distribution (often referred to as alpha)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7303d23f33521283575c763715361627138a4d0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;concentration&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; concentration parameter of the distribution (often referred to as alpha)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12b35f47cab96aa46ad0ddf8952f6968e19886f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;concentration&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; concentration parameter</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0d3ed0a4c0c3905d8f35b9489a4566e61f37346" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;condition&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.BoolTensor&quot;&gt;BoolTensor&lt;/a&gt;) &amp;ndash; When True (nonzero), yield x, otherwise yield y</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb57365756ae0cedf43c6c254c4afd0d2edee70b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;config_dict&lt;/strong&gt; &amp;ndash; Dictionary with ThreeJS classes names and configuration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fb4bebc6aedd7efbc4b85c9b49be4e8e72168db" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;constraint&lt;/strong&gt; (subclass of &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt;) &amp;ndash; A subclass of &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt;, or a singleton object of the desired class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4a1732a0f202d34ba93b1eb562af5be27bdd650" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;context_id&lt;/strong&gt; &amp;ndash; the autograd context id for which we should run the optimizer step.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d5613a1e6301637797b5ae54f8043e61724838c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;context_id&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The autograd context id for which we should retrieve the gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7fa79442f8c4c5b203a624ad01080616555143b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cooldown&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of epochs to wait before resuming normal operation after lr has been reduced. Default: 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c83c6134b0a9c7b9806c160fd52a9c3b5ea1fc9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;count_include_pad&lt;/strong&gt; &amp;ndash; when True, will include the zero-padding in the averaging calculation</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df61d9b0480a95d8ea4a65fcfd105e2c43cf5683" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;count_include_pad&lt;/strong&gt; &amp;ndash; when True, will include the zero-padding in the averaging calculation. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f406dbb58fb0db000240174c73ab3118ad8b663f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;counts&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): (optional) if &lt;code&gt;return_counts&lt;/code&gt; is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="721a11304d1853b3ab9748934df7f4a9fd94d7f7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cov_diag&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; diagonal part of low-rank form of covariance matrix with shape &lt;code&gt;batch_shape + event_shape&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ade6f9899c6b2f63d39e2774c383175ef06c9f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cov_factor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; factor part of low-rank form of covariance matrix with shape &lt;code&gt;batch_shape + event_shape + (rank,)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15c0f21c71f81443b531dc0333c4aa0c9ccb22d7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;covariance_matrix&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; positive-definite covariance matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88193fd13f9166e9983ecc78ecd953269b1f995e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cpp_sources&lt;/strong&gt; &amp;ndash; A string, or list of strings, containing C++ source code.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db9eaf7dd847ebbdf9223f162c6efa88d22df730" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;create_graph&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, both the output and result will be computed in a differentiable way. Note that when &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the result can not require gradients or be disconnected from the inputs. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0591761d9c26012b62edfc0b51c0801c2b3c9cac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;create_graph&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1df45c030ada5dacf28732656dae67b769b52be4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;create_graph&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74bd210617e0c27abaf4e4fd33cd8b31f80e3875" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;create_graph&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, the Hessian will be computed in a differentiable manner. Note that when &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the result can not require gradients or be disconnected from the inputs. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30f1dcaaf70a34ead05956232ad7838aced8b6d1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;create_graph&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, the Jacobian will be computed in a differentiable manner. Note that when &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the result can not require gradients or be disconnected from the inputs. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff1cc36eddc3b5f962bb282dc8359cf8c6757bb3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cuda&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, includes CUDA-specific include paths.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cc28cf38c3a669932c9eedb3625ab6b9d7054e3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cuda_sources&lt;/strong&gt; &amp;ndash; A string, or list of strings, containing CUDA source code.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e54fa7d57ec695fdbf3ba8aa44c0d4c97f847f91" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;custom_decoder&lt;/strong&gt; &amp;ndash; custom decoder (default=None).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ca74733e312c67f2657fe95d1e444ee05127650" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;custom_encoder&lt;/strong&gt; &amp;ndash; custom encoder (default=None).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef960edd24412f8d85098fcead2e50c82545528c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;custom_op_name&lt;/strong&gt; &amp;ndash; (temporary) specify this observer for an operator that doesn&amp;rsquo;t require any observation (Can be used in Graph Mode Passes for special case ops).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7780ca36c95ae6da024d6000ca0554d9dfacacc3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;custom_opsets&lt;/strong&gt; (&lt;em&gt;dict&amp;lt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;int&amp;gt;&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default empty dict&lt;/em&gt;) &amp;ndash; A dictionary to indicate custom opset domain and version at export. If model contains a custom opset, it is optional to specify the domain and opset version in the dictionary: - KEY: opset domain name - VALUE: opset version If the custom opset is not provided in this dictionary, opset version is set to 1 by default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9101a0b9856e2d4350718d233a9dcae3bf9c6f6c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cutoffs&lt;/strong&gt; (&lt;em&gt;Sequence&lt;/em&gt;) &amp;ndash; Cutoffs used to assign targets to their buckets</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="431606af74f6bc090820c363256988b947cbb9c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cycle_momentum&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, momentum is cycled inversely to learning rate between &amp;lsquo;base_momentum&amp;rsquo; and &amp;lsquo;max_momentum&amp;rsquo;. Default: True</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52d9d339631301adfaf686280dfd334d7edccba5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;) &amp;ndash; the floating point dtype to make the default</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13964b8d48f242ad146568039b158f92e2df74ae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/a&gt;) &amp;ndash; If True, force operations to be deterministic. If False, allow non-deterministic operations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3da5f1488cc8bd47b1fccb5b95f3073259325116" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; path to a local folder to save downloaded models &amp;amp; weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16b912c0275c25fd63373867599edc3ab633da37" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d_model&lt;/strong&gt; &amp;ndash; the number of expected features in the encoder/decoder inputs (default=512).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15d1f07b9f1bfeb8b51357949749ed27cc1e3a54" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d_model&lt;/strong&gt; &amp;ndash; the number of expected features in the input (required).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4f81741dff36d8528cc5514641a2dfac6bd1314" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;daemon&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; The spawned processes&amp;rsquo; daemon flag. If set to True, daemonic processes will be created.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="943dcf17840c35c0ee7c4bb503e78210818b2c15" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dampening&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; dampening for momentum (default: 0)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1751a1c65282c1b2a3f97ccf6353f71f8348524a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;data&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; parameter tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="002b70001aea1b9d2d370c56a295bc0004d40317" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;data&lt;/strong&gt; (&lt;em&gt;array_like&lt;/em&gt;) &amp;ndash; Initial data for the tensor. Can be a list, tuple, NumPy &lt;code&gt;ndarray&lt;/code&gt;, scalar, and other types.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="057f28def548d5eeb6eab40e11d3fc6379b98957" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;data&lt;/strong&gt; (&lt;em&gt;array_like&lt;/em&gt;) &amp;ndash; The returned Tensor copies &lt;code&gt;data&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="495c7caaee4a75dcd9104fe7126eb6bcb5260cb4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;data_source&lt;/strong&gt; (&lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt;Dataset&lt;/a&gt;) &amp;ndash; dataset to sample from</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6cb5164bb06d6da92431aba395fa788fafdd491" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dataformats&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c82013a890247925e2e1ad309f2b481e828a51d2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dataset&lt;/strong&gt; &amp;ndash; Dataset used for sampling.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b27874c3d3a54e6026c9501857edee91c6f3d84c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dataset&lt;/strong&gt; (&lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt;Dataset&lt;/a&gt;) &amp;ndash; Dataset to be split</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28a474719d5dfc1b87e383e8b939d2cee605e0b6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dataset&lt;/strong&gt; (&lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt;Dataset&lt;/a&gt;) &amp;ndash; The whole Dataset</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c82d469055b33fc7c758db3c0e18df50ff5c1c68" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dataset&lt;/strong&gt; (&lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt;Dataset&lt;/a&gt;) &amp;ndash; dataset from which to load the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2821891aec8078ce9f410f88ee8adc0c1d56ac2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;datasets&lt;/strong&gt; (&lt;em&gt;iterable of IterableDataset&lt;/em&gt;) &amp;ndash; datasets to be chained together</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca48c96b9d7b4ef3f9a7801fcc260a515ab45c5e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;datasets&lt;/strong&gt; (&lt;em&gt;sequence&lt;/em&gt;) &amp;ndash; List of datasets to be concatenated</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8a0c46d88dac2ffef33068f3be232119d9b9ac2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;decoder_layer&lt;/strong&gt; &amp;ndash; an instance of the TransformerDecoderLayer() class (required).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6debf1cc5a394619a93016d7ca366d8f9a4f042f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;default_mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Base mask from previous pruning</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02df0d48c6b8036586e726b50a5fbc9b6e1751a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;default_mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as &lt;code&gt;t&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5cf70b407f88539d3c6c856333c142253f1a21c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;default_mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; mask from previous pruning iteration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25982b74713391159075b7c31f6a1999d645d6fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;default_mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11ad1e4667cafb8dae7d5e1e9a836f9d6bec1dbf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;defaults&lt;/strong&gt; &amp;ndash; (dict): a dict containing default values of optimization options (used when a parameter group doesn&amp;rsquo;t specify them).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2c8995ad4fd81b344f11dc7dbecd0ebd8ded8d5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;descending&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls the sorting order (ascending or descending)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de9594f438e4acefd09174ebe985fcf079eb149b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;destination&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; Destination positions for each of the original dims. These must also be unique.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad7a6d6fbe793b5fadf8461e7bff610a016520c3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;destination&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a device on which the output will be placed (default: current device).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cda7609bd326665fc6967cfb39795fe07d31662" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;destination&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output device. Can be CPU or CUDA. Default: the current CUDA device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b43c4caf374bfe5b5468766c5d19925926c2fdd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;deterministic&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; flag to choose between a faster non-deterministic calculation, or a slower deterministic calculation. This argument is only available for sparse-dense CUDA bmm. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fc56af4e93713b99d1309a3048de2e1c75734c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; &amp;ndash; parent device, if any</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67ab079c226c54c3f26e4ea0b6f9a1108b087a49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device for the generator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a7f07799c9cad3af3cab4816607f53086fcc521" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, defaults to the device of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aed780ae1f47b9761fbfa7992f19ff1ad4f3e5e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, uses the current device for the default tensor type (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;). &lt;code&gt;device&lt;/code&gt; will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f17f4ea64a618a5f0fee4c7ea43e579c17cf49f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;). &lt;code&gt;device&lt;/code&gt; will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf93823dec63265c654765453f246b1d85c3b014" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The destination GPU id. Defaults to the current device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d93e973425e80658411f9bf156583443e90dc63" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if specified, all parameters will be copied to that device</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a198ab22b8b6fb2b268e0d3eed8f405f9fdfeffb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;) &amp;ndash; The destination GPU device. Defaults to the current CUDA device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f538c82c744a54f89979310b11620a36e4dfdc67" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device of returned tensor. Default: if None, same &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc2a360e77715b27e5f7a225d2304eaa4a281461" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; device index to select. It&amp;rsquo;s a no-op if this argument is a negative integer or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91dd5a21f322151d390c6042702ff47f9a8ffd50" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; selected device. This function is a no-op if this argument is negative.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35afd1472a6f1ba7d14224aac975eefd6b6cd2c5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The device to return the RNG state of. Default: &lt;code&gt;'cuda'&lt;/code&gt; (i.e., &lt;code&gt;torch.device('cuda')&lt;/code&gt;, the current CUDA device).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="524e3a5f4f0a371df8721e9a4a00a6dffbe17aeb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The device to set the RNG state. Default: &lt;code&gt;'cuda'&lt;/code&gt; (i.e., &lt;code&gt;torch.device('cuda')&lt;/code&gt;, the current CUDA device).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="482efc8706169d17d3519e5e1b7705602388b088" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a device on which to allocate the stream. If &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt; (default) or a negative integer, this will use the current device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6a7697a4d63a3d93b6fa7b16f26465afb5438a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by &lt;a href=&quot;#torch.cuda.current_device&quot;&gt;&lt;code&gt;current_device()&lt;/code&gt;&lt;/a&gt;, if &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt; (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b86cc0c543099aaa02324c1e0c85154f5ce025b7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by &lt;a href=&quot;#torch.cuda.current_device&quot;&gt;&lt;code&gt;current_device()&lt;/code&gt;&lt;/a&gt;, if &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt; (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91c6dae608999a0395dd544cb7f66f964ae5f6a1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; device for which to synchronize. It uses the current device, given by &lt;a href=&quot;#torch.cuda.current_device&quot;&gt;&lt;code&gt;current_device()&lt;/code&gt;&lt;/a&gt;, if &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt; (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b7a86c4ef24d2162b92ad5494ba8524c471e009" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; selected device. Returns printout for the current device, given by &lt;a href=&quot;#torch.cuda.current_device&quot;&gt;&lt;code&gt;current_device()&lt;/code&gt;&lt;/a&gt;, if &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt; (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a31fcd7dc7985d0551b476dee1e819d3c43477ce" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; selected device. Returns statistic for the current device, given by &lt;a href=&quot;#torch.cuda.current_device&quot;&gt;&lt;code&gt;current_device()&lt;/code&gt;&lt;/a&gt;, if &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt; (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e34164a99196939615beedb15b9c363033a2cce6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; selected device. Returns statistics for the current device, given by &lt;a href=&quot;#torch.cuda.current_device&quot;&gt;&lt;code&gt;current_device()&lt;/code&gt;&lt;/a&gt;, if &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt; (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2d3cbbf169d3884e013d34bd1d45cb9c47424c5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; selected device. Returns the currently selected &lt;a href=&quot;#torch.cuda.Stream&quot;&gt;&lt;code&gt;Stream&lt;/code&gt;&lt;/a&gt; for the current device, given by &lt;a href=&quot;#torch.cuda.current_device&quot;&gt;&lt;code&gt;current_device()&lt;/code&gt;&lt;/a&gt;, if &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt; (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f23edffe074fe5929a6957bc59ba6621e365b5d4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; selected device. Returns the default &lt;a href=&quot;#torch.cuda.Stream&quot;&gt;&lt;code&gt;Stream&lt;/code&gt;&lt;/a&gt; for the current device, given by &lt;a href=&quot;#torch.cuda.current_device&quot;&gt;&lt;code&gt;current_device()&lt;/code&gt;&lt;/a&gt;, if &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt; (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16a3ba684b70277c97f3887a32acb68677e89ab5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;code&gt;torch.device&lt;/code&gt;) &amp;ndash; the desired device of the parameters and buffers in this module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c13cbea5fe853ad61c4e5b6bdc300c90af893ea8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device_ids&lt;/strong&gt; (&lt;em&gt;list of python:int&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; CUDA devices (default: all devices)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="784500a88b749be8a97a2a8383bb392bde957899" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device_ids&lt;/strong&gt; (&lt;em&gt;list of python:int&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; CUDA devices. This should only be provided when the input module resides on a single CUDA device. For single-device modules, the i&amp;rsquo;th &lt;code&gt;module&lt;/code&gt; replica is placed on &lt;code&gt;device_ids[i]&lt;/code&gt;. For multi-device modules and CPU modules, &lt;code&gt;device_ids&lt;/code&gt; must be &lt;code&gt;None&lt;/code&gt; or an empty list, and input data for the forward pass must be placed on the correct device. (default: all visible devices for single-device modules)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0098733514e18323929fd4ffb49500d3946fa6b3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device_ids&lt;/strong&gt; (&lt;em&gt;list of python:int&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; GPU ids on which to replicate module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec258b8275014fd9f0d8c348dbf61e0048a9ad02" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;devices&lt;/strong&gt; (&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; an iterable of GPU devices, among which to broadcast.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7e837b097fad62cc530d45bef4c54d08f43ec4b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;devices&lt;/strong&gt; (&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; an iterable of GPU devices, among which to broadcast.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd2dc216feecb49d7c0df172557524a1ed810081" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;devices&lt;/strong&gt; (&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; an iterable of GPU devices, among which to scatter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0c9e9433e72da7d6b655bd1674af1bc6aa91348" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;devices&lt;/strong&gt; (&lt;em&gt;iterable of CUDA IDs&lt;/em&gt;) &amp;ndash; CUDA devices for which to fork the RNG. CPU RNG state is always forked. By default, &lt;a href=&quot;#torch.random.fork_rng&quot;&gt;&lt;code&gt;fork_rng()&lt;/code&gt;&lt;/a&gt; operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe51b377539155eb88fd8b7d5e4e63c39f630605" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;df1&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; degrees of freedom parameter 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7e25e21d9c9f6a5a60f7b761df6e51f483ac598" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;df2&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; degrees of freedom parameter 2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26b0cdb742dd7e235595d1a04a5c3e4195cfa892" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;df&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; degrees of freedom</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfb7036f636ca1fdb18517254d9eaf9b37f51ea4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;df&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; shape parameter of the distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59e24ff6691679f07576485b92e551a387011363" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;diagonal&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the diagonal to consider</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98d2682d358cdef81c41f9d22f56660c98f0fea5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; The stride between elements within a sliding window, must be &amp;gt; 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a8bcc8e1ce88b2cf0668486e8d5ccbcb8a4d458" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; a parameter that controls the stride of elements in the window</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8745aafd934cfa2f8a27f7eeb58f0f04b1c8c6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a one-element tuple &lt;code&gt;(dW,)&lt;/code&gt;. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4f0efe675d23b1d986e23b22d467d33af93c1ce" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a tuple &lt;code&gt;(dD, dH, dW)&lt;/code&gt;. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5cc549246c32f7e86fe8237813b71d731de4aeab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a tuple &lt;code&gt;(dH, dW)&lt;/code&gt;. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="717cd865c4cf98b3ffbd6b2307f6f4dbaebad0c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a tuple &lt;code&gt;(dT, dH, dW)&lt;/code&gt;. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ca358d114492e5781f3c8bd655869b64df6739b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a tuple &lt;code&gt;(dW,)&lt;/code&gt;. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6d5af4910c01ee195d562c6b5a6bb2027740a67" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Spacing between kernel elements. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="abdfdeace9aacba2e41cd643c5ea0e49e917e957" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a parameter that controls the stride of elements within the neighborhood. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="472af6ce02e71caf282798bb447e00270b529b2e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim0&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the first dimension to be transposed</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f765d8f8c40b94958f1eab1cc37007590593d31d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim1&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the second dimension to be transposed</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba65b006661843062287a94ba12a78ac85885543" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim1&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; first dimension with respect to which to take diagonal. Default: -2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0dd1507c4f8260c289a9910df29cf487d663e95d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim1&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; first dimension with respect to which to take diagonal. Default: 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83aa8a523aec2c1ab6551640eaf4cb51b830a4c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim2&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; second dimension with respect to which to take diagonal. Default: -1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a71ef13a1ad5cb6ea968b4d4719be6d5961ef225" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim2&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; second dimension with respect to which to take diagonal. Default: 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9b4a2ca457fba9669e7f8ced2b7ed6c30268fd6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which LogSoftmax will be computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34032d1dca3a055630ebba7923bebf4f47a22c38" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which Softmax will be computed (so every slice along dim will sum to 1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01189dd75e2eefc978065b5ff2c0b7f87388d39e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which Softmin will be computed (so every slice along dim will sum to 1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="842005dbd132f5ef7d384c1fccbadec279da8f7c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which log_softmax will be computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb8ee0a50b62dc6ae83f365bac143af1c17f1fd6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which softmax will be computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01b7d7748905016190ad5e2bbe8293a9c597a66c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which softmax will be computed. Default: -1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2777bb64e36f761c1f00ee5be70a591e4cd2145d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which softmin will be computed (so every slice along dim will sum to 1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88bb112f612fa4c72a0f99f7a672d75b2554455b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The dimension along which to integrate. By default, use the last dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90f0ee8c3c7b4d5b8a5616f19077bcfded77ea98" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension along which to index</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8ce25c1fd3bbad54cc79c92db33a7f9d23196c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension along which to split the tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a9295ba14ea4badc07f27ea8db556fe8e515b0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension along which to split the tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a971edb34aad20abc50873c653c145b3af96b2a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension on which to split the input. Default: -1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf3801597d3facaa2cf443350498f58cd1324abb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e66e4cf2db711439adae127683b487fd7e64123" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension to remove</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c917eaab2e1d73b163be73789e03b1bc35b48a89" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; index of the dim along which we define channels to prune.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7285e77d113e976c86c60be543b1f43be38c5bdd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the axis along which to index</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e017cd401e156f255b917cbb7d73173fc5184a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension along which to narrow</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7415821de8da0f78deed554d7e45d50759c33f0b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension in which we index</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be8bc5d715865fc115635841b9517c75441ff8f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to apply unique. If &lt;code&gt;None&lt;/code&gt;, the unique of the flattened input is returned. default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4849740fe6ccbd59b0ec458f024b413bf6df7857" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to do the operation over</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9dfe268e0c2ab5c49cd99f8395aef6a384d8dd63" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0cd1da8b56960f63a95d25cf0421a937673e2ed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8d7f0790b12bc35fd47df215177efe265599f93" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04661f26e0a90dbee7adeed063722879cc2dbedc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce. If &lt;code&gt;None&lt;/code&gt;, the argmax of the flattened input is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="406a26e14fe614e1dacddd572bc1dbd6bd5aa6a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce. If &lt;code&gt;None&lt;/code&gt;, the argmin of the flattened input is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3e09769491b63675c94e11c9dea885116b4a2fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to slice</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8317062bcf63e96ea89234edaf5254e1da5bcb47" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to slice over to get the sub-tensors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c54f0ef48043a5bec2dfca3a0f530fb8661be4ca" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the index at which to insert the singleton dimension</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d89796e28feb4305a6e51573710dca2759c32ff1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; a dimension or a list of dimensions to reduce. Default: reduce over all dims.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1addd18afe889496111cce39db43031d3782741" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; the dimension or dimensions to reduce.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74f088f545a1351da755347a9f47abc2c5b4f78d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dim or tuple of dims along which to count non-zeros.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ea1a19f10e224b77d5a38dec1768437b4ef777c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-tuple of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-list of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;dim&lt;/code&gt; is an int, vector norm will be calculated over the specified dimension. If &lt;code&gt;dim&lt;/code&gt; is a 2-tuple of ints, matrix norm will be calculated over the specified dimensions. If &lt;code&gt;dim&lt;/code&gt; is None, matrix norm will be calculated when the input tensor has two dimensions, and vector norm will be calculated when the input tensor has one dimension. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b085e1cc1d870668a83baea40b3506508d9b0a8b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-tuple of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-list of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2bf5f25244b6a84fc607cb219deedd353fc0f81e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A dimension along which to chunk &lt;code&gt;tensor&lt;/code&gt;. Default: &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39551d8784237f98effe67d5d4fa079ab6e2d66d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimension of vectors. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c530107de3a1e8849bcffa15e8fece38f241cc6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimension where cosine similarity is computed. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9439b0c674bc7c434adc98d580848f39ed9d115e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to repeat values. By default, use the flattened input array, and return a flat output array.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c36ee04034363ccdd358a7c5f8322922bf781fba" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional FFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45818af3fdbc4684e20729f70925408319f7b2e1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional Hermitian FFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e290320ee25d598ee056c54b55e12ed854c6dd23" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional Hermitian IFFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5f98ff50ef003b2c77b0c55e3bf6a58a74600f0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional IFFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4bb3ea939e2fa64fe7995582d47d53ee2e17ec4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional real FFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dea0a42312b17fe5fe3428922ce27d374ec46837" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional real IFFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e62a9f3a5435dd4296d863ba5100409f37e345d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a dimension along which the tensors will be concatenated. Default: &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d32f4b0a48eae0a94c40f8913aaafd8fc5b4e99b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; dimension corresponding to number of outputs, the default is &lt;code&gt;0&lt;/code&gt;, except for modules that are instances of ConvTranspose{1,2,3}d, when it is &lt;code&gt;1&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ff766b40b5bcbfe08f64170324fb2f4d6bf84af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; dimension over which to compute the norm</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a409409533f64a991397140f0cfeef5685c9fa1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if given, the input will be squeezed only in this dimension</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05043e9f4aa7115561003ddcb8522351f1c5f51e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; index of the dim along which we define channels to prune. Default: -1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4945ffee136e1eca9138593201351feb8ce974a6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the desired dimension in which stride is required</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd4c3985b4741ed95097721d4f15edc8c371b9f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension over which the tensors are concatenated</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9ff0c74674eab95e6f6442b562b6f45f41a7a9d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension to find the kth value along</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9422d28d87b196679cbb0c82259c774161568b6b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension to sort along</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28c1fc242c3e6d093897709fa5e6ddd3f6f9dd3b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension to take the cross-product in.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa74b5fd1d3a59aa2fc20f8e622f3d996cbd575d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimensions to be transformed. Default: all dimensions, or the last &lt;code&gt;len(s)&lt;/code&gt; dimensions if &lt;code&gt;s&lt;/code&gt; is given.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="714e04567ca514a499d3251ac5bce29de688ec57" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: all dimensions, or the last &lt;code&gt;len(s)&lt;/code&gt; dimensions if &lt;code&gt;s&lt;/code&gt; is given.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c73040a5128f371bcddeab377f515339135506e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Union&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Dimension to be unflattened</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="612979d2207bccfbcbdc71c971749078ecad7740" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Union&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Dimension to unflatten</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9f96b0e996218b63b5f6697be8d86971ef5ef58" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim_feedforward&lt;/strong&gt; &amp;ndash; the dimension of the feedforward network model (default=2048).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61d8fc5ddd343a37fbd282f3894946f478ef2ebb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dimension&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension in which unfolding happens</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3b04003348060ae8bd467525af95608436f8504" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dimension&lt;/strong&gt; (&lt;em&gt;Int&lt;/em&gt;) &amp;ndash; The dimensionality of the sequence to be drawn</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d084aae186e1378df8faf988ddfde110dfb17385" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dims&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; Axis along which to roll</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c317ee80d0d1d374392208d23e06bdc1d0a08445" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dims&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of two lists of python:integers&lt;/em&gt;) &amp;ndash; number of dimensions to contract or explicit lists of dimensions for &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; respectively</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="391fb595ce6469da76a43990e1b4fedec4415935" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dims&lt;/strong&gt; (&lt;em&gt;a list&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; axis to flip on</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa53309859ad0bb426190a8468616e70a2c2d3ec" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dims&lt;/strong&gt; (&lt;em&gt;a list&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; axis to rotate</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b48119f623a86e0e73cdb61d4b28812b330622a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;distance_function&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A nonnegative, real-valued function that quantifies the closeness of two tensors. If not specified, &lt;code&gt;nn.PairwiseDistance&lt;/code&gt; will be used. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac42c880b1e94d04cad8388a60263bbcb36b448f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;div_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Determines the initial learning rate via initial_lr = max_lr/div_factor Default: 25</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9fdd902ea3281b80f4ee9c58cfabafd6bc9880d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;div_value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; value used as an exponent to compute sizes of the clusters. Default: 4.0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ed00f6805ef09c53fd3940b48ac159868ff1969" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;divide_by_initial_world_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, will divide gradients by the initial &lt;code&gt;world_size&lt;/code&gt; DDP training was launched with. If &lt;code&gt;False&lt;/code&gt;, will compute the effective world size (number of ranks that have not depleted their inputs yet) and divide gradients by that during allreduce. Set &lt;code&gt;divide_by_initial_world_size=True&lt;/code&gt; to ensure every input sample including the uneven inputs have equal weight in terms of how much they contribute to the global gradient. This is achieved by always dividing the gradient by the initial &lt;code&gt;world_size&lt;/code&gt; even when we encounter uneven inputs. If you set this to &lt;code&gt;False&lt;/code&gt;, we divide the gradient by the remaining number of nodes. This ensures parity with training on a smaller &lt;code&gt;world_size&lt;/code&gt; although it also means the uneven inputs would contribute more towards the global gradient. Typically, you would want to set this to &lt;code&gt;True&lt;/code&gt; for cases where the last few inputs of your training job are uneven. In extreme cases, where there is a large discrepancy in the number of inputs, setting this to &lt;code&gt;False&lt;/code&gt; might provide better results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="049bcce7df5c629c4fad3f80f7fb581f440c0337" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;divisor_override&lt;/strong&gt; &amp;ndash; if specified, it will be used as divisor, otherwise &lt;code&gt;kernel_size&lt;/code&gt; will be used</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="853252d1cb8e53ec5f6c1f1eee0e317901dfd0af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;divisor_override&lt;/strong&gt; &amp;ndash; if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25dd43997ffb2286d38c5c4ea638d851750fdcd1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dlpack&lt;/strong&gt; &amp;ndash; a PyCapsule object with the dltensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79e803c5014bf096c8b936c68165b4355eadf0c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;do_constant_folding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; If True, the constant-folding optimization is applied to the model during export. Constant-folding optimization will replace some of the ops that have all constant inputs, with pre-computed constant nodes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="646458e97020d6122f679938c4abf38f5fbbe39e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;drop_last&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, the sampler will drop the last batch if its size would be less than &lt;code&gt;batch_size&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c66830184f5687c6e020fb4f42341d30ed6c7ea" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;drop_last&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, then the sampler will drop the tail of the data to make it evenly divisible across the number of replicas. If &lt;code&gt;False&lt;/code&gt;, the sampler will add extra indices to make the data evenly divisible across the replicas. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a7a53e8243b7f017759831c1f7997760e354a4f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;drop_last&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; set to &lt;code&gt;True&lt;/code&gt; to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If &lt;code&gt;False&lt;/code&gt; and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: &lt;code&gt;False&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93e1868e986f20e5fceebb1ba081741a9c11926d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; If non-zero, introduces a &lt;code&gt;Dropout&lt;/code&gt; layer on the outputs of each GRU layer except the last layer, with dropout probability equal to &lt;code&gt;dropout&lt;/code&gt;. Default: 0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1651899fbda49e6928ea8a0d295e3cfc914c011" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; If non-zero, introduces a &lt;code&gt;Dropout&lt;/code&gt; layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to &lt;code&gt;dropout&lt;/code&gt;. Default: 0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96b84bd1caf45dc07546855562b7416fae84878a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; If non-zero, introduces a &lt;code&gt;Dropout&lt;/code&gt; layer on the outputs of each RNN layer except the last layer, with dropout probability equal to &lt;code&gt;dropout&lt;/code&gt;. Default: 0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f96b6d7bf1dc804d0874288692d31f0b71cf772" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; a Dropout layer on attn_output_weights. Default: 0.0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea212a4ef049bb5d0535242d99d1a83d7b9f7d0a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; the dropout value (default=0.1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16fb29d21cd7a9ad4206efc9c506ce5a9cf98a02" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Destination rank</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebbff8f7d1835173bbe908168f4d760a0e927d2a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Destination rank.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c997f09b2bb6f5bbe4285b29caa42aeff58eb4c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Destination rank (default is 0)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15d2bdadaa3b397c334c7c569c4166d8c09f9b73" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Full path where object will be saved, e.g. &lt;code&gt;/tmp/temporary_file&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1bb062fcb22edc7afbb4e01d163cf5831e179573" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst_tensor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Destination tensor rank within &lt;code&gt;tensor_list&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="819196b391e0255903f0858db2143983b3dedb91" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst_type&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;type&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;string&lt;/em&gt;) &amp;ndash; the desired type</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb25a7ac765f2f5fe4e4d54278609b6627eae92b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; &amp;ndash; Quantized data type</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a4c495b44f4761667112255e495ba6d941ec366" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; &amp;ndash; data type of output Quantized Tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1da9e6e49781bd49b10cf164e54cc534c373f2b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; &amp;ndash; quantization data type to use. Default: &lt;code&gt;torch.quint8&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8b67954b83b4c9aa34630f43e704bf025dc94fd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;) &amp;ndash; the desired data type of returned tensor. Has to be one of the quantized dtypes: &lt;code&gt;torch.quint8&lt;/code&gt;, &lt;code&gt;torch.qint8&lt;/code&gt;, &lt;code&gt;torch.qint32&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e55ae84264ba3b49b96fa73c9f49b02e373e50a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned Tensor. Default: if &lt;code&gt;None&lt;/code&gt;, defaults to the dtype of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36bd900451348a91ef9ce9e64799f6d5a6c72b59" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: &lt;code&gt;torch.int64&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d58f90cb68e79cff8b96e7dc86dd4a2d5606995" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;torch.long&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d27c0b19831af56deba70f9352bc038b1e9a426" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, infers data type from &lt;code&gt;data&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="754f71e33b12e157331fb5bf10e0619a8ecbce0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, uses a global default (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e1aa8968192882a073c78793ea32a16dd1c21d1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, uses a global default (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;). If &lt;code&gt;dtype&lt;/code&gt; is not given, infer the data type from the other input arguments. If any of &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;end&lt;/code&gt;, or &lt;code&gt;stop&lt;/code&gt; are floating-point, the &lt;code&gt;dtype&lt;/code&gt; is inferred to be the default dtype, see &lt;a href=&quot;torch.get_default_dtype#torch.get_default_dtype&quot;&gt;&lt;code&gt;get_default_dtype()&lt;/code&gt;&lt;/a&gt;. Otherwise, the &lt;code&gt;dtype&lt;/code&gt; is inferred to be &lt;code&gt;torch.int64&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b423086bd082fcebc5a428940e93eeaaa4a7025" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, uses a global default (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;). Only floating point types are supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f1237018b147643a39f3e093d71d66749f7520f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if None, infers data type from &lt;code&gt;values&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="242051541efaa2777f5472bfe169a3381046ea24" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. If specified, the input tensor is casted to :attr:&amp;rsquo;dtype&amp;rsquo; while performing the operation. Default: None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="802bfb2cb709fdad8389996e31ce1fc046b7a6a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. If specified, the input tensor is casted to &lt;code&gt;dtype&lt;/code&gt; before the operation is performed. This is useful for preventing data type overflows. Default: None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf97cc7b5c225fd6d7e718d925da7641ae8dd16c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;type&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;string&lt;/em&gt;) &amp;ndash; The desired type</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="697b78c37ec8949c304299db02bbd09794133985" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired type of returned tensor. Default: if None, same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6ad67e837a7a5413b747672aa87555a7798fda9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;) &amp;ndash; the desired floating point type of the floating point parameters and buffers in this module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f854a7599c513f55feb63cd3cceed0c39260832a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;, optional) &amp;ndash; If specified, the input tensor is cast to &lt;code&gt;dtype&lt;/code&gt; before performing the operation, and the returned tensor&amp;rsquo;s type will be &lt;code&gt;dtype&lt;/code&gt;. If this argument is used in conjunction with the &lt;code&gt;out&lt;/code&gt; argument, the output tensor&amp;rsquo;s type must match this argument or a RuntimeError will be raised. This argument is not currently supported for &lt;code&gt;ord='nuc'&lt;/code&gt; or &lt;code&gt;ord='fro'&lt;/code&gt;. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf0e16ba6689c6aefebe41b2f37216499a3bab49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;, optional) &amp;ndash; the desired data type of returned Tensor. Default: dtype of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ddf34144bdd895ef81d56a2e64c3a31dcfb71d3f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;, optional) &amp;ndash; the desired data type of returned tensor. If specified, the input tensor is casted to &lt;code&gt;dtype&lt;/code&gt; before the operation is performed. This is useful for preventing data type overflows. Default: None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5aa3ffb5214bb07d706f4bff69889b064a67a58" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;, optional) &amp;ndash; the desired data type of the returned tensor. Default: &lt;code&gt;torch.float32&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c192f6abd14a8034e24d8a96ca7b9be11ad48f2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dx&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; The distance between points at which &lt;code&gt;y&lt;/code&gt; is sampled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="103894978c9b529418b87fd9904f3950e5930d8e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dynamic_axes&lt;/strong&gt; (&lt;em&gt;dict&amp;lt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;dict&amp;lt;python:int&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;string&amp;gt;&amp;gt;&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;dict&amp;lt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;(&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;&lt;em&gt;&amp;gt;&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default empty dict&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4fbef8984bd074ea0df43f49dda2d6246584439" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;edgeitems&lt;/strong&gt; &amp;ndash; Number of array items in summary at beginning and end of each dimension (default = 3).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42415c6475a98e0f1fa07ebf4b124d7a8ce2a188" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvalues&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): Shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fdaa0feae05488ebb171b346905c4f5b577cb835" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; &lt;code&gt;True&lt;/code&gt; to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="678dbd774fefa060e069a5a389a8baecab662449" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): If &lt;code&gt;eigenvectors=False&lt;/code&gt;, it&amp;rsquo;s an empty tensor. Otherwise, this tensor of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="850ac1cd20b0b0fc94fb9c8edab9b43e67eaf290" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): Shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad4105465a171ee7fcc51139dea5abbe36639f29" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether eigenvectors have to be computed</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a63d80190229ce72c85a7e6560054704380577ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;elementwise_affine&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d18cda3fe2834c321ed263a1762695dafaa4d1b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;embed_dim&lt;/strong&gt; &amp;ndash; total dimension of the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa8b03fcaceb218feac127ba1ed55359bc28eca3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;embedding_dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the size of each embedding vector</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de1af4e04b664f6ae8d83d1e7d608531b740f185" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;embeddings&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as &lt;code&gt;num_embeddings&lt;/code&gt;, second as &lt;code&gt;embedding_dim&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06e3728c866bf657f051e1739af9167591891b44" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;embeddings&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as &amp;lsquo;num_embeddings&amp;rsquo;, second as &amp;lsquo;embedding_dim&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0ff1217368a5b45df32d944d2dd475690d13d9f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enable&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether to enable uneven input detection or not. Pass in &lt;code&gt;enable=False&lt;/code&gt; to disable in cases where you know that inputs are even across participating processes. Default is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7081365cba99386e7771c337648ca106e830b0f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enable_onnx_checker&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default True&lt;/em&gt;) &amp;ndash; If True the onnx model checker will be run as part of the export, to ensure the exported model is a valid ONNX model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="895e955089896ac371f4672b847c341ccc8b1ff3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enable_timing&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; indicates if the event should measure time (default: &lt;code&gt;False&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="986eeeac89dc84a2ea56e1c9bce5f0263e468fa8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enabled&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; if &lt;code&gt;False&lt;/code&gt;, the RNG is not forked. This is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d8edf33fd2314207965e2b072e477dab0fdf74b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enabled&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Setting this to False makes this context manager a no-op. Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="415c46eca69853c5e4dfa53b80ff975d44d8ac3f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enabled&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default=True&lt;/em&gt;) &amp;ndash; Setting &lt;code&gt;enabled=False&lt;/code&gt; makes this context manager a no-op. Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="736574f31db572913a04933de35e73418aa3b198" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enabled&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default=True&lt;/em&gt;) &amp;ndash; Whether autocasting should be enabled in the region.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b5bbce56260acf11245d762cd1bb7f8add8801b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;encoder_layer&lt;/strong&gt; &amp;ndash; an instance of the TransformerEncoderLayer() class (required).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d84a97832788a91df32af257307532f9760f3b9c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor with the ending points</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d5c9c4c24dd3af3a29f20c653c917dfea2df720" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the ending value for the set of points</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6472482679cdc676a7850881f5d00cc512baada7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the ending value for the set of points</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="841fff8e784294f57ea3f8ba62be671371d870ec" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end_dim&lt;/strong&gt; &amp;ndash; last dim to flatten (default = -1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c799ddd223af170e7b8eec13d7b2c9d7165d8603" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end_dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the last dim to flatten</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a6422972f6f1ee1d35eb1fc402a56304ba9816a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enforce_sorted&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, checks that the input contains sequences sorted by length in a decreasing order. If &lt;code&gt;False&lt;/code&gt;, this condition is not checked. Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8c91d834d8c02f59864a331664c622e334c3693" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enforce_sorted&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the input is expected to contain sequences sorted by length in a decreasing order. If &lt;code&gt;False&lt;/code&gt;, the input will get sorted unconditionally. Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5baba4b5ccf6b68b45f524ad14b90f736f67183d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;epochs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The number of epochs to train for. This is used along with steps_per_epoch in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d07cc920f4a588343d52b60256e017a7987131f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; &amp;ndash; a value added to the denominator for numerical stability. Default: 1e-5</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8684696132f475ee9f33febf9c44e66f038a87a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; &amp;ndash; a value added to the denominator for numerical stability. Default: &lt;code&gt;1e-5&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83b2a550335cb37b45d5faa4c3779a2cd427c881" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Minimal decay applied to lr. If the difference between new and old lr is smaller than eps, the update is ignored. Default: 1e-8.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac6c5326b4eb0b53657544fec8194ff31859de16" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; small value to avoid division by zero. Default: 1e-12</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3071da796481042a1779ea93a5637a546557315" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Small value to avoid division by zero. Default: 1e-6</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfa6309f005abf8cb4486bcdb54e88aa5722cf1c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Small value to avoid division by zero. Default: 1e-8</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a35a97bcd21164533c2c178f41069f2aca802198" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Small value to avoid evaluation of</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e84e8770e10ffa80ab0cd44b0a601016d1c45b1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; epsilon for numerical stability in calculating norms</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93812852966ba66d19563befd8bfb79438fe404f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; perturbation for finite differences</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23287961a644af8da325b62e474d3172bd5f3de4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; term added to the denominator to improve numerical stability (default: 1e-10)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfc881323872c2d4d8cd616814cd232147e55d20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; term added to the denominator to improve numerical stability (default: 1e-6)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7750d8fa1fba824c2db7cdcb029debc400b1e1b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; term added to the denominator to improve numerical stability (default: 1e-8)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64bfcd7f406bb75e0a20d11554ef0afad32ae5c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the epsilon for input clamp bound. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7aa30076d7d518c87037ac60ab79f79f7490d2c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;equal_nan&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, then two &lt;code&gt;NaN&lt;/code&gt; s will be considered equal. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f8913ade12e3dc4c4d8fd9b0228a8bcf2b585f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;equation&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; The equation is given in terms of lower case letters (indices) to be associated with each dimension of the operands and result. The left hand side lists the operands dimensions, separated by commas. There should be one index letter per tensor dimension. The right hand side follows after &lt;code&gt;-&amp;gt;&lt;/code&gt; and gives the indices for the output. If the &lt;code&gt;-&amp;gt;&lt;/code&gt; and right hand side are omitted, it implicitly defined as the alphabetically sorted list of all indices appearing exactly once in the left hand side. The indices not apprearing in the output are summed over after multiplying the operands entries. If an index appears several times for the same operand, a diagonal is taken. Ellipses &lt;code&gt;&amp;hellip;&lt;/code&gt; represent a fixed number of dimensions. If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9795d90febecee64ae62240078c8b888a3529f0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eta_min&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Minimum learning rate. Default: 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8244af6c91eeea47bc27c5f4ae207a6917b58ea" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eta_min&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Minimum learning rate. Default: 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1433ae6bce90cf036177044e9c513b2c8d0f5f99" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;etas&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; pair of (etaminus, etaplis), that are multiplicative increase and decrease factors (default: (0.5, 1.2))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbb3573c662548692515c0f7a10112191b156c96" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;event&lt;/strong&gt; (&lt;a href=&quot;#torch.cuda.Event&quot;&gt;Event&lt;/a&gt;) &amp;ndash; an event to wait for.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cb5a854faf1af97d1f14b26d87e6a2fa84d0c7b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;event&lt;/strong&gt; (&lt;a href=&quot;#torch.cuda.Event&quot;&gt;Event&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; event to record. If not given, a new one will be allocated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62122c1e3a5ab3359886af99296a0aefc26ae410" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;event_dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Optional size of &lt;code&gt;event_shape&lt;/code&gt;. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34f745bd094bd2de883ee47fbf1638e33919faba" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;example_inputs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; A tuple of example inputs that will be passed to the function while tracing. The resulting trace can be run with inputs of different types and shapes assuming the traced operations support those types and shapes. &lt;code&gt;example_inputs&lt;/code&gt; may also be a single Tensor in which case it is automatically wrapped in a tuple.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="618d999c8d78a9676692412df718e533abdf676f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;example_outputs&lt;/strong&gt; (&lt;em&gt;tuple of Tensors&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default None&lt;/em&gt;) &amp;ndash; Model&amp;rsquo;s example outputs being exported. example_outputs must be provided when exporting a ScriptModule or TorchScript Function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2456379602b05045dca23f7f350a44c9fd9c512" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;expand&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether to expand the support over the batch dims to match the distribution&amp;rsquo;s &lt;code&gt;batch_shape&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a88c1284ab82b366b962ae9cc9b58fad7708160" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;exponent&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the exponent tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00243251e0e37f0ed072fb8db0c41a52c671837f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;exponent&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tensor&lt;/em&gt;) &amp;ndash; the exponent value</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcb9c92fd7acb8cc56e2900598ba04b5be24aae8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;export_params&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default True&lt;/em&gt;) &amp;ndash; if specified, all parameters will be exported. Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by &lt;code&gt;model.state_dict().values()&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f60c4eee6ac8bd3b83d115e58f8f9d5c6a1f285a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;export_raw_ir&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; [DEPRECATED. use operator_export_type] export the internal IR directly instead of converting it to ONNX ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e82a47709effdd4ed45378b2094bb76a83f236b5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;external_data_format&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; If True, then the model is exported in ONNX external data format, in which case some of the model parameters are stored in external binary files and not in the ONNX model file itself. See link for format details: &lt;a href=&quot;https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423&quot;&gt;https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423&lt;/a&gt; Also, in this case, argument &amp;lsquo;f&amp;rsquo; must be a string specifying the location of the model. The external binary files will be stored in the same location specified by the model location &amp;lsquo;f&amp;rsquo;. If False, then the model is stored in regular format, i.e. model and parameters are all in one file. This argument is ignored for all export types other than ONNX.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="778fd667aea6cff6dc8d68bb20cefa3e389d87e6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;extra_cflags&lt;/strong&gt; &amp;ndash; optional list of compiler flags to forward to the build.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcbab5a7fa61c6ed8414a66172d5505acca2751e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;extra_cuda_cflags&lt;/strong&gt; &amp;ndash; optional list of compiler flags to forward to nvcc when building CUDA sources.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5906d2861e5b04548eb15a5d2b6487f486e2e624" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;extra_include_paths&lt;/strong&gt; &amp;ndash; optional list of include directories to forward to the build.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62224af2d822c58c7d8e8fba25c2a2dc1f3ef2cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;extra_ldflags&lt;/strong&gt; &amp;ndash; optional list of linker flags to forward to the build.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6f0cc911afdf14389ad75575a9c9cbfa3c8ccfb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; A file-like object (has to implement write and flush) or a string containing a file name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0bbf42a4b349b4674d2d35b2fcc87d253370be87" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; a file-like object (has to implement &lt;code&gt;read()&lt;/code&gt;, :meth`readline`, :meth`tell`, and :meth`seek`), or a string or os.PathLike object containing a file name</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54a2a51aa44ae289f486330a5f54f09e6f971a65" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name. A binary Protobuf will be written to this file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="677757134bd8309ac15335a18ed7a21b22ef8fdd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df677cb77b7f5749b31ae401c7fa92ff2d7e7fe5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; a file-like object (has to implement write and flush) or a string or os.PathLike object containing a file name</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="635ef0bc301d974c0ff51acd3ba7ee21d88b4744" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;faces&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Indices of vertices within each triangle. (Optional)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7dac65f2926c30687b712331254bac0c325b018e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d42958804dd0465816b20a88c367eef0dbeec80" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;factorization&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): the factorization of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d41e8bf5537d7429d4d9cda4345adcbe62552ea2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;factory&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;) &amp;ndash; A callable that inputs a constraint object and returns a &lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt;&lt;code&gt;Transform&lt;/code&gt;&lt;/a&gt; object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00769844b4ecc2fca40878cb1764ed386aa09fd9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;figure&lt;/strong&gt; (&lt;em&gt;matplotlib.pyplot.figure&lt;/em&gt;) &amp;ndash; Figure or a list of figures</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62d218fbe21b9a87d144357ac35b2f10bb534799" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;file_name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; path of the file in which to store the key-value pairs</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3c0dee521e2626c0ae3dbbaa2f17cc80f80c2a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;file_name&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; name for the downloaded file. Filename from &lt;code&gt;url&lt;/code&gt; will be used if not set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57269f4bfae1f1ca10e35c956f9fd6e448de9061" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;filename&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; file name to map</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95d303dabfba56a1b6870b6f2026c68bddc1be2f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;filename_suffix&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Suffix added to all event filenames in the log_dir directory. More details on filename construction in tensorboard.summary.writer.event_file_writer.EventFileWriter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdb4a6b44cef668e406b3d993df10539cc0f7054" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fill_value&lt;/strong&gt; &amp;ndash; the number to fill the output tensor with.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc362ae2525be68e4bb10eb869fbbb29641c9bab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fill_value&lt;/strong&gt; (&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; the fill value</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81b7336ac76bc7cac5103fa2f7d2fd03ba164f85" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fill_value&lt;/strong&gt; (&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; the value to fill the output tensor with.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2031b6ba05d0b93a6457f0efd37cfe00b19e577a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fill_value&lt;/strong&gt; (&lt;em&gt;scalar&lt;/em&gt;) &amp;ndash; the number to fill the output tensor with.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="276eb8282c2d9e264a29a26b2d65090e55bac9ea" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;final_div_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Determines the minimum learning rate via min_lr = initial_lr/final_div_factor Default: 1e4</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9096ebbb1f99e7416acc6998a05334d6d6397c19" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;find_unused_parameters&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Traverse the autograd graph from all tensors contained in the return value of the wrapped module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; function. Parameters that don&amp;rsquo;t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all &lt;code&gt;forward&lt;/code&gt; outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don&amp;rsquo;t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using &lt;code&gt;torch.Tensor.detach&lt;/code&gt;. (default: &lt;code&gt;False&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="355396a4b2ab68b82f509c2d4c2526e2d2ba215d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;flush_secs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; How often, in seconds, to flush the pending events and summaries to disk. Default is every two minutes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5d2a692102bf116db7e2c67b38b6073ed5ae103" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fn&lt;/strong&gt; (&lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; -&amp;gt; None) &amp;ndash; function to be applied to each submodule</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad87b9b261bd0e692ec81745b0251ab69e0c920a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fn&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; -&amp;gt; None) &amp;ndash; function to be applied to each submodule</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="053dcf87d7e84dd407dec8695847c296ae6c0c48" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fn&lt;/strong&gt; (&lt;code&gt;Module&lt;/code&gt; -&amp;gt; None) &amp;ndash; function to be applied to each submodule</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b64b5eb22d75d1654b2b39949b562dd2fc7ec84" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fn&lt;/strong&gt; (&lt;em&gt;function&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ef37f45041ae6f49569ea35c632856c2fd1d15d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;force_reload&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to discard the existing cache and force a fresh download. Default is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc70a2bb2f4ed4bcf5ecccaecd8c2e734de9391d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;force_reload&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to force a fresh download of the github repo unconditionally. Does not have any effect if &lt;code&gt;source = 'local'&lt;/code&gt;. Default is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83c137929bf36e8aef1d6a6cc8426a81216a787f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Frames per second</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bde664f21923dc747ecb17124d60a563c8fc2b2c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;freeze&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, the tensor does not get updated in the learning process. Equivalent to &lt;code&gt;embedding.weight.requires_grad = False&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3fd0568363b2aa7dd7b7c7e651d3c4c84a69bed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;freeze&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, the tensor does not get updated in the learning process. Equivalent to &lt;code&gt;embeddingbag.weight.requires_grad = False&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ba291fa349a86b63b98c719b0de8e1e3a0b2ccd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;from&lt;/strong&gt; (&lt;em&gt;dpython:type&lt;/em&gt;) &amp;ndash; The original &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46bb4df94b1ad85e11557fbf62e5528ed469cf66" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;full&lt;/strong&gt; &amp;ndash; whether to compute full loss, i. e. to add the Stirling approximation term. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75533226793ba470c8b0b1c360399ebe87a93520" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;full&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65a3ef606b557f50e8baa66a8411c285745d744f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;func&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;) &amp;ndash; a callable function, such as Python callables, builtin operators (e.g. &lt;a href=&quot;generated/torch.add#torch.add&quot;&gt;&lt;code&gt;add()&lt;/code&gt;&lt;/a&gt;) and annotated TorchScript functions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eab18ced6b9533ad0e16e9973e883f1659631001" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;func&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;torch.nn.Module&lt;/a&gt;) &amp;ndash; A Python function or &lt;code&gt;torch.nn.Module&lt;/code&gt; that will be invoked. If executed in TorchScript, it will execute asynchronously, otherwise it will not. Traced invocations of fork will be captured in the IR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e13542d19949849cdfa6d1ca4a6a3c14a44f62f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;func&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;torch.nn.Module&lt;/a&gt;) &amp;ndash; A Python function or &lt;code&gt;torch.nn.Module&lt;/code&gt; that will be run with &lt;code&gt;example_inputs&lt;/code&gt;. &lt;code&gt;func&lt;/code&gt; arguments and return values must be tensors or (possibly nested) tuples that contain tensors. When a module is passed &lt;code&gt;torch.jit.trace&lt;/code&gt;, only the &lt;code&gt;forward&lt;/code&gt; method is run and traced (see &lt;a href=&quot;torch.jit.trace_module#torch.jit.trace_module&quot;&gt;&lt;code&gt;torch.jit.trace&lt;/code&gt;&lt;/a&gt; for details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56b0f0644254436da01524968b169f5b50136a6f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;func&lt;/strong&gt; (&lt;em&gt;function&lt;/em&gt;) &amp;ndash; a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4bcf898e9ca65627609a065cb0b080712aa4c22c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;func&lt;/strong&gt; (&lt;em&gt;function&lt;/em&gt;) &amp;ndash; a Python function that takes Tensor inputs and returns a Tensor with a single element.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73a52f765d84ab66ab0e999764fd5ec7c3392353" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;func&lt;/strong&gt; (&lt;em&gt;function&lt;/em&gt;) &amp;ndash; a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="76d83916ad9f858f16111887e3f6267b2fca5fed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;function&lt;/strong&gt; &amp;ndash; describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes &lt;code&gt;(activation, hidden)&lt;/code&gt;, &lt;code&gt;function&lt;/code&gt; should correctly use the first input as &lt;code&gt;activation&lt;/code&gt; and the second input as &lt;code&gt;hidden&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1961467ba2825e1c559db8b59ff1bdbcbb0ec039" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;functions&lt;/strong&gt; &amp;ndash; A &lt;a href=&quot;generated/torch.nn.sequential#torch.nn.Sequential&quot;&gt;&lt;code&gt;torch.nn.Sequential&lt;/code&gt;&lt;/a&gt; or the list of modules or functions (comprising the model) to run sequentially.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fb5cc79b41c4d978a0a328aa05327cc1e203720" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;functions&lt;/strong&gt; &amp;ndash; A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2894bf837b3fe8e5a6a993314a795c3b92f661d3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fuser_func&lt;/strong&gt; &amp;ndash; Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb85d27a8748aad626d3af5de9e7af60b244f74d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;futures&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; a list of &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a7221601ceeb930d343f7319c8db14fe03b3e16" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;futures&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; a list of &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba355c7f4beca0e0d4bf68bb3971593069c56322" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gain&lt;/strong&gt; &amp;ndash; an optional scaling factor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b04565ecc2229736f50e2b26ed78404ea530d53b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gain&lt;/strong&gt; &amp;ndash; optional scaling factor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9537c0304197566d9a64aed9da630fcabb6dfaf8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gamma&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Constant in &amp;lsquo;exp_range&amp;rsquo; scaling function: gamma**(cycle iterations) Default: 1.0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a6e1b932343255f06ce86199fe2520eefcb597e1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gamma&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Multiplicative factor of learning rate decay.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7d26412c35f8aaae0db5c942f3ecafc6d5b994e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gamma&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Multiplicative factor of learning rate decay. Default: 0.1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b03754a8da7ef008224ed892c921fb9371f0c076" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gather_list&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ae62591eb58ac1c4da89608590832d7178cb467" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gen_non_contig_grad_outputs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;grad_outputs&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;gen_non_contig_grad_outputs&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the randomly generated gradient outputs are made to be noncontiguous</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d39d3171bd8a988de4ba0ad35fb1fb1306606f7c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;generator&lt;/strong&gt; (&lt;a href=&quot;generated/torch.generator#torch.Generator&quot;&gt;Generator&lt;/a&gt;) &amp;ndash; Generator used for the random permutation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8f474eedb1de962668babbc241de79afb189c26" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;generator&lt;/strong&gt; (&lt;a href=&quot;generated/torch.generator#torch.Generator&quot;&gt;Generator&lt;/a&gt;) &amp;ndash; Generator used in sampling.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5579060ab76636341f5dfd97c022a89f302b7cf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;generator&lt;/strong&gt; (&lt;a href=&quot;torch.generator#torch.Generator&quot;&gt;&lt;code&gt;torch.Generator&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; a pseudorandom number generator for sampling</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06014c6b4c4693a7e3fbaf2b2a37ca8411e8162c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;get_infos&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if set to &lt;code&gt;True&lt;/code&gt;, returns an info IntTensor. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="713f3c8adc9b7748672962092ba1c2254277918b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;github&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; a string with format &amp;ldquo;repo_owner/repo_name[:tag_name]&amp;rdquo; with an optional tag/branch. The default branch is &lt;code&gt;master&lt;/code&gt; if not specified. Example: &amp;lsquo;pytorch/vision[:hub]&amp;rsquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="270e6272159af8294641789a5ae5029a0175a0a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;github&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; a string with format &amp;lt;repo_owner/repo_name[:tag_name]&amp;gt; with an optional tag/branch. The default branch is &lt;code&gt;master&lt;/code&gt; if not specified. Example: &amp;lsquo;pytorch/vision[:hub]&amp;rsquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34a7ff872bd23abb0450fda48dac267f7a245b6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;global_step&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Global step value to record</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59b52c7e39a474423f9fda9eb51f75f802a681a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;graceful&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether to do a graceful shutdown or not. If True, this will 1) wait until there is no pending system messages for &lt;code&gt;UserRRefs&lt;/code&gt; and delete them; 2) block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d7c0034720f0f5384aa3c778777a63be7c08866" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;grad_outputs&lt;/strong&gt; (&lt;em&gt;sequence of Tensor&lt;/em&gt;) &amp;ndash; The &amp;ldquo;vector&amp;rdquo; in the Jacobian-vector product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don&amp;rsquo;t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="787019ca25e342b192fc0fe32846b61af4cbf80c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;grad_outputs&lt;/strong&gt; (&lt;em&gt;tuple of Tensor&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The gradients with respect to the function&amp;rsquo;s outputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3a7adf281106800948e615e1796c2800710f153" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;grad_tensors&lt;/strong&gt; (&lt;em&gt;sequence of&lt;/em&gt;&lt;em&gt; (&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/constants.html#None&quot;&gt;None&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;) &amp;ndash; The &amp;ldquo;vector&amp;rdquo; in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don&amp;rsquo;t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="799013c067a407e423f9d42197eba6a2384dd6a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gradient&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/constants.html#None&quot;&gt;None&lt;/a&gt;) &amp;ndash; Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless &lt;code&gt;create_graph&lt;/code&gt; is True. None values can be specified for scalar Tensors or ones that don&amp;rsquo;t require grad. If a None value would be acceptable then this argument is optional.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed3816b6dfbefeff09bce5c08bcf032810a5e596" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gradient&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/constants.html#None&quot;&gt;None&lt;/a&gt;) &amp;ndash; Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless &lt;code&gt;create_graph&lt;/code&gt; is True. None values can be specified for scalar Tensors or ones that don&amp;rsquo;t require grad. If a None value would be acceptable then this argument is optional.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b29df415efb0eea8f8662bee6b939588b9b78efb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gradient_as_bucket_view&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; This is a prototype feature and subject to changes. When set to &lt;code&gt;True&lt;/code&gt;, gradients will be views pointing to different offsets of &lt;code&gt;allreduce&lt;/code&gt; communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and &lt;code&gt;allreduce&lt;/code&gt; communication buckets. When gradients are views, &lt;code&gt;detach_()&lt;/code&gt; cannot be called on the gradients. If hitting such errors, please fix it by referring to the &lt;a href=&quot;../optim#torch.optim.Optimizer.zero_grad&quot;&gt;&lt;code&gt;zero_grad()&lt;/code&gt;&lt;/a&gt; function in &lt;code&gt;torch/optim/optimizer.py&lt;/code&gt; as a solution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2b0bcce70df29844281ac614b0f8a39a0dfcd19" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;grid&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; flow-field of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4db1041ae201d7e927245f12103bcee3afeae410" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group&lt;/strong&gt; (&lt;em&gt;ProcessGroup&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The process group to work on</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1110361e162f3be4aa9b1a7cde1a491603596b93" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group&lt;/strong&gt; (&lt;em&gt;ProcessGroup&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The process group to work on.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6db70a258153314cde5304ac47d562305e99552a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group&lt;/strong&gt; (&lt;em&gt;ProcessGroup&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of &lt;code&gt;group&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="011a95e14d4b56ac5f95b65e4e55d911eaae7b11" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group_by_input_shapes&lt;/strong&gt; &amp;ndash; group entries by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="76f6d63e480e5617ce79eca4ba9df6a70f24cb2d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group_by_stack_n&lt;/strong&gt; &amp;ndash; group by top n stack trace entries</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85aa3c312919cb9842e6de299fa1fb86a7f7ea49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group_name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;deprecated&lt;/em&gt;) &amp;ndash; Group name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f26fa12d9d61e6585a36de452da97d2ffce8b34d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;groups&lt;/strong&gt; &amp;ndash; split input into groups,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81c8b7e8acb12c23d521a687adf35816ef4ff39c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;groups&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Number of blocked connections from input channels to output channels. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3936cb9ca5f3782dc3736b1eb2710d73d6c56d73" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;groups&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; number of groups in the conv layer (default: 1)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9313fa876c33968ef38e2bc5ed5adcaf010be99" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h&amp;rsquo;&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the next hidden state for each element in the batch</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1592eedf3bb9a47887176fd782c28510508c7820" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_0&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the initial hidden state for each element in the batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfde5becd622ec62656a4cff95b1913fb5911358" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_0&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a088baa848abf407fdf324f68a7fc23226f3bfd9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_0&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0e30ccc3d1652af234e0543d06149d194f40bc0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_1&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the next hidden state for each element in the batch</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1c9ba2b299c711fcc8d467de3dea8569b50a3fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_n&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the hidden state for &lt;code&gt;t = seq_len&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3525fd2429c6604b476c82ddd74f59209ab301ef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_n&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the hidden state for &lt;code&gt;t = seq_len&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba61e8234e80482584a34f381f7534bd9ba1d290" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hard&lt;/strong&gt; &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the returned samples will be discretized as one-hot vectors, but will be differentiated as if it is the soft sample in autograd</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaa96a08f23e0f6a631877636b216cb602e637fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hash_prefix&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If not None, the SHA256 downloaded file should start with &lt;code&gt;hash_prefix&lt;/code&gt;. Default: None</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b66c30a2662d014769668efe291e7362c5d7d9db" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;head_bias&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, adds a bias term to the &amp;lsquo;head&amp;rsquo; of the adaptive softmax. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d30e56690f484d3c8d7a2b255db4bcbd77445f68" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hidden&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e67734f0b9cf7c79bc74ff13156c666a023e5f41" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hidden_size&lt;/strong&gt; &amp;ndash; The number of features in the hidden state &lt;code&gt;h&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e1564d55f92aeff911d4cdf1ca4875875da67b1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;high&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; upper range (exclusive).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3705e6bc9a6ecd869a7951f814f9fed46d9ff630" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;high&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; One above the highest integer to be drawn from the distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc6dd784a95236db99f041d334a725f2f9b1a9a5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;history_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; update history size (default: 100).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c19ea35377296f18df50a468ab31eff648558a09" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hop_length&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the distance between neighboring sliding window frames. Default: &lt;code&gt;None&lt;/code&gt; (treated as equal to &lt;code&gt;floor(n_fft / 4)&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1396026b4c7368bc625f59b4932fadbbf084c72" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hop_length&lt;/strong&gt; (&lt;em&gt;Optional&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; The distance between neighboring sliding window frames. (Default: &lt;code&gt;n_fft // 4&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6876910fda4d6ecccd2051aba33481c94d9368b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;host_name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The hostname or IP Address the server store should run on.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94284103cfc33f19f8a61eebc94176be0db80646" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hparam_dict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; Each key-value pair in the dictionary is the name of the hyper parameter and it&amp;rsquo;s corresponding value. The type of the value can be one of &lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;string&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt;, or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a1b563f2476f046122b214d6a9600f12ff9315b6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hparam_domain_discrete&lt;/strong&gt; &amp;ndash; (Optional[Dict[str, List[Any]]]) A dictionary that contains names of the hyperparameters and all discrete values they can hold</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98aaeaa4225400ecd738ec2048d131fe08b1d564" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;iK&lt;/strong&gt; (&lt;em&gt;tensor&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the input tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e423e9ecf3beffc0eedb4329822c8534728b01ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ignore_index&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies a target value that is ignored and does not contribute to the input gradient. When &lt;code&gt;size_average&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the loss is averaged over non-ignored targets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="607b11f999212773791595ce05953927928c08a6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ignore_index&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies a target value that is ignored and does not contribute to the input gradient. When &lt;code&gt;size_average&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the loss is averaged over non-ignored targets. Default: -100</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c096d38fe26de10e968164a6aecd6f913177ea6e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;imag&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The imaginary part of the complex tensor. Must be same dtype as &lt;a href=&quot;torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3a96d8bd554be6eb7797de17442ad6cb99464a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;img_tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;numpy.array&lt;/em&gt;&lt;em&gt;, or &lt;/em&gt;&lt;em&gt;string/blobname&lt;/em&gt;) &amp;ndash; Image data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e075b9a8dfd668778490588e84bb0c9bbb577a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in1_features&lt;/strong&gt; &amp;ndash; size of each first input sample</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99c9afda0be0c905049d69c6c60279520769cef1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in2_features&lt;/strong&gt; &amp;ndash; size of each second input sample</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="054cbce99ed259e05737255fcff3ec1a33f17b30" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in_channels&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of channels in the input image</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="031258926af2ce0c91f35de69ae4829af92fe5cc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in_features&lt;/strong&gt; &amp;ndash; size of each input sample</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1cd84155beb6cb4e4e554a0fb3f34f7def9bf3d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in_features&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of features in the input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de92c22d2099710d576bc540cf0e439bfe62dc1b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;include_last_offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c77074d3fbe6ed19bba6e5a032d17feba927877" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;include_last_offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;offsets&lt;/code&gt; has one additional element, where the last element is equivalent to the size of &lt;code&gt;indices&lt;/code&gt;. This matches the CSR format.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adc8afe3a34cbeea7690cc1b8d46533e74feca80" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;include_last_offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the size of offsets is equal to the number of bags + 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0058952dc92b6430936a12a9320099bae5295fc6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;increasing&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Order of the powers of the columns. If True, the powers increase from left to right, if False (the default) they are reversed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d624060723fbf002e4598220b5812e19f26a17a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; index to insert.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d2ab5af76f511772ee4673ede1ce34b868d76a5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the index to select with</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35fb618fa2c424e1e9473beb050f9175521a3241" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; indices of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; to select from</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67e5b1c26904309bcbc72d6a5206411049f25fa6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; indices of &lt;code&gt;self&lt;/code&gt; tensor to fill in</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8699c1c42412268aa01357837b44ae957ac901c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the 1-D tensor containing the indices to index</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b1088f3894e3a3aade31aa3da2f7cbaa1c7f3c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices of elements to gather</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7246e33903dd34811bb50beca86c9f179b0ea261" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices of elements to scatter and add, can be either empty or the same size of src. When empty, the operation returns identity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7891bb82e5f646ecc5571231eb40487d6adaedf7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices of elements to scatter, can be either empty or the same size of src. When empty, the operation returns identity</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4489c58da0a616c049ed6d93d7f17fd55991d97a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices into self</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5be31c40930fbda8ef6fadbd69d27440a0a29e3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices into tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f340c71c41bf4a8b6f0ee0a633f1e707d60c41e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;array_like&lt;/em&gt;) &amp;ndash; Initial data for the tensor. Can be a list, tuple, NumPy &lt;code&gt;ndarray&lt;/code&gt;, scalar, and other types. Will be cast to a &lt;code&gt;torch.LongTensor&lt;/code&gt; internally. The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05aa3324e9dca8478c49624094146779c5320358" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;sequence&lt;/em&gt;) &amp;ndash; Indices in the whole set selected for subset</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c088f1e272dfc4a0b7cc5311d10b29ca4ae57c70" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;sequence&lt;/em&gt;) &amp;ndash; a sequence of indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bef6961ff826304ba68b7564bb99ad681fdaf697" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;tuple of LongTensor&lt;/em&gt;) &amp;ndash; tensors used to index into &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d19cbf8a68fcf3b4f455172ecef9b16a565a113c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;infos&lt;/strong&gt; (&lt;em&gt;IntTensor&lt;/em&gt;, &lt;em&gt;optional&lt;/em&gt;): if &lt;code&gt;get_infos&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, this is a tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71c1dcc99d861730834f2921fb9ea642ee26387c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;init&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the initial value of</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3387ec2d138470faa0b9b1e76a459e501c664a81" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;init_method&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The URL to initialize &lt;code&gt;ProcessGroupGloo&lt;/code&gt; (default: &lt;code&gt;env://&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da9829504ae0afe64db4aa47b65791dfaa083f83" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;init_method&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The URL to initialize the distributed store used for rendezvous. It takes any value accepted for the same argument of &lt;a href=&quot;distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; (default: &lt;code&gt;env://&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f325478530b5cf14071a5c700f82577ec5ba84d8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;init_method&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; URL specifying how to initialize the process group. Default is &amp;ldquo;env://&amp;rdquo; if no &lt;code&gt;init_method&lt;/code&gt; or &lt;code&gt;store&lt;/code&gt; is specified. Mutually exclusive with &lt;code&gt;store&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fa891f80017cd81f628a35707cc9e6065a280f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; (Currently not supported) can optionally do the operation in-place.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df1cc134d4a1280a08aae8069753847bc753ab08" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; If set to &lt;code&gt;True&lt;/code&gt;, will do this operation in-place. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a55c724ebe06e5259fc6013fba20d7f4e805cc86" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; bool specifying if fusion happens in place on the model, by default a new model is returned</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a0af1cd98e89baa21a0964493ae5197db8d39c0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; can optionally do the operation in-place. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a338586cc1b9e6364bb31bdac03d5949ded0e36" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; carry out model transformations in-place, the original module is mutated</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75aed03563c53dd2d0764eb5453c6ca6e4140d83" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; perform the computation inplace</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fdf58b63be55a4b22c46032d3fd1d61d0bd927a2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If set to &lt;code&gt;True&lt;/code&gt;, will do this operation in-place</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2baabf2051ecce4e59ed7868711be86074a1121b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; can optionally do the operation in-place. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ca73aff43bbd335b880e2e8b6461721261c7efd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cb4517ba198e148817291a746b67bb3359b3275" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the &lt;code&gt;tau&lt;/code&gt; from &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14436103f8610b62c5af0d3f52320c0e695e6561" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input3&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the matrix to be multiplied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="189471ee9fd0ae26f96b12a2f14fabdfdb2a55c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d43b3b03e5e1cc0bc29a4d3ee11d035641542b7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; A Tensor that is input to &lt;code&gt;functions&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3478ec4abd0f0da74946e0059075f92ae16838f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; Tensor of arbitrary shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64075621f7b49ac8cf3466ffee84214ef27b7e0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; expectation of underlying Poisson distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5c45d37ad1ca2bfd9ba95fc0c1c9aba750297da" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fefd9a33978ca250fb3687e571631fed568e2f69" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; input tensor of any shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32aa8fde0d3b37a5fd0bf094d226ea96d65d5b3e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; input tensor of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e71ece563e570f0828230ce4079bf1d6a1f9bfd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; quantized input</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="324d485d4d64467111174a180760e99e40ae34f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; quantized input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1052a12b95427668122c4a5782e7d1c6ced7fbb7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; quantized input tensor of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="280fc2140640a681bb0a76e2eacb285f4f40a9a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; the first input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c5a3515f7ca1c5139e9f3360aa15d38e06b1c58" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; an input Tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0b781ff3e115b7562060e3796452984aec9e1bc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1-D input vector</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e381244bd054e5229323f3fd533dcf76ded11c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1-d int tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="318151f5ec7778ddab17a5a32f68c531c8b678c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1D vector.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="302e5c41a337c972f93ff08bf742872e6882489e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Must be at least 1-dimensional.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cff6b01a05d93e056d3e7d25e27d95e0d5b44d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Must be at least 2-dimensional.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="508cdbc779d4073e5eafa0ff8836504797cfe9a3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The input tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11314dbf954d397fca1f11546d1c56bacc29ebd7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The input tensor. Expected to be output of &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;stft()&lt;/code&gt;&lt;/a&gt;, can either be complex (&lt;code&gt;channel&lt;/code&gt;, &lt;code&gt;fft_size&lt;/code&gt;, &lt;code&gt;n_frame&lt;/code&gt;), or real (&lt;code&gt;channel&lt;/code&gt;, &lt;code&gt;fft_size&lt;/code&gt;, &lt;code&gt;n_frame&lt;/code&gt;, 2) where the &lt;code&gt;channel&lt;/code&gt; dimension is optional.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25819484e745b900dfb30d16e53f617750bd99ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; a minibatch of examples</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8a8643fc96ff022524bbbbcc16e1f74f4d027fc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; first tensor in the dot product. Its conjugate is used if it&amp;rsquo;s complex.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3329ef02c74e5cee67f1ff96ed2733ed78e48c67" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; first tensor to compare</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6c3db01982d7f9c54ac50853d8f1beed2cda65f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; float tensor to quantize</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c7042dc3bf98959f56c246765073fdcc4437d8b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2add4041984cdf88b12d1f1e2dd5b3c7ef0a1d8c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; matrix to be added</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b95c32681c57401ab545053403e32395835243a1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; matrix to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b1dc1a962b67cc0e0f4fe6ec1e3b9ed6f855288" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; multiple right-hand sides of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd44d464e1aa57b95d0330a7218514f91318409c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; padded batch of variable length sequences.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2df5064f26ed4ff300eeca02d8d23ff3989d592a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the &lt;code&gt;a&lt;/code&gt; from &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32b3debabd5244f6f757a0ecbfc2001f87d51061" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the dividend</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc63e66b77eddc23d0480985c434968ab3a3c432" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first batch of matrices to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8048457126f71ad2252c07afbe21ab1ad62cc0e3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40066a22db2f549d0b2b91e1d0653edab74dded7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first matrix to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d03903d53b2c5ac4f518cd45d6008241190088c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first multiplicand tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f51f99307491138059682bbbc27c557807591a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first tensor to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1accceea4c5085a1b9d5fd922dec4a71a7c24ead" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input 2-D tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6146e5ea09927c514d32a11ed0ac42f33556893d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="708c21e573ae8975fd41f843d793ad7f90dc01c9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ccd5042b98d78e05ed8accf585abbd7c7cd67f8c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor containing probabilities</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8678adfcf46f049849a9950535e8987135ee8c9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor containing the rates of the Poisson distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19b4b70bbc0703888b4dd081987a664f12caeab6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of at least &lt;code&gt;signal_ndim&lt;/code&gt; dimensions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a80664bf19108f6871d4d9cc67ea5ec9d65038f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of at least &lt;code&gt;signal_ndim&lt;/code&gt;&lt;code&gt;+ 1&lt;/code&gt; dimensions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80ea65a4aaf35f42f84d23c01465ca2ef54bdaad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of probability values for the Bernoulli distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b642c0b47336e267e577b35820c587126ea9f39a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9159ecc16b63f9802803f1f98dd7cfcc6322e401" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of size &lt;code&gt;(*, n, n)&lt;/code&gt; where &lt;code&gt;*&lt;/code&gt; is zero or more batch dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7caaa20ec7806018b377988dd677601f3f83e4d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="121b8ceabf101922601fe3bb8a675692baf2cd6c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor. Must be at least 1-dimensional.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80a6967e7a1bc373747e4ceda61d8eca3bcbe71d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor. Must be at least 2-dimensional.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5176c20efe2b991fb3ef27de42aad8bfff75426" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9ce71bbfa8c25d01c8c318346faac5f3f4407cd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the size of &lt;code&gt;input&lt;/code&gt; will determine size of the output tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfcd8ed79796b25e26a5e670250a73272dac91fb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the source tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3cad4fa9c4a49fae5b079d14a2bcba4e124c2ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the square matrix of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09c118f2812817644cc5b84fc5bc2cef3421e809" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to be added</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="648002a80ff2960290b93380fa5ebeba8bd25668" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to be reshaped</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20715c854d53757985d23efabade3db743a6a166" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to compare</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c1865166964450f22d83f1a28613f076a2e569b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to compute the digamma function on</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2bf2493846223fdc5fbfe54614e402279de1747" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to compute the multivariate log-gamma function</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7fbfb129c11e7747b44805a7b05a9ebc40147948" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to narrow</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55c3c22205a1b70d592dec6666248c82e3690335" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to split</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3219155900c01df3eed96409ef1ed4b22f2fc99a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to unbind</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a48e0ca5030fc4d6af5bdfb8605eaef2755a644" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor with the starting points</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cc211cd1b8495af379d1d59ebf66ec1fb368257" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; vector to be added</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0f2899dfd7408899e467222cd4d0bd1c8b163e8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the dividend</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e495db7a10b097798f9ab905342091739c99080" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; N-D tensor or a Scalar containing the search value(s).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54b5b721c1619fe54cfff53778c25cbdce11a2a0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;list of Tensors&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e15a78cbbc8bd023145c6c784e4996207ef9b8e4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b1084c8300aad27ad1784e01fa04c5108343982" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; N-dimensional tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4ba99c5e33ab9d93f9eeb31063659728503213f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Quantized input of type &lt;code&gt;torch.quint8&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e32b4df67a1c6fa976c98f26ed880fe2843dd472" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The input tensor. If dim is None, x must be 1-D or 2-D, unless &lt;code&gt;ord&lt;/code&gt; is None. If both &lt;code&gt;dim&lt;/code&gt; and &lt;code&gt;ord&lt;/code&gt; are None, the 2-norm of the input flattened to 1-D will be returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="750efbed074a69bad5c30928c16477fc2a8db885" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbba8e02a600f5a44e37f994cfdcf15acf5f5674" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71024ef201be154545b2752c7794987629860a82" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc13ec8f5ecb355720aad713d541718fbcff8ff1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; quantized input</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca4fccfc307fc839f56fcd2935b753c3a675a514" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; quantized input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16adc05235f86a0bdba3f8d133f8ad24c146c434" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input SparseTensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f15e5e30b2c90290ac32a42b9dde72a1f212a74f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50bc5249b9927187f00659fce46d0556e5580c69" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor representing a half-Hermitian signal</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01ff6678d557f24c8e79e022f907950c5a82aff9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the real input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="912fbe0aeceffe41c39ef4b4f7a3bd4836fd3f31" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; Tensor containing bags of indices into the embedding matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="062fdb3d419326f615da299be36d7f8d3afff88a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; Tensor containing indices into the embedding matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="584da06db551e34afe167aa9b87c9a9f268850eb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; of shape &lt;code&gt;(batch, input_size)&lt;/code&gt;: tensor containing input features</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a3704197e0eca68967963d49706554f7e113eb5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; of shape &lt;code&gt;(seq_len, batch, input_size)&lt;/code&gt;: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;torch.nn.utils.rnn.pack_padded_sequence()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ae74c54ba996ad29eb2561904532cd22f9106ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; of shape &lt;code&gt;(seq_len, batch, input_size)&lt;/code&gt;: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;torch.nn.utils.rnn.pack_padded_sequence()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.nn.utils.rnn.pack_sequence#torch.nn.utils.rnn.pack_sequence&quot;&gt;&lt;code&gt;torch.nn.utils.rnn.pack_sequence()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d073db700f14add42abc2cdb78180dfcfcd6265b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input_lengths&lt;/strong&gt; &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aab54f56e166a12d78bbb3bb93f9dd9e315d0e5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input_list&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; List of tensors to reduce and scatter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="717931cd002baccf50d9c1a13fdb425f6fc9f174" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input_names&lt;/strong&gt; (&lt;em&gt;list of strings&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default empty list&lt;/em&gt;) &amp;ndash; names to assign to the input nodes of the graph, in order</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f6472508915540bb1432835a90442a80c1103a5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input_size&lt;/strong&gt; &amp;ndash; The number of expected features in the input &lt;code&gt;x&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c79f2d549859c0d70790524045ea3d4bb1424ec0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input_tensor_list&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; List of tensors to scatter one per rank.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9e6c1bd49660ef92968bbc7c3dbb9f033fb2e46" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input_tensor_list&lt;/strong&gt; (&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; List of tensors(on different GPUs) to be broadcast from current process. Note that &lt;code&gt;len(input_tensor_list)&lt;/code&gt; needs to be the same for all the distributed processes calling this function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22e3bf528f143b22c44616b9a8dc2f788e501ebd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input_tensor_lists&lt;/strong&gt; (&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3999ded48b1c7630b579dd8899eda50d65501cf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input_to_model&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;list of torch.Tensor&lt;/em&gt;) &amp;ndash; A variable or a tuple of variables to be fed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2029d7d088f0d74beff905706cfd5a1a9481029" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inputs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; A dict containing sample inputs indexed by method names in &lt;code&gt;mod&lt;/code&gt;. The inputs will be passed to methods whose names correspond to inputs&amp;rsquo; keys while tracing. &lt;code&gt;{ 'forward' : example_forward_input, 'method2': example_method2_input}&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="259474d5c791b17db10ad248774065b0e9b78321" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inputs&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; inputs to the module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f165a76f213ab42e9ce783f6d2d213f4d07554b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inputs&lt;/strong&gt; (&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; an iterable of tensors to add.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65e47b54be8e7f8c4dc3ba2fe581dac20a10b471" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inputs&lt;/strong&gt; (&lt;em&gt;sequence of Tensor&lt;/em&gt;) &amp;ndash; Inputs w.r.t. which the gradient will be returned (and not accumulated into &lt;code&gt;.grad&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="964b6fac878c1326f1958f5fa8f64bdcbae7b497" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inputs&lt;/strong&gt; (&lt;em&gt;tuple of Tensor&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; inputs to the function</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4f77e9ef68c7a234c10cc1f3e81d35349c96a59" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inputs&lt;/strong&gt; (&lt;em&gt;tuple of Tensors&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; inputs to the function &lt;code&gt;func&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66391e4dbaee5f27163769a515f3f8c3c1f8efb1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;interprocess&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the event can be shared between processes (default: &lt;code&gt;False&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f465a3e8d61ca508a03930e50b813d53ad7144f1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inverse_indices&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): (optional) if &lt;code&gt;return_inverse&lt;/code&gt; is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c290b4c2aff5a326fd2f54283da4670def4dca3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;is useful to see which input shapes contribute to the runtime&lt;/strong&gt; (&lt;em&gt;This&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d10b06bfdce2639f080a4d99b489d84dd548cc49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;is_master&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; True when initializing the server store, False for client stores.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="862ecdae56fdc1758ca8dd0945b92aba6b666d54" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;is_python_module&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt; (default), imports the produced shared library as a Python module. If &lt;code&gt;False&lt;/code&gt;, loads it into the process as a plain dynamic library.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e21dc46b7713d2938778d4c4d6aae5e230920ee" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;join&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Perform a blocking join on all processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c36211dc611d5f7962125dc176c17631f0fc9ec6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;k&lt;/strong&gt; &amp;ndash; additive factor. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e95587d3412a9d195f0a6e77a6567373e57381f7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;k&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; k for the k-th smallest element</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="010b00e48fedcebcad1d2f9692050c44dd1f26ea" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;k&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of times to rotate</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1a820c8c605e02de337a9983694a9f1d90c85ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;k&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the k in &amp;ldquo;top-k&amp;rdquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e84cdd778b11f55c6727a09ff5312f00f77186ba" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;k&lt;/strong&gt; (&lt;em&gt;integer&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the number of requested eigenpairs. Default is the number of</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46176694182befc26b76cf4ee9533bb916707744" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kdim&lt;/strong&gt; &amp;ndash; total number of features in key. Default: None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="057988e71b8851f3d3a181e82f00cb24d94ed154" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;keep_initializers_as_inputs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default None&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16ca1136f6f7d7eedc320cddba2ed4a25d910b78" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;keepdim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether the output tensor has &lt;code&gt;dim&lt;/code&gt; retained or not</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18f7933e87f35c73c988304272aacb2824e2088e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;keepdim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether the output tensor has &lt;code&gt;dim&lt;/code&gt; retained or not.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8941cf07c98d813d55809e96bc7f701eb5365e5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;keepdim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether the output tensor has &lt;code&gt;dim&lt;/code&gt; retained or not. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5cc82008e4660ec6acca882ad0953eda363fe762" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;keepdim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether the output tensor has &lt;code&gt;dim&lt;/code&gt; retained or not. Ignored if &lt;code&gt;dim=None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e56a9eeb53ccea32a9904a25d931267678cd1dec" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;keepdim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Determines whether or not to keep the vector dimension. Default: False</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad4ef3b64308f3d8dc47146285eef285afa65b13" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;keepdim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If set to True, the reduced dimensions are retained in the result as dimensions with size one. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49656d1fc9d8e7326c0f9cd0a4a399d7d00a8efc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;keepdim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether the output tensors have &lt;code&gt;dim&lt;/code&gt; retained or not. Ignored if &lt;code&gt;dim&lt;/code&gt; = &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; = &lt;code&gt;None&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e926f558ead95bc431753d8cae0a5a04ab0c981c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; &amp;ndash; The size of the sliding window, must be &amp;gt; 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f384943e590807dc76a3468582f227aedc4a238c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; &amp;ndash; a single int, the size of the window</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="572a73e28152cca7b78a46ac5637d93451f8a981" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; &amp;ndash; size of the pooling region. Can be a single number or a tuple &lt;code&gt;(kH, kW)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="014e7d7bdeac3998dbeeee898f78081432eebec9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; &amp;ndash; size of the pooling region. Can be a single number or a tuple &lt;code&gt;(kT, kH, kW)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a26c47fa3182db8a890cdda0e99882bc0c208430" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; &amp;ndash; the size of the window</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6caea6991ef207d1f8220f4b3d0bf3cc83fa3f4e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; &amp;ndash; the size of the window to take a max over</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="771b7c3eece18f47a1bfb3f3eecd292084766b6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; &amp;ndash; the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple &lt;code&gt;(kh, kw)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86dbe9a6fa4b3ebfff876256dbee957af7b6f899" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; &amp;ndash; the size of the window. Can be a single number or a tuple &lt;code&gt;(kW,)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6983b15f4f38a9ac825f42d5eb9c0f2f04b3cf67" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; Size of the convolving kernel</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be2e08d3d82748f43712995275bcdf4a3dd15ad5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; Size of the max pooling window.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd3efc67efa8a3a8afc9e450066251f14074f4dd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kernel_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; the size of the sliding blocks</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0114fc6c0cd33e2cd2e18270605b3182ff85045a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;key, and value have the same number of features.&lt;/strong&gt; (&lt;em&gt;query&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84a1ee94b9552fbe98a263714fe34401d07c1736" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;key, value&lt;/strong&gt; (&lt;em&gt;query&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;) &amp;ndash; map a query and a set of key-value pairs to an output. See &amp;ldquo;Attention Is All You Need&amp;rdquo; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0822e8093f74e590339677b718f3d7e76f763f1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;key&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The function will return the value associated with this key.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b269c0df40f6a8dea8192305fc8edbdb790e8a51" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;key&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The key in the store whose counter will be incremented.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a79165af1a31b78c12a0c79537ce451875db8c0a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;key&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The key to be added to the store.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a420b4a7c78ffaa362bae22a02d609b5238dbdad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;key&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The key to be deleted from the store</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dbc5adefc4e865a7f69b4455755c908e2aa6cbdc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;key&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; key to pop from the ModuleDict</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f26ac6300ccaa9e27a066b28715d3964edce7d3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;key&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; key to pop from the ParameterDict</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78144a9a3d220664056838664be8661e6c3c9060" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;key_padding_mask&lt;/strong&gt; &amp;ndash; if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. When given a byte mask and a value is non-zero, the corresponding value on the attention layer will be ignored</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64d040096feaaa3ad1d281fdbf7b1e7688897fff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;keys&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; List of keys on which to wait until they are set in the store.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="044d19f693d2f3d8a173c57835f733ffa0c81a94" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kwargs&lt;/strong&gt; &amp;ndash; Any keyword arguments.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1a566ba7fd0d874e168c17e63abfc5faaeda0fb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kwargs&lt;/strong&gt; &amp;ndash; any keyword argument (unused)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="266f88e78551cdfc4dd3b12f3bdb84242fe5cbc6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kwargs&lt;/strong&gt; &amp;ndash; arguments to pass to the optimizer constructor on each worker.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="792aa49fbbca0bb348cbf77a79dc714af59b3ad2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kwargs&lt;/strong&gt; &amp;ndash; keyword arguments passed on to a subclass of a &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fedd0f20c005d43f5156a1e433f31ab84c26bd81" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kwargs&lt;/strong&gt; &amp;ndash; keyword arguments passed on to a subclass of a &lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c0b946c6e8e6d58a96c5dc6767edc5a0986bd04" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kwargs&lt;/strong&gt; &amp;ndash; other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters. If &lt;code&gt;float&lt;/code&gt;, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If &lt;code&gt;int&lt;/code&gt;, it represents the absolute number of parameters to prune.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0fd99fc795f5d48a5e60df282d78e00104ba241" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;kwargs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; is a dictionary of keyword arguments for the &lt;code&gt;func&lt;/code&gt; invocation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13138447f871c2af389a4fca56a99836f8077860" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;label_img&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Images correspond to each data point</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c49724fe69a85b49c33c508646fb23cf737c434f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;labels&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;numpy.array&lt;/em&gt;&lt;em&gt;, or &lt;/em&gt;&lt;em&gt;string/blobname&lt;/em&gt;) &amp;ndash; Ground truth data. Binary label for each element.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="264535b79c947ac6ae0426c447265ae41129aa0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lambd&lt;/strong&gt; &amp;ndash; the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb1bfc3c10cee85b47368e280d5fa5b8b99d7871" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lambd&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; decay term (default: 1e-4)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1661e6401aad1339abbf5bd62f0c278ec8d6d90e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;largest&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether to return largest or smallest elements</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fdf7fc35f25f5906a12f37c633b280b391c3c3d8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;largest&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; when True, solve the eigenproblem for the largest eigenvalues. Otherwise, solve the eigenproblem for smallest eigenvalues. Default is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c816953802bf0955cc5760c186166581884c57d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;last element is the size of the input, or the ending index position of the last bag&lt;/strong&gt; (&lt;em&gt;The&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b2a08a3d9a86f04fd85848e910356b2127aa22d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;last_epoch&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The index of last epoch. Default: -1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee8be3a2ca398755a3635c9afaf9cf0d39b726e3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;last_epoch&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The index of the last batch. This parameter is used when resuming a training job. Since &lt;code&gt;step()&lt;/code&gt; should be invoked after each batch instead of after each epoch, this number represents the total number of &lt;em&gt;batches&lt;/em&gt; computed, not the total number of epochs computed. When last_epoch=-1, the schedule is started from the beginning. Default: -1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac7e5e95f90923fa77510427955802f84f53c2d7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;last_epoch&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The index of last epoch. Default: -1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88e9ebec7af1894b1a89ea6a01d54fd47775eace" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;layout&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; currently only support &lt;code&gt;torch.strided&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bf3f4fbf68fed19ae95979f8c8178ebe6d46e40" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;layout&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired layout of returned Tensor. Default: &lt;code&gt;torch.strided&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94414c640396bd9bbbf2ad80f1e3206b610ac0ef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;layout&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired layout of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, defaults to the layout of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0db1c24c17c8fb5d6f179479b0b643184229505b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;layout&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired layout of returned window tensor. Only &lt;code&gt;torch.strided&lt;/code&gt; (dense layout) is supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46d7f78e6035019338c7eef0206b97309c29f933" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;layout&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; {categoryName: &lt;em&gt;charts&lt;/em&gt;}, where &lt;em&gt;charts&lt;/em&gt; is also a dictionary {chartName: &lt;em&gt;ListOfProperties&lt;/em&gt;}. The first element in &lt;em&gt;ListOfProperties&lt;/em&gt; is the chart&amp;rsquo;s type (one of &lt;strong&gt;Multiline&lt;/strong&gt; or &lt;strong&gt;Margin&lt;/strong&gt;) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e94626f3232ee79b88ea32ac6e17800f0711856b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;length&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the distance to the ending dimension</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f03be3c5e51ff8247aa0a57df0a1b4f93ee5396" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;length&lt;/strong&gt; (&lt;em&gt;Optional&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; The amount to trim the signal by (i.e. the original signal length). (Default: whole signal)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4ad30add260bff538fca1a6ab975e7e36e60203" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lengths&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; list of sequences lengths of each batch element.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46d7256e9acb60210f2e5001a7a7ab48fd8622b3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lengths&lt;/strong&gt; (&lt;em&gt;sequence&lt;/em&gt;) &amp;ndash; lengths of splits to be produced</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d4ae06157920c0edc4da18c286dd2281a28071e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;line_search_fn&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; either &amp;lsquo;strong_wolfe&amp;rsquo; or None (default: None).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2ddc7f0cb0f7aa1279d7434e7a52c24cdfc3d5a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;linewidth&lt;/strong&gt; &amp;ndash; The number of characters per line for the purpose of inserting line breaks (default = 80). Thresholded matrices will ignore this parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a0b73f33b90888a42c750b6b973c0a4519c14cd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;list&lt;/strong&gt; (&lt;em&gt;tensor&lt;/em&gt;) &amp;ndash; List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that &lt;code&gt;len(tensor_list)&lt;/code&gt; is the same for all the distributed processes calling this function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="932d285cb5500eeb2ef857a2e17a707d7fc096b4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loc&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Location parameter of the distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6cbcefb888e1ab9711f04b51a668b64bf12e4ef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loc&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; mean of log of distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc194cc545cdcccb14c930cfbe31012644c22453" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loc&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; mean of the distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="205933e969912634f51b550fa1e983040295221b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loc&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; mean of the distribution (often referred to as mu)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1b86547692726edeab7f94112d4af9d07100389" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loc&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; mode or median of the distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f219815927153fd66f4cdab28f993a70a68b018" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loc&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; mean of the distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9da880570a6da88adfb3bad6c9b9e0a223dc682" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loc&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; mean of the distribution with shape &lt;code&gt;batch_shape + event_shape&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77f321ea22ac7ecd844fd4c85d32d738eb4a9fcd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loc&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Location parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1453ad4abaa60f2ac5cb1c6b5b077012128a2a7c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loc&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; an angle in radians.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a6af9ed1c4f286897f05981280cbd8b1f3ff4ebb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;log_dir&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Save directory location. Default is runs/&lt;strong&gt;CURRENT_DATETIME_HOSTNAME&lt;/strong&gt;, which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in &amp;lsquo;runs/exp1&amp;rsquo;, &amp;lsquo;runs/exp2&amp;rsquo;, etc. for each new experiment to compare across them.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e21a2b0779d46aa79f400ad4cdf3c0dba396033e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;log_input&lt;/strong&gt; &amp;ndash; if &lt;code&gt;True&lt;/code&gt; the loss is computed as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6e337713f719f7f73fae5c0a44576d8936beac7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;log_input&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt; the loss is computed as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f3912ec8124045880111aeedd237bc31f76c3a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;log_probs&lt;/strong&gt; &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a96e8db96ae8039870ee95945a0d311143381324" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;log_target&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; A flag indicating whether &lt;code&gt;target&lt;/code&gt; is passed in the log space. It is recommended to pass certain distributions (like &lt;code&gt;softmax&lt;/code&gt;) in the log space to avoid numerical issues caused by explicit &lt;code&gt;log&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7071da2a7b3f7ca5a3045161a47ac564d02ec18a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;log_target&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies whether &lt;code&gt;target&lt;/code&gt; is passed in the log space. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5bfe7f9ad9a8759384f7463a9f5c13f4bcc1cdb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;logits&lt;/strong&gt; &amp;ndash; &lt;code&gt;[&amp;hellip;, num_features]&lt;/code&gt; unnormalized log probabilities</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7b0e513e660e6ae0aecd8ce0340f25835570b8b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;logits&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Event log-odds</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cd67273d6115306dfd71269c8f730293cecfea1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;logits&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Event log-odds for probabilities of success</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0dd62d6b118866e10be092ef3745e38b09d4ce9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;logits&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; event log probabilities</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2c4916a23bcf24ab00242649727ed2f95b91453" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;logits&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; event log-odds</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7efd530f6262255b871809674d067ec842a82da" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;logits&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the log probability of each event.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f04b583a1f4e8907bdd1dcfa0e69ececfe642707" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;logits&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; real valued parameters whose sigmoid matches &amp;lsquo;probs&amp;rsquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3642c00caa6566edf3d584465c065796bfd40e70" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;logits&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the log-odds of sampling &lt;code&gt;1&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b2712af2da3a7a55410ea9fc1006d769c2d438d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;logits&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the log-odds of sampling &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bef4bf0c6f4823e7e112fc81eda3d3be435f702" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;loss&lt;/strong&gt; is a Scalar representing the computed negative log likelihood loss</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cf1dfa5f62dbefc053a252f2503eebdfb537995" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;low&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; lower range (inclusive).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70e041ec68422ba2e379d712b1ccecdb156099a1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;low&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Lowest integer to be drawn from the distribution. Default: 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c5a7d8df51ea63239d3ca2a365d80e7b7f44e08" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lower&lt;/strong&gt; &amp;ndash; lower bound of the uniform distribution. Default:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7b36d6b2207205361968b5b3da4740b41a65c67" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; learning rate</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0b7b5f75303acbfe6aeda36499b4e6455a92b41" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; learning rate (default: 1)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad462d2123563bca49e628fcb70d455948a9eba3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; coefficient that scale delta before it is applied to the parameters (default: 1.0)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eca11e2fa06744018267d874d481642b6a99b183" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; learning rate (default: 1e-2)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a6cad43fae486eff51402dafbfac4db7b6b965ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; learning rate (default: 1e-3)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="167483491609d9a0143c824fa5f5fb5841848e5f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; learning rate (default: 2e-3)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d35030ab8c415ed71a5635972835c914552863c0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lr_decay&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; learning rate decay (default: 0)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ad1654b3c81c933afbb5f8a4c37726032cd7262" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;lr_lambda&lt;/strong&gt; (&lt;em&gt;function&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f0a9fa0370655d11d3532fe9815ce442456bb8a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;m&lt;/strong&gt; &amp;ndash; A &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; to save.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd123d273fbb02028d3c0300d6ad3c2fce942d1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;m&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the number of columns with default being &lt;code&gt;n&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70f0c622ba7f16dab21096968ca88594ae3eb5fb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;main_tag&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; The parent name for the tags</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44bef03b746085c112465e22dd0b732f7855866e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;map_location&lt;/strong&gt; &amp;ndash; a function, &lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, string or a dict specifying how to remap storage locations</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef33fe86d546f14a70960dd930eacfcbfad6910b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;map_location&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a function or a dict specifying how to remap storage locations (see torch.load)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="914b80d642dfeaba6272f2de3bf65d1022719a77" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;map_location&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; A simplified version of &lt;code&gt;map_location&lt;/code&gt; in &lt;code&gt;torch.jit.save&lt;/code&gt; used to dynamically remap storages to an alternative set of devices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38f1d58703037b5452e6debc51817125c3e4159a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mapping&lt;/strong&gt; &amp;ndash; a dictionary that maps from nn module to nnq module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcff61dc1a63e697b62ab44d808a367653b8b401" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mapping&lt;/strong&gt; &amp;ndash; a dictionary that maps from source module type to target module type, can be overwritten to allow swapping user defined Modules</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aba51605b877792de1fe61b3ad8c44e8616a52a0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mapping&lt;/strong&gt; &amp;ndash; correspondence between original module types and quantized counterparts</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd0f4742cec293420674fc04d6567be19166a206" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mapping&lt;/strong&gt; &amp;ndash; dictionary that maps float modules to quantized modules to be replaced.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ee71f6909a96c9a3f4f2b409a0beb65f931f258" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mapping&lt;/strong&gt; &amp;ndash; maps type of a submodule to a type of corresponding dynamically quantized version with which the submodule needs to be replaced</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a76d819de26b9a69327b4e9ff4618411dff8d317" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;margin&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A non-negative margin representing the minimum difference between the positive and negative distances required for the loss to be 0. Larger margins penalize cases where the negative examples are not distant enough from the anchors, relative to the positives. Default:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d33d454f6271f798670e376263ffba9cbf000630" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;margin&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Default:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56ee260326698f65dd6b6c40f6a508083eb54156" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;margin&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Has a default value of</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="024e1f6072ab1c5c9f6954c815378eb35255dec0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;margin&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Has a default value of &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a1c2406f918b1a2ebe9a9c579789d15e7910666" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;margin&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Should be a number from</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc236878c72bde16cfeffe2cac289d6375b4d7f4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mask&lt;/strong&gt; &amp;ndash; the mask for the src sequence (optional).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e18ffc51ffd4959b08fbd0a3bfc5fa56421b9e3e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mask&lt;/strong&gt; (&lt;a href=&quot;#torch.BoolTensor&quot;&gt;BoolTensor&lt;/a&gt;) &amp;ndash; the boolean mask</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6415d8f4fc74353778af124a9edd9fa991dc2d49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.BoolTensor&quot;&gt;BoolTensor&lt;/a&gt;) &amp;ndash; the tensor containing the binary mask to index with</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09b9037dad942fd7a7535f675e34e4544c3bd0e2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; binary mask to be applied to the parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e34cd5d03db7bc2e1ff2fedd665486796048ea46" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mask&lt;/strong&gt; (&lt;em&gt;SparseTensor&lt;/em&gt;) &amp;ndash; a SparseTensor which we filter &lt;code&gt;input&lt;/code&gt; based on its indices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e019f9c4c52d2f155730f344ebf436e60e0683e6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat1&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first matrix to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b6257ac15daec460e7856dfaf4bb8570f4c2d6c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat1&lt;/strong&gt; (&lt;em&gt;SparseTensor&lt;/em&gt;) &amp;ndash; a sparse matrix to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d5be763c9e61f15bd0bc4d527929a6c1dd8cf14" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat1&lt;/strong&gt; (&lt;em&gt;SparseTensor&lt;/em&gt;) &amp;ndash; the first sparse matrix to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98880e96bbe3ad0fcc2eae35bc3ea2b501838cf1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the second batch of matrices to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0dc66f375564c02c1c9b573824bb73a7b00eaf7d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the second matrix to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f696dccd1ae44ddee20d75e7dbc3ff06722bc099" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat2&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; a dense matrix be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b94541c2bb36a977f8a383b6790acb61c7f9672c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat2&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the second dense matrix to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b163b780efadeab4d13fd7393ace98318c94610" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; matrix to be multiplied</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02d4e6fa23ead0a267a7e63391cb2505b4395991" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; a dense matrix to be added</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a658df425748dff8f4417d65ab2c445b2fdf38ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mat&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;numpy.array&lt;/em&gt;) &amp;ndash; A matrix which each row is the feature vector of the data point</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d80f81e280edcd37fa8ee208223d3ff8f1c41090" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;matrices&lt;/strong&gt; (&lt;em&gt;Tensors...&lt;/em&gt;) &amp;ndash; a sequence of 2 or more 2-D tensors whose product is to be determined.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85dfbb6d8965fcf255941fecf0a3f9636ed92d49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; upper end of the range (inclusive)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebd724e75db34df9a1697eb48e3e36ab4d0a6944" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; maximal value of each element in the output</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="408cb0787a19c61a9e10d267b40c421e0d8c65b8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; upper-bound of the range to be clamped to</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc3151b648e0f145cb6c4ddd3eb3ccbd16d8b4fb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_eval&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; maximal number of function evaluations per optimization step (default: max_iter * 1.25).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b027542905604e3b228f1858a3f524982a5efab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_iter&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; maximal number of iterations per optimization step (default: 20)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1468666ae5d01433e87114e46f75596cf1556e35" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; Upper learning rate boundaries in the cycle for each parameter group.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ce86a6965b6720f122017897d60ca85346a4401" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; Upper learning rate boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="377664c86626ff0059b1069c6f729fe1a4b4aa9e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_momentum&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is &amp;lsquo;max_momentum&amp;rsquo; and learning rate is &amp;lsquo;base_lr&amp;rsquo; Default: 0.95</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48deb872ed492bbcf29d1420a060c32f2d6bafb8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_momentum&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). The momentum at any cycle is the difference of max_momentum and some scaling of the amplitude; therefore base_momentum may not actually be reached depending on scaling function. Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is &amp;lsquo;max_momentum&amp;rsquo; and learning rate is &amp;lsquo;base_lr&amp;rsquo; Default: 0.9</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18d2c9f69eba48f7e795f2b2a4286820cf804061" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_norm&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; max norm of the gradients</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4b953b25b1cba200ce3cd937a97f9f2c77d5eca" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_norm&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If given, each embedding vector with norm larger than &lt;code&gt;max_norm&lt;/code&gt; is renormalized to have norm &lt;code&gt;max_norm&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7395b83c4b6884514e3400766844db4079e5559" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_norm&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If given, each embedding vector with norm larger than &lt;code&gt;max_norm&lt;/code&gt; is renormalized to have norm &lt;code&gt;max_norm&lt;/code&gt;. Note: this will modify &lt;code&gt;weight&lt;/code&gt; in-place.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ccc83f5976a7da4c6cce23bdc0f1b550bc1baf18" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_norm&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="616f5bd5271f84469f902e32daf9266f05355e51" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_norm&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f11300b5f0eb3648d7a253d285d8f366217f4085" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_queue&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Size of the queue for pending events and summaries before one of the &amp;lsquo;add&amp;rsquo; calls forces a flush to disk. Default is ten items.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac66a7d2aa72c2c8115cd465665fc5e11e447283" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;max_val&lt;/strong&gt; &amp;ndash; maximum value of the linear region range. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6600d2ace21bcb0602e70b0bbacf60b6d9d7c70c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;maxnorm&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the maximum norm to keep each sub-tensor under</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a295daf527fa3a8e842c0df2995c65b39e71553" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mean&lt;/strong&gt; &amp;ndash; the mean of the normal distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a707f91e951faba9097d24d07d405e7ec311e6c6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mean&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor of per-element means</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08c50d1f521b5ba09f6ffb7ce6539bb06a4a3f5b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mean&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the mean for all distributions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2955cca12dc9b4a6d69879869214d1c6770dfaaf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mean&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the mean for all distributions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29ba1a569247c06dc81809a7bf872a0c7a96859b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory&lt;/strong&gt; &amp;ndash; the sequence from the last layer of the encoder (required).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="588fc531f83b03eab1da4a2dc038d37bcca7f732" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_efficient&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a683ca0a51a8aaad293fb3004568d17d8503ae0f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_efficient&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; but slower. Default: &lt;em&gt;False&lt;/em&gt;. See &lt;a href=&quot;https://arxiv.org/pdf/1707.06990.pdf&quot;&gt;&amp;ldquo;paper&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="847323568a90086efa211de3ba9bf7786ee86423" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_format&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.memory_format&quot;&gt;&lt;code&gt;torch.memory_format&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired memory format of returned Tensor. Default: &lt;code&gt;torch.contiguous_format&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60c76323a555a35cf5636b2f519a421ab878338e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_format&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.memory_format&quot;&gt;&lt;code&gt;torch.memory_format&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired memory format of returned Tensor. Default: &lt;code&gt;torch.preserve_format&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c42c5648937cafb3804b61b720f4bb1140320b64" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_format&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.memory_format&quot;&gt;&lt;code&gt;torch.memory_format&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired memory format of returned tensor. Default: &lt;code&gt;torch.preserve_format&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb50e6c46386db61725174c03fb49b8c28339fd5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_format&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.memory_format&quot;&gt;&lt;code&gt;torch.memory_format&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; Specifies memory allocation order. Default: &lt;code&gt;torch.contiguous_format&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d82e413d97971822435897c587b2c7a54d9ab8b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_format&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.memory_format&quot;&gt;&lt;code&gt;torch.memory_format&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired memory format of Tensor. Default: &lt;code&gt;torch.contiguous_format&lt;/code&gt;. Note that memory format of &lt;code&gt;self&lt;/code&gt; is going to be unaffected if &lt;code&gt;self.size()&lt;/code&gt; matches &lt;code&gt;sizes&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bce750b8912dc23c58101b3c62c2faebe272c06d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_format&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.memory_format&quot;&gt;&lt;code&gt;torch.memory_format&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired memory format of Tensor. Default: &lt;code&gt;torch.contiguous_format&lt;/code&gt;. Note that memory format of &lt;code&gt;self&lt;/code&gt; is going to be unaffected if &lt;code&gt;self.size()&lt;/code&gt; matches &lt;code&gt;tensor.size()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c209a0208ea88f4d30020b7c6ec0aa20f55f05ef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_format&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.memory_format&quot;&gt;&lt;code&gt;torch.memory_format&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired memory format of returned Tensor. Default: &lt;code&gt;torch.contiguous_format&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e31755556d7e8f828a7fc59bf5245af37dfc95a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_format&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.memory_format&quot;&gt;&lt;code&gt;torch.memory_format&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired memory format of returned Tensor. Default: &lt;code&gt;torch.preserve_format&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="287794474bd0369fafef4c83cd018633d29d34b4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_format&lt;/strong&gt; (&lt;code&gt;torch.memory_format&lt;/code&gt;) &amp;ndash; the desired memory format for 4D parameters and buffers in this module (keyword only argument)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5a2da6780e7a50206f83afa209cf1daa2cf9b20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_key_padding_mask&lt;/strong&gt; &amp;ndash; the ByteTensor mask for memory keys per batch (optional).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad4fdd31c064f05525f133141ae55c8a9f9fb730" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_key_padding_mask&lt;/strong&gt; &amp;ndash; the mask for the memory keys per batch (optional).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="915d393bbda00e2dfc80c100ff44fe1a257646ed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_mask&lt;/strong&gt; &amp;ndash; the additive mask for the encoder output (optional).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="edbbc733c4baf5f4bca52c77bb7dfb7d5e94c518" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;memory_mask&lt;/strong&gt; &amp;ndash; the mask for the memory sequence (optional).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="420038f9d23e71250d8fc76d6db641779f55d944" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;metadata&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; A list of labels, each element will be convert to string</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbe6c83a85e6ac8b011f0e187353dedf1dd97a86" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;method&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; select LOBPCG method. See the description of the function above. Default is &amp;ldquo;ortho&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d982ed54ceb117939247c462414b383f4294a298" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;method&lt;/strong&gt; (&lt;em&gt;subclass of BasePruningMethod&lt;/em&gt;) &amp;ndash; child pruning method to be added to the container.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ffaae191023c8ad384bc60a006a096527ebb2fe7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;metric_dict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; Each key-value pair in the dictionary is the name of the metric and it&amp;rsquo;s corresponding value. Note that the key used here should be unique in the tensorboard record. Otherwise the value you added by &lt;code&gt;add_scalar&lt;/code&gt; will be displayed in hparam plugin. In most cases, this is unwanted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cde916a1fef1faecfaf91715c84b2290e3f3776e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;milestones&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; List of epoch indices. Must be increasing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6dd8004576bc7350c9204a1c0262ab2ddb2d227d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;min&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; lower end of the range (inclusive)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cef73dc49333eeeba5dd9f4d0c270dee0f730953" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;min&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; lower-bound of the range to be clamped to</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55821f56c0bcd501a5ab8f1798c3910d8b63ffff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;min&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; minimal value of each element in the output</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b55691eca82df0da0cd97dd8e15fa3fb8bfbaf8a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;min_lr&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="894532a159ed88aec99c8849664d2c1374ab2e44" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;min_val&lt;/strong&gt; &amp;ndash; minimum value of the linear region range. Default: -1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a625bfe7a46218038f5b0ece187b38a8f18c7b30" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;minlength&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; optional, minimum number of bins. Should be non-negative.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d62614dd84cb7f7218048107638c24812180feed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;missing_keys&lt;/strong&gt; is a list of str containing the missing keys</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a1dfa06520b080421ea46aff220b9d8cba6e0f5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mixture_distribution&lt;/strong&gt; &amp;ndash; &lt;code&gt;torch.distributions.Categorical&lt;/code&gt;-like instance. Manages the probability of selecting component. The number of categories must match the rightmost batch dimension of the &lt;code&gt;component_distribution&lt;/code&gt;. Must have either scalar &lt;code&gt;batch_shape&lt;/code&gt; or &lt;code&gt;batch_shape&lt;/code&gt; matching &lt;code&gt;component_distribution.batch_shape[:-1]&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2dfaac7466031cf7ac17e86ad10d9de9e7ba1a6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mod&lt;/strong&gt; &amp;ndash; input module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="311fd585d83045aa88cc3eccbed4762dcd351324" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mod&lt;/strong&gt; (&lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;) &amp;ndash; a float module, either produced by torch.quantization utilities or provided by the user</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84a7cd86ad210aba9a85651fe50b04b508106e2f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mod&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;torch.nn.Module&lt;/a&gt;) &amp;ndash; A &lt;code&gt;torch.nn.Module&lt;/code&gt; containing methods whose names are specified in &lt;code&gt;inputs&lt;/code&gt;. The given methods will be compiled as a part of a single &lt;code&gt;ScriptModule&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3ad16e3543c64bbf2963c9325a83ed0fc537280" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; &amp;ndash; &lt;code&gt;'constant'&lt;/code&gt;, &lt;code&gt;'reflect'&lt;/code&gt;, &lt;code&gt;'replicate'&lt;/code&gt; or &lt;code&gt;'circular'&lt;/code&gt;. Default: &lt;code&gt;'constant'&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ead1532b9e3bb8d41618811a33c97bd27a1a40b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; &amp;ndash; either &lt;code&gt;'fan_in'&lt;/code&gt; (default) or &lt;code&gt;'fan_out'&lt;/code&gt;. Choosing &lt;code&gt;'fan_in'&lt;/code&gt; preserves the magnitude of the variance of the weights in the forward pass. Choosing &lt;code&gt;'fan_out'&lt;/code&gt; preserves the magnitudes in the backwards pass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de67441ea808eb9718fb3c1203b927e8646bf7b6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Controls whether to enable flush denormal mode or not</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5954b2cd8044ad215ccb68f0b03903d436f396bd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Flag whether to enable anomaly detection (&lt;code&gt;True&lt;/code&gt;), or disable (&lt;code&gt;False&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f0ebb80a4f6d234b3bdc8d88adb240b1631fa72" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Flag whether to enable grad (&lt;code&gt;True&lt;/code&gt;), or disable (&lt;code&gt;False&lt;/code&gt;). This can be used to conditionally enable gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c77c2993453675a1ae62a895e8af92b271ac8912" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether to set training mode (&lt;code&gt;True&lt;/code&gt;) or evaluation mode (&lt;code&gt;False&lt;/code&gt;). Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c363213dc9407a65b9c8b56a57a7e4b83827d58" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; One of &lt;code&gt;min&lt;/code&gt;, &lt;code&gt;max&lt;/code&gt;. In &lt;code&gt;min&lt;/code&gt; mode, lr will be reduced when the quantity monitored has stopped decreasing; in &lt;code&gt;max&lt;/code&gt; mode it will be reduced when the quantity monitored has stopped increasing. Default: &amp;lsquo;min&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e56f1e33b487200e30b2336e22da3a0101453c5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; One of {triangular, triangular2, exp_range}. Values correspond to policies detailed above. If scale_fn is not None, this argument is ignored. Default: &amp;lsquo;triangular&amp;rsquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7c74d3b555f009043988d195fb2bcd2f8149f0e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; algorithm used for upsampling: &lt;code&gt;'nearest'&lt;/code&gt; | &lt;code&gt;'bilinear'&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9c27d3e94ffaa56b5363537c49d87d1a211071d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; algorithm used for upsampling: &lt;code&gt;'nearest'&lt;/code&gt; | &lt;code&gt;'linear'&lt;/code&gt; | &lt;code&gt;'bilinear'&lt;/code&gt; | &lt;code&gt;'bicubic'&lt;/code&gt; | &lt;code&gt;'trilinear'&lt;/code&gt; | &lt;code&gt;'area'&lt;/code&gt;. Default: &lt;code&gt;'nearest'&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="385c5b4f0ae5e9736769d454130ca5949542ab90" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; interpolation mode to calculate output values &lt;code&gt;'bilinear'&lt;/code&gt; | &lt;code&gt;'nearest'&lt;/code&gt;. Default: &lt;code&gt;'bilinear'&lt;/code&gt; Note: When &lt;code&gt;mode='bilinear'&lt;/code&gt; and the input is 5-D, the interpolation mode used internally will actually be trilinear. However, when the input is 4-D, the interpolation mode will legitimately be bilinear.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4471a06de253ea298b84fb5695bcc2a35de32568" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the upsampling algorithm: one of &lt;code&gt;'nearest'&lt;/code&gt;, &lt;code&gt;'linear'&lt;/code&gt;, &lt;code&gt;'bilinear'&lt;/code&gt;, &lt;code&gt;'bicubic'&lt;/code&gt; and &lt;code&gt;'trilinear'&lt;/code&gt;. Default: &lt;code&gt;'nearest'&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e579f4ae9e49fbfc6b51a7253cb4e53b5007b03e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; algorithm used for upsampling: &lt;code&gt;'nearest'&lt;/code&gt; | &lt;code&gt;'bilinear'&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0454ac8b9a020a3d1a98ccef37fc6aef70ca1aae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; algorithm used for upsampling: &lt;code&gt;'nearest'&lt;/code&gt; | &lt;code&gt;'linear'&lt;/code&gt; | &lt;code&gt;'bilinear'&lt;/code&gt; | &lt;code&gt;'bicubic'&lt;/code&gt; | &lt;code&gt;'trilinear'&lt;/code&gt;. Default: &lt;code&gt;'nearest'&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac9a2bfc1170980964589f394256a29c3c8507f9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; &lt;code&gt;&quot;sum&quot;&lt;/code&gt;, &lt;code&gt;&quot;mean&quot;&lt;/code&gt; or &lt;code&gt;&quot;max&quot;&lt;/code&gt;. Specifies the way to reduce the bag. &lt;code&gt;&quot;sum&quot;&lt;/code&gt; computes the weighted sum, taking &lt;code&gt;per_sample_weights&lt;/code&gt; into consideration. &lt;code&gt;&quot;mean&quot;&lt;/code&gt; computes the average of the values in the bag, &lt;code&gt;&quot;max&quot;&lt;/code&gt; computes the max value over each bag. Default: &lt;code&gt;&quot;mean&quot;&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c098c83728e1a75267ad9d800f600477ae405fc6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; &lt;code&gt;&quot;sum&quot;&lt;/code&gt;, &lt;code&gt;&quot;mean&quot;&lt;/code&gt; or &lt;code&gt;&quot;max&quot;&lt;/code&gt;. Specifies the way to reduce the bag. Default: &lt;code&gt;&quot;mean&quot;&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83cf0f99fac2d6532263c56e5750c95e9d23633d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;mode&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation. Default: &lt;code&gt;&quot;mean&quot;&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9774346c37a84f31aa240cbbfdfce4fadf6d9d11" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;model&lt;/strong&gt; &amp;ndash; Model containing the modules to be fused</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29b2c437e7636535db29713e4220f5fd433d8cb9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;model&lt;/strong&gt; &amp;ndash; input float model</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db0e889d7e53c74b060a05b4d68552e00a301946" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;model&lt;/strong&gt; &amp;ndash; input model</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7201a6681f7e3b18a60413f391b0568bbce7574" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;model&lt;/strong&gt; &amp;ndash; input model to be modified in-place</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cce56abc7cd237dbad132a5abb856634ed2030b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;model&lt;/strong&gt; (&lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;torch.nn.Module&lt;/a&gt;) &amp;ndash; Model to draw.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7f141d3875fe7470c5eeb6311e3f52659403c7c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;model&lt;/strong&gt; (&lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;torch.nn.Module&lt;/a&gt;) &amp;ndash; the model to be exported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f38a617ca9f3ce30bb1aa507b57c0f8b9bb5233" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;model&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; a string of entrypoint name defined in repo&amp;rsquo;s hubconf.py</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="522ae153c9de2225d354ad39a76bc7201f1315a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;model&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; the name of a callable (entrypoint) defined in the repo/dir&amp;rsquo;s &lt;code&gt;hubconf.py&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="047b6c0ef907781caf5f3b90b83833193ffbb540" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;model_dir&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; directory in which to save the object</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26db7d624e519d251822dbc050f229070d6875cc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; &amp;ndash; input module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69051b940b13590bcf82fdf23556640963ae7ded" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; &amp;ndash; input module with qconfig attributes for all the leaf modules</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="337535b0d4923600e1e36494036f6ec5fbd41fed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; &amp;ndash; input module with qconfig attributes for all the leaf modules that we want to quantize</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff92bd1f8822db4dee990724fb95f189b28d6861" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;#torch.nn.Module&quot;&gt;Module&lt;/a&gt;) &amp;ndash; child module to be added to the module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4911eeef12d8491f308531700e789704028a53b0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;) &amp;ndash; the module to evaluate in parallel</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c599015a8c55f21c464f35319477082682454deb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;) &amp;ndash; child module to be added to the module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13729f6038be326761df6ac3ec24b201e785b71f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;) &amp;ndash; containing module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f81af303cce1d2b13d56b554d4c54084215c693d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;) &amp;ndash; module to be parallelized</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af97e1697d5f0c0f0186596bfc816ba5d1b7233e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;nn.Module&lt;/a&gt;) &amp;ndash; containing module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca220270bc403ee33276152d6bc7a361e3f02584" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;nn.Module&lt;/a&gt;) &amp;ndash; module containing one or more attr:&lt;code&gt;BatchNorm*D&lt;/code&gt; layers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4aa37d25cda1e46d9840e5ce1e3bb87b97cabc4c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;nn.Module&lt;/a&gt;) &amp;ndash; module containing the tensor to prune</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c58562dc7c80d28ea1d3eb37836c8dd81358feb4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;nn.Module&lt;/a&gt;) &amp;ndash; module to append</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49d181f6c9a5ed2484bd66e1605a7cd8ed49448c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;nn.Module&lt;/a&gt;) &amp;ndash; module to insert</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60a1ae701ce0942d303d74786dea8b40965fbefe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;nn.Module&lt;/a&gt;) &amp;ndash; object that is either pruned or unpruned</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1cf5810a0ac4510111b7830d78e712ab1696f2c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;modules&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;) &amp;ndash; a mapping (dictionary) from string to &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, or an iterable of key-value pairs of type (string, &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36d886b50a323b839fe922cbffadfc70109408b5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;modules&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;) &amp;ndash; iterable of modules to append</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8c11c9ebc42a4e244f2c526e48d9476468d0936" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;modules&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82d83ba7e9a63e6fb88f202a85d0e2b54d0d5137" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;modules&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; an iterable of modules to add</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04bab62073b3560de718947a04af8b4f3a39a01e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;modules_to_fuse&lt;/strong&gt; &amp;ndash; list of list of module names to fuse. Can also be a list of strings if there is only a single list of modules to fuse.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="338d2bdd3427c2acd5798eb121524c531792b37a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;momentum&lt;/strong&gt; &amp;ndash; the value used for the running_mean and running_var computation. Can be set to &lt;code&gt;None&lt;/code&gt; for cumulative moving average (i.e. simple average). Default: 0.1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f0648d0e331c6484261bc1cae891f8cb0fe0f77" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;momentum&lt;/strong&gt; &amp;ndash; the value used for the running_mean and running_var computation. Default: 0.1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c19c4601cb6ff68b31939379514e144918decc6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;momentum&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; momentum factor (default: 0)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8ec4544b5e2bda9b818610177f75772c12c28c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;most and may help with size-specific optimizations or&lt;/strong&gt; (&lt;em&gt;the&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70b75af3861208c958c212d2f7e306b0528d2927" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;msg&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; ASCII message to associate with range</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="477e9d05b66b55fc3c54bb2c97211fbf7a30ec4d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;msg&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; ASCII message to associate with the event.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ffcdc1e2419287f46a2eda06597a5711ebd39cd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the number of rows</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3afc73d777f69bfccc910de139a6b9b0a792a61" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the order of the polygamma function</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d0ffe3929c3dd260792f2cfcd25cf94c93f071d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the power to raise the matrix to</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb3a47c4c5e60d1d87f9833a363f367729f6f4f1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the upper bound (exclusive)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62cd9cb1469a9c478de28e4b1f09e1a6c903f1e0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;inf&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;-inf&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;'fro'&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;'nuc'&lt;/em&gt;) &amp;ndash; See documentation of valid entries for argument &lt;code&gt;p&lt;/code&gt; in &lt;a href=&quot;torch.norm#torch.norm&quot;&gt;&lt;code&gt;torch.norm()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="360665e64fcf6fd508a3beb2effb1f77d9cc3d75" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Output signal length. This determines the length of the output signal. If given, the input will either be zero-padded or trimmed to this length before computing the real IFFT. Defaults to even output: &lt;code&gt;n=2*(input.size(dim) - 1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f3dcf24d971f3f927b31f57cb8d0f3aa39957de" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Output signal length. This determines the length of the real output. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian FFT. Defaults to even output: &lt;code&gt;n=2*(input.size(dim) - 1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="765bbbc030c273dcfddf61dec277dea877826f3b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the FFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="280814a17cd4d7c5c87be0ac03bb06c0096d600b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian IFFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19a143bdd240b89803874ef31fae83cbc10ad3d8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the IFFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="437d2ca26e7ac533d4bd5f39effee7652ff0b3a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the real FFT.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8742179ec0bc2a9eaec8d9e4144c1b9a9f8cd29e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;em&gt;Int&lt;/em&gt;) &amp;ndash; The number of steps to fast-forward by.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a8c721a95c8d2c05e104145a151d6fead527c01" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;em&gt;Int&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The length of sequence of points to draw. Default: 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86e79b2a07cba2a326622f21aedf96b60e044c1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n&lt;/strong&gt; (&lt;em&gt;integer&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9509b9869a54b18b43d0ef80248e150060481aa5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n_classes&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of classes in the dataset</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="664468f137b786aae9b4053649115d89d17971a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n_fft&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Size of Fourier transform</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64c539b9a2fc08964cebb89aafe82cd323a6d67b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n_fft&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; size of Fourier transform</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73f1e40726186373d5acaec57f94cdeb84386c79" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;n_power_iterations&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; number of power iterations to calculate spectral norm</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c72673dc830afffed194953be854bb3754726c42" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;name, input shapes) rather than just event name.&lt;/strong&gt; (&lt;em&gt;(&lt;/em&gt;&lt;em&gt;event&lt;/em&gt;) &amp;ndash;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be6a1d5fe41bddbb64767aa43f1844148b7a2d5d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;name&lt;/strong&gt; &amp;ndash; The name of the extension to build. This MUST be the same as the name of the pybind11 module!</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c69de6701f67b2c147cd64b9cd2a3b47551823b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; a globally unique name of this node. (e.g., &lt;code&gt;Trainer3&lt;/code&gt;, &lt;code&gt;ParameterServer2&lt;/code&gt;, &lt;code&gt;Master&lt;/code&gt;, &lt;code&gt;Worker1&lt;/code&gt;) Name can only contain number, alphabet, underscore, colon, and/or dash, and must be shorter than 128 characters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5183afed8cf3601b43896332cb5eec33dea53c97" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; parameter name within &lt;code&gt;module&lt;/code&gt; on which pruning will act.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd742c4f6eedba1b2162f8cb069fd19852c81f0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; name of weight parameter</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34a6862e100f7bbd8be83c2e2456ade36746f257" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;name&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; name of the buffer. The buffer can be accessed from this module using the given name</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a86d2efe3016120aa2e1ea06fc78fd6bcd8369d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;name&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; name of the child module. The child module can be accessed from this module using the given name</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="369152d10b48dccc5d7997cedd438126e9bd2abc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;name&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; name of the parameter. The parameter can be accessed from this module using the given name</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c67b5abf72fae7d381fa3b1d37bcbc664d754902" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;names&lt;/strong&gt; (&lt;em&gt;iterable of str&lt;/em&gt;) &amp;ndash; The desired dimension ordering of the output tensor. May contain up to one Ellipsis that is expanded to all unmentioned dim names of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18ba741fd674f07602eeb307e77e22ffd8697dcb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;names&lt;/strong&gt; (&lt;em&gt;iterable of str&lt;/em&gt;) &amp;ndash; The desired names of the output tensor. May contain up to one Ellipsis.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dcce6a007ca6ff3a948d0e46c57a9b25d051833" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;need_weights&lt;/strong&gt; &amp;ndash; output attn_output_weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="343ed070ed3d41cc4c3ee63a9a133db9d809bff0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;negative_slope&lt;/strong&gt; &amp;ndash; Controls the angle of the negative slope. Default: 1e-2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8e8bb0627658c20cc0c4866f6e07da9cc54f0c6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;nesterov&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; enables Nesterov momentum (default: False)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2da9e071126ba50e59d5abd1403683c0e2d00d6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;new_interval&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Value to use as the new growth interval.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4bec839b86f5243846ab899511cc5d003c1c78ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;new_scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Value to use as the new scale backoff factor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="022de68da67d2ead164e455f77fd162fdd34ca79" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;new_scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Value to use as the new scale growth factor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7ed1221b27b3bb5a36954c20d1c72571bd4d1d1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;new_scale&lt;/strong&gt; (float or &lt;code&gt;torch.cuda.FloatTensor&lt;/code&gt;, optional, default=None) &amp;ndash; New scale factor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c70cf625cede657dde5d0c120d5a1f74a918a253" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;new_state&lt;/strong&gt; (&lt;em&gt;torch.ByteTensor&lt;/em&gt;) &amp;ndash; The desired state</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7fd74008cc5669b78289c4a9b06d25331220701d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;new_state&lt;/strong&gt; (&lt;em&gt;torch.ByteTensor&lt;/em&gt;) &amp;ndash; The desired state.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cd655a13d52cfc3e39cde6648a63b09b5ee820b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;new_states&lt;/strong&gt; (&lt;em&gt;Iterable of torch.ByteTensor&lt;/em&gt;) &amp;ndash; The desired state for each device</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8c5015026d05f7c2175e0d687fea36008f02797" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;new_strategy&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; Name of the selected strategy. Should be one of the values returned by &lt;a href=&quot;#torch.multiprocessing.get_all_sharing_strategies&quot;&gt;&lt;code&gt;get_all_sharing_strategies()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3684197908f63147ef8bf8af4038b1e7eef5b679" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;nhead&lt;/strong&gt; &amp;ndash; the number of heads in the multiheadattention models (default=8).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c047d9990ce17c7638516b2eb9cd4d4eefd9f13" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;nhead&lt;/strong&gt; &amp;ndash; the number of heads in the multiheadattention models (required).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf9b1d87107478239f509cdbfbfba2359192e4ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;niter&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use &lt;code&gt;-1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="474b45a07e5133e74b4680b4ebef67231879479c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;niter&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="179187db6714db7b86343992867677ff4ff75191" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;non_blocking&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt; and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fda458f2f7dc476cf65c2462d3a8061f49814e9d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;non_blocking&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt; and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5e9c95303e9f5242b9e3eeef2ca091abb7d7587" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;non_blocking&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect.</source>
          <target state="new"/>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
