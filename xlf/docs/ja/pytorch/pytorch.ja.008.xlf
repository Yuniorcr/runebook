<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="14ee5344d06a238323bb82692b3ba5195a694523" translate="yes" xml:space="preserve">
          <source>The RPC package also provides decorators which allow applications to specify how a given function should be treated on the callee side.</source>
          <target state="translated">RPC パッケージは、アプリケーションが着呼側で与えられた関数をどのように扱うかを指定できるようにするデコレータも提供します。</target>
        </trans-unit>
        <trans-unit id="d97e45dd1ab295237092f3a7f655e6e94838339b" translate="yes" xml:space="preserve">
          <source>The RPC tutorials introduce users to the RPC framework, provide several example applications using &lt;a href=&quot;#distributed-rpc-framework&quot;&gt;torch.distributed.rpc&lt;/a&gt; APIs, and demonstrate how to use &lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;the profiler&lt;/a&gt; to profile RPC-based workloads.</source>
          <target state="translated">RPCチュートリアルでは、ユーザーにRPCフレームワークを紹介し、&lt;a href=&quot;#distributed-rpc-framework&quot;&gt;torch.distributed.rpc&lt;/a&gt; APIを使用したいくつかのサンプルアプリケーションを提供し&lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;、プロファイラー&lt;/a&gt;を使用してRPCベースのワークロードをプロファイリングする方法を示します。</target>
        </trans-unit>
        <trans-unit id="1362003f90d57874aad22424c41e2c89849947f1" translate="yes" xml:space="preserve">
          <source>The RRef design note covers the design of the &lt;a href=&quot;#rref&quot;&gt;RRef&lt;/a&gt; (Remote REFerence) protocol used to refer to values on remote workers by the framework.</source>
          <target state="translated">RRefデザインノートは、フレームワークによってリモートワーカーの値を参照するために使用される&lt;a href=&quot;#rref&quot;&gt;RRef&lt;/a&gt;（リモートREFerence）プロトコルの設計をカバーしています。</target>
        </trans-unit>
        <trans-unit id="e195940e5abd6ebbfad00ac76917c59c49c9651e" translate="yes" xml:space="preserve">
          <source>The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time. The interface of this function is modeled after the &lt;a href=&quot;https://librosa.org/doc/latest/generated/librosa.stft.html&quot;&gt;librosa&lt;/a&gt; stft function.</source>
          <target state="translated">STFTは、入力の短いオーバーラップウィンドウのフーリエ変換を計算します。これにより、時間の経過とともに変化する信号の周波数成分が得られます。この関数のインターフェースは、&lt;a href=&quot;https://librosa.org/doc/latest/generated/librosa.stft.html&quot;&gt;librosastft&lt;/a&gt;関数をモデルにしています。</target>
        </trans-unit>
        <trans-unit id="56e279d91a2e6e157edb87095248831fbd79e513" translate="yes" xml:space="preserve">
          <source>The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example:</source>
          <target state="translated">SummaryWriterクラスは、TensorBoardで消費したり可視化したりするためのデータをログに記録するためのメインエントリです。例えば、以下のようになります。</target>
        </trans-unit>
        <trans-unit id="e88b194ff4e6feceb2bfaaa1ec38472db3a49b02" translate="yes" xml:space="preserve">
          <source>The TensorPipe agent, which is the default, leverages &lt;a href=&quot;https://github.com/pytorch/tensorpipe&quot;&gt;the TensorPipe library&lt;/a&gt;, which provides a natively point-to-point communication primitive specifically suited for machine learning that fundamentally addresses some of the limitations of Gloo. Compared to Gloo, it has the advantage of being asynchronous, which allows a large number of transfers to occur simultaneously, each at their own speed, without blocking each other. It will only open pipes between pairs of nodes when needed, on demand, and when one node fails only its incident pipes will be closed, while all other ones will keep working as normal. In addition, it is able to support multiple different transports (TCP, of course, but also shared memory, NVLink, InfiniBand, &amp;hellip;) and can automatically detect their availability and negotiate the best transport to use for each pipe.</source>
          <target state="translated">デフォルトであるTensorPipeエージェントは&lt;a href=&quot;https://github.com/pytorch/tensorpipe&quot;&gt;、TensorPipeライブラリを&lt;/a&gt;活用します、これは、Glooのいくつかの制限に根本的に対処する、機械学習に特に適したネイティブのポイントツーポイント通信プリミティブを提供します。 Glooと比較すると、非同期であるという利点があります。これにより、相互にブロックすることなく、それぞれが独自の速度で多数の転送を同時に実行できます。必要に応じて、必要に応じてノードのペア間のパイプを開くだけで、1つのノードに障害が発生すると、インシデントパイプのみが閉じられ、他のすべてのパイプは通常どおり機能し続けます。さらに、複数の異なるトランスポート（TCPはもちろん、共有メモリ、NVLink、InfiniBandなど）もサポートでき、それらの可用性を自動的に検出して、各パイプに使用する最適なトランスポートをネゴシエートできます。</target>
        </trans-unit>
        <trans-unit id="d362a2db0f7abde4a8479b2b7feada60ead120fd" translate="yes" xml:space="preserve">
          <source>The TensorPipe backend has been introduced in PyTorch v1.6 and is being actively developed. At the moment, it only supports CPU tensors, with GPU support coming soon. It comes with a TCP-based transport, just like Gloo. It is also able to automatically chunk and multiplex large tensors over multiple sockets and threads in order to achieve very high bandwidths. The agent will be able to pick the best transport on its own, with no intervention required.</source>
          <target state="translated">TensorPipeバックエンドはPyTorch v1.6で導入され、活発に開発が進められています。現時点ではCPUテンソルのみをサポートしていますが、GPUのサポートは近日中に予定されています。Glooと同様にTCPベースのトランスポートを搭載しています。また、非常に高い帯域幅を実現するために、大規模なテンソルを複数のソケットやスレッドで自動的にチャンクして多重化することができます。エージェントは、介入を必要とせず、最適なトランスポートを独自に選択することができるようになります。</target>
        </trans-unit>
        <trans-unit id="fe34bfd2269a025104eb8b4a08ae444d0c47f764" translate="yes" xml:space="preserve">
          <source>The TorchScript compiler needs to know the types of &lt;code&gt;module attributes&lt;/code&gt;. Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">TorchScriptコンパイラは、 &lt;code&gt;module attributes&lt;/code&gt; タイプを知る必要があります。ほとんどのタイプは、メンバーの値から推測できます。空のリストとdictは、タイプを推測することはできず、PEP526&lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;スタイルの&lt;/a&gt;クラスアノテーションでタイプにアノテーションを付ける必要があります。タイプを推測できず、明示的に注釈が付けられていない場合、結果の&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; に&lt;/a&gt;属性として追加されません。</target>
        </trans-unit>
        <trans-unit id="d526d524819b552e08647a3450928e08c0be85f0" translate="yes" xml:space="preserve">
          <source>The accuracies of the pre-trained models evaluated on COCO val2017 are as follows</source>
          <target state="translated">COCO val2017で評価した事前学習モデルの精度は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="fd6ae7d26b925d9ddede1463aabdca8219c04ca1" translate="yes" xml:space="preserve">
          <source>The algorithm used for interpolation is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="translated">補間に使用されるアルゴリズムは、 &lt;code&gt;mode&lt;/code&gt; によって決定されます。</target>
        </trans-unit>
        <trans-unit id="50402614be52e24dbcb07aaaf1f84990228308aa" translate="yes" xml:space="preserve">
          <source>The algorithm used for upsampling is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="translated">アップサンプリングに使用されるアルゴリズムは、 &lt;code&gt;mode&lt;/code&gt; によって決定されます。</target>
        </trans-unit>
        <trans-unit id="9dbbacc61ab643f749eabb1318de7d252379ec8c" translate="yes" xml:space="preserve">
          <source>The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.</source>
          <target state="translated">アップサンプリングに利用可能なアルゴリズムは、3D、4D、5Dの入力テンソルに対して、それぞれ最近傍探索、線形、バイリニア、バイリニア、バイビック、トリリニアである。</target>
        </trans-unit>
        <trans-unit id="bdd6673496c6fd17d43bfb63694d67b1a23e7074" translate="yes" xml:space="preserve">
          <source>The approximate decimal resolution of this type, i.e., &lt;code&gt;10**-precision&lt;/code&gt;.</source>
          <target state="translated">このタイプのおおよその10進分解能、つまり &lt;code&gt;10**-precision&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="11e64a47029713e51445813986a3824ac2548079" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;引数は、どの対角を考慮するかを制御します。&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;= 0の場合、主対角線以上のすべての要素が保持されます。正の値は主対角線より上の対角線と同じ数を除外し、同様に負の値は主対角線より下の対角線と同じ数を含みます。主対角線はインデックスのセットです</target>
        </trans-unit>
        <trans-unit id="2720b921eb619ce89ed4f91beabc1047581eca82" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;引数は、どの対角を考慮するかを制御します。&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;= 0の場合、主対角線の上下のすべての要素が保持されます。正の値には主対角線より上の対角線の数が含まれ、同様に負の値には主対角線より下の対角線の数が含まれます。主対角線はインデックスのセットです</target>
        </trans-unit>
        <trans-unit id="eff44d81484d8a3edd9c1b438e09ac7f19f85991" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider:</source>
          <target state="translated">&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;引数は、どの対角を考慮するかを制御します。</target>
        </trans-unit>
        <trans-unit id="5cfcdff9c158bfd56a616516f4454be040717f37" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">引数 &lt;code&gt;offset&lt;/code&gt; は、考慮する対角線を制御します。 &lt;code&gt;offset&lt;/code&gt; = 0の場合、主対角線以上のすべての要素が保持されます。正の値は主対角線より上の対角線と同じ数を除外し、同様に負の値は主対角線より下の対角線と同じ数を含みます。主対角線はインデックスのセットです</target>
        </trans-unit>
        <trans-unit id="d114863c5d0c0b07094ca0c20e0cd672c5d3f900" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">引数 &lt;code&gt;offset&lt;/code&gt; は、考慮する対角線を制御します。 &lt;code&gt;offset&lt;/code&gt; = 0の場合、主対角線の上下のすべての要素が保持されます。正の値には主対角線より上の対角線の数が含まれ、同様に負の値には主対角線より下の対角線の数が含まれます。主対角線はインデックスのセットです</target>
        </trans-unit>
        <trans-unit id="19b889e5a4344dcdd3129928b40b0f46befb2501" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider:</source>
          <target state="translated">引数 &lt;code&gt;offset&lt;/code&gt; は、考慮する対角線を制御します。</target>
        </trans-unit>
        <trans-unit id="54707c812db6f29dfe9835066cfce0408bd514d9" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;. However, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this instead returns the results multiplied by</source>
          <target state="translated">引数の仕様は&lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;とほぼ同じです。ただし、 &lt;code&gt;normalized&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、これは代わりに結果に乗算された結果を返します。</target>
        </trans-unit>
        <trans-unit id="3bf649a8dfc260fcc9e9737948bc65802a397444" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;. Similar to &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by multiplying it with</source>
          <target state="translated">引数の仕様は&lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;とほぼ同じです。&lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;と同様に、 &lt;code&gt;normalized&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、これは結果に乗算することによって結果を正規化します。</target>
        </trans-unit>
        <trans-unit id="cf4c102338f979040fb6ff061754881df7061681" translate="yes" xml:space="preserve">
          <source>The backend of the given process group as a lower case string.</source>
          <target state="translated">指定されたプロセスグループのバックエンドを小文字の文字列で表します。</target>
        </trans-unit>
        <trans-unit id="e19263dbfb8f739b55183f8eb835a3eac6ee2887" translate="yes" xml:space="preserve">
          <source>The backend options class for &lt;code&gt;ProcessGroupAgent&lt;/code&gt;, which is derived from &lt;code&gt;RpcBackendOptions&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;ProcessGroupAgent&lt;/code&gt; から派生した &lt;code&gt;RpcBackendOptions&lt;/code&gt; のバックエンドオプションクラス。</target>
        </trans-unit>
        <trans-unit id="ced0f1f27d8dec6f68219ca478db12401a16eb9d" translate="yes" xml:space="preserve">
          <source>The backend options for &lt;code&gt;TensorPipeAgent&lt;/code&gt;, derived from &lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;&lt;code&gt;RpcBackendOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;TensorPipeAgent&lt;/code&gt; から派生した&lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt; &lt;code&gt;RpcBackendOptions&lt;/code&gt; &lt;/a&gt;のバックエンドオプション。</target>
        </trans-unit>
        <trans-unit id="502ce5e4564798f5189d3ade0d0a77ccd7a4b686" translate="yes" xml:space="preserve">
          <source>The backward method does not support sparse and complex inputs. It works only when &lt;code&gt;B&lt;/code&gt; is not provided (i.e. &lt;code&gt;B == None&lt;/code&gt;). We are actively working on extensions, and the details of the algorithms are going to be published promptly.</source>
          <target state="translated">後方メソッドは、スパースおよび複雑な入力をサポートしていません。それは場合にのみ機能 &lt;code&gt;B&lt;/code&gt; が（つまり設けられていないされていない &lt;code&gt;B == None&lt;/code&gt; ）。私たちは積極的に拡張に取り組んでおり、アルゴリズムの詳細はすぐに公開される予定です。</target>
        </trans-unit>
        <trans-unit id="977219b95a63d49a9a83d568761b5dea94f71247" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used locally.</source>
          <target state="translated">バッチサイズは、ローカルで使用するGPUの数よりも大きくする必要があります。</target>
        </trans-unit>
        <trans-unit id="23b97210326ef320e2d1633f1e69b26cc12a15dc" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used.</source>
          <target state="translated">バッチサイズは、使用するGPUの数よりも大きくする必要があります。</target>
        </trans-unit>
        <trans-unit id="1822f0d582e28f7680fb7e40c87465297eaceb93" translate="yes" xml:space="preserve">
          <source>The behavior depends on the dimensionality of the tensors as follows:</source>
          <target state="translated">挙動は以下のようにテンソルの次元性に依存します。</target>
        </trans-unit>
        <trans-unit id="2f9d3a99640c5733b646d772d2660cd0aaa3e59f" translate="yes" xml:space="preserve">
          <source>The behavior of the model changes depending if it is in training or evaluation mode.</source>
          <target state="translated">トレーニングモードなのか評価モードなのかで、モデルの挙動が変化します。</target>
        </trans-unit>
        <trans-unit id="671bda2b5e08e92d9c6a9bee65efdef513a603fd" translate="yes" xml:space="preserve">
          <source>The boolean argument &lt;code&gt;eigenvectors&lt;/code&gt; defines computation of both eigenvectors and eigenvalues or eigenvalues only.</source>
          <target state="translated">ブール引数 &lt;code&gt;eigenvectors&lt;/code&gt; は、固有ベクトルと固有値または固有値のみの両方の計算を定義します。</target>
        </trans-unit>
        <trans-unit id="a803e781482443f16488bb7ddd92d4c8599d93d8" translate="yes" xml:space="preserve">
          <source>The boolean option &lt;code&gt;sorted&lt;/code&gt; if &lt;code&gt;True&lt;/code&gt;, will make sure that the returned &lt;code&gt;k&lt;/code&gt; elements are themselves sorted</source>
          <target state="translated">ブールオプション &lt;code&gt;sorted&lt;/code&gt; あれば &lt;code&gt;True&lt;/code&gt; が、返されたことを確認します &lt;code&gt;k&lt;/code&gt; 個の要素自体がソートされています</target>
        </trans-unit>
        <trans-unit id="7230c4dcc10bf6edcaa500d739b7ece219bab321" translate="yes" xml:space="preserve">
          <source>The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.</source>
          <target state="translated">キャッシングアロケータは、テンソルが割り当てられたストリームのみを認識しています。この認識により、すでに1つのストリームのみでテンソルのライフサイクルを正しく管理しています。しかし、テンソルが割り当て元のストリームとは異なるストリームで使用された場合、アロケータは予期せずメモリを再利用してしまう可能性があります。このメソッドを呼び出すことで、アロケータはどのストリームでテンソルが使用されたかを知ることができます。</target>
        </trans-unit>
        <trans-unit id="750a4192b0cf8d28e81899b152edaaa9a146f0c9" translate="yes" xml:space="preserve">
          <source>The case when</source>
          <target state="translated">となった場合のケース</target>
        </trans-unit>
        <trans-unit id="1a15a1c2561662d55e6adec5b59058d1e70d3911" translate="yes" xml:space="preserve">
          <source>The columns of the output matrix are elementwise powers of the input vector</source>
          <target state="translated">出力行列の列は,入力ベクトルの要素毎の累乗である</target>
        </trans-unit>
        <trans-unit id="ce6592c2eac0498a5cc7f983f7cee34a07c28d68" translate="yes" xml:space="preserve">
          <source>The constructor of &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; can be called without argument, in which case the class is created for the pytorch default dtype (as returned by &lt;a href=&quot;generated/torch.get_default_dtype#torch.get_default_dtype&quot;&gt;&lt;code&gt;torch.get_default_dtype()&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.finfo&quot;&gt; &lt;code&gt;torch.finfo&lt;/code&gt; &lt;/a&gt;のコンストラクターは、引数なしで呼び出すことができます。その場合、クラスはpytorchのデフォルトのdtype（&lt;a href=&quot;generated/torch.get_default_dtype#torch.get_default_dtype&quot;&gt; &lt;code&gt;torch.get_default_dtype()&lt;/code&gt; &lt;/a&gt;によって返される）用に作成されます。</target>
        </trans-unit>
        <trans-unit id="c4ae76d927f18889b6dd2cecc1dcf754fe0ab549" translate="yes" xml:space="preserve">
          <source>The contents of a tensor can be accessed and modified using Python&amp;rsquo;s indexing and slicing notation:</source>
          <target state="translated">テンソルの内容は、Pythonのインデックス作成とスライス表記を使用してアクセスおよび変更できます。</target>
        </trans-unit>
        <trans-unit id="68f6bd6745cd407cbd762333af1947bd80a473cd" translate="yes" xml:space="preserve">
          <source>The context managers &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.enable_grad#torch.enable_grad&quot;&gt;&lt;code&gt;torch.enable_grad()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt;&lt;code&gt;torch.set_grad_enabled()&lt;/code&gt;&lt;/a&gt; are helpful for locally disabling and enabling gradient computation. See &lt;a href=&quot;autograd#locally-disable-grad&quot;&gt;Locally disabling gradient computation&lt;/a&gt; for more details on their usage. These context managers are thread local, so they won&amp;rsquo;t work if you send work to another thread using the &lt;code&gt;threading&lt;/code&gt; module, etc.</source>
          <target state="translated">コンテキストマネージャー&lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt; &lt;code&gt;torch.no_grad()&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/torch.enable_grad#torch.enable_grad&quot;&gt; &lt;code&gt;torch.enable_grad()&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;generated/torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt; &lt;code&gt;torch.set_grad_enabled()&lt;/code&gt; &lt;/a&gt;は、勾配計算をローカルで無効化および有効化するのに役立ちます。使用法の詳細については、&lt;a href=&quot;autograd#locally-disable-grad&quot;&gt;勾配計算&lt;/a&gt;をローカルで無効にするを参照してください。これらのコンテキストマネージャーはスレッドローカルであるため、 &lt;code&gt;threading&lt;/code&gt; モジュールなどを使用して別のスレッドに作業を送信すると機能しません。</target>
        </trans-unit>
        <trans-unit id="6ca40d5babc478c27564467be749f0f09c316678" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;n&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length &lt;code&gt;n&lt;/code&gt;.</source>
          <target state="translated">エルミート入力の正しい解釈は、 &lt;code&gt;n&lt;/code&gt; で与えられる元のデータの長さに依存します。これは、各入力形状が奇数または偶数の長さの信号に対応する可能性があるためです。デフォルトでは、信号は偶数の長さであると想定され、奇数の信号は適切にラウンドトリップしません。したがって、常に信号長 &lt;code&gt;n&lt;/code&gt; を渡すことをお勧めします。</target>
        </trans-unit>
        <trans-unit id="15f3e882aab04b4ad9da40b79636691c5dd75e80" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;s&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape &lt;code&gt;s&lt;/code&gt;.</source>
          <target state="translated">エルミート入力の正しい解釈は、 &lt;code&gt;s&lt;/code&gt; で与えられるように、元のデータの長さに依存します。これは、各入力形状が奇数または偶数の長さの信号に対応する可能性があるためです。デフォルトでは、信号は偶数の長さであると想定され、奇数の信号は適切にラウンドトリップしません。したがって、常に信号形状 &lt;code&gt;s&lt;/code&gt; を渡すことをお勧めします。</target>
        </trans-unit>
        <trans-unit id="b2f2b30024847cce229c4bda1e1b5ab964a9eaf4" translate="yes" xml:space="preserve">
          <source>The criterion only considers a contiguous block of non-negative targets that starts at the front.</source>
          <target state="translated">この基準では、前面から始まる非負のターゲットの連続ブロックのみを考慮します。</target>
        </trans-unit>
        <trans-unit id="e11b9195cb3521aafd237a69ce124b8d788f6886" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="translated">現在の実装では、多くの操作を実行する複雑な&lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;に対して提示された動作はありません。一部の障害の場合、 &lt;code&gt;grad_input&lt;/code&gt; と &lt;code&gt;grad_output&lt;/code&gt; には、入力と出力のサブセットのグラデーションのみが含まれます。このような&lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;場合、特定の入力または出力で直接&lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt; &lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt; &lt;/a&gt;を使用して、必要なグラデーションを取得する必要があります。</target>
        </trans-unit>
        <trans-unit id="e2bf74abb2f8760663c776314dd933141deb8a40" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="translated">現在の実装では、多くの操作を実行する複雑な&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;に対して提示された動作はありません。一部の障害の場合、 &lt;code&gt;grad_input&lt;/code&gt; と &lt;code&gt;grad_output&lt;/code&gt; には、入力と出力のサブセットのグラデーションのみが含まれます。このような&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;場合、特定の入力または出力で直接&lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt; &lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt; &lt;/a&gt;を使用して、必要なグラデーションを取得する必要があります。</target>
        </trans-unit>
        <trans-unit id="41f3fc6117e6365cbfdd81a5124ee1886a82a466" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;code&gt;Module&lt;/code&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;code&gt;Module&lt;/code&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="translated">現在の実装では、多くの操作を実行する複雑な &lt;code&gt;Module&lt;/code&gt; に対して提示された動作はありません。一部の障害の場合、 &lt;code&gt;grad_input&lt;/code&gt; と &lt;code&gt;grad_output&lt;/code&gt; には、入力と出力のサブセットのグラデーションのみが含まれます。このような &lt;code&gt;Module&lt;/code&gt; 場合、特定の入力または出力で直接&lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt; &lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt; &lt;/a&gt;を使用して、必要なグラデーションを取得する必要があります。</target>
        </trans-unit>
        <trans-unit id="0fcbf7f37e10aae7ed7b4feee2a1e7bd2fefc4c8" translate="yes" xml:space="preserve">
          <source>The default floating point dtype is initially &lt;code&gt;torch.float32&lt;/code&gt;.</source>
          <target state="translated">デフォルトの浮動小数点dtypeは、最初は &lt;code&gt;torch.float32&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="fed344676665a74468953d310a0df7afafaa24e3" translate="yes" xml:space="preserve">
          <source>The default floating point tensor type is initially &lt;code&gt;torch.FloatTensor&lt;/code&gt;.</source>
          <target state="translated">デフォルトの浮動小数点テンソルタイプは、最初は &lt;code&gt;torch.FloatTensor&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="a80bd362bedb3fcb6747d9fd75b4961eb0cdb1f6" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="translated">離散フーリエ変換は分離可能であるため、ここでの&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt;は、2つの1次元&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;呼び出しと同等です。</target>
        </trans-unit>
        <trans-unit id="68e62e57e835a4d91bc43ca48c95e3efa63535c9" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="translated">離散フーリエ変換は分離可能であるため、ここでの&lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt;は、2つの1次元&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;呼び出しと同等です。</target>
        </trans-unit>
        <trans-unit id="1984f4c44a0a732d169d5d6a5c5f08cad523bc50" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to a combination of &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">離散フーリエ変換分離可能であるので、&lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;ここでは、の組み合わせと等価である&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="6def4e7a63f51a47c9f6b51fb1bb8c54875ce717" translate="yes" xml:space="preserve">
          <source>The distance swap is described in detail in the paper &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;Learning shallow convolutional feature descriptors with triplet losses&lt;/a&gt; by V. Balntas, E. Riba et al.</source>
          <target state="translated">距離スワップは、紙に詳細に記載されている&lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;三重項損失で学習浅い畳み込み特徴記述子&lt;/a&gt;V. Balntas、E. Ribaのらによる。</target>
        </trans-unit>
        <trans-unit id="e0ab87bc03e5a5294329af8c45b84a5da57986c5" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework makes it easy to run functions remotely, supports referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backward and update parameters across RPC boundaries. These features can be categorized into four sets of APIs.</source>
          <target state="translated">分散 RPC フレームワークは、リモートでの関数実行を容易にし、実際のデータをコピーすることなくリモートオブジェクトの参照をサポートし、RPC の境界を越えて透過的に後方に実行してパラメータを更新するための autograd およびオプティマイザ API を提供します。これらの機能は、4つのAPIセットに分類することができます。</target>
        </trans-unit>
        <trans-unit id="58cfec2b510135f93e0701fc204e248929b783f2" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework provides mechanisms for multi-machine model training through a set of primitives to allow for remote communication, and a higher-level API to automatically differentiate models split across several machines.</source>
          <target state="translated">分散型RPCフレームワークは、リモート通信を可能にする一連のプリミティブと、複数のマシンに分割されたモデルを自動的に区別するための高レベルAPIを介して、マルチマシンモデルのトレーニングのためのメカニズムを提供します。</target>
        </trans-unit>
        <trans-unit id="10060f95e0764042598d146fc06132ab3dfae090" translate="yes" xml:space="preserve">
          <source>The distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.</source>
          <target state="translated">分散オートグラッドのデザインノートでは、モデル並列訓練などのアプリケーションに有用なRPCベースの分散オートグラッドフレームワークのデザインを取り上げています。</target>
        </trans-unit>
        <trans-unit id="dafbad2c73a3de536f4d90e5d5fa2fe3c91b9950" translate="yes" xml:space="preserve">
          <source>The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed pacakge in &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; (by explicitly creating the store as an alternative to specifying &lt;code&gt;init_method&lt;/code&gt;.) There are 3 choices for Key-Value Stores: &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">分散パッケージには分散Key-Valueストアが付属しており、グループ内のプロセス間で情報を共有したり、&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; で&lt;/a&gt;分散パッケージを初期化したりできます（代わりにストアを明示的に作成することにより） &lt;code&gt;init_method&lt;/code&gt; を指定します。）Key-Valueストアには、&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; の&lt;/a&gt;3つの選択肢があります。</target>
        </trans-unit>
        <trans-unit id="6db33b80979f7c8f4114d1e3c176aabac7ef279b" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">被除数と除数には、整数と浮動小数点数の両方を含めることができます。余りは、配当 &lt;code&gt;input&lt;/code&gt; と同じ符号を持ちます。</target>
        </trans-unit>
        <trans-unit id="77ad1382d4823bfe1261c0964c88050a7ad3d117" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the divisor &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">被除数と除数には、整数と浮動小数点数の両方を含めることができます。余りは除数 &lt;code&gt;other&lt;/code&gt; と同じ符号を持ちます。</target>
        </trans-unit>
        <trans-unit id="3c8c32198d8c41dc6ec53302edb97fbbfa15ce0d" translate="yes" xml:space="preserve">
          <source>The division by</source>
          <target state="translated">による分割</target>
        </trans-unit>
        <trans-unit id="d4d19d774aea3434987d4b4cece2b5f99cec9991" translate="yes" xml:space="preserve">
          <source>The dlpack shares the tensors memory. Note that each dlpack can only be consumed once.</source>
          <target state="translated">dlpackはテンソルメモリを共有します。各dlpackは一度しか消費できないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="a37e21cd1a669630ae636a9ca39104d114477d0e" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic cosine is &lt;code&gt;[1, inf)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for &lt;code&gt;+ INF&lt;/code&gt; for which the output is mapped to &lt;code&gt;+ INF&lt;/code&gt;.</source>
          <target state="translated">逆双曲線余弦のドメインである &lt;code&gt;[1, inf)&lt;/code&gt; 及びこの範囲外の値にマッピングする &lt;code&gt;NaN&lt;/code&gt; を除いて、 &lt;code&gt;+ INF&lt;/code&gt; 出力にマッピングされている &lt;code&gt;+ INF&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b9eae4d9c836ec727b1fd15ef73922995c7e82e1" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic tangent is &lt;code&gt;(-1, 1)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for the values &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;-1&lt;/code&gt; for which the output is mapped to &lt;code&gt;+/-INF&lt;/code&gt; respectively.</source>
          <target state="translated">逆双曲線正接の定義域は &lt;code&gt;(-1, 1)&lt;/code&gt; あり、この範囲外の値は &lt;code&gt;NaN&lt;/code&gt; にマップされます。ただし、出力がそれぞれ &lt;code&gt;+/-INF&lt;/code&gt; にマップされる値 &lt;code&gt;1&lt;/code&gt; と &lt;code&gt;-1&lt;/code&gt; は除きます。</target>
        </trans-unit>
        <trans-unit id="e24f7a4d115e62a42f56d551f1451d37a5e25266" translate="yes" xml:space="preserve">
          <source>The dynamic control flow is captured correctly. We can verify in backends with different loop range.</source>
          <target state="translated">動的制御の流れを正しく捉えています。ループ範囲の異なるバックエンドでの検証が可能です。</target>
        </trans-unit>
        <trans-unit id="a418864112d90d162f2d2d68264a78b5661b0c49" translate="yes" xml:space="preserve">
          <source>The eigenvalues are returned in ascending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order.</source>
          <target state="translated">固有値は昇順で返されます。場合 &lt;code&gt;input&lt;/code&gt; 行列のバッチは、次いで、バッチ内の各行列の固有値を昇順に戻されます。</target>
        </trans-unit>
        <trans-unit id="70c72529d7b55dc5ab2b127843a9a42c7179e611" translate="yes" xml:space="preserve">
          <source>The elements are sorted into equal width bins between &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;. If &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt; are both zero, the minimum and maximum values of the data are used.</source>
          <target state="translated">要素は、&lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt;間の等しい幅のビンにソートされます。場合&lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt;及び&lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; は&lt;/a&gt;両方ともゼロであり、データの最小値と最大値が使用されます。</target>
        </trans-unit>
        <trans-unit id="ef67a3a1e1ceed752fed1303fe693fb7bb4731f9" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;Backend.UNDEFINED&lt;/code&gt; is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.</source>
          <target state="translated">エントリ &lt;code&gt;Backend.UNDEFINED&lt;/code&gt; は存在しますが、一部のフィールドの初期値としてのみ使用されます。ユーザーはそれを直接使用したり、その存在を想定したりしないでください。</target>
        </trans-unit>
        <trans-unit id="c0ed2b956d8f2913276ea868cc2cc1c195e28811" translate="yes" xml:space="preserve">
          <source>The example script above produces the graph:</source>
          <target state="translated">上のスクリプト例では、グラフを生成しています。</target>
        </trans-unit>
        <trans-unit id="8850a69d0e77cfdf6128ac9dcc486c371b38e541" translate="yes" xml:space="preserve">
          <source>The export fails because PyTorch does not support exporting &lt;code&gt;elu&lt;/code&gt; operator. We find &lt;code&gt;virtual Tensor elu(const Tensor &amp;amp; input, Scalar alpha, bool inplace) const override;&lt;/code&gt; in &lt;code&gt;VariableType.h&lt;/code&gt;. This means &lt;code&gt;elu&lt;/code&gt; is an ATen operator. We check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;, and confirm that &lt;code&gt;Elu&lt;/code&gt; is standardized in ONNX. We add the following lines to &lt;code&gt;symbolic_opset9.py&lt;/code&gt;:</source>
          <target state="translated">PyTorchはエクスポートをサポートしていないため、エクスポートは失敗し &lt;code&gt;elu&lt;/code&gt; 演算子を。 &lt;code&gt;virtual Tensor elu(const Tensor &amp;amp; input, Scalar alpha, bool inplace) const override;&lt;/code&gt; を見つけます。で &lt;code&gt;VariableType.h&lt;/code&gt; 。これは、 &lt;code&gt;elu&lt;/code&gt; ATEN演算子です。&lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNXオペレーターリスト&lt;/a&gt;をチェックし、 &lt;code&gt;Elu&lt;/code&gt; がONNXで標準化されていることを確認します。次の行を &lt;code&gt;symbolic_opset9.py&lt;/code&gt; に追加します。</target>
        </trans-unit>
        <trans-unit id="f9a2121d113f583a300c4eeb7ced28dd018af26f" translate="yes" xml:space="preserve">
          <source>The fact that gradients need to be computed for a Tensor do not mean that the &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; attribute will be populated, see &lt;a href=&quot;autograd#torch.Tensor.is_leaf&quot;&gt;&lt;code&gt;is_leaf&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">テンソルの勾配を計算する必要があるという事実は、&lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt;属性が入力されることを意味するわけではありません。詳細については、&lt;a href=&quot;autograd#torch.Tensor.is_leaf&quot;&gt; &lt;code&gt;is_leaf&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="221c8b91c0e460724cf31fb2005c0c9e54071204" translate="yes" xml:space="preserve">
          <source>The first call to add for a given &lt;code&gt;key&lt;/code&gt; creates a counter associated with &lt;code&gt;key&lt;/code&gt; in the store, initialized to &lt;code&gt;amount&lt;/code&gt;. Subsequent calls to add with the same &lt;code&gt;key&lt;/code&gt; increment the counter by the specified &lt;code&gt;amount&lt;/code&gt;. Calling &lt;code&gt;add()&lt;/code&gt; with a key that has already been set in the store by &lt;code&gt;set()&lt;/code&gt; will result in an exception.</source>
          <target state="translated">特定の &lt;code&gt;key&lt;/code&gt; を追加する最初の呼び出しにより、ストア内の &lt;code&gt;key&lt;/code&gt; に関連付けられたカウンターが作成され、 &lt;code&gt;amount&lt;/code&gt; に初期化されます。同じ &lt;code&gt;key&lt;/code&gt; 追加する後続の呼び出しは、指定された &lt;code&gt;amount&lt;/code&gt; カウンターをインクリメントします。 &lt;code&gt;set()&lt;/code&gt; によってストアにすでに設定されているキーを使用して &lt;code&gt;add()&lt;/code&gt; を呼び出すと、例外が発生します。</target>
        </trans-unit>
        <trans-unit id="d04a7a0c088c359fab554cd1a6049882b564ff2e" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph.</source>
          <target state="translated">最初のパラメータは常にエクスポートされたONNXグラフです。</target>
        </trans-unit>
        <trans-unit id="26a3da676366966fcc7fbfc997f2f64e6ee31c9d" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in &lt;code&gt;VariableType.h&lt;/code&gt;, because dispatch is done with keyword arguments.</source>
          <target state="translated">最初のパラメータは常にエクスポートされたONNXグラフです。ディスパッチはキーワード引数を使用して行われるため、パラメータ名は &lt;code&gt;VariableType.h&lt;/code&gt; の名前と完全に一致する必要があります。</target>
        </trans-unit>
        <trans-unit id="5964988cdbc48443556dc471facbf25fa24c6e63" translate="yes" xml:space="preserve">
          <source>The following APIs allow users to remotely execute functions as well as create references (RRefs) to remote data objects. In these APIs, when passing a &lt;code&gt;Tensor&lt;/code&gt; as an argument or a return value, the destination worker will try to create a &lt;code&gt;Tensor&lt;/code&gt; with the same meta (i.e., shape, stride, etc.). We intentionally disallow transmitting CUDA tensors because it might crash if the device lists on source and destination workers do not match. In such cases, applications can always explicitly move the input tensors to CPU on the caller and move it to the desired devices on the callee if necessary.</source>
          <target state="translated">次のAPIを使用すると、ユーザーは関数をリモートで実行したり、リモートデータオブジェクトへの参照（RRef）を作成したりできます。これらのAPIでは、 &lt;code&gt;Tensor&lt;/code&gt; を引数または戻り値として渡すときに、宛先ワーカーは同じメタ（つまり、形状、ストライドなど）を使用して &lt;code&gt;Tensor&lt;/code&gt; を作成しようとします。ソースワーカーと宛先ワーカーのデバイスリストが一致しない場合にクラッシュする可能性があるため、CUDAテンソルの送信を意図的に禁止しています。このような場合、アプリケーションは常に入力テンソルを呼び出し元のCPUに明示的に移動し、必要に応じて呼び出し先の目的のデバイスに移動できます。</target>
        </trans-unit>
        <trans-unit id="0331e44de903db51dab5949d62e369ce763fc069" translate="yes" xml:space="preserve">
          <source>The following Python Expressions are supported.</source>
          <target state="translated">以下のPython式をサポートしています。</target>
        </trans-unit>
        <trans-unit id="f123fb71a763a700ae8133c3cf978d6e15341a63" translate="yes" xml:space="preserve">
          <source>The following factory functions support named tensors:</source>
          <target state="translated">以下のファクトリー関数は、名前付きテンソルをサポートしています。</target>
        </trans-unit>
        <trans-unit id="690193766b4768be386db05b742730b7dbdf34e9" translate="yes" xml:space="preserve">
          <source>The following methods are unique to &lt;a href=&quot;#torch.BoolTensor&quot;&gt;&lt;code&gt;torch.BoolTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">次のメソッドは、&lt;a href=&quot;#torch.BoolTensor&quot;&gt; &lt;code&gt;torch.BoolTensor&lt;/code&gt; に&lt;/a&gt;固有のものです。</target>
        </trans-unit>
        <trans-unit id="2fcc9561d17b3f2c0c3db2c65c4b45e1fd39309f" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will act deterministically when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="translated">次の通常は非決定論的な演算は、 &lt;code&gt;d=True&lt;/code&gt; の場合に決定論的に動作します。</target>
        </trans-unit>
        <trans-unit id="6c856d87fb36615ab5d3e65d2705a558ef7fc7ab" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will throw a &lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt;&lt;code&gt;RuntimeError&lt;/code&gt;&lt;/a&gt; when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="translated">次の通常は非決定的な操作は、 &lt;code&gt;d=True&lt;/code&gt; の場合に&lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt; &lt;code&gt;RuntimeError&lt;/code&gt; &lt;/a&gt;をスローします。</target>
        </trans-unit>
        <trans-unit id="10cc3c798c883edec11b18c9b813ac5a197463fe" translate="yes" xml:space="preserve">
          <source>The following operators are supported:</source>
          <target state="translated">以下の演算子に対応しています。</target>
        </trans-unit>
        <trans-unit id="116feb31ceb0c75370eb04add5159eb16382e56b" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in PyTorch 1.8. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.fftn&quot;&gt;&lt;code&gt;torch.fft.fftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">関数&lt;a href=&quot;#torch.fft&quot;&gt; &lt;code&gt;torch.fft()&lt;/code&gt; &lt;/a&gt;は非推奨になり、PyTorch1.8で削除される予定です。代わりに、&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt;をインポートして&lt;a href=&quot;../fft#torch.fft.fft&quot;&gt; &lt;code&gt;torch.fft.fft()&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;../fft#torch.fft.fftn&quot;&gt; &lt;code&gt;torch.fft.fftn()&lt;/code&gt; &lt;/a&gt;を呼び出すことにより、新しい&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt;モジュール関数を使用します。</target>
        </trans-unit>
        <trans-unit id="18bbed5d4e21fbcafc8624ec507383dc653a9737" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.ifft&quot;&gt;&lt;code&gt;torch.ifft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.ifftn&quot;&gt;&lt;code&gt;torch.fft.ifftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">関数&lt;a href=&quot;#torch.ifft&quot;&gt; &lt;code&gt;torch.ifft()&lt;/code&gt; &lt;/a&gt;は非推奨であり、将来のPyTorchリリースで削除される予定です。代わりに、&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt;をインポートし、&lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt; &lt;code&gt;torch.fft.ifft()&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;../fft#torch.fft.ifftn&quot;&gt; &lt;code&gt;torch.fft.ifftn()&lt;/code&gt; &lt;/a&gt;を呼び出すことにより、新しい&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt;モジュール関数を使用します。</target>
        </trans-unit>
        <trans-unit id="b1a40cf129fc842a7fe39cfe2023111384b05a6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;torch.irfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.irfft&quot;&gt;&lt;code&gt;torch.fft.irfft()&lt;/code&gt;&lt;/a&gt; for one-sided input, or &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; for two-sided input.</source>
          <target state="translated">関数&lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;torch.irfft()&lt;/code&gt; &lt;/a&gt;は非推奨であり、将来のPyTorchリリースで削除される予定です。新しい使用&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fftの&lt;/a&gt;代わりに、インポートすることで、モジュールの機能を&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fftを&lt;/a&gt;して呼び出す&lt;a href=&quot;../fft#torch.fft.irfft&quot;&gt; &lt;code&gt;torch.fft.irfft()&lt;/code&gt; &lt;/a&gt;、片側の入力、またはのための&lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt; &lt;code&gt;torch.fft.ifft()&lt;/code&gt; &lt;/a&gt;両面入力のために。</target>
        </trans-unit>
        <trans-unit id="a0db51d208a07883c44d7be8c9786454460ee522" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.rfft&quot;&gt;&lt;code&gt;torch.rfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.rfft&quot;&gt;&lt;code&gt;torch.fft.rfft()&lt;/code&gt;&lt;/a&gt; for one-sided output, or &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; for two-sided output.</source>
          <target state="translated">関数&lt;a href=&quot;#torch.rfft&quot;&gt; &lt;code&gt;torch.rfft()&lt;/code&gt; &lt;/a&gt;は非推奨であり、将来のPyTorchリリースで削除される予定です。新しい使用&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fftの&lt;/a&gt;代わりに、インポートすることで、モジュールの機能を&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fftを&lt;/a&gt;して呼び出す&lt;a href=&quot;../fft#torch.fft.rfft&quot;&gt; &lt;code&gt;torch.fft.rfft()&lt;/code&gt; &lt;/a&gt;、片側の出力、またはのための&lt;a href=&quot;../fft#torch.fft.fft&quot;&gt; &lt;code&gt;torch.fft.fft()&lt;/code&gt; &lt;/a&gt;両面出力のために。</target>
        </trans-unit>
        <trans-unit id="78c07c2cfdea987b1b930691f54fd99adb7a7932" translate="yes" xml:space="preserve">
          <source>The function is defined as:</source>
          <target state="translated">関数として定義されています。</target>
        </trans-unit>
        <trans-unit id="81e6543a378b1e27df259854cebe693b6ae023b6" translate="yes" xml:space="preserve">
          <source>The gated linear unit. Computes:</source>
          <target state="translated">ゲーテッドリニアユニット。計算します。</target>
        </trans-unit>
        <trans-unit id="c30476379351f558b5c5fb5c388947e81c5eddf9" translate="yes" xml:space="preserve">
          <source>The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying &lt;code&gt;gradient&lt;/code&gt;. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">グラフは連鎖律を使用して区別されます。テンソルが非スカラーであり（つまり、そのデータに複数の要素がある）、勾配が必要な場合、関数はさらに &lt;code&gt;gradient&lt;/code&gt; 指定する必要があります。これは、タイプと場所が一致するテンソルである必要があります。このテンソルには、 &lt;code&gt;self&lt;/code&gt; との微分関数の勾配が含まれています。</target>
        </trans-unit>
        <trans-unit id="63a016154b7848c498c0189a691b12aee6391726" translate="yes" xml:space="preserve">
          <source>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">フックは引数を変更するべきではありませんが、オプションで&lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; の&lt;/a&gt;代わりに使用される新しいグラデーションを返すことができます。</target>
        </trans-unit>
        <trans-unit id="d3b8c649dc7d5a1913425cba9308cbaac351ab8a" translate="yes" xml:space="preserve">
          <source>The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:</source>
          <target state="translated">フックは、Tensorに対する勾配が計算されるたびに呼び出されます。フックは次のようなシグネチャを持つ必要があります。</target>
        </trans-unit>
        <trans-unit id="91b8909b6b0301d53bc827787fcd07621d744139" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; has computed an output. It should have the following signature:</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;が出力を計算した後、フックは毎回呼び出されます。次の署名が必要です。</target>
        </trans-unit>
        <trans-unit id="ea6266d19428a156d329cc252aacfffb34135bcf" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;code&gt;forward()&lt;/code&gt; has computed an output. It should have the following signature:</source>
          <target state="translated">&lt;code&gt;forward()&lt;/code&gt; が出力を計算した後、フックは毎回呼び出されます。次の署名が必要です。</target>
        </trans-unit>
        <trans-unit id="3027e825f33c77d15aef5c4a000dacdcd543639b" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is invoked. It should have the following signature:</source>
          <target state="translated">フックは、&lt;a href=&quot;#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;が呼び出される前に毎回呼び出されます。次の署名が必要です。</target>
        </trans-unit>
        <trans-unit id="658103272c0bdd847853bfee8c1acd84bd1873a4" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;code&gt;forward()&lt;/code&gt; is invoked. It should have the following signature:</source>
          <target state="translated">フックは、 &lt;code&gt;forward()&lt;/code&gt; が呼び出される前に毎回呼び出されます。次の署名が必要です。</target>
        </trans-unit>
        <trans-unit id="b7317d93b9ecea6baa18606aa1761b42a13618f5" translate="yes" xml:space="preserve">
          <source>The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:</source>
          <target state="translated">このフックは、モジュールの入力に対するグラデーションが計算されるたびに呼び出されます。フックは次のようなシグネチャを持つべきです。</target>
        </trans-unit>
        <trans-unit id="47b9323acd4c956c6840fecee45ba64f27f04f5f" translate="yes" xml:space="preserve">
          <source>The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute &amp;ndash; that is, contain a small number of assigned labels.</source>
          <target state="translated">頻繁にアクセスされるクラスター（最初のクラスターのように、最も頻繁なラベルを含む）も、計算コストが低くなる必要があります。つまり、割り当てられたラベルの数が少なくなります。</target>
        </trans-unit>
        <trans-unit id="fd1e3da6ac802ac499dbc929d3c01e7d8d1e592e" translate="yes" xml:space="preserve">
          <source>The implementation is based on the Algorithm 5.1 from Halko et al, 2009.</source>
          <target state="translated">実装は、Halko et al,2009のアルゴリズム5.1に基づいています。</target>
        </trans-unit>
        <trans-unit id="b39161781b1b7b35ca70ad2826501e304e14740d" translate="yes" xml:space="preserve">
          <source>The implementation is based on: Bader, P.; Blanes, S.; Casas, F. Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation. Mathematics 2019, 7, 1174.</source>
          <target state="translated">実装は以下に基づいています。Bader,P.;Blanes,S.;Casas,F.最適化されたテイラー多項式近似を用いた行列指数の計算.数学 2019,7,1174.</target>
        </trans-unit>
        <trans-unit id="d9d89b5b4a75085b7eb81dcbff4669f9ff584606" translate="yes" xml:space="preserve">
          <source>The implementation of SVD on CPU uses the LAPACK routine &lt;code&gt;?gesdd&lt;/code&gt; (a divide-and-conquer algorithm) instead of &lt;code&gt;?gesvd&lt;/code&gt; for speed. Analogously, the SVD on GPU uses the MAGMA routine &lt;code&gt;gesdd&lt;/code&gt; as well.</source>
          <target state="translated">CPUでのSVDの実装では、速度を &lt;code&gt;?gesvd&lt;/code&gt; ために、？gesvdの代わりにLAPACKルーチンの &lt;code&gt;?gesdd&lt;/code&gt; （分割統治アルゴリズム）を使用します。同様に、GPU上のSVDもMAGMAルーチン &lt;code&gt;gesdd&lt;/code&gt; を使用します。</target>
        </trans-unit>
        <trans-unit id="c05db592ff41dfc6119d9765d1f98df7bbeac593" translate="yes" xml:space="preserve">
          <source>The implementations of the models for object detection, instance segmentation and keypoint detection are efficient.</source>
          <target state="translated">オブジェクト検出、インスタンスセグメンテーション、キーポイント検出のためのモデルの実装は効率的である。</target>
        </trans-unit>
        <trans-unit id="2ee40d087e274de07c646cdfb7f973b9dab8f9cf" translate="yes" xml:space="preserve">
          <source>The inferred dtype for python floats in &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Pythonの推定dtypeは&lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; にあり&lt;/a&gt;ます。</target>
        </trans-unit>
        <trans-unit id="6c9a1b1a8c22e228f94e607d9ba23a7a9895221c" translate="yes" xml:space="preserve">
          <source>The input &lt;code&gt;window_length&lt;/code&gt; is a positive integer controlling the returned window size. &lt;code&gt;periodic&lt;/code&gt; flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;. Therefore, if &lt;code&gt;periodic&lt;/code&gt; is true, the</source>
          <target state="translated">入力 &lt;code&gt;window_length&lt;/code&gt; は、返されるウィンドウサイズを制御する正の整数です。 &lt;code&gt;periodic&lt;/code&gt; フラグは、返されたウィンドウが対称ウィンドウから最後の重複値を削除し、&lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;torch.stft()&lt;/code&gt; &lt;/a&gt;などの関数で周期的ウィンドウとして使用する準備ができているかどうかを判別します。したがって、 &lt;code&gt;periodic&lt;/code&gt; が真の場合、</target>
        </trans-unit>
        <trans-unit id="dffb8d48e1eff38cec4958715ef4e4bea17673f9" translate="yes" xml:space="preserve">
          <source>The input channels are separated into &lt;code&gt;num_groups&lt;/code&gt; groups, each containing &lt;code&gt;num_channels / num_groups&lt;/code&gt; channels. The mean and standard-deviation are calculated separately over the each group.</source>
          <target state="translated">入力チャネルは &lt;code&gt;num_groups&lt;/code&gt; グループに分けられ、各グループには &lt;code&gt;num_channels / num_groups&lt;/code&gt; チャネルが含まれます。平均と標準偏差は、グループごとに別々に計算されます。</target>
        </trans-unit>
        <trans-unit id="24d9bce9a9af270abcd63358a34e29c31cf80c6f" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).</source>
          <target state="translated">入力には、モジュールに指定された位置引数のみが含まれます。キーワード引数はフックに渡されず、 &lt;code&gt;forward&lt;/code&gt; のみ渡されます。フックは入力を変更できます。ユーザーは、フックでタプルまたは単一の変更された値を返すことができます。単一の値が返された場合（その値がすでにタプルである場合を除く）、値をタプルにラップします。</target>
        </trans-unit>
        <trans-unit id="501f21d86b3222c931438d1d37f49ef014fe1070" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">入力には、モジュールに指定された位置引数のみが含まれます。キーワード引数はフックに渡されず、 &lt;code&gt;forward&lt;/code&gt; のみ渡されます。フックは出力を変更できます。入力をインプレースで変更できますが、&lt;a href=&quot;#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;が呼び出された後に呼び出されるため、forwardには影響しません。</target>
        </trans-unit>
        <trans-unit id="d84cfbddddc9e1c2d0a053623e51f4c9b23ed343" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;code&gt;forward()&lt;/code&gt; is called.</source>
          <target state="translated">入力には、モジュールに指定された位置引数のみが含まれます。キーワード引数はフックに渡されず、 &lt;code&gt;forward&lt;/code&gt; のみ渡されます。フックは出力を変更できます。入力をインプレースで変更できますが、 &lt;code&gt;forward()&lt;/code&gt; が呼び出された後に呼び出されるため、forwardには影響しません。</target>
        </trans-unit>
        <trans-unit id="e0bbaa069be68586598b46c411d0f2665ffc0e9d" translate="yes" xml:space="preserve">
          <source>The input data is assumed to be of the form &lt;code&gt;minibatch x channels x [optional depth] x [optional height] x width&lt;/code&gt;. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</source>
          <target state="translated">入力データは、 &lt;code&gt;minibatch x channels x [optional depth] x [optional height] x width&lt;/code&gt; の形式であると想定されます。したがって、空間入力の場合は4Dテンソルが必要であり、体積入力の場合は5Dテンソルが必要です。</target>
        </trans-unit>
        <trans-unit id="855506544cad1eeac05a21efea97a3853e2dceb9" translate="yes" xml:space="preserve">
          <source>The input dimensions are interpreted in the form: &lt;code&gt;mini-batch x channels x [optional depth] x [optional height] x width&lt;/code&gt;.</source>
          <target state="translated">入力寸法は、 &lt;code&gt;mini-batch x channels x [optional depth] x [optional height] x width&lt;/code&gt; の形式で解釈されます。</target>
        </trans-unit>
        <trans-unit id="ff7b890633934634eb44c66753eb478acaced44e" translate="yes" xml:space="preserve">
          <source>The input is assumed to be a low-rank matrix.</source>
          <target state="translated">入力は低ランクの行列を想定しています。</target>
        </trans-unit>
        <trans-unit id="aa72bb59a173177382b9660fee81f62bda47bdae" translate="yes" xml:space="preserve">
          <source>The input quantization parameters are propagated to the output.</source>
          <target state="translated">入力された量子化パラメータは出力に伝搬されます。</target>
        </trans-unit>
        <trans-unit id="7c8e7facaf470e54c3381c46dad3dd11548e58de" translate="yes" xml:space="preserve">
          <source>The input quantization parameters propagate to the output.</source>
          <target state="translated">入力された量子化パラメータは出力に伝搬します。</target>
        </trans-unit>
        <trans-unit id="9c93ac513b2302a4f83a43405b9a47bcb2309e67" translate="yes" xml:space="preserve">
          <source>The input to the model is expected to be a list of tensors, each of shape &lt;code&gt;[C, H, W]&lt;/code&gt;, one for each image, and should be in &lt;code&gt;0-1&lt;/code&gt; range. Different images can have different sizes.</source>
          <target state="translated">モデルへの入力は、形状のそれぞれ、テンソルのリストであると予想される &lt;code&gt;[C, H, W]&lt;/code&gt; 、各画像に対して1つ、及びであるべきである &lt;code&gt;0-1&lt;/code&gt; 範囲。画像が異なれば、サイズも異なります。</target>
        </trans-unit>
        <trans-unit id="33129e152019831a4d6451e1dcb0878f56c1bef2" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.&lt;/code&gt; prefix for some operations. See example usage below.</source>
          <target state="translated">このクラスのインスタンスは、 &lt;code&gt;torch.&lt;/code&gt; 代わりに使用できます。一部の操作のプレフィックス。以下の使用例を参照してください。</target>
        </trans-unit>
        <trans-unit id="351e07f8a31af7d82fc6045df4c793836de62f62" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.ops.quantized&lt;/code&gt; prefix. See example usage below.</source>
          <target state="translated">このクラスのインスタンスは、 &lt;code&gt;torch.ops.quantized&lt;/code&gt; プレフィックスの代わりに使用できます。以下の使用例を参照してください。</target>
        </trans-unit>
        <trans-unit id="6c3dcf3c5f75afd5957f352f9917b83b90088988" translate="yes" xml:space="preserve">
          <source>The interface for specifying operator definitions is a Prototype feature; adventurous users should note that the APIs will probably change in a future interface.</source>
          <target state="translated">演算子の定義を指定するインターフェースは Prototype の機能です。</target>
        </trans-unit>
        <trans-unit id="3bb29d8ff742c3f78ac5a2da01f22239940307d9" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数の逆は&lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="3ff786d8f2651e07ce1136605dc8809d77fc8a6a" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数の逆は&lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="bd26976415ff02dd321f8aabd58b78a55df77549" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.irfft#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数の逆は&lt;a href=&quot;torch.irfft#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="51e2eb96b40c3aad396917c7dd98de9a6585beea" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数の逆は&lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="4b9e7887dd437999f146f02a0f60028f96d84bf3" translate="yes" xml:space="preserve">
          <source>The largest difference between TorchScript and the full Python language is that TorchScript only supports a small set of types that are needed to express neural net models. In particular, TorchScript supports:</source>
          <target state="translated">TorchScriptとPython言語の最大の違いは、TorchScriptがニューラルネットモデルを表現するのに必要な少数の型しかサポートしていないことです。特に、TorchScriptがサポートしているのは</target>
        </trans-unit>
        <trans-unit id="3b1d6d882f09df5877abb8ef200d99a4552edbcc" translate="yes" xml:space="preserve">
          <source>The largest representable number.</source>
          <target state="translated">表現可能な最大の数。</target>
        </trans-unit>
        <trans-unit id="63b23adb215af0fe9eaa51ef9153973e0c53546f" translate="yes" xml:space="preserve">
          <source>The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.</source>
          <target state="translated">最後の項は省略することも、スターリング式で近似することもできます。近似は1以上の目標値に使用され、1以下の目標値ではゼロが損失に加算されます。</target>
        </trans-unit>
        <trans-unit id="08f3358598c31282b2114da38bcad5d74e704334" translate="yes" xml:space="preserve">
          <source>The locations are used in the order of</source>
          <target state="translated">の順で使用されています。</target>
        </trans-unit>
        <trans-unit id="9ebd3a640891ee47a9a38fbd56c89724e57e2af2" translate="yes" xml:space="preserve">
          <source>The loss can be described as:</source>
          <target state="translated">損失と表現してもいいでしょう。</target>
        </trans-unit>
        <trans-unit id="f21bd0e4e809cda5ba13223c1dfb3494e56f4f3e" translate="yes" xml:space="preserve">
          <source>The loss function for</source>
          <target state="translated">の損失関数は</target>
        </trans-unit>
        <trans-unit id="86081d5fc32148129e542f3dc9192fb2857da205" translate="yes" xml:space="preserve">
          <source>The loss function for each pair of samples in the mini-batch is:</source>
          <target state="translated">ミニバッチのサンプルの各ペアの損失関数は、次のようになります。</target>
        </trans-unit>
        <trans-unit id="bb7d036d43546cb725a6bf9bf29c5cc2076ac418" translate="yes" xml:space="preserve">
          <source>The loss function for each sample in the mini-batch is:</source>
          <target state="translated">ミニバッチの各サンプルの損失関数は、以下の通りです。</target>
        </trans-unit>
        <trans-unit id="48f8488012ce60a4d13e1aebd7fa0ba10061eaf3" translate="yes" xml:space="preserve">
          <source>The loss function for each sample is:</source>
          <target state="translated">各サンプルの損失関数は</target>
        </trans-unit>
        <trans-unit id="c43bc97db5e0527c42aa84dc0f06c5fb6d69ddbc" translate="yes" xml:space="preserve">
          <source>The loss function then becomes:</source>
          <target state="translated">すると、損失関数は次のようになります。</target>
        </trans-unit>
        <trans-unit id="dbd64b8b1e6f8a768dc1cc623e35d46784438d27" translate="yes" xml:space="preserve">
          <source>The losses are averaged across observations for each minibatch. If the &lt;code&gt;weight&lt;/code&gt; argument is specified then this is a weighted average:</source>
          <target state="translated">損失は​​、各ミニバッチの観測全体で平均化されます。 &lt;code&gt;weight&lt;/code&gt; 引数が指定されている場合、これは加重平均です。</target>
        </trans-unit>
        <trans-unit id="187e00ce183312dd132d6826870b8e0983ae12a8" translate="yes" xml:space="preserve">
          <source>The lower triangular part of the matrix is defined as the elements on and below the diagonal.</source>
          <target state="translated">行列の下三角形部分は、対角線上と対角線下の要素として定義されます。</target>
        </trans-unit>
        <trans-unit id="cba1d04dc7071338e4468867933f6f030f4f19fd" translate="yes" xml:space="preserve">
          <source>The machine with rank 0 will be used to set up all connections.</source>
          <target state="translated">ランク0のマシンで全ての接続を設定します。</target>
        </trans-unit>
        <trans-unit id="f6b0320b620362b6deb5bb70025cb38b632f6a57" translate="yes" xml:space="preserve">
          <source>The main trick for &lt;code&gt;hard&lt;/code&gt; is to do &lt;code&gt;y_hard - y_soft.detach() + y_soft&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;hard&lt;/code&gt; の主なトリックは、 &lt;code&gt;y_hard - y_soft.detach() + y_soft&lt;/code&gt; を実行することです-y_soft.detach（）+ y_soft</target>
        </trans-unit>
        <trans-unit id="61caa3335965d2f3a59a97a54b15a39417eaac38" translate="yes" xml:space="preserve">
          <source>The max-pooling operation is applied in</source>
          <target state="translated">max-pooling 操作は</target>
        </trans-unit>
        <trans-unit id="d65dca627767b86d15d82ece9fa3008acf19101f" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups.</source>
          <target state="translated">平均値と標準偏差は、同一工程群のすべてのミニバッチについて、寸法ごとに計算しています。</target>
        </trans-unit>
        <trans-unit id="c3242427612d3b3c3e0cb9845d6bf7db1596d5b9" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over the mini-batches and</source>
          <target state="translated">平均値と標準偏差は、ミニバッチと</target>
        </trans-unit>
        <trans-unit id="5661df9ad4405c387ea95d88a69a9fe1650b1552" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.</source>
          <target state="translated">平均値と標準偏差は、ミニバッチの対象物ごとに寸法ごとに個別に計算しています。</target>
        </trans-unit>
        <trans-unit id="29fab6215a0696acbd3e84d8cb442b324881c861" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by &lt;code&gt;normalized_shape&lt;/code&gt;.</source>
          <target state="translated">平均と標準偏差は、 &lt;code&gt;normalized_shape&lt;/code&gt; で指定された形状である必要がある最後の特定の数の次元に対して別々に計算されます。</target>
        </trans-unit>
        <trans-unit id="8a930aa85422123ed7ce795a3af91990ae6ee8f7" translate="yes" xml:space="preserve">
          <source>The mean operation still operates over all the elements, and divides by</source>
          <target state="translated">平均演算はまだすべての要素に対して動作しており</target>
        </trans-unit>
        <trans-unit id="619af861d215de1b87a6226cb50c28f6e623bb30" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements in the dimension &lt;code&gt;dim&lt;/code&gt;. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="translated">中央値は、次元 &lt;code&gt;dim&lt;/code&gt; に偶数の要素を持つ &lt;code&gt;input&lt;/code&gt; テンソルに固有ではありません。この場合、2つの中央値の低い方が返されます。 &lt;code&gt;input&lt;/code&gt; の両方の中央値の平均を計算するには、代わりに &lt;code&gt;q=0.5&lt;/code&gt; で&lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; &lt;/a&gt;を使用します。</target>
        </trans-unit>
        <trans-unit id="5d7cf56bfc5cedceb3ca00313d8c3d414937caef" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="translated">中央値は、要素の数が偶数の &lt;code&gt;input&lt;/code&gt; テンソルでは一意ではありません。この場合、2つの中央値の低い方が返されます。 &lt;code&gt;input&lt;/code&gt; の両方の中央値の平均を計算するには、代わりに &lt;code&gt;q=0.5&lt;/code&gt; で&lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; &lt;/a&gt;を使用します。</target>
        </trans-unit>
        <trans-unit id="3568ef228089a375953fda3fd0c97aca0ace05a3" translate="yes" xml:space="preserve">
          <source>The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.</source>
          <target state="translated">モデルは ResNet と同じですが、ボトルネックとなるチャネル数が各ブロックで 2 倍になっていることを除けば、同じです。例えば、ResNet-50 の最後のブロックには 2048-512-2048 のチャンネルがあり、Wide ResNet-50-2 には 2048-1024-2048 のチャンネルがあります。</target>
        </trans-unit>
        <trans-unit id="f15c86c9dfb63d5c112adb09bde9cb61a9fe3f2e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss.</source>
          <target state="translated">モデルは、トレーニング中に &lt;code&gt;Dict[Tensor]&lt;/code&gt; 返します。これには、RPNとR-CNNの両方の分類と回帰の損失、およびキーポイントの損失が含まれます。</target>
        </trans-unit>
        <trans-unit id="065007e637ab5dd9f6a9a41f5c632d9aa1aa3f5e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the mask loss.</source>
          <target state="translated">モデルは、トレーニング中に &lt;code&gt;Dict[Tensor]&lt;/code&gt; 返します。これには、RPNとR-CNNの両方の分類と回帰の損失、およびマスクの損失が含まれます。</target>
        </trans-unit>
        <trans-unit id="a0c3a4815182bd38a59e8d48bd1acda3676df3d3" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN.</source>
          <target state="translated">モデルは、トレーニング中に &lt;code&gt;Dict[Tensor]&lt;/code&gt; 返します。これには、RPNとR-CNNの両方の分類と回帰の損失が含まれます。</target>
        </trans-unit>
        <trans-unit id="25fbe42f5bebb67d721733ebaa470925353914ab" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses.</source>
          <target state="translated">モデルは、トレーニング中に、分類と回帰の損失を含む &lt;code&gt;Dict[Tensor]&lt;/code&gt; 返します。</target>
        </trans-unit>
        <trans-unit id="32c7b059b3336b886e2737e75a7a90bf14890a11" translate="yes" xml:space="preserve">
          <source>The models expect a list of &lt;code&gt;Tensor[C, H, W]&lt;/code&gt;, in the range &lt;code&gt;0-1&lt;/code&gt;. The models internally resize the images so that they have a minimum size of &lt;code&gt;800&lt;/code&gt;. This option can be changed by passing the option &lt;code&gt;min_size&lt;/code&gt; to the constructor of the models.</source>
          <target state="translated">モデルは、のリストを期待 &lt;code&gt;Tensor[C, H, W]&lt;/code&gt; 範囲で、 &lt;code&gt;0-1&lt;/code&gt; 。モデルは、画像のサイズを内部的に変更して、最小サイズが &lt;code&gt;800&lt;/code&gt; になるようにします。このオプションは、オプション &lt;code&gt;min_size&lt;/code&gt; をモデルのコンストラクターに渡すことで変更できます。</target>
        </trans-unit>
        <trans-unit id="b8cd112b9466880b251d727f36c9b5a8099f6121" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for detection:</source>
          <target state="translated">models サブパッケージには,検出のための以下のモデルアーキテクチャの定義が含まれています。</target>
        </trans-unit>
        <trans-unit id="2fd7064c0a6dbdb00e5c05b4bcf7bb996635bfb3" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for image classification:</source>
          <target state="translated">models サブパッケージには,画像分類のための以下のモデルアーキテクチャの定義が含まれています。</target>
        </trans-unit>
        <trans-unit id="9a82a240d4b1d0867d7d72a1a3f555d48973b4c7" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for semantic segmentation:</source>
          <target state="translated">models サブパッケージには、セマンティックセグメンテーションのための以下のモデルアーキテクチャの定義が含まれています。</target>
        </trans-unit>
        <trans-unit id="18bc003911e615e5dba9603961f512f27acb297a" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification.</source>
          <target state="translated">models サブパッケージには、画像分類、ピクセル単位のセマンティックセグメンテーション、オブジェクト検出、インスタンスセグメンテーション、人物キーポイント検出、動画分類など、さまざまなタスクに対応するモデルの定義が含まれています。</target>
        </trans-unit>
        <trans-unit id="3cf46d13a21b259a13e6ce62948cca48ddbc78f6" translate="yes" xml:space="preserve">
          <source>The modes available for resizing are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only), &lt;code&gt;area&lt;/code&gt;</source>
          <target state="translated">サイズ変更に使用できるモードは、 &lt;code&gt;nearest&lt;/code&gt; 、 &lt;code&gt;linear&lt;/code&gt; （3Dのみ）、 &lt;code&gt;bilinear&lt;/code&gt; 、 &lt;code&gt;bicubic&lt;/code&gt; （4Dのみ）、 &lt;code&gt;trilinear&lt;/code&gt; 線形（5Dのみ）、 &lt;code&gt;area&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28d8cfd32d3249e2763e181ffc749144e0ea24a2" translate="yes" xml:space="preserve">
          <source>The modes available for upsampling are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only)</source>
          <target state="translated">アップサンプリングに使用できるモードは、 &lt;code&gt;nearest&lt;/code&gt; 、 &lt;code&gt;linear&lt;/code&gt; （3Dのみ）、 &lt;code&gt;bilinear&lt;/code&gt; 、 &lt;code&gt;bicubic&lt;/code&gt; （4Dのみ）、 &lt;code&gt;trilinear&lt;/code&gt; 線形（5Dのみ）です。</target>
        </trans-unit>
        <trans-unit id="18b2caf95e73a1c791145d0b79ff330ee08dd6df" translate="yes" xml:space="preserve">
          <source>The module can be accessed as an attribute using the given name.</source>
          <target state="translated">モジュールは、与えられた名前を使って属性としてアクセスすることができます。</target>
        </trans-unit>
        <trans-unit id="00f10b867e105cafb015ef514a5b43106e8a7867" translate="yes" xml:space="preserve">
          <source>The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="translated">モジュールの &lt;code&gt;forward&lt;/code&gt; はデフォルトでコンパイルされます。 &lt;code&gt;forward&lt;/code&gt; から呼び出されたメソッドは、forwardで使用される順序で遅延コンパイルされ &lt;code&gt;forward&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f00cd6c1142679bd0fe1c3ebd4ca78600af96a2c" translate="yes" xml:space="preserve">
          <source>The name of the worker.</source>
          <target state="translated">作業員の名前です。</target>
        </trans-unit>
        <trans-unit id="fba68059efcba7726021d64196e7ad3e2e4fcb96" translate="yes" xml:space="preserve">
          <source>The named tensor API is a prototype feature and subject to change.</source>
          <target state="translated">名前付きテンソルAPIはプロトタイプの機能であり、変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="ed535f6a7c3020c739db9c59caec32bca959d97b" translate="yes" xml:space="preserve">
          <source>The named tensor API is experimental and subject to change.</source>
          <target state="translated">名前付きテンソルAPIは実験的なものであり、変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="36555b023721ec5738f0c183a135318d3c9a4466" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss.</source>
          <target state="translated">負の対数尤度損失。</target>
        </trans-unit>
        <trans-unit id="80880349d7cc0a615947c8d7ecadbb60eab216c2" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss. It is useful to train a classification problem with &lt;code&gt;C&lt;/code&gt; classes.</source>
          <target state="translated">負の対数尤度損失。 &lt;code&gt;C&lt;/code&gt; クラスで分類問題をトレーニングすると便利です。</target>
        </trans-unit>
        <trans-unit id="07e33a3c723549700a6c484c42061c0b83f6423c" translate="yes" xml:space="preserve">
          <source>The new backend derives from &lt;code&gt;c10d.ProcessGroup&lt;/code&gt; and registers the backend name and the instantiating interface through &lt;code&gt;torch.distributed.Backend.register_backend()&lt;/code&gt; when imported.</source>
          <target state="translated">新しいバックエンドは &lt;code&gt;c10d.ProcessGroup&lt;/code&gt; から派生し、インポート時に &lt;code&gt;torch.distributed.Backend.register_backend()&lt;/code&gt; を介してバックエンド名とインスタンス化インターフェイスを登録します。</target>
        </trans-unit>
        <trans-unit id="5ef80d57536dd4f889b111944ebd026c0916a56b" translate="yes" xml:space="preserve">
          <source>The new usage looks like this:</source>
          <target state="translated">新しい使い方はこんな感じです。</target>
        </trans-unit>
        <trans-unit id="bfa45b6345caf33366ff2d44c58d6e6325263da6" translate="yes" xml:space="preserve">
          <source>The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.</source>
          <target state="translated">ノルムは、あたかもそれらが1つのベクトルに連結されているかのように、すべてのグラデーションをまとめて計算されます。グラデーションはその場で修正されます。</target>
        </trans-unit>
        <trans-unit id="760d37d3c11fd75fa728a8e00ba020ac1e72a10c" translate="yes" xml:space="preserve">
          <source>The normalization parameters are different from the image classification ones, and correspond to the mean and std from Kinetics-400.</source>
          <target state="translated">正規化パラメータは画像分類のものとは異なり、Kinetics-400の平均値と標準値に対応しています。</target>
        </trans-unit>
        <trans-unit id="6b4b699c449424f5a0ee9c963faafecf0fd0d061" translate="yes" xml:space="preserve">
          <source>The number of bins (size 1) is one larger than the largest value in &lt;code&gt;input&lt;/code&gt; unless &lt;code&gt;input&lt;/code&gt; is empty, in which case the result is a tensor of size 0. If &lt;code&gt;minlength&lt;/code&gt; is specified, the number of bins is at least &lt;code&gt;minlength&lt;/code&gt; and if &lt;code&gt;input&lt;/code&gt; is empty, then the result is tensor of size &lt;code&gt;minlength&lt;/code&gt; filled with zeros. If &lt;code&gt;n&lt;/code&gt; is the value at position &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;out[n] += weights[i]&lt;/code&gt; if &lt;code&gt;weights&lt;/code&gt; is specified else &lt;code&gt;out[n] += 1&lt;/code&gt;.</source>
          <target state="translated">ビンの数（サイズ1）は、 &lt;code&gt;input&lt;/code&gt; が空でない限り、 &lt;code&gt;input&lt;/code&gt; の最大値より1つ大きくなります。空の場合、結果はサイズ0のテンソルになります &lt;code&gt;minlength&lt;/code&gt; が指定されている場合、ビンの数は少なくとも &lt;code&gt;minlength&lt;/code&gt; であり、 &lt;code&gt;input&lt;/code&gt; 場合が空の場合、結果はゼロで満たされた &lt;code&gt;minlength&lt;/code&gt; のサイズのテンソルになります。 &lt;code&gt;n&lt;/code&gt; が位置 &lt;code&gt;i&lt;/code&gt; の値の場合、 &lt;code&gt;out[n] += weights[i]&lt;/code&gt; &lt;code&gt;weights&lt;/code&gt; が指定されている場合）それ以外の場合は &lt;code&gt;out[n] += 1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="792cc1a58f0a3a46d19118877b6d13c493cb98ec" translate="yes" xml:space="preserve">
          <source>The number of bits occupied by the type.</source>
          <target state="translated">型が占有するビット数。</target>
        </trans-unit>
        <trans-unit id="280e9a9c6268d27aec18f9552a0a0e023b3a5d6c" translate="yes" xml:space="preserve">
          <source>The number of keys present in the store.</source>
          <target state="translated">店内に存在する鍵の数。</target>
        </trans-unit>
        <trans-unit id="cb1db44aaa6eaf0c2b9f7e2d3594ec089fe36151" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by &lt;code&gt;TensorPipeAgent&lt;/code&gt; to execute requests.</source>
          <target state="translated">&lt;code&gt;TensorPipeAgent&lt;/code&gt; がリクエストを実行するために使用するスレッドプール内のスレッドの数。</target>
        </trans-unit>
        <trans-unit id="20a68b367a08bf827b8cb29b9a7a5775403402ae" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by ProcessGroupAgent.</source>
          <target state="translated">ProcessGroupAgentが使用するスレッドプールのスレッド数。</target>
        </trans-unit>
        <trans-unit id="25f85e0ff2381c99b81c5c68406ed2a516dd1318" translate="yes" xml:space="preserve">
          <source>The numerical properties of a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; can be accessed through either the &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; or the &lt;a href=&quot;#torch.torch.iinfo&quot;&gt;&lt;code&gt;torch.iinfo&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">数値特性&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; は、&lt;/a&gt;いずれかを介してアクセスすることができる&lt;a href=&quot;#torch.torch.finfo&quot;&gt; &lt;code&gt;torch.finfo&lt;/code&gt; &lt;/a&gt;又は&lt;a href=&quot;#torch.torch.iinfo&quot;&gt; &lt;code&gt;torch.iinfo&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d8e759a2b7197a6d6914a25b2e4bccfa94711180" translate="yes" xml:space="preserve">
          <source>The operation applied is:</source>
          <target state="translated">適用される操作は</target>
        </trans-unit>
        <trans-unit id="0b61e3d8bf7f65d3c8a372be2ffe4aa181113744" translate="yes" xml:space="preserve">
          <source>The operation is defined as:</source>
          <target state="translated">として動作を定義しています。</target>
        </trans-unit>
        <trans-unit id="416029866582b442a7cfd38c944f5ccc41de6ea3" translate="yes" xml:space="preserve">
          <source>The operator set above is sufficient to export the following models:</source>
          <target state="translated">上記の演算子のセットは、以下のモデルをエクスポートするのに十分です。</target>
        </trans-unit>
        <trans-unit id="97549a235d386a02f6c874f871b2a3fc368e849a" translate="yes" xml:space="preserve">
          <source>The order of norm. inf refers to &lt;code&gt;float('inf')&lt;/code&gt;, numpy&amp;rsquo;s &lt;code&gt;inf&lt;/code&gt; object, or any equivalent object. The following norms can be calculated:</source>
          <target state="translated">規範の順序。infは、 &lt;code&gt;float('inf')&lt;/code&gt; 、numpyの &lt;code&gt;inf&lt;/code&gt; オブジェクト、または同等のオブジェクトを指します。次の基準を計算できます。</target>
        </trans-unit>
        <trans-unit id="587438e00b276c5c07c33c3b833d1847c428edb1" translate="yes" xml:space="preserve">
          <source>The original &lt;code&gt;module&lt;/code&gt; with the converted &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layers. If the original &lt;code&gt;module&lt;/code&gt; is a &lt;code&gt;BatchNorm*D&lt;/code&gt; layer, a new &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layer object will be returned instead.</source>
          <target state="translated">変換された&lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt; &lt;/a&gt;レイヤーを持つ元の &lt;code&gt;module&lt;/code&gt; 。元の &lt;code&gt;module&lt;/code&gt; が &lt;code&gt;BatchNorm*D&lt;/code&gt; レイヤーの場合、代わりに新しい&lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt; &lt;/a&gt;レイヤーオブジェクトが返されます。</target>
        </trans-unit>
        <trans-unit id="1c43e721e428fefa0921064ef9d974f9d9c3710a" translate="yes" xml:space="preserve">
          <source>The original module with the spectral norm hook</source>
          <target state="translated">スペクトルノルムフックを持つオリジナルモジュール</target>
        </trans-unit>
        <trans-unit id="1c8ee61f168965198d32cd38694cc41b385bdcc7" translate="yes" xml:space="preserve">
          <source>The original module with the weight norm hook</source>
          <target state="translated">ウェイトノルムフック付きのオリジナルモジュール</target>
        </trans-unit>
        <trans-unit id="a7c71aa7150538e239f1ed9763931d65c1aab1d1" translate="yes" xml:space="preserve">
          <source>The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="translated">出力は,任意の入力サイズに対して,サイズ D x H x W である.出力特徴量の数は入力平面の数に等しい。</target>
        </trans-unit>
        <trans-unit id="8d13bc6b68893874e15ef23c3e8bf09f29542e58" translate="yes" xml:space="preserve">
          <source>The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="translated">出力は,任意の入力サイズに対して,サイズ H x W である.出力特徴量の数は入力平面の数に等しい。</target>
        </trans-unit>
        <trans-unit id="dd1aa83704336b7c60e0ac15205527d8852065d5" translate="yes" xml:space="preserve">
          <source>The output of the &lt;code&gt;model&lt;/code&gt; callable when called with the given &lt;code&gt;*args&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt;.</source>
          <target state="translated">指定された &lt;code&gt;*args&lt;/code&gt; および &lt;code&gt;**kwargs&lt;/code&gt; で呼び出されたときに呼び出し可能な &lt;code&gt;model&lt;/code&gt; の出力。</target>
        </trans-unit>
        <trans-unit id="2624c3f93c8a5ba7f8db76475b9b2160784ad8be" translate="yes" xml:space="preserve">
          <source>The output size is H, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="translated">出力サイズは、任意の入力サイズに対してHである。出力特徴量の数は、入力面の数に等しい。</target>
        </trans-unit>
        <trans-unit id="aaed500780776d71aa7aa3beac0d23a2ad0bee52" translate="yes" xml:space="preserve">
          <source>The output tuple size must match the outputs of &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="translated">出力タプルサイズは、 &lt;code&gt;forward&lt;/code&gt; の出力と一致する必要があります。</target>
        </trans-unit>
        <trans-unit id="69fc4cd3bd84d50752da22ec0fa2da75a028b6c8" translate="yes" xml:space="preserve">
          <source>The package needs to be initialized using the &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; function before calling any other methods. This blocks until all processes have joined.</source>
          <target state="translated">パッケージは、他のメソッドを呼び出す前に、&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt;関数を使用して初期化する必要があります。これは、すべてのプロセスが参加するまでブロックされます。</target>
        </trans-unit>
        <trans-unit id="a24470d7f3dafd97610b1a97cfc4e625080f5d77" translate="yes" xml:space="preserve">
          <source>The padding size by which to pad some dimensions of &lt;code&gt;input&lt;/code&gt; are described starting from the last dimension and moving forward.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 一部の次元をパディングするためのパディングサイズは、最後の次元から始まり、前方に向かって説明されています。</target>
        </trans-unit>
        <trans-unit id="b3d894892755f58e334d42f2a6bb03ad3c9c1029" translate="yes" xml:space="preserve">
          <source>The parallelized &lt;code&gt;module&lt;/code&gt; must have its parameters and buffers on &lt;code&gt;device_ids[0]&lt;/code&gt; before running this &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt; module.</source>
          <target state="translated">並列化された &lt;code&gt;module&lt;/code&gt; は、この&lt;a href=&quot;#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; &lt;/a&gt;モジュールを実行する前に、 &lt;code&gt;device_ids[0]&lt;/code&gt; にパラメーターとバッファーを持っている必要があります。</target>
        </trans-unit>
        <trans-unit id="611179cf7c23e5fa87e86dc65fc6c2d7edcfcdbf" translate="yes" xml:space="preserve">
          <source>The parameter can be accessed as an attribute using given name.</source>
          <target state="translated">パラメータは、与えられた名前を使って属性としてアクセスすることができます。</target>
        </trans-unit>
        <trans-unit id="3673b833c2a753da5c92e7b2bd7867575cba0695" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; can either be:</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; は、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="bb60e522db72c109467c75d771e518ca64062c0c" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can each be an &lt;code&gt;int&lt;/code&gt; or a one-element tuple.</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; 、 &lt;code&gt;padding&lt;/code&gt; は、それぞれ &lt;code&gt;int&lt;/code&gt; または1要素のタプルにすることができます。</target>
        </trans-unit>
        <trans-unit id="df694fbe631671fb53535c127328a8f9b5ccfdfe" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can either be:</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; 、 &lt;code&gt;padding&lt;/code&gt; は、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="0d35449e64c1f14944b10572266aa02084f0787f" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt; can either be:</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; 、 &lt;code&gt;padding&lt;/code&gt; 、 &lt;code&gt;dilation&lt;/code&gt; は、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="16e542ef2d9e6480cf0213002d643e04ad39afd9" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;output_padding&lt;/code&gt; can either be:</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; 、 &lt;code&gt;padding&lt;/code&gt; 、 &lt;code&gt;output_padding&lt;/code&gt; は、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="9df0d4116a648b1a73d8fd1ad856ffa0c5b125d9" translate="yes" xml:space="preserve">
          <source>The parameters represented by a single vector</source>
          <target state="translated">単一のベクトルで表されるパラメータ</target>
        </trans-unit>
        <trans-unit id="16c253cfff05379e83f73283d75913a9d3bc5eb0" translate="yes" xml:space="preserve">
          <source>The pivots returned by the function are 1-indexed. If &lt;code&gt;pivot&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the returned pivots is a tensor filled with zeros of the appropriate size.</source>
          <target state="translated">関数によって返されるピボットは1インデックスです。 &lt;code&gt;pivot&lt;/code&gt; が &lt;code&gt;False&lt;/code&gt; の場合、返されるピボットは適切なサイズのゼロで満たされたテンソルです。</target>
        </trans-unit>
        <trans-unit id="4600b83baa50f6eee75af80c6cdb275baf5a4820" translate="yes" xml:space="preserve">
          <source>The pre-trained models for detection, instance segmentation and keypoint detection are initialized with the classification models in torchvision.</source>
          <target state="translated">事前に学習した検出、インスタンスセグメンテーション、キーポイント検出のモデルは、torchvisionの分類モデルで初期化されています。</target>
        </trans-unit>
        <trans-unit id="bacc13981715b20b015df75a9953a0a7c1d88d4c" translate="yes" xml:space="preserve">
          <source>The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in &lt;code&gt;references/segmentation/coco_utils.py&lt;/code&gt;. The classes that the pre-trained model outputs are the following, in order:</source>
          <target state="translated">事前トレーニング済みのモデルは、COCO train2017のサブセットで、PascalVOCデータセットに存在する20のカテゴリでトレーニングされています。サブセットがどのように選択されたかについての詳細は、 &lt;code&gt;references/segmentation/coco_utils.py&lt;/code&gt; ます。事前トレーニング済みモデルが出力するクラスは、次のとおりです。</target>
        </trans-unit>
        <trans-unit id="5f5bcce5c35434b7def7a2c2abecc7855a7ff6ae" translate="yes" xml:space="preserve">
          <source>The process for obtaining the values of &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;std&lt;/code&gt; is roughly equivalent to:</source>
          <target state="translated">&lt;code&gt;mean&lt;/code&gt; 値と &lt;code&gt;std&lt;/code&gt; 値を取得するプロセスは、ほぼ次のようになります。</target>
        </trans-unit>
        <trans-unit id="cb1f1b1b62d82d262fa3ee527ff2235c6fe33cf0" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse is not necessarily a continuous function in the elements of the matrix &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/0117004&quot;&gt;[1]&lt;/a&gt;. Therefore, derivatives are not always existent, and exist for a constant rank only &lt;a href=&quot;https://www.jstor.org/stable/2156365&quot;&gt;[2]&lt;/a&gt;. However, this method is backprop-able due to the implementation by using SVD results, and could be unstable. Double-backward will also be unstable due to the usage of SVD internally. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">疑似逆行列は、必ずしも行列の要素の連続関数ではありません&lt;a href=&quot;https://epubs.siam.org/doi/10.1137/0117004&quot;&gt;[1]&lt;/a&gt;。したがって、導関数は常に存在するわけではなく、一定のランクに対してのみ存在します&lt;a href=&quot;https://www.jstor.org/stable/2156365&quot;&gt;[2]&lt;/a&gt;。ただし、このメソッドは、SVD結果を使用した実装のためにバックプロパゲーション可能であり、不安定になる可能性があります。内部でSVDを使用しているため、ダブルバックワードも不安定になります。詳細については、&lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="df88bf331dfcd873fd61e2a961e2a4565dca71ab" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of &lt;code&gt;input&lt;/code&gt; of dimensions</source>
          <target state="translated">次元の &lt;code&gt;input&lt;/code&gt; の疑似逆行列</target>
        </trans-unit>
        <trans-unit id="801cd7b0289a0cab3f1164523d810bc285cdd10c" translate="yes" xml:space="preserve">
          <source>The published models should be at least in a branch/tag. It can&amp;rsquo;t be a random commit.</source>
          <target state="translated">公開されたモデルは、少なくともブランチ/タグに含まれている必要があります。ランダムコミットにすることはできません。</target>
        </trans-unit>
        <trans-unit id="346d775a68239ce50fc9c648f8e7ac9c30934f97" translate="yes" xml:space="preserve">
          <source>The range of the linear region</source>
          <target state="translated">線形領域の範囲</target>
        </trans-unit>
        <trans-unit id="a67bc4760995b69a50ba7bb512777bd75a29786d" translate="yes" xml:space="preserve">
          <source>The rank of the process group -1, if not part of the group</source>
          <target state="translated">プロセスグループのランク -1、グループに属していない場合は</target>
        </trans-unit>
        <trans-unit id="1640b5e94a24bbde9b57f55563952bd38f53f1c7" translate="yes" xml:space="preserve">
          <source>The real-to-complex Fourier transform results follow conjugate symmetry:</source>
          <target state="translated">実数-複素フーリエ変換の結果は共役対称性に従う。</target>
        </trans-unit>
        <trans-unit id="f3326dbe4c730be437e49c0a1205659d772531b4" translate="yes" xml:space="preserve">
          <source>The regular implementation uses the (more common in PyTorch) &lt;code&gt;torch.long&lt;/code&gt; dtype.</source>
          <target state="translated">通常の実装では、（PyTorchでより一般的な） &lt;code&gt;torch.long&lt;/code&gt; dtypeを使用します。</target>
        </trans-unit>
        <trans-unit id="f42ad6925b2f350139e05854ee56333bfbf81bfa" translate="yes" xml:space="preserve">
          <source>The relation of &lt;code&gt;(U, S, V)&lt;/code&gt; to PCA is as follows:</source>
          <target state="translated">&lt;code&gt;(U, S, V)&lt;/code&gt; とPCAの関係は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="0dee4722eb608774d930f39be3f46f18bcb0b430" translate="yes" xml:space="preserve">
          <source>The result will never require gradient.</source>
          <target state="translated">結果的にグラデーションを必要とすることはありません。</target>
        </trans-unit>
        <trans-unit id="6b0575a2f172061283c10c05f0db13db17166b95" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;alexnet.onnx&lt;/code&gt; is a binary protobuf file which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The keyword argument &lt;code&gt;verbose=True&lt;/code&gt; causes the exporter to print out a human-readable representation of the network:</source>
          <target state="translated">結果の &lt;code&gt;alexnet.onnx&lt;/code&gt; は、エクスポートしたモデル（この場合はAlexNet）のネットワーク構造とパラメーターの両方を含むバイナリprotobufファイルです。キーワード引数 &lt;code&gt;verbose=True&lt;/code&gt; を指定すると、エクスポータは人間が読める形式のネットワークを出力します。</target>
        </trans-unit>
        <trans-unit id="3c02f08f3d1024ae629cdcc31caf60626bc7a365" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;out&lt;/code&gt; tensor shares it&amp;rsquo;s underlying storage with the &lt;code&gt;input&lt;/code&gt; tensor, so changing the content of one would change the content of the other.</source>
          <target state="translated">結果の &lt;code&gt;out&lt;/code&gt; 、それはとストレージの基礎となるだテンソル株式 &lt;code&gt;input&lt;/code&gt; 1の内容を変更すると、他の内容を変更しますので、テンソル。</target>
        </trans-unit>
        <trans-unit id="6758477f8561982cc2515cdbf897a47792390375" translate="yes" xml:space="preserve">
          <source>The resulting recording of &lt;code&gt;nn.Module.forward&lt;/code&gt; or &lt;code&gt;nn.Module&lt;/code&gt; produces &lt;code&gt;ScriptModule&lt;/code&gt;.</source>
          <target state="translated">結果として得られる &lt;code&gt;nn.Module.forward&lt;/code&gt; または &lt;code&gt;nn.Module&lt;/code&gt; の記録により、ScriptModuleが生成され &lt;code&gt;ScriptModule&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="094611e344ec0108ea463a9054c21e208ca824dd" translate="yes" xml:space="preserve">
          <source>The resulting recording of a standalone function produces &lt;code&gt;ScriptFunction&lt;/code&gt;.</source>
          <target state="translated">結果として得られるスタンドアロン関数の記録により、 &lt;code&gt;ScriptFunction&lt;/code&gt; が生成されます。</target>
        </trans-unit>
        <trans-unit id="14a7d916a182796f96dea780975d504da8d70e49" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object can come from &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; constructor. The example below shows directly using the &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; returned by &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返される&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;オブジェクトは、&lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt; &lt;code&gt;then()&lt;/code&gt; &lt;/a&gt;、または&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;コンストラクターから取得できます。以下の例は、&lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt; &lt;code&gt;then()&lt;/code&gt; &lt;/a&gt;によって返される&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;を直接使用することを示しています。</target>
        </trans-unit>
        <trans-unit id="35586b86275e1afc45c84727656b22c56c0dfc4c" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;out&lt;/code&gt; tensor only has values 0 or 1 and is of the same shape as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返さ &lt;code&gt;out&lt;/code&gt; テンソルの値は0または1のみで、 &lt;code&gt;input&lt;/code&gt; と同じ形状です。</target>
        </trans-unit>
        <trans-unit id="0c1ca22658d19c73ead093d5a482f3f0748946cd" translate="yes" xml:space="preserve">
          <source>The returned Tensor&amp;rsquo;s data will be of size &lt;code&gt;T x B x *&lt;/code&gt;, where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence and &lt;code&gt;B&lt;/code&gt; is the batch size. If &lt;code&gt;batch_first&lt;/code&gt; is True, the data will be transposed into &lt;code&gt;B x T x *&lt;/code&gt; format.</source>
          <target state="translated">返されるTensorのデータのサイズは &lt;code&gt;T x B x *&lt;/code&gt; になります。ここで、 &lt;code&gt;T&lt;/code&gt; は最長のシーケンスの長さ、 &lt;code&gt;B&lt;/code&gt; はバッチサイズです。場合 &lt;code&gt;batch_first&lt;/code&gt; がTrueで、データがに移調されます &lt;code&gt;B x T x *&lt;/code&gt; 形式を。</target>
        </trans-unit>
        <trans-unit id="a86d643d348528641013d71d94cfc9c2436b5f5a" translate="yes" xml:space="preserve">
          <source>The returned matrices will always be transposed, irrespective of the strides of the input matrices. That is, they will have stride &lt;code&gt;(1, m)&lt;/code&gt; instead of &lt;code&gt;(m, 1)&lt;/code&gt;.</source>
          <target state="translated">返される行列は、入力行列のストライドに関係なく、常に転置されます。つまり、 &lt;code&gt;(m, 1)&lt;/code&gt; ではなくストライド &lt;code&gt;(1, m)&lt;/code&gt; になります。</target>
        </trans-unit>
        <trans-unit id="e874ec4d7da2420c99c0303e4f7a958324a670ae" translate="yes" xml:space="preserve">
          <source>The returned tensor and &lt;code&gt;ndarray&lt;/code&gt; share the same memory. Modifications to the tensor will be reflected in the &lt;code&gt;ndarray&lt;/code&gt; and vice versa. The returned tensor is not resizable.</source>
          <target state="translated">返されたテンソルと &lt;code&gt;ndarray&lt;/code&gt; は同じメモリを共有します。テンソルへの変更は &lt;code&gt;ndarray&lt;/code&gt; に反映され、その逆も同様です。返されたテンソルはサイズ変更できません。</target>
        </trans-unit>
        <trans-unit id="93c4d85268a064ece907fbc8a5535a6f396d9a4e" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor</source>
          <target state="translated">返されたテンソルは、元のテンソルと同じストレージを使用しませ&lt;strong&gt;ん&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="52f5aa4074a60eef379e1658572f1bb6509ef7da" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor. If &lt;code&gt;out&lt;/code&gt; has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.</source>
          <target state="translated">返されたテンソルは、元のテンソルと同じストレージを使用しませ&lt;strong&gt;ん&lt;/strong&gt;。 &lt;code&gt;out&lt;/code&gt; の形状が予想と異なる場合は、サイレントに正しい形状に変更し、必要に応じて基になるストレージを再割り当てします。</target>
        </trans-unit>
        <trans-unit id="f46100c8ee7a830bab0c89e9873612006ce16b25" translate="yes" xml:space="preserve">
          <source>The returned tensor has the same number of dimensions as the original tensor (&lt;code&gt;input&lt;/code&gt;). The &lt;code&gt;dim&lt;/code&gt;th dimension has the same size as the length of &lt;code&gt;index&lt;/code&gt;; other dimensions have the same size as in the original tensor.</source>
          <target state="translated">返されるテンソルの次元数は、元のテンソル（ &lt;code&gt;input&lt;/code&gt; ）と同じです。 &lt;code&gt;dim&lt;/code&gt; 番目の次元は長さと同じ大きさを有する &lt;code&gt;index&lt;/code&gt; ; 他の寸法は、元のテンソルと同じサイズです。</target>
        </trans-unit>
        <trans-unit id="c72a07e12ac6ea766a45eac39941a4c32ee2a049" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions</source>
          <target state="translated">返されるテンソルは同じデータを共有し、要素数は同じでなければなりませんが、サイズは異なる場合があります。テンソルが表示されるためには、新しい表示サイズは元のサイズとストライドと互換性がなければならない、つまり、各新しい表示次元は元の次元の部分空間であるか、元の次元を横切るだけでなければならない。</target>
        </trans-unit>
        <trans-unit id="e400aae54e1364ef7f0ae0e4394533a68ef83da8" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same underlying data with this tensor.</source>
          <target state="translated">返されたテンソルは、このテンソルと同じ基礎データを共有しています。</target>
        </trans-unit>
        <trans-unit id="172826726b64a692f9e7f61dbdcbf5a0b8b36f39" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.</source>
          <target state="translated">返されたテンソルは入力テンソルとストレージを共有しているので、一方のテンソルの内容を変更すると他方のテンソルの内容も変更されます。</target>
        </trans-unit>
        <trans-unit id="28c770021d54b212e8c62be1e09939a0369312ae" translate="yes" xml:space="preserve">
          <source>The rows of &lt;code&gt;input&lt;/code&gt; do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; の行を合計して1にする必要はありませんが（この場合、値を重みとして使用します）、非負で有限であり、合計がゼロ以外である必要があります。</target>
        </trans-unit>
        <trans-unit id="1eff8f651f1c1e81084414db9ad8e86ff7d386f5" translate="yes" xml:space="preserve">
          <source>The second argument can be a number or a tensor whose shape is &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the first argument.</source>
          <target state="translated">2番目の引数は、最初の引数で形状を&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;できる数値またはテンソルにすることができます。</target>
        </trans-unit>
        <trans-unit id="35bad0572344e9ca340bcfea7b7e08eb1fb48094" translate="yes" xml:space="preserve">
          <source>The shape of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">テンソルの形状は、可変引数 &lt;code&gt;size&lt;/code&gt; によって定義されます。</target>
        </trans-unit>
        <trans-unit id="44ad26e34a9ec24516da93c641da630606e76045" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt; don&amp;rsquo;t need to match, but the total number of elements in each tensor need to be the same.</source>
          <target state="translated">&lt;a href=&quot;torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;torch.std#torch.std&quot;&gt; &lt;code&gt;std&lt;/code&gt; &lt;/a&gt;の形状は一致する必要はありませんが、各テンソルの要素の総数は同じである必要があります。</target>
        </trans-unit>
        <trans-unit id="7ab58315d98cc62bd1c43b5269219135f914a361" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">形状&lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;、 &lt;code&gt;tensor1&lt;/code&gt; 、および &lt;code&gt;tensor2&lt;/code&gt; がなければなりません&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="472d04a7b4ef19856846b867c2431ae86dfeee09" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; や &lt;code&gt;other&lt;/code&gt; の形状は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能でなければなりません。</target>
        </trans-unit>
        <trans-unit id="4bb8d54bf4e66fa1d51318971fbd84c5dd471f64" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">形状 &lt;code&gt;input&lt;/code&gt; 、 &lt;code&gt;tensor1&lt;/code&gt; 、および &lt;code&gt;tensor2&lt;/code&gt; がなければなりません&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="3c2b5afb57ab65d651f6e8673ddc0bb16bc251d9" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;. If &lt;code&gt;weight&lt;/code&gt; is a tensor, then the shapes of &lt;code&gt;weight&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;start&lt;/code&gt; と &lt;code&gt;end&lt;/code&gt; の形状は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能である必要があります。 &lt;code&gt;weight&lt;/code&gt; がテンソルの場合、 &lt;code&gt;weight&lt;/code&gt; 、 &lt;code&gt;start&lt;/code&gt; 、および &lt;code&gt;end&lt;/code&gt; 形状は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能である必要があります。</target>
        </trans-unit>
        <trans-unit id="30ae8ec04282dfa89f820e2638cc76a4c5308051" translate="yes" xml:space="preserve">
          <source>The shapes of the &lt;code&gt;mask&lt;/code&gt; tensor and the &lt;code&gt;input&lt;/code&gt; tensor don&amp;rsquo;t need to match, but they must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;mask&lt;/code&gt; テンソルと &lt;code&gt;input&lt;/code&gt; テンソルの形状は一致する必要はありませんが、&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能である必要があります。</target>
        </trans-unit>
        <trans-unit id="80ed96242b937c96d491a6608007e2c80ef6fbfe" translate="yes" xml:space="preserve">
          <source>The singular values are returned in descending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.</source>
          <target state="translated">特異値は降順で返されます。場合 &lt;code&gt;input&lt;/code&gt; 行列のバッチは、次いで、バッチ内の各行列の特異値が降順で戻されます。</target>
        </trans-unit>
        <trans-unit id="86c71484bcb812cb0f2e7260fb264f82086dbacb" translate="yes" xml:space="preserve">
          <source>The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for &lt;code&gt;offset&lt;/code&gt; other than</source>
          <target state="translated">新しい行列のサイズは、最後の入力次元のサイズの指定された対角線になるように計算されます。 &lt;code&gt;offset&lt;/code&gt; 以外の場合は注意してください</target>
        </trans-unit>
        <trans-unit id="0739eb19e36226e98c11fd96e8291d8a209e3a7e" translate="yes" xml:space="preserve">
          <source>The smallest positive representable number.</source>
          <target state="translated">正の表現可能な最小の数。</target>
        </trans-unit>
        <trans-unit id="cf0617c3e175f01c2b40e71995108022f6b64f95" translate="yes" xml:space="preserve">
          <source>The smallest representable number (typically &lt;code&gt;-max&lt;/code&gt;).</source>
          <target state="translated">表現可能な最小の数（通常は &lt;code&gt;-max&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="27d91cd31d6556c601166e6b57bbeb8fc2cd13fe" translate="yes" xml:space="preserve">
          <source>The smallest representable number such that &lt;code&gt;1.0 + eps != 1.0&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;1.0 + eps != 1.0&lt;/code&gt; ような表現可能な最小の数。</target>
        </trans-unit>
        <trans-unit id="ff2ebfdbbf9420dca19c2e2c489305049c84e69e" translate="yes" xml:space="preserve">
          <source>The smallest representable number.</source>
          <target state="translated">表現可能な最小の数。</target>
        </trans-unit>
        <trans-unit id="6e4db462bf55509b809c59ea2298379ae2dba77f" translate="yes" xml:space="preserve">
          <source>The sources in &lt;code&gt;cuda_sources&lt;/code&gt; are concatenated into a separate &lt;code&gt;.cu&lt;/code&gt; file and prepended with &lt;code&gt;torch/types.h&lt;/code&gt;, &lt;code&gt;cuda.h&lt;/code&gt; and &lt;code&gt;cuda_runtime.h&lt;/code&gt; includes. The &lt;code&gt;.cpp&lt;/code&gt; and &lt;code&gt;.cu&lt;/code&gt; files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in &lt;code&gt;cuda_sources&lt;/code&gt; per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the &lt;code&gt;cpp_sources&lt;/code&gt; (and include its name in &lt;code&gt;functions&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;cuda_sources&lt;/code&gt; のソースは、個別の &lt;code&gt;.cu&lt;/code&gt; ファイルに連結され、 &lt;code&gt;torch/types.h&lt;/code&gt; 、 &lt;code&gt;cuda.h&lt;/code&gt; 、および &lt;code&gt;cuda_runtime.h&lt;/code&gt; インクルードが付加されます。 &lt;code&gt;.cpp&lt;/code&gt; ファイルと &lt;code&gt;.cu&lt;/code&gt; ファイルが別々にコンパイルが、最終的に1つのライブラリにリンクされています。 &lt;code&gt;cuda_sources&lt;/code&gt; 自体の関数に対してバインディングは生成されないことに注意してください。 CUDAカーネルにバインドするには、それを呼び出すC ++関数を作成し、このC ++関数を &lt;code&gt;cpp_sources&lt;/code&gt; の1つで宣言または定義する（そしてその名前を &lt;code&gt;functions&lt;/code&gt; 含める）必要があります。</target>
        </trans-unit>
        <trans-unit id="f06fdc0c2350c66f3a9d82727c760926c8016f15" translate="yes" xml:space="preserve">
          <source>The stashing logic saves and restores the RNG state for the current device and the device of all cuda Tensor arguments to the &lt;code&gt;run_fn&lt;/code&gt;. However, the logic has no way to anticipate if the user will move Tensors to a new device within the &lt;code&gt;run_fn&lt;/code&gt; itself. Therefore, if you move Tensors to a new device (&amp;ldquo;new&amp;rdquo; meaning not belonging to the set of [current device + devices of Tensor arguments]) within &lt;code&gt;run_fn&lt;/code&gt;, deterministic output compared to non-checkpointed passes is never guaranteed.</source>
          <target state="translated">スタッシュロジックが保存され、現在のデバイスとの全てのCUDAテンソル引数のデバイスのためにRNG状態を復元 &lt;code&gt;run_fn&lt;/code&gt; 。ただし、ロジックには、ユーザーが &lt;code&gt;run_fn&lt;/code&gt; 自体の中でTensorを新しいデバイスに移動するかどうかを予測する方法がありません。あなたが新しいデバイスにテンソルを移動した場合そのため、内（「新」[テンソル引数の現在のデバイス+デバイス]のセットに属していないという意味） &lt;code&gt;run_fn&lt;/code&gt; 保証されることはありません非チェックポイントのパスに比べて、決定論的出力。</target>
        </trans-unit>
        <trans-unit id="b7f3ab8a2df042d6e3025c18a145e6e14f77af25" translate="yes" xml:space="preserve">
          <source>The sum operation still operates over all the elements, and divides by</source>
          <target state="translated">和演算はまだすべての要素にわたって動作し、次のようにして除算します。</target>
        </trans-unit>
        <trans-unit id="b2531e155fe9e77a5cfae31abf7b6f59ffb9fcaa" translate="yes" xml:space="preserve">
          <source>The support of third-party backend is experimental and subject to change.</source>
          <target state="translated">サードパーティ製バックエンドのサポートは実験的なものであり、変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="2873a12c84aba239b7842369e12bb64236d9f288" translate="yes" xml:space="preserve">
          <source>The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once.</source>
          <target state="translated">テンソルはdlpackで表現されたオブジェクトとメモリを共有します。各dlpackは一度しか消費できないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="dc114e497003f25e6fca05f82fcba607f7d78078" translate="yes" xml:space="preserve">
          <source>The tensors &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">テンソル &lt;code&gt;condition&lt;/code&gt; 、 &lt;code&gt;x&lt;/code&gt; 、 &lt;code&gt;y&lt;/code&gt; は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能でなければなりません。</target>
        </trans-unit>
        <trans-unit id="ab7ff2ed427b3e162ddc9c1bc3618ade11c4bc8e" translate="yes" xml:space="preserve">
          <source>The torch package contains data structures for multi-dimensional tensors and mathematical operations over these are defined. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.</source>
          <target state="translated">torch パッケージには多次元テンソルのデータ構造が含まれており、これらに対する数学演算が定義されています。さらに、テンソルや任意の型を効率的にシリアライズするための多くのユーティリティや、その他の便利なユーティリティも提供しています。</target>
        </trans-unit>
        <trans-unit id="437652102ffae6443117ed89a3eae4a8f5b41da3" translate="yes" xml:space="preserve">
          <source>The tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor:</source>
          <target state="translated">トレーサは,トレースされた計算でいくつかの問題のあるパターンに対して警告を表示します.例として、Tensorのスライス(ビュー)上のインプレース代入を含む関数のトレースを見てみましょう。</target>
        </trans-unit>
        <trans-unit id="89850de90f112bfc18c8cdc61311b3872ebf2872" translate="yes" xml:space="preserve">
          <source>The tracer records the example inputs shape in the graph. In case the model should accept inputs of dynamic shape, you can utilize the parameter &lt;code&gt;dynamic_axes&lt;/code&gt; in export api.</source>
          <target state="translated">トレーサーは、入力形状の例をグラフに記録します。モデルが動的形状の入力を受け入れる必要がある場合は、エクスポートAPIでパラメーター &lt;code&gt;dynamic_axes&lt;/code&gt; を利用できます。</target>
        </trans-unit>
        <trans-unit id="f2328cc72d77d5be4ac31362bf776708e1354f25" translate="yes" xml:space="preserve">
          <source>The unreduced (i.e. with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) loss can be described as:</source>
          <target state="translated">&lt;code&gt;reduction&lt;/code&gt; されて &lt;code&gt;'none'&lt;/code&gt; （つまり、削減が「なし」に設定されている）損失は、次のように説明できます。</target>
        </trans-unit>
        <trans-unit id="83a2adc9d7ff925b785017e994bf1c2dfff219e3" translate="yes" xml:space="preserve">
          <source>The unreduced loss (i.e., with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) can be described as:</source>
          <target state="translated">&lt;code&gt;reduction&lt;/code&gt; されて &lt;code&gt;'none'&lt;/code&gt; 損失（つまり、削減が「なし」に設定されている場合）は、次のように説明できます。</target>
        </trans-unit>
        <trans-unit id="55b435ee6855f1f093ca66cf72abe8bd2c30eb03" translate="yes" xml:space="preserve">
          <source>The upper triangular part of the matrix is defined as the elements on and above the diagonal.</source>
          <target state="translated">行列の上三角部分は、対角線上の要素とその上の要素として定義されます。</target>
        </trans-unit>
        <trans-unit id="8ff79d6204f8beff86c1883967aad41f978dae9c" translate="yes" xml:space="preserve">
          <source>The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.</source>
          <target state="translated">このユーティリティは、ノードごとに1つ以上のプロセスが生成されるシングルノード分散トレーニングに使用できます。このユーティリティは、CPU トレーニングまたは GPU トレーニングのいずれかに使用することができます。ユーティリティをGPUトレーニングに使用する場合、各分散プロセスは1つのGPU上で動作します。これにより、シングルノードのトレーニング性能を十分に向上させることができます。また、マルチノード分散トレーニングにも使用することができ、マルチノード分散トレーニングの性能を向上させるために、各ノード上で複数のプロセスを生成します。特に、直接GPUをサポートしている複数のInfinibandインタフェースを持つシステムでは、それらのインタフェースを集約した通信帯域を利用することができるため、大きなメリットがあります。</target>
        </trans-unit>
        <trans-unit id="a8baea02cc4125823d2a9295d67b336747f5d060" translate="yes" xml:space="preserve">
          <source>The value held by this &lt;code&gt;Future&lt;/code&gt;. If the function (callback or RPC) creating the value has thrown an error, this &lt;code&gt;wait&lt;/code&gt; method will also throw an error.</source>
          <target state="translated">この &lt;code&gt;Future&lt;/code&gt; 持つ価値。値を作成する関数（コールバックまたはRPC）がエラーをスローした場合、この &lt;code&gt;wait&lt;/code&gt; メソッドもエラーをスローします。</target>
        </trans-unit>
        <trans-unit id="4c32d99e8bca557bc1750470ea69ae3bfb345050" translate="yes" xml:space="preserve">
          <source>The values of this class are lowercase strings, e.g., &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;. They can be accessed as attributes, e.g., &lt;code&gt;Backend.NCCL&lt;/code&gt;.</source>
          <target state="translated">このクラスの値は小文字の文字列です（例： &lt;code&gt;&quot;gloo&quot;&lt;/code&gt; )。これらは、 &lt;code&gt;Backend.NCCL&lt;/code&gt; などの属性としてアクセスできます。</target>
        </trans-unit>
        <trans-unit id="2d09c2230d5453858141229e7b0087efe234f737" translate="yes" xml:space="preserve">
          <source>The values of this class can be accessed as attributes, e.g., &lt;code&gt;ReduceOp.SUM&lt;/code&gt;. They are used in specifying strategies for reduction collectives, e.g., &lt;a href=&quot;#torch.distributed.reduce&quot;&gt;&lt;code&gt;reduce()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt;&lt;code&gt;all_reduce_multigpu()&lt;/code&gt;&lt;/a&gt;, etc.</source>
          <target state="translated">このクラスの値には、 &lt;code&gt;ReduceOp.SUM&lt;/code&gt; などの属性としてアクセスできます。これらは、&lt;a href=&quot;#torch.distributed.reduce&quot;&gt; &lt;code&gt;reduce()&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt; &lt;code&gt;all_reduce_multigpu()&lt;/code&gt; &lt;/a&gt;などの削減集合の戦略を指定する際に使用されます。</target>
        </trans-unit>
        <trans-unit id="539dcafbce59fbba03d94008a7b26e37638c0f1b" translate="yes" xml:space="preserve">
          <source>The world size of the process group -1, if not part of the group</source>
          <target state="translated">プロセスグループ -1 のワールドサイズ.</target>
        </trans-unit>
        <trans-unit id="a5d66dffec3f1eab9a22e5606deb2eaebb37f157" translate="yes" xml:space="preserve">
          <source>Then &lt;code&gt;dynamic axes&lt;/code&gt; can be defined either as:</source>
          <target state="translated">次に、 &lt;code&gt;dynamic axes&lt;/code&gt; は次のいずれかとして定義できます。</target>
        </trans-unit>
        <trans-unit id="cacf045e7460da89c00e2f44ce242ff34d557d4d" translate="yes" xml:space="preserve">
          <source>Then for any (supported) &lt;code&gt;input&lt;/code&gt; tensor the following equality holds:</source>
          <target state="translated">次に、（サポートされている） &lt;code&gt;input&lt;/code&gt; テンソルに対して、次の等式が成り立ちます。</target>
        </trans-unit>
        <trans-unit id="61c6ce8b4e502ae0f9cf574180a84ce71379a45a" translate="yes" xml:space="preserve">
          <source>Then run the following code in two different processes:</source>
          <target state="translated">次に、以下のコードを2つの異なるプロセスで実行します。</target>
        </trans-unit>
        <trans-unit id="1ce92152c376045e006c7df7e500a93b25016945" translate="yes" xml:space="preserve">
          <source>Then, you can run:</source>
          <target state="translated">ならば、走ればいい。</target>
        </trans-unit>
        <trans-unit id="9157fc1a5d0f6b3459cf4d13b451acd4e1ae988a" translate="yes" xml:space="preserve">
          <source>There are 2 main ways to initialize a process group:</source>
          <target state="translated">プロセスグループを初期化するには、主に2つの方法があります。</target>
        </trans-unit>
        <trans-unit id="989aa62e15f82179c46dd7bca86e4177e7b7bf45" translate="yes" xml:space="preserve">
          <source>There are a few main ways to create a tensor, depending on your use case.</source>
          <target state="translated">テンソルを作成するには、ユースケースに応じて主にいくつかの方法があります。</target>
        </trans-unit>
        <trans-unit id="448eaecf17e019efecc19b1280b7ffb9043292f0" translate="yes" xml:space="preserve">
          <source>There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</source>
          <target state="translated">Tensorsで定義されているインプレースランダムサンプリング関数もいくつかあります。クリックしてドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="4a50a4f02b41f0e4e7768bdf9e0d8e8e8f9ca103" translate="yes" xml:space="preserve">
          <source>There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables:</source>
          <target state="translated">cuDNNおよびCUDAの一部のバージョンでは、RNN関数に非決定論的な問題があることが知られています。以下の環境変数を設定することで、決定論的な動作を強制することができます。</target>
        </trans-unit>
        <trans-unit id="94f93e5cb61707d9a9f9f8c1e8bae87549a0661b" translate="yes" xml:space="preserve">
          <source>There are more examples in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;symbolic_opset9.py&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset10.py&quot;&gt;symbolic_opset10.py&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;symbolic_opset9.py&lt;/a&gt;、&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset10.py&quot;&gt;symbolic_opset10.pyに&lt;/a&gt;はさらに多くの例があります。</target>
        </trans-unit>
        <trans-unit id="e7b2c38d0b8818a513a12fc7c7e90b9479638b6e" translate="yes" xml:space="preserve">
          <source>There are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:</source>
          <target state="translated">与えられたPythonの関数/モジュールのトレースが基礎となるコードを代表するものではない場合、いくつかのエッジケースが存在します。これらのケースには以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="7b0666c05baae6bc9dd7801107606ece55efb258" translate="yes" xml:space="preserve">
          <source>There are two main usages:</source>
          <target state="translated">主に2つの用途があります。</target>
        </trans-unit>
        <trans-unit id="7b78c278fc5d3de5d25f299a98858079b45aa6eb" translate="yes" xml:space="preserve">
          <source>There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired &lt;code&gt;world_size&lt;/code&gt;. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.</source>
          <target state="translated">TCPを使用して初期化する方法は2つあり、どちらもすべてのプロセスから到達可能なネットワークアドレスと目的の &lt;code&gt;world_size&lt;/code&gt; が必要です。最初の方法では、ランク0プロセスに属するアドレスを指定する必要があります。この初期化方法では、すべてのプロセスが手動でランクを指定している必要があります。</target>
        </trans-unit>
        <trans-unit id="f2b97adc02fc81457ca80fb753c694becd93e274" translate="yes" xml:space="preserve">
          <source>There is a subtlety in using the &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; pattern in a &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; wrapped in &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;My recurrent network doesn&amp;rsquo;t work with data parallelism&lt;/a&gt; section in FAQ for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; で&lt;/a&gt;ラップされた&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; パターンを使用することには微妙な点があります。詳細については、FAQの&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;「リカレントネットワークがデータ並列処理で機能しない」&lt;/a&gt;セクションを参照してください。</target>
        </trans-unit>
        <trans-unit id="6bedff7a4f5803a09c69737d429b263e4a4e07c0" translate="yes" xml:space="preserve">
          <source>Therefore, indexing &lt;code&gt;output&lt;/code&gt; at the last dimension (column dimension) gives all values within a certain block.</source>
          <target state="translated">したがって、最後の次元（列次元）で &lt;code&gt;output&lt;/code&gt; にインデックスを付けると、特定のブロック内のすべての値が得られます。</target>
        </trans-unit>
        <trans-unit id="220cff043cb62e0e4cb5366907ebd80ead3bf366" translate="yes" xml:space="preserve">
          <source>Therefore, to invert an &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;normalized&lt;/code&gt; and &lt;code&gt;onesided&lt;/code&gt; arguments should be set identically for &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, and preferably a &lt;code&gt;signal_sizes&lt;/code&gt; is given to avoid size mismatch. See the example below for a case of size mismatch.</source>
          <target state="translated">したがって、&lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;を反転するには、 &lt;code&gt;normalized&lt;/code&gt; された引数と &lt;code&gt;onesided&lt;/code&gt; 引数を&lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;に対して同じように設定する必要があります。サイズの不一致を避けるために、 &lt;code&gt;signal_sizes&lt;/code&gt; を指定することをお勧めします。サイズが一致しない場合は、以下の例を参照してください。</target>
        </trans-unit>
        <trans-unit id="11de0478b61c8d1fd83658744b437989c835c987" translate="yes" xml:space="preserve">
          <source>These are the basic building block for graphs</source>
          <target state="translated">これらはグラフの基本的な構成要素です。</target>
        </trans-unit>
        <trans-unit id="8ca4969bd4e4273b5349ab2f675d16c1de44f187" translate="yes" xml:space="preserve">
          <source>These backends include:</source>
          <target state="translated">これらのバックエンドには、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="3dd026a9fc19a074410394c15ee56c930e5686d0" translate="yes" xml:space="preserve">
          <source>These types and features from the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module are unavailble in TorchScript.</source>
          <target state="translated">&lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt;モジュールのこれらのタイプと機能は、TorchScriptでは使用できません。</target>
        </trans-unit>
        <trans-unit id="aca9bd729c973c4d3bc4eb7f3f8d4e953c82edfd" translate="yes" xml:space="preserve">
          <source>These unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member.</source>
          <target state="translated">これらはループを展開し、タプルの各メンバのボディを生成します。ボディは各メンバに対して正しくタイプチェックを行う必要があります。</target>
        </trans-unit>
        <trans-unit id="5042b3fea81612a742984a1fb55be5b675720283" translate="yes" xml:space="preserve">
          <source>Third-party backends</source>
          <target state="translated">サードパーティのバックエンド</target>
        </trans-unit>
        <trans-unit id="f9ab4d0d07c5bfd671637ba9723d67603253bcae" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;momentum&lt;/code&gt; argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is</source>
          <target state="translated">この &lt;code&gt;momentum&lt;/code&gt; 議論は、オプティマイザークラスや従来の運動量の概念で使用されているものとは異なります。数学的には、ここで統計を実行するための更新ルールは次のとおりです。</target>
        </trans-unit>
        <trans-unit id="4d057f12e63c19fa40f83b9e371a09dbaf3641ca" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;setuptools.build_ext&lt;/code&gt; subclass takes care of passing the minimum required compiler flags (e.g. &lt;code&gt;-std=c++14&lt;/code&gt;) as well as mixed C++/CUDA compilation (and support for CUDA files in general).</source>
          <target state="translated">この &lt;code&gt;setuptools.build_ext&lt;/code&gt; サブクラスは、最低限必要なコンパイラフラグ（例： &lt;code&gt;-std=c++14&lt;/code&gt; ）と、混合C ++ / CUDAコンパイル（および一般的なCUDAファイルのサポート）の受け渡しを処理します。</target>
        </trans-unit>
        <trans-unit id="48b0bfc66718d9ed75062a9f5d57a687fd7749c9" translate="yes" xml:space="preserve">
          <source>This API is in beta and may change in the near future.</source>
          <target state="translated">このAPIはベータ版であり、近い将来変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="33a32d5fcbf1c45d06b8358ae2591cd5047cff16" translate="yes" xml:space="preserve">
          <source>This allows better BC support for &lt;a href=&quot;#torch.nn.Module.load_state_dict&quot;&gt;&lt;code&gt;load_state_dict()&lt;/code&gt;&lt;/a&gt;. In &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt;, the version number will be saved as in the attribute &lt;code&gt;_metadata&lt;/code&gt; of the returned state dict, and thus pickled. &lt;code&gt;_metadata&lt;/code&gt; is a dictionary with keys that follow the naming convention of state dict. See &lt;code&gt;_load_from_state_dict&lt;/code&gt; on how to use this information in loading.</source>
          <target state="translated">これにより、&lt;a href=&quot;#torch.nn.Module.load_state_dict&quot;&gt; &lt;code&gt;load_state_dict()&lt;/code&gt; の&lt;/a&gt;BCサポートが向上します。で&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;、バージョン番号は属性のように保存されます &lt;code&gt;_metadata&lt;/code&gt; 返された状態のdictの、ひいては漬け。 &lt;code&gt;_metadata&lt;/code&gt; は、statedictの命名規則に従ったキーを持つ辞書です。ロード時にこの情報を使用する方法については、 &lt;code&gt;_load_from_state_dict&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="09b607c5ebd52b97aa3c6cf95a474cc71af1ac47" translate="yes" xml:space="preserve">
          <source>This allows for different samples to have variable amounts of target classes.</source>
          <target state="translated">これにより、異なるサンプルが可変量のターゲットクラスを持つことができます。</target>
        </trans-unit>
        <trans-unit id="dc869bc2254cbb18e88bb0a8c7e186c575d69c2e" translate="yes" xml:space="preserve">
          <source>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.</source>
          <target state="translated">これはまた、関連するパラメータやバッファを別のオブジェクトにします。そのため、最適化中にモジュールがGPU上で動作するのであれば、オプティマイザを構築する前に呼ばれるべきです。</target>
        </trans-unit>
        <trans-unit id="cd79f292616deed21e20e4449558742671a5f03e" translate="yes" xml:space="preserve">
          <source>This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set &lt;code&gt;use_external_data_format&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; to successfully export such models.</source>
          <target state="translated">この引数により、大規模モデルをONNXにエクスポートできます。protobufのサイズ制限のため、2GBを超えるモデルを1つのファイルにエクスポートすることはできません。このようなモデルを正常にエクスポートするには、ユーザーは &lt;code&gt;use_external_data_format&lt;/code&gt; を &lt;code&gt;True&lt;/code&gt; に設定する必要があります。</target>
        </trans-unit>
        <trans-unit id="d9aed446ae3aa4dc28d8d65d1c929ff54cb2a74f" translate="yes" xml:space="preserve">
          <source>This attribute is &lt;code&gt;None&lt;/code&gt; by default and becomes a Tensor the first time a call to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; computes gradients for &lt;code&gt;self&lt;/code&gt;. The attribute will then contain the gradients computed and future calls to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; will accumulate (add) gradients into it.</source>
          <target state="translated">この属性はデフォルトでは &lt;code&gt;None&lt;/code&gt; であり、&lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; の&lt;/a&gt;呼び出しが &lt;code&gt;self&lt;/code&gt; の勾配を初めて計算するときにテンソルになります。次に、属性には計算されたグラデーションが含まれ、&lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt;への今後の呼び出しはグラデーションを蓄積（追加）します。</target>
        </trans-unit>
        <trans-unit id="27eaef81d3fd1f9e704b27f64a092d8671cfb020" translate="yes" xml:space="preserve">
          <source>This can be called as</source>
          <target state="translated">として呼び出すことができます。</target>
        </trans-unit>
        <trans-unit id="e5eeeb2d9e42f39a239e76c72a9314e3136e5540" translate="yes" xml:space="preserve">
          <source>This can then be visualized with TensorBoard, which should be installable and runnable with:</source>
          <target state="translated">これをTensorBoardで可視化することができるので、TensorBoardでインストールして実行できるようにする必要があります。</target>
        </trans-unit>
        <trans-unit id="2306bf42c44895d1fc76396938e439c137c2dffb" translate="yes" xml:space="preserve">
          <source>This class can be directly called to parse the string, e.g., &lt;code&gt;Backend(backend_str)&lt;/code&gt; will check if &lt;code&gt;backend_str&lt;/code&gt; is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., &lt;code&gt;Backend(&quot;GLOO&quot;)&lt;/code&gt; returns &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;.</source>
          <target state="translated">このクラスを直接呼び出して文字列を解析できます。たとえば、 &lt;code&gt;Backend(backend_str)&lt;/code&gt; は、 &lt;code&gt;backend_str&lt;/code&gt; が有効かどうかを確認し、有効な場合は解析された小文字の文字列を返します。また、大文字の文字列も受け入れます。たとえば、 &lt;code&gt;Backend(&quot;GLOO&quot;)&lt;/code&gt; は &lt;code&gt;&quot;gloo&quot;&lt;/code&gt; を返します。</target>
        </trans-unit>
        <trans-unit id="5ef03a595a40d9415bbad765363aad2d964d5b16" translate="yes" xml:space="preserve">
          <source>This class does not provide a &lt;code&gt;forward&lt;/code&gt; hook. Instead, you must use one of the underlying functions (e.g. &lt;code&gt;add&lt;/code&gt;).</source>
          <target state="translated">このクラスは &lt;code&gt;forward&lt;/code&gt; フックを提供しません。代わりに、基礎となる関数の1つを使用する必要があります（例： &lt;code&gt;add&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="dc115d83b6af36150f881200c064c4d238e34462" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;.</source>
          <target state="translated">このクラスは、 &lt;code&gt;interpolate()&lt;/code&gt; を優先して非推奨になりました。</target>
        </trans-unit>
        <trans-unit id="c13ac423de7a3c762fee4c3a9510054ffdd5d384" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;. It is equivalent to &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="translated">このクラスは、 &lt;code&gt;interpolate()&lt;/code&gt; を優先して非推奨になりました。それは同等です &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f63c41dd97936df60be517dad06b50e25bfe14d0" translate="yes" xml:space="preserve">
          <source>This class uses &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; in order to retrieve the gradients for specific parameters.</source>
          <target state="translated">このクラスは、特定のパラメーターのグラデーションを取得するために&lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt; &lt;code&gt;get_gradients()&lt;/code&gt; &lt;/a&gt;を使用します。</target>
        </trans-unit>
        <trans-unit id="1862d470ef8a5cd214866c60dc3b2e18ed9571b7" translate="yes" xml:space="preserve">
          <source>This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().</source>
          <target state="translated">この集合体は、グループ全体がこの関数に入るまでプロセスをブロックします。</target>
        </trans-unit>
        <trans-unit id="43ff3e72065cb78dedbfc0cb5795ef6d313a86b8" translate="yes" xml:space="preserve">
          <source>This composition also works for &lt;code&gt;nn.Module&lt;/code&gt;s as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module.</source>
          <target state="translated">この構成は &lt;code&gt;nn.Module&lt;/code&gt; でも機能し、スクリプトモジュールのメソッドから呼び出すことができるトレースを使用してサブモジュールを生成するために使用できます。</target>
        </trans-unit>
        <trans-unit id="d56b8d3c3a6cd08249ac0400c9bd4fa666010028" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given &lt;code&gt;module&lt;/code&gt; by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.</source>
          <target state="translated">このコンテナーは、バッチディメンションでチャンク化することにより、指定されたデバイス間で入力を分割することにより、指定された &lt;code&gt;module&lt;/code&gt; アプリケーションを並列化します（他のオブジェクトはデバイスごとに1回コピーされます）。フォワードパスでは、モジュールは各デバイスに複製され、各レプリカが入力の一部を処理します。後方パス中に、各レプリカからの勾配が元のモジュールに合計されます。</target>
        </trans-unit>
        <trans-unit id="3e99fa6e39b3592c4eea5075ef0e0720a6948ff1" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.</source>
          <target state="translated">このコンテナは、バッチ次元でのチャンキングによって指定されたデバイス間で入力を分割することによって、指定されたモジュールのアプリケーションを並列化します。モジュールは各マシンと各デバイス上で複製され、各複製は入力の一部を処理します。バックワードパスの間、各ノードからの勾配が平均化されます。</target>
        </trans-unit>
        <trans-unit id="c09652531ac8f4b111870bcb71b813e31279457c" translate="yes" xml:space="preserve">
          <source>This context manager is thread local; it will not affect computation in other threads.</source>
          <target state="translated">このコンテキストマネージャーはスレッドローカルで、他のスレッドでの計算には影響しません。</target>
        </trans-unit>
        <trans-unit id="26ef7031b1f75f10e86902c0f567ce05a4ea44b5" translate="yes" xml:space="preserve">
          <source>This context manager will keep track of already-joined DDP processes, and &amp;ldquo;shadow&amp;rdquo; the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes.</source>
          <target state="translated">このコンテキストマネージャーは、すでに参加しているDDPプロセスを追跡し、参加していないDDPプロセスによって作成されたものと一致するように集合的な通信操作を挿入することにより、フォワードパスとバックワードパスを「シャドウ」します。これにより、各集合呼び出しに、すでに参加しているDDPプロセスによる対応する呼び出しが確実に行われ、プロセス間で不均一な入力を使用してトレーニングするときに発生するハングやエラーが防止されます。</target>
        </trans-unit>
        <trans-unit id="09d1e555fd19e686f48ac7306fa2525b334ba962" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt;&lt;code&gt;nn.LogSoftmax()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;nn.NLLLoss()&lt;/code&gt;&lt;/a&gt; in one single class.</source>
          <target state="translated">この基準は、&lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt; &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt; &lt;code&gt;nn.NLLLoss()&lt;/code&gt; &lt;/a&gt;を1つのクラスにまとめたものです。</target>
        </trans-unit>
        <trans-unit id="925d7f6dcda18d5ae787b2ea0d4e4cf02702d590" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;log_softmax&lt;/code&gt; and &lt;code&gt;nll_loss&lt;/code&gt; in a single function.</source>
          <target state="translated">この基準は、 &lt;code&gt;log_softmax&lt;/code&gt; と &lt;code&gt;nll_loss&lt;/code&gt; を1つの関数に組み合わせたものです。</target>
        </trans-unit>
        <trans-unit id="c7ea31e3bc10ddc7b5db55c0a7c9d23739bfa2db" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; and &lt;code&gt;nn.NLLLoss()&lt;/code&gt; in one single class.</source>
          <target state="translated">この基準は、 &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; と &lt;code&gt;nn.NLLLoss()&lt;/code&gt; を1つのクラスにまとめたものです。</target>
        </trans-unit>
        <trans-unit id="4a017ebd876454e242cede3ce02b356f5f295daa" translate="yes" xml:space="preserve">
          <source>This criterion expects a &lt;code&gt;target&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; of the same size as the &lt;code&gt;input&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">この基準は、 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;Tensor&lt;/code&gt; と同じサイズの &lt;code&gt;target&lt;/code&gt; &lt;code&gt;Tensor&lt;/code&gt; を想定しています。</target>
        </trans-unit>
        <trans-unit id="c10376150f504915e5bd847798be0f3a52dc669c" translate="yes" xml:space="preserve">
          <source>This criterion expects a class index in the range</source>
          <target state="translated">この基準は、範囲内のクラス・インデックスを求めます。</target>
        </trans-unit>
        <trans-unit id="4c181079125dc2e25b97bd2d3da70b20b81a75a7" translate="yes" xml:space="preserve">
          <source>This decorator also works with RRef helpers, i.e., . &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_sync&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_sync()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_async&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_async()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.rpc.RRef.remote&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.remote()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">このデコレータは、RRefヘルパーとも連携します。&lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_sync&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.rpc_sync()&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_async&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.rpc_async()&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;#torch.distributed.rpc.RRef.remote&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.remote()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ba581390e34d5e57c67e068b92dac8e4709b3ad6" translate="yes" xml:space="preserve">
          <source>This decorator indicates that a method on an &lt;code&gt;nn.Module&lt;/code&gt; is used as an entry point into a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and should be compiled.</source>
          <target state="translated">このデコレータは、上の方法いることを示し &lt;code&gt;nn.Module&lt;/code&gt; がへのエントリポイントとして使用され&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;コンパイルされるべきです。</target>
        </trans-unit>
        <trans-unit id="174b3d4b823448bf6302b0b53449be4c6d5ad12e" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.</source>
          <target state="translated">このデコレータは、関数やメソッドを無視してPythonの関数として残すことをコンパイラに示します。</target>
        </trans-unit>
        <trans-unit id="6d10437bc80b062c46cb5ff8be0bb49322ddb59f" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function. This allows you to leave code in your model that is not yet TorchScript compatible. If called from TorchScript, ignored functions will dispatch the call to the Python interpreter. Models with ignored functions cannot be exported; use &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">このデコレータは、関数またはメソッドを無視してPython関数として残す必要があることをコンパイラに示します。これにより、TorchScriptとまだ互換性のないコードをモデルに残すことができます。TorchScriptから呼び出された場合、無視された関数はPythonインタープリターに呼び出しをディスパッチします。関数が無視されたモデルはエクスポートできません。代わりに&lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt;を使用してください。</target>
        </trans-unit>
        <trans-unit id="1508df53ef2be98f34c189e45c94ca9319ce75f2" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.</source>
          <target state="translated">このデコレータは、関数やメソッドを無視して例外を発生させることをコンパイラに指示します。</target>
        </trans-unit>
        <trans-unit id="75d6e9d0a82393986fc62b4441accfe0e14ee3a9" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.</source>
          <target state="translated">このデコレータは、関数やメソッドを無視して例外の発生に置き換えることをコンパイラに指示します。これにより、TorchScriptと互換性のないコードをモデルに残しておいても、モデルをエクスポートすることができます。</target>
        </trans-unit>
        <trans-unit id="8952c5fbe2a12cf21923d9bc22bc2c081674eb1e" translate="yes" xml:space="preserve">
          <source>This defines</source>
          <target state="translated">これは</target>
        </trans-unit>
        <trans-unit id="e40f466c8c675241a4989cdf735bce5f4ddce6bc" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?orgqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-orgqr&quot;&gt;LAPACK documentation for orgqr&lt;/a&gt; for further details.</source>
          <target state="translated">これにより、基盤となるLAPACK関数 &lt;code&gt;?orgqr&lt;/code&gt; が直接呼び出されます。詳細&lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-orgqr&quot;&gt;については、orgqrのLAPACKドキュメントを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="9fb3c5a1f24bdd83af6fd14c07d4d7ef76ec7527" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?ormqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-ormqr&quot;&gt;LAPACK documentation for ormqr&lt;/a&gt; for further details.</source>
          <target state="translated">これにより、基盤となるLAPACK関数 &lt;code&gt;?ormqr&lt;/code&gt; が直接呼び出されます。詳細&lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-ormqr&quot;&gt;については、ormqrのLAPACKドキュメントを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="d7d3fb60afba5b578d1ed91b3b2d20aba1664bb0" translate="yes" xml:space="preserve">
          <source>This error usually means that the method you are tracing uses a module&amp;rsquo;s parameters and you are passing the module&amp;rsquo;s method instead of the module instance (e.g. &lt;code&gt;my_module_instance.forward&lt;/code&gt; vs &lt;code&gt;my_module_instance&lt;/code&gt;).</source>
          <target state="translated">このエラーは通常、トレースしているメソッドがモジュールのパラメーターを使用し、モジュールインスタンスの代わりにモジュールのメソッドを渡していることを意味します（例： &lt;code&gt;my_module_instance.forward&lt;/code&gt; と &lt;code&gt;my_module_instance&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="c753cc91ef9f274f764a4ae41747b732c7fe2261" translate="yes" xml:space="preserve">
          <source>This feature is in beta, and its design and implementation may change in the future.</source>
          <target state="translated">この機能はベータ版であり、今後設計や実装が変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="b7614e828d8ac51033f6b1d436da1e7ec19a65d5" translate="yes" xml:space="preserve">
          <source>This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; object by accessing its &lt;code&gt;.data&lt;/code&gt; attribute.</source>
          <target state="translated">この関数は、少なくとも2つの次元を持つすべての入力を受け入れます。これを適用してラベルをパックし、RNNの出力をラベルとともに使用して損失を直接計算できます。Tensorは、その &lt;code&gt;.data&lt;/code&gt; 属性にアクセスすることにより、&lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; &lt;/a&gt;オブジェクトから取得できます。</target>
        </trans-unit>
        <trans-unit id="97ac98bd75da52df5620120c1ca469a7f25204b3" translate="yes" xml:space="preserve">
          <source>This function accumulates gradients in the leaves - you might need to zero &lt;code&gt;.grad&lt;/code&gt; attributes or set them to &lt;code&gt;None&lt;/code&gt; before calling it. See &lt;a href=&quot;autograd#default-grad-layouts&quot;&gt;Default gradient layouts&lt;/a&gt; for details on the memory layout of accumulated gradients.</source>
          <target state="translated">この関数は、葉にグラデーションを蓄積します。呼び出す前に、 &lt;code&gt;.grad&lt;/code&gt; 属性をゼロにするか、 &lt;code&gt;None&lt;/code&gt; に設定する必要がある場合があります。参照してください。&lt;a href=&quot;autograd#default-grad-layouts&quot;&gt;デフォルト勾配レイアウト&lt;/a&gt;累積勾配のメモリレイアウトの詳細については、を。</target>
        </trans-unit>
        <trans-unit id="5919d90354daacc619c8dc367427cc669c661c6e" translate="yes" xml:space="preserve">
          <source>This function behaves exactly like &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;, but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of &lt;a href=&quot;#torch.utils.cpp_extension.load_inline&quot;&gt;&lt;code&gt;load_inline()&lt;/code&gt;&lt;/a&gt; is identical to &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数は&lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt; &lt;code&gt;load()&lt;/code&gt; &lt;/a&gt;とまったく同じように動作しますが、ソースをファイル名ではなく文字列として受け取ります。これらの文字列はビルドディレクトリ内のファイルに保存され、その後、&lt;a href=&quot;#torch.utils.cpp_extension.load_inline&quot;&gt; &lt;code&gt;load_inline()&lt;/code&gt; の&lt;/a&gt;動作はload（）と同じになり&lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt; &lt;code&gt;load()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4742c80d8253ffcd737e46fcd5dffcfb774bfb9a" translate="yes" xml:space="preserve">
          <source>This function calculates all eigenvalues (and vectors) of &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="translated">この関数は、 &lt;code&gt;input&lt;/code&gt; すべての固有値（およびベクトル）を次のように計算します。</target>
        </trans-unit>
        <trans-unit id="f591ac1d01d2706090dd5158a48cfafbf7078fcd" translate="yes" xml:space="preserve">
          <source>This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the &lt;code&gt;ord&lt;/code&gt; parameter.</source>
          <target state="translated">この関数は、縮小次元の数と &lt;code&gt;ord&lt;/code&gt; パラメーターの値の両方に応じて、8つの異なるタイプの行列ノルムの1つ、または無限の数のベクトルノルムの1つを計算できます。</target>
        </trans-unit>
        <trans-unit id="a99e80c93a86e3189bf95a219e4fc70666128f91" translate="yes" xml:space="preserve">
          <source>This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.</source>
          <target state="translated">この関数はバージョン0.4.1でシグネチャを変更しました。以前のシグネチャで呼び出すとエラーが発生したり、正しくない結果が返ってきたりすることがあります。</target>
        </trans-unit>
        <trans-unit id="2742ba18db0e477006e0d4a879a88ee8aaa73a1c" translate="yes" xml:space="preserve">
          <source>This function checks if all &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; satisfy the condition:</source>
          <target state="translated">この関数は、すべての &lt;code&gt;input&lt;/code&gt; および &lt;code&gt;other&lt;/code&gt; 条件を満たすかどうかをチェックします。</target>
        </trans-unit>
        <trans-unit id="4ad58a0a805b7dcf7c19d64d7ab1a5b0774c3bf4" translate="yes" xml:space="preserve">
          <source>This function does exact same thing as &lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt;&lt;code&gt;torch.addmm()&lt;/code&gt;&lt;/a&gt; in the forward, except that it supports backward for sparse matrix &lt;code&gt;mat1&lt;/code&gt;. &lt;code&gt;mat1&lt;/code&gt; need to have &lt;code&gt;sparse_dim = 2&lt;/code&gt;. Note that the gradients of &lt;code&gt;mat1&lt;/code&gt; is a coalesced sparse tensor.</source>
          <target state="translated">この関数は、スパース行列 &lt;code&gt;mat1&lt;/code&gt; の後方サポートを除いて、前方の&lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt; &lt;code&gt;torch.addmm()&lt;/code&gt; &lt;/a&gt;とまったく同じことを行います。 &lt;code&gt;mat1&lt;/code&gt; は &lt;code&gt;sparse_dim = 2&lt;/code&gt; 必要があります。 &lt;code&gt;mat1&lt;/code&gt; の勾配は、合体したスパーステンソルであることに注意してください。</target>
        </trans-unit>
        <trans-unit id="b687a9d19b48f08afdcc0c1bcf3778a2f67002f3" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;.</source>
          <target state="translated">この関数は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;しません。</target>
        </trans-unit>
        <trans-unit id="6eaaf4ac25e2420bf8da7dbd7b25891750193e61" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;. For broadcasting matrix products, see &lt;a href=&quot;torch.matmul#torch.matmul&quot;&gt;&lt;code&gt;torch.matmul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;しません。マトリックス製品のブロードキャストについては、&lt;a href=&quot;torch.matmul#torch.matmul&quot;&gt; &lt;code&gt;torch.matmul()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="1273ec3288d9debd903d3f9e04111f9b0d5a8d6b" translate="yes" xml:space="preserve">
          <source>This function does not check if the factorization was successful or not if &lt;code&gt;get_infos&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; since the status of the factorization is present in the third element of the return tuple.</source>
          <target state="translated">この関数は、因数分解のステータスが戻りタプルの3番目の要素に存在するため、因数分解が成功したかどうか、または &lt;code&gt;get_infos&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; であるかどうかをチェックしません。</target>
        </trans-unit>
        <trans-unit id="68c688ded2e6ac8d7e4721168f69f94608270e24" translate="yes" xml:space="preserve">
          <source>This function does not optimize the given expression, so a different formula for the same computation may run faster or consume less memory. Projects like opt_einsum (&lt;a href=&quot;https://optimized-einsum.readthedocs.io/en/stable/&quot;&gt;https://optimized-einsum.readthedocs.io/en/stable/&lt;/a&gt;) can optimize the formula for you.</source>
          <target state="translated">この関数は指定された式を最適化しないため、同じ計算に対して異なる式を実行すると、実行速度が速くなったり、メモリの消費量が少なくなる可能性があります。opt_einsum（&lt;a href=&quot;https://optimized-einsum.readthedocs.io/en/stable/&quot;&gt;https://optimized-einsum.readthedocs.io/en/stable/&lt;/a&gt;）のようなプロジェクトは、数式を最適化できます。</target>
        </trans-unit>
        <trans-unit id="f44824ec132a2e35c26075b94e4bfe941180a2db" translate="yes" xml:space="preserve">
          <source>This function doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="translated">この関数は、ログがSoftmaxとそれ自体の間で計算されることを期待するNLLLossでは直接機能しません。代わりにlog_softmaxを使用してください（より高速で、数値特性が優れています）。</target>
        </trans-unit>
        <trans-unit id="4c40534c7583c126c2b6a9ea9f9ce426b4120e2b" translate="yes" xml:space="preserve">
          <source>This function is a front-end to the following LOBPCG algorithms selectable via &lt;code&gt;method&lt;/code&gt; argument:</source>
          <target state="translated">この関数は、 &lt;code&gt;method&lt;/code&gt; 引数を介して選択可能な次のLOBPCGアルゴリズムのフロントエンドです。</target>
        </trans-unit>
        <trans-unit id="773eb7977e3d589f5a2a946752611ffc03872d99" translate="yes" xml:space="preserve">
          <source>This function is deprecated and may be removed in a future release. It can be implemented using &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; as &lt;code&gt;alpha * torch.outer(vec1, vec2) + beta * input&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is not zero, and as &lt;code&gt;alpha * torch.outer(vec1, vec2)&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is zero.</source>
          <target state="translated">この機能は非推奨であり、将来のリリースで削除される可能性があります。これは、使用して実装することができる&lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt;のよう &lt;code&gt;alpha * torch.outer(vec1, vec2) + beta * input&lt;/code&gt; 場合 &lt;code&gt;beta&lt;/code&gt; ゼロではない、とのような &lt;code&gt;alpha * torch.outer(vec1, vec2)&lt;/code&gt; 場合 &lt;code&gt;beta&lt;/code&gt; ゼロです。</target>
        </trans-unit>
        <trans-unit id="ecdb962fda9e357b596ac207e636ee312e395193" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future PyTorch release. Use &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">この関数は非推奨であり、将来のPyTorchリリースで削除される予定です。代わりに&lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; を&lt;/a&gt;使用してください。</target>
        </trans-unit>
        <trans-unit id="32ba15aae127598880c645601e7ae7f8189c6c82" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future release because its behavior is inconsistent with Python&amp;rsquo;s range builtin. Instead, use &lt;a href=&quot;torch.arange#torch.arange&quot;&gt;&lt;code&gt;torch.arange()&lt;/code&gt;&lt;/a&gt;, which produces values in [start, end).</source>
          <target state="translated">この関数は非推奨であり、その動作がPythonの組み込みの範囲と矛盾しているため、将来のリリースで削除される予定です。代わりに、[start、end）に値を生成する&lt;a href=&quot;torch.arange#torch.arange&quot;&gt; &lt;code&gt;torch.arange()&lt;/code&gt; を&lt;/a&gt;使用してください。</target>
        </trans-unit>
        <trans-unit id="cd06f9dfe663c3caf3762160bc99450c126a39a9" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.functional.interpolate(...)&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="1632348f060a8b22bffa765800a936ba8981c455" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="e11fc8a8842739d9e68d3ea9f36d243b26e33ee2" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.functional.interpolate(..., mode='nearest')&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="409b1beff6de19c9e0fc8534a682b00b07c2667b" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.quantized.functional.interpolate(...)&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="f088ca0f34a9882f6e67d133eea7956f99a05c61" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="a1e1948ca54d505e36aa98bcd4ccf28bf3f09403" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.quantized.functional.interpolate(..., mode='nearest')&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="6111b83606e5f76d2d19a4fd63d2df2a457f3738" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique#torch.unique&quot;&gt;&lt;code&gt;torch.unique()&lt;/code&gt;&lt;/a&gt; in the sense that this function only eliminates consecutive duplicate values. This semantics is similar to &lt;code&gt;std::unique&lt;/code&gt; in C++.</source>
          <target state="translated">この関数は、連続する重複値のみを削除するという意味で&lt;a href=&quot;torch.unique#torch.unique&quot;&gt; &lt;code&gt;torch.unique()&lt;/code&gt; &lt;/a&gt;とは異なります。このセマンティクスは、C ++の &lt;code&gt;std::unique&lt;/code&gt; に似ています。</target>
        </trans-unit>
        <trans-unit id="c84bc16a342354e3160aa386d7ad206e77f3ce71" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt; in the sense that this function also eliminates non-consecutive duplicate values.</source>
          <target state="translated">この関数は、連続していない重複値も排除するという意味で、&lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt; &lt;code&gt;torch.unique_consecutive()&lt;/code&gt; &lt;/a&gt;とは異なります。</target>
        </trans-unit>
        <trans-unit id="42a1f69345a02122fc091a770868079bb8638971" translate="yes" xml:space="preserve">
          <source>This function is differentiable, so gradients will flow back from the result of this operation to &lt;code&gt;input&lt;/code&gt;. To create a tensor without an autograd relationship to &lt;code&gt;input&lt;/code&gt; see &lt;a href=&quot;../autograd#torch.Tensor.detach&quot;&gt;&lt;code&gt;detach()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数は微分可能であるため、勾配はこの操作の結果から &lt;code&gt;input&lt;/code&gt; 逆流します。 &lt;code&gt;input&lt;/code&gt; へのautograd関係のないテンソルを作成するには、&lt;a href=&quot;../autograd#torch.Tensor.detach&quot;&gt; &lt;code&gt;detach()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="0fb9aa5da82ff7de2350851193131992c19c52a8" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.cdist(input,&amp;rsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="translated">この関数は、 &lt;code&gt;scipy.spatial.distance.cdist(input,&amp;rsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="cc680c0e90287ae914b5b4470b9652c693f1647a" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.pdist(input, &amp;lsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="translated">この関数は、 &lt;code&gt;scipy.spatial.distance.pdist(input, &amp;lsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="ff6a78f486235a2c4ee2bb0612d674661defb9c5" translate="yes" xml:space="preserve">
          <source>This function is here for legacy reasons, may be removed from nn.Functional in the future.</source>
          <target state="translated">この関数はレガシーな理由でここにあり、将来的にはnn.Functionalから削除される可能性があります。</target>
        </trans-unit>
        <trans-unit id="4ffbcd1ffd822d93e92f3e882d1d47ebf27a02de" translate="yes" xml:space="preserve">
          <source>This function is implemented only for nonnegative integers</source>
          <target state="translated">この関数は非負の整数に対してのみ実装されます。</target>
        </trans-unit>
        <trans-unit id="4af64360b4b0bf60d1db0c5268a216c0039ae031" translate="yes" xml:space="preserve">
          <source>This function is more accurate than &lt;a href=&quot;torch.log#torch.log&quot;&gt;&lt;code&gt;torch.log()&lt;/code&gt;&lt;/a&gt; for small values of &lt;code&gt;input&lt;/code&gt;</source>
          <target state="translated">この関数は、 &lt;code&gt;input&lt;/code&gt; 値が小さい場合、&lt;a href=&quot;torch.log#torch.log&quot;&gt; &lt;code&gt;torch.log()&lt;/code&gt; &lt;/a&gt;よりも正確です。</target>
        </trans-unit>
        <trans-unit id="f9a118660ffcea349bb4668844a1f15351a99bbb" translate="yes" xml:space="preserve">
          <source>This function is not defined for &lt;code&gt;torch.cuda.Tensor&lt;/code&gt; yet.</source>
          <target state="translated">この関数は、 &lt;code&gt;torch.cuda.Tensor&lt;/code&gt; に対してまだ定義されていません。</target>
        </trans-unit>
        <trans-unit id="69c6cf6408f0b3c6058b2d09b95171f65a52d054" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt;&lt;code&gt;affine_grid()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="translated">この関数は、&lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;空間トランスフォーマーネットワーク&lt;/a&gt;を構築するために&lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt; &lt;code&gt;affine_grid()&lt;/code&gt; &lt;/a&gt;と組み合わせて使用​​されることがよくあります。</target>
        </trans-unit>
        <trans-unit id="3bda1093b38d267fe7d744a3f96bb51ca111b18d" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="translated">この関数は、&lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt;を構築するために&lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt; &lt;code&gt;grid_sample()&lt;/code&gt; &lt;/a&gt;と組み合わせて使用​​されることがよくあります。</target>
        </trans-unit>
        <trans-unit id="d26ea75bdd03743d020bb8fd3f0b4f05e5fca44d" translate="yes" xml:space="preserve">
          <source>This function only works with CPU tensors and should not be used in code sections that require high performance.</source>
          <target state="translated">この関数はCPUテンソルのみで動作し、高性能を必要とするコード部では使用しないでください。</target>
        </trans-unit>
        <trans-unit id="7ed8f6139c0d2640189a9321e1da51e7b538d228" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;max(dim=0)&lt;/code&gt;</source>
          <target state="translated">この関数は、 &lt;code&gt;max(dim=0)&lt;/code&gt; とは異なり、決定論的な（サブ）勾配を生成します。</target>
        </trans-unit>
        <trans-unit id="49eba71d4c9b31d15ee67772326e88d1462e9393" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;median(dim=0)&lt;/code&gt;</source>
          <target state="translated">この関数は、 &lt;code&gt;median(dim=0)&lt;/code&gt; とは異なり、決定論的な（サブ）勾配を生成します</target>
        </trans-unit>
        <trans-unit id="d9251c4cd19a437c8f31a2c44be4bdf2e2282d65" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;min(dim=0)&lt;/code&gt;</source>
          <target state="translated">この関数は、 &lt;code&gt;min(dim=0)&lt;/code&gt; とは異なり、決定論的な（サブ）勾配を生成します。</target>
        </trans-unit>
        <trans-unit id="7d3e2fb10adcbb7f259d94a424b7b3b695d7efa3" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e.</source>
          <target state="translated">この関数は,多線式を計算する方法を提供します.</target>
        </trans-unit>
        <trans-unit id="6dd319ec8d2bd7cb7695497b2e2c0c7da18093d6" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.</source>
          <target state="translated">この関数は、Einsteinの和則を用いた多線式(積の和など)を計算する方法を提供します。</target>
        </trans-unit>
        <trans-unit id="464a58780f1e7ad5a78398d14f9e91d688f64e19" translate="yes" xml:space="preserve">
          <source>This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.</source>
          <target state="translated">この関数は、メイングループ内のすべてのプロセス(すなわち、分散ジョブの一部であるすべてのプロセス)が、グループのメンバーになる予定のないプロセスであっても、この関数に入る必要があります。さらに、グループはすべてのプロセスで同じ順序で作成されなければなりません。</target>
        </trans-unit>
        <trans-unit id="a52b7a21105c565c661abb413198331f8bc8a45e" translate="yes" xml:space="preserve">
          <source>This function returns a Tensor of size &lt;code&gt;T x B x *&lt;/code&gt; or &lt;code&gt;B x T x *&lt;/code&gt; where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.</source>
          <target state="translated">この関数は、サイズ &lt;code&gt;T x B x *&lt;/code&gt; または &lt;code&gt;B x T x *&lt;/code&gt; テンソルを返します。ここで、 &lt;code&gt;T&lt;/code&gt; は最長のシーケンスの長さです。この関数は、シーケンス内のすべてのテンソルの末尾の次元とタイプが同じであることを前提としています。</target>
        </trans-unit>
        <trans-unit id="c177e0164ff9028b4584fc5df4d1e3daee9e253a" translate="yes" xml:space="preserve">
          <source>This function returns a handle with a method &lt;code&gt;handle.remove()&lt;/code&gt; that removes the hook from the module.</source>
          <target state="translated">この関数は、モジュールからフックを削除するメソッド &lt;code&gt;handle.remove()&lt;/code&gt; を持つハンドルを返します。</target>
        </trans-unit>
        <trans-unit id="a0496aaa431bc80a1684ef66937522b9d485ea0d" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the nearly optimal approximation of a singular value decomposition of a centered matrix</source>
          <target state="translated">この関数は、中心行列の特異値分解のほぼ最適な近似である名前付きタプル &lt;code&gt;(U, S, V)&lt;/code&gt; を返します。</target>
        </trans-unit>
        <trans-unit id="eda5e94a6c116342560b85fa8d7ac690f3e68684" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the singular value decomposition of a input real matrix or batches of real matrices &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="translated">この関数はnamedtuple返し &lt;code&gt;(U, S, V)&lt;/code&gt; 入力実行列または実数行列のバッチの特異値分解で &lt;code&gt;input&lt;/code&gt; その結果を</target>
        </trans-unit>
        <trans-unit id="5aeb212c2592eaa5e981d2676890741ba536e05d" translate="yes" xml:space="preserve">
          <source>This function returns eigenvalues and eigenvectors of a real symmetric matrix &lt;code&gt;input&lt;/code&gt; or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).</source>
          <target state="translated">この関数は、名前付きタプル（eigenvalues、eigenvectors）で表される、実対称行列 &lt;code&gt;input&lt;/code&gt; または実対称行列のバッチの固有値と固有ベクトルを返します。</target>
        </trans-unit>
        <trans-unit id="2f955323e5a9f295f213a25f7ecc7dec98b5caa2" translate="yes" xml:space="preserve">
          <source>This function returns the solution to the system of linear equations represented by</source>
          <target state="translated">この関数は,次式で表される一次方程式系の解を返します.</target>
        </trans-unit>
        <trans-unit id="7d7a84c0650abce4d3f9c668ace3ede1befabda4" translate="yes" xml:space="preserve">
          <source>This function&amp;rsquo;s name is a misnomer. It actually rounds the quotient towards zero instead of taking its floor. This behavior will be deprecated in a future PyTorch release.</source>
          <target state="translated">この関数の名前は誤った名称です。実際には、商を床に置くのではなく、ゼロに向かって丸めます。この動作は、将来のPyTorchリリースで非推奨になります。</target>
        </trans-unit>
        <trans-unit id="c29a81af2b62737c72764612d782901c3f2eb4d7" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;a href=&quot;torch.nn.dropout#torch.nn.Dropout&quot;&gt;&lt;code&gt;Dropout&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="translated">これは、特定のモジュールにのみ影響します。&lt;a href=&quot;torch.nn.dropout#torch.nn.Dropout&quot;&gt; &lt;code&gt;Dropout&lt;/code&gt; &lt;/a&gt;、 &lt;code&gt;BatchNorm&lt;/code&gt; などの影響を受ける場合、トレーニング/評価モードでの動作の詳細については、特定のモジュールのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="a56c6998fe86297bbe85815fb835af9e00568e26" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;code&gt;Dropout&lt;/code&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="translated">これは、特定のモジュールにのみ影響します。 &lt;code&gt;Dropout&lt;/code&gt; 、 &lt;code&gt;BatchNorm&lt;/code&gt; などの影響を受ける場合、トレーニング/評価モードでの動作の詳細については、特定のモジュールのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="4f8efe32a81509d5ad4a6e1e586b778004252c39" translate="yes" xml:space="preserve">
          <source>This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt; .</source>
          <target state="translated">これは、論文&lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;「特徴検出器の共適応を防ぐことによるニューラルネットワークの改善」で&lt;/a&gt;説明されているように、正則化とニューロンの共適応を防ぐための効果的な手法であることが証明されています。</target>
        </trans-unit>
        <trans-unit id="16cf55bdf2b28322a9b3129eb62ae4e3fc4f60f8" translate="yes" xml:space="preserve">
          <source>This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 1111. It uses direction numbers to generate these sequences, and these numbers have been adapted from &lt;a href=&quot;https://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111&quot;&gt;here&lt;/a&gt;.</source>
          <target state="translated">Sobolシーケンス用のエンジンのこの実装は、最大次元1111までのシーケンスをサンプリングできます。方向番号を使用してこれらのシーケンスを生成し、これらの番号は&lt;a href=&quot;https://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111&quot;&gt;ここ&lt;/a&gt;から適応されています。</target>
        </trans-unit>
        <trans-unit id="e5d17292f2576cd2e36da2cca0252f9780e064b5" translate="yes" xml:space="preserve">
          <source>This invariant is maintained throughout &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; class, and all functions that construct a &lt;code&gt;:class:PackedSequence&lt;/code&gt; in PyTorch (i.e., they only pass in tensors conforming to this constraint).</source>
          <target state="translated">この不変条件は、&lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; &lt;/a&gt;クラス全体、および &lt;code&gt;:class:PackedSequence&lt;/code&gt; で：class：PackedSequenceを構築するすべての関数で維持されます（つまり、この制約に準拠するテンソルのみを渡します）。</target>
        </trans-unit>
        <trans-unit id="0dcba97fb0267a928459453464ca8f56762a0535" translate="yes" xml:space="preserve">
          <source>This is TorchScript&amp;rsquo;s compilation of the code for the &lt;code&gt;forward&lt;/code&gt; method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.</source>
          <target state="translated">これは、TorchScriptによる &lt;code&gt;forward&lt;/code&gt; メソッドのコードのコンパイルです。これを使用して、TorchScript（トレースまたはスクリプト）がモデルコードを正しくキャプチャしたことを確認できます。</target>
        </trans-unit>
        <trans-unit id="78aa499d132d9b73782bab5e2924798d784eafb1" translate="yes" xml:space="preserve">
          <source>This is a &lt;strong&gt;Prototype&lt;/strong&gt; function.</source>
          <target state="translated">これは&lt;strong&gt;プロトタイプ&lt;/strong&gt;関数です。</target>
        </trans-unit>
        <trans-unit id="554d0b95763033953c158f9820e38365e62ff2d9" translate="yes" xml:space="preserve">
          <source>This is a generalized version of &lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt;&lt;code&gt;torch.hann_window()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt; &lt;code&gt;torch.hann_window()&lt;/code&gt; の&lt;/a&gt;一般化されたバージョンです。</target>
        </trans-unit>
        <trans-unit id="27da13545c38e0ad916bfb253add6c85dbfec69d" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly.</source>
          <target state="translated">LAPACKを直接呼び出すための低レベル関数です。</target>
        </trans-unit>
        <trans-unit id="baa9ff1557d00c12989b3535b16426973504c66e" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in &lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACK documentation for geqrf&lt;/a&gt; .</source>
          <target state="translated">これは、LAPACKを直接呼び出すための低レベルの関数です。この関数は、geqrfの&lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACKドキュメントで&lt;/a&gt;定義されているnamedtuple（a、tau）を返します。</target>
        </trans-unit>
        <trans-unit id="bed0909b657a7fcd560ee9c527be9120da241148" translate="yes" xml:space="preserve">
          <source>This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt;, which checks for contiguity, or &lt;a href=&quot;#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, which copies data if needed. To change the size in-place with custom strides, see &lt;a href=&quot;#torch.Tensor.set_&quot;&gt;&lt;code&gt;set_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは低レベルの方法です。ストレージは、現在のストライドを無視して、C連続として再解釈されます（ターゲットサイズが現在のサイズと等しい場合を除きます。この場合、テンソルは変更されません）。ほとんどの場合、代わりに、隣接性をチェックする&lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt;、または必要に応じてデータをコピー&lt;a href=&quot;#torch.Tensor.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt;reshape （）を使用することをお勧めします。カスタムストライドを使用してインプレースでサイズを変更するには、&lt;a href=&quot;#torch.Tensor.set_&quot;&gt; &lt;code&gt;set_()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="f3b5e88eb67e346fdbdf0577eada21e1b383ed17" translate="yes" xml:space="preserve">
          <source>This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.</source>
          <target state="translated">これは、すでに共有メモリ内にあるストレージと、プロセス間で共有するために移動する必要のない CUDA ストレージに対しては、実行しません。共有メモリ内のストレージはサイズ変更できません。</target>
        </trans-unit>
        <trans-unit id="f5b9f918ec25aa03605059dcc957950d2df96994" translate="yes" xml:space="preserve">
          <source>This is a no-op if the tensor is already of the correct type. This is equivalent to &lt;code&gt;self.type(tensor.type())&lt;/code&gt;</source>
          <target state="translated">テンソルがすでに正しいタイプである場合、これはノーオペレーションです。これは &lt;code&gt;self.type(tensor.type())&lt;/code&gt; と同等です</target>
        </trans-unit>
        <trans-unit id="be9417a8a097120c57731208ecd0b2e0d9e0a45b" translate="yes" xml:space="preserve">
          <source>This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized.</source>
          <target state="translated">これは、基礎となるストレージが既に共有メモリ内にあり、CUDAテンソルの場合には実行できません。共有メモリ内のテンソルはサイズ変更できません。</target>
        </trans-unit>
        <trans-unit id="584f2eb806dc59e0bb28f52080bdd509e138734b" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 1dとBatch Norm 1dモジュールを呼び出すシーケンシャルコンテナです。量子化の際に、これは対応するフュージョンモジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="3187ce0a4f646a0f02690921fbe6d7114d95c7e3" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 1dとReLUモジュールを呼び出すシーケンシャルコンテナです。量子化の際には、これは対応する融合モジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="3a2d1424088d04fbdd7c029c2c5a864c5f132531" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 1d、Batch Norm 1d、ReLUモジュールを呼び出すシーケンシャルコンテナです。量子化の際には、これは対応するフュージョンモジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="ee8eabed86139ec26051c16d9d19973c0fd6b09c" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 2dモジュールとBatch Norm 2dモジュールを呼び出すシーケンシャルコンテナです。量子化の際に、これは対応するフュージョンモジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="12efc861b006501b2b4700d386ad4f25b9ff0f7e" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 2dモジュールとReLUモジュールを呼び出すシーケンシャルコンテナです。量子化の際には、これは対応する融合モジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="98a705e3678fe480b570b51f1f965a2f3d037e83" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 2d、Batch Norm 2d、ReLUモジュールを呼び出すシーケンシャルコンテナです。量子化の際には、これは対応するフュージョンモジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="fd838137a8dd7616f8151e3b75e72da4794d6fa0" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist.</source>
          <target state="translated">これは、 &lt;code&gt;NaN&lt;/code&gt; 値を「無視」する&lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; の&lt;/a&gt;変形であり、 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;NaN&lt;/code&gt; 値が存在しないかのように分位数 &lt;code&gt;q&lt;/code&gt; を計算します。</target>
        </trans-unit>
        <trans-unit id="57134b77fa81442b10e02b41eaf3243756f7f5ee" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist. If all values in a reduced row are &lt;code&gt;NaN&lt;/code&gt; then the quantiles for that reduction will be &lt;code&gt;NaN&lt;/code&gt;. See the documentation for &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、 &lt;code&gt;NaN&lt;/code&gt; 値を「無視」する&lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; の&lt;/a&gt;変形であり、 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;NaN&lt;/code&gt; 値が存在しないかのように分位数 &lt;code&gt;q&lt;/code&gt; を計算します。減少列のすべての値である場合 &lt;code&gt;NaN&lt;/code&gt; 、その低減のための変位値になります &lt;code&gt;NaN&lt;/code&gt; 。&lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; の&lt;/a&gt;ドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="55c07d098e34194632b960dfc4bd298727835fac" translate="yes" xml:space="preserve">
          <source>This is always &lt;code&gt;True&lt;/code&gt; for CUDA tensors.</source>
          <target state="translated">これは常に &lt;code&gt;True&lt;/code&gt; CUDAテンソルのため。</target>
        </trans-unit>
        <trans-unit id="16c88bb46cbf5ef3f448ca4fe755467e88b5e578" translate="yes" xml:space="preserve">
          <source>This is different from &lt;a href=&quot;../tensors#torch.Tensor.repeat&quot;&gt;&lt;code&gt;torch.Tensor.repeat()&lt;/code&gt;&lt;/a&gt; but similar to &lt;code&gt;numpy.repeat&lt;/code&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;../tensors#torch.Tensor.repeat&quot;&gt; &lt;code&gt;torch.Tensor.repeat()&lt;/code&gt; &lt;/a&gt;とは異なりますが、numpy.repeatに似てい &lt;code&gt;numpy.repeat&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="db053cd2631236023d48272222af395e026941e8" translate="yes" xml:space="preserve">
          <source>This is equivalent to &lt;code&gt;self.log_pob(input).argmax(dim=1)&lt;/code&gt;, but is more efficient in some cases.</source>
          <target state="translated">これは &lt;code&gt;self.log_pob(input).argmax(dim=1)&lt;/code&gt; と同等ですが、場合によってはより効率的です。</target>
        </trans-unit>
        <trans-unit id="374b7dab14fec694b4f5430f4c9e29dd3262cee0" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_2d#torch.atleast_2d&quot;&gt;&lt;code&gt;torch.atleast_2d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、すべての1-Dテンソルが&lt;a href=&quot;torch.atleast_2d#torch.atleast_2d&quot;&gt; &lt;code&gt;torch.atleast_2d()&lt;/code&gt; &lt;/a&gt;によって再形成された後の、最初の軸に沿った連結に相当します。</target>
        </trans-unit>
        <trans-unit id="b6ef108509ff368b6991089b8608a37aca821f0d" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.</source>
          <target state="translated">これは、1-D テンソルについては第 1 軸に沿って連結し、他のすべてのテンソルについては第 2 軸に沿って連結することに相当する。</target>
        </trans-unit>
        <trans-unit id="c5b9e7c481799905ba918331b66fdb7b66a8da08" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_3d#torch.atleast_3d&quot;&gt;&lt;code&gt;torch.atleast_3d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;torch.atleast_3d#torch.atleast_3d&quot;&gt; &lt;code&gt;torch.atleast_3d()&lt;/code&gt; &lt;/a&gt;によって1-Dおよび2-Dテンソルが再形成された後の3番目の軸に沿った連結に相当します。</target>
        </trans-unit>
        <trans-unit id="c60beaf37836d9a93c155f39fdfc6ecdf807a30e" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;#torch.nn.Module.train&quot;&gt; &lt;code&gt;self.train(False)&lt;/code&gt; &lt;/a&gt;と同等です。</target>
        </trans-unit>
        <trans-unit id="d6810c532f9495d317b56fc8164ab42f154df6d1" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;torch.nn.module#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;torch.nn.module#torch.nn.Module.train&quot;&gt; &lt;code&gt;self.train(False)&lt;/code&gt; &lt;/a&gt;と同等です。</target>
        </trans-unit>
        <trans-unit id="18f227b9fbcb58581a48e62cc2b1e4b0a1736273" translate="yes" xml:space="preserve">
          <source>This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model &lt;em&gt;before&lt;/em&gt; saving it ensures that the tracer has the correct device information.</source>
          <target state="translated">トレーサーが特定のデバイスでテンソルの作成を目撃する可能性があるため、これをお勧めします。そのため、既にロードされているモデルをキャストすると、予期しない影響が生じる可能性があります。保存する&lt;em&gt;前&lt;/em&gt;にモデルをキャストすると、トレーサーに正しいデバイス情報が確実に含まれます。</target>
        </trans-unit>
        <trans-unit id="61139b0235e460eab7709588bf0db5fef3c1ed33" translate="yes" xml:space="preserve">
          <source>This is supported for &lt;a href=&quot;#module-attributes&quot;&gt;module attributes&lt;/a&gt; class attribute annotations but not for functions</source>
          <target state="translated">これは、&lt;a href=&quot;#module-attributes&quot;&gt;モジュール属性&lt;/a&gt;クラス属性アノテーションではサポートされていますが、関数ではサポートされていません</target>
        </trans-unit>
        <trans-unit id="5d93a3f447e3870288983d617bf3d5ca0babcaca" translate="yes" xml:space="preserve">
          <source>This is the default method, meaning that &lt;code&gt;init_method&lt;/code&gt; does not have to be specified (or can be &lt;code&gt;env://&lt;/code&gt;).</source>
          <target state="translated">これはデフォルトのメソッドです。つまり、 &lt;code&gt;init_method&lt;/code&gt; を指定する必要はありません（または &lt;code&gt;env://&lt;/code&gt; にすることができます）。</target>
        </trans-unit>
        <trans-unit id="b58bf6260a34528b9c54af5a74ed650426bd462d" translate="yes" xml:space="preserve">
          <source>This is the functional version of the DataParallel module.</source>
          <target state="translated">これはDataParallelモジュールの機能版です。</target>
        </trans-unit>
        <trans-unit id="9f2aa90c9ddd953d8134c3b2013f0dd5a5e04793" translate="yes" xml:space="preserve">
          <source>This is the quantized equivalent of &lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt;&lt;code&gt;ELU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt; &lt;code&gt;ELU&lt;/code&gt; &lt;/a&gt;の量子化された同等物です。</target>
        </trans-unit>
        <trans-unit id="051cad7dbbb9111a95c9efe4f04cfeb331282a9e" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="ea4fe276ddec4ebc483841a1170deaf100cb0956" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt;&lt;code&gt;BatchNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt; &lt;code&gt;BatchNorm3d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="e5ce0ce0859be0cbdcd66ae4d96e1834e80fb440" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.groupnorm#torch.nn.GroupNorm&quot;&gt;&lt;code&gt;GroupNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.groupnorm#torch.nn.GroupNorm&quot;&gt; &lt;code&gt;GroupNorm&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="4fd9da5ac2b1219bc045ac6f1d4efc7962f29b18" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt;&lt;code&gt;Hardswish&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt; &lt;code&gt;Hardswish&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="81ee4b5a559a55d776be5e5ad40fea15e08c77c3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt;&lt;code&gt;InstanceNorm1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt; &lt;code&gt;InstanceNorm1d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="306614a03d85542a417f30827501dbc097b0e4e2" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt;&lt;code&gt;InstanceNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt; &lt;code&gt;InstanceNorm2d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="5c54c49dfa0b1be230f77c6acfc8832f75ff793b" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt;&lt;code&gt;InstanceNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt; &lt;code&gt;InstanceNorm3d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="1ab22dac0481c7e9d72041caf66a8d6e6bce24a3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt;&lt;code&gt;LayerNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt; &lt;code&gt;LayerNorm&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="1c1757bb112d3facda0c6b8d39d0c0a08d49265a" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;nn.functional#torch.nn.functional.hardswish&quot;&gt;&lt;code&gt;hardswish()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;nn.functional#torch.nn.functional.hardswish&quot;&gt; &lt;code&gt;hardswish()&lt;/code&gt; の&lt;/a&gt;量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="c6493b5bad0951f0b43e4846b14cd1a429b62cc1" translate="yes" xml:space="preserve">
          <source>This is the reverse operation of the manner described in &lt;a href=&quot;#torch.Tensor.gather&quot;&gt;&lt;code&gt;gather()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;#torch.Tensor.gather&quot;&gt; &lt;code&gt;gather()&lt;/code&gt; &lt;/a&gt;で説明されている方法の逆の操作です。</target>
        </trans-unit>
        <trans-unit id="5295faab37629f799f6e6fbb1e9825a33f998b68" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;torch.max()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">これは、&lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;torch.max()&lt;/code&gt; &lt;/a&gt;によって返される2番目の値です。このメソッドの正確なセマンティクスについては、そのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="19c006dca5dfcf6b24613e9eb4ec7d4564e3ffde" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;torch.min()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">これは、&lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;torch.min()&lt;/code&gt; &lt;/a&gt;によって返される2番目の値です。このメソッドの正確なセマンティクスについては、そのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="ea4ca9f34dc42157c431ad05a09921e13d264890" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.sort#torch.sort&quot;&gt;&lt;code&gt;torch.sort()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">これは、&lt;a href=&quot;torch.sort#torch.sort&quot;&gt; &lt;code&gt;torch.sort()&lt;/code&gt; &lt;/a&gt;によって返される2番目の値です。このメソッドの正確なセマンティクスについては、そのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="f4162fd55661c8ae3715301b7e45fb9d304f83a8" translate="yes" xml:space="preserve">
          <source>This is typically passed to an optimizer.</source>
          <target state="translated">これは通常、オプティマイザに渡されます。</target>
        </trans-unit>
        <trans-unit id="8881d58b1f068203ff9ef6d05a7b8193f5aad690" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは通常、モデルパラメータと見なされるべきではないバッファを登録するために使用されます。たとえば、BatchNormの &lt;code&gt;running_mean&lt;/code&gt; はパラメーターではありませんが、モジュールの状態の一部です。デフォルトでは、バッファは永続的であり、パラメータと一緒に保存されます。この動作は、 &lt;code&gt;persistent&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定することで変更できます。永続バッファと非永続バッファの唯一の違いは、後者はこのモジュールの&lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; の&lt;/a&gt;一部ではないということです。</target>
        </trans-unit>
        <trans-unit id="bc05416a2fcfa2651cf566b1804c2fef547cfa8c" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは通常、モデルパラメータと見なされるべきではないバッファを登録するために使用されます。たとえば、BatchNormの &lt;code&gt;running_mean&lt;/code&gt; はパラメーターではありませんが、モジュールの状態の一部です。デフォルトでは、バッファは永続的であり、パラメータと一緒に保存されます。この動作は、 &lt;code&gt;persistent&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定することで変更できます。永続バッファと非永続バッファの唯一の違いは、後者はこのモジュールの&lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; の&lt;/a&gt;一部ではないということです。</target>
        </trans-unit>
        <trans-unit id="cdf0d415e18e8e03494fb14369b3744608b2dde4" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは通常、モデルパラメータと見なされるべきではないバッファを登録するために使用されます。たとえば、BatchNormの &lt;code&gt;running_mean&lt;/code&gt; はパラメーターではありませんが、モジュールの状態の一部です。デフォルトでは、バッファは永続的であり、パラメータと一緒に保存されます。この動作は、 &lt;code&gt;persistent&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定することで変更できます。永続バッファと非永続バッファの唯一の違いは、後者はこのモジュールの&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; の&lt;/a&gt;一部ではないということです。</target>
        </trans-unit>
        <trans-unit id="387b12018432b2012dd14d16de801f3f2b4ba6be" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは通常、モデルパラメータと見なされるべきではないバッファを登録するために使用されます。たとえば、BatchNormの &lt;code&gt;running_mean&lt;/code&gt; はパラメーターではありませんが、モジュールの状態の一部です。デフォルトでは、バッファは永続的であり、パラメータと一緒に保存されます。この動作は、 &lt;code&gt;persistent&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定することで変更できます。永続バッファと非永続バッファの唯一の違いは、後者はこのモジュールの&lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; の&lt;/a&gt;一部ではないということです。</target>
        </trans-unit>
        <trans-unit id="18ae6606a0b9fa5a1d8375a29d5adf1260f68aa6" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets</source>
          <target state="translated">これは,例えば自動エンコーダーの再構成の誤差を測定するために使用されます.ターゲット</target>
        </trans-unit>
        <trans-unit id="44bcfdd8636d4fef63e74c75c8c5e2e8049a6723" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets &lt;code&gt;t[i]&lt;/code&gt; should be numbers between 0 and 1.</source>
          <target state="translated">これは、たとえばオートエンコーダでの再構成のエラーを測定するために使用されます。ターゲット &lt;code&gt;t[i]&lt;/code&gt; は0から1までの数字でなければならないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="82ff1110b5f2f18a81804af99bb7a1033487ea4f" translate="yes" xml:space="preserve">
          <source>This is useful for implementing efficient sub-pixel convolution with a stride of</source>
          <target state="translated">のストライドで効率的なサブピクセル畳み込みを実装するのに便利です。</target>
        </trans-unit>
        <trans-unit id="0b3e0c96ed0785872533b5ba8bff1ae227ce271e" translate="yes" xml:space="preserve">
          <source>This layer uses statistics computed from input data in both training and evaluation modes.</source>
          <target state="translated">このレイヤは、トレーニングモードと評価モードの両方で、入力データから計算された統計量を使用します。</target>
        </trans-unit>
        <trans-unit id="e61f8043e790e86849febfe55d39af2c5aac3218" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class.</source>
          <target state="translated">この損失は、 &lt;code&gt;Sigmoid&lt;/code&gt; 層と &lt;code&gt;BCELoss&lt;/code&gt; を1つのクラスにまとめたものです。</target>
        </trans-unit>
        <trans-unit id="940b923ef28b8c58cfd52fc019408d07c1f8a33f" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class. This version is more numerically stable than using a plain &lt;code&gt;Sigmoid&lt;/code&gt; followed by a &lt;code&gt;BCELoss&lt;/code&gt; as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.</source>
          <target state="translated">この損失は、 &lt;code&gt;Sigmoid&lt;/code&gt; 層と &lt;code&gt;BCELoss&lt;/code&gt; を1つのクラスにまとめたものです。このバージョンは、単純な &lt;code&gt;Sigmoid&lt;/code&gt; 後に &lt;code&gt;BCELoss&lt;/code&gt; を使用するよりも数値的に安定しています。これは、操作を1つのレイヤーに結合することで、数値安定性のためにlog-sum-expトリックを利用するためです。</target>
        </trans-unit>
        <trans-unit id="b85bf08f0c04a7b3a3773bc9f4c6784cf3720ebd" translate="yes" xml:space="preserve">
          <source>This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version &amp;lt; 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.</source>
          <target state="translated">これにより、これらのグラフを実行するバックエンド/ランタイムによるより良い最適化（定数畳み込みなど）が可能になる場合があります。指定されていない場合（デフォルトはなし）、動作は次のように自動的に選択されます。 operator_export_typeがOperatorExportTypes.ONNXの場合、動作はこの引数をFalseに設定することと同じです。 operator_export_typeの他の値の場合、動作はこの引数をTrueに設定することと同じです。 ONNX opsetバージョン&amp;lt;9の場合、初期化子はグラフ入力の一部である必要があることに注意してください。したがって、opset_version引数が8以下に設定されている場合、この引数は無視されます。</target>
        </trans-unit>
        <trans-unit id="0b5f23050e1225367270ed2f80d8587f70aa2118" translate="yes" xml:space="preserve">
          <source>This message indicates to us that the computation differed between when we first traced it and when we traced it with the &lt;code&gt;check_inputs&lt;/code&gt;. Indeed, the loop within the body of &lt;code&gt;loop_in_traced_fn&lt;/code&gt; depends on the shape of the input &lt;code&gt;x&lt;/code&gt;, and thus when we try another &lt;code&gt;x&lt;/code&gt; with a different shape, the trace differs.</source>
          <target state="translated">このメッセージは、最初にトレースしたときと &lt;code&gt;check_inputs&lt;/code&gt; を使用してトレースしたときで計算が異なることを示しています。実際、 &lt;code&gt;loop_in_traced_fn&lt;/code&gt; の本体内のループは、入力 &lt;code&gt;x&lt;/code&gt; の形状に依存するため、異なる形状の別の &lt;code&gt;x&lt;/code&gt; を試すと、トレースが異なります。</target>
        </trans-unit>
        <trans-unit id="6b6071e9753ecc5843a463c244f78990925791d4" translate="yes" xml:space="preserve">
          <source>This method assumes that the file system supports locking using &lt;code&gt;fcntl&lt;/code&gt; - most local systems and NFS support it.</source>
          <target state="translated">この方法は、ファイルシステムが &lt;code&gt;fcntl&lt;/code&gt; を使用したロックをサポートしていることを前提としています。ほとんどのローカルシステムとNFSはそれをサポートしています。</target>
        </trans-unit>
        <trans-unit id="56fec11901471926f8e8051da013976d59b168bf" translate="yes" xml:space="preserve">
          <source>This method can only be called on a coalesced sparse tensor. See &lt;code&gt;Tensor.coalesce()&lt;/code&gt; for details.</source>
          <target state="translated">このメソッドは、合体したスパーステンソルでのみ呼び出すことができます。詳細については、 &lt;code&gt;Tensor.coalesce()&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="ab188eb8949a2274bed5b2cb6c64d4adfb8be156" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="translated">このメソッドは,複素-複素離散フーリエ変換を計算します.バッチ次元を無視して,次の式を計算します.</target>
        </trans-unit>
        <trans-unit id="9b6b6de7677cc3f29ad4b4bb71322ca8119f722c" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex inverse discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="translated">このメソッドは,複素-複素逆離散フーリエ変換を計算します.バッチ次元を無視して,次の式を計算します.</target>
        </trans-unit>
        <trans-unit id="07d469ff88f699ae509e6d44562a14e49142eb33" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-real inverse discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="translated">このメソッドは、複素数から実数への逆離散フーリエ変換を計算します。これは&lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;と数学的に同等ですが、入力と出力の形式のみが異なります。</target>
        </trans-unit>
        <trans-unit id="f0df83d3e836abc896e3d3ef88cddff7fe41858c" translate="yes" xml:space="preserve">
          <source>This method computes the real-to-complex discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="translated">このメソッドは、実数から複素数への離散フーリエ変換を計算します。これは&lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;と数学的に同等ですが、入力と出力の形式のみが異なります。</target>
        </trans-unit>
        <trans-unit id="cdf0174c78d429d9d5f3575a0eab21bb4e7cb40a" translate="yes" xml:space="preserve">
          <source>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</source>
          <target state="translated">この方法は、モデルの一部を個別に微調整したり、トレーニングしたりするためにモジュールの一部を凍結するのに便利です(GANトレーニングなど)。</target>
        </trans-unit>
        <trans-unit id="ca8069362a7e2b0181c1b2a91e97eec6d51986a6" translate="yes" xml:space="preserve">
          <source>This method is implemented using the Singular Value Decomposition.</source>
          <target state="translated">このメソッドは、特異値分解を用いて実装されています。</target>
        </trans-unit>
        <trans-unit id="a51d9e13859426129477705a920dac6fd0c48f90" translate="yes" xml:space="preserve">
          <source>This method modifies the module in-place.</source>
          <target state="translated">このメソッドは、その場でモジュールを変更します。</target>
        </trans-unit>
        <trans-unit id="44d70c89b433cfca3df93bd2dca81609164685a8" translate="yes" xml:space="preserve">
          <source>This method sets the parameters&amp;rsquo; &lt;code&gt;requires_grad&lt;/code&gt; attributes in-place.</source>
          <target state="translated">このメソッドは、パラメーターの &lt;code&gt;requires_grad&lt;/code&gt; 属性をインプレースで設定します。</target>
        </trans-unit>
        <trans-unit id="ba184c175718a9033afc302007ca57b3b2566fe2" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D complex-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with last dimension of size 2, representing the real and imaginary components of complex numbers, and should have at least &lt;code&gt;signal_ndim + 1&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="translated">このメソッドは、 &lt;code&gt;signal_ndim&lt;/code&gt; で示される1D、2D、および3Dの複合体から複合体への変換をサポートします。 &lt;code&gt;input&lt;/code&gt; は、複素数の実数成分と虚数成分を表すサイズ2の最後の次元のテンソルである必要があり、少なくとも &lt;code&gt;signal_ndim + 1&lt;/code&gt; 次元と、オプションで任意の数の先行バッチ次元が必要です。 &lt;code&gt;normalized&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、これは結果をで除算することによって結果を正規化します。</target>
        </trans-unit>
        <trans-unit id="4d0394dd315af559db4d9ad47ffb52f54e5c2212" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D real-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with at least &lt;code&gt;signal_ndim&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="translated">このメソッドは、 &lt;code&gt;signal_ndim&lt;/code&gt; で示される1D、2D、および3Dの実数から複素数への変換をサポートします。 &lt;code&gt;input&lt;/code&gt; は、少なくとも &lt;code&gt;signal_ndim&lt;/code&gt; 次元を持ち、オプションで任意の数の先行バッチ次元を持つテンソルである必要があります。 &lt;code&gt;normalized&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、これは結果をで除算することによって結果を正規化します。</target>
        </trans-unit>
        <trans-unit id="75583911adf2f93140c081540537180003c155f0" translate="yes" xml:space="preserve">
          <source>This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">このメソッドは常にファイルを作成し、プログラムの最後にファイルをクリーンアップして削除するために最善を尽くします。つまり、file initメソッドを使用した各初期化では、初期化を成功させるために、まったく新しい空のファイルが必要になります。以前の初期化で使用されたものと同じファイル（たまたまクリーンアップされない）が再度使用される場合、これは予期しない動作であり、デッドロックや障害を引き起こす可能性があります。したがって、この方法ではファイルのクリーンアップに最善を尽くしますが、自動削除が失敗した場合は、トレーニングの最後にファイルが削除され、同じファイルが削除されないようにする必要があります。次回は再利用。これは、&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt;を呼び出す場合に特に重要です。同じファイル名で複数回。つまり、ファイルが削除/クリーンアップされておらず、そのファイル&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt;再度呼び出すと、失敗が予想されます。ここでの経験則は、&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt;が呼び出されるたびに、ファイルが存在しないか空であることを確認することです。</target>
        </trans-unit>
        <trans-unit id="36d7188a05dab83ac9defa225d250cd47190ba2f" translate="yes" xml:space="preserve">
          <source>This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:</source>
          <target state="translated">このメソッドは環境変数から設定を読み込み、情報の取得方法を完全にカスタマイズすることができます。設定する変数は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="57da4e0220bd8e9709ab47673f34882d1449aae9" translate="yes" xml:space="preserve">
          <source>This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX. Exported falls through and exports the operator as is, as custom op. Exporting custom operators enables users to register and implement the operator as part of their runtime backend.</source>
          <target state="translated">このモードは、ONNXに登録されておらずサポートされていない演算子(ATenまたは非ATen)をエクスポートするために使用できます。Exportedは、オペレータをそのまま、カスタムオペレータとしてエクスポートします。 カスタムオペレータをエクスポートすることで、ユーザは、ランタイムバックエンドの一部としてオペレータを登録して実装することができます。</target>
        </trans-unit>
        <trans-unit id="da76922486c33377aa2791af6603164e435bb6ab" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.</source>
          <target state="translated">このモードは、すべてのオペレータをATen opsとしてエクスポートし、ONNXへの変換を避けるために使用されます。</target>
        </trans-unit>
        <trans-unit id="d9d848a6942846852bedaf5aea8e5e5bd3995b73" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as regular ONNX operators. This is the default &lt;code&gt;operator_export_type&lt;/code&gt; mode.</source>
          <target state="translated">このモードは、すべてのオペレーターを通常のONNXオペレーターとしてエクスポートするために使用されます。これはデフォルトの &lt;code&gt;operator_export_type&lt;/code&gt; モードです。</target>
        </trans-unit>
        <trans-unit id="15b7320d744a4575a9e6699705096be13d718c1b" translate="yes" xml:space="preserve">
          <source>This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose &lt;code&gt;torch.memory_format&lt;/code&gt; is &lt;code&gt;torch.contiguous_format&lt;/code&gt; and others whose format is &lt;code&gt;torch.channels_last&lt;/code&gt;. However, corresponding parameters in different processes must have the same strides.</source>
          <target state="translated">このモジュールでは、行メジャーが連続していないストライドを持つパラメーターを使用できます。たとえば、モデルには、 &lt;code&gt;torch.memory_format&lt;/code&gt; が &lt;code&gt;torch.contiguous_format&lt;/code&gt; であるパラメーターと、フォーマットが &lt;code&gt;torch.channels_last&lt;/code&gt; であるパラメーターが含まれている場合があります。ただし、異なるプロセスの対応するパラメータは、同じストライドを持っている必要があります。</target>
        </trans-unit>
        <trans-unit id="db9962266fa6ecf5ecb4337d45f21434962d46f1" translate="yes" xml:space="preserve">
          <source>This module also contains any parameters that the original module had as well.</source>
          <target state="translated">このモジュールには、元のモジュールが持っていたパラメータも含まれています。</target>
        </trans-unit>
        <trans-unit id="5790600450f4acb47de56be8f01d6569e2e765da" translate="yes" xml:space="preserve">
          <source>This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of &lt;code&gt;fp16&lt;/code&gt; and &lt;code&gt;fp32&lt;/code&gt;, the gradient reduction on these mixed types of parameters will just work fine.</source>
          <target state="translated">このモジュールは、混合精度の分散トレーニングもサポートします。これは、モデルが &lt;code&gt;fp16&lt;/code&gt; と &lt;code&gt;fp32&lt;/code&gt; の混合タイプなど、さまざまなタイプのパラメーターを持つことができることを意味します。これらの混合タイプのパラメーターの勾配の削減は問題なく機能します。</target>
        </trans-unit>
        <trans-unit id="aaad9c62985785aa5e132959be1bf299d5f15d45" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.</source>
          <target state="translated">このモジュールは、モデルが作成された時点で、すべてのパラメータがモデルに登録されていることを前提としています。後でパラメータを追加したり削除したりしてはいけません。バッファについても同様です。</target>
        </trans-unit>
        <trans-unit id="56c78b422ae4f13f13cfea0f3331ef6b5469addb" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient &lt;code&gt;allreduce&lt;/code&gt; following the reverse order of the registered parameters of the model. In other words, it is users&amp;rsquo; responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.</source>
          <target state="translated">このモジュールは、すべてのパラメーターが各分散プロセスのモデルに登録されていることを前提としています。モジュール自体は、モデルの登録されたパラメーターの逆の順序に従って勾配 &lt;code&gt;allreduce&lt;/code&gt; を実行します。言い換えると、各分散プロセスがまったく同じモデルを持ち、したがってまったく同じパラメーター登録順序を持っていることを確認するのはユーザーの責任です。</target>
        </trans-unit>
        <trans-unit id="a2fc61472d65b92ddda57c12782f0c49ee43ce07" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">このモジュールは、入力に対する Conv1d の勾配と見ることができます。これは,フラクショナルストライド畳み込みやデコンボリューションとしても知られています (実際のデコンボリューション操作ではありませんが)。</target>
        </trans-unit>
        <trans-unit id="51bd62541eadcc8d3cc881058d8a71b8bed71b7f" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">このモジュールは、入力に対するConv2dの勾配と見ることができます。これは,フラクショナルストライド畳み込みやデコンボリューションとしても知られています (実際のデコンボリューション操作ではありませんが)。</target>
        </trans-unit>
        <trans-unit id="03c2d1d66e7773d2c26345cff96e3404e927c5ec" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">このモジュールは、入力に対するConv3dの勾配と見ることができます。これは,分数的な畳み込みやデコンボリューションとしても知られています(実際のデコンボリューション操作ではありませんが).</target>
        </trans-unit>
        <trans-unit id="8a37a41f99ccf18be289e494dc942de371c38958" translate="yes" xml:space="preserve">
          <source>This module currently does not support custom distributed collective operations in the forward pass, such as &lt;code&gt;SyncBatchNorm&lt;/code&gt; or other custom defined collectives in the model&amp;rsquo;s forward pass.</source>
          <target state="translated">このモジュールは現在、 &lt;code&gt;SyncBatchNorm&lt;/code&gt; やモデルのフォワードパス内の他のカスタム定義されたコレクティブなど、フォワードパス内のカスタム分散コレクティブ操作をサポートしていません。</target>
        </trans-unit>
        <trans-unit id="e4e13a3367c83e826e07b89f6231cff61b5213ab" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use &lt;code&gt;LogSoftmax&lt;/code&gt; instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="translated">このモジュールは、ログがSoftmaxとそれ自体の間で計算されることを想定しているNLLLossとは直接連携しません。代わりに &lt;code&gt;LogSoftmax&lt;/code&gt; を使用してください（より高速で、数値特性も優れています）。</target>
        </trans-unit>
        <trans-unit id="b724d7ccdd2d4a2fad1d30f042a783b90f3e1b70" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work with &lt;a href=&quot;../autograd#torch.autograd.grad&quot;&gt;&lt;code&gt;torch.autograd.grad()&lt;/code&gt;&lt;/a&gt; (i.e. it will only work if gradients are to be accumulated in &lt;code&gt;.grad&lt;/code&gt; attributes of parameters).</source>
          <target state="translated">このモジュールは&lt;a href=&quot;../autograd#torch.autograd.grad&quot;&gt; &lt;code&gt;torch.autograd.grad()&lt;/code&gt; &lt;/a&gt;では機能しません（つまり、パラメーターの &lt;code&gt;.grad&lt;/code&gt; 属性にグラデーションが累積される場合にのみ機能します）。</target>
        </trans-unit>
        <trans-unit id="b31e8020fcc76734bdb0706e9d94b6fab41f09de" translate="yes" xml:space="preserve">
          <source>This module implements the combined (fused) modules conv + relu which can be then quantized.</source>
          <target state="translated">このモジュールは,量子化されたモジュール conv+relu を組み合わせた(融合した)モジュールを実装しています.</target>
        </trans-unit>
        <trans-unit id="a5f06261e4042915626c1433e6142f8f0b72218a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized implementations of fused operations like conv + relu.</source>
          <target state="translated">このモジュールは,conv+relu のような融合演算の量子化実装を実装しています.</target>
        </trans-unit>
        <trans-unit id="fe84855e7e341d93fbb671f10f878fe740d68b6a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized versions of the nn layers such as ~`torch.nn.Conv2d` and &lt;code&gt;torch.nn.ReLU&lt;/code&gt;.</source>
          <target state="translated">このモジュールは、〜`torch.nn.Conv2d`や &lt;code&gt;torch.nn.ReLU&lt;/code&gt; などのnnレイヤーの量子化バージョンを実装します。</target>
        </trans-unit>
        <trans-unit id="1cc16ec4495bf63da75c9357c640897793fcbd84" translate="yes" xml:space="preserve">
          <source>This module implements the versions of those fused operations needed for quantization aware training.</source>
          <target state="translated">このモジュールは、量子化を意識したトレーニングに必要な融合操作のバージョンを実装しています。</target>
        </trans-unit>
        <trans-unit id="cd564c5f343a0b5d3e2281d9491b97ab89a390d6" translate="yes" xml:space="preserve">
          <source>This module implements versions of the key nn modules &lt;strong&gt;Conv2d()&lt;/strong&gt; and &lt;strong&gt;Linear()&lt;/strong&gt; which run in FP32 but with rounding applied to simulate the effect of INT8 quantization.</source>
          <target state="translated">このモジュールは、FP32で実行されるキーnnモジュール&lt;strong&gt;Conv2d（）&lt;/strong&gt;および&lt;strong&gt;Linear（）の&lt;/strong&gt;バージョンを実装しますが、INT8量子化の効果をシミュレートするために丸めが適用されます。</target>
        </trans-unit>
        <trans-unit id="2d28957208d801ad4a8275f10517753872c57b97" translate="yes" xml:space="preserve">
          <source>This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.</source>
          <target state="translated">このモジュールは、インデックスを使用して単語の埋め込みを取得するためによく使用されます。このモジュールへの入力はインデックスと埋め込み行列のリストで、出力は対応する単語の埋め込みです。</target>
        </trans-unit>
        <trans-unit id="a56a23f8f5587064fad32f9351698184c9934864" translate="yes" xml:space="preserve">
          <source>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</source>
          <target state="translated">このモジュールは、単語の埋め込みを格納し、インデックスを使用してそれらを検索するためによく使用されます。このモジュールへの入力はインデックスのリストで、出力は対応する単語の埋め込みです。</target>
        </trans-unit>
        <trans-unit id="8c517186ab821b45b2c9090ee2862c5050c7fd67" translate="yes" xml:space="preserve">
          <source>This module provides an RPC-based distributed autograd framework that can be used for applications such as model parallel training. In short, applications may send and receive gradient recording tensors over RPC. In the forward pass, we record when gradient recording tensors are sent over RPC and during the backward pass we use this information to perform a distributed backward pass using RPC. For more details see &lt;a href=&quot;rpc/distributed_autograd#distributed-autograd-design&quot;&gt;Distributed Autograd Design&lt;/a&gt;.</source>
          <target state="translated">このモジュールは、モデル並列トレーニングなどのアプリケーションに使用できるRPCベースの分散autogradフレームワークを提供します。つまり、アプリケーションはRPCを介して勾配記録テンソルを送受信できます。フォワードパスでは、勾配記録テンソルがRPCを介して送信されたときに記録し、バックワードパスでは、この情報を使用してRPCを使用して分散バックワードパスを実行します。詳細については、&lt;a href=&quot;rpc/distributed_autograd#distributed-autograd-design&quot;&gt;Distributed AutogradDesignを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="87af39490036eebaa3381d1990a65c31f6408fae" translate="yes" xml:space="preserve">
          <source>This module returns a &lt;code&gt;NamedTuple&lt;/code&gt; with &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;loss&lt;/code&gt; fields. See further documentation for details.</source>
          <target state="translated">このモジュールは、 &lt;code&gt;output&lt;/code&gt; フィールドと &lt;code&gt;loss&lt;/code&gt; フィールドを持つ &lt;code&gt;NamedTuple&lt;/code&gt; を返します。詳細については、詳細なドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="59e9905faed8adfff4eeb0ecdbe57c1cac83f3a4" translate="yes" xml:space="preserve">
          <source>This module supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="translated">このモジュールは&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32を&lt;/a&gt;サポートします。</target>
        </trans-unit>
        <trans-unit id="c9c9e1a196eef3e1f967008237f4372d9edc34d7" translate="yes" xml:space="preserve">
          <source>This module works only with the multi-process, single-device usage of &lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt;, which means that a single process works on a single GPU.</source>
          <target state="translated">このモジュールは、&lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt; &lt;/a&gt;のマルチプロセス、シングルデバイスの使用でのみ機能します。つまり、単一のプロセスが単一のGPUで機能します。</target>
        </trans-unit>
        <trans-unit id="31368b5a919564a12c569127df416d6acb1973be" translate="yes" xml:space="preserve">
          <source>This op should be disambiguated with &lt;a href=&quot;torch.logsumexp#torch.logsumexp&quot;&gt;&lt;code&gt;torch.logsumexp()&lt;/code&gt;&lt;/a&gt; which performs a reduction on a single tensor.</source>
          <target state="translated">この操作は、単一のテンソルでリダクションを実行する&lt;a href=&quot;torch.logsumexp#torch.logsumexp&quot;&gt; &lt;code&gt;torch.logsumexp()&lt;/code&gt; &lt;/a&gt;で明確にする必要があります。</target>
        </trans-unit>
        <trans-unit id="3256e874629b9e3bf7609aa4774907058a898427" translate="yes" xml:space="preserve">
          <source>This operation is not differentiable.</source>
          <target state="translated">この操作は微分できません。</target>
        </trans-unit>
        <trans-unit id="f253d72c955897e96cbe7e655ade3b1e375b9508" translate="yes" xml:space="preserve">
          <source>This operation is useful for explicit broadcasting by names (see examples).</source>
          <target state="translated">この操作は、名前による明示的なブロードキャストに便利です(例を参照してください)。</target>
        </trans-unit>
        <trans-unit id="be6c030e3433204ea07b32a8e6ce714fff330e3a" translate="yes" xml:space="preserve">
          <source>This operator supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="translated">この演算子は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32を&lt;/a&gt;サポートします。</target>
        </trans-unit>
        <trans-unit id="836145aa5caebff4eaffcf9dc94e2f473ff1df15" translate="yes" xml:space="preserve">
          <source>This package provides a &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects. Currently, the &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type is primarily used by the &lt;a href=&quot;rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt;.</source>
          <target state="translated">このパッケージは、非同期実行をカプセル化する&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;タイプと、&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;オブジェクトの操作を簡素化する一連のユーティリティ関数を提供します。現在、&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;タイプは、主に&lt;a href=&quot;rpc#distributed-rpc-framework&quot;&gt;Distributed RPCFrameworkで&lt;/a&gt;使用されています。</target>
        </trans-unit>
        <trans-unit id="510d5659f81fbb2c92add3848324ac3e769287d8" translate="yes" xml:space="preserve">
          <source>This requires &lt;code&gt;scipy&lt;/code&gt; to be installed</source>
          <target state="translated">これには &lt;code&gt;scipy&lt;/code&gt; をインストールする必要があります</target>
        </trans-unit>
        <trans-unit id="7d1f42ea5265cdd17159cef860e2867af1a5fed2" translate="yes" xml:space="preserve">
          <source>This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2.</source>
          <target state="translated">このセクションでは、PyTorch 1.2 での TorchScript の変更点を説明します。TorchScript に慣れていない方は、このセクションを読み飛ばしても構いません。PyTorch 1.2 での TorchScript API の主な変更点は 2 つあります。</target>
        </trans-unit>
        <trans-unit id="792e85a6083fd7bff7900d359e6ad50e4d63f9a7" translate="yes" xml:space="preserve">
          <source>This subset is restricted:</source>
          <target state="translated">このサブセットは制限されています。</target>
        </trans-unit>
        <trans-unit id="f610c707a33d251ea07e96805fc742b6f94fb773" translate="yes" xml:space="preserve">
          <source>This will call &lt;a href=&quot;optim#torch.optim.Optimizer.step&quot;&gt;&lt;code&gt;torch.optim.Optimizer.step()&lt;/code&gt;&lt;/a&gt; on each worker containing parameters to be optimized, and will block until all workers return. The provided &lt;code&gt;context_id&lt;/code&gt; will be used to retrieve the corresponding &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;context&lt;/code&gt;&lt;/a&gt; that contains the gradients that should be applied to the parameters.</source>
          <target state="translated">これにより、最適化するパラメーターを含む各ワーカーで&lt;a href=&quot;optim#torch.optim.Optimizer.step&quot;&gt; &lt;code&gt;torch.optim.Optimizer.step()&lt;/code&gt; &lt;/a&gt;が呼び出され、すべてのワーカーが戻るまでブロックされます。指定された &lt;code&gt;context_id&lt;/code&gt; は、パラメーターに適用する必要のあるグラデーションを含む対応する&lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt; &lt;code&gt;context&lt;/code&gt; &lt;/a&gt;を取得するために使用されます。</target>
        </trans-unit>
        <trans-unit id="c51f7b72279e26fd7cf66d9d61e7f7204bf3b26a" translate="yes" xml:space="preserve">
          <source>Threshold</source>
          <target state="translated">Threshold</target>
        </trans-unit>
        <trans-unit id="4c8543e85c70d9042806658de48a8eae05e8c630" translate="yes" xml:space="preserve">
          <source>Threshold is defined as:</source>
          <target state="translated">しきい値は次のように定義されています。</target>
        </trans-unit>
        <trans-unit id="a2ed3cfe5a92d0e9667b7a33a4666b01b859c7a8" translate="yes" xml:space="preserve">
          <source>Thresholds each element of the input Tensor.</source>
          <target state="translated">入力テンソルの各要素にしきい値を設定します。</target>
        </trans-unit>
        <trans-unit id="880f2f41d4783f36e126ec98db6135c252954795" translate="yes" xml:space="preserve">
          <source>To achieve this, developers need to touch the source code of PyTorch. Please follow the &lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot;&gt;instructions&lt;/a&gt; for installing PyTorch from source. If the wanted operator is standardized in ONNX, it should be easy to add support for exporting such operator (adding a symbolic function for the operator). To confirm whether the operator is standardized or not, please check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;.</source>
          <target state="translated">これを実現するには、開発者はPyTorchのソースコードに触れる必要があります。ソースからPyTorchをインストールするための&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot;&gt;指示&lt;/a&gt;に従ってください。必要な演算子がONNXで標準化されている場合、そのような演算子のエクスポートのサポートを簡単に追加できます（演算子のシンボリック関数を追加します）。オペレーターが標準化されているかどうかを確認するには、&lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNXオペレーターリスト&lt;/a&gt;を確認してください。</target>
        </trans-unit>
        <trans-unit id="b5aa3a35df66cb08f05248ae2964bf5546e98afe" translate="yes" xml:space="preserve">
          <source>To align a tensor to a specific order, use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">テンソルを特定の順序に揃えるには、&lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; を&lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="42a1cb8b698578c9fe48bbb02d735034e28323f6" translate="yes" xml:space="preserve">
          <source>To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please avoid use of &lt;code&gt;torch.Tensor.item()&lt;/code&gt;. Torch supports implicit cast of single-element tensors to numbers. E.g.:</source>
          <target state="translated">ONNXモデルの一部として可変スカラーテンソルを固定値定数としてエクスポートしないようにするには、 &lt;code&gt;torch.Tensor.item()&lt;/code&gt; の使用を避けてください。Torchは、単一要素テンソルの数値への暗黙的なキャストをサポートします。例えば：</target>
        </trans-unit>
        <trans-unit id="e33cac23c601d0b5ef35b2617bd1e0b84d5ac6c7" translate="yes" xml:space="preserve">
          <source>To be able to save a module, it must not make any calls to native Python functions. This means that all submodules must be subclasses of &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; as well.</source>
          <target state="translated">モジュールを保存できるようにするには、ネイティブPython関数を呼び出さないでください。これは、すべてのサブモジュールが&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; の&lt;/a&gt;サブクラスでもある必要があることを意味します。</target>
        </trans-unit>
        <trans-unit id="517547cd8d7cdf6793257301cea557fd580f4509" translate="yes" xml:space="preserve">
          <source>To change an existing tensor&amp;rsquo;s &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; and/or &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, consider using &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt; method on the tensor.</source>
          <target state="translated">既存のテンソルの変更するには&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;および/または&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; を&lt;/a&gt;、使用することを検討してください&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt;テンソルの方法。</target>
        </trans-unit>
        <trans-unit id="cccb194237ee7a6be8a8fca8580f1ea27dde7026" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; (and recursively compile anything it calls), add the &lt;a href=&quot;../jit#torch.jit.export&quot;&gt;&lt;code&gt;@torch.jit.export&lt;/code&gt;&lt;/a&gt; decorator to the method. To opt out of compilation use &lt;a href=&quot;torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;forward&lt;/code&gt; 以外のメソッドをコンパイルする（そしてそれが呼び出すものを再帰的にコンパイルする）には、&lt;a href=&quot;../jit#torch.jit.export&quot;&gt; &lt;code&gt;@torch.jit.export&lt;/code&gt; &lt;/a&gt;デコレータをメソッドに追加します。コンパイルをオプトアウトするには、&lt;a href=&quot;torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="13132861c718e28cce4e074280b9a030df11d21a" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; that is not called from &lt;code&gt;forward&lt;/code&gt;, add &lt;code&gt;@torch.jit.export&lt;/code&gt;.</source>
          <target state="translated">以外の方法でコンパイルするには &lt;code&gt;forward&lt;/code&gt; から呼び出されていない &lt;code&gt;forward&lt;/code&gt; 、追加 &lt;code&gt;@torch.jit.export&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1f8e9d3b1e59225987eeef2681c83b1bcdde57d0" translate="yes" xml:space="preserve">
          <source>To compile the sources, the default system compiler (&lt;code&gt;c++&lt;/code&gt;) is used, which can be overridden by setting the &lt;code&gt;CXX&lt;/code&gt; environment variable. To pass additional arguments to the compilation process, &lt;code&gt;extra_cflags&lt;/code&gt; or &lt;code&gt;extra_ldflags&lt;/code&gt; can be provided. For example, to compile your extension with optimizations, pass &lt;code&gt;extra_cflags=['-O3']&lt;/code&gt;. You can also use &lt;code&gt;extra_cflags&lt;/code&gt; to pass further include directories.</source>
          <target state="translated">ソースをコンパイルするには、デフォルトのシステムコンパイラ（ &lt;code&gt;c++&lt;/code&gt; ）を使用します。これは、 &lt;code&gt;CXX&lt;/code&gt; 環境変数を設定することで上書きできます。コンパイルプロセスに追加の引数を渡すために、 &lt;code&gt;extra_cflags&lt;/code&gt; または &lt;code&gt;extra_ldflags&lt;/code&gt; を指定できます。たとえば、最適化を使用して拡張機能をコンパイルするには、 &lt;code&gt;extra_cflags=['-O3']&lt;/code&gt; を渡します。 &lt;code&gt;extra_cflags&lt;/code&gt; を使用して、さらにインクルードディレクトリを渡すこともできます。</target>
        </trans-unit>
        <trans-unit id="e66cd2fec20ece4ce742fe906879dd31b4bc3e7a" translate="yes" xml:space="preserve">
          <source>To compute log-probabilities for all classes, the &lt;code&gt;log_prob&lt;/code&gt; method can be used.</source>
          <target state="translated">すべてのクラスの対数確率を計算するには、 &lt;code&gt;log_prob&lt;/code&gt; メソッドを使用できます。</target>
        </trans-unit>
        <trans-unit id="bcee88acc8b939fa9c28961779b7b64da43214ae" translate="yes" xml:space="preserve">
          <source>To create a tensor with pre-existing data, use &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">既存のデータを使用してテンソルを作成するには、&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; を&lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="427cce7803261872f64a384400396e0d1af0bda9" translate="yes" xml:space="preserve">
          <source>To create a tensor with similar type but different size as another tensor, use &lt;code&gt;tensor.new_*&lt;/code&gt; creation ops.</source>
          <target state="translated">別のテンソルと同様のタイプでサイズが異なるテンソルを作成するには、 &lt;code&gt;tensor.new_*&lt;/code&gt; 作成操作を使用します。</target>
        </trans-unit>
        <trans-unit id="ed5a59600ac4cfd9c95f09892fb740465f6d0959" translate="yes" xml:space="preserve">
          <source>To create a tensor with specific size, use &lt;code&gt;torch.*&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="translated">特定のサイズ、用途とテンソル作成するには &lt;code&gt;torch.*&lt;/code&gt; テンソル作成OPS（参照&lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;創造オプスを&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="a84b4146d0aefbfce9733ab7943da8a8972fc9bd" translate="yes" xml:space="preserve">
          <source>To create a tensor with the same size (and similar types) as another tensor, use &lt;code&gt;torch.*_like&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="translated">別のテンソル、使用のものと同じ大きさ（と似たタイプ）とのテンソルを作成するには &lt;code&gt;torch.*_like&lt;/code&gt; テンソル作成OPS（参照&lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;創造オプスを&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="e323c9fb04b24bbe21e136c6130c8b1036dfb2e1" translate="yes" xml:space="preserve">
          <source>To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (&amp;ldquo;CPU total time is much greater than CUDA total time&amp;rdquo;). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.</source>
          <target state="translated">どの（CPUのみモードまたはCUDAモード）autogradプロファイラー出力を確認するかを決定するには、最初にスクリプトがCPUバウンドであるかどうかを確認する必要があります（「CPU合計時間はCUDA合計時間よりはるかに長い」）。CPUにバインドされている場合は、CPUモードのautogradプロファイラーの結果を確認すると役立ちます。一方、スクリプトがほとんどの時間をGPUでの実行に費やしている場合は、CUDAモードのautogradプロファイラーの出力で責任のあるCUDAオペレーターを探し始めるのが理にかなっています。</target>
        </trans-unit>
        <trans-unit id="1d6487001340d96dfbd24e0dfe67d91d448bd378" translate="yes" xml:space="preserve">
          <source>To enable &lt;code&gt;backend == Backend.MPI&lt;/code&gt;, PyTorch needs to be built from source on a system that supports MPI.</source>
          <target state="translated">&lt;code&gt;backend == Backend.MPI&lt;/code&gt; を有効にするには、MPIをサポートするシステム上のソースからPyTorchをビルドする必要があります。</target>
        </trans-unit>
        <trans-unit id="928a8a1dba70cf38d7b64d41e9f2de79497ca1b2" translate="yes" xml:space="preserve">
          <source>To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a &lt;code&gt;Future&lt;/code&gt; object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with &lt;code&gt;@staticmethod&lt;/code&gt; or &lt;code&gt;@classmethod&lt;/code&gt;, &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt;.</source>
          <target state="translated">非同期実行を有効にするには、アプリケーションはこのデコレータによって返された関数オブジェクトをRPCAPIに渡す必要があります。RPCがこのデコレータによってインストールされた属性を検出した場合、RPCはこの関数が &lt;code&gt;Future&lt;/code&gt; オブジェクトを返し、それに応じてそれを処理することを認識しています。ただし、これは、関数を定義するときにこのデコレータが最も外側にある必要があるという意味ではありません。たとえば、 &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; &lt;code&gt;@staticmethod&lt;/code&gt; または &lt;code&gt;@classmethod&lt;/code&gt; と組み合わせる場合、ターゲット関数を静的関数またはクラス関数として認識できるようにするには、@ rpc.functions.async_executionを内部デコレータにする必要があります。静的メソッドまたはクラスメソッドは、アクセスされたときに &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; によってインストールされた属性を保持するため、このターゲット関数は引き続き非同期で実行できます。。</target>
        </trans-unit>
        <trans-unit id="16234789078f98ba40aad60005ffa95f2e9e8ac6" translate="yes" xml:space="preserve">
          <source>To ensure that the correct number of threads is used, set_num_threads must be called before running eager, JIT or autograd code.</source>
          <target state="translated">正しいスレッド数が使用されるようにするには、eager、JIT、またはautogradコードを実行する前にset_num_threadsを呼び出す必要があります。</target>
        </trans-unit>
        <trans-unit id="2cd4366390e742467d09cd376578959df8585e90" translate="yes" xml:space="preserve">
          <source>To export a raw ir.</source>
          <target state="translated">生のIRをエクスポートするには</target>
        </trans-unit>
        <trans-unit id="13a45cf8b5a671e88848c78052e09b8b9f6c109f" translate="yes" xml:space="preserve">
          <source>To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly. In the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.</source>
          <target state="translated">ONNXでサポートされていないATenオペレータにフォールバックすること。サポートされている演算子は、定期的にONNXにエクスポートされます。次の例では、aten::triuはONNXではサポートされていません。エクスポータはこの演算子にフォールバックする。</target>
        </trans-unit>
        <trans-unit id="64ba89c98e2ef72ba75fa028267bbaa3251bdd0e" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a complex data type, the property &lt;a href=&quot;generated/torch.is_complex#torch.is_complex&quot;&gt;&lt;code&gt;is_complex&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a complex data type.</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;が複素数データ型であるかどうかを確認するには、プロパティ&lt;a href=&quot;generated/torch.is_complex#torch.is_complex&quot;&gt; &lt;code&gt;is_complex&lt;/code&gt; &lt;/a&gt;を使用できます。これは、データ型が複素数データ型の場合に &lt;code&gt;True&lt;/code&gt; を返します。</target>
        </trans-unit>
        <trans-unit id="4bde254844ec10d73414a4f9097cbf8e8d5d1c09" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a floating point data type, the property &lt;a href=&quot;generated/torch.is_floating_point#torch.is_floating_point&quot;&gt;&lt;code&gt;is_floating_point&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a floating point data type.</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;が浮動小数点データ型であるかどうかを確認するには、プロパティ&lt;a href=&quot;generated/torch.is_floating_point#torch.is_floating_point&quot;&gt; &lt;code&gt;is_floating_point&lt;/code&gt; &lt;/a&gt;を使用できます。これは、データ型が浮動小数点データ型の場合に &lt;code&gt;True&lt;/code&gt; を返します。</target>
        </trans-unit>
        <trans-unit id="48e44e1c5bae8e339aa29cc00850c62cff2a0ea7" translate="yes" xml:space="preserve">
          <source>To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It&amp;rsquo;s also helpful to include a minimal working example.</source>
          <target state="translated">ユーザーがドキュメントを前後に参照せずに探索できるように、リポジトリの所有者は関数のヘルプメッセージを明確かつ簡潔にすることを強くお勧めします。最小限の実例を含めることも役立ちます。</target>
        </trans-unit>
        <trans-unit id="6e370e4d333aa52e5158136385c8ce683abfc7e1" translate="yes" xml:space="preserve">
          <source>To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.</source>
          <target state="translated">拡張機能をロードするには、Ninja ビルドファイルが発行され、与えられたソースをダイナミックライブラリにコンパイルするために使用されます。このライブラリはモジュールとして現在の Python プロセスにロードされ、この関数から返されます。</target>
        </trans-unit>
        <trans-unit id="0f1500bbb514e4eeb3d360549b7085361d0903f1" translate="yes" xml:space="preserve">
          <source>To look up what optional arguments this module offers:</source>
          <target state="translated">このモジュールが提供するオプションの引数を調べます。</target>
        </trans-unit>
        <trans-unit id="efe5a1ff1c27c51217ed9e88d6d14c167e44ae98" translate="yes" xml:space="preserve">
          <source>To make it easier to understand, here is a small example:</source>
          <target state="translated">わかりやすくするために、ちょっとした例をご紹介します。</target>
        </trans-unit>
        <trans-unit id="4861c71c5547ba6da8095cee191825540e015436" translate="yes" xml:space="preserve">
          <source>To make writing TorchScript more convenient, we allow script code to refer to Python values in the surrounding scope. For instance, any time there is a reference to &lt;code&gt;torch&lt;/code&gt;, the TorchScript compiler is actually resolving it to the &lt;code&gt;torch&lt;/code&gt; Python module when the function is declared. These Python values are not a first class part of TorchScript. Instead they are de-sugared at compile-time into the primitive types that TorchScript supports. This depends on the dynamic type of the Python valued referenced when compilation occurs. This section describes the rules that are used when accessing Python values in TorchScript.</source>
          <target state="translated">TorchScriptの記述をより便利にするために、スクリプトコードが周囲のスコープ内のPython値を参照できるようにします。たとえば、 &lt;code&gt;torch&lt;/code&gt; への参照があるときはいつでも、関数が宣言されたときにTorchScriptコンパイラが実際に &lt;code&gt;torch&lt;/code&gt; Pythonモジュールにそれを解決しています。これらのPython値は、TorchScriptのファーストクラスの部分ではありません。代わりに、コンパイル時にTorchScriptがサポートするプリミティブ型に糖抜きされます。これは、コンパイル時に参照されるPython値の動的タイプによって異なります。このセクションでは、TorchScriptでPython値にアクセスするときに使用されるルールについて説明します。</target>
        </trans-unit>
        <trans-unit id="56b4b291b8193f650ab440b4d0108041657026e3" translate="yes" xml:space="preserve">
          <source>To obtain repeatable results, reset the seed for the pseudorandom number generator</source>
          <target state="translated">再現性のある結果を得るためには、擬似乱数発生器のシードをリセットします。</target>
        </trans-unit>
        <trans-unit id="cdaaaf6ab8e173c9d7ceb41b8dea6cb46e09c870" translate="yes" xml:space="preserve">
          <source>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</source>
          <target state="translated">カスタマイズされた追加情報を印刷するには、自分のモジュールでこの方法を再実装する必要があります。一行文字列でも複数行文字列でもどちらでも構いません。</target>
        </trans-unit>
        <trans-unit id="ff43c0b33f5574b95fded09d54e409788745cf85" translate="yes" xml:space="preserve">
          <source>To run the exported script with &lt;a href=&quot;https://caffe2.ai/&quot;&gt;caffe2&lt;/a&gt;, you will need to install &lt;code&gt;caffe2&lt;/code&gt;: If you don&amp;rsquo;t have one already, Please &lt;a href=&quot;https://caffe2.ai/docs/getting-started.html&quot;&gt;follow the install instructions&lt;/a&gt;.</source>
          <target state="translated">エクスポートされたスクリプトを&lt;a href=&quot;https://caffe2.ai/&quot;&gt;caffe2&lt;/a&gt;で実行するには、 &lt;code&gt;caffe2&lt;/code&gt; をインストールする必要があります。まだインストールしていない場合&lt;a href=&quot;https://caffe2.ai/docs/getting-started.html&quot;&gt;は、インストール手順に従って&lt;/a&gt;ください。</target>
        </trans-unit>
        <trans-unit id="c62b3cc0b531750df7ee34e20beaf808665f78f8" translate="yes" xml:space="preserve">
          <source>To specify the scale, it takes either the &lt;code&gt;size&lt;/code&gt; or the &lt;code&gt;scale_factor&lt;/code&gt; as it&amp;rsquo;s constructor argument.</source>
          <target state="translated">スケールを指定するには、コンストラクター引数として &lt;code&gt;size&lt;/code&gt; または &lt;code&gt;scale_factor&lt;/code&gt; のいずれかを取ります。</target>
        </trans-unit>
        <trans-unit id="8bc8fb9e3000235214e485d36fb9186012476b04" translate="yes" xml:space="preserve">
          <source>To stop the compiler from compiling a method, add &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;@ignore&lt;/code&gt; leaves the</source>
          <target state="translated">コンパイラがメソッドをコンパイルしないようにするには、&lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt;追加します。 &lt;code&gt;@ignore&lt;/code&gt; は</target>
        </trans-unit>
        <trans-unit id="83e075942db53d26dad4b0cea801b6df02d892d5" translate="yes" xml:space="preserve">
          <source>To take a batch diagonal, pass in dim1=-2, dim2=-1.</source>
          <target state="translated">バッチの対角線を取るには、dim1=-2、dim2=-1を渡します。</target>
        </trans-unit>
        <trans-unit id="06d7e723aad8726de16a32b689408fd5f6a39bff" translate="yes" xml:space="preserve">
          <source>To trace a specific method on a module, see &lt;a href=&quot;generated/torch.jit.trace_module#torch.jit.trace_module&quot;&gt;&lt;code&gt;torch.jit.trace_module&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">モジュールの特定のメソッドをトレースするには、&lt;a href=&quot;generated/torch.jit.trace_module#torch.jit.trace_module&quot;&gt; &lt;code&gt;torch.jit.trace_module&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="7cbbf69a127d5627b468752b233e45c932bef649" translate="yes" xml:space="preserve">
          <source>To use &lt;code&gt;DistributedDataParallel&lt;/code&gt; on a host with N GPUs, you should spawn up &lt;code&gt;N&lt;/code&gt; processes, ensuring that each process exclusively works on a single GPU from 0 to N-1. This can be done by either setting &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; for every process or by calling:</source>
          <target state="translated">N GPUを搭載したホストで &lt;code&gt;DistributedDataParallel&lt;/code&gt; を使用するには、 &lt;code&gt;N&lt;/code&gt; プロセスを生成し、各プロセスが0からN-1までの単一のGPUで排他的に動作するようにする必要があります。これは、すべてのプロセスに &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; を設定するか、以下を呼び出すことによって実行できます。</target>
        </trans-unit>
        <trans-unit id="55e1a9559362bb4b36c706d26afd5c7744bfa4b4" translate="yes" xml:space="preserve">
          <source>To use a &lt;code&gt;nn.ModuleList&lt;/code&gt; inside a compiled method, it must be marked constant by adding the name of the attribute to the &lt;code&gt;__constants__&lt;/code&gt; list for the type. For loops over a &lt;code&gt;nn.ModuleList&lt;/code&gt; will unroll the body of the loop at compile time, with each member of the constant module list.</source>
          <target state="translated">コンパイルされたメソッド内で &lt;code&gt;nn.ModuleList&lt;/code&gt; を使用するには、型の &lt;code&gt;__constants__&lt;/code&gt; リストに属性の名前を追加して定数としてマークする必要があります。 &lt;code&gt;nn.ModuleList&lt;/code&gt; を介したforループは、コンパイル時にループの本体を展開し、定数モジュールリストの各メンバーを使用します。</target>
        </trans-unit>
        <trans-unit id="527375a7ef1d0e262542dedd5c0253dc05250f0d" translate="yes" xml:space="preserve">
          <source>To use these functions the torch.fft module must be imported since its name conflicts with the &lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">これらの関数を使用するには、torch.fftモジュールの名前が&lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt; &lt;code&gt;torch.fft()&lt;/code&gt; &lt;/a&gt;関数と競合するため、torch.fftモジュールをインポートする必要があります。</target>
        </trans-unit>
        <trans-unit id="43427ba5c0a00d9ec53c4149e3f5de362381fec9" translate="yes" xml:space="preserve">
          <source>To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.</source>
          <target state="translated">これを使用して、プロセス間で入力が不均一なトレーニングを可能にするには、このコンテキストマネージャーをトレーニングループの周りにラップするだけです。モデルの変更やデータのロードは必要ありません。</target>
        </trans-unit>
        <trans-unit id="6239832323d9bb66e30c43f099010c4cf674fbce" translate="yes" xml:space="preserve">
          <source>To utilize &lt;em&gt;script-based&lt;/em&gt; exporter for capturing the dynamic loop, we can write the loop in script, and call it from the regular nn.Module:</source>
          <target state="translated">&lt;em&gt;スクリプトベースの&lt;/em&gt;エクスポーターを利用して動的ループをキャプチャするには、スクリプトでループを記述し、通常のnn.Moduleから呼び出します。</target>
        </trans-unit>
        <trans-unit id="111ccaae56ca75ec4b6793420aa1331efeccd7da" translate="yes" xml:space="preserve">
          <source>Top-1 error</source>
          <target state="translated">トップ1エラー</target>
        </trans-unit>
        <trans-unit id="f1240994c38def01302ee353c1f3e6ab783c6922" translate="yes" xml:space="preserve">
          <source>Top-5 error</source>
          <target state="translated">トップ5エラー</target>
        </trans-unit>
        <trans-unit id="ecebe7e89f3537ec9d48a23fefb140aef1ac7e73" translate="yes" xml:space="preserve">
          <source>Torch defines 10 tensor types with CPU and GPU variants which are as follows:</source>
          <target state="translated">Torchでは、CPUとGPUのバリアントを持つ10種類のテンソルを定義しており、以下のようになっています。</target>
        </trans-unit>
        <trans-unit id="2270c240add89c3f53429a527826adf7f8c68848" translate="yes" xml:space="preserve">
          <source>Torch hub works by importing the package as if it was installed. There&amp;rsquo;re some side effects introduced by importing in Python. For example, you can see new items in Python caches &lt;code&gt;sys.modules&lt;/code&gt; and &lt;code&gt;sys.path_importer_cache&lt;/code&gt; which is normal Python behavior.</source>
          <target state="translated">トーチハブは、パッケージがインストールされているかのようにインポートすることで機能します。Pythonにインポートすることで発生するいくつかの副作用があります。たとえば、Pythonキャッシュ &lt;code&gt;sys.modules&lt;/code&gt; および &lt;code&gt;sys.path_importer_cache&lt;/code&gt; に新しいアイテムが表示されます。これは通常のPythonの動作です。</target>
        </trans-unit>
        <trans-unit id="e678d1ba0a8c71917974dd6b809d475b07d566ed" translate="yes" xml:space="preserve">
          <source>Torch mobile supports &lt;code&gt;torch.mobile_optimizer.optimize_for_mobile&lt;/code&gt; utility to run a list of optimization pass with modules in eval mode. The method takes the following parameters: a torch.jit.ScriptModule object, a blacklisting optimization set and a preserved method list</source>
          <target state="translated">Torch mobileは、 &lt;code&gt;torch.mobile_optimizer.optimize_for_mobile&lt;/code&gt; ユーティリティをサポートして、評価モードのモジュールで最適化パスのリストを実行します。このメソッドは、次のパラメーターを取ります：torch.jit.ScriptModuleオブジェクト、ブラックリスト最適化セット、および保存されたメソッドリスト</target>
        </trans-unit>
        <trans-unit id="03340697e5da4f35bf9335b934c01e6bb269d07d" translate="yes" xml:space="preserve">
          <source>Torch supports sparse tensors in COO(rdinate) format, which can efficiently store and process tensors for which the majority of elements are zeros.</source>
          <target state="translated">TorchはCOO(rdinate)形式のスパーステンソルをサポートしており、要素の大部分が0であるテンソルを効率的に格納して処理することができます。</target>
        </trans-unit>
        <trans-unit id="efa8e2bf58145beaa70cc1139b800b0cc73441ae" translate="yes" xml:space="preserve">
          <source>TorchElastic</source>
          <target state="translated">TorchElastic</target>
        </trans-unit>
        <trans-unit id="3b90040dd3c7ea550e3ae7ebb31cbbf38c50d775" translate="yes" xml:space="preserve">
          <source>TorchScript</source>
          <target state="translated">TorchScript</target>
        </trans-unit>
        <trans-unit id="604c8e5563e16356b9e1a864727a3495aaa4b8f6" translate="yes" xml:space="preserve">
          <source>TorchScript Classes</source>
          <target state="translated">TorchScript クラス</target>
        </trans-unit>
        <trans-unit id="5d73d728706f6b5bfb8bc6488f8b896532317b47" translate="yes" xml:space="preserve">
          <source>TorchScript Enums</source>
          <target state="translated">TorchScript Enums</target>
        </trans-unit>
        <trans-unit id="2d3d498afcd6dbfb2767a5e73bfdf3c641826801" translate="yes" xml:space="preserve">
          <source>TorchScript Language</source>
          <target state="translated">トーチスクリプト言語</target>
        </trans-unit>
        <trans-unit id="655dca0ad6a50c42b950785617112aa26c2b7ea8" translate="yes" xml:space="preserve">
          <source>TorchScript Language Reference</source>
          <target state="translated">TorchScript 言語リファレンス</target>
        </trans-unit>
        <trans-unit id="4ca55dbce27b090f25e1beef5f42bc0b74a3e196" translate="yes" xml:space="preserve">
          <source>TorchScript Unsupported Pytorch Constructs</source>
          <target state="translated">TorchScript がサポートされていない Pytorch コンストラクタ</target>
        </trans-unit>
        <trans-unit id="f37751da08a275fc84f6719af858bb6fca4efcaa" translate="yes" xml:space="preserve">
          <source>TorchScript also has a representation at a lower level than the code pretty- printer, in the form of IR graphs.</source>
          <target state="translated">TorchScriptは、IRグラフの形で、コードのプリティプリンタよりも低いレベルの表現も持っています。</target>
        </trans-unit>
        <trans-unit id="1f6a6dd855f78002eaa186408d37b5a14d38fb6e" translate="yes" xml:space="preserve">
          <source>TorchScript also provides a way to use constants that are defined in Python. These can be used to hard-code hyper-parameters into the function, or to define universal constants. There are two ways of specifying that a Python value should be treated as a constant.</source>
          <target state="translated">TorchScriptは、Pythonで定義されている定数を使用する方法も提供しています。これらは、関数にハイパーパラメータをハードコーディングしたり、普遍的な定数を定義したりするために使用することができます。Pythonの値を定数として扱うことを指定するには、2つの方法があります。</target>
        </trans-unit>
        <trans-unit id="252b708f15ebb50ba55ac0b5f321607abb84f702" translate="yes" xml:space="preserve">
          <source>TorchScript can call Python functions. This functionality is very useful when incrementally converting a model to TorchScript. The model can be moved function-by-function to TorchScript, leaving calls to Python functions in place. This way you can incrementally check the correctness of the model as you go.</source>
          <target state="translated">TorchScriptはPythonの関数を呼び出すことができます。この機能は、モデルをインクリメンタルにTorchScriptに変換する際に非常に便利です。Pythonの関数を呼び出したまま、モデルを関数単位でTorchScriptに移動させることができます。このようにして、モデルが正しいかどうかをインクリメンタルにチェックすることができます。</target>
        </trans-unit>
        <trans-unit id="0e634a60c8e32dbe73734d8f74e5fc62a8bbd892" translate="yes" xml:space="preserve">
          <source>TorchScript can lookup attributes on modules. &lt;code&gt;Builtin functions&lt;/code&gt; like &lt;code&gt;torch.add&lt;/code&gt; are accessed this way. This allows TorchScript to call functions defined in other modules.</source>
          <target state="translated">TorchScriptは、モジュールの属性を検索できます。 &lt;code&gt;torch.add&lt;/code&gt; のような組み込み &lt;code&gt;Builtin functions&lt;/code&gt; はこの方法でアクセスされます。これにより、TorchScriptは他のモジュールで定義された関数を呼び出すことができます。</target>
        </trans-unit>
        <trans-unit id="05a14206c4e373e5e12c57e024877cf642631ecf" translate="yes" xml:space="preserve">
          <source>TorchScript class support is experimental. Currently it is best suited for simple record-like types (think a &lt;code&gt;NamedTuple&lt;/code&gt; with methods attached).</source>
          <target state="translated">TorchScriptクラスのサポートは実験的なものです。現在、単純なレコードのようなタイプに最適です（メソッドがアタッチされた &lt;code&gt;NamedTuple&lt;/code&gt; を考えてください）。</target>
        </trans-unit>
        <trans-unit id="40d12a53eedc122f7c1c26cc26562fe87abc6002" translate="yes" xml:space="preserve">
          <source>TorchScript classes are statically typed. Members can only be declared by assigning to self in the &lt;code&gt;__init__()&lt;/code&gt; method.</source>
          <target state="translated">TorchScriptクラスは静的に型付けされます。メンバーは、 &lt;code&gt;__init__()&lt;/code&gt; メソッドでselfに割り当てることによってのみ宣言できます。</target>
        </trans-unit>
        <trans-unit id="6fd4fa394dbd8f52548eb83a0ab865caf3d46d44" translate="yes" xml:space="preserve">
          <source>TorchScript does not support &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#bytes&quot;&gt;&lt;code&gt;bytes&lt;/code&gt;&lt;/a&gt; so this type is not used</source>
          <target state="translated">TorchScriptは&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#bytes&quot;&gt; &lt;code&gt;bytes&lt;/code&gt; &lt;/a&gt;サポートしていないため、このタイプは使用されません</target>
        </trans-unit>
        <trans-unit id="95d83676fd42ab283c45a040ec05adf9e17c4b2a" translate="yes" xml:space="preserve">
          <source>TorchScript does not support all features and types of the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module. Some of these are more fundamental things that are unlikely to be added in the future while others may be added if there is enough user demand to make it a priority.</source>
          <target state="translated">TorchScriptは、&lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt;モジュールのすべての機能とタイプをサポートしているわけではありません。これらのいくつかは、将来追加される可能性が低いより基本的なものですが、それを優先するのに十分なユーザーの要求がある場合は、他のものが追加される可能性があります。</target>
        </trans-unit>
        <trans-unit id="155bc1507dbbed5e44c922d48bd933f969683779" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python that can either be written directly (using the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; decorator) or generated automatically from Python code via tracing. When using tracing, code is automatically converted into this subset of Python by recording only the actual operators on tensors and simply executing and discarding the other surrounding Python code.</source>
          <target state="translated">TorchScriptは、静的に型指定されたPythonのサブセットであり、直接（&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt;デコレータを使用して）書き込むことも、トレースを介してPythonコードから自動的に生成することもできます。トレースを使用する場合、コードはテンソルに実際の演算子のみを記録し、周囲の他のPythonコードを実行して破棄するだけで、Pythonのこのサブセットに自動的に変換されます。</target>
        </trans-unit>
        <trans-unit id="6e3be4ae1ba105244947e696b16b923455648d05" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full &lt;a href=&quot;jit_language_reference#language-reference&quot;&gt;TorchScript Language Reference&lt;/a&gt; for details.</source>
          <target state="translated">TorchScriptは静的に型指定されたPythonのサブセットであるため、多くのPython機能がTorchScriptに直接適用されます。詳細については、完全な&lt;a href=&quot;jit_language_reference#language-reference&quot;&gt;TorchScript言語リファレンス&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="b80f73ada7844bf2ed31f70d01150d3e0f521094" translate="yes" xml:space="preserve">
          <source>TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.</source>
          <target state="translated">TorchScriptは、PyTorchコードからシリアライズ可能で最適化可能なモデルを作成する方法です。任意のTorchScriptプログラムをPythonプロセスから保存し、Pythonに依存しないプロセスにロードすることができます。</target>
        </trans-unit>
        <trans-unit id="f3730c4db0b9ee76825d3365cc7e6ee5d81c09f3" translate="yes" xml:space="preserve">
          <source>TorchScript provides a code pretty-printer for all &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; instances. This pretty-printer gives an interpretation of the script method&amp;rsquo;s code as valid Python syntax. For example:</source>
          <target state="translated">TorchScriptは、すべての&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;インスタンスにコードプリティプリンターを提供します。このプリティプリンターは、スクリプトメソッドのコードを有効なPython構文として解釈します。例えば：</target>
        </trans-unit>
        <trans-unit id="6f800d70869c5af16f03f352efad0babdc4a4cee" translate="yes" xml:space="preserve">
          <source>TorchScript support in RPC is a prototype feature and subject to change. Since v1.5.0, &lt;code&gt;torch.distributed.rpc&lt;/code&gt; supports calling TorchScript functions as RPC target functions, and this will help improve parallelism on the callee side as executing TorchScript functions does not require GIL.</source>
          <target state="translated">RPCでのTorchScriptのサポートはプロトタイプ機能であり、変更される可能性があります。 &lt;code&gt;torch.distributed.rpc&lt;/code&gt; 以降、torch.distributed.rpcはRPCターゲット関数としてのTorchScript関数の呼び出しをサポートします。これにより、TorchScript関数の実行にGILが必要ないため、呼び出し先側の並列処理が向上します。</target>
        </trans-unit>
        <trans-unit id="7d483bee6192edebb2157f5cffc7ebe0cf7bf9b5" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of Python&amp;rsquo;s variable resolution (i.e. scoping) rules. Local variables behave the same as in Python, except for the restriction that a variable must have the same type along all paths through a function. If a variable has a different type on different branches of an if statement, it is an error to use it after the end of the if statement.</source>
          <target state="translated">TorchScriptは、Pythonの可変解決（つまりスコープ）ルールのサブセットをサポートします。ローカル変数は、関数を通るすべてのパスで変数が同じ型でなければならないという制限を除いて、Pythonと同じように動作します。変数がifステートメントの異なるブランチで異なるタイプを持っている場合、ifステートメントの終了後にそれを使用することはエラーです。</target>
        </trans-unit>
        <trans-unit id="439e6a570c3e92811a193d2bb2eb0c532aac85c2" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the &lt;code&gt;torch&lt;/code&gt; namespace, all functions in &lt;code&gt;torch.nn.functional&lt;/code&gt; and most modules from &lt;code&gt;torch.nn&lt;/code&gt; are supported in TorchScript.</source>
          <target state="translated">TorchScriptは、PyTorchが提供するテンソルおよびニューラルネットワーク関数のサブセットをサポートします。Tensorのほとんどのメソッドと、 &lt;code&gt;torch&lt;/code&gt; 名前空間の関数、 &lt;code&gt;torch.nn.functional&lt;/code&gt; のすべての関数、およびtorch.nnのほとんどのモジュールが &lt;code&gt;torch.nn&lt;/code&gt; でサポートされています。</target>
        </trans-unit>
        <trans-unit id="85815975862ed9b632ab9ab2cb6516181e624194" translate="yes" xml:space="preserve">
          <source>TorchScript supports the following types of statements:</source>
          <target state="translated">TorchScriptは以下のタイプのステートメントをサポートしています。</target>
        </trans-unit>
        <trans-unit id="237a371f4a7260a3005de6488e3d8470546b5ec9" translate="yes" xml:space="preserve">
          <source>TorchScript supports the use of most PyTorch functions and many Python built-ins. See &lt;a href=&quot;jit_builtin_functions#builtin-functions&quot;&gt;TorchScript Builtins&lt;/a&gt; for a full reference of supported functions.</source>
          <target state="translated">TorchScriptは、ほとんどのPyTorch関数と多くのPython組み込み関数の使用をサポートしています。サポートされている関数の完全なリファレンスについては、&lt;a href=&quot;jit_builtin_functions#builtin-functions&quot;&gt;TorchScriptBuiltinsを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="f626090730ab6e08eee60582294fe0a9f90efe3d" translate="yes" xml:space="preserve">
          <source>TorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example:</source>
          <target state="translated">TorchScript は、計算を表現するために静的単一代入(SSA)中間表現(IR)を使用します。この形式の命令は、ATen(PyTorchのC++バックエンド)演算子と、ループや条件式の制御フロー演算子を含む他のプリミティブ演算子で構成されています。一例として</target>
        </trans-unit>
        <trans-unit id="ee07cfab05a06e606f9327817d635689c5cd9b5d" translate="yes" xml:space="preserve">
          <source>TorchScript will refine the type of a variable of type &lt;code&gt;Optional[T]&lt;/code&gt; when a comparison to &lt;code&gt;None&lt;/code&gt; is made inside the conditional of an if-statement or checked in an &lt;code&gt;assert&lt;/code&gt;. The compiler can reason about multiple &lt;code&gt;None&lt;/code&gt; checks that are combined with &lt;code&gt;and&lt;/code&gt;, &lt;code&gt;or&lt;/code&gt;, and &lt;code&gt;not&lt;/code&gt;. Refinement will also occur for else blocks of if-statements that are not explicitly written.</source>
          <target state="translated">TorchScriptは、ifステートメントの条件内で &lt;code&gt;None&lt;/code&gt; との比較が行われるか、 &lt;code&gt;assert&lt;/code&gt; でチェックされると、 &lt;code&gt;Optional[T]&lt;/code&gt; 型の変数の型を調整します。コンパイラーは、 &lt;code&gt;and&lt;/code&gt; 、 &lt;code&gt;or&lt;/code&gt; 、および &lt;code&gt;not&lt;/code&gt; と組み合わされた複数の &lt;code&gt;None&lt;/code&gt; チェックについて推論できます。明示的に記述されていないifステートメントのelseブロックに対しても、絞り込みが行われます。</target>
        </trans-unit>
        <trans-unit id="db3578de30ac281698832393e03259941ecc59a4" translate="yes" xml:space="preserve">
          <source>TorchServe</source>
          <target state="translated">TorchServe</target>
        </trans-unit>
        <trans-unit id="174786ece7d01b50c5a292bbe7c461a354fd217b" translate="yes" xml:space="preserve">
          <source>TorchVision support</source>
          <target state="translated">トーチビジョンのサポート</target>
        </trans-unit>
        <trans-unit id="eb01e02a87150ef514202bea3337f1bbd7e6a429" translate="yes" xml:space="preserve">
          <source>Total norm of the parameters (viewed as a single vector).</source>
          <target state="translated">パラメータの全ノルム(1つのベクトルとして見た場合).</target>
        </trans-unit>
        <trans-unit id="117b4c950645374477e1f2b2a96517111745fe19" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="translated">関数をトレースし、ジャストインタイムコンパイルを使用して最適化される実行可能ファイルまたは&lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;を返します。</target>
        </trans-unit>
        <trans-unit id="4cae3f60263770c81b066cebd1a77d6ae0c2a925" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. Tracing is ideal for code that operates only on &lt;code&gt;Tensor&lt;/code&gt;s and lists, dictionaries, and tuples of &lt;code&gt;Tensor&lt;/code&gt;s.</source>
          <target state="translated">関数をトレースし、ジャストインタイムコンパイルを使用して最適化される実行可能ファイルまたは&lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;を返します。トレースはのみで動作するコードのための理想的である &lt;code&gt;Tensor&lt;/code&gt; Sとリスト、辞書、タプルの &lt;code&gt;Tensor&lt;/code&gt; S。</target>
        </trans-unit>
        <trans-unit id="20f26cd87e3024b30b8eeb8b74edbe9cdd81f4b6" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="translated">モジュールをトレースし、ジャストインタイムコンパイルを使用して最適化される実行可能&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;を返します。</target>
        </trans-unit>
        <trans-unit id="3bad49a27d3b91711b9d48661ccd1bb2133d17d3" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. When a module is passed to &lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt;&lt;code&gt;torch.jit.trace&lt;/code&gt;&lt;/a&gt;, only the &lt;code&gt;forward&lt;/code&gt; method is run and traced. With &lt;code&gt;trace_module&lt;/code&gt;, you can specify a dictionary of method names to example inputs to trace (see the &lt;code&gt;inputs&lt;/code&gt;) argument below.</source>
          <target state="translated">モジュールをトレースし、ジャストインタイムコンパイルを使用して最適化される実行可能&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;を返します。モジュールが&lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt; &lt;code&gt;torch.jit.trace&lt;/code&gt; &lt;/a&gt;に渡されると、 &lt;code&gt;forward&lt;/code&gt; メソッドのみが実行されてトレースされます。では &lt;code&gt;trace_module&lt;/code&gt; 、あなたがトレースする例入力にメソッド名の辞書を指定することができます（参照 &lt;code&gt;inputs&lt;/code&gt; 以下の引数を）。</target>
        </trans-unit>
        <trans-unit id="d93ef3f27cd83786d560cf0e7159be09c5658df2" translate="yes" xml:space="preserve">
          <source>Traced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly.</source>
          <target state="translated">トレースされた関数はスクリプト関数を呼び出すことができます。これは、モデルの大部分が単なるフィードフォワード・ネットワークであるにもかかわらず、モデルのごく一部が何らかの制御フローを必要とする場合に便利です。トレース関数によって呼び出されたスクリプト関数内の制御フローは正しく保存されます。</target>
        </trans-unit>
        <trans-unit id="0fce73bbb5357ac93c61680e5f58be84db277c7b" translate="yes" xml:space="preserve">
          <source>Tracer</source>
          <target state="translated">Tracer</target>
        </trans-unit>
        <trans-unit id="d09bb53da249750b0b3b27f27eabe9dbddadc0ed" translate="yes" xml:space="preserve">
          <source>Tracer Warnings</source>
          <target state="translated">トレーサーの警告</target>
        </trans-unit>
        <trans-unit id="bf933c3083c5a6cb4cedac1b1e0c22d6553dab1d" translate="yes" xml:space="preserve">
          <source>Tracing Edge Cases</source>
          <target state="translated">エッジケースのトレース</target>
        </trans-unit>
        <trans-unit id="c5153b07a6f011eb8b3585b8b94ddfea62a7a0ce" translate="yes" xml:space="preserve">
          <source>Tracing of control flow that is dependent on inputs (e.g. tensor shapes)</source>
          <target state="translated">入力に依存する制御フローのトレース(テンソル形状など</target>
        </trans-unit>
        <trans-unit id="656eeb39a9810745d15b70b8f4cca3cf416cf8d6" translate="yes" xml:space="preserve">
          <source>Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)</source>
          <target state="translated">テンソルビューのインプレース操作のトレース(例:割り当ての左側のインデックス作成)</target>
        </trans-unit>
        <trans-unit id="e793ff03e25bf5dd4f85e642caecdeee487a3a73" translate="yes" xml:space="preserve">
          <source>Tracing only correctly records functions and modules which are not data dependent (e.g., do not have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned &lt;code&gt;ScriptModule&lt;/code&gt; will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,</source>
          <target state="translated">トレースは、データに依存せず（たとえば、テンソルのデータに条件がない）、追跡されていない外部依存関係がない（たとえば、入出力の実行やグローバル変数へのアクセス）関数とモジュールのみを正しく記録します。トレースは、指定された関数が指定されたテンソルで実行されたときに実行された操作のみを記録します。したがって、返された &lt;code&gt;ScriptModule&lt;/code&gt; は、どの入力でも常に同じトレースグラフを実行します。これは、入力やモジュールの状態に応じて、モジュールがさまざまな操作のセットを実行することが予想される場合に、いくつかの重要な意味を持ちます。例えば、</target>
        </trans-unit>
        <trans-unit id="1289e51c253ed91b441a90b66b33a5a7cc2af8cf" translate="yes" xml:space="preserve">
          <source>Tracing vs Scripting</source>
          <target state="translated">トレースとスクリプト</target>
        </trans-unit>
        <trans-unit id="4de8ec680853c82acd4d7ddcd35ef7c092a558c3" translate="yes" xml:space="preserve">
          <source>Tracing will not record any control-flow like if-statements or loops. When this control-flow is constant across your module, this is fine and it often inlines the control-flow decisions. But sometimes the control-flow is actually part of the model itself. For instance, a recurrent network is a loop over the (possibly dynamic) length of an input sequence.</source>
          <target state="translated">トレースは、ifステートメントやループのような制御フローを記録しません。このコントロールフローがモジュール全体で一定である場合、これは問題なく、コントロールフローの決定をインラインで行うことが多いです。しかし、コントロールフローが実際にはモデル自体の一部であることもあります。例えば、リカレント・ネットワークは入力シーケンスの(動的な)長さのループです。</target>
        </trans-unit>
        <trans-unit id="b6fe7f5e79177b05f6d251ecb9c162d45455045d" translate="yes" xml:space="preserve">
          <source>Training</source>
          <target state="translated">Training</target>
        </trans-unit>
        <trans-unit id="68c170c0011cf476eed353d994b12887940cfc96" translate="yes" xml:space="preserve">
          <source>Transformer</source>
          <target state="translated">Transformer</target>
        </trans-unit>
        <trans-unit id="6e42fdb55f8e197d60cafca548c1e82579acbea4" translate="yes" xml:space="preserve">
          <source>Transformer Layers</source>
          <target state="translated">変圧器の層</target>
        </trans-unit>
        <trans-unit id="ef303bb941bf717e2006e7273f0810b07b78b045" translate="yes" xml:space="preserve">
          <source>TransformerDecoder</source>
          <target state="translated">TransformerDecoder</target>
        </trans-unit>
        <trans-unit id="39490c0e215073daec405a4b72d513bc1f4fb82e" translate="yes" xml:space="preserve">
          <source>TransformerDecoder is a stack of N decoder layers</source>
          <target state="translated">TransformerDecoderは、N個のデコーダ層のスタックです。</target>
        </trans-unit>
        <trans-unit id="92f20fa7687824fdbb370a6b475f6eeb6f79194b" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer</source>
          <target state="translated">TransformerDecoderLayer</target>
        </trans-unit>
        <trans-unit id="98ae4e9bf414c9fe8b37e6dbc99426a1c7335c2f" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</source>
          <target state="translated">TransformerDecoderLayerは、自己ATTN、マルチヘッドATTN、およびフィードフォワードネットワークで構成されています。</target>
        </trans-unit>
        <trans-unit id="04a4a3c2fb795f7db70756477e2d518c4a892786" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="translated">TransformerDecoderLayerは、自己属性、マルチヘッド属性、およびフィードフォワードネットワークで構成されています。この標準デコーダーレイヤーは、「Attention Is AllYouNeed」という論文に基づいています。 Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N Gomez、Lukasz Kaiser、Illia Polosukhin 2017.必要なのは注意だけです。ニューラル情報処理システムの進歩、6000〜6010ページ。ユーザーは、アプリケーション中に別の方法で変更または実装できます。</target>
        </trans-unit>
        <trans-unit id="32d7343edd3b94812b03b1cdf834b1b16cc2a3fc" translate="yes" xml:space="preserve">
          <source>TransformerEncoder</source>
          <target state="translated">TransformerEncoder</target>
        </trans-unit>
        <trans-unit id="933db464a961834a0fb72e3b98e0d537c401c215" translate="yes" xml:space="preserve">
          <source>TransformerEncoder is a stack of N encoder layers</source>
          <target state="translated">TransformerEncoder は、N 個のエンコーダ層のスタックです。</target>
        </trans-unit>
        <trans-unit id="2a732f60462f98f99de0f8f387f8c71e6c32aba7" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer</source>
          <target state="translated">TransformerEncoderLayer</target>
        </trans-unit>
        <trans-unit id="7b3ae5e9124dac923ef7ce7bbf1287aa019083f2" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network.</source>
          <target state="translated">TransformerEncoderLayerは、自己ATTNとフィードフォワードネットワークで構成されています。</target>
        </trans-unit>
        <trans-unit id="0c8dccb3d09a0f9d0d9b17ea9825251d663f04d4" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="translated">TransformerEncoderLayerは、自己応答型ネットワークとフィードフォワードネットワークで構成されています。この標準エンコーダ層は、「注意が必要なすべて」という論文に基づいています。Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N Gomez、Lukasz Kaiser、Illia Polosukhin 2017.必要なのは注意だけです。ニューラル情報処理システムの進歩、6000〜6010ページ。ユーザーは、アプリケーション中に別の方法で変更または実装できます。</target>
        </trans-unit>
        <trans-unit id="a9cb51a530db6f55c97c4e0720722c1b583dafb7" translate="yes" xml:space="preserve">
          <source>TripletMarginLoss</source>
          <target state="translated">TripletMarginLoss</target>
        </trans-unit>
        <trans-unit id="858db382b48e5cc5988f431f28f1130330c1eaf1" translate="yes" xml:space="preserve">
          <source>TripletMarginWithDistanceLoss</source>
          <target state="translated">TripletMarginWithDistanceLoss</target>
        </trans-unit>
        <trans-unit id="88b33e4e12f75ac8bf792aebde41f1a090f3a612" translate="yes" xml:space="preserve">
          <source>True</source>
          <target state="translated">True</target>
        </trans-unit>
        <trans-unit id="6692b9e66830b65bbf15ff9de3c8b1af8bfbc5eb" translate="yes" xml:space="preserve">
          <source>Tuple Construction</source>
          <target state="translated">タプル構造</target>
        </trans-unit>
        <trans-unit id="2c809de45543797accbb7ba7433a5877747ac289" translate="yes" xml:space="preserve">
          <source>Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to &lt;code&gt;pack_padded_sequence&lt;/code&gt; or &lt;code&gt;pack_sequence&lt;/code&gt;.</source>
          <target state="translated">パディングされたシーケンスを含むTensorのタプル、およびバッチ内の各シーケンスの長さのリストを含むTensor。バッチ要素は、バッチが &lt;code&gt;pack_padded_sequence&lt;/code&gt; または &lt;code&gt;pack_sequence&lt;/code&gt; に渡されたときに最初に順序付けられたとおりに並べ替えられます。</target>
        </trans-unit>
        <trans-unit id="654171647baa6be8557a5d627cf35c7075ebb257" translate="yes" xml:space="preserve">
          <source>Tutorials</source>
          <target state="translated">Tutorials</target>
        </trans-unit>
        <trans-unit id="19fd25590b1f3de52164e96d6a585a4ae3f02132" translate="yes" xml:space="preserve">
          <source>Two names &lt;em&gt;match&lt;/em&gt; if they are equal (string equality) or if at least one is &lt;code&gt;None&lt;/code&gt;. Nones are essentially a special &amp;ldquo;wildcard&amp;rdquo; name.</source>
          <target state="translated">2つの名前が等しい場合（文字列が等しい場合）、または少なくとも1つが &lt;code&gt;None&lt;/code&gt; の場合、2つの名前は&lt;em&gt;一致し&lt;/em&gt;ます。どれも本質的に特別な「ワイルドカード」名です。</target>
        </trans-unit>
        <trans-unit id="3deb7456519697ecf4eefc455516c969a3681bae" translate="yes" xml:space="preserve">
          <source>Type</source>
          <target state="translated">Type</target>
        </trans-unit>
        <trans-unit id="74a99ad458c9adf9d415d4bdb125da11688a37f7" translate="yes" xml:space="preserve">
          <source>Type Info</source>
          <target state="translated">タイプ情報</target>
        </trans-unit>
        <trans-unit id="58cf557846d7f65d34e8a92739eef2665164fad4" translate="yes" xml:space="preserve">
          <source>Type aliases</source>
          <target state="translated">タイプエイリアス</target>
        </trans-unit>
        <trans-unit id="93b9e289e2842469d001eccf7ad5d79f3c302dc9" translate="yes" xml:space="preserve">
          <source>Types</source>
          <target state="translated">Types</target>
        </trans-unit>
        <trans-unit id="c0d285234eb927c53c2fc1dda528e04061e0d90d" translate="yes" xml:space="preserve">
          <source>Types produced by &lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt;&lt;code&gt;collections.namedtuple&lt;/code&gt;&lt;/a&gt; can be used in TorchScript.</source>
          <target state="translated">&lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt; &lt;code&gt;collections.namedtuple&lt;/code&gt; &lt;/a&gt;によって生成されたタイプは、TorchScriptで使用できます。</target>
        </trans-unit>
        <trans-unit id="b2c7c0caa10a0cca5ea7d69e54018ae0c0389dd6" translate="yes" xml:space="preserve">
          <source>U</source>
          <target state="translated">U</target>
        </trans-unit>
        <trans-unit id="4e5f249d2049283bfab86474c53e19434fabe07e" translate="yes" xml:space="preserve">
          <source>URL specifying how to initialize the process group. Default is &lt;code&gt;env://&lt;/code&gt;</source>
          <target state="translated">プロセスグループを初期化する方法を指定するURL。デフォルトは &lt;code&gt;env://&lt;/code&gt; です</target>
        </trans-unit>
        <trans-unit id="9016840b6ab501762ae42c3bc2d77284127ecd9d" translate="yes" xml:space="preserve">
          <source>Unflatten</source>
          <target state="translated">Unflatten</target>
        </trans-unit>
        <trans-unit id="22753787538a4df3368517dc3a28771d6a6bd1c2" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape.</source>
          <target state="translated">テンソルディムを任意の形状に展開してアンフラットします。</target>
        </trans-unit>
        <trans-unit id="ffd6987181a5e9a56284c36d90107b63ec0ad279" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape. For use with &lt;code&gt;Sequential&lt;/code&gt;.</source>
          <target state="translated">テンソルの薄暗い部分を平らにして、目的の形状に拡張します。 &lt;code&gt;Sequential&lt;/code&gt; で使用します。</target>
        </trans-unit>
        <trans-unit id="02d85ea4efaa0d1ca099f19843e9be07e28b954d" translate="yes" xml:space="preserve">
          <source>Unfold</source>
          <target state="translated">Unfold</target>
        </trans-unit>
        <trans-unit id="27c8f884a26740cbb923c350e9fa436fb8314b34" translate="yes" xml:space="preserve">
          <source>Unfortunately, the concrete &lt;code&gt;subset&lt;/code&gt; that was used is lost. For more information see &lt;a href=&quot;https://github.com/pytorch/vision/issues/1439&quot;&gt;this discussion&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/vision/pull/1965&quot;&gt;these experiments&lt;/a&gt;.</source>
          <target state="translated">残念ながら、使用された具体的な &lt;code&gt;subset&lt;/code&gt; は失われます。詳細については、&lt;a href=&quot;https://github.com/pytorch/vision/issues/1439&quot;&gt;このディスカッション&lt;/a&gt;または&lt;a href=&quot;https://github.com/pytorch/vision/pull/1965&quot;&gt;これらの実験を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="b69bc99e876e93b97ea0728c1acca97b14bbffef" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand()&lt;/code&gt;&lt;/a&gt;, this function copies the tensor&amp;rsquo;s data.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.expand&quot;&gt; &lt;code&gt;expand()&lt;/code&gt; &lt;/a&gt;とは異なり、この関数はテンソルのデータをコピーします。</target>
        </trans-unit>
        <trans-unit id="ef5392b685d4622304e5dfc150347ec19f0ee0af" translate="yes" xml:space="preserve">
          <source>Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the &lt;code&gt;affine&lt;/code&gt; option, Layer Normalization applies per-element scale and bias with &lt;code&gt;elementwise_affine&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;affine&lt;/code&gt; オプションを使用してチャネル/平面全体にスカラースケールとバイアスを適用するバッチ正規化とインスタンス正規化とは異なり、レイヤー正規化は &lt;code&gt;elementwise_affine&lt;/code&gt; を使用して要素ごとのスケールとバイアスを適用します。</target>
        </trans-unit>
        <trans-unit id="2498c971c849991894b17969e87a3329f48f1d2d" translate="yes" xml:space="preserve">
          <source>Unlike Python, each variable in TorchScript function must have a single static type. This makes it easier to optimize TorchScript functions.</source>
          <target state="translated">Pythonとは異なり、TorchScript関数の各変数は1つの静的型を持つ必要があります。これにより、TorchScript関数の最適化が容易になります。</target>
        </trans-unit>
        <trans-unit id="3d7bdd35d7e8961eff68645ed326f054b5482c52" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented</source>
          <target state="translated">実装されそうにない</target>
        </trans-unit>
        <trans-unit id="a5b6f222f736f9cef541160a005848fe4ef77888" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented (however &lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;&gt;&lt;code&gt;typing.Optional&lt;/code&gt;&lt;/a&gt; is supported)</source>
          <target state="translated">実装される可能性は&lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;&gt; &lt;code&gt;typing.Optional&lt;/code&gt; &lt;/a&gt;です（ただし、入力します。オプションがサポートされています）</target>
        </trans-unit>
        <trans-unit id="01773bdc2e10d2279820d7115b2522610a09e4f3" translate="yes" xml:space="preserve">
          <source>Unpacks the data and pivots from a LU factorization of a tensor.</source>
          <target state="translated">データをアンパックし、テンソルのLU因数分解からピボットします。</target>
        </trans-unit>
        <trans-unit id="a7afe79ac1105fe133bbed5120636913c111a929" translate="yes" xml:space="preserve">
          <source>Unsupported Typing Constructs</source>
          <target state="translated">サポートされていない型付けコンストラクト</target>
        </trans-unit>
        <trans-unit id="b2d1fe571301d72d9191816bf953a47985e426ec" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;ModuleDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt; &lt;code&gt;ModuleDict&lt;/code&gt; &lt;/a&gt;を、マッピングまたは反復可能な既存のキーを上書きするキーと値のペアで更新します。</target>
        </trans-unit>
        <trans-unit id="4eca5c6565cbf1cfd8391f002e24b24767bdf27d" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt;&lt;code&gt;ParameterDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="translated">マッピングまたは反復可能な既存のキーを上書きするキーと値のペアで&lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt; &lt;code&gt;ParameterDict&lt;/code&gt; &lt;/a&gt;を更新します。</target>
        </trans-unit>
        <trans-unit id="e9f4cec65260954e7a90830aa6f73b90e5c8817f" translate="yes" xml:space="preserve">
          <source>Upsample</source>
          <target state="translated">Upsample</target>
        </trans-unit>
        <trans-unit id="0b472c3013e4cd26ac1f5f84d9b57c5b6b4ca455" translate="yes" xml:space="preserve">
          <source>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</source>
          <target state="translated">与えられたマルチチャンネルの1D(時間的)、2D(空間的)、3D(体積的)データをアップサンプリングします。</target>
        </trans-unit>
        <trans-unit id="55dfe2d567c845d264928977c08cd8d377265025" translate="yes" xml:space="preserve">
          <source>Upsamples the input to either the given &lt;code&gt;size&lt;/code&gt; or the given &lt;code&gt;scale_factor&lt;/code&gt;</source>
          <target state="translated">指定 &lt;code&gt;size&lt;/code&gt; れたサイズまたは指定された &lt;code&gt;scale_factor&lt;/code&gt; のいずれかに入力をアップサンプリングします</target>
        </trans-unit>
        <trans-unit id="c9b84912595a004dd92417c63046087231bc7d36" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using bilinear upsampling.</source>
          <target state="translated">バイリニアアップサンプリングを使用して、入力をアップサンプリングします。</target>
        </trans-unit>
        <trans-unit id="a31dc0017d79484c9069d06f3ac78a206e41dbfa" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using nearest neighbours&amp;rsquo; pixel values.</source>
          <target state="translated">最も近い隣人のピクセル値を使用して、入力をアップサンプリングします。</target>
        </trans-unit>
        <trans-unit id="fef8a1cac9f9d013e57f5acb6da8a9f36cede8d1" translate="yes" xml:space="preserve">
          <source>UpsamplingBilinear2d</source>
          <target state="translated">UpsamplingBilinear2d</target>
        </trans-unit>
        <trans-unit id="2a7960d23688a71e6707ef6d16f626ed000e779c" translate="yes" xml:space="preserve">
          <source>UpsamplingNearest2d</source>
          <target state="translated">UpsamplingNearest2d</target>
        </trans-unit>
        <trans-unit id="680c6c9f314b68f86ff46d8fdf9a289c7fb0c343" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_as&quot;&gt;&lt;code&gt;align_as()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to align tensor dimensions by name to a specified ordering. This is useful for performing &amp;ldquo;broadcasting by names&amp;rdquo;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.align_as&quot;&gt; &lt;code&gt;align_as()&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt;を使用して、テンソル次元を名前で指定された順序に揃えます。これは、「名前によるブロードキャスト」を実行する場合に役立ちます。</target>
        </trans-unit>
        <trans-unit id="66804d0e4e83078e14bfdbaa37e395675103792c" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to permute large amounts of dimensions without mentioning all of them as in required by &lt;a href=&quot;tensors#torch.Tensor.permute&quot;&gt;&lt;code&gt;permute()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt;を使用して、permute（）で必要とされるように、すべてのディメンションに言及せずに大量のディメンションを&lt;a href=&quot;tensors#torch.Tensor.permute&quot;&gt; &lt;code&gt;permute()&lt;/code&gt; &lt;/a&gt;ます。</target>
        </trans-unit>
        <trans-unit id="ede9bff88acacf3e23cc669406c39e0d8aba75b8" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.item&quot;&gt;&lt;code&gt;torch.Tensor.item()&lt;/code&gt;&lt;/a&gt; to get a Python number from a tensor containing a single value:</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.item&quot;&gt; &lt;code&gt;torch.Tensor.item()&lt;/code&gt; &lt;/a&gt;を使用して、単一の値を含むテンソルからPython番号を取得します。</target>
        </trans-unit>
        <trans-unit id="e75b6839717cab84ecfa02664778f32a2c198780" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; to access the dimension names of a tensor and &lt;a href=&quot;#torch.Tensor.rename&quot;&gt;&lt;code&gt;rename()&lt;/code&gt;&lt;/a&gt; to rename named dimensions.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt;を使用してテンソルの次元名にアクセスし、&lt;a href=&quot;#torch.Tensor.rename&quot;&gt; &lt;code&gt;rename()&lt;/code&gt; &lt;/a&gt;を使用して名前付き次元の名前を変更します。</target>
        </trans-unit>
        <trans-unit id="b5d22900f5f0642196ebec69d6c5572874ee7c1b" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;tensors#torch.Tensor.flatten&quot;&gt;&lt;code&gt;flatten()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.Tensor.unflatten&quot;&gt;&lt;code&gt;unflatten()&lt;/code&gt;&lt;/a&gt; to flatten and unflatten dimensions, respectively. These methods are more verbose than &lt;a href=&quot;tensors#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensors#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, but have more semantic meaning to someone reading the code.</source>
          <target state="translated">&lt;a href=&quot;tensors#torch.Tensor.flatten&quot;&gt; &lt;code&gt;flatten()&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#torch.Tensor.unflatten&quot;&gt; &lt;code&gt;unflatten()&lt;/code&gt; &lt;/a&gt;を使用して、それぞれ寸法を平坦化と非平坦化します。これらのメソッドは、&lt;a href=&quot;tensors#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt;や&lt;a href=&quot;tensors#torch.Tensor.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt;よりも冗長ですが、コードを読んでいる人にとっては意味的な意味があります。</target>
        </trans-unit>
        <trans-unit id="641cc18d21edd9671570b1c6c598d26a3e5396f4" translate="yes" xml:space="preserve">
          <source>Use Gloo, unless you have specific reasons to use MPI.</source>
          <target state="translated">MPIを使用する特別な理由がない限り、Glooを使用してください。</target>
        </trans-unit>
        <trans-unit id="4b4d173cba748d683c0210fec722f5787173d83a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)</source>
          <target state="translated">NCCLを使用してください。特にマルチプロセス・シングルノードまたはマルチノードの分散トレーニングでは、現在最高の分散GPUトレーニング性能を提供しています。NCCLで問題が発生した場合は、代替オプションとしてGlooを使用してください。(Glooは現在のところGPUではNCCLよりも動作が遅いことに注意してください)。</target>
        </trans-unit>
        <trans-unit id="9b1bf082bc9bf3ac7914e6187c00d474a91fb73a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it&amp;rsquo;s the only backend that currently supports InfiniBand and GPUDirect.</source>
          <target state="translated">NCCLは、現在InfiniBandとGPUDirectをサポートしている唯一のバックエンドであるため、NCCLを使用してください。</target>
        </trans-unit>
        <trans-unit id="5e1700010ee503b77904278f8d396bfe42da63c7" translate="yes" xml:space="preserve">
          <source>Use external data format</source>
          <target state="translated">外部データ形式を使用する</target>
        </trans-unit>
        <trans-unit id="9d0232b01e54b3a2e55b622100adbb68b4edb340" translate="yes" xml:space="preserve">
          <source>Use of Python Values</source>
          <target state="translated">Pythonの値の使用</target>
        </trans-unit>
        <trans-unit id="73922dca1fc1eb81dc09b31de16429c2bad893e0" translate="yes" xml:space="preserve">
          <source>Use the Gloo backend for distributed &lt;strong&gt;CPU&lt;/strong&gt; training.</source>
          <target state="translated">分散&lt;strong&gt;CPU&lt;/strong&gt;トレーニングにはGlooバックエンドを使用してください。</target>
        </trans-unit>
        <trans-unit id="fc3648b93e2309e15d382b23a53006e5ccb31b40" translate="yes" xml:space="preserve">
          <source>Use the NCCL backend for distributed &lt;strong&gt;GPU&lt;/strong&gt; training</source>
          <target state="translated">分散&lt;strong&gt;GPU&lt;/strong&gt;トレーニングにNCCLバックエンドを使用する</target>
        </trans-unit>
        <trans-unit id="9ca66ca0ae6872cefb7cdaba9a1b1d7c4fb946f1" translate="yes" xml:space="preserve">
          <source>Used to infer dtype for python complex numbers. The default complex dtype is set to &lt;code&gt;torch.complex128&lt;/code&gt; if default floating point dtype is &lt;code&gt;torch.float64&lt;/code&gt;, otherwise it&amp;rsquo;s set to &lt;code&gt;torch.complex64&lt;/code&gt;</source>
          <target state="translated">Pythonの複素数のdtypeを推測するために使用されます。デフォルト複雑DTYPEは次のように設定されて &lt;code&gt;torch.complex128&lt;/code&gt; デフォルトの浮動小数点DTYPEがある場合 &lt;code&gt;torch.float64&lt;/code&gt; 、とそうでない場合のセット &lt;code&gt;torch.complex64&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fac6c061fc6d273aebc0e9ab0c99f04ae81e1097" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Argument</source>
          <target state="translated">オーナーを引数に持つユーザ共有RRef</target>
        </trans-unit>
        <trans-unit id="a20f912ad88b795f826c479ed261f676c30da2f7" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Return Value</source>
          <target state="translated">オーナーとのユーザー共有RRefをリターン値として</target>
        </trans-unit>
        <trans-unit id="a7e43a6c0dede8cc47d9e15900c78d9da28530c0" translate="yes" xml:space="preserve">
          <source>User Share RRef with User</source>
          <target state="translated">ユーザーとRRefを共有する</target>
        </trans-unit>
        <trans-unit id="78d0e495aa5431e6e0a8139c0666804c17a54940" translate="yes" xml:space="preserve">
          <source>User extensions can register their own location tags and tagging and deserialization methods using &lt;code&gt;torch.serialization.register_package()&lt;/code&gt;.</source>
          <target state="translated">ユーザー拡張機能は、 &lt;code&gt;torch.serialization.register_package()&lt;/code&gt; を使用して、独自のロケーションタグとタグ付けおよび逆シリアル化メソッドを登録できます。</target>
        </trans-unit>
        <trans-unit id="a2628232b96e27d1cfe74641b5e5eef1e4a68d67" translate="yes" xml:space="preserve">
          <source>Users can force a reload by calling &lt;code&gt;hub.load(..., force_reload=True)&lt;/code&gt;. This will delete the existing github folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.</source>
          <target state="translated">ユーザーは、 &lt;code&gt;hub.load(..., force_reload=True)&lt;/code&gt; 呼び出すことにより、強制的にリロードできます。これにより、既存のgithubフォルダーとダウンロードされた重みが削除され、新しいダウンロードが再初期化されます。これは、更新が同じブランチに公開されている場合に役立ちます。ユーザーは最新のリリースについていくことができます。</target>
        </trans-unit>
        <trans-unit id="df3d36e2101db7508ee9c65bba6c79f4162eb560" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;DistributedDataParallel&lt;/code&gt; in conjunction with the &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt; is experimental and subject to change.</source>
          <target state="translated">&lt;code&gt;DistributedDataParallel&lt;/code&gt; をDistributedRPC &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Framework&lt;/a&gt;と組み合わせて使用することは実験的であり、変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="6f8c8c4de2dead884cf1cd1b1efe492df15bdbf3" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;torch.jit.trace&lt;/code&gt; and &lt;code&gt;torch.jit.trace_module&lt;/code&gt;, you can turn an existing module or Python function into a TorchScript &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.</source>
          <target state="translated">&lt;code&gt;torch.jit.trace&lt;/code&gt; および &lt;code&gt;torch.jit.trace_module&lt;/code&gt; を使用すると、既存のモジュールまたはPython関数を&lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; に変換でき&lt;/a&gt;ます。入力例を提供する必要があります。関数を実行して、すべてのテンソルで実行された操作を記録します。</target>
        </trans-unit>
        <trans-unit id="016141762b3eda50d55b6434765c656b21e2c21e" translate="yes" xml:space="preserve">
          <source>Using GPU tensors as arguments or return values of &lt;code&gt;func&lt;/code&gt; is not supported since we don&amp;rsquo;t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of &lt;code&gt;func&lt;/code&gt;.</source>
          <target state="translated">GPUテンソルをネットワーク経由で送信することはサポートされていないため、引数または &lt;code&gt;func&lt;/code&gt; の戻り値としてGPUテンソルを使用することはサポートされていません。GPUテンソルを引数または &lt;code&gt;func&lt;/code&gt; の戻り値として使用する前に、明示的にCPUにコピーする必要があります。</target>
        </trans-unit>
        <trans-unit id="40f8083d709ef1dfeba3266d73cd40bc2120c4c0" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute matrix norms:</source>
          <target state="translated">使用 &lt;code&gt;dim&lt;/code&gt; 計算行列ノルムに引数を：</target>
        </trans-unit>
        <trans-unit id="34eeb9d75aa34a25cf342ae4a398eead839c1594" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute vector norms:</source>
          <target state="translated">使用 &lt;code&gt;dim&lt;/code&gt; 計算ベクトル規範に引数を：</target>
        </trans-unit>
        <trans-unit id="892e242741e1c766ef8440ce7e813adfc429718b" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv2d&lt;/code&gt; modules.</source>
          <target state="translated">通常、入力は &lt;code&gt;nn.Conv2d&lt;/code&gt; モジュールから取得されます。</target>
        </trans-unit>
        <trans-unit id="62c50ebd5d624de117b146ade6f9d5575d770fc3" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv3d&lt;/code&gt; modules.</source>
          <target state="translated">通常、入力は &lt;code&gt;nn.Conv3d&lt;/code&gt; モジュールから取得されます。</target>
        </trans-unit>
        <trans-unit id="18fdc5ee8b1f8fba8dabaa933373c0483ab7fad7" translate="yes" xml:space="preserve">
          <source>Utilities</source>
          <target state="translated">Utilities</target>
        </trans-unit>
        <trans-unit id="ad96fcc3041d4053b9449ab4a648c6341e2217d7" translate="yes" xml:space="preserve">
          <source>Utility functions in other modules</source>
          <target state="translated">他のモジュールのユーティリティ機能</target>
        </trans-unit>
        <trans-unit id="1c41457151932f1c1abddf366a843f91dd5ea098" translate="yes" xml:space="preserve">
          <source>Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.</source>
          <target state="translated">単位の剪定を行わずに、1のマスクで剪定パラメタライズを生成するユーティリティ剪定メソッド。</target>
        </trans-unit>
        <trans-unit id="c9ee5681d3c59f7541c27a38b67edf46259e187b" translate="yes" xml:space="preserve">
          <source>V</source>
          <target state="translated">V</target>
        </trans-unit>
        <trans-unit id="1eafbd543ea2d8baa3df59414ec62e236836b78c" translate="yes" xml:space="preserve">
          <source>V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses: &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&lt;/a&gt;</source>
          <target state="translated">V. Balntas、et al。：三重項損失を伴う浅い畳み込み特徴記述子の学習：&lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;http&lt;/a&gt;：//www.bmva.org/bmvc/2016/papers/paper119/index.html</target>
        </trans-unit>
        <trans-unit id="2badedb167ecf8f97a1ffcdd2817401074fa728e" translate="yes" xml:space="preserve">
          <source>VGG</source>
          <target state="translated">VGG</target>
        </trans-unit>
        <trans-unit id="76dc2de0cc7c95272fc67f826f2dc5fa09c8ded2" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) from &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」の&lt;/a&gt;VGG11層モデル（構成「A」）</target>
        </trans-unit>
        <trans-unit id="efac66ad38738b85f5ef46a914f1b2bede87a54e" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">バッチ正規化を使用したVGG11層モデル（構成「A」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8ab10add11dcdf5fb182aad6d9f35aa59134c466" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 13層モデル（構成「B」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c68abe1f69acd7242c26b04b450c20879de247dc" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">バッチ正規化を使用したVGG13層モデル（構成「B」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7df2c09a84b7ee83b6d8493aa3dccf2e12cc714a" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 16層モデル（構成「D」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4979b51da2376b295925fa358ed83201a60846eb" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">バッチ正規化を使用したVGG16層モデル（構成「D」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="734385d469c0b7b16d39310aab3a223999973e9e" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;ldquo;E&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 19層モデル（構成「E」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b468de2d615302723a63a19cf8ef07ba2f87d240" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;lsquo;E&amp;rsquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">バッチ正規化を使用したVGG19層モデル（構成「E」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22dca5ed30facb9e021b2a416131b71d9addd5df" translate="yes" xml:space="preserve">
          <source>VGG-11</source>
          <target state="translated">VGG-11</target>
        </trans-unit>
        <trans-unit id="59c5eb8db8735266b63e11d68abfe0513f53e937" translate="yes" xml:space="preserve">
          <source>VGG-11 with batch normalization</source>
          <target state="translated">VGG-11 バッチ正規化機能付き</target>
        </trans-unit>
        <trans-unit id="275a0abbe0e323332b36f91fd1e0bbabaa98823b" translate="yes" xml:space="preserve">
          <source>VGG-13</source>
          <target state="translated">VGG-13</target>
        </trans-unit>
        <trans-unit id="00e8568346d410021e9a56f3b1b1f1983ac75021" translate="yes" xml:space="preserve">
          <source>VGG-13 with batch normalization</source>
          <target state="translated">VGG-13とバッチ正規化</target>
        </trans-unit>
        <trans-unit id="3fa5851c47708aca43af1f031237ec76652d5fb8" translate="yes" xml:space="preserve">
          <source>VGG-16</source>
          <target state="translated">VGG-16</target>
        </trans-unit>
        <trans-unit id="6794221fe94aa99e89cd4a3fc469cd560c5b1798" translate="yes" xml:space="preserve">
          <source>VGG-16 with batch normalization</source>
          <target state="translated">VGG-16 バッチ正規化機能付き</target>
        </trans-unit>
        <trans-unit id="e33f8119a3d18ad4bc21aa5913ffff22adbc62fa" translate="yes" xml:space="preserve">
          <source>VGG-19</source>
          <target state="translated">VGG-19</target>
        </trans-unit>
        <trans-unit id="6a06101d542844efc9851734dd33c0a3fcfb9071" translate="yes" xml:space="preserve">
          <source>VGG-19 with batch normalization</source>
          <target state="translated">VGG-19 バッチ正規化機能付き</target>
        </trans-unit>
        <trans-unit id="08ab4ecc000363002865da057bb07b708354e689" translate="yes" xml:space="preserve">
          <source>Valid operation names:</source>
          <target state="translated">有効な操作名。</target>
        </trans-unit>
        <trans-unit id="8ab2f6ea14647497320511c9699ad1fa98390d6e" translate="yes" xml:space="preserve">
          <source>Value associated with &lt;code&gt;key&lt;/code&gt; if &lt;code&gt;key&lt;/code&gt; is in the store.</source>
          <target state="translated">値に関連付けられた &lt;code&gt;key&lt;/code&gt; 場合 &lt;code&gt;key&lt;/code&gt; 店です。</target>
        </trans-unit>
        <trans-unit id="8f65e3050b4aa23d2b9ffeafc8ef420283f1bc31" translate="yes" xml:space="preserve">
          <source>Values looked up as attributes of a module are assumed to be constant:</source>
          <target state="translated">モジュールの属性としてルックアップされた値は、一定であると仮定します。</target>
        </trans-unit>
        <trans-unit id="766a55330fbdeceea3990ed650f19ff9d7ebc2b2" translate="yes" xml:space="preserve">
          <source>Vandermonde matrix. If increasing is False, the first column is</source>
          <target state="translated">バンデルモンデ行列。増加が False の場合、最初の列は</target>
        </trans-unit>
        <trans-unit id="8993fc586517fabcc3d0fce5c92e800dc4e3de15" translate="yes" xml:space="preserve">
          <source>Variable Resolution</source>
          <target state="translated">可変分解能</target>
        </trans-unit>
        <trans-unit id="ac018db1f7b00972061adff843d37497d8ee153c" translate="yes" xml:space="preserve">
          <source>Variables</source>
          <target state="translated">Variables</target>
        </trans-unit>
        <trans-unit id="b1c39119660f33e5be726c1652639e668fcdfcd8" translate="yes" xml:space="preserve">
          <source>Verifies that the given compiler is ABI-compatible with PyTorch.</source>
          <target state="translated">与えられたコンパイラが PyTorch と ABI 互換であることを確認します。</target>
        </trans-unit>
        <trans-unit id="991eeb7d6acd3a1b90daf8e907ad4a1126fbf67e" translate="yes" xml:space="preserve">
          <source>Via a string and device ordinal:</source>
          <target state="translated">文字列とデバイス序列を介して。</target>
        </trans-unit>
        <trans-unit id="78f371ae51565c626e223c43a305351baadcdfcb" translate="yes" xml:space="preserve">
          <source>Via a string:</source>
          <target state="translated">文字列を介して。</target>
        </trans-unit>
        <trans-unit id="3eb4e2b65c3b6adcd10d477d9e1ded0579505479" translate="yes" xml:space="preserve">
          <source>Video classification</source>
          <target state="translated">動画の分類</target>
        </trans-unit>
        <trans-unit id="61ad6c7f7397fbdc6a6513e489917c12a6953f11" translate="yes" xml:space="preserve">
          <source>View this tensor as the same size as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.view_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.view(other.size())&lt;/code&gt;.</source>
          <target state="translated">このテンソルを &lt;code&gt;other&lt;/code&gt; テンソルと同じサイズとして表示します。 &lt;code&gt;self.view_as(other)&lt;/code&gt; は &lt;code&gt;self.view(other.size())&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="68b460da28c94fa06be12e6f2242c79c2eca8bd2" translate="yes" xml:space="preserve">
          <source>Vision Layers</source>
          <target state="translated">ビジョンレイヤー</target>
        </trans-unit>
        <trans-unit id="8cd7ed1352bc6bdd613ec698de479a989aa9357b" translate="yes" xml:space="preserve">
          <source>Vision functions</source>
          <target state="translated">ビジョン機能</target>
        </trans-unit>
        <trans-unit id="e2415cb7f63df0c9de23362326ad3c37a9adfc96" translate="yes" xml:space="preserve">
          <source>W</source>
          <target state="translated">W</target>
        </trans-unit>
        <trans-unit id="46965f2770b3f862f5982cb5b877918de164026c" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}</source>
          <target state="translated">W_{out}=(W_{in}-1)♪ ♪times \times</target>
        </trans-unit>
        <trans-unit id="5b7347a76af1465d2b26baeed335756321992f36" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06cfb3f342da0b25161bf11f15bf2d81bfb676a7" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="144094cbaa937780b12d5cb8356e1fc4a6c692aa" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23da97705d0e8d5a571139859c1eefc7a25b59f7" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}</source>
          <target state="translated">W_{out}=W_{in}+\text{padding_left}+\text{padding_right}.</target>
        </trans-unit>
        <trans-unit id="bf2c9a7a90367be7032bd4f58614c0c9ad3779f8" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} \times \text{upscale\_factor}</source>
          <target state="translated">W_{out}=W_{in}\times ﾃｷｽﾄ{upscale_factor}.</target>
        </trans-unit>
        <trans-unit id="c5a97d192d1067a74706f036b87ed2b96b8d5895" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{times ≫ ≪Text{scale_factor}≫</target>
        </trans-unit>
        <trans-unit id="dd2204cd0212ab9c7330f7c2e6a0b40cae1ae693" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]} \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+2*\times (Text{kernel_size[1]}-1)-1}{Text{stride[1]}}+1\rightrfloor</target>
        </trans-unit>
        <trans-unit id="876c8e0e601e7178678d337712548c5cc7269f61" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+2 \times ≪W_{in}≫+2 ≪W_{in}≫ ≪W_{in}≫ ≪W_{out}≫ ≪W_{out}≫ ≪W_{out}≫+2 ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫</target>
        </trans-unit>
        <trans-unit id="6e0657f1536a7a37c826480069677e9010835607" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+2 ≪W_{in}≫+2 ≪Times ≫ ≪Text{padding}[1]-≪Text{kernel_size}[1]}≪Text{stride}[1]}+1 Right\rfloor</target>
        </trans-unit>
        <trans-unit id="a11c0191402c7be2c31a4ea7dcec53e255776ef6" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+≪W_{in}≫+2 \times ≪Text{padding}[2]-≪Text{dilation}[2]≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{out}≫+2 ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫</target>
        </trans-unit>
        <trans-unit id="b4191f6bab2de7c1a0f2413fa95cad94e0003ac5" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+≪W_{in}+2 \times ≫ ≪Text{padding}[2]-≫ ≪Text{kernel_size}[2]}≪Text{stride}[2]}+1 right\rheight®floor</target>
        </trans-unit>
        <trans-unit id="90b3ee06862d0af7ed835a4f57d3b9839c234435" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫-≪W_{in}≫-≪W_{in}≫-≪W_{in][1]}≪W_{in][1]}+1right\rfloor</target>
        </trans-unit>
        <trans-unit id="02d96942e51e668861c82b6ad6a1888bbaffdfc3" translate="yes" xml:space="preserve">
          <source>Waits for all provided futures to be complete, and returns the list of completed values.</source>
          <target state="translated">提供されたすべての先物が完了するのを待ち、完了した値のリストを返します。</target>
        </trans-unit>
        <trans-unit id="0a15d8c4d33065db8154fb49b0e2d2a98624af19" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store, and throws an exception if the keys have not been set by the supplied &lt;code&gt;timeout&lt;/code&gt;.</source>
          <target state="translated">各キーを待つ &lt;code&gt;keys&lt;/code&gt; ストアに追加すること、およびキーを供給することによって設定されていない場合に例外をスロー &lt;code&gt;timeout&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="24a824e1b1fcd89978ae586133c59626ca034951" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store. If not all keys are set before the &lt;code&gt;timeout&lt;/code&gt; (set during store initialization), then &lt;code&gt;wait&lt;/code&gt; will throw an exception.</source>
          <target state="translated">キーの各キーがストアに追加されるのを待ち &lt;code&gt;keys&lt;/code&gt; 。 &lt;code&gt;timeout&lt;/code&gt; （ストアの初期化中に設定）の前にすべてのキーが設定されていない場合、 &lt;code&gt;wait&lt;/code&gt; は例外をスローします。</target>
        </trans-unit>
        <trans-unit id="e9c45563358e813f157ba81b33143542165ba84e" translate="yes" xml:space="preserve">
          <source>Warning</source>
          <target state="translated">Warning</target>
        </trans-unit>
        <trans-unit id="322f5a9cd5158cc9dec2b3e7da850004249c9e4e" translate="yes" xml:space="preserve">
          <source>We accumulate the gradients in the appropriate &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;torch.distributed.autograd.context&lt;/code&gt;&lt;/a&gt; on each of the nodes. The autograd context to be used is looked up given the &lt;code&gt;context_id&lt;/code&gt; that is passed in when &lt;a href=&quot;#torch.distributed.autograd.backward&quot;&gt;&lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt;&lt;/a&gt; is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; API.</source>
          <target state="translated">各ノードの適切な&lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt; &lt;code&gt;torch.distributed.autograd.context&lt;/code&gt; &lt;/a&gt;にグラデーションを蓄積します。使用されるautogradコンテキストは、&lt;a href=&quot;#torch.distributed.autograd.backward&quot;&gt; &lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt; &lt;/a&gt;が呼び出されたときに渡される &lt;code&gt;context_id&lt;/code&gt; を指定して検索されます。指定されたIDに対応する有効なautogradコンテキストがない場合、エラーをスローします。&lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt; &lt;code&gt;get_gradients()&lt;/code&gt; &lt;/a&gt; APIを使用して、累積されたグラデーションを取得できます。</target>
        </trans-unit>
        <trans-unit id="db56b28279fe3aea1467a7671b69e8c448f2218c" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt;&lt;code&gt;torch.nn.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt; &lt;code&gt;torch.nn.Linear&lt;/code&gt; &lt;/a&gt;と同じインターフェースを採用しています。</target>
        </trans-unit>
        <trans-unit id="3f8d51566ad98edb597de3ba663338465830f744" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv2d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv2d&quot;&gt; &lt;code&gt;torch.nn.quantized.Conv2d&lt;/code&gt; &lt;/a&gt;と同じインターフェースを採用しています。</target>
        </trans-unit>
        <trans-unit id="2e48e5f00566ef4a4b3bcb10eab8d225acffc2e4" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv3d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv3d&quot;&gt; &lt;code&gt;torch.nn.quantized.Conv3d&lt;/code&gt; &lt;/a&gt;と同じインターフェースを採用しています。</target>
        </trans-unit>
        <trans-unit id="a3fb7d904d476590d98359b7241880b1be2f9a76" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Linear&quot;&gt;&lt;code&gt;torch.nn.quantized.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Linear&quot;&gt; &lt;code&gt;torch.nn.quantized.Linear&lt;/code&gt; &lt;/a&gt;と同じインターフェースを採用しています。</target>
        </trans-unit>
        <trans-unit id="9fb0293b1874dcfe3feb591ccd49a1edaf2d5f3f" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Conv2d&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&quot;&gt;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&lt;/a&gt; for documentation.</source>
          <target state="translated">&lt;code&gt;torch.nn.Conv2d&lt;/code&gt; と同じインターフェースを採用しています。ドキュメントについては、&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&quot;&gt;https：&lt;/a&gt;//pytorch.org/docs/stable/nn.html？highlight = conv2d＃torch.nn.Conv2dを参照してください。</target>
        </trans-unit>
        <trans-unit id="27cfadeddc9160f1498530cc3eb1f719bfd8434a" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Linear&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt; for documentation.</source>
          <target state="translated">&lt;code&gt;torch.nn.Linear&lt;/code&gt; と同じインターフェースを採用しています。ドキュメントについては、&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https：&lt;/a&gt;//pytorch.org/docs/stable/nn.html#torch.nn.Linearを参照してください。</target>
        </trans-unit>
        <trans-unit id="5105f862983064b0cf7594afba8d668c8f56e9ff" translate="yes" xml:space="preserve">
          <source>We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements of a part of a model. Checkout this example:</source>
          <target state="translated">トレーシングとスクリプトの混在が可能です。モデルの一部の特定の要件に合わせて、トレースとスクリプトを構成することができます。この例をチェックしてみてください。</target>
        </trans-unit>
        <trans-unit id="6defebafebb2e25f76bcf411ab1ec455c032f365" translate="yes" xml:space="preserve">
          <source>We also do not support the following subsystems, though some may work out of the box:</source>
          <target state="translated">また、以下のサブシステムもサポートしていません。</target>
        </trans-unit>
        <trans-unit id="3fa9a188f6de578d3bee11e42ad3ca8321060293" translate="yes" xml:space="preserve">
          <source>We can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with &lt;code&gt;torch.cat&lt;/code&gt;:</source>
          <target state="translated">インプレース更新を使用しないようにコードを変更することでこれを修正できますが、 &lt;code&gt;torch.cat&lt;/code&gt; を使用して結果テンソルをアウトオブプレースで構築します。</target>
        </trans-unit>
        <trans-unit id="7aac442ad82fddd2074ccf19e4043ee60a7cdfb0" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; の&lt;/a&gt;インターフェースを組み合わせました。</target>
        </trans-unit>
        <trans-unit id="4304f8e1f80a53c4c4788f88c99ef4dcb6d0bc02" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt;&lt;code&gt;torch.nn.ReLU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt; &lt;code&gt;torch.nn.ReLU&lt;/code&gt; &lt;/a&gt;のインターフェースを組み合わせました。</target>
        </trans-unit>
        <trans-unit id="94fba3bcda49d77ec84730c68d223d594ab23cb3" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt; の&lt;/a&gt;インターフェースを組み合わせました。</target>
        </trans-unit>
        <trans-unit id="a068c518b2d5e22e14c479569a01c9c9d06bceec" translate="yes" xml:space="preserve">
          <source>We highly recommend taking a look at the original paper for more details.</source>
          <target state="translated">詳しくは原紙をご覧になることを強くお勧めします。</target>
        </trans-unit>
        <trans-unit id="86ad4838215218ac000a7d87b4b48e01c6961c5f" translate="yes" xml:space="preserve">
          <source>We provide models for action recognition pre-trained on Kinetics-400. They have all been trained with the scripts provided in &lt;code&gt;references/video_classification&lt;/code&gt;.</source>
          <target state="translated">Kinetics-400で事前トレーニングされた行動認識のモデルを提供します。それらはすべて、 &lt;code&gt;references/video_classification&lt;/code&gt; で提供されるスクリプトでトレーニングされています。</target>
        </trans-unit>
        <trans-unit id="e921c07692e5a21fc62c5f94070c73cf433f0bb7" translate="yes" xml:space="preserve">
          <source>We provide pre-trained models, using the PyTorch &lt;a href=&quot;../model_zoo#module-torch.utils.model_zoo&quot;&gt;&lt;code&gt;torch.utils.model_zoo&lt;/code&gt;&lt;/a&gt;. These can be constructed by passing &lt;code&gt;pretrained=True&lt;/code&gt;:</source>
          <target state="translated">PyTorch &lt;a href=&quot;../model_zoo#module-torch.utils.model_zoo&quot;&gt; &lt;code&gt;torch.utils.model_zoo&lt;/code&gt; &lt;/a&gt;を使用して、事前にトレーニングされたモデルを提供します。これらは、 &lt;code&gt;pretrained=True&lt;/code&gt; を渡すことで構築できます：</target>
        </trans-unit>
        <trans-unit id="62c9fd377e2aad0f66ac5c26c69e0a5867347f36" translate="yes" xml:space="preserve">
          <source>We provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons.</source>
          <target state="translated">純粋なPythonプログラムから、スタンドアロンのC++プログラムなど、Pythonとは独立して動作するTorchScriptプログラムにモデルをインクリメンタルに移行させるためのツールを提供します。これにより、Pythonの使い慣れたツールを使ってPyTorchでモデルを学習し、Pythonプログラムがパフォーマンスやマルチスレッドの理由で不利になる可能性がある本番環境にTorchScript経由でモデルをエクスポートすることが可能になります。</target>
        </trans-unit>
        <trans-unit id="bd73422aa80da87d8ae75185414e27781227d850" translate="yes" xml:space="preserve">
          <source>We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.</source>
          <target state="translated">提供されたルートを使用して、autogradグラフを発見し、適切な依存関係を計算します。このメソッドは、autogradの計算全体が完了するまでブロックします。</target>
        </trans-unit>
        <trans-unit id="48996658259127412cd98e4a4e7b4a204e6d1b0a" translate="yes" xml:space="preserve">
          <source>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by &lt;code&gt;name&lt;/code&gt; (e.g. &lt;code&gt;'weight'&lt;/code&gt;) with two parameters: one specifying the magnitude (e.g. &lt;code&gt;'weight_g'&lt;/code&gt;) and one specifying the direction (e.g. &lt;code&gt;'weight_v'&lt;/code&gt;). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every &lt;code&gt;forward()&lt;/code&gt; call.</source>
          <target state="translated">重みの正規化は、重みテンソルの大きさをその方向から切り離す再パラメーター化です。これにより、 &lt;code&gt;name&lt;/code&gt; 指定されたパラメーター（例： &lt;code&gt;'weight'&lt;/code&gt; ）が2つのパラメーターに置き換えられます。1つは大きさを指定し（例： &lt;code&gt;'weight_g'&lt;/code&gt; ）、もう1つは方向を指定します（例： &lt;code&gt;'weight_v'&lt;/code&gt; ）。重みの正規化は、すべての &lt;code&gt;forward()&lt;/code&gt; 呼び出しの前に、大きさと方向から重みテンソルを再計算するフックを介して実装されます。</target>
        </trans-unit>
        <trans-unit id="d53879f401fe4a3643952a897298dc95fdaf8d7e" translate="yes" xml:space="preserve">
          <source>Weight:</source>
          <target state="translated">Weight:</target>
        </trans-unit>
        <trans-unit id="d4a914f8dcec6b172849d1c8fb16401fbdbb7604" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when &lt;code&gt;align_corners = False&lt;/code&gt;. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at &lt;code&gt;-1&lt;/code&gt;. From version 1.3.0, under &lt;code&gt;align_corners = True&lt;/code&gt; all grid points along a unit dimension are considered to be at &lt;code&gt;`0&lt;/code&gt; (the center of the input image).</source>
          <target state="translated">&lt;code&gt;align_corners = True&lt;/code&gt; 場合、1Dデータでの2Dアフィン変換と2Dデータでの3Dアフィン変換（つまり、空間次元の1つに単位サイズがある場合）は明確に定義されておらず、意図された使用例ではありません。 &lt;code&gt;align_corners = False&lt;/code&gt; 場合、これは問題ではありません。バージョン1.2.0までは、単位寸法に沿ったすべてのグリッドポイントは任意に &lt;code&gt;-1&lt;/code&gt; であると見なされていました。バージョン1.3.0以降、 &lt;code&gt;align_corners = True&lt;/code&gt; 、単位寸法に沿ったすべてのグリッドポイントは &lt;code&gt;`0&lt;/code&gt; （入力画像の中心）にあると見なされます。</target>
        </trans-unit>
        <trans-unit id="71f1f4c1d858ca7758a6c256f77db87995af50dc" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was &lt;code&gt;align_corners = True&lt;/code&gt;. Since then, the default behavior has been changed to &lt;code&gt;align_corners = False&lt;/code&gt;, in order to bring it in line with the default for &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;interpolate()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">場合 &lt;code&gt;align_corners = True&lt;/code&gt; 、グリッド位置は、入力画像サイズに画素サイズの相対的に依存し、によってサンプリング位置に&lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt; &lt;code&gt;grid_sample()&lt;/code&gt; は、&lt;/a&gt;（アップサンプリングまたはダウンサンプリングされた後に）異なる解像度で与えられた同じ入力に対して異なるであろう。バージョン1.2.0までのデフォルトの動作は &lt;code&gt;align_corners = True&lt;/code&gt; でした。それ以降、&lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;interpolate()&lt;/code&gt; の&lt;/a&gt;デフォルトと一致させるために、デフォルトの動作が &lt;code&gt;align_corners = False&lt;/code&gt; 変更されました。</target>
        </trans-unit>
        <trans-unit id="19d86416250dfee41099689ccdc474523b9de05a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;compute_uv&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, backward cannot be performed since &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; from the forward pass is required for the backward operation.</source>
          <target state="translated">&lt;code&gt;compute_uv&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; の場合、フォワードパスからの &lt;code&gt;U&lt;/code&gt; と &lt;code&gt;V&lt;/code&gt; がバックワード操作に必要であるため、バックワードを実行できません。</target>
        </trans-unit>
        <trans-unit id="2e150d7511a776c744a811c73ef08944e9bb6434" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;dim&lt;/code&gt; is given, a squeeze operation is done only in the given dimension. If &lt;code&gt;input&lt;/code&gt; is of shape:</source>
          <target state="translated">とき &lt;code&gt;dim&lt;/code&gt; が与えられ、スクイズ操作だけ与えられた次元で行われます。 &lt;code&gt;input&lt;/code&gt; が形状の場合：</target>
        </trans-unit>
        <trans-unit id="b1120f7dfd992f23bbea9418ec57a1af2fe9af61" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a scalar value, the operation applied is:</source>
          <target state="translated">場合 &lt;code&gt;exponent&lt;/code&gt; スカラー値であり、動作は適用しました。</target>
        </trans-unit>
        <trans-unit id="66481b64a8fde71d4312aa50639d9ce101db880a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the operation applied is:</source>
          <target state="translated">とき &lt;code&gt;exponent&lt;/code&gt; テンソルで、操作は、ISを適用します：</target>
        </trans-unit>
        <trans-unit id="25f39b6b9d40581f69f611ee72f68c18106be3d1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;exponent&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">とき &lt;code&gt;exponent&lt;/code&gt; テンソルである、の形状 &lt;code&gt;input&lt;/code&gt; と &lt;code&gt;exponent&lt;/code&gt; でなければなりません&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2903dec45ead9638a2d9f01c40302928711917b1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;groups == in_channels&lt;/code&gt; and &lt;code&gt;out_channels == K * in_channels&lt;/code&gt;, where &lt;code&gt;K&lt;/code&gt; is a positive integer, this operation is also termed in literature as depthwise convolution.</source>
          <target state="translated">場合 &lt;code&gt;groups == in_channels&lt;/code&gt; と &lt;code&gt;out_channels == K * in_channels&lt;/code&gt; 、 &lt;code&gt;K&lt;/code&gt; は正の整数であり、この操作はまた、奥行き畳み込みとして文献に呼ばれます。</target>
        </trans-unit>
        <trans-unit id="c1b8e1328795e35aa33cb3682bf9e0a128163cf6" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;input&lt;/code&gt; is on CUDA, &lt;a href=&quot;#torch.nonzero&quot;&gt;&lt;code&gt;torch.nonzero()&lt;/code&gt;&lt;/a&gt; causes host-device synchronization.</source>
          <target state="translated">場合 &lt;code&gt;input&lt;/code&gt; CUDAである、&lt;a href=&quot;#torch.nonzero&quot;&gt; &lt;code&gt;torch.nonzero()&lt;/code&gt; &lt;/a&gt;ホスト・デバイスの同期化を引き起こします。</target>
        </trans-unit>
        <trans-unit id="9fd8de791261b8fe5e3fd820c55e05ef945d4e88" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;module&lt;/code&gt; returns a scalar (i.e., 0-dimensional tensor) in &lt;code&gt;forward()&lt;/code&gt;, this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</source>
          <target state="translated">場合 &lt;code&gt;module&lt;/code&gt; スカラー（すなわち、0次元テンソル）を返す &lt;code&gt;forward()&lt;/code&gt; 、このラッパーは、各デバイスからの結果を含む、データを並列に使用されるデバイスの数に等しい長さのベクトルを返します。</target>
        </trans-unit>
        <trans-unit id="14811e48d579473d679dcda9d3180b5365c2423a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shape of &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor</source>
          <target state="translated">ときに &lt;code&gt;other&lt;/code&gt; テンソルである、の形状 &lt;code&gt;other&lt;/code&gt; でなければならない&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;下地テンソルの形状に</target>
        </trans-unit>
        <trans-unit id="6d6234d9ff0cd3ff058351e570bff6a3e90e8d1e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;other&lt;/code&gt; がテンソルである場合、 &lt;code&gt;input&lt;/code&gt; および &lt;code&gt;other&lt;/code&gt; の形状は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能でなければなりません。</target>
        </trans-unit>
        <trans-unit id="2b23daf961076c6f771f3b98153804afb65cab2e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;size&lt;/code&gt; is given, it is the output size of the image &lt;code&gt;(h, w)&lt;/code&gt;.</source>
          <target state="translated">場合 &lt;code&gt;size&lt;/code&gt; 与えられ、それは画像の出力サイズである &lt;code&gt;(h, w)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d240a83215b54cebb671916345cdadc2427a5a89" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;some&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, the gradients on &lt;code&gt;U[..., :, min(m, n):]&lt;/code&gt; and &lt;code&gt;V[..., :, min(m, n):]&lt;/code&gt; will be ignored in backward as those vectors can be arbitrary bases of the subspaces.</source>
          <target state="translated">ときに &lt;code&gt;some&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; を、上の勾配 &lt;code&gt;U[..., :, min(m, n):]&lt;/code&gt; と &lt;code&gt;V[..., :, min(m, n):]&lt;/code&gt; これらのベクターは、できる限り後方に無視されます部分空間の任意の基底である。</target>
        </trans-unit>
        <trans-unit id="85caf16edf789cb76e0cd678a0b93ec725f0105b" translate="yes" xml:space="preserve">
          <source>When a model is trained on &lt;code&gt;M&lt;/code&gt; nodes with &lt;code&gt;batch=N&lt;/code&gt;, the gradient will be &lt;code&gt;M&lt;/code&gt; times smaller when compared to the same model trained on a single node with &lt;code&gt;batch=M*N&lt;/code&gt; (because the gradients between different nodes are averaged). You should take this into consideration when you want to obtain a mathematically equivalent training process compared to the local training counterpart.</source>
          <target state="translated">モデルが &lt;code&gt;batch=N&lt;/code&gt; の &lt;code&gt;M&lt;/code&gt; ノードでトレーニングされる場合、 &lt;code&gt;batch=M*N&lt;/code&gt; の単一ノードでトレーニングされた同じモデルと比較すると、勾配は &lt;code&gt;M&lt;/code&gt; 分の1になります（異なるノード間の勾配が平均化されるため）。ローカルトレーニングの対応物と比較して数学的に同等のトレーニングプロセスを取得したい場合は、これを考慮に入れる必要があります。</target>
        </trans-unit>
        <trans-unit id="bc06fbb059ebdabdac77c068a81b8512376f9ddd" translate="yes" xml:space="preserve">
          <source>When called with &lt;code&gt;dims&lt;/code&gt; of the list form, the given dimensions will be contracted in place of the last</source>
          <target state="translated">呼ばれると &lt;code&gt;dims&lt;/code&gt; 、リスト形式の、与えられた寸法は、最後の代わりに収縮されます</target>
        </trans-unit>
        <trans-unit id="8f0991d5226cadceec16a5e2277a3d90d9a71503" translate="yes" xml:space="preserve">
          <source>When called with a non-negative integer argument &lt;code&gt;dims&lt;/code&gt; =</source>
          <target state="translated">負でない整数の引数で呼び出された場合 &lt;code&gt;dims&lt;/code&gt; =</target>
        </trans-unit>
        <trans-unit id="6abce014373fbb8ba1a8a6700a958f442096752d" translate="yes" xml:space="preserve">
          <source>When combined with TorchScript decorators, this decorator must be the outmost one.</source>
          <target state="translated">TorchScriptのデコレータと組み合わせる場合は、このデコレータを一番外にしなければなりません。</target>
        </trans-unit>
        <trans-unit id="01692078c2de1ab235032349659ea1bad48180a7" translate="yes" xml:space="preserve">
          <source>When combined with static or class method, this decorator must be the inner one.</source>
          <target state="translated">静的メソッドやクラスメソッドと組み合わせる場合、このデコレータは内側のものでなければなりません。</target>
        </trans-unit>
        <trans-unit id="24460dec44dc8e232523a0b3b6cffc083f18ab5b" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.Tensor.new_tensor&quot;&gt;&lt;code&gt;new_tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;tensor.new_tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;tensor.new_tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="translated">データがテンソル &lt;code&gt;x&lt;/code&gt; の場合、&lt;a href=&quot;#torch.Tensor.new_tensor&quot;&gt; &lt;code&gt;new_tensor()&lt;/code&gt; &lt;/a&gt;は渡されたものから「データ」を読み取り、リーフ変数を作成します。したがって、 &lt;code&gt;tensor.new_tensor(x)&lt;/code&gt; は &lt;code&gt;x.clone().detach()&lt;/code&gt; と &lt;code&gt;tensor.new_tensor(x, requires_grad=True)&lt;/code&gt; あり、tensor.new_tensor（x、requires_grad = True）は &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt; と同等です。 &lt;code&gt;clone()&lt;/code&gt; と &lt;code&gt;detach()&lt;/code&gt; を使用した同等のものをお勧めします。</target>
        </trans-unit>
        <trans-unit id="44313813a50d98d5898f4bddc561c8a3109b1400" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;torch.tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;torch.tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="translated">データがテンソル &lt;code&gt;x&lt;/code&gt; の場合、&lt;a href=&quot;#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt;は渡されたものから「データ」を読み取り、リーフ変数を作成します。したがって、 &lt;code&gt;torch.tensor(x)&lt;/code&gt; は &lt;code&gt;x.clone().detach()&lt;/code&gt; と &lt;code&gt;torch.tensor(x, requires_grad=True)&lt;/code&gt; あり、torch.tensor（x、requires_grad = True）は &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt; と同等です。 &lt;code&gt;clone()&lt;/code&gt; と &lt;code&gt;detach()&lt;/code&gt; を使用した同等のものをお勧めします。</target>
        </trans-unit>
        <trans-unit id="4d87a694539e44d9bb0922817749f14565db02a2" translate="yes" xml:space="preserve">
          <source>When drawn without replacement, &lt;code&gt;num_samples&lt;/code&gt; must be lower than number of non-zero elements in &lt;code&gt;input&lt;/code&gt; (or the min number of non-zero elements in each row of &lt;code&gt;input&lt;/code&gt; if it is a matrix).</source>
          <target state="translated">置換せずに描画する場合、 &lt;code&gt;num_samples&lt;/code&gt; は、 &lt;code&gt;input&lt;/code&gt; の非ゼロ要素の数（または、行列の場合は &lt;code&gt;input&lt;/code&gt; 各行の非ゼロ要素の最小数）よりも小さくする必要があります。</target>
        </trans-unit>
        <trans-unit id="aeab232209859fdb56ecfdd5e27289109b3779cd" translate="yes" xml:space="preserve">
          <source>When given an image of &lt;code&gt;Channels x Height x Width&lt;/code&gt;, it will apply &lt;code&gt;Softmax&lt;/code&gt; to each location</source>
          <target state="translated">&lt;code&gt;Channels x Height x Width&lt;/code&gt; の画像が与えられると、各場所に &lt;code&gt;Softmax&lt;/code&gt; が適用されます</target>
        </trans-unit>
        <trans-unit id="9635ed5ff58b5f10f0a861bfe4da315b19a3d9ec" translate="yes" xml:space="preserve">
          <source>When manually importing this backend and invoking &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; with the corresponding backend name, the &lt;code&gt;torch.distributed&lt;/code&gt; package runs on the new backend.</source>
          <target state="translated">このバックエンドを手動でインポートし、対応するバックエンド名で&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt;を呼び出すと、 &lt;code&gt;torch.distributed&lt;/code&gt; パッケージが新しいバックエンドで実行されます。</target>
        </trans-unit>
        <trans-unit id="6c58952c2db520eb2d0c000e5b5179ffccb22124" translate="yes" xml:space="preserve">
          <source>When passed to the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;torch.jit.script&lt;/code&gt;&lt;/a&gt; function, a &lt;code&gt;torch.nn.Module&lt;/code&gt;&amp;rsquo;s data is copied to a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and the TorchScript compiler compiles the module. The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;, as well as any &lt;code&gt;@torch.jit.export&lt;/code&gt; methods.</source>
          <target state="translated">渡されたとき&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;torch.jit.script&lt;/code&gt; &lt;/a&gt;機能、 &lt;code&gt;torch.nn.Module&lt;/code&gt; のデータがコピーされ&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;とTorchScriptコンパイラは、モジュールをコンパイルします。モジュールの &lt;code&gt;forward&lt;/code&gt; はデフォルトでコンパイルされます。 &lt;code&gt;forward&lt;/code&gt; から呼び出されたメソッドは、 &lt;code&gt;@torch.jit.export&lt;/code&gt; メソッドと同様に、 &lt;code&gt;forward&lt;/code&gt; で使用される順序で遅延コンパイルされます。</target>
        </trans-unit>
        <trans-unit id="98975218294f0d2cf1d9e027d0c9f18c335eccc0" translate="yes" xml:space="preserve">
          <source>When running on CUDA, &lt;code&gt;row * col&lt;/code&gt; must be less than</source>
          <target state="translated">CUDAで実行する場合、 &lt;code&gt;row * col&lt;/code&gt; は以下でなければなりません</target>
        </trans-unit>
        <trans-unit id="6442e147505eb082b7d915b41a9300ebc0cdaab5" translate="yes" xml:space="preserve">
          <source>When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.</source>
          <target state="translated">scale_factorが指定されている場合、recompute_scale_factor=Trueの場合、scale_factorは出力サイズの計算に使用され、補間のための新しいスケールを推論するために使用されます。recompute_scale_factorのデフォルトの動作は1.6.0でFalseに変更され、補間計算ではscale_factorが使用されます。</target>
        </trans-unit>
        <trans-unit id="6134dc991dfe53224afc4b25750c288358d06db2" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;divisor&lt;/code&gt; tensor contains no zero elements, then &lt;code&gt;fold&lt;/code&gt; and &lt;code&gt;unfold&lt;/code&gt; operations are inverses of each other (up to constant divisor).</source>
          <target state="translated">場合 &lt;code&gt;divisor&lt;/code&gt; テンソルは全くゼロ要素を含まず、その後 &lt;code&gt;fold&lt;/code&gt; と &lt;code&gt;unfold&lt;/code&gt; 操作はお互い（定数除数まで）の逆数です。</target>
        </trans-unit>
        <trans-unit id="ea57553addc94dbd66c0aab4dca41d371d835f2e" translate="yes" xml:space="preserve">
          <source>When the dtypes of inputs to an arithmetic operation (&lt;code&gt;add&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;) differ, we promote by finding the minimum dtype that satisfies the following rules:</source>
          <target state="translated">算術演算（ &lt;code&gt;add&lt;/code&gt; 、 &lt;code&gt;sub&lt;/code&gt; 、 &lt;code&gt;div&lt;/code&gt; 、 &lt;code&gt;mul&lt;/code&gt; ）への入力のdtypeが異なる場合、次のルールを満たす最小のdtypeを見つけることによって促進します。</target>
        </trans-unit>
        <trans-unit id="b36d5c15d9cebdd9841a3cbecf930ef5be3e8fb3" translate="yes" xml:space="preserve">
          <source>When the input Tensor is a sparse tensor then the unspecifed values are treated as &lt;code&gt;-inf&lt;/code&gt;.</source>
          <target state="translated">入力テンソルがスパーステンソルの場合、指定されていない値は &lt;code&gt;-inf&lt;/code&gt; として扱われます。</target>
        </trans-unit>
        <trans-unit id="adc8c2e11d80dad257b6a86b35591121148d959c" translate="yes" xml:space="preserve">
          <source>When the shapes do not match, the shape of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; is used as the shape for the returned output tensor</source>
          <target state="translated">形状が一致しない場合、&lt;a href=&quot;torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt;の形状が返される出力テンソルの形状として使用されます</target>
        </trans-unit>
        <trans-unit id="7a635c724cadbffe9cd8e217f6606360c059e6c0" translate="yes" xml:space="preserve">
          <source>When using &lt;a href=&quot;#torch.utils.cpp_extension.BuildExtension&quot;&gt;&lt;code&gt;BuildExtension&lt;/code&gt;&lt;/a&gt;, it is allowed to supply a dictionary for &lt;code&gt;extra_compile_args&lt;/code&gt; (rather than the usual list) that maps from languages (&lt;code&gt;cxx&lt;/code&gt; or &lt;code&gt;nvcc&lt;/code&gt;) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.</source>
          <target state="translated">&lt;a href=&quot;#torch.utils.cpp_extension.BuildExtension&quot;&gt; &lt;code&gt;BuildExtension&lt;/code&gt; &lt;/a&gt;を使用する場合、言語（ &lt;code&gt;cxx&lt;/code&gt; または &lt;code&gt;nvcc&lt;/code&gt; ）からコンパイラーに提供する追加のコンパイラーフラグのリストにマップする &lt;code&gt;extra_compile_args&lt;/code&gt; （通常のリストではなく）の辞書を提供できます。これにより、混合コンパイル中にC ++コンパイラとCUDAコンパイラに異なるフラグを指定できます。</target>
        </trans-unit>
        <trans-unit id="7635dd8102bfabbafa2a3b5c97436e5cb54d9ba1" translate="yes" xml:space="preserve">
          <source>When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;Reproducibility&lt;/a&gt; for background.</source>
          <target state="translated">CUDAバックエンドを使用する場合、この操作は、簡単にオフにできないバックワードパスで非決定的な動作を引き起こす可能性があります。背景については、&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;再現性&lt;/a&gt;に関する注記を参照してください。</target>
        </trans-unit>
        <trans-unit id="79561f3cf985a5e4719b8247ff531adef7e29706" translate="yes" xml:space="preserve">
          <source>When writing TorchScript directly using &lt;code&gt;@torch.jit.script&lt;/code&gt; decorator, the programmer must only use the subset of Python supported in TorchScript. This section documents what is supported in TorchScript as if it were a language reference for a stand alone language. Any features of Python not mentioned in this reference are not part of TorchScript. See &lt;code&gt;Builtin Functions&lt;/code&gt; for a complete reference of available Pytorch tensor methods, modules, and functions.</source>
          <target state="translated">&lt;code&gt;@torch.jit.script&lt;/code&gt; デコレータを使用してTorchScriptを直接記述する場合、プログラマはTorchScriptでサポートされているPythonのサブセットのみを使用する必要があります。このセクションでは、TorchScriptでサポートされているものを、スタンドアロン言語の言語リファレンスであるかのように説明します。このリファレンスに記載されていないPythonの機能は、TorchScriptの一部ではありません。使用可能なPytorchテンソルのメソッド、モジュール、および関数の完全なリファレンスについては、組み込み &lt;code&gt;Builtin Functions&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="50bf66ea03e8d3f7a8a4f3f33c0d423a99cbb251" translate="yes" xml:space="preserve">
          <source>When you call &lt;a href=&quot;#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt; on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call &lt;code&gt;torch.load(.., map_location='cpu')&lt;/code&gt; and then &lt;code&gt;load_state_dict()&lt;/code&gt; to avoid GPU RAM surge when loading a model checkpoint.</source>
          <target state="translated">GPUテンソルを含むファイルで&lt;a href=&quot;#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt;を呼び出すと、それらのテンソルはデフォルトでGPUにロードされます。あなたは呼び出すことができます &lt;code&gt;torch.load(.., map_location='cpu')&lt;/code&gt; 、その後 &lt;code&gt;load_state_dict()&lt;/code&gt; モデルのチェックポイントをロードするときにGPU RAMのサージを避けるために。</target>
        </trans-unit>
        <trans-unit id="d4b9a73359398be175a62d4f2cbf23eca1eb7394" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ここで、 &lt;code&gt;n = prod(s)&lt;/code&gt; は論理FFTサイズです。同じ正規化モードで後方変換（&lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt;）を呼び出すと、2つの変換間に &lt;code&gt;1/n&lt;/code&gt; の全体的な正規化が適用されます。これは、&lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; を&lt;/a&gt;完全に逆にするために必要です。</target>
        </trans-unit>
        <trans-unit id="540833bd1b024f45b94c5fe10bdcce398da41e2a" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ここで、 &lt;code&gt;n = prod(s)&lt;/code&gt; は論理FFTサイズです。同じ正規化モードで逆方向変換（&lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt;）を呼び出すと、2つの変換間に &lt;code&gt;1/n&lt;/code&gt; の全体的な正規化が適用されます。これは、&lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; を&lt;/a&gt;完全に逆にするために必要です。</target>
        </trans-unit>
        <trans-unit id="4fa609678777cb0f1d2bea5c36804f3e7f699ab3" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ここで、 &lt;code&gt;n = prod(s)&lt;/code&gt; は論理IFFTサイズです。同じ正規化モードで順方向変換（&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt;）を呼び出すと、2つの変換間に &lt;code&gt;1/n&lt;/code&gt; の全体的な正規化が適用されます。これは、&lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; を&lt;/a&gt;完全に逆にするために必要です。</target>
        </trans-unit>
        <trans-unit id="983301015460563605882b2d43f60b6f31ca13f7" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ここで、 &lt;code&gt;n = prod(s)&lt;/code&gt; は論理IFFTサイズです。同じ正規化モードで順方向変換（&lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;）を呼び出すと、2つの変換間に &lt;code&gt;1/n&lt;/code&gt; の全体的な正規化が適用されます。これは、&lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; を&lt;/a&gt;完全に逆にするために必要です。</target>
        </trans-unit>
        <trans-unit id="dcabf659c01386b96c4252ad5247bb9f50b66574" translate="yes" xml:space="preserve">
          <source>Where are my downloaded models saved?</source>
          <target state="translated">ダウンロードしたモデルはどこに保存されていますか?</target>
        </trans-unit>
        <trans-unit id="017d4a3464bb8236ca4dfe2a53c2f262d76de8ab" translate="yes" xml:space="preserve">
          <source>Which backend to use?</source>
          <target state="translated">どのバックエンドを使うか?</target>
        </trans-unit>
        <trans-unit id="3631991286d3139c1de9a41ef4982f8e5d886ce2" translate="yes" xml:space="preserve">
          <source>Which produces:</source>
          <target state="translated">どっちが産むんだ?</target>
        </trans-unit>
        <trans-unit id="a68a67a970d91d390715c4a5723442211582200e" translate="yes" xml:space="preserve">
          <source>While Loops</source>
          <target state="translated">Whileループ</target>
        </trans-unit>
        <trans-unit id="48f3d5bde3297b3b923d0f7759d754056d7f6c40" translate="yes" xml:space="preserve">
          <source>While it is assumed that &lt;code&gt;A&lt;/code&gt; is symmetric, &lt;code&gt;A.grad&lt;/code&gt; is not. To make sure that &lt;code&gt;A.grad&lt;/code&gt; is symmetric, so that &lt;code&gt;A - t * A.grad&lt;/code&gt; is symmetric in first-order optimization routines, prior to running &lt;code&gt;lobpcg&lt;/code&gt; we do the following symmetrization map: &lt;code&gt;A -&amp;gt; (A + A.t()) / 2&lt;/code&gt;. The map is performed only when the &lt;code&gt;A&lt;/code&gt; requires gradients.</source>
          <target state="translated">&lt;code&gt;A&lt;/code&gt; は対称であると想定されていますが、 &lt;code&gt;A.grad&lt;/code&gt; は対称ではありません。 &lt;code&gt;A.grad&lt;/code&gt; が対称であることを確認し、 &lt;code&gt;A - t * A.grad&lt;/code&gt; が1次最適化ルーチンで対称になるようにするには、 &lt;code&gt;lobpcg&lt;/code&gt; を実行する前に、次の対称化マップを実行します &lt;code&gt;A -&amp;gt; (A + A.t()) / 2&lt;/code&gt; 。マップは、 &lt;code&gt;A&lt;/code&gt; がグラデーションを必要とする場合にのみ実行されます。</target>
        </trans-unit>
        <trans-unit id="f69597c76f979c6f47613f7588234a4093ae500a" translate="yes" xml:space="preserve">
          <source>While it should always give you a valid decomposition, it may not give you the same one across platforms - it will depend on your LAPACK implementation.</source>
          <target state="translated">常に有効な分解を提供する必要がありますが、プラットフォームをまたいで同じものを提供するとは限りません-それはあなたのLAPACKの実装に依存します。</target>
        </trans-unit>
        <trans-unit id="90f928cf1ec6eb30bffa5d56a4ee00f33a26ccf6" translate="yes" xml:space="preserve">
          <source>While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.</source>
          <target state="translated">数学的には log(softmax(x))と同等ですが、これら2つの演算を別々に行うと遅くなり、数値的にも不安定になります。この関数は,出力と勾配を正しく計算するために別の定式化を用いています.</target>
        </trans-unit>
        <trans-unit id="adfc4c1cf043279d74d69b96592d62572f9a8648" translate="yes" xml:space="preserve">
          <source>Wide ResNet</source>
          <target state="translated">ワイドレスネット</target>
        </trans-unit>
        <trans-unit id="eec7e605ef42b4a1da3f9a4494e1414462efb313" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2</source>
          <target state="translated">ワイドResNet-101-2</target>
        </trans-unit>
        <trans-unit id="ac86db1353b797ed1c0d73605a56146ab6cb1914" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">「WideResidualNetworks &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;」の&lt;/a&gt;WideResNet-101-2モデル</target>
        </trans-unit>
        <trans-unit id="b631c349f7132ef6c2a8879b09c961b30fce8aba" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2</source>
          <target state="translated">ワイドResNet-50-2</target>
        </trans-unit>
        <trans-unit id="726f31b3e4c15ecbbc898cae3f9e8f73a3460020" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">「WideResidualNetworks &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;」の&lt;/a&gt;WideResNet-50-2モデル</target>
        </trans-unit>
        <trans-unit id="d2b826d3f7d8e2201135c671569ea283afb245af" translate="yes" xml:space="preserve">
          <source>Will result in:</source>
          <target state="translated">結果として</target>
        </trans-unit>
        <trans-unit id="8fa871c4385dea4733bc80aea73b5f5364e4ef06" translate="yes" xml:space="preserve">
          <source>Windows FAQ</source>
          <target state="translated">Windows FAQ</target>
        </trans-unit>
        <trans-unit id="8ac66d0e68a85c8d292fec91f6cbcf0bd809b4e9" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;bilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="translated">&lt;code&gt;align_corners = True&lt;/code&gt; 、直線補間モード（ &lt;code&gt;bilinear&lt;/code&gt; ）比例出力と入力画素を整列していないので、出力値は入力の大きさに依存することができます。これは、バージョン0.3.1までのこれらのモードのデフォルトの動作でした。それ以降、デフォルトの動作は &lt;code&gt;align_corners = False&lt;/code&gt; です。これが出力にどのように影響するかについての具体的な例については、&lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt; &lt;code&gt;Upsample&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="05c8bfe5d3e24898d75d1b114e7f0550313f9961" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See below for concrete examples on how this affects the outputs.</source>
          <target state="translated">&lt;code&gt;align_corners = True&lt;/code&gt; 、直線補間モード（ &lt;code&gt;linear&lt;/code&gt; 、 &lt;code&gt;bilinear&lt;/code&gt; 、 &lt;code&gt;bicubic&lt;/code&gt; 、および &lt;code&gt;trilinear&lt;/code&gt; ）比例出力と入力画素を整列していないので、出力値は入力の大きさに依存することができます。これは、バージョン0.3.1までのこれらのモードのデフォルトの動作でした。それ以降、デフォルトの動作は &lt;code&gt;align_corners = False&lt;/code&gt; です。これが出力にどのように影響するかについての具体的な例については、以下を参照してください。</target>
        </trans-unit>
        <trans-unit id="9b1242167a707b16c47db2fe5f3c0130b4361833" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="translated">&lt;code&gt;align_corners = True&lt;/code&gt; 、直線補間モード（ &lt;code&gt;linear&lt;/code&gt; 、 &lt;code&gt;bilinear&lt;/code&gt; 、および &lt;code&gt;trilinear&lt;/code&gt; ）比例出力と入力画素を整列していないので、出力値は入力の大きさに依存することができます。これは、バージョン0.3.1までのこれらのモードのデフォルトの動作でした。それ以降、デフォルトの動作は &lt;code&gt;align_corners = False&lt;/code&gt; です。これが出力にどのように影響するかについての具体的な例については、&lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt; &lt;code&gt;Upsample&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="343e3c563441d98e78b2af38a8ee16b2107d4b5b" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;mode='bicubic'&lt;/code&gt;, it&amp;rsquo;s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call &lt;code&gt;result.clamp(min=0, max=255)&lt;/code&gt; if you want to reduce the overshoot when displaying the image.</source>
          <target state="translated">で &lt;code&gt;mode='bicubic'&lt;/code&gt; 、それは言い換えれば、それはイメージのため255よりも大きな負の値または値を生成することができ、オーバーシュートが発生することが可能です。画像を表示するときのオーバーシュートを減らしたい場合は &lt;code&gt;result.clamp(min=0, max=255)&lt;/code&gt; 明示的に呼び出します。</target>
        </trans-unit>
        <trans-unit id="670f4308b71dfa44f0ce3d603252c9a80431b7a1" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;padding_idx&lt;/code&gt; set, the embedding vector at &lt;code&gt;padding_idx&lt;/code&gt; is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from &lt;a href=&quot;#torch.nn.Embedding&quot;&gt;&lt;code&gt;Embedding&lt;/code&gt;&lt;/a&gt; is always zero.</source>
          <target state="translated">&lt;code&gt;padding_idx&lt;/code&gt; セット、の埋め込みベクトル &lt;code&gt;padding_idx&lt;/code&gt; は、すべてゼロに初期化されます。ただし、このベクトルは後で変更できることに注意してください。たとえば、カスタマイズされた初期化方法を使用して、出力のパディングに使用されるベクトルを変更します。&lt;a href=&quot;#torch.nn.Embedding&quot;&gt; &lt;code&gt;Embedding&lt;/code&gt; &lt;/a&gt;からのこのベクトルの勾配は常にゼロです。</target>
        </trans-unit>
        <trans-unit id="245747fecda85d5b71d626b9e2eb04627690bcb2" translate="yes" xml:space="preserve">
          <source>With &lt;em&gt;trace-based&lt;/em&gt; exporter, we get the result ONNX graph which unrolls the for loop:</source>
          <target state="translated">で&lt;em&gt;、トレースベースの&lt;/em&gt;輸出、我々はループのアンロール結果ONNXグラフを取得します：</target>
        </trans-unit>
        <trans-unit id="19fffb59deac1debbb0ed0b605bbe04aebbddb29" translate="yes" xml:space="preserve">
          <source>With the default arguments it uses the Euclidean norm over vectors along dimension</source>
          <target state="translated">デフォルトの引数では,次元に沿ったベクトルのユークリッドノルムを使用します.</target>
        </trans-unit>
        <trans-unit id="67793521614b6b44f958c708ad8988aeadfd2309" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length in the last dimension:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; に&lt;/a&gt;出力長を指定しないと、入力が最後の次元で奇数長であるため、出力は適切にラウンドトリップしません。</target>
        </trans-unit>
        <trans-unit id="4263d953681d9ae30565f30202d1492e42e9e8ac" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; に&lt;/a&gt;出力長を指定しないと、入力が奇数長であるため、出力は適切にラウンドトリップしません。</target>
        </trans-unit>
        <trans-unit id="5375f310fdbbbaceba22e923b5eb13a5f74fab7f" translate="yes" xml:space="preserve">
          <source>Wrapper around a &lt;code&gt;torch._C.Future&lt;/code&gt; which encapsulates an asynchronous execution of a callable, e.g. &lt;a href=&quot;rpc#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;. It also exposes a set of APIs to add callback functions and set results.</source>
          <target state="translated">呼び出し可能オブジェクトの非同期実行をカプセル化する &lt;code&gt;torch._C.Future&lt;/code&gt; ラッパー。例：&lt;a href=&quot;rpc#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;。また、コールバック関数を追加して結果を設定するための一連のAPIも公開しています。</target>
        </trans-unit>
        <trans-unit id="65acb3d4bdbad8b7e153886b6b2199ad76a756c8" translate="yes" xml:space="preserve">
          <source>Wrapper class for quantized operations.</source>
          <target state="translated">量子化処理のラッパークラス.</target>
        </trans-unit>
        <trans-unit id="2a31ffa99fd6d35b029d1f439707a5bd7c9b4da8" translate="yes" xml:space="preserve">
          <source>Writes all values from the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor. For each value in &lt;code&gt;src&lt;/code&gt;, its output index is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">テンソルからすべての値を書き込み &lt;code&gt;src&lt;/code&gt; に &lt;code&gt;self&lt;/code&gt; に指定されたインデックスの &lt;code&gt;index&lt;/code&gt; テンソル。各値について &lt;code&gt;src&lt;/code&gt; 、その出力インデックスは、そのインデックスで指定された &lt;code&gt;src&lt;/code&gt; のための &lt;code&gt;dimension != dim&lt;/code&gt; との対応する値によって &lt;code&gt;index&lt;/code&gt; のための &lt;code&gt;dimension = dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5adf3c6baa6091132b20eef0ff93f0984499afa5" translate="yes" xml:space="preserve">
          <source>Writes entries directly to event files in the log_dir to be consumed by TensorBoard.</source>
          <target state="translated">TensorBoardが消費するlog_dirのイベントファイルに直接エントリを書き込みます。</target>
        </trans-unit>
        <trans-unit id="c032adc1ff629c9b66f22749ad667e6beadf144b" translate="yes" xml:space="preserve">
          <source>X</source>
          <target state="translated">X</target>
        </trans-unit>
        <trans-unit id="29822901b44f81c6464586704f28915142f932bc" translate="yes" xml:space="preserve">
          <source>X (Tensor): tensor of eigenvectors of size</source>
          <target state="translated">X(テンソル):サイズの固有ベクトルのテンソル</target>
        </trans-unit>
        <trans-unit id="030b21a9a5ef307806a06b78be434c1972173393" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = X^*[N_1 - \omega_1, \dots, N_d - \omega_d],</source>
          <target state="translated">X[\omega_1,♪♪dots,♪omega_d]=X^*[N_1-\omega_1,♪dots,N_d-\omega_d]。</target>
        </trans-unit>
        <trans-unit id="ed1d6f505827ea7067cb8c300c0904dbfa5fbd7a" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="translated">X[\omega_1,I\dots,I\omega_d]=&quot;N_i&quot;}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}</target>
        </trans-unit>
        <trans-unit id="746d94247ac643f86d1ee17dab642515e2ff9085" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a14f6ead4d7bc2b1f74caf9a0cc9b1f3d45fbb66" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = X[m, \text{n\_fft} - \omega]^*</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e6854a6a1b525f353a271b3c9b646d1b5c28d3f" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = \sum_{k = 0}^{\text{win\_length-1}}% \text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ % \exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14ebad2cda6dcca683250b526859348822d9d5c8" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. E.g.:</source>
          <target state="translated">はい、これはONNXopsetバージョン&amp;gt; = 11で現在サポートされています。例：</target>
        </trans-unit>
        <trans-unit id="5684cedf8d1212771db72a004af08859ca152372" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. ONNX introduced the concept of Sequence in opset 11. Similar to list, Sequence is a data type that contains arbitrary number of Tensors. Associated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc. However, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace add operator. E.g.:</source>
          <target state="translated">はい、これは現在ONNXopsetバージョン&amp;gt; = 11でサポートされています。ONNXはopset11にSequenceの概念を導入しました。リストと同様に、Sequenceは任意の数のテンソルを含むデータ型です。SequenceInsert、SequenceAtなどの関連演算子もONNXに導入されています。ただし、ループ内のインプレースリスト追加はONNXにエクスポートできません。これを実装するには、インプレース追加演算子を使用してください。例えば：</target>
        </trans-unit>
        <trans-unit id="738a2b66281e5ca4973cbceebc923d1996e03dad" translate="yes" xml:space="preserve">
          <source>Yields</source>
          <target state="translated">Yields</target>
        </trans-unit>
        <trans-unit id="7c19fb2e314a3137e93fb758f531465aacc3456b" translate="yes" xml:space="preserve">
          <source>You can also construct hybrid sparse tensors, where only the first n dimensions are sparse, and the rest of the dimensions are dense.</source>
          <target state="translated">また、最初のn次元だけが疎で、残りの次元は密であるハイブリッドスパーステンソルを構築することもできます。</target>
        </trans-unit>
        <trans-unit id="d617602dcc62272743d7a795be368e5ffbe5433d" translate="yes" xml:space="preserve">
          <source>You can also run the exported model with &lt;a href=&quot;https://github.com/microsoft/onnxruntime&quot;&gt;ONNX Runtime&lt;/a&gt;, you will need to install &lt;code&gt;ONNX Runtime&lt;/code&gt;: please &lt;a href=&quot;https://github.com/microsoft/onnxruntime#installation&quot;&gt;follow these instructions&lt;/a&gt;.</source>
          <target state="translated">あなたはまたしてエクスポートされたモデルで実行することができます&lt;a href=&quot;https://github.com/microsoft/onnxruntime&quot;&gt;ONNX Runtimeを&lt;/a&gt;インストールする必要があります、 &lt;code&gt;ONNX Runtime&lt;/code&gt; を：してください&lt;a href=&quot;https://github.com/microsoft/onnxruntime#installation&quot;&gt;これらの指示に従ってください&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c8c75fa187b181f202978d8a86ab28cca3e9eb36" translate="yes" xml:space="preserve">
          <source>You can also verify the protobuf using the &lt;a href=&quot;https://github.com/onnx/onnx/&quot;&gt;ONNX&lt;/a&gt; library. You can install &lt;code&gt;ONNX&lt;/code&gt; with conda:</source>
          <target state="translated">&lt;a href=&quot;https://github.com/onnx/onnx/&quot;&gt;ONNX&lt;/a&gt;ライブラリを使用してprotobufを確認することもできます。 &lt;code&gt;ONNX&lt;/code&gt; してONNXをインストールできます。</target>
        </trans-unit>
        <trans-unit id="ec539497ce517be7d2c9354eae2288ab7aed0f7d" translate="yes" xml:space="preserve">
          <source>You can construct a model with random weights by calling its constructor:</source>
          <target state="translated">コンストラクタを呼び出すことで、ランダムな重みを持つモデルを構築することができます。</target>
        </trans-unit>
        <trans-unit id="762890e45c5b225da275ae73984ed756702605c8" translate="yes" xml:space="preserve">
          <source>You should never try to change your model&amp;rsquo;s parameters after wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;. Because, when wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;, the constructor of &lt;code&gt;DistributedDataParallel&lt;/code&gt; will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model&amp;rsquo;s parameters afterwards, gradient redunction functions no longer match the correct set of parameters.</source>
          <target state="translated">モデルを &lt;code&gt;DistributedDataParallel&lt;/code&gt; でラップした後は、モデルのパラメーターを変更しようとしないでください。モデル包むとき、ので &lt;code&gt;DistributedDataParallel&lt;/code&gt; を、コンストラクタの &lt;code&gt;DistributedDataParallel&lt;/code&gt; は、建設時のモデル自体のすべてのパラメータに追加の勾配の減少関数を登録します。後でモデルのパラメーターを変更すると、勾配縮小関数が正しいパラメーターのセットと一致しなくなります。</target>
        </trans-unit>
        <trans-unit id="ccd4aee0db1253438f0c3a9d3c74816b28340f08" translate="yes" xml:space="preserve">
          <source>You&amp;rsquo;ll generally want to use &lt;a href=&quot;torch.qr#torch.qr&quot;&gt;&lt;code&gt;torch.qr()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">通常、代わりに&lt;a href=&quot;torch.qr#torch.qr&quot;&gt; &lt;code&gt;torch.qr()&lt;/code&gt; &lt;/a&gt;を使用することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="f259809289921b4be91b7ff0e29bbdb10551660f" translate="yes" xml:space="preserve">
          <source>Your models should also subclass this class.</source>
          <target state="translated">モデルもこのクラスをサブクラス化する必要があります。</target>
        </trans-unit>
        <trans-unit id="93d450611dc79948223d0cdb9f4a99610848c9d6" translate="yes" xml:space="preserve">
          <source>ZeroPad2d</source>
          <target state="translated">ZeroPad2d</target>
        </trans-unit>
        <trans-unit id="90bc5acc0a3b091bfe56eb668e9c84ac53428130" translate="yes" xml:space="preserve">
          <source>[* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1] \times \ldots \times \text{normalized\_shape}[-1]]</source>
          <target state="translated">[* \ times \ text {normalized \ _shape} [0] \ times \ text {normalized \ _shape} [1] \ times \ ldots \ times \ text {normalized \ _shape} [-1]]</target>
        </trans-unit>
        <trans-unit id="b61210c8825bb88613c060ba48aae051333bd30a" translate="yes" xml:space="preserve">
          <source>[-1, 1]</source>
          <target state="translated">[-1、1]</target>
        </trans-unit>
        <trans-unit id="ae17aa1eaf46c89eecaf929c74d8fda9a55db49b" translate="yes" xml:space="preserve">
          <source>[0, 1)</source>
          <target state="translated">[0、1）</target>
        </trans-unit>
        <trans-unit id="c49d95c97e6b97b46f0fa87c4c3d5517b2dd2ac1" translate="yes" xml:space="preserve">
          <source>[0, C-1]</source>
          <target state="translated">[0、C-1]</target>
        </trans-unit>
        <trans-unit id="c00ab2adc7412d84a0750550969f7439fdbed02f" translate="yes" xml:space="preserve">
          <source>[0] a pretty-printed representation (as valid Python syntax) of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;code&gt;code&lt;/code&gt;. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant&amp;rsquo;s values.</source>
          <target state="translated">[0] &lt;code&gt;forward&lt;/code&gt; メソッドの内部グラフの（有効なPython構文としての）きれいに印刷された表現。 &lt;code&gt;code&lt;/code&gt; 参照してください。[1] [0]の出力のCONSTANT.cN形式に従ったConstMap。[0]出力のインデックスは、基になる定数の値へのキーです。</target>
        </trans-unit>
        <trans-unit id="bc47f02dcecd076fd210f14fb47e8772c796bc8d" translate="yes" xml:space="preserve">
          <source>[1] D. W. Griffin and J. S. Lim, &amp;ldquo;Signal estimation from modified short-time Fourier transform,&amp;rdquo; IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.</source>
          <target state="translated">[1] DWGriffinおよびJSLim、「修正された短時間フーリエ変換からの信号推定」、IEEETrans。ASSP、vol.32、no.2、pp.236-243、1984年4月。</target>
        </trans-unit>
        <trans-unit id="9390898997c65a41ca007de79555abfa5f0ba5df" translate="yes" xml:space="preserve">
          <source>[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A Robust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/17M1129830&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/17M1129830&lt;/a&gt;</source>
          <target state="translated">[DuerschEtal2018] Jed A. Duersch、Meiyue Shao、Chao Yang、Ming Gu （2018）LOBPCGの堅牢で効率的な実装。SIAM J.Sci。計算、40（5）、C655-C676。（22ページ）&lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/17M1129830&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/17M1129830&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f08253d232026d76249c2db42c2940e24ba3bacf" translate="yes" xml:space="preserve">
          <source>[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2), 517-541. (25 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&lt;/a&gt;</source>
          <target state="translated">[Knyazev2001] Andrew V.Knyazev。 （2001）最適な前処理付き固有ソルバーに向けて：局所的に最適なブロック前処理付き共役勾配法。 SIAM J.Sci。計算、23（2）、517-541。 （25ページ）&lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ff64b4c2d43518023be089f31430ef90034f605e" translate="yes" xml:space="preserve">
          <source>[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/S1064827500370883&quot;&gt;https://epubs.siam.org/doi/10.1137/S1064827500370883&lt;/a&gt;</source>
          <target state="translated">[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/S1064827500370883&quot;&gt;https://epubs.siam.org/doi/10.1137/S1064827500370883&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f1e9b6692d359e3ebb28163afd3f06ce34d6b2df" translate="yes" xml:space="preserve">
          <source>[[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(&amp;lsquo;A&amp;rsquo;, &amp;lsquo;B1&amp;rsquo;, &amp;lsquo;B2&amp;rsquo;))</source>
          <target state="translated">[[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(&amp;lsquo;A&amp;rsquo;, &amp;lsquo;B1&amp;rsquo;, &amp;lsquo;B2&amp;rsquo;))</target>
        </trans-unit>
        <trans-unit id="38ac8c676253501b3fb6819ff9373ce6cfe5b055" translate="yes" xml:space="preserve">
          <source>\ ^*</source>
          <target state="translated">\ ^*</target>
        </trans-unit>
        <trans-unit id="80a1cbf9743b017c73b23f243284395f0c518670" translate="yes" xml:space="preserve">
          <source>\Gamma(\cdot)</source>
          <target state="translated">\Gamma(\cdot)</target>
        </trans-unit>
        <trans-unit id="15e33ea71862e7b6632cf90aebb8ab938c698f9d" translate="yes" xml:space="preserve">
          <source>\Phi(x)</source>
          <target state="translated">\Phi(x)</target>
        </trans-unit>
        <trans-unit id="2a6c118642891e41137cb68e1346d34dabbd128c" translate="yes" xml:space="preserve">
          <source>\Vert x \Vert _p = \left( \sum_{i=1}^n \vert x_i \vert ^ p \right) ^ {1/p}.</source>
          <target state="translated">\Vert x \Vert _p = \left( \sum_{i=1}^n \vert x_i \vert ^ p \right) ^ {1/p}.</target>
        </trans-unit>
        <trans-unit id="f7c665b45932a814215e979bc2611080b4948e68" translate="yes" xml:space="preserve">
          <source>\alpha</source>
          <target state="translated">\alpha</target>
        </trans-unit>
        <trans-unit id="1eafe1d2e67a6ccfd6c43fcc6a7aba05feb684c2" translate="yes" xml:space="preserve">
          <source>\alpha = 1.6732632423543772848170429916717</source>
          <target state="translated">\alpha = 1.6732632423543772848170429916717</target>
        </trans-unit>
        <trans-unit id="cbe1c5b26860bdd639e58977c2072d9b65aece25" translate="yes" xml:space="preserve">
          <source>\alpha=1.6732632423543772848170429916717</source>
          <target state="translated">\alpha=1.6732632423543772848170429916717</target>
        </trans-unit>
        <trans-unit id="eef3f6c4a900c50c6ccae139e6900e18ddc9222e" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k, \text{stride[1]} \times h + m, \text{stride[2]} \times w + n) \end{aligned}</source>
          <target state="translated">\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k, \text{stride[1]} \times h + m, \text{stride[2]} \times w + n) \end{aligned}</target>
        </trans-unit>
        <trans-unit id="3444423e4aa6a27dc2d4e17fde867139b4f23a7b" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\ &amp;amp; \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k, \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)} {kD \times kH \times kW} \end{aligned}</source>
          <target state="translated">\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\ &amp;amp; \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k, \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)} {kD \times kH \times kW} \end{aligned}</target>
        </trans-unit>
        <trans-unit id="9b5fbddfbbd75f7006abb89ad6964ec0f13538c7" translate="yes" xml:space="preserve">
          <source>\begin{aligned} out(N_i, C_j, h, w) ={} &amp;amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m, \text{stride[1]} \times w + n) \end{aligned}</source>
          <target state="translated">\begin{aligned} out(N_i, C_j, h, w) ={} &amp;amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m, \text{stride[1]} \times w + n) \end{aligned}</target>
        </trans-unit>
        <trans-unit id="4ea19e5cef880f790f54322d49c339a67ef7eba6" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \\ i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\ f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\ g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\ o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\ c_t = f_t \odot c_{t-1} + i_t \odot g_t \\ h_t = o_t \odot \tanh(c_t) \\ \end{array}</source>
          <target state="translated">\begin{array}{ll} \\ i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\ f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\ g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\ o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\ c_t = f_t \odot c_{t-1} + i_t \odot g_t \\ h_t = o_t \odot \tanh(c_t) \\ \end{array}</target>
        </trans-unit>
        <trans-unit id="6b6cb34a158b8201fbf48f50465b1ece00a7eb5f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|AX-B\|_2. \end{array}</source>
          <target state="translated">\begin{array}{ll} \min_X &amp;amp; \|AX-B\|_2. \end{array}</target>
        </trans-unit>
        <trans-unit id="9312cff9d66b9a6753766d98374f8fbc6cf98e2f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|X\|_2 &amp;amp; \text{subject to} &amp;amp; AX = B. \end{array}</source>
          <target state="translated">\begin{array}{ll} \min_X &amp;amp; \|X\|_2 &amp;amp; \text{subject to} &amp;amp; AX = B. \end{array}</target>
        </trans-unit>
        <trans-unit id="930f69e97734f3784bc6b17a4919f47bb6fa5660" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\ f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\ g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\ o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\ c' = f * c + i * g \\ h' = o * \tanh(c') \\ \end{array}</source>
          <target state="translated">\begin{array}{ll} i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\ f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\ g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\ o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\ c' = f * c + i * g \\ h' = o * \tanh(c') \\ \end{array}</target>
        </trans-unit>
        <trans-unit id="e6bea3b60c648ba0107ad5a273553cd4d585a9b3" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\ z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\ n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\ h' = (1 - z) * n + z * h \end{array}</source>
          <target state="translated">\begin{array}{ll} r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\ z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\ n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\ h' = (1 - z) * n + z * h \end{array}</target>
        </trans-unit>
        <trans-unit id="e41fd097ab91cc853c05e5d0d9e0d37ff3dd7662" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\ z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\ n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \end{array}</source>
          <target state="translated">\begin{array}{ll} r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\ z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\ n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \end{array}</target>
        </trans-unit>
        <trans-unit id="6499d503bfc00cadae1440b191c52a8632e2f8c4" translate="yes" xml:space="preserve">
          <source>\beta</source>
          <target state="translated">\beta</target>
        </trans-unit>
        <trans-unit id="6ceee6706c4f97ef457cd65c6ed19732a4a4c7e0" translate="yes" xml:space="preserve">
          <source>\delta^{(l-1)}_t</source>
          <target state="translated">\delta^{(l-1)}_t</target>
        </trans-unit>
        <trans-unit id="986673ab936df8ce1454d7dc93297bd7bbc51c4b" translate="yes" xml:space="preserve">
          <source>\ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</source>
          <target state="translated">\ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</target>
        </trans-unit>
        <trans-unit id="4d38ace96b9720e15bef838db0ce85ef5b456092" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n) + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n) + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</target>
        </trans-unit>
        <trans-unit id="ec2ad56a41af8d7cfc9a26f61f048cf6cbd7e78d" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</target>
        </trans-unit>
        <trans-unit id="de7b2379237fe388c3459990f0a508ab1bd86588" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</target>
        </trans-unit>
        <trans-unit id="f6a58890a57b18854cfa1707f659a4fd6ff6e4cb" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left( x_n - y_n \right)^2,</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left( x_n - y_n \right)^2,</target>
        </trans-unit>
        <trans-unit id="bbe073a5e1335cf9e8392124ee3bc939a82d8704" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right|,</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right|,</target>
        </trans-unit>
        <trans-unit id="295ee7beff5d61b430ac876642e47ec1ac559795" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';} \\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';} \\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</target>
        </trans-unit>
        <trans-unit id="4d5ca23779a198f768fff42eaa5060e03279277c" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</target>
        </trans-unit>
        <trans-unit id="3c1589dd2fb19090831cbb7f520baedd8d65a3e4" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{`mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{`sum'.} \end{cases}</source>
          <target state="translated">\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{`mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{`sum'.} \end{cases}</target>
        </trans-unit>
        <trans-unit id="8ba1942b6f9513bd39cc8cb1f1c52037ed6d49d3" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;amp; \text{if reduction} = \text{'mean';}\\ \sum_{n=1}^N l_n, &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;amp; \text{if reduction} = \text{'mean';}\\ \sum_{n=1}^N l_n, &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</target>
        </trans-unit>
        <trans-unit id="5a3a211a3bdfba5d642037c3946f715f2ce73187" translate="yes" xml:space="preserve">
          <source>\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c}) + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</source>
          <target state="translated">\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c}) + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</target>
        </trans-unit>
        <trans-unit id="c04fbcdd9308d49d4c2300f2ebbb1e83c44e8b83" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target} * \text{input}</source>
          <target state="translated">\exp(\text{input}) - \text{target} * \text{input}</target>
        </trans-unit>
        <trans-unit id="cd47433a58e73b2601de69c1a9744b288212fb5f" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target}*\text{input}</source>
          <target state="translated">\exp(\text{input}) - \text{target}*\text{input}</target>
        </trans-unit>
        <trans-unit id="fe43eb66c8bdd27010ca6db1c98281c7d4c4731f" translate="yes" xml:space="preserve">
          <source>\exp^A = \sum_{k=0}^\infty A^k / k!.</source>
          <target state="translated">\exp^A = \sum_{k=0}^\infty A^k / k!.</target>
        </trans-unit>
        <trans-unit id="14f6d7074093dd4d9ef81de502fd654605e4bb67" translate="yes" xml:space="preserve">
          <source>\forall i = d, \dots, d+k-1</source>
          <target state="translated">\forall i = d, \dots, d+k-1</target>
        </trans-unit>
        <trans-unit id="6aed51b4bd21bf9aaad2932aa936dab4bdd62611" translate="yes" xml:space="preserve">
          <source>\frac{1}{1-p}</source>
          <target state="translated">\frac{1}{1-p}</target>
        </trans-unit>
        <trans-unit id="037205eaa442691656469a316bf2dff320c2f572" translate="yes" xml:space="preserve">
          <source>\frac{1}{2} N (N - 1)</source>
          <target state="translated">\frac{1}{2} N (N - 1)</target>
        </trans-unit>
        <trans-unit id="64c94d13eeb330b494061e86538db66574ad0f7d" translate="yes" xml:space="preserve">
          <source>\frac{1}{3}</source>
          <target state="translated">\frac{1}{3}</target>
        </trans-unit>
        <trans-unit id="5947a169159fe867f85f3fd8b9690019b48152f5" translate="yes" xml:space="preserve">
          <source>\frac{1}{8}</source>
          <target state="translated">\frac{1}{8}</target>
        </trans-unit>
        <trans-unit id="f81c864ea46e4c42b16663b744993f9011be95f2" translate="yes" xml:space="preserve">
          <source>\frac{300}{100}=3</source>
          <target state="translated">\frac{300}{100}=3</target>
        </trans-unit>
        <trans-unit id="436bcf2181eea8fe79cdb015a5567ca1b8d69652" translate="yes" xml:space="preserve">
          <source>\frac{5}{3}</source>
          <target state="translated">\frac{5}{3}</target>
        </trans-unit>
        <trans-unit id="a9c2bfb5b8138830fd93a3a13faef54bc4dc94a2" translate="yes" xml:space="preserve">
          <source>\frac{m}{2} \leq</source>
          <target state="translated">\frac{m}{2} \leq</target>
        </trans-unit>
        <trans-unit id="b8fb95020958e9f0b58822f35b35fb490054c02e" translate="yes" xml:space="preserve">
          <source>\frac{p - 1}{2}</source>
          <target state="translated">\frac{p - 1}{2}</target>
        </trans-unit>
        <trans-unit id="67833ee2012ec1c6254b6c009dc72bf0dc48aa6d" translate="yes" xml:space="preserve">
          <source>\gamma</source>
          <target state="translated">\gamma</target>
        </trans-unit>
        <trans-unit id="27634ea2c473bc36e59149a7f0c457ffb292325a" translate="yes" xml:space="preserve">
          <source>\hat{x}</source>
          <target state="translated">\hat{x}</target>
        </trans-unit>
        <trans-unit id="b9e47e1f86f68634e0d0a997d4ea3952fae1e892" translate="yes" xml:space="preserve">
          <source>\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t</source>
          <target state="translated">\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t</target>
        </trans-unit>
        <trans-unit id="770b7843ffee64a984041915fe8461fd98a76060" translate="yes" xml:space="preserve">
          <source>\in [0, \infty]</source>
          <target state="translated">\in [0, \infty]</target>
        </trans-unit>
        <trans-unit id="9b97f26fbeb1b84327c736de515f10980d9c7d68" translate="yes" xml:space="preserve">
          <source>\infty</source>
          <target state="translated">\infty</target>
        </trans-unit>
        <trans-unit id="b237071f96360004cf37b06340000864b59f54c3" translate="yes" xml:space="preserve">
          <source>\int y\,dx</source>
          <target state="translated">\int y\,dx</target>
        </trans-unit>
        <trans-unit id="a10251c74fceb1b1b9e9c45471b613f216beb4a9" translate="yes" xml:space="preserve">
          <source>\int_a^b f = -\int_b^a f</source>
          <target state="translated">\int_a^b f = -\int_b^a f</target>
        </trans-unit>
        <trans-unit id="b3931f1ce298c536432fd324b3a1ab4337120689" translate="yes" xml:space="preserve">
          <source>\lambda</source>
          <target state="translated">\lambda</target>
        </trans-unit>
        <trans-unit id="1b47c3b18de49455a713efe91ac03ec27aad59a5" translate="yes" xml:space="preserve">
          <source>\lbrace (i, i) \rbrace</source>
          <target state="translated">\lbrace (i, i) \rbrace</target>
        </trans-unit>
        <trans-unit id="0fa91d11567564172cce99f01e48d1e73a28bc31" translate="yes" xml:space="preserve">
          <source>\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]</source>
          <target state="translated">\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]</target>
        </trans-unit>
        <trans-unit id="89ca5f286a6835c142dd1390cd67d2698dca7bcb" translate="yes" xml:space="preserve">
          <source>\left[\text{-clip\_value}, \text{clip\_value}\right]</source>
          <target state="translated">\left[\text{-clip\_value}, \text{clip\_value}\right]</target>
        </trans-unit>
        <trans-unit id="6ceb7ce51c6d9da5e34a800614788f55dff712c0" translate="yes" xml:space="preserve">
          <source>\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</source>
          <target state="translated">\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</target>
        </trans-unit>
        <trans-unit id="a74c86baea5a7bb180947e759ccafb91eee50e79" translate="yes" xml:space="preserve">
          <source>\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</source>
          <target state="translated">\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</target>
        </trans-unit>
        <trans-unit id="ed56196dc362da2ecbc9244bc832a7311739bb3d" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="de0088faf4547f5a22dd4cd77b23209a7dd8113e" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="88061aa72baf92fe1d129c82b73d16c37a6af8f0" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="f0a9328764eddcc1a62bf8fbb7c249505ed2c539" translate="yes" xml:space="preserve">
          <source>\leq</source>
          <target state="translated">\leq</target>
        </trans-unit>
        <trans-unit id="49abff7af14753dc3cdab27a1bf7f8ed5f2c2fa1" translate="yes" xml:space="preserve">
          <source>\leq 256</source>
          <target state="translated">\leq 256</target>
        </trans-unit>
        <trans-unit id="a719b606b8fa813d6dc2397930551b66016965ba" translate="yes" xml:space="preserve">
          <source>\leq S</source>
          <target state="translated">\leq S</target>
        </trans-unit>
        <trans-unit id="f7d8821758716417ac6285ab84b0661734c3439c" translate="yes" xml:space="preserve">
          <source>\leq T</source>
          <target state="translated">\leq T</target>
        </trans-unit>
        <trans-unit id="9ca5d36772ef139985921c4d545788d48fddcf0c" translate="yes" xml:space="preserve">
          <source>\lfloor \frac{N_d}{2} \rfloor + 1</source>
          <target state="translated">\lfloor \frac{N_d}{2} \rfloor + 1</target>
        </trans-unit>
        <trans-unit id="bc0c94864255e5978415bf4d290dee47e2677194" translate="yes" xml:space="preserve">
          <source>\lfloor\frac{\text{input planes}}{sT}\rfloor</source>
          <target state="translated">\lfloor\frac{\text{input planes}}{sT}\rfloor</target>
        </trans-unit>
        <trans-unit id="9878e94e87e5bd7098242aa36f29f409bc60ed2b" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty</source>
          <target state="translated">\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty</target>
        </trans-unit>
        <trans-unit id="26c67f72ffeaf83ec8c07ff1c00c57b7b4ce05e6" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \log (x) = -\infty</source>
          <target state="translated">\lim_{x\to 0} \log (x) = -\infty</target>
        </trans-unit>
        <trans-unit id="d92b1f0baf6824880fb434c0077a618ba265ea33" translate="yes" xml:space="preserve">
          <source>\log (0) = -\infty</source>
          <target state="translated">\log (0) = -\infty</target>
        </trans-unit>
        <trans-unit id="99e33ed0cf12197cde63048ced916ea73cbc9c3e" translate="yes" xml:space="preserve">
          <source>\log(0)</source>
          <target state="translated">\log(0)</target>
        </trans-unit>
        <trans-unit id="9f7859c28e08a9c15994c8896e613dd27080b6a5" translate="yes" xml:space="preserve">
          <source>\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)</source>
          <target state="translated">\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)</target>
        </trans-unit>
        <trans-unit id="c1e3a3daacbe46fc4916de547be4049f250f3288" translate="yes" xml:space="preserve">
          <source>\log(\text{Softmax}(x))</source>
          <target state="translated">\log(\text{Softmax}(x))</target>
        </trans-unit>
        <trans-unit id="322a0b1861bac0e7bc727021b82a0313c2a2b303" translate="yes" xml:space="preserve">
          <source>\log\left(e^x + e^y\right)</source>
          <target state="translated">\log\left(e^x + e^y\right)</target>
        </trans-unit>
        <trans-unit id="8783c75c38eec2ddd68cac88f7dc5ac00e806274" translate="yes" xml:space="preserve">
          <source>\log_2\left(2^x + 2^y\right)</source>
          <target state="translated">\log_2\left(2^x + 2^y\right)</target>
        </trans-unit>
        <trans-unit id="8935b97a0f600209c234abf4d224c5fe967c9fa1" translate="yes" xml:space="preserve">
          <source>\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert</source>
          <target state="translated">\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert</target>
        </trans-unit>
        <trans-unit id="eafd018e3a8aa85682e4f0b33612ce6efed642ca" translate="yes" xml:space="preserve">
          <source>\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})}, \sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}</source>
          <target state="translated">\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})}, \sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}</target>
        </trans-unit>
        <trans-unit id="201a130976ca261c5bc7ef67511ced9f15d7653a" translate="yes" xml:space="preserve">
          <source>\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}</source>
          <target state="translated">\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}</target>
        </trans-unit>
        <trans-unit id="df511dffcdad49a67cd5945ee0ab7aef8bb4f9fc" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 0.01)</source>
          <target state="translated">\mathcal{N}(0, 0.01)</target>
        </trans-unit>
        <trans-unit id="8f3ba18912099017c093331676f6188e744efdac" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 1)</source>
          <target state="translated">\mathcal{N}(0, 1)</target>
        </trans-unit>
        <trans-unit id="50c7a47dd01b89919d5422d0295475337aab6ace" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, \text{std}^2)</source>
          <target state="translated">\mathcal{N}(0, \text{std}^2)</target>
        </trans-unit>
        <trans-unit id="92c992916044275832483fbb98179a2d00ca4c1e" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(\text{mean}, \text{std}^2)</source>
          <target state="translated">\mathcal{N}(\text{mean}, \text{std}^2)</target>
        </trans-unit>
        <trans-unit id="87b1ef0bb4d4a0924cc95ef538f83b2bb3e53e42" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\sqrt{k}, \sqrt{k})</source>
          <target state="translated">\mathcal{U}(-\sqrt{k}, \sqrt{k})</target>
        </trans-unit>
        <trans-unit id="d7f6ed3e182dc030701ad2deb57bf7caa5b88d6f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\text{bound}, \text{bound})</source>
          <target state="translated">\mathcal{U}(-\text{bound}, \text{bound})</target>
        </trans-unit>
        <trans-unit id="325a9ed78efc110eaab4160ef2c97f9ac713df84" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-a, a)</source>
          <target state="translated">\mathcal{U}(-a, a)</target>
        </trans-unit>
        <trans-unit id="b28ca9812620dfef2c686761b7aa8334acb0312f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(0, 1)</source>
          <target state="translated">\mathcal{U}(0, 1)</target>
        </trans-unit>
        <trans-unit id="b9712b8b025515dc1dea5f46b97d3c4fe634d954" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(\text{lower}, \text{upper})</source>
          <target state="translated">\mathcal{U}(\text{lower}, \text{upper})</target>
        </trans-unit>
        <trans-unit id="e216b98083013f9a980a8176c0057ec95ff5e691" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(a, b)</source>
          <target state="translated">\mathcal{U}(a, b)</target>
        </trans-unit>
        <trans-unit id="2df7cbd2cee6b04f561cf080750aba56226af4cb" translate="yes" xml:space="preserve">
          <source>\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="translated">\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</target>
        </trans-unit>
        <trans-unit id="20a69b6b57674088939e407b3b5098441f752171" translate="yes" xml:space="preserve">
          <source>\mathrm{erfinv}(\mathrm{erf}(x)) = x</source>
          <target state="translated">\mathrm{erfinv}(\mathrm{erf}(x)) = x</target>
        </trans-unit>
        <trans-unit id="56d825970c3fb1fe7543e6c61686e2830fac7d9f" translate="yes" xml:space="preserve">
          <source>\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="translated">\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</target>
        </trans-unit>
        <trans-unit id="3e9c0d2c84e11d8338d1dac942d5dede72bd1acf" translate="yes" xml:space="preserve">
          <source>\min(input.size(-1), input.size(-2))</source>
          <target state="translated">\min(input.size(-1), input.size(-2))</target>
        </trans-unit>
        <trans-unit id="3a4e56595df1d02f21e5c3ff7b0e9d80921858ff" translate="yes" xml:space="preserve">
          <source>\mu</source>
          <target state="translated">\mu</target>
        </trans-unit>
        <trans-unit id="c8e2d1a0bf50a27d43ade30cfb048d99feb31ad1" translate="yes" xml:space="preserve">
          <source>\odot</source>
          <target state="translated">\odot</target>
        </trans-unit>
        <trans-unit id="73b077a63e22815fe5c8ee82dab9894be842b19c" translate="yes" xml:space="preserve">
          <source>\omega</source>
          <target state="translated">\omega</target>
        </trans-unit>
        <trans-unit id="72166555a6db55785fc6fbe9a7c5bbe72be28db8" translate="yes" xml:space="preserve">
          <source>\otimes</source>
          <target state="translated">\otimes</target>
        </trans-unit>
        <trans-unit id="6dc2ada78a76bad95d3b921558b1195549985eab" translate="yes" xml:space="preserve">
          <source>\prod(\text{kernel\_size})</source>
          <target state="translated">\prod(\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="02cea321dbc3a3edfc9cc1780dfe84f694c585f0" translate="yes" xml:space="preserve">
          <source>\psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}</source>
          <target state="translated">\psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}</target>
        </trans-unit>
        <trans-unit id="b30c154a70c78f1c5dcecce58dde2e9ff4ec56aa" translate="yes" xml:space="preserve">
          <source>\psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)</source>
          <target state="translated">\psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)</target>
        </trans-unit>
        <trans-unit id="69c15416b63fd933850978302bcda59da879774e" translate="yes" xml:space="preserve">
          <source>\sigma</source>
          <target state="translated">\sigma</target>
        </trans-unit>
        <trans-unit id="bfe16f27ebc966df6f10ba356a1547b6e7242dd7" translate="yes" xml:space="preserve">
          <source>\sqrt{2}</source>
          <target state="translated">\sqrt{2}</target>
        </trans-unit>
        <trans-unit id="358161536f25000be6a773604fcf4a28afd7b7bd" translate="yes" xml:space="preserve">
          <source>\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}</source>
          <target state="translated">\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}</target>
        </trans-unit>
        <trans-unit id="64126f7d8d3d661d4e29a191268f98bf759903a3" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^K N_i}</source>
          <target state="translated">\sqrt{\prod_{i=1}^K N_i}</target>
        </trans-unit>
        <trans-unit id="976e9a0789eb95323325b92e6edc3b432c6754b8" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^d N_i}</source>
          <target state="translated">\sqrt{\prod_{i=1}^d N_i}</target>
        </trans-unit>
        <trans-unit id="9312c2748ccad7c25f8d92bc855a5a0b38989a51" translate="yes" xml:space="preserve">
          <source>\star</source>
          <target state="translated">\star</target>
        </trans-unit>
        <trans-unit id="3df82d7a797b96797b79d72f235bc018cb8155c9" translate="yes" xml:space="preserve">
          <source>\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0</source>
          <target state="translated">\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0</target>
        </trans-unit>
        <trans-unit id="a12be8b8923c0e92cce3f35968e39960bc665833" translate="yes" xml:space="preserve">
          <source>\tanh</source>
          <target state="translated">\tanh</target>
        </trans-unit>
        <trans-unit id="053658991aeb9a94a57c16cbe979538b3eca3b46" translate="yes" xml:space="preserve">
          <source>\texttt{n\_classes}</source>
          <target state="translated">\texttt{n\_classes}</target>
        </trans-unit>
        <trans-unit id="79df4825cae48291f72d91cc9840f2adcb2716c4" translate="yes" xml:space="preserve">
          <source>\texttt{result[i]}</source>
          <target state="translated">\texttt{result[i]}</target>
        </trans-unit>
        <trans-unit id="8a01c6904694a6040cbbe4d31b35e67c5fb8c5bc" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p\_tensor[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p\_tensor[i]})</target>
        </trans-unit>
        <trans-unit id="248f05fb27cede89d993bc3bb9750c4fde2467c2" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p})</target>
        </trans-unit>
        <trans-unit id="15ca5fa99d2411cc7a336f5df78382fd090360c9" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{self[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{self[i]})</target>
        </trans-unit>
        <trans-unit id="f4308a3c6285dab040bdf907914e0906e5918d43" translate="yes" xml:space="preserve">
          <source>\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</source>
          <target state="translated">\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</target>
        </trans-unit>
        <trans-unit id="1c5fbaf107a1bd80dcb32b25718623eef6cabd9f" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; 0\\ \alpha * (\exp(x) - 1), &amp;amp; \text{ if } x \leq 0 \end{cases}</source>
          <target state="translated">\text{ELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; 0\\ \alpha * (\exp(x) - 1), &amp;amp; \text{ if } x \leq 0 \end{cases}</target>
        </trans-unit>
        <trans-unit id="ee2c000c326328286929b5abe78d5f28ecc355ca" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))</source>
          <target state="translated">\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))</target>
        </trans-unit>
        <trans-unit id="1ae2cb648d379e90a17c82c0bca8393180166a13" translate="yes" xml:space="preserve">
          <source>\text{GELU}(x) = x * \Phi(x)</source>
          <target state="translated">\text{GELU}(x) = x * \Phi(x)</target>
        </trans-unit>
        <trans-unit id="98d15f980c451db5bf1f8450a31ea12ac80b261b" translate="yes" xml:space="preserve">
          <source>\text{GLU}(a, b) = a \otimes \sigma(b)</source>
          <target state="translated">\text{GLU}(a, b) = a \otimes \sigma(b)</target>
        </trans-unit>
        <trans-unit id="2b0fd43cdc620c274316d22c100c5a1cedf8758f" translate="yes" xml:space="preserve">
          <source>\text{HardShrink}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\text{HardShrink}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</target>
        </trans-unit>
        <trans-unit id="905f89b6d5fe72acb1c8befdbd1fbd394ccc9c79" translate="yes" xml:space="preserve">
          <source>\text{HardTanh}(x) = \begin{cases} 1 &amp;amp; \text{ if } x &amp;gt; 1 \\ -1 &amp;amp; \text{ if } x &amp;lt; -1 \\ x &amp;amp; \text{ otherwise } \\ \end{cases}</source>
          <target state="translated">\text{HardTanh}(x) = \begin{cases} 1 &amp;amp; \text{ if } x &amp;gt; 1 \\ -1 &amp;amp; \text{ if } x &amp;lt; -1 \\ x &amp;amp; \text{ otherwise } \\ \end{cases}</target>
        </trans-unit>
        <trans-unit id="c0b2885acfe7c1d78f13cbb375808a8a70622e9c" translate="yes" xml:space="preserve">
          <source>\text{Hardsigmoid}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ 1 &amp;amp; \text{if~} x \ge +3, \\ x / 6 + 1 / 2 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="translated">\text{Hardsigmoid}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ 1 &amp;amp; \text{if~} x \ge +3, \\ x / 6 + 1 / 2 &amp;amp; \text{otherwise} \end{cases}</target>
        </trans-unit>
        <trans-unit id="3707baa731a2a1d93fe314a962f04a7440710fa1" translate="yes" xml:space="preserve">
          <source>\text{Hardswish}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ x &amp;amp; \text{if~} x \ge +3, \\ x \cdot (x + 3) /6 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="translated">\text{Hardswish}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ x &amp;amp; \text{if~} x \ge +3, \\ x \cdot (x + 3) /6 &amp;amp; \text{otherwise} \end{cases}</target>
        </trans-unit>
        <trans-unit id="f4d15ebefe7344cee7608a90993d80b6ed04f631" translate="yes" xml:space="preserve">
          <source>\text{LeakyRELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ \text{negative\_slope} \times x, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\text{LeakyRELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ \text{negative\_slope} \times x, &amp;amp; \text{ otherwise } \end{cases}</target>
        </trans-unit>
        <trans-unit id="68988527b0b6237352fc1f66e3f8116ea81dd84a" translate="yes" xml:space="preserve">
          <source>\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</source>
          <target state="translated">\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</target>
        </trans-unit>
        <trans-unit id="51a86607b1de32af3fd6faa8dfe21281f15bc36b" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)</source>
          <target state="translated">\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)</target>
        </trans-unit>
        <trans-unit id="a4939da8f3a870de43d45395bdbb72f415fd6a5c" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</source>
          <target state="translated">\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</target>
        </trans-unit>
        <trans-unit id="a58af29fea78e99ffe2b4d0392c259d952558379" translate="yes" xml:space="preserve">
          <source>\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</source>
          <target state="translated">\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</target>
        </trans-unit>
        <trans-unit id="96135669cb29b74a14e76462df05bf97eef0ca84" translate="yes" xml:space="preserve">
          <source>\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</source>
          <target state="translated">\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</target>
        </trans-unit>
        <trans-unit id="ccbab99b824f6a91f1e1bf82a1a0fdaf575b1bb6" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ ax, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\text{PReLU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ ax, &amp;amp; \text{ otherwise } \end{cases}</target>
        </trans-unit>
        <trans-unit id="caa4e9686c6c337eb30002e3bdcb6a130c86d148" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</source>
          <target state="translated">\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</target>
        </trans-unit>
        <trans-unit id="5b347087764b6a93bb6f3ca448de0809ea32466b" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + a * \min(0,x)</source>
          <target state="translated">\text{PReLU}(x) = \max(0,x) + a * \min(0,x)</target>
        </trans-unit>
        <trans-unit id="8ab73665e19333454b7dcdadd14407c6f4eaf162" translate="yes" xml:space="preserve">
          <source>\text{RReLU}(x) = \begin{cases} x &amp;amp; \text{if } x \geq 0 \\ ax &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\text{RReLU}(x) = \begin{cases} x &amp;amp; \text{if } x \geq 0 \\ ax &amp;amp; \text{ otherwise } \end{cases}</target>
        </trans-unit>
        <trans-unit id="ca059eb696d6eaad86cfafe12af59ddef8aeb677" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(0,x), 6)</source>
          <target state="translated">\text{ReLU6}(x) = \min(\max(0,x), 6)</target>
        </trans-unit>
        <trans-unit id="be84129fb353e250c9169d66a9b03062f62f4151" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(x_0, x), q(6))</source>
          <target state="translated">\text{ReLU6}(x) = \min(\max(x_0, x), q(6))</target>
        </trans-unit>
        <trans-unit id="3bfab1dc2c67446ab1cfb128a9c2bc89afb1f336" translate="yes" xml:space="preserve">
          <source>\text{ReLU}</source>
          <target state="translated">\text{ReLU}</target>
        </trans-unit>
        <trans-unit id="08bf65cb7cffc00d2f4597ea4d30a22a63976e74" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x) = (x)^+ = \max(0, x)</source>
          <target state="translated">\text{ReLU}(x) = (x)^+ = \max(0, x)</target>
        </trans-unit>
        <trans-unit id="836fcdcc8abf1c798414718b995cf2a357c6c5a8" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x)= \max(x_0, x)</source>
          <target state="translated">\text{ReLU}(x)= \max(x_0, x)</target>
        </trans-unit>
        <trans-unit id="cab6677fddc14bf3ddd26b8fe4fe9e7d772b3259" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="translated">\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</target>
        </trans-unit>
        <trans-unit id="0eacb9a4c9b22d795a671d9085d3576f65f88226" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="translated">\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</target>
        </trans-unit>
        <trans-unit id="14b5832965c3a723de26aea464be1a7e1207b0e7" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="translated">\ text {Sigmoid}（x）= \ frac {1} {1 + \ exp（-x）}</target>
        </trans-unit>
        <trans-unit id="dd6633f7f328cd88d5fded389f4a69b2a11f8bed" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="translated">\ text {Sigmoid}（x）= \ sigma（x）= \ frac {1} {1 + \ exp（-x）}</target>
        </trans-unit>
        <trans-unit id="fa68ac444b17ef606bcb6fc0587fbb0c08130721" translate="yes" xml:space="preserve">
          <source>\text{SoftShrinkage}(x) = \begin{cases} x - \lambda, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x + \lambda, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {SoftShrinkage}（x）= \ begin {cases} x- \ lambda、＆\ text {if} x&amp;gt; \ lambda \\ x + \ lambda、＆\ text {if} x &amp;lt;-\ lambda \\ 0 、＆​​\ text {それ以外の場合} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="16ab4540ec544f6557be83896c80fd5ae2348d4a" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{ 1 + |x|}</source>
          <target state="translated">\ text {SoftSign}（x）= \ frac {x} {1 + | x |}</target>
        </trans-unit>
        <trans-unit id="b4e8d4178c0a5a0115f54178dd9083d36fddd624" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{1 + |x|}</source>
          <target state="translated">\ text {SoftSign}（x）= \ frac {x} {1 + | x |}</target>
        </trans-unit>
        <trans-unit id="a2f536bf3a8bd1033dd978de393c848edde7120d" translate="yes" xml:space="preserve">
          <source>\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</source>
          <target state="translated">\ text {Softmax}（x_ {i}）= \ frac {\ exp（x_i）} {\ sum_j \ exp（x_j）}</target>
        </trans-unit>
        <trans-unit id="5b81ade15ec338b468467f739fb0a7a9ffbb69c9" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x) = \text{Softmax}(-x)</source>
          <target state="translated">\ text {Softmin}（x）= \ text {Softmax}（-x）</target>
        </trans-unit>
        <trans-unit id="f07f1bdd8c02f9e56e3ec0dac4ae00ebee846aae" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}</source>
          <target state="translated">\ text {Softmin}（x_ {i}）= \ frac {\ exp（-x_i）} {\ sum_j \ exp（-x_j）}</target>
        </trans-unit>
        <trans-unit id="2b624f5a535d4c37714f3d81447e39c785f806f7" translate="yes" xml:space="preserve">
          <source>\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))</source>
          <target state="translated">\ text {Softplus}（x）= \ frac {1} {\ beta} * \ log（1 + \ exp（\ beta * x））</target>
        </trans-unit>
        <trans-unit id="c0fd3a2a1e0641ec623557f2f43fda5f7c89dc6c" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \tanh(x)</source>
          <target state="translated">\ text {Tanhshrink}（x）= x- \ tanh（x）</target>
        </trans-unit>
        <trans-unit id="995fcea13ed41fcda67e8f62153baff1abf82a33" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \text{Tanh}(x)</source>
          <target state="translated">\ text {Tanhshrink}（x）= x- \ text {Tanh}（x）</target>
        </trans-unit>
        <trans-unit id="d0334b6a3fe1d679df6ff4bfb438db0b64230682" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}</source>
          <target state="translated">\ text {Tanh}（x）= \ tanh（x）= \ frac {\ exp（x）-\ exp（-x）} {\ exp（x）+ \ exp（-x）}</target>
        </trans-unit>
        <trans-unit id="75a10715f4ce8ac09932b779e24b1a0a5468348f" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}</source>
          <target state="translated">\ text {Tanh}（x）= \ tanh（x）= \ frac {\ exp（x）-\ exp（-x）} {\ exp（x）+ \ exp（-x）}</target>
        </trans-unit>
        <trans-unit id="f169080e8d1eb07929eead4219c50bde74fb23da" translate="yes" xml:space="preserve">
          <source>\text{batch1} \mathbin{@} \text{batch2}</source>
          <target state="translated">\ text {batch1} \ mathbin {@} \ text {batch2}</target>
        </trans-unit>
        <trans-unit id="229245bb08fb0569d7993b5cea15398607e69900" translate="yes" xml:space="preserve">
          <source>\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}}</source>
          <target state="translated">\ text {bound} = \ text {gain} \ times \ sqrt {\ frac {3} {\ text {fan \ _mode}}}</target>
        </trans-unit>
        <trans-unit id="3e2f5fb306be83947e6aad79d83c918b084bbee4" translate="yes" xml:space="preserve">
          <source>\text{in\_channels}</source>
          <target state="translated">\text{in\_channels}</target>
        </trans-unit>
        <trans-unit id="26c469932ffffddf5378f023f97a627052bcde45" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;gt; \text{other}</source>
          <target state="translated">\ text {input}&amp;gt; \ text {other}</target>
        </trans-unit>
        <trans-unit id="c15cd245b2e6c1904e5b0a29dc06bb37cb23bb5e" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;lt; \text{other}</source>
          <target state="translated">\ text {input} &amp;lt;\ text {other}</target>
        </trans-unit>
        <trans-unit id="f395bedc77d1db475982b0b075906a247b84c2eb" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target} * \log(\text{input}+\text{eps})</source>
          <target state="translated">\ text {input}-\ text {target} * \ log（\ text {input} + \ text {eps}）</target>
        </trans-unit>
        <trans-unit id="686d55b3f5e0392f4e8fcfd75b77c03037718ecf" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target}*\log(\text{input}+\text{eps})</source>
          <target state="translated">\ text {input}-\ text {target} * \ log（\ text {input} + \ text {eps}）</target>
        </trans-unit>
        <trans-unit id="6da1a408fc1384eb95e3208a559d3a3e0f03e99f" translate="yes" xml:space="preserve">
          <source>\text{input} = Q R</source>
          <target state="translated">\ text {input} = QR</target>
        </trans-unit>
        <trans-unit id="c4dd474d77793c7a92038a0755b05fe0ba4adf17" translate="yes" xml:space="preserve">
          <source>\text{input} = V \text{diag}(e) V^T</source>
          <target state="translated">\ text {input} = V \ text {diag}（e）V ^ T</target>
        </trans-unit>
        <trans-unit id="e721787652f4fc0b6f66950d386917c38d8957f8" translate="yes" xml:space="preserve">
          <source>\text{input} \geq \text{other}</source>
          <target state="translated">\ text {input} \ geq \ text {other}</target>
        </trans-unit>
        <trans-unit id="3cbe8a936fc7cb3a9fdf09ad329713fbe61317e4" translate="yes" xml:space="preserve">
          <source>\text{input} \leq \text{other}</source>
          <target state="translated">\ text {input} \ leq \ text {other}</target>
        </trans-unit>
        <trans-unit id="3901704e8ee508e9522d87ef1f88b52ef027532b" translate="yes" xml:space="preserve">
          <source>\text{input} \neq \text{other}</source>
          <target state="translated">\ text {input} \ neq \ text {other}</target>
        </trans-unit>
        <trans-unit id="ccf6648a02673a0bbbd4f96889ba48888a0e40f4" translate="yes" xml:space="preserve">
          <source>\text{input}[i, j]</source>
          <target state="translated">\ text {input} [i、j]</target>
        </trans-unit>
        <trans-unit id="47eeab2292dd874946bd7e66f27cc66b11eafc5e" translate="yes" xml:space="preserve">
          <source>\text{input}_{i}</source>
          <target state="translated">\text{input}_{i}</target>
        </trans-unit>
        <trans-unit id="dd2a901c455f604c6af98c920667ce74d801ef8c" translate="yes" xml:space="preserve">
          <source>\text{input}_{i} / \text{other}_{i}</source>
          <target state="translated">\ text {input} _ {i} / \ text {other} _ {i}</target>
        </trans-unit>
        <trans-unit id="c920fc92a2f279db08ab80d5217f4a1714bd40d6" translate="yes" xml:space="preserve">
          <source>\text{i}^{th}</source>
          <target state="translated">\text{i}^{th}</target>
        </trans-unit>
        <trans-unit id="6ca0545c1ccedc4ff77a3d4acb74e3770c0fbbdd" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]})</source>
          <target state="translated">\ text {kernel \ _size [0]}、\ text {kernel \ _size [1]}）</target>
        </trans-unit>
        <trans-unit id="9229970eeb833df18b71461a132cd3b0ce98ef3a" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})</source>
          <target state="translated">\ text {kernel \ _size [0]}、\ text {kernel \ _size [1]}、\ text {kernel \ _size [2]}）</target>
        </trans-unit>
        <trans-unit id="15869cb2da85e3f8dc3a924d02c0d13f080d42d1" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size})</source>
          <target state="translated">\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="80b2d9d0a1aff139a212f622b578c27a2cb6c126" translate="yes" xml:space="preserve">
          <source>\text{k}^{th}</source>
          <target state="translated">\text{k}^{th}</target>
        </trans-unit>
        <trans-unit id="0f84b2b9c4507f4c38935799e7145beab872e3ff" translate="yes" xml:space="preserve">
          <source>\text{logcumsumexp}(x)_{ij} = \log \sum\limits_{j=0}^{i} \exp(x_{ij})</source>
          <target state="translated">\ text {logcumsumexp}（x）_ {ij} = \ log \ sum \ limits_ {j = 0} ^ {i} \ exp（x_ {ij}）</target>
        </trans-unit>
        <trans-unit id="e4fff2f19ba639fc2ee13a3cda790993ad87fdfe" translate="yes" xml:space="preserve">
          <source>\text{logsumexp}(x)_{i} = \log \sum_j \exp(x_{ij})</source>
          <target state="translated">\ text {logsumexp}（x）_ {i} = \ log \ sum_j \ exp（x_ {ij}）</target>
        </trans-unit>
        <trans-unit id="e990f63d798552bb3d1da254e1d0c3c11424c266" translate="yes" xml:space="preserve">
          <source>\text{loss} = \frac{\sum^{N}_{i=1} loss(i, class[i])}{\sum^{N}_{i=1} weight[class[i]]}</source>
          <target state="translated">\ text {loss} = \ frac {\ sum ^ {N} _ {i = 1} loss（i、class [i]）} {\ sum ^ {N} _ {i = 1} weight [class [i] ]}</target>
        </trans-unit>
        <trans-unit id="65ca80a02b47d73c67e5a3e63c9b314787018d8d" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) = -x[class] + \log\left(\sum_j \exp(x[j])\right)</source>
          <target state="translated">\ text {loss}（x、class）=-\ log \ left（\ frac {\ exp（x [class]）} {\ sum_j \ exp（x [j]）} \ right）= -x [class] + \ log \ left（\ sum_j \ exp（x [j]）\ right）</target>
        </trans-unit>
        <trans-unit id="019a711532dd32707cd8b1b52a85f9dcf92bf993" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</source>
          <target state="translated">\ text {loss}（x、class）= weight [class] \ left（-x [class] + \ log \ left（\ sum_j \ exp（x [j]）\ right）\ right）</target>
        </trans-unit>
        <trans-unit id="ffd0b22341cc6c018e0b5728d553ab798985dbfc" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \begin{cases} 1 - \cos(x_1, x_2), &amp;amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp;amp; \text{if } y = -1 \end{cases}</source>
          <target state="translated">\ text {loss}（x、y）= \ begin {cases} 1- \ cos（x_1、x_2）、＆\ text {if} y = 1 \\ \ max（0、\ cos（x_1、x_2）- \ text {margin}）、＆\ text {if} y = -1 \ end {cases}</target>
        </trans-unit>
        <trans-unit id="c59e75cdc56fc3f83b35cdb9f628a1f6dc39d344" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}</source>
          <target state="translated">\ text {loss}（x、y）= \ frac {1} {n} \ sum_ {i} z_ {i}</target>
        </trans-unit>
        <trans-unit id="b06e3acda76e096ac3266b690a1f15f3c86a036c" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}</source>
          <target state="translated">\ text {loss}（x、y）= \ frac {\ sum_i \ max（0、\ text {margin} --x [y] + x [i]））^ p} {\ text {x.size}（ 0）}</target>
        </trans-unit>
        <trans-unit id="932b5db2aa0a39e3cde91ee0926767654acd252f" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}</source>
          <target state="translated">\ text {loss}（x、y）= \ frac {\ sum_i \ max（0、w [y] *（\ text {margin} --x [y] + x [i]））^ p）} {\ text {x.size}（0）}</target>
        </trans-unit>
        <trans-unit id="1be1a15f43a5fe29e30cefc8a77697b52665cc11" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</source>
          <target state="translated">\ text {loss}（x、y）= \ sum_i \ frac {\ log（1 + \ exp（-y [i] * x [i]））} {\ text {x.nelement}（）}</target>
        </trans-unit>
        <trans-unit id="be8b3997d164f84206bbdb59fc30b16e4df28e7b" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</source>
          <target state="translated">\ text {loss}（x、y）= \ sum_ {ij} \ frac {\ max（0、1-（x [y [j]]-x [i]））} {\ text {x.size} （0）}</target>
        </trans-unit>
        <trans-unit id="ed773e5223ed4e8e7a2085c415f1e22caa9ee61a" translate="yes" xml:space="preserve">
          <source>\text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})</source>
          <target state="translated">\ text {loss}（x1、x2、y）= \ max（0、-y *（x1-x2）+ \ text {margin}）</target>
        </trans-unit>
        <trans-unit id="1a70593a5153f0c40ba1299b72f0b57903179f3e" translate="yes" xml:space="preserve">
          <source>\text{other}_{i}</source>
          <target state="translated">\text{other}_{i}</target>
        </trans-unit>
        <trans-unit id="17f7519844253d6013b5ffbe16cec0f08dcc6ff0" translate="yes" xml:space="preserve">
          <source>\text{out} = -1 \times \text{input}</source>
          <target state="translated">\ text {out} = -1 \ times \ text {input}</target>
        </trans-unit>
        <trans-unit id="90a41651feeb08c3eedd09a310ec016d8b14ad0c" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \（\ text {mat1} _i \ mathbin {@} \ text {mat2} _i）</target>
        </trans-unit>
        <trans-unit id="bb3213012476cae8440fbe19a6e07cd71a7793bd" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \（\ text {mat} \ mathbin {@} \ text {vec}）</target>
        </trans-unit>
        <trans-unit id="e949532f9deb6bd6d752ab7f572576835541d193" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \（\ text {vec1} \ otimes \ text {vec2}）</target>
        </trans-unit>
        <trans-unit id="0358e27d2d15a38fee22f3930f0704830355a8e2" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{abs} \cdot \cos(\text{angle}) + \text{abs} \cdot \sin(\text{angle}) \cdot j</source>
          <target state="translated">\ text {out} = \ text {abs} \ cdot \ cos（\ text {angle}）+ \ text {abs} \ cdot \ sin（\ text {angle}）\ cdot j</target>
        </trans-unit>
        <trans-unit id="36eb11cd26daa728869e9dc0ff4a77afceb36308" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{alpha} \times \text{other}</source>
          <target state="translated">\ text {out} = \ text {input} + \ text {alpha} \ times \ text {other}</target>
        </trans-unit>
        <trans-unit id="256d0e45774d81cdb6896827af46bd880155747c" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{other}</source>
          <target state="translated">\ text {out} = \ text {input} + \ text {other}</target>
        </trans-unit>
        <trans-unit id="b32a02ba7ef69d1c7496b06c7c87e64e5599ad7d" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1} \text{input}(N_i, C_j, \text{stride} \times l + m)</source>
          <target state="translated">\ text {out}（N_i、C_j、l）= \ frac {1} {k} \ sum_ {m = 0} ^ {k-1} \ text {input}（N_i、C_j、\ text {stride} \ l + mの倍）</target>
        </trans-unit>
        <trans-unit id="6ffc018e43b40a0d8b659cfa4eb86a17bb9a021a" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="translated">\ text {out}（N_i、C _ {\ text {out} _j}）= \ text {bias}（C _ {\ text {out} _j}）+ \ sum_ {k = 0} ^ {C _ {\ text { in}}-1} \ text {weight}（C _ {\ text {out} _j}、k）\ star \ text {input}（N_i、k）</target>
        </trans-unit>
        <trans-unit id="1329f06a718ae43bb8cf775c59ab239d1fb3b8cd" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="translated">\ text {out}（N_i、C _ {\ text {out} _j}）= \ text {bias}（C _ {\ text {out} _j}）+ \ sum_ {k = 0} ^ {C_ {in}- 1} \ text {weight}（C _ {\ text {out} _j}、k）\ star \ text {input}（N_i、k）</target>
        </trans-unit>
        <trans-unit id="43e8084d30aefee2bbf06206dd0a8ab14834163f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \begin{cases} \text{x}_i &amp;amp; \text{if } \text{condition}_i \\ \text{y}_i &amp;amp; \text{otherwise} \\ \end{cases}</source>
          <target state="translated">\ text {out} _i = \ begin {cases} \ text {x} _i＆\ text {if} \ text {condition} _i \\ \ text {y} _i＆\ text {otherwise} \\ \ end {cases }</target>
        </trans-unit>
        <trans-unit id="887bfc6ee868fe88195f2b1a697c2be49d6ee9a3" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)</source>
          <target state="translated">\ text {out} _i = \ beta \ \ text {input} _i + \ alpha \（\ text {batch1} _i \ mathbin {@} \ text {batch2} _i）</target>
        </trans-unit>
        <trans-unit id="65617250468b820b83705d0c5a443bdbe573e367" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \frac{\text{input}_i}{\text{other}_i}</source>
          <target state="translated">\ text {out} _i = \ frac {\ text {input} _i} {\ text {other} _i}</target>
        </trans-unit>
        <trans-unit id="07bf2f98a46000c9d46f464978022925a2bd7e3f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}</source>
          <target state="translated">\ text {out} _i = \ text {input} _i + \ text {value} \ times \ frac {\ text {tensor1} _i} {\ text {tensor2} _i}</target>
        </trans-unit>
        <trans-unit id="3da52d8260a7cb2e43645767f57dc80dcb7dfe10" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i + \ text {value} \ times \ text {tensor1} _i \ times \ text {tensor2} _i</target>
        </trans-unit>
        <trans-unit id="62f87f35ab6f8ad57797570d5573c2164ad3b265" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i \ mathbin {@} \ text {mat2} _i</target>
        </trans-unit>
        <trans-unit id="7a9589802a74e5f4ebdc7b3bf9edf66a572b8b2d" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \times \text{other}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i \ times \ text {other} _i</target>
        </trans-unit>
        <trans-unit id="598abd50041125894fc9d56c763e555dc53626ff" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{other} \times \text{input}_i</source>
          <target state="translated">\ text {out} _i = \ text {other} \ times \ text {input} _i</target>
        </trans-unit>
        <trans-unit id="f19bf9db2f65318c8d3279e96d9b9001602fb09a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{self} ^ {\text{exponent}_i}</source>
          <target state="translated">\ text {out} _i = \ text {self} ^ {\ text {exponent} _i}</target>
        </trans-unit>
        <trans-unit id="a7a2515cc1e93998b3a18636546dddfa38daeec0" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)</source>
          <target state="translated">\ text {out} _i = \ text {start} _i + \ text {weight} _i \ times（\ text {end} _i- \ text {start} _i）</target>
        </trans-unit>
        <trans-unit id="0cb57186f10cc53449c5a3667b3e9e3171f06840" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ \text{exponent}</source>
          <target state="translated">\ text {out} _i = x_i ^ \ text {exponent}</target>
        </trans-unit>
        <trans-unit id="fc2ce21ca05c23e8481f3ba74137e106e861116a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ {\text{exponent}_i}</source>
          <target state="translated">\ text {out} _i = x_i ^ {\ text {exponent} _i}</target>
        </trans-unit>
        <trans-unit id="199c9bafa2de3cddd25186555e3341b49eafa3be" translate="yes" xml:space="preserve">
          <source>\text{out}_i \sim \text{Poisson}(\text{input}_i)</source>
          <target state="translated">\ text {out} _i \ sim \ text {Poisson}（\ text {input} _i）</target>
        </trans-unit>
        <trans-unit id="ec5b93008f32dc9277b22a2479b439d557449a63" translate="yes" xml:space="preserve">
          <source>\text{out}_{i+1} = \text{out}_i + \text{step}.</source>
          <target state="translated">\ text {out} _ {i + 1} = \ text {out} _i + \ text {step}。</target>
        </trans-unit>
        <trans-unit id="ebe2d70cc579db08dc5e67ffe414031300d09f4c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = I_0(\text{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2}</source>
          <target state="translated">\ text {out} _ {i} = I_0（\ text {input} _ {i}）= \ sum_ {k = 0} ^ {\ infty} \ frac {（\ text {input} _ {i} ^ 2 / 4）^ k} {（k！）^ 2}</target>
        </trans-unit>
        <trans-unit id="6b1003c5523ecc63d8ba6aacefacf06606afa128" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cos（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="f24a159b70e10e5357a57a49e58cd32d187c432a" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cos ^ {-1}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="21103602f770a5f7ee7fc1da1e89140f56599227" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cosh（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="84def1c39988847c83528f8fb8f04adcd967d458" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cosh ^ {-1}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="0f2819fdd8d29dbcba0965986f63a77a316be87c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}</source>
          <target state="translated">\ text {out} _ {i} = \ frac {1} {1 + e ^ {-\ text {input} _ {i}}}</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
