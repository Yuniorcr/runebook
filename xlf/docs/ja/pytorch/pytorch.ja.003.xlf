<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="1b9c74bd1fba9cae2375c562757e49b9f3f24764" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;load_state_dict(state_dict: Dict[str, torch.Tensor], strict: bool = True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.load_state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;load_state_dict(state_dict: Dict[str, torch.Tensor], strict: bool = True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.load_state_dict&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="62f66bbcb0b08c54e10a39245830a0c3663b3ee0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;local_rank&lt;/code&gt; is NOT globally unique: it is only unique per process on a machine. Thus, don&amp;rsquo;t use it to decide if you should, e.g., write to a networked filesystem. See &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/12042&quot;&gt;https://github.com/pytorch/pytorch/issues/12042&lt;/a&gt; for an example of how things can go wrong if you don&amp;rsquo;t do this correctly.</source>
          <target state="translated">&lt;code&gt;local_rank&lt;/code&gt; はグローバルに一意ではありません。マシン上のプロセスごとにのみ一意です。したがって、ネットワーク化されたファイルシステムに書き込む必要があるかどうかを判断するために使用しないでください。これを正しく行わないと問題が発生する可能性のある例については、&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/12042&quot;&gt;https：//github.com/pytorch/pytorch/issues/12042&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="aa7c545b613756907a429504c3115615d7789a1a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(input: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(input: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.log_prob&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bc5a71e50b3f16dc1145f31275ba17755e3208f3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;lu(pivot=True, get_infos=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.lu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;lu(pivot=True, get_infos=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.lu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bd04b20a72f6e3eefecb7fc21edef53df71f8ef7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;matmul(A, V[:, :k])&lt;/code&gt; projects data to the first k principal components</source>
          <target state="translated">&lt;code&gt;matmul(A, V[:, :k])&lt;/code&gt; は、最初のk個の主成分にデータを射影します</target>
        </trans-unit>
        <trans-unit id="7139ac5f6d0a956209f36cd92ff11f349707f99e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;method=&amp;rdquo;basic&amp;rdquo;&lt;/code&gt; - the LOBPCG method introduced by Andrew Knyazev, see [Knyazev2001]. A less robust method, may fail when Cholesky is applied to singular input.</source>
          <target state="translated">&lt;code&gt;method=&amp;rdquo;basic&amp;rdquo;&lt;/code&gt; -AndrewKnyazevによって導入されたLOBPCGメソッド。[Knyazev2001]を参照してください。コレスキーが特異入力に適用されると、堅牢性の低い方法が失敗する可能性があります。</target>
        </trans-unit>
        <trans-unit id="5cc2c230f41d50cec42c5a73738ea776357df1f2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;method=&amp;rdquo;ortho&amp;rdquo;&lt;/code&gt; - the LOBPCG method with orthogonal basis selection [StathopoulosEtal2002]. A robust method.</source>
          <target state="translated">&lt;code&gt;method=&amp;rdquo;ortho&amp;rdquo;&lt;/code&gt; -直交基底選択を使用したLOBPCGメソッド[StathopoulosEtal2002]。堅牢な方法。</target>
        </trans-unit>
        <trans-unit id="31632cd895f74d5feae903615972b9be78a76f37" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;modules() &amp;rarr; Iterator[torch.nn.modules.module.Module]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.modules&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;modules() &amp;rarr; Iterator[torch.nn.modules.module.Module]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.modules&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="32e35227c212f090980012e3160ed30e44efafd0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;named_buffers(prefix: str = '', recurse: bool = True) &amp;rarr; Iterator[Tuple[str, torch.Tensor]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_buffers&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;named_buffers(prefix: str = '', recurse: bool = True) &amp;rarr; Iterator[Tuple[str, torch.Tensor]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_buffers&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="048db148a1be9becc9e39073dbeb4b604c390c68" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;named_children() &amp;rarr; Iterator[Tuple[str, torch.nn.modules.module.Module]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_children&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;named_children() &amp;rarr; Iterator[Tuple[str, torch.nn.modules.module.Module]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_children&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="85bacda5665b89ed5cefaace04dda54af89f48fe" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;named_modules(memo: Optional[Set[Module]] = None, prefix: str = '')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_modules&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;named_modules(memo: Optional[Set[Module]] = None, prefix: str = '')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_modules&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22a7ee3e0da6af405272a0ed0646ab619f9fb34b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;named_parameters(prefix: str = '', recurse: bool = True) &amp;rarr; Iterator[Tuple[str, torch.Tensor]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_parameters&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;named_parameters(prefix: str = '', recurse: bool = True) &amp;rarr; Iterator[Tuple[str, torch.Tensor]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_parameters&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dfec0664bec327f4859fdc164e8ab345ca09035a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;names[idx]&lt;/code&gt; corresponds to the name of tensor dimension &lt;code&gt;idx&lt;/code&gt;. Names are either a string if the dimension is named or &lt;code&gt;None&lt;/code&gt; if the dimension is unnamed.</source>
          <target state="translated">&lt;code&gt;names[idx]&lt;/code&gt; は、テンソル次元 &lt;code&gt;idx&lt;/code&gt; の名前に対応します。名前は、ディメンションに名前が付けられている場合は文字列、ディメンションに名前が付いてい &lt;code&gt;None&lt;/code&gt; 場合はNoneのいずれかです。</target>
        </trans-unit>
        <trans-unit id="64961805f0c40621fceb6e60083709ff6fd8c3f7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;nccl&lt;/code&gt; backend is currently the fastest and highly recommended backend when using GPUs. This applies to both single-node and multi-node distributed training.</source>
          <target state="translated">&lt;code&gt;nccl&lt;/code&gt; バックエンドは、GPUを使用する場合、現在最も高速で強く推奨されるバックエンドです。これは、シングルノードとマルチノードの両方の分散トレーニングに適用されます。</target>
        </trans-unit>
        <trans-unit id="b98aabe4badf856a18d383aa4bbcf3819012f841" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;no_sync()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.no_sync&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;no_sync()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.no_sync&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="243529964a8f02b21c1cfb5a2cf1416a48cdb560" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;norm(p='fro', dim=None, keepdim=False, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;norm(p='fro', dim=None, keepdim=False, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.norm&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="67fdf5912494b352e2db89b729e0ae31d14aa6b3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;optimize_for_mobile&lt;/code&gt; will also invoke freeze_module pass which only preserves &lt;code&gt;forward&lt;/code&gt; method. If you have other method to that needed to be preserved, add them into the preserved method list and pass into the method.</source>
          <target state="translated">&lt;code&gt;optimize_for_mobile&lt;/code&gt; は、 &lt;code&gt;forward&lt;/code&gt; メソッドのみを保持するfreeze_moduleパスも呼び出します。保存する必要のあるメソッドが他にある場合は、それらを保存済みメソッドリストに追加して、メソッドに渡します。</target>
        </trans-unit>
        <trans-unit id="6860a3cc1bb85eed802ff8a945080f941221eb86" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;out&lt;/code&gt; can have integral &lt;code&gt;dtype&lt;/code&gt;, but &lt;code&gt;input&lt;/code&gt; must have floating point &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;out&lt;/code&gt; は整数 &lt;code&gt;dtype&lt;/code&gt; を持つことができますが、 &lt;code&gt;input&lt;/code&gt; は浮動小数点 &lt;code&gt;dtype&lt;/code&gt; でなければなりません。</target>
        </trans-unit>
        <trans-unit id="57df9e7349923cce6d85797c07fe7176a04c4730" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;output&lt;/code&gt;: aggregated embedding values of shape &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;output&lt;/code&gt; ：形状の集約された埋め込み値 &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="20bc1b9a285a7d6c1e598719cab22ec2e30a86ed" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;output_padding&lt;/code&gt; controls the additional size added to one side of the output shape. See note below for details.</source>
          <target state="translated">&lt;code&gt;output_padding&lt;/code&gt; は、出力シェイプの片側に追加される追加のサイズを制御します。詳細については、以下の注を参照してください。</target>
        </trans-unit>
        <trans-unit id="c43d9da08c42122b25a623aed7e1ec7d4a1ed5e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;output_size&lt;/code&gt; (optional): the targeted output size</source>
          <target state="translated">&lt;code&gt;output_size&lt;/code&gt; （オプション）：ターゲットの出力サイズ</target>
        </trans-unit>
        <trans-unit id="2f9c7a3c6933d1325ba659f80f703c1d21ba8d09" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;output_size&lt;/code&gt; describes the spatial shape of the large containing tensor of the sliding local blocks. It is useful to resolve the ambiguity when multiple input shapes map to same number of sliding blocks, e.g., with &lt;code&gt;stride &amp;gt; 0&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;output_size&lt;/code&gt; は、スライディングローカルブロックの大きな包含テンソルの空間形状を記述します。複数の入力形状が同じ数のスライディングブロックにマッピングされる場合、たとえば &lt;code&gt;stride &amp;gt; 0&lt;/code&gt; 場合、あいまいさを解決すると便利です。</target>
        </trans-unit>
        <trans-unit id="d1243c702a5a6e39b0598b1177c888c80420b192" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;p_tensor&lt;/code&gt; should be a tensor containing probabilities to be used for drawing the binary random number.</source>
          <target state="translated">&lt;code&gt;p_tensor&lt;/code&gt; は、2進数の乱数を描画するために使用される確率を含むテンソルである必要があります。</target>
        </trans-unit>
        <trans-unit id="71d214acd804f5a2164bbac6db12b07145c9f82a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pad_mode&lt;/code&gt; determines the padding method used on &lt;code&gt;input&lt;/code&gt; when &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. See &lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt;&lt;code&gt;torch.nn.functional.pad()&lt;/code&gt;&lt;/a&gt; for all available options. Default is &lt;code&gt;&quot;reflect&quot;&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;pad_mode&lt;/code&gt; は、 &lt;code&gt;center&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; の場合に &lt;code&gt;input&lt;/code&gt; 使用されるパディング方法を決定します。使用可能なすべてのオプションについては、&lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt; &lt;code&gt;torch.nn.functional.pad()&lt;/code&gt; &lt;/a&gt;を参照してください。デフォルトは &lt;code&gt;&quot;reflect&quot;&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="98da630d9b8562a9fa983197c13c21292635e0d8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pad_sequence&lt;/code&gt; stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size &lt;code&gt;L x *&lt;/code&gt; and if batch_first is False, and &lt;code&gt;T x B x *&lt;/code&gt; otherwise.</source>
          <target state="translated">&lt;code&gt;pad_sequence&lt;/code&gt; は、新しい次元に沿ってテンソルのリストをスタックし、それらを同じ長さにパディングします。たとえば、入力がサイズ &lt;code&gt;L x *&lt;/code&gt; のシーケンスのリストであり、batch_firstがFalseの場合、それ以外の場合は &lt;code&gt;T x B x *&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="5523aea5f1d24240fad2b5cb687abb71fdd8696c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding&lt;/code&gt; controls the amount of implicit zero-paddings on both sides for &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; number of points. See note below for details.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; は、 &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; ための両側の暗黙的なゼロパディングの量を制御します*（kernel_size-1）-ポイントのパディング数。詳細については、以下の注を参照してください。</target>
        </trans-unit>
        <trans-unit id="0965d5762b82b5756e9509e59423399f67ccc74c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding&lt;/code&gt; controls the amount of implicit zero-paddings on both sides for &lt;code&gt;padding&lt;/code&gt; number of points for each dimension before reshaping.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; は、再形成する前に各次元のポイント数を &lt;code&gt;padding&lt;/code&gt; ために、両側の暗黙的なゼロパディングの量を制御します。</target>
        </trans-unit>
        <trans-unit id="87faaf2c7b8606fb40058aa40aed778ec31e990d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding&lt;/code&gt; controls the amount of implicit zero-paddings on both sides for &lt;code&gt;padding&lt;/code&gt; number of points for each dimension.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; は、各次元のポイント数を &lt;code&gt;padding&lt;/code&gt; ために、両側の暗黙的なゼロパディングの量を制御します。</target>
        </trans-unit>
        <trans-unit id="873aa63cc9ed82344812e9f5d81f5ab1f6110075" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding&lt;/code&gt; controls the amount of implicit zero-paddings on both sides for &lt;code&gt;padding&lt;/code&gt; number of points.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; は、ポイント数を &lt;code&gt;padding&lt;/code&gt; ための両側の暗黙的なゼロパディングの量を制御します。</target>
        </trans-unit>
        <trans-unit id="e777921c9b14589989930ecd3404d8f890efee0e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding_mode=&quot;border&quot;&lt;/code&gt;: use border values for out-of-bound grid locations,</source>
          <target state="translated">&lt;code&gt;padding_mode=&quot;border&quot;&lt;/code&gt; ：範囲外のグリッド位置に境界値を使用します。</target>
        </trans-unit>
        <trans-unit id="9bf609a621bd6df6b08d69937c59fb9bda6a4e18" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding_mode=&quot;reflection&quot;&lt;/code&gt;: use values at locations reflected by the border for out-of-bound grid locations. For location far away from the border, it will keep being reflected until becoming in bound, e.g., (normalized) pixel location &lt;code&gt;x = -3.5&lt;/code&gt; reflects by border &lt;code&gt;-1&lt;/code&gt; and becomes &lt;code&gt;x' = 1.5&lt;/code&gt;, then reflects by border &lt;code&gt;1&lt;/code&gt; and becomes &lt;code&gt;x'' = -0.5&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;padding_mode=&quot;reflection&quot;&lt;/code&gt; ：境界外のグリッド位置の境界によって反映される位置の値を使用します。遠く離れた境界からの位置のため、それは、例えば、結合になるまで反射され続ける、（正規化）画素位置が &lt;code&gt;x = -3.5&lt;/code&gt; 境界によって反映 &lt;code&gt;-1&lt;/code&gt; となる &lt;code&gt;x' = 1.5&lt;/code&gt; 、次いで境界によって反映 &lt;code&gt;1&lt;/code&gt; となる &lt;code&gt;x'' = -0.5&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="65643d381010e2d95d811ab0babc2d8ceb40b002" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding_mode=&quot;zeros&quot;&lt;/code&gt;: use &lt;code&gt;0&lt;/code&gt; for out-of-bound grid locations,</source>
          <target state="translated">&lt;code&gt;padding_mode=&quot;zeros&quot;&lt;/code&gt; ： &lt;code&gt;0&lt;/code&gt; 外のグリッド位置には0を使用します。</target>
        </trans-unit>
        <trans-unit id="2fd5be269042a70bd6bfcef4a1beb1fad3f8b126" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;parameters(recurse: bool = True) &amp;rarr; Iterator[torch.nn.parameter.Parameter]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.parameters&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;parameters(recurse: bool = True) &amp;rarr; Iterator[torch.nn.parameter.Parameter]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.parameters&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="389b3925161df2a316dfd68b4bd9f091bc6d9283" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;per_index_weights&lt;/code&gt; (Tensor, optional)</source>
          <target state="translated">&lt;code&gt;per_index_weights&lt;/code&gt; （Tensor、オプション）</target>
        </trans-unit>
        <trans-unit id="a3171c9a0247049dc9a6cc2032a93513a93d4b9e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;per_sample_weights&lt;/code&gt; (Tensor, optional). Has the same shape as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;per_sample_weights&lt;/code&gt; （Tensor、オプション）。 &lt;code&gt;input&lt;/code&gt; と同じ形状です。</target>
        </trans-unit>
        <trans-unit id="e9bcadfa3ee1d7befc56f0936be4d842be3c16e4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pop(key: str) &amp;rarr; Parameter&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.pop&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;pop(key: str) &amp;rarr; Parameter&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.pop&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2d3bcc0091bf56d060c58f75cf9a87f90235fe42" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pop(key: str) &amp;rarr; torch.nn.modules.module.Module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.pop&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;pop(key: str) &amp;rarr; torch.nn.modules.module.Module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.pop&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cecc34a8f924fcf63ac4e86b4df399986ca30840" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;predict(input: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.predict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;predict(input: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.predict&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3feeb93547a40ff63ef93c1edbe8bdb7276b339e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;prune(t, default_mask=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.prune&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;prune(t, default_mask=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.prune&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a0006fd153e3e41368ff174db197035583852421" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;reduction&lt;/code&gt; = &lt;code&gt;'mean'&lt;/code&gt; doesn&amp;rsquo;t return the true kl divergence value, please use &lt;code&gt;reduction&lt;/code&gt; = &lt;code&gt;'batchmean'&lt;/code&gt; which aligns with KL math definition. In the next major release, &lt;code&gt;'mean'&lt;/code&gt; will be changed to be the same as &lt;code&gt;'batchmean'&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;reduction&lt;/code&gt; = &lt;code&gt;'mean'&lt;/code&gt; は真のkl発散値を返しません &lt;code&gt;'batchmean'&lt;/code&gt; 数学の定義と一致する &lt;code&gt;reduction&lt;/code&gt; = 'batchmean'を使用してください。次のメジャーリリースでは、 &lt;code&gt;'mean'&lt;/code&gt; は &lt;code&gt;'batchmean'&lt;/code&gt; と同じになるように変更されます。</target>
        </trans-unit>
        <trans-unit id="9c40fd54ed9b4a947e07d2d97749d3a5a6960fd4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;refine_names(*names)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.refine_names&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;refine_names(*names)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.refine_names&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a26fb9fd98b2c03b3515681e4d92a08a2f7d05f5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_backward_hook(hook: Callable[[Module, Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[None, torch.Tensor]]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_backward_hook&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_backward_hook(hook: Callable[[Module, Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[None, torch.Tensor]]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_backward_hook&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c3fc6039af03180c3ab53c4d591ca38e5fab4f39" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_buffer&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_buffer&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cb44a0e61508db210c21280300a86c46e5d6bdf7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_forward_hook(hook: Callable[..., None]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_forward_hook&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_forward_hook(hook: Callable[..., None]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_forward_hook&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="158c35384b89f58225b64148f33d94a37ef4fc57" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_forward_pre_hook(hook: Callable[..., None]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_forward_pre_hook(hook: Callable[..., None]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="936eaf9b14caaa34ea920fae06e38528e4361e53" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_hook(hook)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.register_hook&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_hook(hook)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.register_hook&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5292ac761c061e747a376f42b7b666c69eaa24fb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_parameter&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_parameter&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9b1596aad33a69c18bddd2a50faac7da7dfe74e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;remove(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.remove&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;remove(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.remove&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a7c7a4c7bb3781e3a5ff82b15f960d784474e526" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rename(*names, **rename_map)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.rename&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rename(*names, **rename_map)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.rename&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3f56c6acc1a2fd9e54a77cd2f6182d7653e821c2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rename_(*names, **rename_map)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.rename_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rename_(*names, **rename_map)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.rename_&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b78f9895d15f5a907942047841acbda1473e2a92" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;requires_grad_(requires_grad: bool = True) &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.requires_grad_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;requires_grad_(requires_grad: bool = True) &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.requires_grad_&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2a6e3641f634a4bff93ea55f39b2b25ad71a52c4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;reset()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine.reset&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;reset()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine.reset&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d5491ec86d7e376ac6bea33efc116ab49c35fa70" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;retain_grad()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.retain_grad&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;retain_grad()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.retain_grad&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="30afcf2664bfd55bdc73c874505670a40ceba5c5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.bfloat16()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.bfloat16)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.bfloat16()&lt;/code&gt; は &lt;code&gt;self.to(torch.bfloat16)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="ba8b07afb3887e1378823dcb6c0026111669eec7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.bool()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.bool)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.bool()&lt;/code&gt; は &lt;code&gt;self.to(torch.bool)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="235e528e147f1d69ea9f027ab1d5f71c3e2b6504" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.byte()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.uint8)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.byte()&lt;/code&gt; は &lt;code&gt;self.to(torch.uint8)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="696fbff031feb55dfc6a3f759d73b60530f439b6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.char()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.int8)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.char()&lt;/code&gt; は &lt;code&gt;self.to(torch.int8)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="dd991c33642a908ee15c44c4567ca13834b6d0e7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.double()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.float64)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.double()&lt;/code&gt; は &lt;code&gt;self.to(torch.float64)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="bb530f2cded8fb18cd0f6bd9e52d044bc47fa9cf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.float()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.float32)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.float()&lt;/code&gt; は &lt;code&gt;self.to(torch.float32)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="c39462395fa123a645a00da7f7061280a0d3b20a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.half()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.float16)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.half()&lt;/code&gt; は &lt;code&gt;self.to(torch.float16)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="ccde90fcc390e97ee3b4f21b89a8c42e1d03051e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.int()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.int32)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.int()&lt;/code&gt; は &lt;code&gt;self.to(torch.int32)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="5058cc3abe53f662df7631dd5211dc4a50368575" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.long()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.int64)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.long()&lt;/code&gt; は &lt;code&gt;self.to(torch.int64)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="2b88b4ac2f04fe1799583ed6e4d6bfc76ea2f417" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.rename(**rename_map)&lt;/code&gt; returns a view on tensor that has dims renamed as specified in the mapping &lt;code&gt;rename_map&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self.rename(**rename_map)&lt;/code&gt; は、マッピング &lt;code&gt;rename_map&lt;/code&gt; で指定されたように名前が変更されたdimを持つテンソルのビューを返します。</target>
        </trans-unit>
        <trans-unit id="6ee77ff93d503175bd69ae14f8b2767c810fb5f1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.rename(*names)&lt;/code&gt; returns a view on tensor, renaming all dimensions positionally using &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;. Use &lt;code&gt;self.rename(None)&lt;/code&gt; to drop names on a tensor.</source>
          <target state="translated">&lt;code&gt;self.rename(*names)&lt;/code&gt; はテンソルのビューを返し、&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt;を使用してすべての次元の名前を位置的に変更します。 &lt;code&gt;self.rename(None)&lt;/code&gt; を使用して、テンソルに名前をドロップします。</target>
        </trans-unit>
        <trans-unit id="11363fccfc09fecbbf6487dc95db457ac7c9d1bb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.short()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.int16)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.short()&lt;/code&gt; は &lt;code&gt;self.to(torch.int16)&lt;/code&gt; ）と同等です。&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="745574568faaeba1d561f1e5ae137746578c731c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.where(condition, y)&lt;/code&gt; is equivalent to &lt;code&gt;torch.where(condition, self, y)&lt;/code&gt;. See &lt;a href=&quot;generated/torch.where#torch.where&quot;&gt;&lt;code&gt;torch.where()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;self.where(condition, y)&lt;/code&gt; は &lt;code&gt;torch.where(condition, self, y)&lt;/code&gt; と同等です。&lt;a href=&quot;generated/torch.where#torch.where&quot;&gt; &lt;code&gt;torch.where()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="80d12cdf60b2ebf7897bf33eeda8316c6146ee5e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self&lt;/code&gt; can have integral &lt;code&gt;dtype&lt;/code&gt;, but &lt;code&gt;p_tensor&lt;/code&gt; must have floating point &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; は積分 &lt;code&gt;dtype&lt;/code&gt; を持つことができますが、 &lt;code&gt;p_tensor&lt;/code&gt; は浮動小数点 &lt;code&gt;dtype&lt;/code&gt; を持たなければなりません。</target>
        </trans-unit>
        <trans-unit id="04afe95e87078af73461e45f69231c868514d31c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self&lt;/code&gt; is a scalar &lt;code&gt;float&lt;/code&gt; value, and &lt;code&gt;exponent&lt;/code&gt; is a tensor. The returned tensor &lt;code&gt;out&lt;/code&gt; is of the same shape as &lt;code&gt;exponent&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; はスカラー &lt;code&gt;float&lt;/code&gt; 値であり、 &lt;code&gt;exponent&lt;/code&gt; はテンソルです。返されたテンソル &lt;code&gt;out&lt;/code&gt; 同じ形状である &lt;code&gt;exponent&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9f8bc9073599a1c05ac8a8256c20ef81117a207c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self&lt;/code&gt;, &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;src&lt;/code&gt; (if it is a Tensor) should have same number of dimensions. It is also required that &lt;code&gt;index.size(d) &amp;lt;= src.size(d)&lt;/code&gt; for all dimensions &lt;code&gt;d&lt;/code&gt;, and that &lt;code&gt;index.size(d) &amp;lt;= self.size(d)&lt;/code&gt; for all dimensions &lt;code&gt;d != dim&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 、 &lt;code&gt;index&lt;/code&gt; 、および &lt;code&gt;src&lt;/code&gt; （Tensorの場合）は同じ数の次元を持つ必要があります。また、ことが必要とされる &lt;code&gt;index.size(d) &amp;lt;= src.size(d)&lt;/code&gt; 全ての寸法については、 &lt;code&gt;d&lt;/code&gt; は、その &lt;code&gt;index.size(d) &amp;lt;= self.size(d)&lt;/code&gt; すべての次元のための &lt;code&gt;d != dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="84785e3dd4238879d9071b5f542dcf76287afa5f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self&lt;/code&gt;, &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;src&lt;/code&gt; should have same number of dimensions. It is also required that &lt;code&gt;index.size(d) &amp;lt;= src.size(d)&lt;/code&gt; for all dimensions &lt;code&gt;d&lt;/code&gt;, and that &lt;code&gt;index.size(d) &amp;lt;= self.size(d)&lt;/code&gt; for all dimensions &lt;code&gt;d != dim&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 、 &lt;code&gt;index&lt;/code&gt; 、および &lt;code&gt;src&lt;/code&gt; の次元数は同じである必要があります。また、ことが必要とされる &lt;code&gt;index.size(d) &amp;lt;= src.size(d)&lt;/code&gt; 全ての寸法については、 &lt;code&gt;d&lt;/code&gt; は、その &lt;code&gt;index.size(d) &amp;lt;= self.size(d)&lt;/code&gt; すべての次元のための &lt;code&gt;d != dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b8db8aa8a39114a3bb4d5fd0455bd46f531532d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sequences&lt;/code&gt; should be a list of Tensors of size &lt;code&gt;L x *&lt;/code&gt;, where &lt;code&gt;L&lt;/code&gt; is the length of a sequence and &lt;code&gt;*&lt;/code&gt; is any number of trailing dimensions, including zero.</source>
          <target state="translated">&lt;code&gt;sequences&lt;/code&gt; は、サイズ &lt;code&gt;L x *&lt;/code&gt; のテンソルのリストである必要があります。ここで、 &lt;code&gt;L&lt;/code&gt; はシーケンスの長さであり、 &lt;code&gt;*&lt;/code&gt; はゼロを含む任意の数の末尾の次元です。</target>
        </trans-unit>
        <trans-unit id="735447a30cd08376a5904648fe0520b6748f549e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_grad_enabled&lt;/code&gt; will enable or disable grads based on its argument &lt;a href=&quot;torch.mode#torch.mode&quot;&gt;&lt;code&gt;mode&lt;/code&gt;&lt;/a&gt;. It can be used as a context-manager or as a function.</source>
          <target state="translated">&lt;code&gt;set_grad_enabled&lt;/code&gt; は、引数&lt;a href=&quot;torch.mode#torch.mode&quot;&gt; &lt;code&gt;mode&lt;/code&gt; &lt;/a&gt;基づいて卒業生を有効または無効にします。コンテキストマネージャーまたは関数として使用できます。</target>
        </trans-unit>
        <trans-unit id="7a841331f9feb2d5c5159d5c906fc881114af61e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_result(result: T) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.set_result&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;set_result(result: T) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.set_result&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a0fd6f8837947d9dfd2011aecf91b34593d64433" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;share_memory_()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.share_memory_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;share_memory_()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.share_memory_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5ada337e154f8b6156cdfc96e0db2f536f0893f6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;size&lt;/code&gt; is the number of elements in the storage. If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the file must contain at least &lt;code&gt;size * sizeof(Type)&lt;/code&gt; bytes (&lt;code&gt;Type&lt;/code&gt; is the type of storage). If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; the file will be created if needed.</source>
          <target state="translated">&lt;code&gt;size&lt;/code&gt; は、ストレージ内の要素の数です。 &lt;code&gt;shared&lt;/code&gt; が &lt;code&gt;False&lt;/code&gt; の場合、ファイルには少なくとも &lt;code&gt;size * sizeof(Type)&lt;/code&gt; バイトが含まれている必要があります（ &lt;code&gt;Type&lt;/code&gt; はストレージのタイプです）。場合は &lt;code&gt;shared&lt;/code&gt; ある &lt;code&gt;True&lt;/code&gt; 必要に応じてファイルが作成されます。</target>
        </trans-unit>
        <trans-unit id="775a9cd83a519b88484214ea6efd0a87345ed211" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;size_average&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; are in the process of being deprecated, and in the meantime, specifying either of those two args will override &lt;code&gt;reduction&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;size_average&lt;/code&gt; と &lt;code&gt;reduce&lt;/code&gt; は非推奨になる過程にあり、その間、これら2つの引数のいずれかを指定すると &lt;code&gt;reduction&lt;/code&gt; が上書きされます。</target>
        </trans-unit>
        <trans-unit id="968d0dce1541ed993d4b1f29fd9e1515b8a4c0e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sizes&lt;/code&gt; is the new shape of the unflattened dimension and it can be a &lt;code&gt;Tuple[int]&lt;/code&gt; as well as &lt;code&gt;torch.Size&lt;/code&gt; if &lt;code&gt;self&lt;/code&gt; is a &lt;code&gt;Tensor&lt;/code&gt;, or &lt;code&gt;namedshape&lt;/code&gt; (Tuple[(name: str, size: int)]) if &lt;code&gt;self&lt;/code&gt; is a &lt;code&gt;NamedTensor&lt;/code&gt;. The total number of elements in sizes must match the number of elements in the original dim being unflattened.</source>
          <target state="translated">&lt;code&gt;sizes&lt;/code&gt; 、平坦化されない寸法の新たな形状であり、それはあることができる &lt;code&gt;Tuple[int]&lt;/code&gt; ならびに &lt;code&gt;torch.Size&lt;/code&gt; があれば &lt;code&gt;self&lt;/code&gt; ある &lt;code&gt;Tensor&lt;/code&gt; 、または &lt;code&gt;namedshape&lt;/code&gt; （タプル[（名称：STR、サイズ：INT）]）場合に &lt;code&gt;self&lt;/code&gt; であります &lt;code&gt;NamedTensor&lt;/code&gt; 。サイズの要素の総数は、平坦化されていない元の薄暗い要素の数と一致する必要があります。</target>
        </trans-unit>
        <trans-unit id="e28ebaeabfc2c0d6b3aa7c33dff1b51cdc74c19f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;split(split_size, dim=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.split&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;split(split_size, dim=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.split&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2d70995f5b7bd944127ae56e30134e1589387bff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;state_dict(destination=None, prefix='', keep_vars=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;state_dict(destination=None, prefix='', keep_vars=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.state_dict&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9478657a6bc48f9b2e80d90d60e5b04ab9b9aae2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(context_id)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/optim/optimizer.html#DistributedOptimizer.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(context_id)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/optim/optimizer.html#DistributedOptimizer.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e5a5904fe6a7743321ff5c71f315d635b92b3c9e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stft(n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.stft&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;stft(n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.stft&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a2d4b82f8a4a08f439941b4fe2cd2f4e127ee11" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stride&lt;/code&gt; controls the stride for the cross-correlation, a single number or a one-element tuple.</source>
          <target state="translated">&lt;code&gt;stride&lt;/code&gt; は、相互相関、単一の数値、または1つの要素のタプルのストライドを制御します。</target>
        </trans-unit>
        <trans-unit id="a32aa3c0b68f959b763c5ab1ff65ebd608f9255b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stride&lt;/code&gt; controls the stride for the cross-correlation, a single number or a tuple.</source>
          <target state="translated">&lt;code&gt;stride&lt;/code&gt; は、相互相関、単一の数値、またはタプルのストライドを制御します。</target>
        </trans-unit>
        <trans-unit id="a7cc6339d97b370d095743b2c41bc1edfe10d964" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stride&lt;/code&gt; controls the stride for the cross-correlation.</source>
          <target state="translated">&lt;code&gt;stride&lt;/code&gt; は、相互相関のストライドを制御します。</target>
        </trans-unit>
        <trans-unit id="742806613e72fcf7d80117cd493eb9ec43cefbce" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stride&lt;/code&gt; controls the stride for the sliding blocks.</source>
          <target state="translated">&lt;code&gt;stride&lt;/code&gt; は、スライディングブロックのストライドを制御します。</target>
        </trans-unit>
        <trans-unit id="56b9b812f41a06ed4638217119c49e950638afeb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;tensor&lt;/code&gt; must have the same number of elements in all processes participating in the collective.</source>
          <target state="translated">&lt;code&gt;tensor&lt;/code&gt; は、集合に参加するすべてのプロセスで同じ数の要素を持っている必要があります。</target>
        </trans-unit>
        <trans-unit id="224069f965b222e903606022d10e7a98e065b9ae" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;tensor&lt;/code&gt; must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU</source>
          <target state="translated">&lt;code&gt;tensor&lt;/code&gt; は、コレクティブに参加しているすべてのプロセスのすべてのGPUに同じ数の要素を持っている必要があります。リスト内の各テンソルは異なるGPU上にある必要があります</target>
        </trans-unit>
        <trans-unit id="3d41bbb307c82a2c9b49f046126aa9c2345c1070" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;then(callback)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.then&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;then(callback)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.then&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="578af95c3bd45de88eac7fe154557b4d4985f0d0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(*args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(*args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7ac5282afe4693a20c19177c7dc6ba853efba47f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(*args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#PackedSequence.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(*args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#PackedSequence.to&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3dc0e16d9d7f541291e6dbab067b29c301d2e7ec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(device=None, dtype=None, non_blocking=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(device=None, dtype=None, non_blocking=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3e596c69ad0590b92b57b912529efbcc355b682e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(dtype, non_blocking=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(dtype, non_blocking=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8ccfbe77f465473aa1da76dc9c34f89236b6724c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(memory_format=torch.channels_last)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(memory_format=torch.channels_last)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f08eeed12af7805bba2c17a64a2bdeecb1e24711" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(tensor, non_blocking=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(tensor, non_blocking=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eab345e7e4a5581554ed11c71dbb6edfff8c371e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;tol&lt;/code&gt; is the threshold below which the singular values (or the eigenvalues when &lt;code&gt;symmetric&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;) are considered to be 0. If &lt;code&gt;tol&lt;/code&gt; is not specified, &lt;code&gt;tol&lt;/code&gt; is set to &lt;code&gt;S.max() * max(S.size()) * eps&lt;/code&gt; where &lt;code&gt;S&lt;/code&gt; is the singular values (or the eigenvalues when &lt;code&gt;symmetric&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;), and &lt;code&gt;eps&lt;/code&gt; is the epsilon value for the datatype of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;tol&lt;/code&gt; （場合又は固有値以下特異値閾値で &lt;code&gt;symmetric&lt;/code&gt; ある &lt;code&gt;True&lt;/code&gt; ）場合は0であると考えられている &lt;code&gt;tol&lt;/code&gt; 指定されていない、 &lt;code&gt;tol&lt;/code&gt; に設定されている &lt;code&gt;S.max() * max(S.size()) * eps&lt;/code&gt; ここで、 &lt;code&gt;S&lt;/code&gt; は特異値（または &lt;code&gt;symmetric&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; の場合は固有値）であり、 &lt;code&gt;eps&lt;/code&gt; は &lt;code&gt;input&lt;/code&gt; のデータタイプのイプシロン値です。</target>
        </trans-unit>
        <trans-unit id="18fe92a88d4b832404cc2dbd206ba724755bd2f8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.Assert(condition, message)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#Assert&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.Assert(condition, message)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#Assert&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d791ae567ecad27f4445cb2aaf15f6029d488510" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.atleast_1d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.atleast_1d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1656ab860bd5829558905cf483ea11bc0039dbff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.atleast_2d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.atleast_2d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dd52a2bd34a2723d8cb64d098bfcb5aade97fdc1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.atleast_3d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.atleast_3d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="807982babd7e2697d02339a41138a44e111d331f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.cuda.is_built()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cuda.html#is_built&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.cuda.is_built()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cuda.html#is_built&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cad84bdf2432c05fed2fe1958d51387bbc6c0020" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.cudnn.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cudnn.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.cudnn.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cudnn.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="00b7a3cf7f24c2b2bebb11277acabb7c78d50317" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.cudnn.version()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cudnn.html#version&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.cudnn.version()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cudnn.html#version&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9dfbb3781e5878595a4417510d5aae33943c7ef2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/mkl.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/mkl.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="72e4f98e5910cf117b1472f53b8f73df836ca78a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.mkldnn.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/mkldnn.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.mkldnn.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/mkldnn.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7054ed97a3665299406dd42d55c4229d95f96305" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.openmp.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/openmp.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.openmp.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/openmp.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4db6ff3c64dff7c5f1e9ecefcab9b3fe7f8bd5ef" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends&lt;/code&gt; controls the behavior of various backends that PyTorch supports.</source>
          <target state="translated">&lt;code&gt;torch.backends&lt;/code&gt; は、PyTorchがサポートするさまざまなバックエンドの動作を制御します。</target>
        </trans-unit>
        <trans-unit id="4798618741fa5295873eda72f10f638e521941e3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.block_diag(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#block_diag&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.block_diag(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#block_diag&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="561c4a9dc39238e682070f0062e9aef0052df081" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.broadcast_tensors(*tensors) &amp;rarr; List of Tensors&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#broadcast_tensors&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.broadcast_tensors(*tensors) &amp;rarr; List of Tensors&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#broadcast_tensors&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f9abf70758cd66412725ba8672052a7932024490" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cartesian_prod(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#cartesian_prod&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cartesian_prod(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#cartesian_prod&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eec3865daff49e31eb283e5df540594f2c021646" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#cdist&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#cdist&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dd3bb7208ad3447bd28402045aaa3520bba00449" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.chain_matmul(*matrices)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#chain_matmul&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.chain_matmul(*matrices)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#chain_matmul&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9b7ede943ebb499689b4fa6f38f84fe9ace9c3ad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.channels_last&lt;/code&gt;: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in &lt;code&gt;strides[0] &amp;gt; strides[2] &amp;gt; strides[3] &amp;gt; strides[1] == 1&lt;/code&gt; aka NHWC order.</source>
          <target state="translated">&lt;code&gt;torch.channels_last&lt;/code&gt; ：テンソルは、密集した重複しないメモリに割り当てられているか、割り当てられます。strides &lt;code&gt;strides[0] &amp;gt; strides[2] &amp;gt; strides[3] &amp;gt; strides[1] == 1&lt;/code&gt; strides [3]&amp;gt; strides [1] == 1、別名NHWCオーダーの値で表されるストライド。</target>
        </trans-unit>
        <trans-unit id="7261c6034d86aa2ddc9431902e39f474a13e60a4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cholesky_solve(b, u)&lt;/code&gt; can take in 2D inputs &lt;code&gt;b, u&lt;/code&gt; or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs &lt;code&gt;c&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.cholesky_solve(b, u)&lt;/code&gt; は、2D入力 &lt;code&gt;b, u&lt;/code&gt; または2D行列のバッチである入力を取り込むことができます。入力がバッチの場合、バッチ出力を返します &lt;code&gt;c&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d68363efb3452b6e83ed851d4699a5aa0f779741" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.compiled_with_cxx11_abi()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#compiled_with_cxx11_abi&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.compiled_with_cxx11_abi()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#compiled_with_cxx11_abi&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e126cacbad4b261249efa0e8913738a1877bbdfa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.complex128&lt;/code&gt; or &lt;code&gt;torch.cdouble&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.complex128&lt;/code&gt; または &lt;code&gt;torch.cdouble&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f39fd8648813e09f8476a9ac8da4abf78e93db3b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.complex64&lt;/code&gt; or &lt;code&gt;torch.cfloat&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.complex64&lt;/code&gt; または &lt;code&gt;torch.cfloat&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="dbaed180a18c68c202c3fc91f47ffeb99ac16e81" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.contiguous_format&lt;/code&gt;: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.</source>
          <target state="translated">&lt;code&gt;torch.contiguous_format&lt;/code&gt; ：テンソルは、高密度の重複しないメモリに割り当てられているか、割り当てられます。降順の値で表されるストライド。</target>
        </trans-unit>
        <trans-unit id="3684d59244595e52fd2828b50ca6282d62fd4739" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_gather(tensor_list, tensor, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_gather&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_gather(tensor_list, tensor, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_gather&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="288bfa32c25fcb8306179eca04e736e2a02186b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_gather_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_gather_multigpu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8413a7e854b6ea51eaa6b4f60e55aab063717a94" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_reduce(tensor, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_reduce&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_reduce(tensor, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_reduce&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8e54a6af7a1934d02149ec3e7822e2c3aacde8c6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_reduce_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_reduce_multigpu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b02380125bbd88799d9b414d91da3147fc32cac4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_to_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_to_all&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="811431042ba4ac28ba32b7911f195c224a3cc8ed" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.barrier(group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#barrier&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.barrier(group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#barrier&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="972f101974bfb6f5d4198acab57d132de3def05d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.broadcast(tensor, src, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#broadcast&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.broadcast(tensor, src, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#broadcast&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="42f137f701d871ed16f93e8f54ccf11f3751a266" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.broadcast_multigpu(tensor_list, src, group=&amp;lt;object object&amp;gt;, async_op=False, src_tensor=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#broadcast_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.broadcast_multigpu(tensor_list, src, group=&amp;lt;object object&amp;gt;, async_op=False, src_tensor=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#broadcast_multigpu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a91a9c5b16b5a59e71ee3db5306f6bfcf11d1b07" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.gather(tensor, gather_list=None, dst=0, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#gather&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.gather(tensor, gather_list=None, dst=0, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#gather&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="469f0c245793ba3173d50a4ba6d0d4b02a50ac40" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.get_backend(group=&amp;lt;object object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_backend&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.get_backend(group=&amp;lt;object object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_backend&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d9abfee4a0b4cccfc17184dd0b1cc26e5d371ee3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.get_rank(group=&amp;lt;object object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_rank&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.get_rank(group=&amp;lt;object object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_rank&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3d2f006f10e722cd486cd5167f1f023ac4348cde" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.get_world_size(group=&amp;lt;object object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_world_size&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.get_world_size(group=&amp;lt;object object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_world_size&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3cd2bdef011082ccaf0f6cdb5ff29f262f1a95e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(0, 1800), world_size=-1, rank=-1, store=None, group_name='')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#init_process_group&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(0, 1800), world_size=-1, rank=-1, store=None, group_name='')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#init_process_group&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="be9e65a88a9fe81456b9d46075c4010654d4e9b0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.irecv(tensor, src, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#irecv&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.irecv(tensor, src, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#irecv&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="20f496294e21fb47e152e675602dbf3c461a9b08" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="223963009f0daaa6463b96377bfe6ba435636eb6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.is_initialized()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_initialized&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.is_initialized()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_initialized&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cbd076810efd1ed9ebbabe499f72753a91996038" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.is_mpi_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_mpi_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.is_mpi_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_mpi_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="db46fe559b0d0bbdf0cef5a5524c681f3d238f08" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.is_nccl_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_nccl_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.is_nccl_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_nccl_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e6b9418c661191c8470ab193315235ef4714a868" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.isend(tensor, dst, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#isend&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.isend(tensor, dst, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#isend&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9dd059c98fffcac9f54ce1694e15b99004a8591e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.launch&lt;/code&gt; is a module that spawns up multiple distributed training processes on each of the training nodes.</source>
          <target state="translated">&lt;code&gt;torch.distributed.launch&lt;/code&gt; は、各トレーニングノードで複数の分散トレーニングプロセスを生成するモジュールです。</target>
        </trans-unit>
        <trans-unit id="00f170d324ae5b762271dfa97f204b0ab1323083" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(0, 1800), backend=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#new_group&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(0, 1800), backend=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#new_group&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c722cbc71a64e0dc42cb9a9d3c7a5edc03b0d4ef" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.recv(tensor, src=None, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#recv&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.recv(tensor, src=None, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#recv&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4934f4472f321622371e7a7b50861754714e2974" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.reduce(tensor, dst, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.reduce(tensor, dst, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4853070b0ea1c763754c27135933c602998a4e60" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.reduce_multigpu(tensor_list, dst, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False, dst_tensor=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.reduce_multigpu(tensor_list, dst, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False, dst_tensor=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_multigpu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="73023ab881e640865f6cc8d6a5f7e0c4057841ad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.reduce_scatter(output, input_list, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_scatter&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.reduce_scatter(output, input_list, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_scatter&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2dbd0e9e1283295f2349e8245482b78e102b7f5f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_scatter_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_scatter_multigpu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="658d9ec010176f539c7002ca6f7244824cf0ff2e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.functions.async_execution(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/functions.html#async_execution&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.functions.async_execution(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/functions.html#async_execution&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="11ee0ae675da3185be2d88cf7686272070008035" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.get_worker_info(worker_name=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#get_worker_info&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.get_worker_info(worker_name=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#get_worker_info&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0ac681363a6df2035cf0af007fbe15e5f40b1313" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.init_rpc(name, backend=None, rank=-1, world_size=None, rpc_backend_options=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc.html#init_rpc&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.init_rpc(name, backend=None, rank=-1, world_size=None, rpc_backend_options=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc.html#init_rpc&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a807f1699fa2b1f80a8a85a3c47163cacb1811a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.remote(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#remote&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.remote(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#remote&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4298605ee9a8f80fa78f8bafe3d0d10bc0b63530" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.rpc_async(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#rpc_async&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.rpc_async(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#rpc_async&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0ea65440f151425bd963fae7e719909e0e6f7799" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.rpc_sync(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#rpc_sync&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.rpc_sync(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#rpc_sync&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d1578868b0db95ef02e8c1751bf13334567357d8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.shutdown(graceful=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#shutdown&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.shutdown(graceful=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#shutdown&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="74962bc84981f59ffb46c79d8d0a32c1fa2c88fb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.scatter(tensor, scatter_list=None, src=0, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#scatter&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.scatter(tensor, scatter_list=None, src=0, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#scatter&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1a91666abb9d1adeac875045b37d1d67269e7268" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.send(tensor, dst, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#send&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.send(tensor, dst, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#send&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b242b4c4d95c3dfc78fd815399bf2a8ab4321962" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed&lt;/code&gt; supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.</source>
          <target state="translated">&lt;code&gt;torch.distributed&lt;/code&gt; は、それぞれ異なる機能を持つ3つの組み込みバックエンドをサポートします。次の表は、CPU / CUDAテンソルで使用できる関数を示しています。MPIは、PyTorchの構築に使用された実装がCUDAをサポートしている場合にのみCUDAをサポートします。</target>
        </trans-unit>
        <trans-unit id="8cbefef3cb884be3385871dce8b2b62386448a14" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.einsum(equation, *operands) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#einsum&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.einsum(equation, *operands) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#einsum&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="640fe0a067d6b473ac9256af65e952ac6dfc3b61" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.half&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.float16&lt;/code&gt; または &lt;code&gt;torch.half&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="bae15d4af90dddb94594a94fec29f67ac605b467" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.float32&lt;/code&gt; or &lt;code&gt;torch.float&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.float32&lt;/code&gt; または &lt;code&gt;torch.float&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="90d6fe76faa60bedbd01aec13dc05b425977c0c5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.float64&lt;/code&gt; or &lt;code&gt;torch.double&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.float64&lt;/code&gt; または &lt;code&gt;torch.double&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="265f8d449a3becac9b19de67b27384583c69e026" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.futures.collect_all(futures: List[torch.jit.Future]) &amp;rarr; torch.futures.Future[List[torch.jit.Future]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#collect_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.futures.collect_all(futures: List[torch.jit.Future]) &amp;rarr; torch.futures.Future[List[torch.jit.Future]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#collect_all&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bc976900f35641949861ea9c8ea17e248ec3810f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.futures.wait_all(futures: List[torch.jit.Future]) &amp;rarr; List&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#wait_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.futures.wait_all(futures: List[torch.jit.Future]) &amp;rarr; List&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#wait_all&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ae80701e04934a777dbdb12d87e5a0f70bd523c5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.get_rng_state() &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#get_rng_state&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.get_rng_state() &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#get_rng_state&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3bcc059f1d0a236e31a5f5312334d77b1ac57459" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#download_url_to_file&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#download_url_to_file&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f87f9cf1e2fb7ea9e5ce5d50dcecf4a61cc11583" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.get_dir()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#get_dir&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.get_dir()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#get_dir&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6f4fba1da5aa27b4afa12ceb8026cb1b5c9a7926" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.help(github, model, force_reload=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#help&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.help(github, model, force_reload=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#help&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="882b251a8fb497576db4d5562fe368823616bfec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.list(github, force_reload=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#list&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.list(github, force_reload=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#list&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5cd79bde08bc8ad0034543204f4d73cd57529deb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.load(repo_or_dir, model, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#load&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.load(repo_or_dir, model, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#load&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="79c43b2a087c0324bade258ba72699891cf8e21a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#load_state_dict_from_url&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#load_state_dict_from_url&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1d5ca2b155b44bdba1204740a35f42adbc43fcba" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.set_dir(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#set_dir&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.set_dir(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#set_dir&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c5f8c8ccc73401651bfcc5f84e5b8bd9b3b988f2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.index_add_()&lt;/code&gt; when called on a CUDA tensor</source>
          <target state="translated">&lt;code&gt;torch.index_add_()&lt;/code&gt; CUDAテンソルで呼び出されたときのtorch.index_add_（）</target>
        </trans-unit>
        <trans-unit id="e8c9e450410f2773a3b1ffb535039963fc75b581" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.initial_seed() &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#initial_seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.initial_seed() &amp;rarr; int&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#initial_seed&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a2618a717b800b2250c20f08796353ec5f6174dc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.int16&lt;/code&gt; or &lt;code&gt;torch.short&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.int16&lt;/code&gt; または &lt;code&gt;torch.short&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c846dd030e173dece1f667ac83f5f7c89e7ef730" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.int32&lt;/code&gt; or &lt;code&gt;torch.int&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.int32&lt;/code&gt; または &lt;code&gt;torch.int&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f84a6d005309b8b164f747fff43a7112692349b3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.int64&lt;/code&gt; or &lt;code&gt;torch.long&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.int64&lt;/code&gt; または &lt;code&gt;torch.long&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2d7902807a818755d110fce5fb71486036a8b000" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.is_deterministic()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_deterministic&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.is_deterministic()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_deterministic&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b61e78ddb3d215366857032848241968e697c5cf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.is_storage(obj)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_storage&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.is_storage(obj)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_storage&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3fba0f4d5b1df772c7aadec0c1a6d0985cd38b1a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.is_tensor(obj)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_tensor&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.is_tensor(obj)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_tensor&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e0e4a4b6b75b92120aeb93fcdb25e4b132a47ecd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.istft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#istft&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.istft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#istft&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89e22ec54610807a0b00f1d4c91a1d7ec876f10a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.export(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#export&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.export(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#export&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b8bbb3f29cb40c5d3ea9776b88cf43c34c1878da" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.fork(func, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_async.html#fork&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.fork(func, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_async.html#fork&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="421559b3abd5edb03900fba11315d83fc334603d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.ignore(drop=False, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#ignore&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.ignore(drop=False, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#ignore&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eeb12aa66d7656a4ffc0d96d046cf3b6b08a0bde" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.is_scripting()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#is_scripting&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.is_scripting()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#is_scripting&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f88e9a93ad8baaaea0c628df0a8e7ac2c56e5c41" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.load(f, map_location=None, _extra_files=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_serialization.html#load&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.load(f, map_location=None, _extra_files=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_serialization.html#load&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e92c3fdd876652458f659e7da2613f06913984b5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.save(m, f, _extra_files=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_serialization.html#save&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.save(m, f, _extra_files=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_serialization.html#save&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6643232796915d1c4cfa96d674412c4c0bb284ea" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.script(obj, optimize=None, _frames_up=0, _rcb=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_script.html#script&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.script(obj, optimize=None, _frames_up=0, _rcb=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_script.html#script&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4db5d8c45560a499d468c5dac1c897f99ebef493" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.script&lt;/code&gt; can be used as a function for modules and functions, and as a decorator &lt;code&gt;@torch.jit.script&lt;/code&gt; for &lt;a href=&quot;../jit_language_reference#id2&quot;&gt;TorchScript Classes&lt;/a&gt; and functions.</source>
          <target state="translated">&lt;code&gt;torch.jit.script&lt;/code&gt; は、モジュールや関数の関数として使用され、デコレータなどすることができます &lt;code&gt;@torch.jit.script&lt;/code&gt; ため&lt;a href=&quot;../jit_language_reference#id2&quot;&gt;TorchScriptクラス&lt;/a&gt;と関数。</target>
        </trans-unit>
        <trans-unit id="a98b44186e36671ec40c3bdafe708c3fee23035e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.trace(func, example_inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=&amp;lt;torch._C.CompilationUnit object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_trace.html#trace&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.trace(func, example_inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=&amp;lt;torch._C.CompilationUnit object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_trace.html#trace&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c9049933bf5baeb51bda091d7ad902658b9ddde4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.trace_module(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=&amp;lt;torch._C.CompilationUnit object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_trace.html#trace_module&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.trace_module(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=&amp;lt;torch._C.CompilationUnit object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_trace.html#trace_module&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="909e5d79046847b4d02782a7a02f0a2ed016af7c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.unused(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#unused&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.unused(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#unused&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f65c9de77cddf9268ba06604577821b9c763e902" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.wait(future)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_async.html#wait&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.wait(future)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_async.html#wait&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fb47c5bcc3ae1dfb1d388e26ba7411471ab097bf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.load(f, map_location=None, pickle_module=&amp;lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&amp;gt;, **pickle_load_args)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/serialization.html#load&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.load(f, map_location=None, pickle_module=&amp;lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&amp;gt;, **pickle_load_args)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/serialization.html#load&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8f43737887598a9e805be62f92deca699d0575a2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.lobpcg(A: torch.Tensor, k: Optional[int] = None, B: Optional[torch.Tensor] = None, X: Optional[torch.Tensor] = None, n: Optional[int] = None, iK: Optional[torch.Tensor] = None, niter: Optional[int] = None, tol: Optional[float] = None, largest: Optional[bool] = None, method: Optional[str] = None, tracker: None = None, ortho_iparams: Optional[Dict[str, int]] = None, ortho_fparams: Optional[Dict[str, float]] = None, ortho_bparams: Optional[Dict[str, bool]] = None) &amp;rarr; Tuple[torch.Tensor, torch.Tensor]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lobpcg.html#lobpcg&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.lobpcg(A: torch.Tensor, k: Optional[int] = None, B: Optional[torch.Tensor] = None, X: Optional[torch.Tensor] = None, n: Optional[int] = None, iK: Optional[torch.Tensor] = None, niter: Optional[int] = None, tol: Optional[float] = None, largest: Optional[bool] = None, method: Optional[str] = None, tracker: None = None, ortho_iparams: Optional[Dict[str, int]] = None, ortho_fparams: Optional[Dict[str, float]] = None, ortho_bparams: Optional[Dict[str, bool]] = None) &amp;rarr; Tuple[torch.Tensor, torch.Tensor]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lobpcg.html#lobpcg&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0bea827671aef5f3b7eda50b4794348264f9aff5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#lu_unpack&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#lu_unpack&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5d5a6b9339a9ae4c05a5ba1a9d565042e21e439d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.manual_seed(seed) &amp;rarr; torch._C.Generator&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#manual_seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.manual_seed(seed) &amp;rarr; torch._C.Generator&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#manual_seed&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="073d41ae4569e0590b24bb02a2cde22f383b83d9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.meshgrid(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#meshgrid&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.meshgrid(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#meshgrid&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a39a57d1a7c56039fc95abb2d9bced03e8b97998" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.FractionalMaxPool3d&lt;/code&gt; when called on a CUDA tensor that requires grad</source>
          <target state="translated">&lt;code&gt;torch.nn.FractionalMaxPool3d&lt;/code&gt; gradを必要とするCUDAテンソルで呼び出された場合のtorch.nn.FractionalMaxPool3d</target>
        </trans-unit>
        <trans-unit id="d2ef2c94f97ed017c925779284bb77ad0ca08325" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.ModuleList&lt;/code&gt; which can be used in a TorchScript for loop</source>
          <target state="translated">&lt;code&gt;torch.nn.ModuleList&lt;/code&gt; ループで使用できるtorch.nn.ModuleList</target>
        </trans-unit>
        <trans-unit id="32366e9745ef65d7bfb28a4d81602df1879fc1f0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.adaptive_avg_pool2d(input, output_size)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#adaptive_avg_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.adaptive_avg_pool2d(input, output_size)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#adaptive_avg_pool2d&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="91c0ea25976bc55d2262d00f521746f256fd7d87" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.adaptive_avg_pool3d(input, output_size)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#adaptive_avg_pool3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.adaptive_avg_pool3d(input, output_size)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#adaptive_avg_pool3d&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b1097d0a9532bd8fca9736e541af64e621dd46ea" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.affine_grid(theta, size, align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#affine_grid&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.affine_grid(theta, size, align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#affine_grid&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c10efc3580c8c69cc035cb33b2726bd8f14a73ab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#alpha_dropout&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#alpha_dropout&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c9e17e7f47d7822e2cc0d473c5d222b14fdb92ae" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#batch_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#batch_norm&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3f5717edf64a59d5e363eebc9908da8813501fda" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.bilinear(input1, input2, weight, bias=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#bilinear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.bilinear(input1, input2, weight, bias=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#bilinear&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="59ddc7997f3b59e0fae45c87ee15abd7cfb5c780" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#binary_cross_entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#binary_cross_entropy&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6d7201823c06556ca24e08ebd1f5fb4265f72f2b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#binary_cross_entropy_with_logits&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#binary_cross_entropy_with_logits&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1f9da1009f60918b558c97a1d7a7e883d1c08ef5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.celu(input, alpha=1., inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#celu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.celu(input, alpha=1., inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#celu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="33d37a2b8a8128d94d1481492dac9ef306f0a856" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#cosine_embedding_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#cosine_embedding_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="57abf7da230c9f41633856a8b01cff56c970873e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#cross_entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#cross_entropy&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f1407e69c5d1e13d69f6e78ff47b7322aba83bca" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#ctc_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#ctc_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f969e2514bd7bfecbf147a73c1d2c27799a7bb5f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e6dc24391ef1a7c85edc1df1e473c9eaf7bb6c02" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout2d&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="616431983c70ac7b49c82cdf658d71df42a37921" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout3d&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6423f00d771b3620d1b3b711d04b3367cc6b7f97" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.elu(input, alpha=1.0, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#elu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.elu(input, alpha=1.0, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#elu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="066d8804893169dfbca3fe7dd9656b071610464f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#embedding&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#embedding&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5fe5f0a3587fe920a916a80da67623f5a29b0698" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False, per_sample_weights=None, include_last_offset=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#embedding_bag&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False, per_sample_weights=None, include_last_offset=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#embedding_bag&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8214fa8ec6f51b9ba17070449e398de1a1a749fb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#feature_alpha_dropout&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#feature_alpha_dropout&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="39356278d3eb3659a3cd4fb5a8875a52d82b1225" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#fold&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#fold&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89c06fb7800616c516855b4f04ceecdc17518e9f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.gelu(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#gelu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.gelu(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#gelu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4a218695613bb5a0de8369ed33bfc9afab383391" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.glu(input, dim=-1) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#glu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.glu(input, dim=-1) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#glu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7021625a38b746ce1a39ee32b502f2a6db9c4585" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#grid_sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#grid_sample&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a060aa4e1d34c9907ee63c0b90e99edc50dbbb3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#gumbel_softmax&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#gumbel_softmax&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5cda7558b3ad9b28cbdef77c8deaf73ae843265e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hardshrink(input, lambd=0.5) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardshrink&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hardshrink(input, lambd=0.5) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardshrink&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a7788357e1132c7c9f4fa67b9498a151b9b5344" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hardsigmoid(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardsigmoid&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hardsigmoid(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardsigmoid&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c3d47479a5aaf650bd1b5e35f8569bccb683bc13" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hardswish(input: torch.Tensor, inplace: bool = False) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardswish&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hardswish(input: torch.Tensor, inplace: bool = False) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardswish&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e678c28b805974934fa864eaa15336e3e3c6f449" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardtanh&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardtanh&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="58d4e43d719885efde48902367d9278eb45afaec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hinge_embedding_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hinge_embedding_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="50068fafd498873bbc84c37c079bdad47af2fd62" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#instance_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#instance_norm&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="51feb15b8eec41004d8f24c92329feec6a5d9a4b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#interpolate&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#interpolate&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4604fd22bca040ed4adef631c4303a0928807837" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#kl_div&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#kl_div&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2ce8753729c7c13600dbf6967ed8327871c4593f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#l1_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#l1_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="042a048fb27abd8cb71530f91c0f8ea35ccf4c3f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#layer_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#layer_norm&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee6d48c9626543647597f538ce12bde573f38c3b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#leaky_relu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#leaky_relu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a4cf8df1543c4662c88c825d378daccd8af5cac9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.linear(input, weight, bias=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#linear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.linear(input, weight, bias=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#linear&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1894e3d0ea0f07955092bee7408de704155bb002" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#local_response_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#local_response_norm&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="30f3ecb91a729dbee3572f54bfcb95d8e01f9c7f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#log_softmax&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#log_softmax&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="972472a778af86e2cc6ab6c107489a0f2bbf470b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#lp_pool1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#lp_pool1d&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0fdc0f3b7e0449603fc73493406027c87149dbf3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#lp_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#lp_pool2d&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0958859b9ae499a4857d4cd6aaec3f5f14bd2c86" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#margin_ranking_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#margin_ranking_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c2336c5e17dc37a4bcd303ad18064b289aec7a00" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool1d&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="909e9fe0f0dd81343d67d3049d4157cb713660c2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool2d&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8ff10e746b2a75339884b3a9e908991fdf3966f4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool3d&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="236f40331ff3004b076767f9b10c6574701143b0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#mse_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#mse_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8eeb3878caecd3953dc4e0e2c213444e69c755e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multi_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multi_margin_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f1ee4be3b183fc3aa98b897157f8528b20c0ab5e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multilabel_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multilabel_margin_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3a0ce36944943aeea79ad873bdc50b48d24a9813" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multilabel_soft_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multilabel_soft_margin_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cf1e6a52728af5a23fdafaff75cb840fbffcb4b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#nll_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#nll_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1b1d925500bf42746b1b5d13e110a776e5606671" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#normalize&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#normalize&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="93db9f8b73ef84aa64b0ef7c4aa681dfd9d1e05b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#pairwise_distance&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#pairwise_distance&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5170cc63198565dc974ddde740562cc7342b749a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#poisson_nll_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#poisson_nll_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d54df6a29bf6ce2dd719fcad0f54888591eb1141" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.prelu(input, weight) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#prelu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.prelu(input, weight) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#prelu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="414a24a106fc3b57b23f73bd9ef56f10b48b500d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.relu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#relu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.relu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#relu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="08c43c85b6fec232c61ab3d0fac43966e9dca202" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.relu6(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#relu6&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.relu6(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#relu6&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="363444915890f651770dba4e0ee139a8bd0e34ad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#rrelu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#rrelu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="988850f057511c9e65c7596f53b05e71f72224e3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.selu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#selu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.selu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#selu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6a67420270ae790a8aafa44865eaff88ba0d203f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.sigmoid(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#sigmoid&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.sigmoid(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#sigmoid&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d0ba03acac585036f1c75ac22544f646935e1e39" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.silu(input, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#silu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.silu(input, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#silu&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8976e3b61e7e02e09db919b7126ee6c1b55242b5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean', beta=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#smooth_l1_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean', beta=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#smooth_l1_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b11cb9c00fdb5d5d830ce06235b6b66872822b1a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#soft_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#soft_margin_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d7a1c434aa606493bf190a0bf88a2893392f5f51" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softmax&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softmax&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="185fc46c45a103fa23ebbf763c968235e69d4f91" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softmin&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softmin&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d76065e3268c5e424219969024a1ca1d8873a52e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.softsign(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softsign&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.softsign(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softsign&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="850af8d2773f3c55122e00b4af5eca41549cfe3a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.tanh(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#tanh&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.tanh(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#tanh&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c1d3cdf7abf7ca22a35e5af528ba4719fba901cb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.tanhshrink(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#tanhshrink&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.tanhshrink(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#tanhshrink&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="10b4e0d06c12b3eca2b5536d2f4dd8e12f1afe63" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#triplet_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#triplet_margin_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d0d87f333f2566c2b6cf4938778c9c85531c6c10" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, *, distance_function=None, margin=1.0, swap=False, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#triplet_margin_with_distance_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, *, distance_function=None, margin=1.0, swap=False, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#triplet_margin_with_distance_loss&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="16ab53e73c0ea07934feaf16eb7a2b3412c8c52d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#unfold&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#unfold&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e53c770d65bb0ba2d44ae8c0c45269c018a3cfbe" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="35c396cdfcf1f7742a14e8fe14ea4839369708d7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample_bilinear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample_bilinear&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="46210cb9fc54c5c8d19c399913fa6564ee472fa6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample_nearest&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample_nearest&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6f6616285fb50e039b0a50f40c8cc182e30b9ad3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.calculate_gain(nonlinearity, param=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#calculate_gain&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.calculate_gain(nonlinearity, param=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#calculate_gain&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d5e152873cf5a634ac4fc22712597672cedec772" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.constant_(tensor, val)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#constant_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.constant_(tensor, val)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#constant_&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3e07661fc90ed5f8c330b4c199c178b913c9f1c7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.dirac_(tensor, groups=1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#dirac_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.dirac_(tensor, groups=1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#dirac_&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f79be36611abc8d4a155dd9204a72975ac5e4773" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.eye_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#eye_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.eye_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#eye_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c2bf176e81d0d3dc053d580d4a2bca53d53776a5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#kaiming_normal_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#kaiming_normal_&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="829f255be8c47a0cc32b32e3e7a39b50f03bbb28" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#kaiming_uniform_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#kaiming_uniform_&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d74bb441e80c3db03c439b39588a3d7289372829" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.normal_(tensor, mean=0.0, std=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#normal_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.normal_(tensor, mean=0.0, std=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#normal_&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cda901e36d8a0dea4c9edd368b65c875ba5b5524" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.ones_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#ones_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.ones_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#ones_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="76cd921a26c8bc82c432454c6208f7007838adc0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.orthogonal_(tensor, gain=1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#orthogonal_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.orthogonal_(tensor, gain=1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#orthogonal_&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8d09409fb3770c88ebcd026eab2c58df33a7730e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.sparse_(tensor, sparsity, std=0.01)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#sparse_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.sparse_(tensor, sparsity, std=0.01)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#sparse_&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e6ef0d5afce7bb2d1cbfe668611dc89920c0ae59" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.uniform_(tensor, a=0.0, b=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#uniform_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.uniform_(tensor, a=0.0, b=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#uniform_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="85278dfd6aeb0f31979ad65f57d07305852e15d6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.xavier_normal_(tensor, gain=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#xavier_normal_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.xavier_normal_(tensor, gain=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#xavier_normal_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1b21f1cd018490607178af5dd8bf83497f69a9ed" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.xavier_uniform_(tensor, gain=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#xavier_uniform_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.xavier_uniform_(tensor, gain=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#xavier_uniform_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a46d62f5de757e1314a9fdb13f301b29dd3064bb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.zeros_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#zeros_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.zeros_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#zeros_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cabbf4b7897497900a45a7b2817218dc42a094a4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/data_parallel.html#data_parallel&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/data_parallel.html#data_parallel&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1820a94045081d62850e6015322b8f1bcd529902" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.adaptive_avg_pool2d(input: torch.Tensor, output_size: None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#adaptive_avg_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.adaptive_avg_pool2d(input: torch.Tensor, output_size: None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#adaptive_avg_pool2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e49a56be44c1c84f8d24fa8b0f0a773df40282a1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#avg_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#avg_pool2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fc695b6cf5e70404648a96381625d3b4242dfbc0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b4fafcb23bac88610267cfd4c65156bfb6814790" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a1f61a8e9a1e417e172cf9ee69a3ac703666b76c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cd0daa95478860b380dc09345e82221940cc1573" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.hardswish(input: torch.Tensor, scale: float, zero_point: int) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#hardswish&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.hardswish(input: torch.Tensor, scale: float, zero_point: int) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#hardswish&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3111ef1fc4e980e9ab5ca9059aba510612419184" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#interpolate&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#interpolate&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4a74909e8cec504214450b346ecf69f391517d12" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.linear(input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, scale: Optional[float] = None, zero_point: Optional[int] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#linear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.linear(input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, scale: Optional[float] = None, zero_point: Optional[int] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#linear&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="73c3f5a9236c23bb8967520b8b62180ad2c998ea" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#max_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#max_pool2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c862b119f84e81983c7b7be77dd88a11b46ed28" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.relu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#relu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.relu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#relu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b8d860cd9cbb88773d5c3ccd609b1b067ad8e1ec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="580dc9c2b76d8a1672544e8ee223a9a9de122d26" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample_bilinear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample_bilinear&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3a1660fcc28bce5d877097c2d858685ee9ac889c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample_nearest&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample_nearest&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee48c314ebbb974007123f98f539ecffcf9858aa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.clip_grad_norm_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], max_norm: float, norm_type: float = 2.0) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.clip_grad_norm_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], max_norm: float, norm_type: float = 2.0) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3c6b682d79cb2d7285847c9284685f7cc01c3e7c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.clip_grad_value_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], clip_value: float) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/clip_grad.html#clip_grad_value_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.clip_grad_value_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], clip_value: float) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/clip_grad.html#clip_grad_value_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6d2fddcb8f6b20b88995ff4d154426897cebd261" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.parameters_to_vector(parameters: Iterable[torch.Tensor]) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/convert_parameters.html#parameters_to_vector&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.parameters_to_vector(parameters: Iterable[torch.Tensor]) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/convert_parameters.html#parameters_to_vector&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="19a161f43edebdc3fe1ed0ffe30c004ecf54c828" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.custom_from_mask(module, name, mask)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#custom_from_mask&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.custom_from_mask(module, name, mask)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#custom_from_mask&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="db191bbfd3345206e1e35b684c02fd017ba13f9f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.global_unstructured(parameters, pruning_method, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#global_unstructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.global_unstructured(parameters, pruning_method, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#global_unstructured&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bc9622697e3c53544bdd5add28b3a2c57c2527ff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.is_pruned(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#is_pruned&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.is_pruned(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#is_pruned&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="96abbe2d8d461fb8bd2ee1faf7eb77f02db4de18" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.l1_unstructured(module, name, amount)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#l1_unstructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.l1_unstructured(module, name, amount)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#l1_unstructured&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b7d651eb7a59a2dc63e2fc226de440a5894c6c35" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.ln_structured(module, name, amount, n, dim)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#ln_structured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.ln_structured(module, name, amount, n, dim)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#ln_structured&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bd20f8272b0cdf40603ce7846f3ecd5e93a2297d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.random_structured(module, name, amount, dim)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#random_structured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.random_structured(module, name, amount, dim)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#random_structured&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="71942a64f4dce21658109edbb7ad07b1ac54058d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.random_unstructured(module, name, amount)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#random_unstructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.random_unstructured(module, name, amount)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#random_unstructured&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b1e7eeb03ba5f784fbc103d766bcd491a80e68b6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.remove(module, name)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#remove&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.remove(module, name)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#remove&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="593e2b8edbe9f814474088519b5d51467a8371e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.remove_spectral_norm(module: T_module, name: str = 'weight') &amp;rarr; T_module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/spectral_norm.html#remove_spectral_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.remove_spectral_norm(module: T_module, name: str = 'weight') &amp;rarr; T_module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/spectral_norm.html#remove_spectral_norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="28b96908ea924c1b164eb56c90b4f06cbbbb7ac7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.remove_weight_norm(module: T_module, name: str = 'weight') &amp;rarr; T_module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/weight_norm.html#remove_weight_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.remove_weight_norm(module: T_module, name: str = 'weight') &amp;rarr; T_module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/weight_norm.html#remove_weight_norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f70a4d724e38fb65885f35aa43b8a140dbffcef2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pack_padded_sequence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pack_padded_sequence&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2e6fdd6a0dda25c6ab4d31cc62e42257e961a19a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pack_sequence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pack_sequence&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="044c8f76ea2d98a2cab6ae17546088498a4b752d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pad_packed_sequence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pad_packed_sequence&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a814f84114cb8c05fa4c22be341926d765e90456" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pad_sequence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pad_sequence&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="881e9c21d63a899692b7a30837ad6eba7a8c7f16" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.spectral_norm(module: T_module, name: str = 'weight', n_power_iterations: int = 1, eps: float = 1e-12, dim: Optional[int] = None) &amp;rarr; T_module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/spectral_norm.html#spectral_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.spectral_norm(module: T_module, name: str = 'weight', n_power_iterations: int = 1, eps: float = 1e-12, dim: Optional[int] = None) &amp;rarr; T_module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/spectral_norm.html#spectral_norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="26317539c048e00421573ffe352a58a215f08fe6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.vector_to_parameters(vec: torch.Tensor, parameters: Iterable[torch.Tensor]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/convert_parameters.html#vector_to_parameters&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.vector_to_parameters(vec: torch.Tensor, parameters: Iterable[torch.Tensor]) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/convert_parameters.html#vector_to_parameters&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dcc88b942534bdb73dedece8f4a1cbf8e3e89bb5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.weight_norm(module: T_module, name: str = 'weight', dim: int = 0) &amp;rarr; T_module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/weight_norm.html#weight_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.weight_norm(module: T_module, name: str = 'weight', dim: int = 0) &amp;rarr; T_module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/weight_norm.html#weight_norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2501cdd2c8ea16585c06298503d128b6a0ffd1cd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="acdcacd99d319a55aa1c12b5685ea208060bb46d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.export(model, args, f, export_params=True, verbose=False, training=TrainingMode.EVAL, input_names=None, output_names=None, aten=False, export_raw_ir=False, operator_export_type=None, opset_version=None, _retain_param_name=True, do_constant_folding=True, example_outputs=None, strip_doc_string=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#export&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.export(model, args, f, export_params=True, verbose=False, training=TrainingMode.EVAL, input_names=None, output_names=None, aten=False, export_raw_ir=False, operator_export_type=None, opset_version=None, _retain_param_name=True, do_constant_folding=True, example_outputs=None, strip_doc_string=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#export&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="03322601d6585b4a92619debfccba83a650e2502" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.export_to_pretty_string(*args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#export_to_pretty_string&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.export_to_pretty_string(*args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#export_to_pretty_string&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e25b4aec2e26a0048d40d93bf92b951ca79caac4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.is_in_onnx_export()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#is_in_onnx_export&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.is_in_onnx_export()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#is_in_onnx_export&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5a40567375223347d13ca6f163cb9d650229468a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.operators.shape_as_tensor(x)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx/operators.html#shape_as_tensor&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.operators.shape_as_tensor(x)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx/operators.html#shape_as_tensor&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="976d3f529500cdddc27096243ac8d2a42aa02e7a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#register_custom_op_symbolic&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#register_custom_op_symbolic&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="815d0399a34390b7363b99b8598c5d34a64d4a72" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.select_model_mode_for_export(model, mode)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#select_model_mode_for_export&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.select_model_mode_for_export(model, mode)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#select_model_mode_for_export&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="59d28dfd09f75864c49c9ad2a7227c7d7578174a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.pca_lowrank(A, q=None, center=True, niter=2)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lowrank.html#pca_lowrank&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.pca_lowrank(A, q=None, center=True, niter=2)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lowrank.html#pca_lowrank&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="03de177ec561a49fdd476ac650a4389100c73080" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.preserve_format&lt;/code&gt;: Used in functions like &lt;code&gt;clone&lt;/code&gt; to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow &lt;code&gt;torch.contiguous_format&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.preserve_format&lt;/code&gt; : Used in functions like &lt;code&gt;clone&lt;/code&gt; to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow &lt;code&gt;torch.contiguous_format&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="bb81152ea7d6334b6a16b9c3108f06ba82952254" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.save(obj, f: Union[str, os.PathLike, BinaryIO], pickle_module=&amp;lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&amp;gt;, pickle_protocol=2, _use_new_zipfile_serialization=True) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/serialization.html#save&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.save(obj, f: Union[str, os.PathLike, BinaryIO], pickle_module=&amp;lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&amp;gt;, pickle_protocol=2, _use_new_zipfile_serialization=True) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/serialization.html#save&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="149a91842a887af3527d7028eea82d5c2bd4684c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.scatter_add_()&lt;/code&gt; when called on a CUDA tensor</source>
          <target state="translated">&lt;code&gt;torch.scatter_add_()&lt;/code&gt; when called on a CUDA tensor</target>
        </trans-unit>
        <trans-unit id="7829623f67d02ddc28ca91d4d4b46f343d79b572" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.seed() &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.seed() &amp;rarr; int&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#seed&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="145f4576eacb498e971a1e27b7989cde7e848da6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_default_dtype(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_default_dtype&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_default_dtype(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_default_dtype&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="10d4c72bdc704e59d50cf2910ff7471a3ab49eca" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_default_tensor_type(t)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_default_tensor_type&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_default_tensor_type(t)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_default_tensor_type&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="65c48c613ed4b2f78ed6e02a56b179139c405127" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_deterministic(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_deterministic&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_deterministic(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_deterministic&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1a9084271510ec6bbc33b1814e39096e72ad5953" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_tensor_str.html#set_printoptions&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_tensor_str.html#set_printoptions&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="13ffc0d262be2520fdff6d4a0f52ba63324ddfab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_rng_state(new_state) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#set_rng_state&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_rng_state(new_state) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#set_rng_state&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ed79abf5eac107985737d970652704fa05dc0ade" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.solve(B, A)&lt;/code&gt; can take in 2D inputs &lt;code&gt;B, A&lt;/code&gt; or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs &lt;code&gt;solution, LU&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;torch.solve(B, A)&lt;/code&gt; can take in 2D inputs &lt;code&gt;B, A&lt;/code&gt; or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs &lt;code&gt;solution, LU&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b665df2b2d96bef5eb69cda0cd781918b869fa3b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.sparse.addmm(mat: torch.Tensor, mat1: torch.Tensor, mat2: torch.Tensor, beta: float = 1.0, alpha: float = 1.0) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#addmm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.sparse.addmm(mat: torch.Tensor, mat1: torch.Tensor, mat2: torch.Tensor, beta: float = 1.0, alpha: float = 1.0) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#addmm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="aa2dfd43ccd4a1a35fc8de1d1304f193c8e2a864" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.sparse.mm(mat1: torch.Tensor, mat2: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#mm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.sparse.mm(mat1: torch.Tensor, mat2: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#mm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7727e69ca5233c84e597b026bcaedcd72dec04d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.sparse.sum(input: torch.Tensor, dim: Optional[Tuple[int]] = None, dtype: Optional[int] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#sum&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.sparse.sum(input: torch.Tensor, dim: Optional[Tuple[int]] = None, dtype: Optional[int] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#sum&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6e65754e6516685feae5bc613180e4719d49f3f8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.split(tensor, split_size_or_sections, dim=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#split&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.split(tensor, split_size_or_sections, dim=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#split&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e35490d70ccd40103394665a6c8f64a5fda9d8c1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.stft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#stft&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.stft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#stft&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3a3816ac7e969e46494429240e45f840ebeb330a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.strided&lt;/code&gt; represents dense Tensors and is the memory layout that is most commonly used. Each strided tensor has an associated &lt;code&gt;torch.Storage&lt;/code&gt;, which holds its data. These tensors provide multi-dimensional, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;strided&lt;/a&gt; view of a storage. Strides are a list of integers: the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor. This concept makes it possible to perform many tensor operations efficiently.</source>
          <target state="translated">&lt;code&gt;torch.strided&lt;/code&gt; represents dense Tensors and is the memory layout that is most commonly used. Each strided tensor has an associated &lt;code&gt;torch.Storage&lt;/code&gt; , which holds its data. These tensors provide multi-dimensional, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;strided&lt;/a&gt; view of a storage. Strides are a list of integers: the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor. This concept makes it possible to perform many tensor operations efficiently.</target>
        </trans-unit>
        <trans-unit id="562d195ae00895df7b44cc1509c8dc7828657990" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.svd_lowrank(A, q=6, niter=2, M=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lowrank.html#svd_lowrank&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.svd_lowrank(A, q=6, niter=2, M=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lowrank.html#svd_lowrank&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="95d8fe6a396e85b79844a3279acdb3eba5b10f87" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.tensordot(a, b, dims=2)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#tensordot&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.tensordot(a, b, dims=2)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#tensordot&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e6a2b543d94504add4daf1a3eaf97669036641b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.triangular_solve(b, A)&lt;/code&gt; can take in 2D inputs &lt;code&gt;b, A&lt;/code&gt; or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs &lt;code&gt;X&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.triangular_solve(b, A)&lt;/code&gt; can take in 2D inputs &lt;code&gt;b, A&lt;/code&gt; or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs &lt;code&gt;X&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4d3e0234b25f4c9163c86f9066ba6ade5028b7ab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.bottleneck&lt;/code&gt; is a tool that can be used as an initial step for debugging bottlenecks in your program. It summarizes runs of your script with the Python profiler and PyTorch&amp;rsquo;s autograd profiler.</source>
          <target state="translated">&lt;code&gt;torch.utils.bottleneck&lt;/code&gt; is a tool that can be used as an initial step for debugging bottlenecks in your program. It summarizes runs of your script with the Python profiler and PyTorch&amp;rsquo;s autograd profiler.</target>
        </trans-unit>
        <trans-unit id="9e9227f91a147c6bfddb1ea807ad178da61c2f01" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.checkpoint.checkpoint(function, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/checkpoint.html#checkpoint&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.checkpoint.checkpoint(function, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/checkpoint.html#checkpoint&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9f35c5fbe34347d80748ad19cf587e88d314a446" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.checkpoint.checkpoint_sequential(functions, segments, input, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/checkpoint.html#checkpoint_sequential&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.checkpoint.checkpoint_sequential(functions, segments, input, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/checkpoint.html#checkpoint_sequential&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="82743a1bc0d193a8d5c5ef819ec1fa2a75d3a567" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.BuildExtension(*args, **kwargs) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#BuildExtension&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.BuildExtension(*args, **kwargs) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#BuildExtension&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e1fcb6bd9919299ae2ddc98ca71e5ceaf224bcb1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CUDAExtension&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CUDAExtension&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bacc6afc48e6da0551f4f2b3cb8604b66cb184b0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CppExtension&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CppExtension&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fa82ce120a2fb2cab4f7a383ceb7f5257beb9cb5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler) &amp;rarr; bool&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#check_compiler_abi_compatibility&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler) &amp;rarr; bool&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#check_compiler_abi_compatibility&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="04175775569bb5f47df2787c65e8108be88d672c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.include_paths(cuda: bool = False) &amp;rarr; List[str]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#include_paths&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.include_paths(cuda: bool = False) &amp;rarr; List[str]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#include_paths&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2d2cc603ad32cc623c7afe161c11ea47e80c35fc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.is_ninja_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#is_ninja_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.is_ninja_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#is_ninja_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="127692b7ee4252253192d5f2c1ec1661e298d13c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.load(name, sources: List[str], extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda: Optional[bool] = None, is_python_module=True, keep_intermediates=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.load(name, sources: List[str], extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda: Optional[bool] = None, is_python_module=True, keep_intermediates=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bc490ba01580aa87b6738971cbd6c2f4a833c59c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, with_pytorch_error_handling=True, keep_intermediates=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load_inline&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, with_pytorch_error_handling=True, keep_intermediates=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load_inline&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="282b9c63f989793b3e3cc7e3e2e6b74a269f4472" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.verify_ninja_availability()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#verify_ninja_availability&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.verify_ninja_availability()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#verify_ninja_availability&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="87bb3cae7ea056553e89c9188b938e7264f5dd5d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.mobile_optimizer.optimize_for_mobile(script_module, optimization_blocklist: Set[torch._C.MobileOptimizerType] = None, preserved_methods: List[AnyStr] = None, backend: str = 'CPU')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/mobile_optimizer.html#optimize_for_mobile&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.mobile_optimizer.optimize_for_mobile(script_module, optimization_blocklist: Set[torch._C.MobileOptimizerType] = None, preserved_methods: List[AnyStr] = None, backend: str = 'CPU')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/mobile_optimizer.html#optimize_for_mobile&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0735be3015f610c6bc97e587de840634628e8077" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.where(condition)&lt;/code&gt; is identical to &lt;code&gt;torch.nonzero(condition, as_tuple=True)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;torch.where(condition)&lt;/code&gt; is identical to &lt;code&gt;torch.nonzero(condition, as_tuple=True)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="55e6ef83c7ef85f702c91d8eef5ea0dbd8c749b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.alexnet(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/alexnet.html#alexnet&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.alexnet(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/alexnet.html#alexnet&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0d38d82c4a2980d1cd54f65f24d369937605975c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.densenet121(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet121&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.densenet121(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet121&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5d48becd24b6a141911d63cbcfa4b5388d0e2f2a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.densenet161(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet161&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.densenet161(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet161&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c66a106d591302ae9a23564d21b9f0fecca36a45" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.densenet169(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet169&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.densenet169(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet169&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4a4dc470d2c9b1dd63a8b3c3a2f09a2b731ee1ff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.densenet201(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet201&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.densenet201(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet201&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d37101a2a2ce0b122b3186b37ea5f3eebbf7af88" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/faster_rcnn.html#fasterrcnn_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/faster_rcnn.html#fasterrcnn_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fddcba641d93f705c38ea5e99e37277ffb808f18" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=2, num_keypoints=17, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/keypoint_rcnn.html#keypointrcnn_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=2, num_keypoints=17, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/keypoint_rcnn.html#keypointrcnn_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ca21903101f18b0557dcbc8fea49917fe47b38fc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/mask_rcnn.html#maskrcnn_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/mask_rcnn.html#maskrcnn_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8321d00bdd9e79fbbae3bed723d0da2d8ff44e18" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/retinanet.html#retinanet_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/retinanet.html#retinanet_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e68c6f2f62746247926299a2e294d2b695793904" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.googlenet(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/googlenet.html#googlenet&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.googlenet(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/googlenet.html#googlenet&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c29cab4aa7a85507392da05a8545872aeefa41c6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.inception_v3(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/inception.html#inception_v3&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.inception_v3(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/inception.html#inception_v3&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f0c9417db1439ddf7338a83567afce51a263b5dd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mnasnet0_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_5&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mnasnet0_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_5&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d47807e0f8165cf93963dfc4dc78e77f742858e1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mnasnet0_75(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_75&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mnasnet0_75(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_75&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f5a9e5d5450b4dafb844ae224f4ad180dd226121" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mnasnet1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_0&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mnasnet1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_0&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="77b69bd7be274f38453186938001d277495a6c2f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mnasnet1_3(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_3&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mnasnet1_3(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_3&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="969edc85d9e553749ddbb016e31fdad62783c095" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mobilenet_v2(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mobilenet.html#mobilenet_v2&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mobilenet_v2(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mobilenet.html#mobilenet_v2&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8bffa4c41d6459f1920f6b09592d01a2d8bed629" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet101(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet101&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet101(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet101&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8f51b7405b05ff1069b3eed30d5d1a4b4a048135" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet152(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet152&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet152(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet152&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="58b4a120cb5b9ef8aa6791731d8b227114babaf4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet18(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet18&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet18(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet18&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6c548ae7c119869cb346706a64804cd9a2772320" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet34(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet34&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet34(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet34&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="70c0819d42401f678755dd1664c6da919188b89e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet50(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet50&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet50(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet50&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bcdda78061e32c5045ff0ce9dde8acb096118a2f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnext101_32x8d(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext101_32x8d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnext101_32x8d(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext101_32x8d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4cbcacd435d5999826e0ac993eac4f3fc585eeb4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnext50_32x4d(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext50_32x4d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnext50_32x4d(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext50_32x4d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c6417c808ce60d23a185910dca6539019af3ba56" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet101&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet101&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1d366590337e483500734a3650d1ef622765c7eb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.segmentation.deeplabv3_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet50&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.segmentation.deeplabv3_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet50&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="69c183fd9ad3fbdedaca14cf2875725c54150e77" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.segmentation.fcn_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet101&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.segmentation.fcn_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet101&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dadae370f707b7834deb997ef740ebf2926cec26" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.segmentation.fcn_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet50&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.segmentation.fcn_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet50&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="985d1d71995c5a858dab5d7f67674d973e8f084d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.shufflenet_v2_x0_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x0_5&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.shufflenet_v2_x0_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x0_5&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d06ebbf7a18ac9324d20801b61faff9eb323112e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.shufflenet_v2_x1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_0&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.shufflenet_v2_x1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_0&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="69605c4002be97c73eb40b6b5dac9238f0a6f129" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.shufflenet_v2_x1_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_5&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.shufflenet_v2_x1_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_5&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2bb85f32d69a2c716a4157554b61ab0a69d223d2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.shufflenet_v2_x2_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x2_0&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.shufflenet_v2_x2_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x2_0&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="01dc12333c85efbfdfa76203ff11e0218683f488" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.squeezenet1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_0&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.squeezenet1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_0&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9c04456665e568beadaf956c8033bee8db4ce676" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.squeezenet1_1(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_1&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.squeezenet1_1(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_1&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1cd5db2e4fbd76c79a9cd5f06e13284c03c17302" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg11(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg11(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="50ac0ac46bd916f5d37b20fddc28e9c32dc160d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg11_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11_bn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg11_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11_bn&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7519bb1b3c7dacdd2e7ab4174d31903bddfe4872" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg13(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg13(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ec4029edda490c50f9dfa4eaf54f78b869d1dedb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg13_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13_bn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg13_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13_bn&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d89c3bd8e0058098ec4b822cf6070e6491c1046f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg16(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg16(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b33e5eec705427e4d3f1ab25e07ed270b3ac99ea" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg16_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16_bn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg16_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16_bn&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="563b55e8c50e9495de20d93fa1609d3ca5095650" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg19(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg19(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e8d2010441ce8fd04d6bccd5dc17972367f6612e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg19_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19_bn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg19_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19_bn&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="248d2d3c5cde4f33b37185cfc2f168d45a6085e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.video.mc3_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#mc3_18&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.video.mc3_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#mc3_18&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="049d72606ad866c4a386e75f3387cf447c24e806" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.video.r2plus1d_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r2plus1d_18&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.video.r2plus1d_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r2plus1d_18&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1f5b003328cdb0ee7e5456320bb4679a846567da" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.video.r3d_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r3d_18&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.video.r3d_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r3d_18&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="976b5b89506d0a20776447fa2c6b030b688bd840" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.wide_resnet101_2(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet101_2&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.wide_resnet101_2(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet101_2&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="883785b05ab2fa730630f3ad88bcabd0f8a7b4f0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.wide_resnet50_2(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet50_2&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.wide_resnet50_2(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet50_2&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a147531339c74e3b87db4d26b7214df4792298dc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;total_length&lt;/code&gt; is useful to implement the &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; pattern in a &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; wrapped in &lt;a href=&quot;torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;this FAQ section&lt;/a&gt; for details.</source>
          <target state="translated">&lt;code&gt;total_length&lt;/code&gt; は、&lt;a href=&quot;torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; で&lt;/a&gt;ラップされた&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; パターンを実装するのに役立ちます。詳細については、&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;このFAQセクション&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="7fbb2269f1128397ff52b8755f83fcf30131e444" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;train(mode: bool = True) &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.train&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;train(mode: bool = True) &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.train&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="17490d06aec9bb591bde79be08f472ba510005d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;true&lt;/code&gt; if &lt;code&gt;key&lt;/code&gt; was deleted, otherwise &lt;code&gt;false&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;true&lt;/code&gt; &lt;code&gt;key&lt;/code&gt; が削除された場合はtrue、それ以外の場合は &lt;code&gt;false&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="73de5004ed63d179d11e7c607cd226ac72fef763" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;type(dst_type: Union[torch.dtype, str]) &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.type&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;type(dst_type: Union[torch.dtype, str]) &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.type&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b726d74ca27828505ae027190d9488e64e8a432c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unflatten(dim, sizes)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unflatten&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;unflatten(dim, sizes)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unflatten&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bb232cf62834f167804d6c86e5ae22859b7db331" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unflattened_size&lt;/code&gt; is the new shape of the unflattened dimension of the tensor and it can be a &lt;code&gt;tuple&lt;/code&gt; of ints or &lt;code&gt;torch.Size&lt;/code&gt; for &lt;code&gt;Tensor&lt;/code&gt; input or a &lt;code&gt;NamedShape&lt;/code&gt; (tuple of &lt;code&gt;(name, size)&lt;/code&gt; tuples) for &lt;code&gt;NamedTensor&lt;/code&gt; input.</source>
          <target state="translated">&lt;code&gt;unflattened_size&lt;/code&gt; はテンソルの平坦化されない寸法の新たな形状であり、それはあることができる &lt;code&gt;tuple&lt;/code&gt; int型または &lt;code&gt;torch.Size&lt;/code&gt; ため &lt;code&gt;Tensor&lt;/code&gt; 入力または &lt;code&gt;NamedShape&lt;/code&gt; （のタプル &lt;code&gt;(name, size)&lt;/code&gt; のためのタプル） &lt;code&gt;NamedTensor&lt;/code&gt; の入力。</target>
        </trans-unit>
        <trans-unit id="114ecd93b94acf20f3f824c124da10585acafbf1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unify(A, B)&lt;/code&gt; determines which of the names &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; to propagate to the outputs. It returns the more &lt;em&gt;specific&lt;/em&gt; of the two names, if they match. If the names do not match, then it errors.</source>
          <target state="translated">&lt;code&gt;unify(A, B)&lt;/code&gt; は、名前 &lt;code&gt;A&lt;/code&gt; と &lt;code&gt;B&lt;/code&gt; のどちらを出力に伝播するかを決定します。2つの名前が一致する場合は、より&lt;em&gt;具体的な&lt;/em&gt;名前を返します。名前が一致しない場合は、エラーが発生します。</target>
        </trans-unit>
        <trans-unit id="f81a8067f75099cdee88dc8df395ee2a70a06955" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unique(sorted=True, return_inverse=False, return_counts=False, dim=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unique&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;unique(sorted=True, return_inverse=False, return_counts=False, dim=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unique&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2f19f4fd7da0a567778731041cc786315cffe617" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unique_consecutive(return_inverse=False, return_counts=False, dim=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unique_consecutive&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;unique_consecutive(return_inverse=False, return_counts=False, dim=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unique_consecutive&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8da2e713f043aee7915ced857730729c9fc23a12" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;update(modules: Mapping[str, torch.nn.modules.module.Module]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.update&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;update(modules: Mapping[str, torch.nn.modules.module.Module]) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.update&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4d3c990fce28e84265072d86fa81e98efcd00c95" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;update(parameters: Mapping[str, Parameter]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.update&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;update(parameters: Mapping[str, Parameter]) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.update&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d4d99c2f207829a30f2ddddb647b4a8b864fd17f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;use_external_data_format&lt;/code&gt; argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument &amp;lsquo;f&amp;rsquo; must be a string specifying the location of the model.</source>
          <target state="translated">&lt;code&gt;use_external_data_format&lt;/code&gt; export APIのuse_external_data_format引数を使用すると、モデルをONNX外部データ形式でエクスポートできます。このオプションを有効にすると、エクスポータはいくつかのモデルパラメータをONNXファイル自体ではなく外部バイナリファイルに保存します。これらの外部バイナリファイルは、ONNXファイルと同じ場所に保存されます。引数 'f'は、モデルの場所を指定する文字列である必要があります。</target>
        </trans-unit>
        <trans-unit id="9356c0eda1c275025f5aa20e4ff2a7158f04f8d7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;use_ninja&lt;/code&gt; (bool): If &lt;code&gt;use_ninja&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard &lt;code&gt;setuptools.build_ext&lt;/code&gt;. Fallbacks to the standard distutils backend if Ninja is not available.</source>
          <target state="translated">&lt;code&gt;use_ninja&lt;/code&gt; （bool）： &lt;code&gt;use_ninja&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; （デフォルト）の場合、Ninjaバックエンドを使用してビルドを試みます。Ninjaは、標準の &lt;code&gt;setuptools.build_ext&lt;/code&gt; と比較してコンパイルを大幅に高速化します。Ninjaが利用できない場合、標準のdistutilsバックエンドへのフォールバック。</target>
        </trans-unit>
        <trans-unit id="1254a0cf82203e9a1531fc62d7d0d216e9e743f7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;values() &amp;rarr; Iterable[Parameter]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.values&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;values() &amp;rarr; Iterable[Parameter]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.values&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="96bfd43ff7e24163f6d4c88a8036d233913ce8fe" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;values() &amp;rarr; Iterable[torch.nn.modules.module.Module]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.values&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;values() &amp;rarr; Iterable[torch.nn.modules.module.Module]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.values&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="104e14056b814916b6776efe34cc558abcb82dbd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;wait() &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.wait&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;wait() &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.wait&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="516fb2fe6efc6e35e85e90b516645bccab59738b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;wait()&lt;/code&gt; - will block the process until the operation is finished.</source>
          <target state="translated">&lt;code&gt;wait()&lt;/code&gt; -操作が終了するまでプロセスをブロックします。</target>
        </trans-unit>
        <trans-unit id="941139d436ab4f418d83f1d8c7ec9b9119cbf52b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;wait()&lt;/code&gt; - will block the process until the operation is finished. &lt;code&gt;is_completed()&lt;/code&gt; is guaranteed to return True once it returns.</source>
          <target state="translated">&lt;code&gt;wait()&lt;/code&gt; -操作が終了するまでプロセスをブロックします。 &lt;code&gt;is_completed()&lt;/code&gt; は、戻るとTrueを返すことが保証されています。</target>
        </trans-unit>
        <trans-unit id="75193eb58fde7a23d585e351a7da7779cc2d15b2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;weight&lt;/code&gt; (Tensor): the learnable weights of the module of shape &lt;code&gt;(num_embeddings, embedding_dim)&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;weight&lt;/code&gt; （テンソル）：形状のモジュールの学習可能な重み &lt;code&gt;(num_embeddings, embedding_dim)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a60b5944e5c5872c821c93c5da8c56e577b4b1c8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;window&lt;/code&gt; can be a 1-D tensor of size &lt;code&gt;win_length&lt;/code&gt;, e.g., from &lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt;&lt;code&gt;torch.hann_window()&lt;/code&gt;&lt;/a&gt;. If &lt;code&gt;window&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; (default), it is treated as if having</source>
          <target state="translated">&lt;code&gt;window&lt;/code&gt; サイズの1次元テンソルであることができる &lt;code&gt;win_length&lt;/code&gt; から、例えば、&lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt; &lt;code&gt;torch.hann_window()&lt;/code&gt; &lt;/a&gt;。場合は &lt;code&gt;window&lt;/code&gt; ある &lt;code&gt;None&lt;/code&gt; （デフォルト）、それを持つかのように扱われます</target>
        </trans-unit>
        <trans-unit id="fcb35e51fe3891a319d8cca2cc7d08a9312f5fd0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;zero_grad(set_to_none: bool = False) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.zero_grad&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;zero_grad(set_to_none: bool = False) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.zero_grad&quot;&gt;[ソース]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9e228d56108279e5de96783fa1fcf187bda5858b" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;(string, Module)&lt;/em&gt; &amp;ndash; Tuple containing a name and child module</source>
          <target state="translated">&lt;em&gt;（文字列、モジュール）&lt;/em&gt; &amp;ndash;名前と子モジュールを含むタプル</target>
        </trans-unit>
        <trans-unit id="5fb3c9a95d7120f0c30ae272a854d5feba5d48b6" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;(string, Module)&lt;/em&gt; &amp;ndash; Tuple of name and module</source>
          <target state="translated">&lt;em&gt;（文字列、モジュール）&lt;/em&gt; &amp;ndash;名前とモジュールのタプル</target>
        </trans-unit>
        <trans-unit id="319263d4f6bb9e6d32586c5cff620cbc3fa67237" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;(string, Parameter)&lt;/em&gt; &amp;ndash; Tuple containing the name and parameter</source>
          <target state="translated">&lt;em&gt;（文字列、パラメーター）&lt;/em&gt; &amp;ndash;名前とパラメーターを含むタプル</target>
        </trans-unit>
        <trans-unit id="dc72ec06d2fb8eafc561c043b35fadba57166206" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;(string, torch.Tensor)&lt;/em&gt; &amp;ndash; Tuple containing the name and buffer</source>
          <target state="translated">&lt;em&gt;（string、torch.Tensor）&lt;/em&gt; &amp;ndash;名前とバッファーを含むタプル</target>
        </trans-unit>
        <trans-unit id="a754dfd4e3d629d6e648310d555fb7a646535624" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Beta:&lt;/em&gt; Features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.</source>
          <target state="translated">&lt;em&gt;ベータ版：&lt;/em&gt;ユーザーのフィードバックに基づいてAPIが変更される可能性があるため、パフォーマンスを改善する必要があるため、またはオペレーター全体のカバレッジがまだ完了していないため、機能は&lt;em&gt;ベータ版&lt;/em&gt;としてタグ付けされます。ベータ機能については、安定した分類まで機能を確認することをお約束します。ただし、下位互換性については確約していません。</target>
        </trans-unit>
        <trans-unit id="0a298939dfd8866323bf58103d52da9a00fff3eb" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Module&lt;/em&gt; &amp;ndash; a child module</source>
          <target state="translated">&lt;em&gt;モジュール&lt;/em&gt;&amp;ndash;子モジュール</target>
        </trans-unit>
        <trans-unit id="55c1945ebbb505a29f113e4158c303559a846bd3" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Module&lt;/em&gt; &amp;ndash; a module in the network</source>
          <target state="translated">&lt;em&gt;モジュール&lt;/em&gt;&amp;ndash;ネットワーク内のモジュール</target>
        </trans-unit>
        <trans-unit id="7efeeaf6ac232e4e1bd4481ee3ccf55f14e08c1b" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Parameter&lt;/em&gt; &amp;ndash; module parameter</source>
          <target state="translated">&lt;em&gt;パラメータ&lt;/em&gt;&amp;ndash;モジュールパラメータ</target>
        </trans-unit>
        <trans-unit id="b1299c276055de8aa531b38f8e62b765ac94d8a5" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Prototype:&lt;/em&gt; These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</source>
          <target state="translated">&lt;em&gt;プロトタイプ：&lt;/em&gt;これらの機能は、実行時フラグの背後にある場合を除いて、通常、PyPIやCondaなどのバイナリディストリビューションの一部としては利用できず、フィードバックとテストの初期段階にあります。</target>
        </trans-unit>
        <trans-unit id="d4f12aa72178917bae1166015f8ad3889662fd74" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Stable:&lt;/em&gt; These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).</source>
          <target state="translated">&lt;em&gt;安定性：&lt;/em&gt;これらの機能は長期間維持され、通常、パフォーマンスの大きな制限やドキュメントのギャップはありません。また、下位互換性を維持することも期待しています（ただし、重大な変更が発生する可能性があり、1つのリリースが事前に通知されます）。</target>
        </trans-unit>
        <trans-unit id="058a377bedcdaeade66857847812a0e554998ab7" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;returned index satisfies&lt;/em&gt;</source>
          <target state="translated">&lt;em&gt;返されたインデックスは&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="2310781c0d4ea448d2442fafbd9795c680825b97" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;script-based&lt;/em&gt; means that the model you are trying to export is a &lt;a href=&quot;jit&quot;&gt;ScriptModule&lt;/a&gt;. &lt;code&gt;ScriptModule&lt;/code&gt; is the core data structure in &lt;code&gt;TorchScript&lt;/code&gt;, and &lt;code&gt;TorchScript&lt;/code&gt; is a subset of Python language, that creates serializable and optimizable models from PyTorch code.</source>
          <target state="translated">&lt;em&gt;スクリプトベース&lt;/em&gt;とは、エクスポートしようとしているモデルが&lt;a href=&quot;jit&quot;&gt;ScriptModuleである&lt;/a&gt;こと&lt;em&gt;を&lt;/em&gt;意味します。 &lt;code&gt;ScriptModule&lt;/code&gt; はTorchScriptのコアデータ構造で &lt;code&gt;TorchScript&lt;/code&gt; 、 &lt;code&gt;TorchScript&lt;/code&gt; はPython言語のサブセットであり、PyTorchコードからシリアル化および最適化可能なモデルを作成します。</target>
        </trans-unit>
        <trans-unit id="f6626f7675f0e61b3b27dbc625972f11d257da92" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;torch.Tensor&lt;/em&gt; &amp;ndash; module buffer</source>
          <target state="translated">&lt;em&gt;torch.Tensor&lt;/em&gt; &amp;ndash;モジュールバッファー</target>
        </trans-unit>
        <trans-unit id="2908cf8505ca66bca3222f57f081464f869918f4" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;trace-based&lt;/em&gt; means that it operates by executing your model once, and exporting the operators which were actually run during this run. This means that if your model is dynamic, e.g., changes behavior depending on input data, the export won&amp;rsquo;t be accurate. Similarly, a trace is likely to be valid only for a specific input size (which is one reason why we require explicit inputs on tracing.) We recommend examining the model trace and making sure the traced operators look reasonable. If your model contains control flows like for loops and if conditions, &lt;em&gt;trace-based&lt;/em&gt; exporter will unroll the loops and if conditions, exporting a static graph that is exactly the same as this run. If you want to export your model with dynamic control flows, you will need to use the &lt;em&gt;script-based&lt;/em&gt; exporter.</source>
          <target state="translated">&lt;em&gt;トレースベースと&lt;/em&gt;は、モデルを1回実行し、この実行中に実際に実行された演算子をエクスポートすることによって動作すること&lt;em&gt;を&lt;/em&gt;意味します。これは、モデルが動的である場合、たとえば入力データに応じて動作が変化する場合、エクスポートは正確ではないことを意味します。同様に、トレースは特定の入力サイズに対してのみ有効である可能性があります（これが、トレースで明示的な入力が必要な理由の1つです）。モデルトレースを調べて、トレースされた演算子が適切に見えることを確認することをお勧めします。モデルにforループやif条件などの制御フローが含まれている場合、&lt;em&gt;トレースベースの&lt;/em&gt;エクスポーターはループを展開し、if条件では、この実行とまったく同じ静的グラフをエクスポートします。動的制御フローを使用してモデルをエクスポートする場合は、&lt;em&gt;スクリプトベース&lt;/em&gt;を使用する必要があります&lt;em&gt;&lt;/em&gt; 輸出業者。</target>
        </trans-unit>
        <trans-unit id="f3e32b5c7adc9869a8580e1b9e6a76857d105f7c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;**kwargs&lt;/strong&gt; &amp;ndash; For compatibility, may contain the key &lt;code&gt;async&lt;/code&gt; in place of the &lt;code&gt;non_blocking&lt;/code&gt; argument.</source>
          <target state="translated">&lt;strong&gt;** kwargs&lt;/strong&gt; &amp;ndash;互換性のために、 &lt;code&gt;non_blocking&lt;/code&gt; 引数の代わりにキー &lt;code&gt;async&lt;/code&gt; を含めることができます。</target>
        </trans-unit>
        <trans-unit id="65f37625b43f8177a68af42c34ddcf34a3a157c3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;**kwargs&lt;/strong&gt; &amp;ndash; For compatibility, may contain the key &lt;code&gt;async&lt;/code&gt; in place of the &lt;code&gt;non_blocking&lt;/code&gt; argument. The &lt;code&gt;async&lt;/code&gt; arg is deprecated.</source>
          <target state="translated">&lt;strong&gt;** kwargs&lt;/strong&gt; &amp;ndash;互換性のために、 &lt;code&gt;non_blocking&lt;/code&gt; 引数の代わりにキー &lt;code&gt;async&lt;/code&gt; を含めることができます。 &lt;code&gt;async&lt;/code&gt; argは廃止されました。</target>
        </trans-unit>
        <trans-unit id="72bb91ae39a4577d326534e0a366239f4d2f6032" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;**kwargs&lt;/strong&gt; (&lt;em&gt;*args&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;) &amp;ndash; arguments to invoke &lt;code&gt;func&lt;/code&gt; with.</source>
          <target state="translated">&lt;strong&gt;** kwargs&lt;/strong&gt;（&lt;em&gt;* args &lt;/em&gt;&lt;em&gt;、&lt;/em&gt;）&amp;ndash; &lt;code&gt;func&lt;/code&gt; を呼び出すための引数。</target>
        </trans-unit>
        <trans-unit id="b5847a4bbfe6774c5d419725faf44bb56045bd86" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;**kwargs&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the corresponding kwargs for callable &lt;code&gt;model&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;** kwargs&lt;/strong&gt;（&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;呼び出し可能な &lt;code&gt;model&lt;/code&gt; 対応するkwargs 。</target>
        </trans-unit>
        <trans-unit id="06aa7e1e95a236279512459a9af7e6f136714af2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*args&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the corresponding args for callable &lt;code&gt;model&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;* args&lt;/strong&gt;（&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;呼び出し可能な &lt;code&gt;model&lt;/code&gt; 対応する引数。</target>
        </trans-unit>
        <trans-unit id="4786a01430f5e00df37e0a3c12e05a6d1bccb369" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*dims&lt;/strong&gt; (&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; The desired ordering of dimensions</source>
          <target state="translated">&lt;strong&gt;* dims&lt;/strong&gt;（&lt;em&gt;int..。&lt;/em&gt;）&amp;ndash;希望する次元の順序</target>
        </trans-unit>
        <trans-unit id="3319d9e4baa0f129856cf05f1bd63cb7e15f6a69" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*sizes&lt;/strong&gt; (&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; the desired expanded size</source>
          <target state="translated">&lt;strong&gt;* size&lt;/strong&gt;（&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt;または&lt;/em&gt;&lt;em&gt;int..。&lt;/em&gt;）&amp;ndash;必要な拡張サイズ</target>
        </trans-unit>
        <trans-unit id="d88e4c4171034ef9674db627d11a0997babe55fc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*tensors&lt;/strong&gt; &amp;ndash; One or more tensors with 0, 1, or 2 dimensions.</source>
          <target state="translated">&lt;strong&gt;*テンソル&lt;/strong&gt;&amp;ndash; 0、1、または2次元の1つ以上のテンソル。</target>
        </trans-unit>
        <trans-unit id="fdd4b81b9fbfd6112e5abe2aed8111a733309e32" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*tensors&lt;/strong&gt; &amp;ndash; any number of 1 dimensional tensors.</source>
          <target state="translated">&lt;strong&gt;*テンソル&lt;/strong&gt;&amp;ndash;任意の数の1次元テンソル。</target>
        </trans-unit>
        <trans-unit id="37d44625a57d67ad98c1d08c2860e67b1f2734af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*tensors&lt;/strong&gt; &amp;ndash; any number of tensors of the same type</source>
          <target state="translated">&lt;strong&gt;*テンソル&lt;/strong&gt;&amp;ndash;同じタイプの任意の数のテンソル</target>
        </trans-unit>
        <trans-unit id="542ee1fa5dcd61ea46e6f051760a45f8429629ae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input square matrix of size</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;サイズの入力正方行列</target>
        </trans-unit>
        <trans-unit id="66a7223a6944ee5be2aaeedb91548f693b323337" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="2ff3d5c63e7af23218d3654b0732721e878d975c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of size</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;サイズの入力テンソル</target>
        </trans-unit>
        <trans-unit id="beaf7caea311d31a62bf35cffe9c4e562b9e1679" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input triangular coefficient matrix of size</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;サイズの入力三角係数行列</target>
        </trans-unit>
        <trans-unit id="a1fd8981c104831012bef0ab84387ae714973280" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to factor of size</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;サイズを因数分解するテンソル</target>
        </trans-unit>
        <trans-unit id="11cfe4616774835a1dba4fea5c0a02f4903b11f5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;B&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the input tensor of size</source>
          <target state="translated">&lt;strong&gt;B&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;サイズの入力テンソル</target>
        </trans-unit>
        <trans-unit id="1cbcbf4a0ee70b68dac705e32d72b1cc1467e9a2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Check names&lt;/strong&gt;: an operator may perform automatic checks at runtime that check that certain dimension names must match.</source>
          <target state="translated">&lt;strong&gt;名前の確認&lt;/strong&gt;：オペレーターは、実行時に特定のディメンション名が一致する必要があることを確認する自動チェックを実行できます。</target>
        </trans-unit>
        <trans-unit id="e00a3d6d56d0570d3c90d64bf2e85703c34bcf75" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Check names&lt;/strong&gt;: check that the names of the two tensors &lt;em&gt;match&lt;/em&gt;.</source>
          <target state="translated">&lt;strong&gt;名前の確認&lt;/strong&gt;：2つのテンソルの名前が&lt;em&gt;一致&lt;/em&gt;することを&lt;strong&gt;確認します&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="eaa4895ce797f363fa068fb68e3589cf17ee5901" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Conv packed params hoisting&lt;/strong&gt; (blacklisting option &lt;code&gt;MobileOptimizerType::HOIST_CONV_PACKED_PARAMS&lt;/code&gt;): This optimization pass moves convolution packed params to the root module, so that the convolution structs can be deleted. This decreases model size without impacting numerics.</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt; &lt;code&gt;MobileOptimizerType::HOIST_CONV_PACKED_PARAMS&lt;/code&gt; &lt;strong&gt;パックされた&lt;/strong&gt;パラメータの&lt;strong&gt;ホイスト&lt;/strong&gt;（ブラックリストオプションMobileOptimizerType :: HOIST_CONV_PACKED_PARAMS）：この最適化パスは、畳み込みパックされたパラメータをルートモジュールに移動し、畳み込み構造体を削除できるようにします。これにより、数値に影響を与えることなくモデルサイズが減少します。</target>
        </trans-unit>
        <trans-unit id="467e03838e306f1cc9b995f9ab07314a0c223da6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Conv2D + BatchNorm fusion&lt;/strong&gt; (blacklisting option &lt;code&gt;MobileOptimizerType::CONV_BN_FUSION&lt;/code&gt;): This optimization pass folds &lt;code&gt;Conv2d-BatchNorm2d&lt;/code&gt; into &lt;code&gt;Conv2d&lt;/code&gt; in &lt;code&gt;forward&lt;/code&gt; method of this module and all its submodules. The weight and bias of the &lt;code&gt;Conv2d&lt;/code&gt; are correspondingly updated.</source>
          <target state="translated">&lt;strong&gt;Conv2D + BatchNormフュージョン&lt;/strong&gt;（ブラックリストオプション &lt;code&gt;MobileOptimizerType::CONV_BN_FUSION&lt;/code&gt; ）：この最適化パスは、このモジュールとそのすべてのサブモジュールの &lt;code&gt;forward&lt;/code&gt; メソッドで &lt;code&gt;Conv2d-BatchNorm2d&lt;/code&gt; を &lt;code&gt;Conv2d&lt;/code&gt; にフォールドします。 &lt;code&gt;Conv2d&lt;/code&gt; の重みとバイアスはそれに応じて更新されます。</target>
        </trans-unit>
        <trans-unit id="797ae6ccf0f3fa0fcbeb9b7b388baedb5b006fbf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Distributed Autograd&lt;/strong&gt; stitches together local autograd engines on all the workers involved in the forward pass, and automatically reach out to them during the backward pass to compute gradients. This is especially helpful if the forward pass needs to span multiple machines when conducting, e.g., distributed model parallel training, parameter-server training, etc. With this feature, user code no longer needs to worry about how to send gradients across RPC boundaries and in which order should the local autograd engines be launched, which can become quite complicated where there are nested and inter-dependent RPC calls in the forward pass.</source>
          <target state="translated">&lt;strong&gt;分散Autograd&lt;/strong&gt;は、フォワードパスに関与するすべてのワーカーのローカルautogradエンジンをつなぎ合わせ、バックワードパス中に自動的にそれらに連絡して勾配を計算します。これは、分散モデルの並列トレーニング、パラメーターサーバートレーニングなどを実行するときにフォワードパスが複数のマシンにまたがる必要がある場合に特に役立ちます。この機能により、ユーザーコードはRPC境界を越えて勾配を送信する方法について心配する必要がなくなります。ローカルautogradエンジンを起動する順序。これは、フォワードパスにネストされた相互依存のRPC呼び出しがある場合、非常に複雑になる可能性があります。</target>
        </trans-unit>
        <trans-unit id="1bb7e35d8fadfa0ec492eaeee55b03e9a384fe57" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Distributed Optimizer&lt;/strong&gt;&amp;rsquo;s constructor takes a &lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;&lt;code&gt;Optimizer()&lt;/code&gt;&lt;/a&gt; (e.g., &lt;a href=&quot;optim#torch.optim.SGD&quot;&gt;&lt;code&gt;SGD()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;optim#torch.optim.Adagrad&quot;&gt;&lt;code&gt;Adagrad()&lt;/code&gt;&lt;/a&gt;, etc.) and a list of parameter RRefs, creates an &lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;&lt;code&gt;Optimizer()&lt;/code&gt;&lt;/a&gt; instance on each distinct RRef owner, and updates parameters accordingly when running &lt;code&gt;step()&lt;/code&gt;. When you have distributed forward and backward passes, parameters and gradients will be scattered across multiple workers, and hence it requires an optimizer on each of the involved workers. Distributed Optimizer wraps all those local optimizers into one, and provides a concise constructor and &lt;code&gt;step()&lt;/code&gt; API.</source>
          <target state="translated">&lt;strong&gt;Distributed Optimizer&lt;/strong&gt;のコンストラクターは、&lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt; &lt;code&gt;Optimizer()&lt;/code&gt; &lt;/a&gt;（&lt;a href=&quot;optim#torch.optim.SGD&quot;&gt; &lt;code&gt;SGD()&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;optim#torch.optim.Adagrad&quot;&gt; &lt;code&gt;Adagrad()&lt;/code&gt; &lt;/a&gt;など）とパラメーターRRefのリストを受け取り、個別のRRef所有者ごとに&lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt; &lt;code&gt;Optimizer()&lt;/code&gt; &lt;/a&gt;インスタンスを作成し、 &lt;code&gt;step()&lt;/code&gt; 実行時にそれに応じてパラメーターを更新します。（）。順方向パスと逆方向パスを分散すると、パラメーターと勾配が複数のワーカーに分散するため、関係する各ワーカーにオプティマイザーが必要になります。 Distributed Optimizerは、これらすべてのローカルオプティマイザーを1つにラップし、簡潔なコンストラクターと &lt;code&gt;step()&lt;/code&gt; APIを提供します。</target>
        </trans-unit>
        <trans-unit id="2d5f8bc4ac56cfd97f97d119eb653f063ce9a8c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Dropout removal&lt;/strong&gt; (blacklisting option &lt;code&gt;MobileOptimizerType::REMOVE_DROPOUT&lt;/code&gt;): This optimization pass removes &lt;code&gt;dropout&lt;/code&gt; and &lt;code&gt;dropout_&lt;/code&gt; nodes from this module when training is false.</source>
          <target state="translated">&lt;strong&gt;ドロップアウトの除去&lt;/strong&gt;（ブラックリストオプション &lt;code&gt;MobileOptimizerType::REMOVE_DROPOUT&lt;/code&gt; ）：この最適化パス削除しは &lt;code&gt;dropout&lt;/code&gt; と &lt;code&gt;dropout_&lt;/code&gt; 訓練がfalseの場合、このモジュールからのノード。</target>
        </trans-unit>
        <trans-unit id="c3e372f6656879627d1504382a4b7846f22e788b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;GLOO_SOCKET_IFNAME&lt;/strong&gt;, for example &lt;code&gt;export GLOO_SOCKET_IFNAME=eth0&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;GLOO_SOCKET_IFNAME&lt;/strong&gt;、たとえば &lt;code&gt;export GLOO_SOCKET_IFNAME=eth0&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1f6592a881a64ce2fc84fda7388b35a82d8e09ea" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;How to use this module:&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;このモジュールの使用方法：&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="1f89fecb44ae27dd51a780204df3a0f0d43ccc86" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Important Notices:&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;重要なお知らせ：&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="a9e3d7be29b190f07aadef5e37d2c8e47444424a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Important&lt;/strong&gt;: In contrast to the other models the inception_v3 expects tensors with a size of N x 3 x 299 x 299, so ensure your images are sized accordingly.</source>
          <target state="translated">&lt;strong&gt;重要&lt;/strong&gt;：他のモデルとは対照的に、inception_v3はN x 3 x 299 x 299のサイズのテンソルを想定しているため、画像のサイズが適切であることを確認してください。</target>
        </trans-unit>
        <trans-unit id="1df66398ad89ce8863dff1901f021a972d0a77b3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Insert and Fold prepacked ops&lt;/strong&gt; (blacklisting option &lt;code&gt;MobileOptimizerType::INSERT_FOLD_PREPACK_OPS&lt;/code&gt;): This optimization pass rewrites the graph to replace 2D convolutions and linear ops with their prepacked counterparts. Prepacked ops are stateful ops in that, they require some state to be created, such as weight prepacking and use this state, i.e. prepacked weights, during op execution. XNNPACK is one such backend that provides prepacked ops, with kernels optimized for mobile platforms (such as ARM CPUs). Prepacking of weight enables efficient memory access and thus faster kernel execution. At the moment &lt;code&gt;optimize_for_mobile&lt;/code&gt; pass rewrites the graph to replace &lt;code&gt;Conv2D/Linear&lt;/code&gt; with 1) op that pre-packs weight for XNNPACK conv2d/linear ops and 2) op that takes pre-packed weight and activation as input and generates output activations. Since 1 needs to be done only once, we fold the weight pre-packing such that it is done only once at model load time. This pass of the &lt;code&gt;optimize_for_mobile&lt;/code&gt; does 1 and 2 and then folds, i.e. removes, weight pre-packing ops.</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt; &lt;code&gt;MobileOptimizerType::INSERT_FOLD_PREPACK_OPS&lt;/code&gt; &lt;strong&gt;パックされたopsの挿入と&lt;/strong&gt;折りたたみ（ブラックリストオプションMobileOptimizerType :: INSERT_FOLD_PREPACK_OPS）：この最適化パスは、グラフを書き換えて、2D畳み込みと線形opsを事前にパックされたものに置き換えます。事前パックされたオペレーションはステートフルオペレーションであり、ウェイトの事前パッキングなどの状態を作成する必要があり、オペレーションの実行中にこの状態、つまり事前パックされたウェイトを使用します。 XNNPACKは、モバイルプラットフォーム（ARM CPUなど）用に最適化されたカーネルを備えた、事前にパックされたopsを提供するそのようなバックエンドの1つです。重みの事前パッキングにより、効率的なメモリアクセスが可能になり、カーネルの実行が高速化されます。現時点では、 &lt;code&gt;optimize_for_mobile&lt;/code&gt; パスはグラフを書き換えて &lt;code&gt;Conv2D/Linear&lt;/code&gt; を置き換えます1）XNNPACK conv2d / linear opsの重みを事前にパックするop、および2）事前にパックされた重みとアクティブ化を入力として受け取り、出力のアクティブ化を生成するop。1は1回だけ実行する必要があるため、モデルのロード時に1回だけ実行されるように、ウェイトの事前梱包を折ります。このパス &lt;code&gt;optimize_for_mobile&lt;/code&gt; は1と2を行い、その後、ひだ、すなわち、削除、体重事前梱包OPS。</target>
        </trans-unit>
        <trans-unit id="c203022c4b7e41a789c0850a218035678246aafc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;LU_data&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the packed LU factorization data</source>
          <target state="translated">&lt;strong&gt;LU_data&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;パックされたLU分解データ</target>
        </trans-unit>
        <trans-unit id="b68de83e526debfd8f4bba73fb2af28a093a044c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;LU_data&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the pivoted LU factorization of A from &lt;a href=&quot;torch.lu#torch.lu&quot;&gt;&lt;code&gt;torch.lu()&lt;/code&gt;&lt;/a&gt; of size</source>
          <target state="translated">&lt;strong&gt;LU_data&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;サイズの&lt;a href=&quot;torch.lu#torch.lu&quot;&gt; &lt;code&gt;torch.lu()&lt;/code&gt; &lt;/a&gt;からのAのピボットLU分解</target>
        </trans-unit>
        <trans-unit id="6c5a7f1d3cffe65ae015e850f4296ab899102cff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;LU_pivots&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the packed LU factorization pivots</source>
          <target state="translated">&lt;strong&gt;LU_pivots&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;パックされたLU分解ピボット</target>
        </trans-unit>
        <trans-unit id="8f3d2fa0664b850596925f91912a8226903a5e6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;LU_pivots&lt;/strong&gt; (&lt;em&gt;IntTensor&lt;/em&gt;) &amp;ndash; the pivots of the LU factorization from &lt;a href=&quot;torch.lu#torch.lu&quot;&gt;&lt;code&gt;torch.lu()&lt;/code&gt;&lt;/a&gt; of size</source>
          <target state="translated">&lt;strong&gt;LU_pivots&lt;/strong&gt;（&lt;em&gt;IntTensor&lt;/em&gt;）&amp;ndash;サイズの&lt;a href=&quot;torch.lu#torch.lu&quot;&gt; &lt;code&gt;torch.lu()&lt;/code&gt; &lt;/a&gt;からのLU分解のピボット</target>
        </trans-unit>
        <trans-unit id="cde0f3c0abde6143690d2cb7605d91fbcc4fe6ec" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;N&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Number of columns in the output. If N is not specified, a square array is returned</source>
          <target state="translated">&lt;strong&gt;N&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;出力の列数。Nが指定されていない場合、正方形の配列が返されます</target>
        </trans-unit>
        <trans-unit id="04a56de9a19a514a1fcf10373351c67efd1b49d3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;NCCL_SOCKET_IFNAME&lt;/strong&gt;, for example &lt;code&gt;export NCCL_SOCKET_IFNAME=eth0&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;NCCL_SOCKET_IFNAME&lt;/strong&gt;、たとえば &lt;code&gt;export NCCL_SOCKET_IFNAME=eth0&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ec5bc033e1610e4a75f2543da0a843e4cbbadc81" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Note&lt;/strong&gt; &amp;ndash; if kdim and vdim are None, they will be set to embed_dim such that</source>
          <target state="translated">&lt;strong&gt;注&lt;/strong&gt;&amp;ndash; kdimとvdimがNoneの場合、次のようにembed_dimに設定されます。</target>
        </trans-unit>
        <trans-unit id="576aa905938e327a5ed4b8fede98a6c9f70e96e9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Propagate names&lt;/strong&gt;: &lt;em&gt;unify&lt;/em&gt; the names to select which one to propagate. In the case of &lt;code&gt;x + y&lt;/code&gt;, &lt;code&gt;unify('X', None) = 'X'&lt;/code&gt; because &lt;code&gt;'X'&lt;/code&gt; is more specific than &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;名前の伝達&lt;/strong&gt;：名前を&lt;em&gt;統合して&lt;/em&gt;、&lt;strong&gt;伝達&lt;/strong&gt;する名前を選択します。 &lt;code&gt;x + y&lt;/code&gt; 場合、 &lt;code&gt;unify('X', None) = 'X'&lt;/code&gt; です。これは、 &lt;code&gt;'X'&lt;/code&gt; が &lt;code&gt;None&lt;/code&gt; よりも具体的であるためです。</target>
        </trans-unit>
        <trans-unit id="5ee6ef9bb908af00d480a75de89ef5668a859963" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Propagate names&lt;/strong&gt;: name inference propagates names to output tensors.</source>
          <target state="translated">&lt;strong&gt;名前の伝播&lt;/strong&gt;：名前の推論は、名前を出力テンソルに伝播します。</target>
        </trans-unit>
        <trans-unit id="af407aa1e4067bd772f2d564ff50863c94343551" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;QR&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): the details of the QR factorization</source>
          <target state="translated">&lt;strong&gt;QR&lt;/strong&gt;（&lt;em&gt;テンソル&lt;/em&gt;）：QR分解の詳細</target>
        </trans-unit>
        <trans-unit id="6af63ab37345973eb43a4a5fd4d33de8547a1e3b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ReLU/Hardtanh fusion&lt;/strong&gt;: XNNPACK ops support fusion of clamping. That is clamping of output activation is done as part of the kernel, including for 2D convolution and linear op kernels. Thus clamping effectively comes for free. Thus any op that can be expressed as clamping op, such as &lt;code&gt;ReLU&lt;/code&gt; or &lt;code&gt;hardtanh&lt;/code&gt;, can be fused with previous &lt;code&gt;Conv2D&lt;/code&gt; or &lt;code&gt;linear&lt;/code&gt; op in XNNPACK. This pass rewrites graph by finding &lt;code&gt;ReLU/hardtanh&lt;/code&gt; ops that follow XNNPACK &lt;code&gt;Conv2D/linear&lt;/code&gt; ops, written by the previous pass, and fuses them together.</source>
          <target state="translated">&lt;strong&gt;ReLU / Hardtanhフュージョン&lt;/strong&gt;：XNNPACKopsはクランプのフュージョンをサポートします。つまり、出力アクティベーションのクランプは、2D畳み込みカーネルや線形opカーネルを含め、カーネルの一部として実行されます。したがって、クランプは効果的に無料で提供されます。したがって、 &lt;code&gt;ReLU&lt;/code&gt; や &lt;code&gt;hardtanh&lt;/code&gt; などのクランプopとして表現できるopは、XNNPACKの以前の &lt;code&gt;Conv2D&lt;/code&gt; または &lt;code&gt;linear&lt;/code&gt; opと融合できます。このパスは、前のパスで書き込まれたXNNPACK &lt;code&gt;Conv2D/linear&lt;/code&gt; opsに続く &lt;code&gt;ReLU/hardtanh&lt;/code&gt; opsを見つけることによってグラフを書き換え、それらを融合します。</target>
        </trans-unit>
        <trans-unit id="fa93a38017c6f0838a547724d9c4d119cf4afed6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Remote Procedure Call (RPC)&lt;/strong&gt; supports running a function on the specified destination worker with the given arguments and getting the return value back or creating a reference to the return value. There are three main RPC APIs: &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt;&lt;code&gt;rpc_sync()&lt;/code&gt;&lt;/a&gt; (synchronous), &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt; (asynchronous), and &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt;&lt;code&gt;remote()&lt;/code&gt;&lt;/a&gt; (asynchronous and returns a reference to the remote return value). Use the synchronous API if the user code cannot proceed without the return value. Otherwise, use the asynchronous API to get a future, and wait on the future when the return value is needed on the caller. The &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt;&lt;code&gt;remote()&lt;/code&gt;&lt;/a&gt; API is useful when the requirement is to create something remotely but never need to fetch it to the caller. Imagine the case that a driver process is setting up a parameter server and a trainer. The driver can create an embedding table on the parameter server and then share the reference to the embedding table with the trainer, but itself will never use the embedding table locally. In this case, &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt;&lt;code&gt;rpc_sync()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt; are no longer appropriate, as they always imply that the return value will be returned to the caller immediately or in the future.</source>
          <target state="translated">&lt;strong&gt;リモートプロシージャコール（RPC）&lt;/strong&gt;は、指定された引数を使用して指定された宛先ワーカーで関数を実行し、戻り値を取得するか、戻り値への参照を作成することをサポートします。 3つの主要な&lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt; &lt;code&gt;rpc_sync()&lt;/code&gt; &lt;/a&gt;があります：rpc_sync（）（同期）、&lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;（非同期）、および&lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt; &lt;code&gt;remote()&lt;/code&gt; &lt;/a&gt;（非同期で、リモート戻り値への参照を返します）。ユーザーコードが戻り値なしで続行できない場合は、同期APIを使用します。それ以外の場合は、非同期APIを使用してfutureを取得し、呼び出し元で戻り値が必要になったときにfutureを待ちます。&lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt; &lt;code&gt;remote()&lt;/code&gt; &lt;/a&gt;APIは、何かをリモートで作成する必要があるが、それを呼び出し元にフェッチする必要がない場合に役立ちます。ドライバープロセスがパラメーターサーバーとトレーナーをセットアップしている場合を想像してみてください。ドライバーはパラメーターサーバー上に埋め込みテーブルを作成してから、埋め込みテーブルへの参照をトレーナーと共有できますが、ドライバー自体が埋め込みテーブルをローカルで使用することはありません。この場合、&lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt; &lt;code&gt;rpc_sync()&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;は、戻り値がすぐにまたは将来的に呼び出し元に返されることを常に意味するため、適切ではなくなりました。</target>
        </trans-unit>
        <trans-unit id="5314a5e0f78e161be1308e7f0f36425e0e43560f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Remote Reference (RRef)&lt;/strong&gt; serves as a distributed shared pointer to a local or remote object. It can be shared with other workers and reference counting will be handled transparently. Each RRef only has one owner and the object only lives on that owner. Non-owner workers holding RRefs can get copies of the object from the owner by explicitly requesting it. This is useful when a worker needs to access some data object, but itself is neither the creator (the caller of &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt;&lt;code&gt;remote()&lt;/code&gt;&lt;/a&gt;) or the owner of the object. The distributed optimizer, as we will discuss below, is one example of such use cases.</source>
          <target state="translated">&lt;strong&gt;リモート参照（RRef）&lt;/strong&gt;は、ローカルオブジェクトまたはリモートオブジェクトへの分散共有ポインターとして機能します。他のワーカーと共有でき、参照カウントは透過的に処理されます。各RRefには1人の所有者しかなく、オブジェクトはその所有者にのみ存在します。RRefを保持している所有者以外のワーカーは、明示的に要求することにより、所有者からオブジェクトのコピーを取得できます。これは、ワーカーがデータオブジェクトにアクセスする必要があるが、それ自体がオブジェクトの作成者（&lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt; &lt;code&gt;remote()&lt;/code&gt; の&lt;/a&gt;呼び出し元）でも所有者でもない場合に役立ちます。以下で説明するように、分散オプティマイザはそのようなユースケースの一例です。</target>
        </trans-unit>
        <trans-unit id="9ac1d5992a6fca7da1c35caf72d55ee459b83fab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Same dims as t.&lt;/strong&gt; (&lt;em&gt;applied.&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;tと同じ薄暗くなります。&lt;/strong&gt;（&lt;em&gt;適用されます。&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="962e703212f421c013940c950f0179bcfd7e5558" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Scripting a function&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;関数のスクリプト&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="8db690c93c292d8d188cd36e3aadd7bd6d0213c5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Scripting an nn.Module&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;nn.Moduleのスクリプト&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="8f1531884e296514d90d800035577a8e1accf01f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;When&lt;/strong&gt;&lt;code&gt;as_tuple&lt;/code&gt;&lt;strong&gt;is ``False`` (default)&lt;/strong&gt;:</source>
          <target state="translated">&lt;strong&gt;とき&lt;/strong&gt; &lt;code&gt;as_tuple&lt;/code&gt; は&lt;strong&gt;`` False``を（デフォルト）です&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="5ab18d59c167aab4a342a25877738485f51a7096" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;When&lt;/strong&gt;&lt;code&gt;as_tuple&lt;/code&gt;&lt;strong&gt;is ``True``&lt;/strong&gt;:</source>
          <target state="translated">&lt;strong&gt;とき&lt;/strong&gt; &lt;code&gt;as_tuple&lt;/code&gt; は&lt;strong&gt;`` True``にあります&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="e7078285e2211f504ade0f00ffd2f939c57da6bd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;X&lt;/strong&gt; (&lt;em&gt;tensor&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the input tensor of size</source>
          <target state="translated">&lt;strong&gt;X&lt;/strong&gt;（&lt;em&gt;テンソル&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;サイズの入力テンソル</target>
        </trans-unit>
        <trans-unit id="b1b67d6a132e9c963db47428cb2379e1e546861a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;_extra_files&lt;/strong&gt; &amp;ndash; Map from filename to contents which will be stored as part of &lt;code&gt;f&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;_extra_files&lt;/strong&gt; &amp;ndash;ファイル名から &lt;code&gt;f&lt;/code&gt; の一部として保存されるコンテンツにマップします。</target>
        </trans-unit>
        <trans-unit id="a683a25cede5c84fac0359282cc1ba91c60e4ca5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;_extra_files&lt;/strong&gt; (&lt;em&gt;dictionary of filename to content&lt;/em&gt;) &amp;ndash; The extra filenames given in the map would be loaded and their content would be stored in the provided map.</source>
          <target state="translated">&lt;strong&gt;_extra_files&lt;/strong&gt;（&lt;em&gt;ファイル名からコンテンツへの辞書&lt;/em&gt;）&amp;ndash;マップで指定された追加のファイル名がロードされ、それらのコンテンツが提供されたマップに保存されます。</target>
        </trans-unit>
        <trans-unit id="9e611d50a4e099f661790454d298a621c6db3772" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;a&lt;/strong&gt; &amp;ndash; the lower bound of the uniform distribution</source>
          <target state="translated">&lt;strong&gt;a&lt;/strong&gt; &amp;ndash;一様分布の下限</target>
        </trans-unit>
        <trans-unit id="6d6b643d3bc99d8356ba238e8f3bc442ddc42326" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;a&lt;/strong&gt; &amp;ndash; the negative slope of the rectifier used after this layer (only used with &lt;code&gt;'leaky_relu'&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;a&lt;/strong&gt; &amp;ndash;このレイヤーの後に使用される整流器の負の勾配（ &lt;code&gt;'leaky_relu'&lt;/code&gt; のみ使用）</target>
        </trans-unit>
        <trans-unit id="13e08f7f03b31160851ef342f7107a5a0a61cecd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;a&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Left tensor to contract</source>
          <target state="translated">&lt;strong&gt;a&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;収縮する左テンソル</target>
        </trans-unit>
        <trans-unit id="48c6d9a7b5991de5ceb0f2f0002b9b581b68e5fe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;abs&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The absolute value the complex tensor. Must be float or double.</source>
          <target state="translated">&lt;strong&gt;abs&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;複素テンソルの絶対値。浮動小数点または倍精度である必要があります。</target>
        </trans-unit>
        <trans-unit id="769675c7e81eef43f6366734f82732784d9c6c35" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;accumulate&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether to accumulate into self</source>
          <target state="translated">&lt;strong&gt;蓄積&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;自己に蓄積するかどうか</target>
        </trans-unit>
        <trans-unit id="06c11a2977c3992817e33b18690cf09337f0cacb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;activation&lt;/strong&gt; &amp;ndash; the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).</source>
          <target state="translated">&lt;strong&gt;活性化&lt;/strong&gt;&amp;ndash;エンコーダー/デコーダー中間層、reluまたはgeluの活性化関数（デフォルト= relu）。</target>
        </trans-unit>
        <trans-unit id="923b9fe69988df8bf8cfd3d90e121105f87077db" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;activation&lt;/strong&gt; &amp;ndash; the activation function of intermediate layer, relu or gelu (default=relu).</source>
          <target state="translated">&lt;strong&gt;活性化&lt;/strong&gt;&amp;ndash;中間層、reluまたはgeluの活性化関数（デフォルト= relu）。</target>
        </trans-unit>
        <trans-unit id="761a00d8ce0801730293c2567fab0839a725bb91" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;add_bias_kv&lt;/strong&gt; &amp;ndash; add bias to the key and value sequences at dim=0.</source>
          <target state="translated">&lt;strong&gt;add_bias_kv&lt;/strong&gt; &amp;ndash; dim = 0でキーと値のシーケンスにバイアスを追加します。</target>
        </trans-unit>
        <trans-unit id="9907f733a01182c1fbfd28915afc66890b30f3df" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;add_zero_attn&lt;/strong&gt; &amp;ndash; add a new batch of zeros to the key and value sequences at dim=1.</source>
          <target state="translated">&lt;strong&gt;add_zero_attn&lt;/strong&gt; &amp;ndash; dim = 1のキーと値のシーケンスにゼロの新しいバッチを追加します。</target>
        </trans-unit>
        <trans-unit id="25e60582f1deda7c8462bf51f20fc8de0b64b938" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;affine&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;affine&lt;/strong&gt; &amp;ndash; &lt;code&gt;True&lt;/code&gt; に設定すると、このモジュールに学習可能なアフィンパラメーターがあり、バッチ正規化の場合と同じ方法で初期化されるブール値。デフォルト： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4c8473b78a9df928d3a9e2a5c428e31960781087" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;affine&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module has learnable affine parameters. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;affine&lt;/strong&gt; &amp;ndash; &lt;code&gt;True&lt;/code&gt; に設定すると、このモジュールに学習可能なアフィンパラメーターを持つブール値。デフォルト： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="643d4c7ac1c81e9da7cfa09a73aadce98fb242af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;affine&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;affine&lt;/strong&gt; &amp;ndash; &lt;code&gt;True&lt;/code&gt; に設定すると、このモジュールに学習可能なチャネルごとのアフィンパラメーターが1（重みの場合）と0（バイアスの場合）に初期化されるブール値。デフォルト： &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="559454a2b31e0657d70c2b51eea7f95541808926" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Geometrically, we consider the pixels of the input and output as squares rather than points. If set to &lt;code&gt;True&lt;/code&gt;, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to &lt;code&gt;False&lt;/code&gt;, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation &lt;em&gt;independent&lt;/em&gt; of input size when &lt;code&gt;scale_factor&lt;/code&gt; is kept the same. This only has an effect when &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;'bilinear'&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;align_corners&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;幾何学的に、入力と出力のピクセルを点ではなく正方形と見なします。 &lt;code&gt;True&lt;/code&gt; に設定すると、入力テンソルと出力テンソルはコーナーピクセルの中心点によって整列され、コーナーピクセルの値が保持されます。 &lt;code&gt;False&lt;/code&gt; に設定すると、入力テンソルと出力テンソルはコーナーピクセルのコーナーポイントによって整列され、補間では境界外の値にエッジ値のパディングが使用されるため、 &lt;code&gt;scale_factor&lt;/code&gt; が同じに保たれている場合、この操作は入力サイズに&lt;em&gt;依存&lt;/em&gt;しません。これは、 &lt;code&gt;mode&lt;/code&gt; が &lt;code&gt;'bilinear'&lt;/code&gt; 場合にのみ効果があります。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="36cb44cb5ed893a7388ab1e5bafec71dbcb4bf49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Geometrically, we consider the pixels of the input and output as squares rather than points. If set to &lt;code&gt;True&lt;/code&gt;, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to &lt;code&gt;False&lt;/code&gt;, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation &lt;em&gt;independent&lt;/em&gt; of input size when &lt;code&gt;scale_factor&lt;/code&gt; is kept the same. This only has an effect when &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;'linear'&lt;/code&gt;, &lt;code&gt;'bilinear'&lt;/code&gt;, &lt;code&gt;'bicubic'&lt;/code&gt; or &lt;code&gt;'trilinear'&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;align_corners&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;幾何学的に、入力と出力のピクセルを点ではなく正方形と見なします。 &lt;code&gt;True&lt;/code&gt; に設定すると、入力テンソルと出力テンソルはコーナーピクセルの中心点によって整列され、コーナーピクセルの値が保持されます。 &lt;code&gt;False&lt;/code&gt; に設定すると、入力テンソルと出力テンソルはコーナーピクセルのコーナーポイントによって整列され、補間では境界外の値にエッジ値のパディングが使用されるため、 &lt;code&gt;scale_factor&lt;/code&gt; が同じに保たれている場合、この操作は入力サイズに&lt;em&gt;依存&lt;/em&gt;しません。これは、 &lt;code&gt;mode&lt;/code&gt; が &lt;code&gt;'linear'&lt;/code&gt; 、 &lt;code&gt;'bilinear'&lt;/code&gt; 線形」の場合にのみ効果があります。 &lt;code&gt;'bicubic'&lt;/code&gt; または &lt;code&gt;'trilinear'&lt;/code&gt; 。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9cfac0e91a78176be96ddc6ef793ea60d709b2eb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Geometrically, we consider the pixels of the input as squares rather than points. If set to &lt;code&gt;True&lt;/code&gt;, the extrema (&lt;code&gt;-1&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;) are considered as referring to the center points of the input&amp;rsquo;s corner pixels. If set to &lt;code&gt;False&lt;/code&gt;, they are instead considered as referring to the corner points of the input&amp;rsquo;s corner pixels, making the sampling more resolution agnostic. This option parallels the &lt;code&gt;align_corners&lt;/code&gt; option in &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;interpolate()&lt;/code&gt;&lt;/a&gt;, and so whichever option is used here should also be used there to resize the input image before grid sampling. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;align_corners&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;幾何学的には、入力のピクセルを点ではなく正方形と見なします。 &lt;code&gt;True&lt;/code&gt; に設定すると、極値（ &lt;code&gt;-1&lt;/code&gt; および &lt;code&gt;1&lt;/code&gt; ）は、入力のコーナーピクセルの中心点を参照していると見なされます。 &lt;code&gt;False&lt;/code&gt; に設定すると、代わりに入力のコーナーピクセルのコーナーポイントを参照していると見なされ、サンプリングの解像度に依存しなくなります。このオプションは、&lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;interpolate()&lt;/code&gt; の&lt;/a&gt; &lt;code&gt;align_corners&lt;/code&gt; オプションに対応しているため、ここで使用するオプションは、グリッドサンプリングの前に入力画像のサイズを変更するためにも使用する必要があります。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="18bc31d486c6b3f3dc8b60be043591c427b7d925" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, consider &lt;code&gt;-1&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; to refer to the centers of the corner pixels rather than the image corners. Refer to &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; for a more complete description. A grid generated by &lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt;&lt;code&gt;affine_grid()&lt;/code&gt;&lt;/a&gt; should be passed to &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; with the same setting for this option. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;align_corners&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、 &lt;code&gt;-1&lt;/code&gt; と &lt;code&gt;1&lt;/code&gt; を考慮して、画像のコーナーではなくコーナーピクセルの中心を参照します。より完全な説明については、&lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt; &lt;code&gt;grid_sample()&lt;/code&gt; &lt;/a&gt;を参照してください。&lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt; &lt;code&gt;affine_grid()&lt;/code&gt; &lt;/a&gt;によって生成されたグリッドは、このオプションと同じ設定で&lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt; &lt;code&gt;grid_sample()&lt;/code&gt; に&lt;/a&gt;渡される必要があります。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b7e8f77490dec1d2b29fdbb675f0df9b2418c385" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;align_corners&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;'linear'&lt;/code&gt;, &lt;code&gt;'bilinear'&lt;/code&gt;, or &lt;code&gt;'trilinear'&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;align_corners&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、入力テンソルと出力テンソルのコーナーピクセルが整列されるため、これらのピクセルの値が保持されます。これは、 &lt;code&gt;mode&lt;/code&gt; が &lt;code&gt;'linear'&lt;/code&gt; 、 &lt;code&gt;'bilinear'&lt;/code&gt; 、または &lt;code&gt;'trilinear'&lt;/code&gt; 線形」の場合にのみ効果があります。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0b7ed549d17ccf06c2a73555f8c110d2b2777c64" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; &amp;ndash; multiplicative factor. Default: 0.0001</source>
          <target state="translated">&lt;strong&gt;alpha&lt;/strong&gt; &amp;ndash;乗法係数。デフォルト：0.0001</target>
        </trans-unit>
        <trans-unit id="0c7e987f8b1b60db336f8816506d789632b3a056" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; &amp;ndash; the</source>
          <target state="translated">&lt;strong&gt;アルファ&lt;/strong&gt;&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="9144dd58af56a7e78f94a1c6d24c0dd775ffa08a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; &amp;ndash; the alpha constant</source>
          <target state="translated">&lt;strong&gt;alpha&lt;/strong&gt; &amp;ndash;アルファ定数</target>
        </trans-unit>
        <trans-unit id="aaed323126d545c766d6b20c8792516dd1012883" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The coefficient</source>
          <target state="translated">&lt;strong&gt;alpha&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;係数</target>
        </trans-unit>
        <trans-unit id="bef472f2822264f8844272a566f6ae7e013eac0f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the scalar multiplier for &lt;code&gt;other&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;alpha&lt;/strong&gt;（&lt;em&gt;数値&lt;/em&gt;）&amp;ndash; &lt;code&gt;other&lt;/code&gt; スカラー倍算</target>
        </trans-unit>
        <trans-unit id="eafad06cad281ac6182faa727319bec1cdc6e990" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for</source>
          <target state="translated">&lt;strong&gt;alpha&lt;/strong&gt;（&lt;em&gt;数値&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;の乗数</target>
        </trans-unit>
        <trans-unit id="c0c3d78d14051f159b2c380fbcdbccf7032cc7c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for &lt;code&gt;batch1 @ batch2&lt;/code&gt; (</source>
          <target state="translated">&lt;strong&gt;alpha&lt;/strong&gt;（&lt;em&gt;数値&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;batch1 @ batch2&lt;/code&gt; の乗数（</target>
        </trans-unit>
        <trans-unit id="01962c7a9caccb6944e0dd97ce4d52d57152f949" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;alpha&lt;/strong&gt; (&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; the scalar multiplier for &lt;code&gt;other&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;alpha&lt;/strong&gt;（&lt;em&gt;Scalar&lt;/em&gt;）&amp;ndash; &lt;code&gt;other&lt;/code&gt; スカラー倍算</target>
        </trans-unit>
        <trans-unit id="65c245be1540fcaab2e6ed5fa6d61bf7293f1117" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;amount&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The quantity by which the counter will be incremented.</source>
          <target state="translated">&lt;strong&gt;amount&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;カウンターが増分される量。</target>
        </trans-unit>
        <trans-unit id="a86a8ab17b48cd324b8df6130d66a8bfdd18e3f5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;amount&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; quantity of channels to prune. If &lt;code&gt;float&lt;/code&gt;, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If &lt;code&gt;int&lt;/code&gt;, it represents the absolute number of parameters to prune.</source>
          <target state="translated">&lt;strong&gt;amount&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;プルーニングするチャネルの量。 &lt;code&gt;float&lt;/code&gt; の場合、0.0から1.0の間で、プルーニングするパラメーターの割合を表す必要があります。 &lt;code&gt;int&lt;/code&gt; の場合、プルーニングするパラメーターの絶対数を表します。</target>
        </trans-unit>
        <trans-unit id="3a209b964ca3729de2ffee2abca2f378072e01b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;amount&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; quantity of parameters to prune. If &lt;code&gt;float&lt;/code&gt;, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If &lt;code&gt;int&lt;/code&gt;, it represents the absolute number of parameters to prune.</source>
          <target state="translated">&lt;strong&gt;amount&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;プルーニングするパラメーターの量。 &lt;code&gt;float&lt;/code&gt; の場合、0.0から1.0の間で、プルーニングするパラメーターの割合を表す必要があります。 &lt;code&gt;int&lt;/code&gt; の場合、プルーニングするパラメーターの絶対数を表します。</target>
        </trans-unit>
        <trans-unit id="412d2ada083b0f76466ad50fa67e45bff30d28be" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;angle&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The angle of the complex tensor. Must be same dtype as &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;angle&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;複素テンソルの角度。&lt;a href=&quot;torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs&lt;/code&gt; &lt;/a&gt;と同じdtypeである必要があります。</target>
        </trans-unit>
        <trans-unit id="f3296f07398c854ae12376a870bec69a75a03688" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; any argument (unused)</source>
          <target state="translated">&lt;strong&gt;args&lt;/strong&gt; &amp;ndash;任意の引数（未使用）</target>
        </trans-unit>
        <trans-unit id="58fda4a765460c36ef0809232ca862a52a746df4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; arguments passed on to a subclass of &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;strong&gt;args &lt;/strong&gt;&lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod&quot;&gt; &lt;code&gt;BasePruningMethod&lt;/code&gt; &lt;/a&gt;サブクラスに渡される引数</target>
        </trans-unit>
        <trans-unit id="7cf26c4412d3af55e77c722a46e12a3de8c0a087" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; arguments passed on to a subclass of &lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;strong&gt;args &lt;/strong&gt;&lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt; &lt;code&gt;BasePruningMethod&lt;/code&gt; &lt;/a&gt;サブクラスに渡される引数</target>
        </trans-unit>
        <trans-unit id="06452ebbd4e53eb2a57ca1d68b73dffabe3a9e69" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; arguments to pass to the optimizer constructor on each worker.</source>
          <target state="translated">&lt;strong&gt;args&lt;/strong&gt; &amp;ndash;各ワーカーのオプティマイザーコンストラクターに渡す引数。</target>
        </trans-unit>
        <trans-unit id="527ad6571a32b831a6fa84b982cf26783e444b79" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; tuple containing inputs to the &lt;code&gt;function&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;args&lt;/strong&gt; &amp;ndash; &lt;code&gt;function&lt;/code&gt; への入力を含むタプル</target>
        </trans-unit>
        <trans-unit id="bbbea77d989716da38960ac97dc3c1b13738a750" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; the argument tuple for the &lt;code&gt;func&lt;/code&gt; invocation.</source>
          <target state="translated">&lt;strong&gt;args&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;） &lt;code&gt;func&lt;/code&gt; 呼び出しの引数タプル。</target>
        </trans-unit>
        <trans-unit id="a6ccd94c0969142ac42560e759fc19e3bb7b28e2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;args&lt;/strong&gt; (&lt;em&gt;tuple of arguments&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; the inputs to the model, e.g., such that &lt;code&gt;model(*args)&lt;/code&gt; is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args. If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor. (Note: passing keyword arguments to the model is not currently supported. Give us a shout if you need it.)</source>
          <target state="translated">&lt;strong&gt;args&lt;/strong&gt;（&lt;em&gt;引数のタプル&lt;/em&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;モデルへの入力。たとえば、 &lt;code&gt;model(*args)&lt;/code&gt; がモデルの有効な呼び出しです。テンソル以外の引数は、エクスポートされたモデルにハードコードされます。Tensor引数は、引数で発生する順序で、エクスポートされたモデルの入力になります。argsがテンソルの場合、これはそのテンソルの1元タプルで呼び出したのと同じです。（注：モデルへのキーワード引数の受け渡しは現在サポートされていません。必要に応じてお知らせください。）</target>
        </trans-unit>
        <trans-unit id="16fdafe6c67075fc015e420e0bb0f8d630e5f4e5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;as torch.nn.quantized.Conv2d&lt;/strong&gt; (&lt;em&gt;Same&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;torch.nn.quantized.Conv2d&lt;/strong&gt;（&lt;em&gt;同じ&lt;/em&gt;）&lt;strong&gt;として&lt;/strong&gt;&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="79c9815e7f5a48dc393a28760411256901b78dc1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;as torch.nn.quantized.Linear&lt;/strong&gt; (&lt;em&gt;Same&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;torch.nn.quantized.Linear&lt;/strong&gt;（&lt;em&gt;同じ&lt;/em&gt;）&lt;strong&gt;として&lt;/strong&gt;&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="b96cfefc433ac6d54b2e6d75776bf06a2aa2c2ef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;async_op&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Whether this op should be an async op</source>
          <target state="translated">&lt;strong&gt;async_op&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;この操作を非同期操作にするかどうか</target>
        </trans-unit>
        <trans-unit id="1ef0166b6388a4f0216cd0af7040aee2dea01134" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;async_op&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Whether this op should be an async op.</source>
          <target state="translated">&lt;strong&gt;async_op&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;この操作を非同期操作にするかどうか。</target>
        </trans-unit>
        <trans-unit id="3c393f84aad8fa71c90593f9038432da8c10f4ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;aten&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset&amp;lt;version&amp;gt;.py are exported as ATen ops.</source>
          <target state="translated">&lt;strong&gt;aten&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;デフォルトはFalse&lt;/em&gt;）&amp;ndash; [非推奨。useoperator_export_type]モデルをatenモードでエクスポートします。atenモードを使用している場合、symbolic_opset &amp;lt;version&amp;gt; .pyの関数によってエクスポートされた元のすべてのopsはATenopsとしてエクスポートされます。</target>
        </trans-unit>
        <trans-unit id="d413d0f3f92358bb21c09129aebaafca7327d9cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;atol&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; absolute tolerance. Default: 1e-08</source>
          <target state="translated">&lt;strong&gt;atol&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;絶対許容誤差。デフォルト：1e-08</target>
        </trans-unit>
        <trans-unit id="3edeccc3509794121a4c8cbb0c9c3947569f0355" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;attn_mask&lt;/strong&gt; &amp;ndash; 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.</source>
          <target state="translated">&lt;strong&gt;attn_mask&lt;/strong&gt; &amp;ndash;特定の位置への注意を妨げる2Dまたは3Dマスク。2Dマスクはすべてのバッチに対してブロードキャストされますが、3Dマスクでは各バッチのエントリに異なるマスクを指定できます。</target>
        </trans-unit>
        <trans-unit id="5b70ffd8c9aa3ced153863f4307b36931662a4e6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;aux_logits&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, add an auxiliary branch that can improve training. Default: &lt;em&gt;True&lt;/em&gt;</source>
          <target state="translated">&lt;strong&gt;aux_logits&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash; Trueの場合、トレーニングを改善できる補助ブランチを追加します。デフォルト：&lt;em&gt;True&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="b7f0d0ce983d8eb3de6cf4b329489cdf94ceff22" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;aux_logits&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, adds two auxiliary branches that can improve training. Default: &lt;em&gt;False&lt;/em&gt; when pretrained is True otherwise &lt;em&gt;True&lt;/em&gt;</source>
          <target state="translated">&lt;strong&gt;aux_logits&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash; Trueの場合、トレーニングを改善できる2つの補助ブランチを追加します。デフォルト：事前トレーニング済みの場合は&lt;em&gt;False&lt;/em&gt;がTrue、それ以外の場合は&lt;em&gt;True&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="a7ddd6c4129c6131d8812cc22b9dde600a187a20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;axis&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension on which apply per-channel quantization</source>
          <target state="translated">&lt;strong&gt;axis&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;チャネルごとの量子化を適用する次元</target>
        </trans-unit>
        <trans-unit id="8531d8872ce2920e620f2844e86bbe33088a4efd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;b&lt;/strong&gt; &amp;ndash; the upper bound of the uniform distribution</source>
          <target state="translated">&lt;strong&gt;b&lt;/strong&gt; &amp;ndash;一様分布の上限</target>
        </trans-unit>
        <trans-unit id="b8325c83f74f3a97ea28f95055f6ccadb9fe0420" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;b&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Right tensor to contract</source>
          <target state="translated">&lt;strong&gt;b&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;契約する正しいテンソル</target>
        </trans-unit>
        <trans-unit id="41ffccfc315d025808792ff986c6bf85fc3cfb20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;b&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the RHS tensor of size</source>
          <target state="translated">&lt;strong&gt;b&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;サイズのRHSテンソル</target>
        </trans-unit>
        <trans-unit id="bedd1fb2f39140f6b07702dfe6d9850270d68d48" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;backend&lt;/strong&gt; &amp;ndash; Device type to use for running the result model (&amp;lsquo;CPU&amp;rsquo;(default) or &amp;lsquo;Vulkan&amp;rsquo;).</source>
          <target state="translated">&lt;strong&gt;バックエンド&lt;/strong&gt;&amp;ndash;結果モデルの実行に使用するデバイスタイプ（「CPU」（デフォルト）または「Vulkan」）。</target>
        </trans-unit>
        <trans-unit id="857b931bd7b5bfc52eb0de68ac6430dc390f6af7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;backend&lt;/strong&gt; (&lt;a href=&quot;#torch.distributed.rpc.BackendType&quot;&gt;BackendType&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The type of RPC backend implementation. Supported values include &lt;code&gt;BackendType.TENSORPIPE&lt;/code&gt; (the default) and &lt;code&gt;BackendType.PROCESS_GROUP&lt;/code&gt;. See &lt;a href=&quot;#rpc-backends&quot;&gt;Backends&lt;/a&gt; for more information.</source>
          <target state="translated">&lt;strong&gt;backend&lt;/strong&gt;（&lt;a href=&quot;#torch.distributed.rpc.BackendType&quot;&gt;BackendType &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;RPCバックエンド実装のタイプ。サポートされている値には、 &lt;code&gt;BackendType.TENSORPIPE&lt;/code&gt; （デフォルト）および &lt;code&gt;BackendType.PROCESS_GROUP&lt;/code&gt; が含まれます。詳細については、&lt;a href=&quot;#rpc-backends&quot;&gt;バックエンド&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="2deecff901759888bdb71ea34d20b18f5d55de46" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;backend&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;#torch.distributed.Backend&quot;&gt;Backend&lt;/a&gt;) &amp;ndash; The backend to use. Depending on build-time configurations, valid values include &lt;code&gt;mpi&lt;/code&gt;, &lt;code&gt;gloo&lt;/code&gt;, and &lt;code&gt;nccl&lt;/code&gt;. This field should be given as a lowercase string (e.g., &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;), which can also be accessed via &lt;a href=&quot;#torch.distributed.Backend&quot;&gt;&lt;code&gt;Backend&lt;/code&gt;&lt;/a&gt; attributes (e.g., &lt;code&gt;Backend.GLOO&lt;/code&gt;). If using multiple processes per machine with &lt;code&gt;nccl&lt;/code&gt; backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks.</source>
          <target state="translated">&lt;strong&gt;backend&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;#torch.distributed.Backend&quot;&gt;Backend&lt;/a&gt;）&amp;ndash;使用するバックエンド。ビルド時の構成に応じて、有効な値には &lt;code&gt;mpi&lt;/code&gt; 、 &lt;code&gt;gloo&lt;/code&gt; 、および &lt;code&gt;nccl&lt;/code&gt; が含まれます。このフィールドは小文字の文字列（ &lt;code&gt;&quot;gloo&quot;&lt;/code&gt; ）で指定する必要があります。この文字列には、&lt;a href=&quot;#torch.distributed.Backend&quot;&gt; &lt;code&gt;Backend&lt;/code&gt; &lt;/a&gt;属性（ &lt;code&gt;Backend.GLOO&lt;/code&gt; など）からもアクセスできます。 &lt;code&gt;nccl&lt;/code&gt; バックエンドを使用してマシンごとに複数のプロセスを使用する場合、プロセス間でGPUを共有するとデッドロックが発生する可能性があるため、各プロセスは使用するすべてのGPUに排他的にアクセスできる必要があります。</target>
        </trans-unit>
        <trans-unit id="611838a036cbfc666e9b5fee5ea917b77c462eae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;backend&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;#torch.distributed.Backend&quot;&gt;Backend&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The backend to use. Depending on build-time configurations, valid values are &lt;code&gt;gloo&lt;/code&gt; and &lt;code&gt;nccl&lt;/code&gt;. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;), which can also be accessed via &lt;a href=&quot;#torch.distributed.Backend&quot;&gt;&lt;code&gt;Backend&lt;/code&gt;&lt;/a&gt; attributes (e.g., &lt;code&gt;Backend.GLOO&lt;/code&gt;).</source>
          <target state="translated">&lt;strong&gt;バックエンド&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;#torch.distributed.Backend&quot;&gt;バックエンド&lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;使用するバックエンド。ビルド時の構成に応じて、有効な値は &lt;code&gt;gloo&lt;/code&gt; と &lt;code&gt;nccl&lt;/code&gt; です。デフォルトでは、グローバルグループと同じバックエンドを使用します。このフィールドは小文字の文字列（ &lt;code&gt;&quot;gloo&quot;&lt;/code&gt; ）で指定する必要があります。この文字列には、&lt;a href=&quot;#torch.distributed.Backend&quot;&gt; &lt;code&gt;Backend&lt;/code&gt; &lt;/a&gt;属性（ &lt;code&gt;Backend.GLOO&lt;/code&gt; など）からもアクセスできます。</target>
        </trans-unit>
        <trans-unit id="dd1b20b89d2cc0ec5cb752e68d4f7968fe15164e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;base&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; base of the logarithm function. Default: &lt;code&gt;10.0&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;base&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;対数関数の底。デフォルト： &lt;code&gt;10.0&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2d70c00dc7a0b2333944c557a49e48e57f71ef5e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch1&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first batch of matrices to be multiplied</source>
          <target state="translated">&lt;strong&gt;batch1&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;乗算される行列の最初のバッチ</target>
        </trans-unit>
        <trans-unit id="f6f89ac68899abcdeec75e50915476dc96eff70e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the second batch of matrices to be multiplied</source>
          <target state="translated">&lt;strong&gt;batch2&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;乗算される行列の2番目のバッチ</target>
        </trans-unit>
        <trans-unit id="7cf203097252d2e89f058a192c342938374bbb59" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, then the input and output tensors are provided as (batch, seq, feature). Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;batch_first&lt;/strong&gt; &amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、入力テンソルと出力テンソルは（batch、seq、feature）として提供されます。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c2bb4ed84deb95504a0fd7b72a8e70cbe91a0c79" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, then the input and output tensors are provided as &lt;code&gt;(batch, seq, feature)&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;batch_first&lt;/strong&gt; &amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、入力テンソルと出力テンソルは &lt;code&gt;(batch, seq, feature)&lt;/code&gt; として提供されます。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="bb58228cb0588bbf40e1aa70f7aa94c4d87966f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the input is expected in &lt;code&gt;B x T x *&lt;/code&gt; format.</source>
          <target state="translated">&lt;strong&gt;batch_first&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、入力は &lt;code&gt;B x T x *&lt;/code&gt; 形式で期待されます。</target>
        </trans-unit>
        <trans-unit id="97878838dd43a3b22c60de0a798485d3232a77f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the output will be in &lt;code&gt;B x T x *&lt;/code&gt; format.</source>
          <target state="translated">&lt;strong&gt;batch_first&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、出力は &lt;code&gt;B x T x *&lt;/code&gt; 形式になります。</target>
        </trans-unit>
        <trans-unit id="3ed5b9e169e5e04e47a2bda761131d2763152218" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;batch_first&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; output will be in &lt;code&gt;B x T x *&lt;/code&gt; if True, or in &lt;code&gt;T x B x *&lt;/code&gt; otherwise</source>
          <target state="translated">&lt;strong&gt;batch_first&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;出力はTrueの場合は &lt;code&gt;B x T x *&lt;/code&gt; 、それ以外の場合は &lt;code&gt;T x B x *&lt;/code&gt; になります</target>
        </trans-unit>
        <trans-unit id="4a9f6fb92330f139eb9547e65e6c72e563c7f9e2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; &amp;ndash; exponent. Default: 0.75</source>
          <target state="translated">&lt;strong&gt;ベータ&lt;/strong&gt;&amp;ndash;指数。デフォルト：0.75</target>
        </trans-unit>
        <trans-unit id="ef999fec40e863a728ff14d8247d251aee96fa89" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; &amp;ndash; the</source>
          <target state="translated">&lt;strong&gt;ベータ版&lt;/strong&gt;&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="0cf575cbd58c43c32309649550fcd836702cf2fe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies the threshold at which to change between L1 and L2 loss. This value defaults to 1.0.</source>
          <target state="translated">&lt;strong&gt;beta&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;L1損失とL2損失の間で変更するしきい値を指定します。この値のデフォルトは1.0です。</target>
        </trans-unit>
        <trans-unit id="3c471f0e5d8fd5619e33288b2fd6dc3e2c59c406" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The coefficient</source>
          <target state="translated">&lt;strong&gt;ベータ&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;フロート&lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;係数</target>
        </trans-unit>
        <trans-unit id="842828693822b4d9f2aa884edc9269cce4696288" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; shape parameter for the window.</source>
          <target state="translated">&lt;strong&gt;beta&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;ウィンドウの形状パラメーター。</target>
        </trans-unit>
        <trans-unit id="b11887536b01d5ee066bad51f6fc21a475d853cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for &lt;code&gt;input&lt;/code&gt; (</source>
          <target state="translated">&lt;strong&gt;ベータ&lt;/strong&gt;（&lt;em&gt;数値&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;input&lt;/code&gt; 乗数（</target>
        </trans-unit>
        <trans-unit id="fa1cf47ccd539344b4c2e84e2076311ee96e5b1f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;beta&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for &lt;code&gt;mat&lt;/code&gt; (</source>
          <target state="translated">&lt;strong&gt;ベータ&lt;/strong&gt;（&lt;em&gt;数値&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;mat&lt;/code&gt; 乗数（</target>
        </trans-unit>
        <trans-unit id="fc78c217f09fc1013e5acbefa8a13316ec9a1fca" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; &lt;strong&gt;non-quantized&lt;/strong&gt; bias tensor of shape</source>
          <target state="translated">&lt;strong&gt;バイアス&lt;/strong&gt;&amp;ndash;形状の&lt;strong&gt;非量子化&lt;/strong&gt;バイアステンソル</target>
        </trans-unit>
        <trans-unit id="5cf46533d930efcc3fd64418202381a80648d2fc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; If &lt;code&gt;False&lt;/code&gt;, then the layer does not use bias weights &lt;code&gt;b_ih&lt;/code&gt; and &lt;code&gt;b_hh&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;バイアス&lt;/strong&gt;&amp;ndash; &lt;code&gt;False&lt;/code&gt; の場合、レイヤーはバイアスの重み &lt;code&gt;b_ih&lt;/code&gt; と &lt;code&gt;b_hh&lt;/code&gt; を使用しません。デフォルト： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9a6cf6d24a3d1d4d2e99d86a09905c2dcc844ee4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; If set to &lt;code&gt;False&lt;/code&gt;, the layer will not learn an additive bias. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;バイアス&lt;/strong&gt;&amp;ndash; &lt;code&gt;False&lt;/code&gt; に設定されている場合、レイヤーは加法バイアスを学習しません。デフォルト： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9166de183aefb5a5420210ed060a3775a566bedc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; If set to False, the layer will not learn an additive bias. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;バイアス&lt;/strong&gt;&amp;ndash; Falseに設定されている場合、レイヤーは加法バイアスを学習しません。デフォルト： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="81d19585b9271c2697fbe2d0820cd2cd9a202b52" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; add bias as module parameter. Default: True.</source>
          <target state="translated">&lt;strong&gt;バイアス&lt;/strong&gt;&amp;ndash;モジュールパラメータとしてバイアスを追加します。デフォルト：True。</target>
        </trans-unit>
        <trans-unit id="f1d7f9d42640778760e245fa86bc8ea0c619a3a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; optional bias of shape</source>
          <target state="translated">&lt;strong&gt;バイアス&lt;/strong&gt;&amp;ndash;オプションの形状バイアス</target>
        </trans-unit>
        <trans-unit id="9e06c7b1cc26cfb4e243f63f4508fe3ad4aaecc3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; &amp;ndash; optional bias tensor of shape</source>
          <target state="translated">&lt;strong&gt;バイアス&lt;/strong&gt;&amp;ndash;形状のオプションのバイアステンソル</target>
        </trans-unit>
        <trans-unit id="e03a417e197c4f988306955bf8e28f75e1103c81" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, adds a learnable bias to the output. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;バイアス&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、学習可能なバイアスを出力に追加します。デフォルト： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c804fd0b8536d52a938e789eaf231e2d8488e976" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bias&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; None or fp32 bias of type &lt;code&gt;torch.float&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;バイアス&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;なしまたはtorch.floatタイプの &lt;code&gt;torch.float&lt;/code&gt; バイアス</target>
        </trans-unit>
        <trans-unit id="a33731aa2d3126a1f9307fbc74003f4b76cba728" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bidirectional&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, becomes a bidirectional GRU. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;双方向&lt;/strong&gt;&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、双方向GRUになります。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f4d0b0c0ab949b4fb4278979fa4bb9deb02a9fbe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bidirectional&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, becomes a bidirectional LSTM. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;双方向&lt;/strong&gt;&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、双方向LSTMになります。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d8771da0d807097f9c9e787cd58800e69ca9ba9e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bidirectional&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, becomes a bidirectional RNN. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;双方向&lt;/strong&gt;&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、双方向RNNになります。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7eb56a898db0c0dd4ebf0e179f5fd52a33dbbba3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bins&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of histogram bins</source>
          <target state="translated">&lt;strong&gt;bins&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;ヒストグラムビンの数</target>
        </trans-unit>
        <trans-unit id="6d0079f1170ed85a5972e6fbc1007549726ed554" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bins&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; One of {&amp;lsquo;tensorflow&amp;rsquo;,&amp;rsquo;auto&amp;rsquo;, &amp;lsquo;fd&amp;rsquo;, &amp;hellip;}. This determines how the bins are made. You can find other options in: &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html&quot;&gt;https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html&lt;/a&gt;</source>
          <target state="translated">&lt;strong&gt;ビン&lt;/strong&gt;（&lt;em&gt;文字列&lt;/em&gt;） -の一つ{ 'tensorflow'、 '自動車'、 'FD'、...}。これにより、ビンの作成方法が決まります。他のオプションは&lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html&quot;&gt;https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.htmlにあります。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="be7c2ee257bfd0c3170743cdce62c675328d09dd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;blank&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Blank label. Default</source>
          <target state="translated">&lt;strong&gt;空白&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;空白のラベル。デフォルト</target>
        </trans-unit>
        <trans-unit id="98d9784c554d3e6070ecb671487ffb5edf2addb7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;blank&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; blank label. Default</source>
          <target state="translated">&lt;strong&gt;空白&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;空白のラベル。デフォルト</target>
        </trans-unit>
        <trans-unit id="124b59c3f64e2d2878e908f98928989224419b13" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;boundaries&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1-D tensor, must contain a monotonically increasing sequence.</source>
          <target state="translated">&lt;strong&gt;境界&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash; 1次元テンソルは、単調に増加するシーケンスを含む必要があります。</target>
        </trans-unit>
        <trans-unit id="5da31c2835927c49a7bcd45bbe610fbd0fda6d6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;broadcast_buffers&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Flag that enables syncing (broadcasting) buffers of the module at beginning of the &lt;code&gt;forward&lt;/code&gt; function. (default: &lt;code&gt;True&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;broadcast_buffers&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;ブール値&lt;/a&gt;） -の初めにモジュールの（放送）のバッファを同期可能フラグ &lt;code&gt;forward&lt;/code&gt; 機能。（デフォルト： &lt;code&gt;True&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="3d5abee92111d698306a568507b4404bca34ef32" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;bucket_cap_mb&lt;/strong&gt; &amp;ndash; &lt;code&gt;DistributedDataParallel&lt;/code&gt; will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. &lt;code&gt;bucket_cap_mb&lt;/code&gt; controls the bucket size in MegaBytes (MB). (default: 25)</source>
          <target state="translated">&lt;strong&gt;Bucket_cap_mb&lt;/strong&gt; &amp;ndash; &lt;code&gt;DistributedDataParallel&lt;/code&gt; は、パラメーターを複数のバケットにバケット化するため、各バケットの勾配の削減が後方計算と重複する可能性があります。 &lt;code&gt;bucket_cap_mb&lt;/code&gt; は、バケットサイズをメガバイト（MB）で制御します。（デフォルト：25）</target>
        </trans-unit>
        <trans-unit id="2fa537aa1114daae5c6231d8a64de02ef3a1e4f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;build_directory&lt;/strong&gt; &amp;ndash; optional path to use as build workspace.</source>
          <target state="translated">&lt;strong&gt;build_directory&lt;/strong&gt; &amp;ndash;ビルドワークスペースとして使用するオプションのパス。</target>
        </trans-unit>
        <trans-unit id="c3141d5b804d7f3a2ff206cba71370ed4fb7cf6b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;c_0&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the initial cell state for each element in the batch.</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状の&lt;strong&gt;c_0 &lt;/strong&gt; &lt;code&gt;(batch, hidden_size)&lt;/code&gt; ：バッチ内の各要素の初期セル状態を含むテンソル。</target>
        </trans-unit>
        <trans-unit id="2cdfc2f84c394419e5070447dbe32bbe5a849b2c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;c_0&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the initial cell state for each element in the batch.</source>
          <target state="translated">&lt;strong&gt;C_0&lt;/strong&gt;形状の &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt; ：テンソルは、バッチ内の各素子の初期セル状態を含みます。</target>
        </trans-unit>
        <trans-unit id="f9b02af500b7cb6986cf95c49ee08bafe6ac6c55" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;c_1&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the next cell state for each element in the batch</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状の&lt;strong&gt;c_1 &lt;/strong&gt; &lt;code&gt;(batch, hidden_size)&lt;/code&gt; ：バッチ内の各要素の次のセル状態を含むテンソル</target>
        </trans-unit>
        <trans-unit id="bc8b348167534518488e0ae2031d0338daba2877" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;c_n&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the cell state for &lt;code&gt;t = seq_len&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状の&lt;strong&gt;c_n &lt;/strong&gt; &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt; ： &lt;code&gt;t = seq_len&lt;/code&gt; セル状態を含むテンソル。</target>
        </trans-unit>
        <trans-unit id="b3afc2463b07a6cb3c6c5f91f7a5a9121d9323c6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;callback&lt;/strong&gt; (&lt;code&gt;Callable&lt;/code&gt;) &amp;ndash; a &lt;code&gt;Callable&lt;/code&gt; that takes this &lt;code&gt;Future&lt;/code&gt; as the only argument.</source>
          <target state="translated">&lt;strong&gt;コールバック&lt;/strong&gt;（ &lt;code&gt;Callable&lt;/code&gt; ）&amp;ndash;この &lt;code&gt;Future&lt;/code&gt; を唯一の引数として取る &lt;code&gt;Callable&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a50ab6a8fd5d6ca7187e241b339e3ce04e06ad53" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window.</source>
          <target state="translated">&lt;strong&gt;ceil_mode&lt;/strong&gt; -もし &lt;code&gt;True&lt;/code&gt; が、使用されます &lt;code&gt;ceil&lt;/code&gt; するのではなく &lt;code&gt;floor&lt;/code&gt; 出力形状を計算します。これにより、入力テンソルのすべての要素がスライディングウィンドウで覆われるようになります。</target>
        </trans-unit>
        <trans-unit id="c8f8c031ecdef7a290512f2d2c0f3fb9843f10ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; when True, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; in the formula to compute the output shape</source>
          <target state="translated">&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; Trueの場合、数式で &lt;code&gt;floor&lt;/code&gt; の代わりに &lt;code&gt;ceil&lt;/code&gt; を使用して出力形状を計算します</target>
        </trans-unit>
        <trans-unit id="4607006703c79ed95d46d4bdf76b0c53e13b775b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; when True, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; in the formula to compute the output shape. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; Trueの場合、数式で &lt;code&gt;floor&lt;/code&gt; の代わりに &lt;code&gt;ceil&lt;/code&gt; を使用して出力形状を計算します。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="11fbe0576979d5cf6a08896bdf3211baf5a56d94" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; when True, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; to compute the output shape</source>
          <target state="translated">&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; Trueの場合、 &lt;code&gt;floor&lt;/code&gt; の代わりに &lt;code&gt;ceil&lt;/code&gt; を使用して出力形状を計算します</target>
        </trans-unit>
        <trans-unit id="b293b825d6cac1646ea030a31f09a3ccbb476cad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; when True, will use &lt;code&gt;ceil&lt;/code&gt; instead of &lt;code&gt;floor&lt;/code&gt; to compute the output shape. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;ceil_mode&lt;/strong&gt; &amp;ndash; Trueの場合、 &lt;code&gt;floor&lt;/code&gt; の代わりに &lt;code&gt;ceil&lt;/code&gt; を使用して出力形状を計算します。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="39ee4edbfa46d12a64c7dc2b9fa7acda768a096e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;center&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether &lt;code&gt;input&lt;/code&gt; was padded on both sides so that the</source>
          <target state="translated">&lt;strong&gt;center&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash; &lt;code&gt;input&lt;/code&gt; が両側に埋め込まれているかどうか</target>
        </trans-unit>
        <trans-unit id="554e667848e98458fc2cceae44866e4bc318c449" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;center&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if True, center the input tensor, otherwise, assume that the input is centered.</source>
          <target state="translated">&lt;strong&gt;center&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; Trueの場合、入力テンソルを中央に配置します。それ以外の場合は、入力が中央に配置されていると見なします。</target>
        </trans-unit>
        <trans-unit id="f9f7299f6837ef46cae44a83afb72d7f43c0f17b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;center&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to pad &lt;code&gt;input&lt;/code&gt; on both sides so that the</source>
          <target state="translated">&lt;strong&gt;center&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;両側に &lt;code&gt;input&lt;/code&gt; を埋めて、</target>
        </trans-unit>
        <trans-unit id="901e29d2efd439abae8b39739b45f91fde3c2e1f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_hash&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If True, the filename part of the URL should follow the naming convention &lt;code&gt;filename-&amp;lt;sha256&amp;gt;.ext&lt;/code&gt; where &lt;code&gt;&amp;lt;sha256&amp;gt;&lt;/code&gt; is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False</source>
          <target state="translated">&lt;strong&gt;check_hash&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash; Trueの場合、URLのファイル名部分は命名規則 &lt;code&gt;filename-&amp;lt;sha256&amp;gt;.ext&lt;/code&gt; に従う必要があります。ここで、 &lt;code&gt;&amp;lt;sha256&amp;gt;&lt;/code&gt; はファイルの内容のSHA256ハッシュの最初の8桁以上です。ハッシュは、一意の名前を確認し、ファイルの内容を確認するために使用されます。デフォルト：False</target>
        </trans-unit>
        <trans-unit id="ef5504f62a92974b3ed5764b39be00b40da7ec96" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_inputs&lt;/strong&gt; (&lt;em&gt;list of dicts&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A list of dicts of input arguments that should be used to check the trace against what is expected. Each tuple is equivalent to a set of input arguments that would be specified in &lt;code&gt;inputs&lt;/code&gt;. For best results, pass in a set of checking inputs representative of the space of shapes and types of inputs you expect the network to see. If not specified, the original &lt;code&gt;inputs&lt;/code&gt; are used for checking</source>
          <target state="translated">&lt;strong&gt;check_inputs&lt;/strong&gt;（&lt;strong&gt;dictの&lt;/strong&gt;&lt;em&gt;リスト&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;トレースを期待されるものと照合するために使用する必要がある入力引数の&lt;strong&gt;dictの&lt;/strong&gt;&lt;em&gt;リスト&lt;/em&gt;。各タプルは、 &lt;code&gt;inputs&lt;/code&gt; で指定される入力引数のセットと同等です。最良の結果を得るには、ネットワークに表示されると予想される形状の空間と入力のタイプを表す一連のチェック入力を渡します。指定しない場合、元の &lt;code&gt;inputs&lt;/code&gt; がチェックに使用されます</target>
        </trans-unit>
        <trans-unit id="c341222b235229269c7fc1f716c9a876b11bd749" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_inputs&lt;/strong&gt; (&lt;em&gt;list of tuples&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A list of tuples of input arguments that should be used to check the trace against what is expected. Each tuple is equivalent to a set of input arguments that would be specified in &lt;code&gt;example_inputs&lt;/code&gt;. For best results, pass in a set of checking inputs representative of the space of shapes and types of inputs you expect the network to see. If not specified, the original &lt;code&gt;example_inputs&lt;/code&gt; are used for checking</source>
          <target state="translated">&lt;strong&gt;check_inputs&lt;/strong&gt;（&lt;em&gt;タプルのリスト&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;トレースを期待されるものと照合するために使用する必要がある入力引数のタプルのリスト。各タプルは、 &lt;code&gt;example_inputs&lt;/code&gt; で指定される入力引数のセットと同等です。最良の結果を得るには、ネットワークに表示されると予想される形状の空間と入力のタイプを表す一連のチェック入力を渡します。指定しない場合、元の &lt;code&gt;example_inputs&lt;/code&gt; がチェックに使用されます</target>
        </trans-unit>
        <trans-unit id="5be047e7257e74bb986cba2cbddd77287afb54a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_reduction&lt;/strong&gt; &amp;ndash; This argument is deprecated.</source>
          <target state="translated">&lt;strong&gt;check_reduction&lt;/strong&gt; &amp;ndash;この引数は非推奨です。</target>
        </trans-unit>
        <trans-unit id="5b901bf06f9dd61829b7a11e137cee91046aa7d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_tolerance&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Floating-point comparison tolerance to use in the checker procedure. This can be used to relax the checker strictness in the event that results diverge numerically for a known reason, such as operator fusion.</source>
          <target state="translated">&lt;strong&gt;check_tolerance&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;チェッカープロシージャで使用する浮動小数点比較許容値。これは、演算子の融合などの既知の理由で結果が数値的に発散する場合に、チェッカーの厳密さを緩和するために使用できます。</target>
        </trans-unit>
        <trans-unit id="7d4b51194efc0c3988804d99693826828fb517f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;check_trace&lt;/strong&gt; (&lt;code&gt;bool&lt;/code&gt;, optional) &amp;ndash; Check if the same inputs run through traced code produce the same outputs. Default: &lt;code&gt;True&lt;/code&gt;. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.</source>
          <target state="translated">&lt;strong&gt;check_trace&lt;/strong&gt;（ &lt;code&gt;bool&lt;/code&gt; 、オプション）&amp;ndash;トレースされたコードを介して実行される同じ入力が同じ出力を生成するかどうかを確認します。デフォルト： &lt;code&gt;True&lt;/code&gt; 。たとえば、ネットワークに非決定論的な操作が含まれている場合や、チェッカーに障害が発生してもネットワークが正しいことが確実な場合は、これを無効にすることをお勧めします。</target>
        </trans-unit>
        <trans-unit id="9b22f20f0f0764fbf8ab51098534c41733587e08" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;chunks&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of chunks to return</source>
          <target state="translated">&lt;strong&gt;チャンク&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;返すチャンクの数</target>
        </trans-unit>
        <trans-unit id="768622c9f916e93f3f54009c2cae67ec11e32f7c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;clip_value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; maximum allowed value of the gradients. The gradients are clipped in the range</source>
          <target state="translated">&lt;strong&gt;clip_value&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;グラデーションの最大許容値。グラデーションは範囲内でクリップされます</target>
        </trans-unit>
        <trans-unit id="469c227d8924eacf04bdee3cc8cf8084affddf1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;close&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Flag to automatically close the figure</source>
          <target state="translated">&lt;strong&gt;close&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;図を自動的に閉じるフラグ</target>
        </trans-unit>
        <trans-unit id="cf87633936d5aa46fae578ecedd6b3692d516d64" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;col&lt;/strong&gt; (&lt;code&gt;int&lt;/code&gt;) &amp;ndash; number of columns in the 2-D matrix.</source>
          <target state="translated">&lt;strong&gt;col&lt;/strong&gt;（ &lt;code&gt;int&lt;/code&gt; ）&amp;ndash;2次元行列の列数。</target>
        </trans-unit>
        <trans-unit id="a942bb7a89546ea8db2e80c5772f25d6c0dc388d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;colors&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Colors for each vertex</source>
          <target state="translated">&lt;strong&gt;色&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;各頂点の色</target>
        </trans-unit>
        <trans-unit id="af25ed4733fad11d3e062d79140e6b8c23a8a955" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;comment&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Comment log_dir suffix appended to the default &lt;code&gt;log_dir&lt;/code&gt;. If &lt;code&gt;log_dir&lt;/code&gt; is assigned, this argument has no effect.</source>
          <target state="translated">&lt;strong&gt;コメント&lt;/strong&gt;（&lt;em&gt;文字列&lt;/em&gt;）&amp;ndash;デフォルトの &lt;code&gt;log_dir&lt;/code&gt; に追加されるコメントlog_dirサフィックス。 &lt;code&gt;log_dir&lt;/code&gt; が割り当てられている場合、この引数は効果がありません。</target>
        </trans-unit>
        <trans-unit id="88100fd7e4f3c66560dd9b530a7164b8a95efdd5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;compiler&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The compiler executable name to check (e.g. &lt;code&gt;g++&lt;/code&gt;). Must be executable in a shell process.</source>
          <target state="translated">&lt;strong&gt;コンパイラ&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;チェックするコンパイラの実行可能ファイル名（ &lt;code&gt;g++&lt;/code&gt; ）。シェルプロセスで実行可能である必要があります。</target>
        </trans-unit>
        <trans-unit id="1808bd911df345d583c080a56c28591067ce66ed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;compute_mode&lt;/strong&gt; &amp;ndash; &amp;lsquo;use_mm_for_euclid_dist_if_necessary&amp;rsquo; - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P &amp;gt; 25 or R &amp;gt; 25 &amp;lsquo;use_mm_for_euclid_dist&amp;rsquo; - will always use matrix multiplication approach to calculate euclidean distance (p = 2) &amp;lsquo;donot_use_mm_for_euclid_dist&amp;rsquo; - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.</source>
          <target state="translated">&lt;strong&gt;compute_mode&lt;/strong&gt; &amp;ndash;'use_mm_for_euclid_dist_if_necessary'-P&amp;gt; 25またはR&amp;gt; 25の場合、行列乗算アプローチを使用してユークリッド距離（p = 2）を計算します 'use_mm_for_euclid_dist'-常に行列乗算アプローチを使用してユークリッド距離（p = 2）を計算します' donot_use_mm_for_euclid -ユークリッド距離の計算に行列乗算アプローチを使用することはありません（p = 2）デフォルト：use_mm_for_euclid_dist_if_necessary。</target>
        </trans-unit>
        <trans-unit id="0b7321414a4a3b3cf3f67b84bd0c573e753c0ef0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;compute_uv&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; option whether to compute &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; or not</source>
          <target state="translated">&lt;strong&gt;compute_uv&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;BOOL &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;） -オプションを計算するかどうかを &lt;code&gt;U&lt;/code&gt; 及び &lt;code&gt;V&lt;/code&gt; か否かを</target>
        </trans-unit>
        <trans-unit id="a0d3ed0a4c0c3905d8f35b9489a4566e61f37346" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;condition&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.BoolTensor&quot;&gt;BoolTensor&lt;/a&gt;) &amp;ndash; When True (nonzero), yield x, otherwise yield y</source>
          <target state="translated">&lt;strong&gt;condition&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.BoolTensor&quot;&gt;BoolTensor&lt;/a&gt;）&amp;ndash; True（ゼロ以外）の場合はxを生成し、それ以外の場合はyを生成します。</target>
        </trans-unit>
        <trans-unit id="fb57365756ae0cedf43c6c254c4afd0d2edee70b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;config_dict&lt;/strong&gt; &amp;ndash; Dictionary with ThreeJS classes names and configuration.</source>
          <target state="translated">&lt;strong&gt;config_dict&lt;/strong&gt; &amp;ndash;ThreeJSクラスの名前と構成を含む辞書。</target>
        </trans-unit>
        <trans-unit id="c4a1732a0f202d34ba93b1eb562af5be27bdd650" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;context_id&lt;/strong&gt; &amp;ndash; the autograd context id for which we should run the optimizer step.</source>
          <target state="translated">&lt;strong&gt;context_id&lt;/strong&gt; &amp;ndash;オプティマイザーステップを実行する必要があるautogradコンテキストID。</target>
        </trans-unit>
        <trans-unit id="4d5613a1e6301637797b5ae54f8043e61724838c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;context_id&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The autograd context id for which we should retrieve the gradients.</source>
          <target state="translated">&lt;strong&gt;context_id&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;グラデーションを取得する必要があるautogradコンテキストID。</target>
        </trans-unit>
        <trans-unit id="7c83c6134b0a9c7b9806c160fd52a9c3b5ea1fc9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;count_include_pad&lt;/strong&gt; &amp;ndash; when True, will include the zero-padding in the averaging calculation</source>
          <target state="translated">&lt;strong&gt;count_include_pad&lt;/strong&gt; &amp;ndash; Trueの場合、平均化計算にゼロパディングが含まれます</target>
        </trans-unit>
        <trans-unit id="df61d9b0480a95d8ea4a65fcfd105e2c43cf5683" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;count_include_pad&lt;/strong&gt; &amp;ndash; when True, will include the zero-padding in the averaging calculation. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;count_include_pad&lt;/strong&gt; &amp;ndash; Trueの場合、平均化計算にゼロパディングが含まれます。デフォルト： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f406dbb58fb0db000240174c73ab3118ad8b663f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;counts&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): (optional) if &lt;code&gt;return_counts&lt;/code&gt; is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.</source>
          <target state="translated">&lt;strong&gt;counts&lt;/strong&gt;（&lt;em&gt;Tensor&lt;/em&gt;）:(オプション） &lt;code&gt;return_counts&lt;/code&gt; がTrueの場合、一意の値またはテンソルごとの出現回数を表す追加の返されるテンソル（dimが指定されている場合はoutputまたはoutput.size（dim）と同じ形状）が返されます。</target>
        </trans-unit>
        <trans-unit id="88193fd13f9166e9983ecc78ecd953269b1f995e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cpp_sources&lt;/strong&gt; &amp;ndash; A string, or list of strings, containing C++ source code.</source>
          <target state="translated">&lt;strong&gt;cpp_sources&lt;/strong&gt; &amp;ndash; C ++ソースコードを含む文字列または文字列のリスト。</target>
        </trans-unit>
        <trans-unit id="1df45c030ada5dacf28732656dae67b769b52be4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;create_graph&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;create_graph&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、導関数のグラフが作成され、高階導関数の積を計算できます。デフォルトは &lt;code&gt;False&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="ff1cc36eddc3b5f962bb282dc8359cf8c6757bb3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cuda&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, includes CUDA-specific include paths.</source>
          <target state="translated">&lt;strong&gt;cuda&lt;/strong&gt; &amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、CUDA固有のインクルードパスを含みます。</target>
        </trans-unit>
        <trans-unit id="9cc28cf38c3a669932c9eedb3625ab6b9d7054e3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cuda_sources&lt;/strong&gt; &amp;ndash; A string, or list of strings, containing CUDA source code.</source>
          <target state="translated">&lt;strong&gt;cuda_sources &amp;ndash;CUDA&lt;/strong&gt;ソースコードを含む文字列または文字列のリスト。</target>
        </trans-unit>
        <trans-unit id="e54fa7d57ec695fdbf3ba8aa44c0d4c97f847f91" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;custom_decoder&lt;/strong&gt; &amp;ndash; custom decoder (default=None).</source>
          <target state="translated">&lt;strong&gt;custom_decoder&lt;/strong&gt; &amp;ndash;カスタムデコーダー（デフォルト=なし）。</target>
        </trans-unit>
        <trans-unit id="2ca74733e312c67f2657fe95d1e444ee05127650" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;custom_encoder&lt;/strong&gt; &amp;ndash; custom encoder (default=None).</source>
          <target state="translated">&lt;strong&gt;custom_encoder&lt;/strong&gt; &amp;ndash;カスタムエンコーダー（デフォルト=なし）。</target>
        </trans-unit>
        <trans-unit id="7780ca36c95ae6da024d6000ca0554d9dfacacc3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;custom_opsets&lt;/strong&gt; (&lt;em&gt;dict&amp;lt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;int&amp;gt;&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default empty dict&lt;/em&gt;) &amp;ndash; A dictionary to indicate custom opset domain and version at export. If model contains a custom opset, it is optional to specify the domain and opset version in the dictionary: - KEY: opset domain name - VALUE: opset version If the custom opset is not provided in this dictionary, opset version is set to 1 by default.</source>
          <target state="translated">&lt;strong&gt;custom_opsets&lt;/strong&gt;（&lt;em&gt;dict &amp;lt;string &lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;int&amp;gt; &lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;default empty dict&lt;/em&gt;）&amp;ndash;エクスポート時のカスタムopsetドメインとバージョンを示す辞書。モデルにカスタムオプセットが含まれている場合、ディクショナリでドメインとオプセットバージョンを指定することはオプションです。-キー：オプセットドメイン名-値：オプセットバージョンカスタムオプセットがこのディクショナリで提供されていない場合、オプセットバージョンはによって1に設定されます。デフォルト。</target>
        </trans-unit>
        <trans-unit id="9101a0b9856e2d4350718d233a9dcae3bf9c6f6c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;cutoffs&lt;/strong&gt; (&lt;em&gt;Sequence&lt;/em&gt;) &amp;ndash; Cutoffs used to assign targets to their buckets</source>
          <target state="translated">&lt;strong&gt;カットオフ&lt;/strong&gt;（&lt;em&gt;シーケンス&lt;/em&gt;）&amp;ndash;ターゲットをバケットに割り当てるために使用される&lt;strong&gt;カットオフ&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="52d9d339631301adfaf686280dfd334d7edccba5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;) &amp;ndash; the floating point dtype to make the default</source>
          <target state="translated">&lt;strong&gt;d&lt;/strong&gt;（&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;）&amp;ndash;デフォルトにする浮動小数点dtype</target>
        </trans-unit>
        <trans-unit id="13964b8d48f242ad146568039b158f92e2df74ae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/a&gt;) &amp;ndash; If True, force operations to be deterministic. If False, allow non-deterministic operations.</source>
          <target state="translated">&lt;strong&gt;d&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt; &lt;code&gt;bool&lt;/code&gt; &lt;/a&gt;）&amp;ndash; Trueの場合、操作を強制的に決定論的にします。Falseの場合、非決定論的操作を許可します。</target>
        </trans-unit>
        <trans-unit id="3da5f1488cc8bd47b1fccb5b95f3073259325116" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; path to a local folder to save downloaded models &amp;amp; weights.</source>
          <target state="translated">&lt;strong&gt;d&lt;/strong&gt;（&lt;em&gt;文字列&lt;/em&gt;）&amp;ndash;ダウンロードしたモデルと重みを保存するためのローカルフォルダーへのパス。</target>
        </trans-unit>
        <trans-unit id="16b912c0275c25fd63373867599edc3ab633da37" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d_model&lt;/strong&gt; &amp;ndash; the number of expected features in the encoder/decoder inputs (default=512).</source>
          <target state="translated">&lt;strong&gt;d_model&lt;/strong&gt; &amp;ndash;エンコーダー/デコーダー入力で期待される機能の数（デフォルト= 512）。</target>
        </trans-unit>
        <trans-unit id="15d1f07b9f1bfeb8b51357949749ed27cc1e3a54" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;d_model&lt;/strong&gt; &amp;ndash; the number of expected features in the input (required).</source>
          <target state="translated">&lt;strong&gt;d_model&lt;/strong&gt; &amp;ndash;入力で予想される機能の数（必須）。</target>
        </trans-unit>
        <trans-unit id="1751a1c65282c1b2a3f97ccf6353f71f8348524a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;data&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; parameter tensor.</source>
          <target state="translated">&lt;strong&gt;データ&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;パラメーターテンソル。</target>
        </trans-unit>
        <trans-unit id="002b70001aea1b9d2d370c56a295bc0004d40317" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;data&lt;/strong&gt; (&lt;em&gt;array_like&lt;/em&gt;) &amp;ndash; Initial data for the tensor. Can be a list, tuple, NumPy &lt;code&gt;ndarray&lt;/code&gt;, scalar, and other types.</source>
          <target state="translated">&lt;strong&gt;data&lt;/strong&gt;（&lt;em&gt;array_like&lt;/em&gt;）&amp;ndash;テンソルの初期データ。リスト、タプル、NumPy &lt;code&gt;ndarray&lt;/code&gt; 、スカラー、およびその他のタイプにすることができます。</target>
        </trans-unit>
        <trans-unit id="057f28def548d5eeb6eab40e11d3fc6379b98957" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;data&lt;/strong&gt; (&lt;em&gt;array_like&lt;/em&gt;) &amp;ndash; The returned Tensor copies &lt;code&gt;data&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;data&lt;/strong&gt;（&lt;em&gt;array_like&lt;/em&gt;）&amp;ndash;返されたTensorは &lt;code&gt;data&lt;/code&gt; コピーします。</target>
        </trans-unit>
        <trans-unit id="f6cb5164bb06d6da92431aba395fa788fafdd491" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dataformats&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc.</source>
          <target state="translated">&lt;strong&gt;dataformats&lt;/strong&gt;（&lt;em&gt;文字列&lt;/em&gt;）&amp;ndash; NCHW、NHWC、CHW、HWC、HW、WHなどの形式の画像データ形式の仕様。</target>
        </trans-unit>
        <trans-unit id="a8a0c46d88dac2ffef33068f3be232119d9b9ac2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;decoder_layer&lt;/strong&gt; &amp;ndash; an instance of the TransformerDecoderLayer() class (required).</source>
          <target state="translated">&lt;strong&gt;decode_layer&lt;/strong&gt; &amp;ndash; TransformerDecoderLayer（）クラスのインスタンス（必須）。</target>
        </trans-unit>
        <trans-unit id="6debf1cc5a394619a93016d7ca366d8f9a4f042f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;default_mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Base mask from previous pruning</source>
          <target state="translated">&lt;strong&gt;default_mask&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;前の剪定からのベースマスク</target>
        </trans-unit>
        <trans-unit id="02df0d48c6b8036586e726b50a5fbc9b6e1751a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;default_mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as &lt;code&gt;t&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;default_mask&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;以前のプルーニング反復からのベースマスク。新しいマスクが適用された後に尊重する必要があります。 &lt;code&gt;t&lt;/code&gt; と同じ薄暗くなります。</target>
        </trans-unit>
        <trans-unit id="5cf70b407f88539d3c6c856333c142253f1a21c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;default_mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; mask from previous pruning iteration.</source>
          <target state="translated">&lt;strong&gt;default_mask&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;前のプルーニング反復からのマスク。</target>
        </trans-unit>
        <trans-unit id="25982b74713391159075b7c31f6a1999d645d6fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;default_mask&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</source>
          <target state="translated">&lt;strong&gt;default_mask&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;前のプルーニング反復からのマスク（存在&lt;em&gt;する&lt;/em&gt;&lt;em&gt;場合&lt;/em&gt;）。剪定が作用するテンソルの部分を決定するときに考慮されます。Noneの場合、デフォルトで1のマスクになります。</target>
        </trans-unit>
        <trans-unit id="d2c8995ad4fd81b344f11dc7dbecd0ebd8ded8d5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;descending&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls the sorting order (ascending or descending)</source>
          <target state="translated">&lt;strong&gt;降順&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;並べ替え順序（昇順または降順）を制御します</target>
        </trans-unit>
        <trans-unit id="de9594f438e4acefd09174ebe985fcf079eb149b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;destination&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; Destination positions for each of the original dims. These must also be unique.</source>
          <target state="translated">&lt;strong&gt;destination&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;em&gt;python：intsのタプル&lt;/em&gt;）&amp;ndash;元の各dimの宛先位置。これらも一意である必要があります。</target>
        </trans-unit>
        <trans-unit id="7b43c4caf374bfe5b5468766c5d19925926c2fdd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;deterministic&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; flag to choose between a faster non-deterministic calculation, or a slower deterministic calculation. This argument is only available for sparse-dense CUDA bmm. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;決定論的&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;より高速な非決定論的計算またはより低速な決定論的計算のいずれかを選択するためのフラグ。この引数は、疎密なCUDAbmmでのみ使用できます。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="67ab079c226c54c3f26e4ea0b6f9a1108b087a49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device for the generator.</source>
          <target state="translated">&lt;strong&gt;デバイス&lt;/strong&gt;（&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;、オプション）&amp;ndash;ジェネレーターに必要なデバイス。</target>
        </trans-unit>
        <trans-unit id="7a7f07799c9cad3af3cab4816607f53086fcc521" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, defaults to the device of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;デバイス&lt;/strong&gt;（&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;、オプション）&amp;ndash;返されるテンソルの目的のデバイス。デフォルト： &lt;code&gt;None&lt;/code&gt; の場合、デフォルトで &lt;code&gt;input&lt;/code&gt; のデバイスになります。</target>
        </trans-unit>
        <trans-unit id="5aed780ae1f47b9761fbfa7992f19ff1ad4f3e5e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, uses the current device for the default tensor type (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;). &lt;code&gt;device&lt;/code&gt; will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</source>
          <target state="translated">&lt;strong&gt;デバイス&lt;/strong&gt;（&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;、オプション）&amp;ndash;返されるテンソルの目的のデバイス。デフォルト： &lt;code&gt;None&lt;/code&gt; の場合、デフォルトのテンソルタイプに現在のデバイスを使用します（&lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt; &lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt; を&lt;/a&gt;参照）。 &lt;code&gt;device&lt;/code&gt; は、CPUテンソルタイプの場合はCPUになり、CUDAテンソルタイプの場合は現在のCUDAデバイスになります。</target>
        </trans-unit>
        <trans-unit id="0f17f4ea64a618a5f0fee4c7ea43e579c17cf49f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;). &lt;code&gt;device&lt;/code&gt; will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</source>
          <target state="translated">&lt;strong&gt;デバイス&lt;/strong&gt;（&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;、オプション）&amp;ndash;返されるテンソルの目的のデバイス。デフォルト：Noneの場合、デフォルトのテンソルタイプに現在のデバイスを使用します（&lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt; &lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt; を&lt;/a&gt;参照）。 &lt;code&gt;device&lt;/code&gt; は、CPUテンソルタイプの場合はCPUになり、CUDAテンソルタイプの場合は現在のCUDAデバイスになります。</target>
        </trans-unit>
        <trans-unit id="cf93823dec63265c654765453f246b1d85c3b014" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The destination GPU id. Defaults to the current device.</source>
          <target state="translated">&lt;strong&gt;device&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;宛先GPUID。デフォルトは現在のデバイスです。</target>
        </trans-unit>
        <trans-unit id="2d93e973425e80658411f9bf156583443e90dc63" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if specified, all parameters will be copied to that device</source>
          <target state="translated">&lt;strong&gt;device&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;指定されている場合、すべてのパラメーターがそのデバイスにコピーされます</target>
        </trans-unit>
        <trans-unit id="a198ab22b8b6fb2b268e0d3eed8f405f9fdfeffb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;) &amp;ndash; The destination GPU device. Defaults to the current CUDA device.</source>
          <target state="translated">&lt;strong&gt;device&lt;/strong&gt;（&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;）&amp;ndash;宛先GPUデバイス。デフォルトは現在のCUDAデバイスです。</target>
        </trans-unit>
        <trans-unit id="f538c82c744a54f89979310b11620a36e4dfdc67" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired device of returned tensor. Default: if None, same &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">&lt;strong&gt;デバイス&lt;/strong&gt;（&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;、オプション）&amp;ndash;返されるテンソルの目的のデバイス。デフォルト：Noneの場合、このテンソルと同じ&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="16a3ba684b70277c97f3887a32acb68677e89ab5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device&lt;/strong&gt; (&lt;code&gt;torch.device&lt;/code&gt;) &amp;ndash; the desired device of the parameters and buffers in this module</source>
          <target state="translated">&lt;strong&gt;device&lt;/strong&gt;（ &lt;code&gt;torch.device&lt;/code&gt; ）&amp;ndash;このモジュールのパラメーターとバッファーの目的のデバイス</target>
        </trans-unit>
        <trans-unit id="c13cbea5fe853ad61c4e5b6bdc300c90af893ea8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device_ids&lt;/strong&gt; (&lt;em&gt;list of python:int&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; CUDA devices (default: all devices)</source>
          <target state="translated">&lt;strong&gt;device_ids&lt;/strong&gt;（&lt;em&gt;python：int&lt;/em&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.deviceのリスト&lt;/a&gt;）&amp;ndash; CUDAデバイス（デフォルト：すべてのデバイス）</target>
        </trans-unit>
        <trans-unit id="784500a88b749be8a97a2a8383bb392bde957899" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device_ids&lt;/strong&gt; (&lt;em&gt;list of python:int&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; CUDA devices. This should only be provided when the input module resides on a single CUDA device. For single-device modules, the i&amp;rsquo;th &lt;code&gt;module&lt;/code&gt; replica is placed on &lt;code&gt;device_ids[i]&lt;/code&gt;. For multi-device modules and CPU modules, &lt;code&gt;device_ids&lt;/code&gt; must be &lt;code&gt;None&lt;/code&gt; or an empty list, and input data for the forward pass must be placed on the correct device. (default: all visible devices for single-device modules)</source>
          <target state="translated">&lt;strong&gt;device_ids&lt;/strong&gt;（&lt;em&gt;python：int&lt;/em&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.deviceのリスト&lt;/a&gt;）&amp;ndash;CUDAデバイス。これは、入力モジュールが単一のCUDAデバイスに存在する場合にのみ提供する必要があります。単一デバイスモジュールの場合、i番目の &lt;code&gt;module&lt;/code&gt; レプリカは &lt;code&gt;device_ids[i]&lt;/code&gt; 配置されます。マルチデバイスモジュールおよびCPUモジュールの場合、 &lt;code&gt;device_ids&lt;/code&gt; は &lt;code&gt;None&lt;/code&gt; または空のリストである必要があり、フォワードパスの入力データは正しいデバイスに配置する必要があります。（デフォルト：単一デバイスモジュールのすべての表示可能なデバイス）</target>
        </trans-unit>
        <trans-unit id="0098733514e18323929fd4ffb49500d3946fa6b3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;device_ids&lt;/strong&gt; (&lt;em&gt;list of python:int&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; GPU ids on which to replicate module</source>
          <target state="translated">&lt;strong&gt;device_ids&lt;/strong&gt;（&lt;em&gt;python：int&lt;/em&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.deviceのリスト&lt;/a&gt;）&amp;ndash;モジュールを複製するGPU ID</target>
        </trans-unit>
        <trans-unit id="59e24ff6691679f07576485b92e551a387011363" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;diagonal&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the diagonal to consider</source>
          <target state="translated">&lt;strong&gt;対角線&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;考慮すべき対角線</target>
        </trans-unit>
        <trans-unit id="98d2682d358cdef81c41f9d22f56660c98f0fea5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; The stride between elements within a sliding window, must be &amp;gt; 0.</source>
          <target state="translated">&lt;strong&gt;拡張&lt;/strong&gt;&amp;ndash;スライディングウィンドウ内の要素間のストライドは&amp;gt; 0でなければなりません。</target>
        </trans-unit>
        <trans-unit id="0a8bcc8e1ce88b2cf0668486e8d5ccbcb8a4d458" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; a parameter that controls the stride of elements in the window</source>
          <target state="translated">&lt;strong&gt;膨張&lt;/strong&gt;&amp;ndash;ウィンドウ内の要素のストライドを制御するパラメーター</target>
        </trans-unit>
        <trans-unit id="a8745aafd934cfa2f8a27f7eeb58f0f04b1c8c6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a one-element tuple &lt;code&gt;(dW,)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;拡張&lt;/strong&gt;&amp;ndash;カーネル要素間の間隔。単一の数値または1つの要素のタプル &lt;code&gt;(dW,)&lt;/code&gt; にすることができます。デフォルト：1</target>
        </trans-unit>
        <trans-unit id="e4f0efe675d23b1d986e23b22d467d33af93c1ce" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a tuple &lt;code&gt;(dD, dH, dW)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;拡張&lt;/strong&gt;&amp;ndash;カーネル要素間の間隔。単一の数値またはタプル &lt;code&gt;(dD, dH, dW)&lt;/code&gt; にすることができます。デフォルト：1</target>
        </trans-unit>
        <trans-unit id="5cc549246c32f7e86fe8237813b71d731de4aeab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a tuple &lt;code&gt;(dH, dW)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;拡張&lt;/strong&gt;&amp;ndash;カーネル要素間の間隔。単一の数値またはタプル &lt;code&gt;(dH, dW)&lt;/code&gt; することができます。デフォルト：1</target>
        </trans-unit>
        <trans-unit id="717cd865c4cf98b3ffbd6b2307f6f4dbaebad0c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a tuple &lt;code&gt;(dT, dH, dW)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;拡張&lt;/strong&gt;&amp;ndash;カーネル要素間の間隔。単一の数値またはタプル &lt;code&gt;(dT, dH, dW)&lt;/code&gt; にすることができます。デフォルト：1</target>
        </trans-unit>
        <trans-unit id="9ca358d114492e5781f3c8bd655869b64df6739b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; &amp;ndash; the spacing between kernel elements. Can be a single number or a tuple &lt;code&gt;(dW,)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;拡張&lt;/strong&gt;&amp;ndash;カーネル要素間の間隔。単一の数値またはタプル &lt;code&gt;(dW,)&lt;/code&gt; することができます。デフォルト：1</target>
        </trans-unit>
        <trans-unit id="b6d5af4910c01ee195d562c6b5a6bb2027740a67" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Spacing between kernel elements. Default: 1</source>
          <target state="translated">&lt;strong&gt;dilation&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;カーネル要素間の間隔。デフォルト：1</target>
        </trans-unit>
        <trans-unit id="abdfdeace9aacba2e41cd643c5ea0e49e917e957" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dilation&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a parameter that controls the stride of elements within the neighborhood. Default: 1</source>
          <target state="translated">&lt;strong&gt;dilation&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;近傍内の要素のストライドを制御するパラメーター。デフォルト：1</target>
        </trans-unit>
        <trans-unit id="472af6ce02e71caf282798bb447e00270b529b2e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim0&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the first dimension to be transposed</source>
          <target state="translated">&lt;strong&gt;dim0&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;転置される最初の次元</target>
        </trans-unit>
        <trans-unit id="f765d8f8c40b94958f1eab1cc37007590593d31d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim1&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the second dimension to be transposed</source>
          <target state="translated">&lt;strong&gt;dim1&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;転置される2番目の次元</target>
        </trans-unit>
        <trans-unit id="ba65b006661843062287a94ba12a78ac85885543" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim1&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; first dimension with respect to which to take diagonal. Default: -2.</source>
          <target state="translated">&lt;strong&gt;dim1&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;対角線を取る最初の次元。デフォルト：-2。</target>
        </trans-unit>
        <trans-unit id="0dd1507c4f8260c289a9910df29cf487d663e95d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim1&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; first dimension with respect to which to take diagonal. Default: 0.</source>
          <target state="translated">&lt;strong&gt;dim1&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;対角線を取る最初の次元。デフォルト：0。</target>
        </trans-unit>
        <trans-unit id="83aa8a523aec2c1ab6551640eaf4cb51b830a4c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim2&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; second dimension with respect to which to take diagonal. Default: -1.</source>
          <target state="translated">&lt;strong&gt;dim2&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;対角線を取る2番目の次元。デフォルト：-1。</target>
        </trans-unit>
        <trans-unit id="a71ef13a1ad5cb6ea968b4d4719be6d5961ef225" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim2&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; second dimension with respect to which to take diagonal. Default: 1.</source>
          <target state="translated">&lt;strong&gt;dim2&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;対角線を取る2番目の次元。デフォルト：1。</target>
        </trans-unit>
        <trans-unit id="c9b4a2ca457fba9669e7f8ced2b7ed6c30268fd6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which LogSoftmax will be computed.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;LogSoftmaxが計算される次元。</target>
        </trans-unit>
        <trans-unit id="34032d1dca3a055630ebba7923bebf4f47a22c38" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which Softmax will be computed (so every slice along dim will sum to 1).</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash; Softmaxが計算される次元（したがって、dimに沿ったすべてのスライスの合計は1になります）。</target>
        </trans-unit>
        <trans-unit id="01189dd75e2eefc978065b5ff2c0b7f87388d39e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which Softmin will be computed (so every slice along dim will sum to 1).</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash; Softminが計算される次元（したがって、dimに沿ったすべてのスライスの合計は1になります）。</target>
        </trans-unit>
        <trans-unit id="842005dbd132f5ef7d384c1fccbadec279da8f7c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which log_softmax will be computed.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;log_softmaxが計算される次元。</target>
        </trans-unit>
        <trans-unit id="fb8ee0a50b62dc6ae83f365bac143af1c17f1fd6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which softmax will be computed.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;ソフトマックスが計算される次元。</target>
        </trans-unit>
        <trans-unit id="01b7d7748905016190ad5e2bbe8293a9c597a66c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which softmax will be computed. Default: -1.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;ソフトマックスが計算される次元。デフォルト：-1。</target>
        </trans-unit>
        <trans-unit id="2777bb64e36f761c1f00ee5be70a591e4cd2145d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; A dimension along which softmin will be computed (so every slice along dim will sum to 1).</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash; softminが計算される次元（したがって、dimに沿ったすべてのスライスの合計は1になります）。</target>
        </trans-unit>
        <trans-unit id="88bb112f612fa4c72a0f99f7a672d75b2554455b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The dimension along which to integrate. By default, use the last dimension.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;統合する次元。デフォルトでは、最後のディメンションを使用します。</target>
        </trans-unit>
        <trans-unit id="90f0ee8c3c7b4d5b8a5616f19077bcfded77ea98" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension along which to index</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;インデックスを作成するディメンション</target>
        </trans-unit>
        <trans-unit id="b8ce25c1fd3bbad54cc79c92db33a7f9d23196c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension along which to split the tensor</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;テンソルを分割する次元</target>
        </trans-unit>
        <trans-unit id="3a9295ba14ea4badc07f27ea8db556fe8e515b0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension along which to split the tensor.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;テンソルを分割する次元。</target>
        </trans-unit>
        <trans-unit id="2a971edb34aad20abc50873c653c145b3af96b2a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension on which to split the input. Default: -1</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;入力を分割する次元。デフォルト：-1</target>
        </trans-unit>
        <trans-unit id="bf3801597d3facaa2cf443350498f58cd1324abb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive)</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;挿入する寸法。0から連結テンソルの次元数（両端を含む）の間でなければなりません</target>
        </trans-unit>
        <trans-unit id="2e66e4cf2db711439adae127683b487fd7e64123" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension to remove</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;削除する寸法</target>
        </trans-unit>
        <trans-unit id="c917eaab2e1d73b163be73789e03b1bc35b48a89" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; index of the dim along which we define channels to prune.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;プルーニングするチャネルを定義するdimのインデックス。</target>
        </trans-unit>
        <trans-unit id="7285e77d113e976c86c60be543b1f43be38c5bdd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the axis along which to index</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;インデックスを作成する軸</target>
        </trans-unit>
        <trans-unit id="4e017cd401e156f255b917cbb7d73173fc5184a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension along which to narrow</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;狭める次元</target>
        </trans-unit>
        <trans-unit id="7415821de8da0f78deed554d7e45d50759c33f0b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension in which we index</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;インデックスを作成するディメンション</target>
        </trans-unit>
        <trans-unit id="be8bc5d715865fc115635841b9517c75441ff8f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to apply unique. If &lt;code&gt;None&lt;/code&gt;, the unique of the flattened input is returned. default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;一意に適用するディメンション。 &lt;code&gt;None&lt;/code&gt; の場合、フラット化された入力の一意が返されます。デフォルト： &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4849740fe6ccbd59b0ec458f024b413bf6df7857" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to do the operation over</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;操作を実行する次元</target>
        </trans-unit>
        <trans-unit id="9dfe268e0c2ab5c49cd99f8395aef6a384d8dd63" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;削減する次元</target>
        </trans-unit>
        <trans-unit id="a0cd1da8b56960f63a95d25cf0421a937673e2ed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;縮小する次元。</target>
        </trans-unit>
        <trans-unit id="b8d7f0790b12bc35fd47df215177efe265599f93" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce. Default: 1</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;縮小する次元。デフォルト：1</target>
        </trans-unit>
        <trans-unit id="04661f26e0a90dbee7adeed063722879cc2dbedc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce. If &lt;code&gt;None&lt;/code&gt;, the argmax of the flattened input is returned.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;縮小する次元。 &lt;code&gt;None&lt;/code&gt; の場合、フラット化された入力のargmaxが返されます。</target>
        </trans-unit>
        <trans-unit id="406a26e14fe614e1dacddd572bc1dbd6bd5aa6a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce. If &lt;code&gt;None&lt;/code&gt;, the argmin of the flattened input is returned.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to reduce. If &lt;code&gt;None&lt;/code&gt; , the argmin of the flattened input is returned.</target>
        </trans-unit>
        <trans-unit id="d3e09769491b63675c94e11c9dea885116b4a2fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to slice</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to slice</target>
        </trans-unit>
        <trans-unit id="8317062bcf63e96ea89234edaf5254e1da5bcb47" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to slice over to get the sub-tensors</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the dimension to slice over to get the sub-tensors</target>
        </trans-unit>
        <trans-unit id="c54f0ef48043a5bec2dfca3a0f530fb8661be4ca" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the index at which to insert the singleton dimension</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the index at which to insert the singleton dimension</target>
        </trans-unit>
        <trans-unit id="d89796e28feb4305a6e51573710dca2759c32ff1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; a dimension or a list of dimensions to reduce. Default: reduce over all dims.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; a dimension or a list of dimensions to reduce. Default: reduce over all dims.</target>
        </trans-unit>
        <trans-unit id="f1addd18afe889496111cce39db43031d3782741" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; the dimension or dimensions to reduce.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; the dimension or dimensions to reduce.</target>
        </trans-unit>
        <trans-unit id="74f088f545a1351da755347a9f47abc2c5b4f78d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dim or tuple of dims along which to count non-zeros.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dim or tuple of dims along which to count non-zeros.</target>
        </trans-unit>
        <trans-unit id="7ea1a19f10e224b77d5a38dec1768437b4ef777c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-tuple of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-list of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;dim&lt;/code&gt; is an int, vector norm will be calculated over the specified dimension. If &lt;code&gt;dim&lt;/code&gt; is a 2-tuple of ints, matrix norm will be calculated over the specified dimensions. If &lt;code&gt;dim&lt;/code&gt; is None, matrix norm will be calculated when the input tensor has two dimensions, and vector norm will be calculated when the input tensor has one dimension. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-tuple of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-list of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;dim&lt;/code&gt; is an int, vector norm will be calculated over the specified dimension. If &lt;code&gt;dim&lt;/code&gt; is a 2-tuple of ints, matrix norm will be calculated over the specified dimensions. If &lt;code&gt;dim&lt;/code&gt; is None, matrix norm will be calculated when the input tensor has two dimensions, and vector norm will be calculated when the input tensor has one dimension. Default: &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b085e1cc1d870668a83baea40b3506508d9b0a8b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-tuple of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-list of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-tuple of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;2-list of python:ints&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.</target>
        </trans-unit>
        <trans-unit id="39551d8784237f98effe67d5d4fa079ab6e2d66d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimension of vectors. Default: 1</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimension of vectors. Default: 1</target>
        </trans-unit>
        <trans-unit id="7c530107de3a1e8849bcffa15e8fece38f241cc6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimension where cosine similarity is computed. Default: 1</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimension where cosine similarity is computed. Default: 1</target>
        </trans-unit>
        <trans-unit id="9439b0c674bc7c434adc98d580848f39ed9d115e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to repeat values. By default, use the flattened input array, and return a flat output array.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to repeat values. By default, use the flattened input array, and return a flat output array.</target>
        </trans-unit>
        <trans-unit id="c36ee04034363ccdd358a7c5f8322922bf781fba" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional FFT.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional FFT.</target>
        </trans-unit>
        <trans-unit id="45818af3fdbc4684e20729f70925408319f7b2e1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional Hermitian FFT.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional Hermitian FFT.</target>
        </trans-unit>
        <trans-unit id="e290320ee25d598ee056c54b55e12ed854c6dd23" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional Hermitian IFFT.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional Hermitian IFFT.</target>
        </trans-unit>
        <trans-unit id="f5f98ff50ef003b2c77b0c55e3bf6a58a74600f0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional IFFT.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional IFFT.</target>
        </trans-unit>
        <trans-unit id="d4bb3ea939e2fa64fe7995582d47d53ee2e17ec4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional real FFT.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional real FFT.</target>
        </trans-unit>
        <trans-unit id="dea0a42312b17fe5fe3428922ce27d374ec46837" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional real IFFT.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The dimension along which to take the one dimensional real IFFT.</target>
        </trans-unit>
        <trans-unit id="d32f4b0a48eae0a94c40f8913aaafd8fc5b4e99b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; dimension corresponding to number of outputs, the default is &lt;code&gt;0&lt;/code&gt;, except for modules that are instances of ConvTranspose{1,2,3}d, when it is &lt;code&gt;1&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; dimension corresponding to number of outputs, the default is &lt;code&gt;0&lt;/code&gt; , except for modules that are instances of ConvTranspose{1,2,3}d, when it is &lt;code&gt;1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0ff766b40b5bcbfe08f64170324fb2f4d6bf84af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; dimension over which to compute the norm</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; dimension over which to compute the norm</target>
        </trans-unit>
        <trans-unit id="a409409533f64a991397140f0cfeef5685c9fa1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if given, the input will be squeezed only in this dimension</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if given, the input will be squeezed only in this dimension</target>
        </trans-unit>
        <trans-unit id="05043e9f4aa7115561003ddcb8522351f1c5f51e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; index of the dim along which we define channels to prune. Default: -1.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; index of the dim along which we define channels to prune. Default: -1.</target>
        </trans-unit>
        <trans-unit id="4945ffee136e1eca9138593201351feb8ce974a6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the desired dimension in which stride is required</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the desired dimension in which stride is required</target>
        </trans-unit>
        <trans-unit id="cd4c3985b4741ed95097721d4f15edc8c371b9f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension over which the tensors are concatenated</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension over which the tensors are concatenated</target>
        </trans-unit>
        <trans-unit id="e9ff0c74674eab95e6f6442b562b6f45f41a7a9d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension to find the kth value along</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension to find the kth value along</target>
        </trans-unit>
        <trans-unit id="9422d28d87b196679cbb0c82259c774161568b6b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension to sort along</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension to sort along</target>
        </trans-unit>
        <trans-unit id="28c1fc242c3e6d093897709fa5e6ddd3f6f9dd3b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension to take the cross-product in.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the dimension to take the cross-product in.</target>
        </trans-unit>
        <trans-unit id="fa74b5fd1d3a59aa2fc20f8e622f3d996cbd575d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimensions to be transformed. Default: all dimensions, or the last &lt;code&gt;len(s)&lt;/code&gt; dimensions if &lt;code&gt;s&lt;/code&gt; is given.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimensions to be transformed. Default: all dimensions, or the last &lt;code&gt;len(s)&lt;/code&gt; dimensions if &lt;code&gt;s&lt;/code&gt; is given.</target>
        </trans-unit>
        <trans-unit id="714e04567ca514a499d3251ac5bce29de688ec57" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: all dimensions, or the last &lt;code&gt;len(s)&lt;/code&gt; dimensions if &lt;code&gt;s&lt;/code&gt; is given.</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: all dimensions, or the last &lt;code&gt;len(s)&lt;/code&gt; dimensions if &lt;code&gt;s&lt;/code&gt; is given.</target>
        </trans-unit>
        <trans-unit id="1c73040a5128f371bcddeab377f515339135506e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Union&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Dimension to be unflattened</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Union&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Dimension to be unflattened</target>
        </trans-unit>
        <trans-unit id="612979d2207bccfbcbdc71c971749078ecad7740" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Union&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Dimension to unflatten</source>
          <target state="translated">&lt;strong&gt;dim&lt;/strong&gt; (&lt;em&gt;Union&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Dimension to unflatten</target>
        </trans-unit>
        <trans-unit id="d9f96b0e996218b63b5f6697be8d86971ef5ef58" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dim_feedforward&lt;/strong&gt; &amp;ndash; the dimension of the feedforward network model (default=2048).</source>
          <target state="translated">&lt;strong&gt;dim_feedforward&lt;/strong&gt; &amp;ndash; the dimension of the feedforward network model (default=2048).</target>
        </trans-unit>
        <trans-unit id="61d8fc5ddd343a37fbd282f3894946f478ef2ebb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dimension&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension in which unfolding happens</source>
          <target state="translated">&lt;strong&gt;dimension&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; dimension in which unfolding happens</target>
        </trans-unit>
        <trans-unit id="e3b04003348060ae8bd467525af95608436f8504" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dimension&lt;/strong&gt; (&lt;em&gt;Int&lt;/em&gt;) &amp;ndash; The dimensionality of the sequence to be drawn</source>
          <target state="translated">&lt;strong&gt;dimension&lt;/strong&gt; (&lt;em&gt;Int&lt;/em&gt;) &amp;ndash; The dimensionality of the sequence to be drawn</target>
        </trans-unit>
        <trans-unit id="d084aae186e1378df8faf988ddfde110dfb17385" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dims&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; Axis along which to roll</source>
          <target state="translated">&lt;strong&gt;dims&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; Axis along which to roll</target>
        </trans-unit>
        <trans-unit id="c317ee80d0d1d374392208d23e06bdc1d0a08445" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dims&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of two lists of python:integers&lt;/em&gt;) &amp;ndash; number of dimensions to contract or explicit lists of dimensions for &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; respectively</source>
          <target state="translated">&lt;strong&gt;dims&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of two lists of python:integers&lt;/em&gt;) &amp;ndash; number of dimensions to contract or explicit lists of dimensions for &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; respectively</target>
        </trans-unit>
        <trans-unit id="391fb595ce6469da76a43990e1b4fedec4415935" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dims&lt;/strong&gt; (&lt;em&gt;a list&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; axis to flip on</source>
          <target state="translated">&lt;strong&gt;dims&lt;/strong&gt; (&lt;em&gt;a list&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; axis to flip on</target>
        </trans-unit>
        <trans-unit id="fa53309859ad0bb426190a8468616e70a2c2d3ec" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dims&lt;/strong&gt; (&lt;em&gt;a list&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; axis to rotate</source>
          <target state="translated">&lt;strong&gt;dims&lt;/strong&gt; (&lt;em&gt;a list&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; axis to rotate</target>
        </trans-unit>
        <trans-unit id="4b48119f623a86e0e73cdb61d4b28812b330622a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;distance_function&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A nonnegative, real-valued function that quantifies the closeness of two tensors. If not specified, &lt;code&gt;nn.PairwiseDistance&lt;/code&gt; will be used. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;distance_function&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; A nonnegative, real-valued function that quantifies the closeness of two tensors. If not specified, &lt;code&gt;nn.PairwiseDistance&lt;/code&gt; will be used. Default: &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d9fdd902ea3281b80f4ee9c58cfabafd6bc9880d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;div_value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; value used as an exponent to compute sizes of the clusters. Default: 4.0</source>
          <target state="translated">&lt;strong&gt;div_value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; value used as an exponent to compute sizes of the clusters. Default: 4.0</target>
        </trans-unit>
        <trans-unit id="2ed00f6805ef09c53fd3940b48ac159868ff1969" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;divide_by_initial_world_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, will divide gradients by the initial &lt;code&gt;world_size&lt;/code&gt; DDP training was launched with. If &lt;code&gt;False&lt;/code&gt;, will compute the effective world size (number of ranks that have not depleted their inputs yet) and divide gradients by that during allreduce. Set &lt;code&gt;divide_by_initial_world_size=True&lt;/code&gt; to ensure every input sample including the uneven inputs have equal weight in terms of how much they contribute to the global gradient. This is achieved by always dividing the gradient by the initial &lt;code&gt;world_size&lt;/code&gt; even when we encounter uneven inputs. If you set this to &lt;code&gt;False&lt;/code&gt;, we divide the gradient by the remaining number of nodes. This ensures parity with training on a smaller &lt;code&gt;world_size&lt;/code&gt; although it also means the uneven inputs would contribute more towards the global gradient. Typically, you would want to set this to &lt;code&gt;True&lt;/code&gt; for cases where the last few inputs of your training job are uneven. In extreme cases, where there is a large discrepancy in the number of inputs, setting this to &lt;code&gt;False&lt;/code&gt; might provide better results.</source>
          <target state="translated">&lt;strong&gt;divide_by_initial_world_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt; , will divide gradients by the initial &lt;code&gt;world_size&lt;/code&gt; DDP training was launched with. If &lt;code&gt;False&lt;/code&gt; , will compute the effective world size (number of ranks that have not depleted their inputs yet) and divide gradients by that during allreduce. Set &lt;code&gt;divide_by_initial_world_size=True&lt;/code&gt; to ensure every input sample including the uneven inputs have equal weight in terms of how much they contribute to the global gradient. This is achieved by always dividing the gradient by the initial &lt;code&gt;world_size&lt;/code&gt; even when we encounter uneven inputs. If you set this to &lt;code&gt;False&lt;/code&gt; , we divide the gradient by the remaining number of nodes. This ensures parity with training on a smaller &lt;code&gt;world_size&lt;/code&gt; although it also means the uneven inputs would contribute more towards the global gradient. Typically, you would want to set this to &lt;code&gt;True&lt;/code&gt; for cases where the last few inputs of your training job are uneven. In extreme cases, where there is a large discrepancy in the number of inputs, setting this to &lt;code&gt;False&lt;/code&gt; might provide better results.</target>
        </trans-unit>
        <trans-unit id="049bcce7df5c629c4fad3f80f7fb581f440c0337" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;divisor_override&lt;/strong&gt; &amp;ndash; if specified, it will be used as divisor, otherwise &lt;code&gt;kernel_size&lt;/code&gt; will be used</source>
          <target state="translated">&lt;strong&gt;divisor_override&lt;/strong&gt; &amp;ndash; if specified, it will be used as divisor, otherwise &lt;code&gt;kernel_size&lt;/code&gt; will be used</target>
        </trans-unit>
        <trans-unit id="853252d1cb8e53ec5f6c1f1eee0e317901dfd0af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;divisor_override&lt;/strong&gt; &amp;ndash; if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None</source>
          <target state="translated">&lt;strong&gt;divisor_override&lt;/strong&gt; &amp;ndash; if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None</target>
        </trans-unit>
        <trans-unit id="25dd43997ffb2286d38c5c4ea638d851750fdcd1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dlpack&lt;/strong&gt; &amp;ndash; a PyCapsule object with the dltensor</source>
          <target state="translated">&lt;strong&gt;dlpack&lt;/strong&gt; &amp;ndash; a PyCapsule object with the dltensor</target>
        </trans-unit>
        <trans-unit id="79e803c5014bf096c8b936c68165b4355eadf0c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;do_constant_folding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; If True, the constant-folding optimization is applied to the model during export. Constant-folding optimization will replace some of the ops that have all constant inputs, with pre-computed constant nodes.</source>
          <target state="translated">&lt;strong&gt;do_constant_folding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; If True, the constant-folding optimization is applied to the model during export. Constant-folding optimization will replace some of the ops that have all constant inputs, with pre-computed constant nodes.</target>
        </trans-unit>
        <trans-unit id="93e1868e986f20e5fceebb1ba081741a9c11926d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; If non-zero, introduces a &lt;code&gt;Dropout&lt;/code&gt; layer on the outputs of each GRU layer except the last layer, with dropout probability equal to &lt;code&gt;dropout&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; If non-zero, introduces a &lt;code&gt;Dropout&lt;/code&gt; layer on the outputs of each GRU layer except the last layer, with dropout probability equal to &lt;code&gt;dropout&lt;/code&gt; . Default: 0</target>
        </trans-unit>
        <trans-unit id="f1651899fbda49e6928ea8a0d295e3cfc914c011" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; If non-zero, introduces a &lt;code&gt;Dropout&lt;/code&gt; layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to &lt;code&gt;dropout&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; If non-zero, introduces a &lt;code&gt;Dropout&lt;/code&gt; layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to &lt;code&gt;dropout&lt;/code&gt; . Default: 0</target>
        </trans-unit>
        <trans-unit id="96b84bd1caf45dc07546855562b7416fae84878a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; If non-zero, introduces a &lt;code&gt;Dropout&lt;/code&gt; layer on the outputs of each RNN layer except the last layer, with dropout probability equal to &lt;code&gt;dropout&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; If non-zero, introduces a &lt;code&gt;Dropout&lt;/code&gt; layer on the outputs of each RNN layer except the last layer, with dropout probability equal to &lt;code&gt;dropout&lt;/code&gt; . Default: 0</target>
        </trans-unit>
        <trans-unit id="5f96b6d7bf1dc804d0874288692d31f0b71cf772" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; a Dropout layer on attn_output_weights. Default: 0.0.</source>
          <target state="translated">&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; a Dropout layer on attn_output_weights. Default: 0.0.</target>
        </trans-unit>
        <trans-unit id="ea212a4ef049bb5d0535242d99d1a83d7b9f7d0a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; the dropout value (default=0.1).</source>
          <target state="translated">&lt;strong&gt;dropout&lt;/strong&gt; &amp;ndash; the dropout value (default=0.1).</target>
        </trans-unit>
        <trans-unit id="16fb29d21cd7a9ad4206efc9c506ce5a9cf98a02" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Destination rank</source>
          <target state="translated">&lt;strong&gt;dst&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Destination rank</target>
        </trans-unit>
        <trans-unit id="ebbff8f7d1835173bbe908168f4d760a0e927d2a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Destination rank.</source>
          <target state="translated">&lt;strong&gt;dst&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Destination rank.</target>
        </trans-unit>
        <trans-unit id="c997f09b2bb6f5bbe4285b29caa42aeff58eb4c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Destination rank (default is 0)</source>
          <target state="translated">&lt;strong&gt;dst&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Destination rank (default is 0)</target>
        </trans-unit>
        <trans-unit id="15d2bdadaa3b397c334c7c569c4166d8c09f9b73" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Full path where object will be saved, e.g. &lt;code&gt;/tmp/temporary_file&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;dst&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Full path where object will be saved, e.g. &lt;code&gt;/tmp/temporary_file&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1bb062fcb22edc7afbb4e01d163cf5831e179573" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst_tensor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Destination tensor rank within &lt;code&gt;tensor_list&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;dst_tensor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Destination tensor rank within &lt;code&gt;tensor_list&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="819196b391e0255903f0858db2143983b3dedb91" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dst_type&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;type&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;string&lt;/em&gt;) &amp;ndash; the desired type</source>
          <target state="translated">&lt;strong&gt;dst_type&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;type&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;string&lt;/em&gt;) &amp;ndash; the desired type</target>
        </trans-unit>
        <trans-unit id="9a4c495b44f4761667112255e495ba6d941ec366" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; &amp;ndash; data type of output Quantized Tensor</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; &amp;ndash; data type of output Quantized Tensor</target>
        </trans-unit>
        <trans-unit id="c1da9e6e49781bd49b10cf164e54cc534c373f2b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; &amp;ndash; quantization data type to use. Default: &lt;code&gt;torch.quint8&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; &amp;ndash; quantization data type to use. Default: &lt;code&gt;torch.quint8&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e8b67954b83b4c9aa34630f43e704bf025dc94fd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;) &amp;ndash; the desired data type of returned tensor. Has to be one of the quantized dtypes: &lt;code&gt;torch.quint8&lt;/code&gt;, &lt;code&gt;torch.qint8&lt;/code&gt;, &lt;code&gt;torch.qint32&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;) &amp;ndash; the desired data type of returned tensor. Has to be one of the quantized dtypes: &lt;code&gt;torch.quint8&lt;/code&gt; , &lt;code&gt;torch.qint8&lt;/code&gt; , &lt;code&gt;torch.qint32&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3e55ae84264ba3b49b96fa73c9f49b02e373e50a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned Tensor. Default: if &lt;code&gt;None&lt;/code&gt;, defaults to the dtype of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned Tensor. Default: if &lt;code&gt;None&lt;/code&gt; , defaults to the dtype of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="36bd900451348a91ef9ce9e64799f6d5a6c72b59" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: &lt;code&gt;torch.int64&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: &lt;code&gt;torch.int64&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1d58f90cb68e79cff8b96e7dc86dd4a2d5606995" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;torch.long&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt; , &lt;code&gt;torch.long&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1d27c0b19831af56deba70f9352bc038b1e9a426" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, infers data type from &lt;code&gt;data&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt; , infers data type from &lt;code&gt;data&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="754f71e33b12e157331fb5bf10e0619a8ecbce0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, uses a global default (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt; , uses a global default (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt; &lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt; &lt;/a&gt;).</target>
        </trans-unit>
        <trans-unit id="5e1aa8968192882a073c78793ea32a16dd1c21d1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, uses a global default (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;). If &lt;code&gt;dtype&lt;/code&gt; is not given, infer the data type from the other input arguments. If any of &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;end&lt;/code&gt;, or &lt;code&gt;stop&lt;/code&gt; are floating-point, the &lt;code&gt;dtype&lt;/code&gt; is inferred to be the default dtype, see &lt;a href=&quot;torch.get_default_dtype#torch.get_default_dtype&quot;&gt;&lt;code&gt;get_default_dtype()&lt;/code&gt;&lt;/a&gt;. Otherwise, the &lt;code&gt;dtype&lt;/code&gt; is inferred to be &lt;code&gt;torch.int64&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt; , uses a global default (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt; &lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt; &lt;/a&gt;). If &lt;code&gt;dtype&lt;/code&gt; is not given, infer the data type from the other input arguments. If any of &lt;code&gt;start&lt;/code&gt; , &lt;code&gt;end&lt;/code&gt; , or &lt;code&gt;stop&lt;/code&gt; are floating-point, the &lt;code&gt;dtype&lt;/code&gt; is inferred to be the default dtype, see &lt;a href=&quot;torch.get_default_dtype#torch.get_default_dtype&quot;&gt; &lt;code&gt;get_default_dtype()&lt;/code&gt; &lt;/a&gt;. Otherwise, the &lt;code&gt;dtype&lt;/code&gt; is inferred to be &lt;code&gt;torch.int64&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3b423086bd082fcebc5a428940e93eeaaa4a7025" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt;, uses a global default (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt;&lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt;&lt;/a&gt;). Only floating point types are supported.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if &lt;code&gt;None&lt;/code&gt; , uses a global default (see &lt;a href=&quot;torch.set_default_tensor_type#torch.set_default_tensor_type&quot;&gt; &lt;code&gt;torch.set_default_tensor_type()&lt;/code&gt; &lt;/a&gt;). Only floating point types are supported.</target>
        </trans-unit>
        <trans-unit id="1f1237018b147643a39f3e093d71d66749f7520f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if None, infers data type from &lt;code&gt;values&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. Default: if None, infers data type from &lt;code&gt;values&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="242051541efaa2777f5472bfe169a3381046ea24" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. If specified, the input tensor is casted to :attr:&amp;rsquo;dtype&amp;rsquo; while performing the operation. Default: None.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. If specified, the input tensor is casted to :attr:&amp;rsquo;dtype&amp;rsquo; while performing the operation. Default: None.</target>
        </trans-unit>
        <trans-unit id="802bfb2cb709fdad8389996e31ce1fc046b7a6a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. If specified, the input tensor is casted to &lt;code&gt;dtype&lt;/code&gt; before the operation is performed. This is useful for preventing data type overflows. Default: None.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired data type of returned tensor. If specified, the input tensor is casted to &lt;code&gt;dtype&lt;/code&gt; before the operation is performed. This is useful for preventing data type overflows. Default: None.</target>
        </trans-unit>
        <trans-unit id="bf97cc7b5c225fd6d7e718d925da7641ae8dd16c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;type&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;string&lt;/em&gt;) &amp;ndash; The desired type</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;type&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;string&lt;/em&gt;) &amp;ndash; The desired type</target>
        </trans-unit>
        <trans-unit id="697b78c37ec8949c304299db02bbd09794133985" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; the desired type of returned tensor. Default: if None, same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; (&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;, optional) &amp;ndash; the desired type of returned tensor. Default: if None, same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; as this tensor.</target>
        </trans-unit>
        <trans-unit id="e6ad67e837a7a5413b747672aa87555a7798fda9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;) &amp;ndash; the desired floating point type of the floating point parameters and buffers in this module</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; ( &lt;code&gt;torch.dtype&lt;/code&gt; ) &amp;ndash; the desired floating point type of the floating point parameters and buffers in this module</target>
        </trans-unit>
        <trans-unit id="f854a7599c513f55feb63cd3cceed0c39260832a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;, optional) &amp;ndash; If specified, the input tensor is cast to &lt;code&gt;dtype&lt;/code&gt; before performing the operation, and the returned tensor&amp;rsquo;s type will be &lt;code&gt;dtype&lt;/code&gt;. If this argument is used in conjunction with the &lt;code&gt;out&lt;/code&gt; argument, the output tensor&amp;rsquo;s type must match this argument or a RuntimeError will be raised. This argument is not currently supported for &lt;code&gt;ord='nuc'&lt;/code&gt; or &lt;code&gt;ord='fro'&lt;/code&gt;. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; ( &lt;code&gt;torch.dtype&lt;/code&gt; , optional) &amp;ndash; If specified, the input tensor is cast to &lt;code&gt;dtype&lt;/code&gt; before performing the operation, and the returned tensor&amp;rsquo;s type will be &lt;code&gt;dtype&lt;/code&gt; . If this argument is used in conjunction with the &lt;code&gt;out&lt;/code&gt; argument, the output tensor&amp;rsquo;s type must match this argument or a RuntimeError will be raised. This argument is not currently supported for &lt;code&gt;ord='nuc'&lt;/code&gt; or &lt;code&gt;ord='fro'&lt;/code&gt; . Default: &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="cf0e16ba6689c6aefebe41b2f37216499a3bab49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;, optional) &amp;ndash; the desired data type of returned Tensor. Default: dtype of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; ( &lt;code&gt;torch.dtype&lt;/code&gt; , optional) &amp;ndash; the desired data type of returned Tensor. Default: dtype of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ddf34144bdd895ef81d56a2e64c3a31dcfb71d3f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;, optional) &amp;ndash; the desired data type of returned tensor. If specified, the input tensor is casted to &lt;code&gt;dtype&lt;/code&gt; before the operation is performed. This is useful for preventing data type overflows. Default: None.</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; ( &lt;code&gt;torch.dtype&lt;/code&gt; , optional) &amp;ndash; the desired data type of returned tensor. If specified, the input tensor is casted to &lt;code&gt;dtype&lt;/code&gt; before the operation is performed. This is useful for preventing data type overflows. Default: None.</target>
        </trans-unit>
        <trans-unit id="b5aa3ffb5214bb07d706f4bff69889b064a67a58" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dtype&lt;/strong&gt; (&lt;code&gt;torch.dtype&lt;/code&gt;, optional) &amp;ndash; the desired data type of the returned tensor. Default: &lt;code&gt;torch.float32&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;dtype&lt;/strong&gt; ( &lt;code&gt;torch.dtype&lt;/code&gt; , optional) &amp;ndash; the desired data type of the returned tensor. Default: &lt;code&gt;torch.float32&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7c192f6abd14a8034e24d8a96ca7b9be11ad48f2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dx&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; The distance between points at which &lt;code&gt;y&lt;/code&gt; is sampled.</source>
          <target state="translated">&lt;strong&gt;dx&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; The distance between points at which &lt;code&gt;y&lt;/code&gt; is sampled.</target>
        </trans-unit>
        <trans-unit id="103894978c9b529418b87fd9904f3950e5930d8e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;dynamic_axes&lt;/strong&gt; (&lt;em&gt;dict&amp;lt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;dict&amp;lt;python:int&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;string&amp;gt;&amp;gt;&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;dict&amp;lt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;(&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;&lt;em&gt;&amp;gt;&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default empty dict&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;dynamic_axes&lt;/strong&gt; (&lt;em&gt;dict&amp;lt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;dict&amp;lt;python:int&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;string&amp;gt;&amp;gt;&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;dict&amp;lt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;(&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;&lt;em&gt;&amp;gt;&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default empty dict&lt;/em&gt;) &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="c4fbef8984bd074ea0df43f49dda2d6246584439" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;edgeitems&lt;/strong&gt; &amp;ndash; Number of array items in summary at beginning and end of each dimension (default = 3).</source>
          <target state="translated">&lt;strong&gt;edgeitems&lt;/strong&gt; &amp;ndash; Number of array items in summary at beginning and end of each dimension (default = 3).</target>
        </trans-unit>
        <trans-unit id="42415c6475a98e0f1fa07ebf4b124d7a8ce2a188" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvalues&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): Shape</source>
          <target state="translated">&lt;strong&gt;eigenvalues&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): Shape</target>
        </trans-unit>
        <trans-unit id="fdaa0feae05488ebb171b346905c4f5b577cb835" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; &lt;code&gt;True&lt;/code&gt; to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed</source>
          <target state="translated">&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; &lt;code&gt;True&lt;/code&gt; to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed</target>
        </trans-unit>
        <trans-unit id="678dbd774fefa060e069a5a389a8baecab662449" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): If &lt;code&gt;eigenvectors=False&lt;/code&gt;, it&amp;rsquo;s an empty tensor. Otherwise, this tensor of shape</source>
          <target state="translated">&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): If &lt;code&gt;eigenvectors=False&lt;/code&gt; , it&amp;rsquo;s an empty tensor. Otherwise, this tensor of shape</target>
        </trans-unit>
        <trans-unit id="850ac1cd20b0b0fc94fb9c8edab9b43e67eaf290" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): Shape</source>
          <target state="translated">&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): Shape</target>
        </trans-unit>
        <trans-unit id="ad4105465a171ee7fcc51139dea5abbe36639f29" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether eigenvectors have to be computed</source>
          <target state="translated">&lt;strong&gt;eigenvectors&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether eigenvectors have to be computed</target>
        </trans-unit>
        <trans-unit id="a63d80190229ce72c85a7e6560054704380577ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;elementwise_affine&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;elementwise_affine&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt; , this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5d18cda3fe2834c321ed263a1762695dafaa4d1b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;embed_dim&lt;/strong&gt; &amp;ndash; total dimension of the model.</source>
          <target state="translated">&lt;strong&gt;embed_dim&lt;/strong&gt; &amp;ndash; total dimension of the model.</target>
        </trans-unit>
        <trans-unit id="aa8b03fcaceb218feac127ba1ed55359bc28eca3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;embedding_dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the size of each embedding vector</source>
          <target state="translated">&lt;strong&gt;embedding_dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the size of each embedding vector</target>
        </trans-unit>
        <trans-unit id="de1af4e04b664f6ae8d83d1e7d608531b740f185" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;embeddings&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as &lt;code&gt;num_embeddings&lt;/code&gt;, second as &lt;code&gt;embedding_dim&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;embeddings&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as &lt;code&gt;num_embeddings&lt;/code&gt; , second as &lt;code&gt;embedding_dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="06e3728c866bf657f051e1739af9167591891b44" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;embeddings&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as &amp;lsquo;num_embeddings&amp;rsquo;, second as &amp;lsquo;embedding_dim&amp;rsquo;.</source>
          <target state="translated">&lt;strong&gt;embeddings&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as &amp;lsquo;num_embeddings&amp;rsquo;, second as &amp;lsquo;embedding_dim&amp;rsquo;.</target>
        </trans-unit>
        <trans-unit id="f0ff1217368a5b45df32d944d2dd475690d13d9f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enable&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether to enable uneven input detection or not. Pass in &lt;code&gt;enable=False&lt;/code&gt; to disable in cases where you know that inputs are even across participating processes. Default is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;enable&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether to enable uneven input detection or not. Pass in &lt;code&gt;enable=False&lt;/code&gt; to disable in cases where you know that inputs are even across participating processes. Default is &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c7081365cba99386e7771c337648ca106e830b0f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enable_onnx_checker&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default True&lt;/em&gt;) &amp;ndash; If True the onnx model checker will be run as part of the export, to ensure the exported model is a valid ONNX model.</source>
          <target state="translated">&lt;strong&gt;enable_onnx_checker&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default True&lt;/em&gt;) &amp;ndash; If True the onnx model checker will be run as part of the export, to ensure the exported model is a valid ONNX model.</target>
        </trans-unit>
        <trans-unit id="8b5bbce56260acf11245d762cd1bb7f8add8801b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;encoder_layer&lt;/strong&gt; &amp;ndash; an instance of the TransformerEncoderLayer() class (required).</source>
          <target state="translated">&lt;strong&gt;encoder_layer&lt;/strong&gt; &amp;ndash; an instance of the TransformerEncoderLayer() class (required).</target>
        </trans-unit>
        <trans-unit id="d84a97832788a91df32af257307532f9760f3b9c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor with the ending points</source>
          <target state="translated">&lt;strong&gt;end&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor with the ending points</target>
        </trans-unit>
        <trans-unit id="7d5c9c4c24dd3af3a29f20c653c917dfea2df720" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the ending value for the set of points</source>
          <target state="translated">&lt;strong&gt;end&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the ending value for the set of points</target>
        </trans-unit>
        <trans-unit id="6472482679cdc676a7850881f5d00cc512baada7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the ending value for the set of points</source>
          <target state="translated">&lt;strong&gt;end&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the ending value for the set of points</target>
        </trans-unit>
        <trans-unit id="841fff8e784294f57ea3f8ba62be671371d870ec" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end_dim&lt;/strong&gt; &amp;ndash; last dim to flatten (default = -1).</source>
          <target state="translated">&lt;strong&gt;end_dim&lt;/strong&gt; &amp;ndash; last dim to flatten (default = -1).</target>
        </trans-unit>
        <trans-unit id="c799ddd223af170e7b8eec13d7b2c9d7165d8603" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;end_dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the last dim to flatten</source>
          <target state="translated">&lt;strong&gt;end_dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the last dim to flatten</target>
        </trans-unit>
        <trans-unit id="5a6422972f6f1ee1d35eb1fc402a56304ba9816a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enforce_sorted&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, checks that the input contains sequences sorted by length in a decreasing order. If &lt;code&gt;False&lt;/code&gt;, this condition is not checked. Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;enforce_sorted&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt; , checks that the input contains sequences sorted by length in a decreasing order. If &lt;code&gt;False&lt;/code&gt; , this condition is not checked. Default: &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f8c91d834d8c02f59864a331664c622e334c3693" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;enforce_sorted&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the input is expected to contain sequences sorted by length in a decreasing order. If &lt;code&gt;False&lt;/code&gt;, the input will get sorted unconditionally. Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;enforce_sorted&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt; , the input is expected to contain sequences sorted by length in a decreasing order. If &lt;code&gt;False&lt;/code&gt; , the input will get sorted unconditionally. Default: &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d07cc920f4a588343d52b60256e017a7987131f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; &amp;ndash; a value added to the denominator for numerical stability. Default: 1e-5</source>
          <target state="translated">&lt;strong&gt;eps&lt;/strong&gt; &amp;ndash; a value added to the denominator for numerical stability. Default: 1e-5</target>
        </trans-unit>
        <trans-unit id="f8684696132f475ee9f33febf9c44e66f038a87a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; &amp;ndash; a value added to the denominator for numerical stability. Default: &lt;code&gt;1e-5&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;eps&lt;/strong&gt; &amp;ndash;数値安定性のために分母に追加される値。デフォルト： &lt;code&gt;1e-5&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ac6c5326b4eb0b53657544fec8194ff31859de16" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; small value to avoid division by zero. Default: 1e-12</source>
          <target state="translated">&lt;strong&gt;eps&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;ゼロによる除算を避けるための小さな値。デフォルト：1e-12</target>
        </trans-unit>
        <trans-unit id="a3071da796481042a1779ea93a5637a546557315" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Small value to avoid division by zero. Default: 1e-6</source>
          <target state="translated">&lt;strong&gt;eps&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;ゼロによる除算を避けるための小さな値。デフォルト：1e-6</target>
        </trans-unit>
        <trans-unit id="bfa6309f005abf8cb4486bcdb54e88aa5722cf1c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Small value to avoid division by zero. Default: 1e-8</source>
          <target state="translated">&lt;strong&gt;eps&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;ゼロによる除算を避けるための小さな値。デフォルト：1e-8</target>
        </trans-unit>
        <trans-unit id="a35a97bcd21164533c2c178f41069f2aca802198" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Small value to avoid evaluation of</source>
          <target state="translated">&lt;strong&gt;eps&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;の評価を避けるための小さな値</target>
        </trans-unit>
        <trans-unit id="3e84e8770e10ffa80ab0cd44b0a601016d1c45b1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; epsilon for numerical stability in calculating norms</source>
          <target state="translated">&lt;strong&gt;eps&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;ノルムの計算における数値安定性のためのイプシロン</target>
        </trans-unit>
        <trans-unit id="64bfcd7f406bb75e0a20d11554ef0afad32ae5c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;eps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the epsilon for input clamp bound. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;eps&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;入力クランプバウンドのイプシロン。デフォルト： &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7aa30076d7d518c87037ac60ab79f79f7490d2c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;equal_nan&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, then two &lt;code&gt;NaN&lt;/code&gt; s will be considered equal. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;equal_nan&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、2つの &lt;code&gt;NaN&lt;/code&gt; は等しいと見なされます。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1f8913ade12e3dc4c4d8fd9b0228a8bcf2b585f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;equation&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; The equation is given in terms of lower case letters (indices) to be associated with each dimension of the operands and result. The left hand side lists the operands dimensions, separated by commas. There should be one index letter per tensor dimension. The right hand side follows after &lt;code&gt;-&amp;gt;&lt;/code&gt; and gives the indices for the output. If the &lt;code&gt;-&amp;gt;&lt;/code&gt; and right hand side are omitted, it implicitly defined as the alphabetically sorted list of all indices appearing exactly once in the left hand side. The indices not apprearing in the output are summed over after multiplying the operands entries. If an index appears several times for the same operand, a diagonal is taken. Ellipses &lt;code&gt;&amp;hellip;&lt;/code&gt; represent a fixed number of dimensions. If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output.</source>
          <target state="translated">&lt;strong&gt;方程式&lt;/strong&gt;（&lt;em&gt;文字列&lt;/em&gt;）&amp;ndash;方程式は、オペランドと結果の各次元に関連付けられる小文字（インデックス）で示されます。左側には、オペランドの次元がコンマで区切られてリストされています。テンソル次元ごとに1つのインデックス文字が必要です。右側は &lt;code&gt;-&amp;gt;&lt;/code&gt; の後に続き、出力のインデックスを示します。場合 &lt;code&gt;-&amp;gt;&lt;/code&gt; と右辺それは暗黙的に左側に一度だけ登場するすべてのインデックスのアルファベット順にソートされたリストとして定義され、省略されています。出力に表示されないインデックスは、オペランドエントリを乗算した後に合計されます。同じオペランドに対してインデックスが複数回出現する場合、対角線が取られます。省略記号 &lt;code&gt;&amp;hellip;&lt;/code&gt; 固定数の次元を表します。右側が推測される場合、省略記号の寸法は出力の先頭にあります。</target>
        </trans-unit>
        <trans-unit id="34f745bd094bd2de883ee47fbf1638e33919faba" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;example_inputs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; A tuple of example inputs that will be passed to the function while tracing. The resulting trace can be run with inputs of different types and shapes assuming the traced operations support those types and shapes. &lt;code&gt;example_inputs&lt;/code&gt; may also be a single Tensor in which case it is automatically wrapped in a tuple.</source>
          <target state="translated">&lt;strong&gt;example_inputs&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;トレース中に関数に渡されるサンプル入力のタプル。結果のトレースは、トレースされた操作がそれらのタイプと形状をサポートしていると仮定して、さまざまなタイプと形状の入力で実行できます。 &lt;code&gt;example_inputs&lt;/code&gt; は単一のTensorの場合もあり、その場合は自動的にタプルにラップされます。</target>
        </trans-unit>
        <trans-unit id="618d999c8d78a9676692412df718e533abdf676f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;example_outputs&lt;/strong&gt; (&lt;em&gt;tuple of Tensors&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default None&lt;/em&gt;) &amp;ndash; Model&amp;rsquo;s example outputs being exported. example_outputs must be provided when exporting a ScriptModule or TorchScript Function.</source>
          <target state="translated">&lt;strong&gt;example_outputs&lt;/strong&gt;（&lt;em&gt;テンソルのタプル&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;デフォルトはなし&lt;/em&gt;）&amp;ndash;エクスポートされるモデルのサンプル出力。ScriptModuleまたはTorchScript関数をエクスポートするときは、example_outputsを指定する必要があります。</target>
        </trans-unit>
        <trans-unit id="1a88c1284ab82b366b962ae9cc9b58fad7708160" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;exponent&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the exponent tensor</source>
          <target state="translated">&lt;strong&gt;指数&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;指数テンソル</target>
        </trans-unit>
        <trans-unit id="00243251e0e37f0ed072fb8db0c41a52c671837f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;exponent&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tensor&lt;/em&gt;) &amp;ndash; the exponent value</source>
          <target state="translated">&lt;strong&gt;指数&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;em&gt;テンソル&lt;/em&gt;）&amp;ndash;指数値</target>
        </trans-unit>
        <trans-unit id="fcb9c92fd7acb8cc56e2900598ba04b5be24aae8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;export_params&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default True&lt;/em&gt;) &amp;ndash; if specified, all parameters will be exported. Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by &lt;code&gt;model.state_dict().values()&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;export_params&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;デフォルトはTrue&lt;/em&gt;）&amp;ndash;指定されている場合、すべてのパラメーターがエクスポートされます。トレーニングされていないモデルをエクスポートする場合は、これをFalseに設定します。この場合、エクスポートされたモデルは、最初にすべてのパラメーターを引数として &lt;code&gt;model.state_dict().values()&lt;/code&gt; 、model.state_dict（）。values（）で指定された順序になります。</target>
        </trans-unit>
        <trans-unit id="f60c4eee6ac8bd3b83d115e58f8f9d5c6a1f285a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;export_raw_ir&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; [DEPRECATED. use operator_export_type] export the internal IR directly instead of converting it to ONNX ops.</source>
          <target state="translated">&lt;strong&gt;export_raw_ir&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;デフォルトはFalse&lt;/em&gt;）&amp;ndash; [&lt;strong&gt;非推奨&lt;/strong&gt;。使用operator_export_type]内部IRをONNXopsに変換するのではなく、直接エクスポートします。</target>
        </trans-unit>
        <trans-unit id="e82a47709effdd4ed45378b2094bb76a83f236b5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;external_data_format&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; If True, then the model is exported in ONNX external data format, in which case some of the model parameters are stored in external binary files and not in the ONNX model file itself. See link for format details: &lt;a href=&quot;https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423&quot;&gt;https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423&lt;/a&gt; Also, in this case, argument &amp;lsquo;f&amp;rsquo; must be a string specifying the location of the model. The external binary files will be stored in the same location specified by the model location &amp;lsquo;f&amp;rsquo;. If False, then the model is stored in regular format, i.e. model and parameters are all in one file. This argument is ignored for all export types other than ONNX.</source>
          <target state="translated">&lt;strong&gt;external_data_format&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;）&amp;ndash; Trueの場合、モデルはONNX外部データ形式でエクスポートされます。この場合、モデルパラメーターの一部はONNXモデルファイル自体ではなく、外部バイナリファイルに保存されます。フォーマットの詳細については、リンクを参照してください：&lt;a href=&quot;https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423&quot;&gt;https&lt;/a&gt;：//github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423また、この場合、引数「f」はモデルの場所を指定する文字列である必要があります。外部バイナリファイルは、モデルの場所「f」で指定されたのと同じ場所に保存されます。 Falseの場合、モデルは通常の形式で保存されます。つまり、モデルとパラメーターはすべて1つのファイルにあります。この引数は、ONNX以外のすべてのエクスポートタイプでは無視されます。</target>
        </trans-unit>
        <trans-unit id="778fd667aea6cff6dc8d68bb20cefa3e389d87e6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;extra_cflags&lt;/strong&gt; &amp;ndash; optional list of compiler flags to forward to the build.</source>
          <target state="translated">&lt;strong&gt;extra_cflags&lt;/strong&gt; &amp;ndash;ビルドに転送するコンパイラフラグのオプションのリスト。</target>
        </trans-unit>
        <trans-unit id="fcbab5a7fa61c6ed8414a66172d5505acca2751e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;extra_cuda_cflags&lt;/strong&gt; &amp;ndash; optional list of compiler flags to forward to nvcc when building CUDA sources.</source>
          <target state="translated">&lt;strong&gt;extra_cuda_cflags &amp;ndash;CUDA&lt;/strong&gt;ソースを構築するときにnvccに転送するコンパイラフラグのオプションのリスト。</target>
        </trans-unit>
        <trans-unit id="5906d2861e5b04548eb15a5d2b6487f486e2e624" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;extra_include_paths&lt;/strong&gt; &amp;ndash; optional list of include directories to forward to the build.</source>
          <target state="translated">&lt;strong&gt;extra_include_paths&lt;/strong&gt; &amp;ndash;ビルドに転送するインクルードディレクトリのオプションのリスト。</target>
        </trans-unit>
        <trans-unit id="62224af2d822c58c7d8e8fba25c2a2dc1f3ef2cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;extra_ldflags&lt;/strong&gt; &amp;ndash; optional list of linker flags to forward to the build.</source>
          <target state="translated">&lt;strong&gt;extra_ldflags&lt;/strong&gt; &amp;ndash;ビルドに転送するリンカーフラグのオプションのリスト。</target>
        </trans-unit>
        <trans-unit id="c6f0cc911afdf14389ad75575a9c9cbfa3c8ccfb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; A file-like object (has to implement write and flush) or a string containing a file name.</source>
          <target state="translated">&lt;strong&gt;f&lt;/strong&gt; &amp;ndash;ファイルのようなオブジェクト（書き込みとフラッシュを実装する必要があります）またはファイル名を含む文字列。</target>
        </trans-unit>
        <trans-unit id="0bbf42a4b349b4674d2d35b2fcc87d253370be87" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; a file-like object (has to implement &lt;code&gt;read()&lt;/code&gt;, :meth`readline`, :meth`tell`, and :meth`seek`), or a string or os.PathLike object containing a file name</source>
          <target state="translated">&lt;strong&gt;f&lt;/strong&gt; &amp;ndash;ファイルのようなオブジェクト（ &lt;code&gt;read()&lt;/code&gt; 、： meth`readline`、：meth`tell`、および：meth`seek`を実装する必要があります）、またはファイル名を含む文字列またはos.PathLikeオブジェクト</target>
        </trans-unit>
        <trans-unit id="54a2a51aa44ae289f486330a5f54f09e6f971a65" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name. A binary Protobuf will be written to this file.</source>
          <target state="translated">&lt;strong&gt;f&lt;/strong&gt; &amp;ndash;ファイルのようなオブジェクト（ファイル記述子を返すfilenoを実装する必要があります）またはファイル名を含む文字列。バイナリProtobufがこのファイルに書き込まれます。</target>
        </trans-unit>
        <trans-unit id="677757134bd8309ac15335a18ed7a21b22ef8fdd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name</source>
          <target state="translated">&lt;strong&gt;f&lt;/strong&gt; &amp;ndash;ファイルのようなオブジェクト（read、readline、tell、seekを実装する必要があります）、またはファイル名を含む文字列</target>
        </trans-unit>
        <trans-unit id="df677cb77b7f5749b31ae401c7fa92ff2d7e7fe5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;f&lt;/strong&gt; &amp;ndash; a file-like object (has to implement write and flush) or a string or os.PathLike object containing a file name</source>
          <target state="translated">&lt;strong&gt;f&lt;/strong&gt; &amp;ndash;ファイルのようなオブジェクト（書き込みとフラッシュを実装する必要があります）またはファイル名を含む文字列またはos.PathLikeオブジェクト</target>
        </trans-unit>
        <trans-unit id="635ef0bc301d974c0ff51acd3ba7ee21d88b4744" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;faces&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Indices of vertices within each triangle. (Optional)</source>
          <target state="translated">&lt;strong&gt;面&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;各三角形内の頂点のインデックス。（オプション）</target>
        </trans-unit>
        <trans-unit id="6d42958804dd0465816b20a88c367eef0dbeec80" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;factorization&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): the factorization of size</source>
          <target state="translated">&lt;strong&gt;因数分解&lt;/strong&gt;（&lt;em&gt;テンソル&lt;/em&gt;）：サイズの因数分解</target>
        </trans-unit>
        <trans-unit id="00769844b4ecc2fca40878cb1764ed386aa09fd9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;figure&lt;/strong&gt; (&lt;em&gt;matplotlib.pyplot.figure&lt;/em&gt;) &amp;ndash; Figure or a list of figures</source>
          <target state="translated">&lt;strong&gt;figure&lt;/strong&gt;（&lt;em&gt;matplotlib.pyplot.figure&lt;/em&gt;）&amp;ndash;図または図のリスト</target>
        </trans-unit>
        <trans-unit id="62d218fbe21b9a87d144357ac35b2f10bb534799" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;file_name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; path of the file in which to store the key-value pairs</source>
          <target state="translated">&lt;strong&gt;file_name&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;キーと値のペアを保存するファイルのパス</target>
        </trans-unit>
        <trans-unit id="c3c0dee521e2626c0ae3dbbaa2f17cc80f80c2a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;file_name&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; name for the downloaded file. Filename from &lt;code&gt;url&lt;/code&gt; will be used if not set.</source>
          <target state="translated">&lt;strong&gt;file_name&lt;/strong&gt;（&lt;em&gt;文字列&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;ダウンロードしたファイルの名前。設定されていない場合は、 &lt;code&gt;url&lt;/code&gt; ファイル名が使用されます。</target>
        </trans-unit>
        <trans-unit id="57269f4bfae1f1ca10e35c956f9fd6e448de9061" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;filename&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; file name to map</source>
          <target state="translated">&lt;strong&gt;filename&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;マップするファイル名</target>
        </trans-unit>
        <trans-unit id="95d303dabfba56a1b6870b6f2026c68bddc1be2f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;filename_suffix&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Suffix added to all event filenames in the log_dir directory. More details on filename construction in tensorboard.summary.writer.event_file_writer.EventFileWriter.</source>
          <target state="translated">&lt;strong&gt;filename_suffix&lt;/strong&gt;（&lt;em&gt;string&lt;/em&gt;）&amp;ndash;log_dirディレクトリ内のすべてのイベントファイル名に追加されるサフィックス。tensorboard.summary.writer.event_file_writer.EventFileWriterでのファイル名構成の詳細。</target>
        </trans-unit>
        <trans-unit id="cdb4a6b44cef668e406b3d993df10539cc0f7054" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fill_value&lt;/strong&gt; &amp;ndash; the number to fill the output tensor with.</source>
          <target state="translated">&lt;strong&gt;fill_value&lt;/strong&gt; &amp;ndash;出力テンソルを埋める数値。</target>
        </trans-unit>
        <trans-unit id="cc362ae2525be68e4bb10eb869fbbb29641c9bab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fill_value&lt;/strong&gt; (&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; the fill value</source>
          <target state="translated">&lt;strong&gt;fill_value&lt;/strong&gt;（&lt;em&gt;Scalar&lt;/em&gt;）&amp;ndash;塗りつぶし値</target>
        </trans-unit>
        <trans-unit id="81b7336ac76bc7cac5103fa2f7d2fd03ba164f85" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fill_value&lt;/strong&gt; (&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; the value to fill the output tensor with.</source>
          <target state="translated">&lt;strong&gt;fill_value&lt;/strong&gt;（&lt;em&gt;Scalar&lt;/em&gt;）&amp;ndash;出力テンソルを埋める値。</target>
        </trans-unit>
        <trans-unit id="2031b6ba05d0b93a6457f0efd37cfe00b19e577a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fill_value&lt;/strong&gt; (&lt;em&gt;scalar&lt;/em&gt;) &amp;ndash; the number to fill the output tensor with.</source>
          <target state="translated">&lt;strong&gt;fill_value&lt;/strong&gt;（&lt;em&gt;スカラー&lt;/em&gt;）&amp;ndash;出力テンソルを埋める数値。</target>
        </trans-unit>
        <trans-unit id="9096ebbb1f99e7416acc6998a05334d6d6397c19" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;find_unused_parameters&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Traverse the autograd graph from all tensors contained in the return value of the wrapped module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; function. Parameters that don&amp;rsquo;t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all &lt;code&gt;forward&lt;/code&gt; outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don&amp;rsquo;t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using &lt;code&gt;torch.Tensor.detach&lt;/code&gt;. (default: &lt;code&gt;False&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;find_unused_pa​​rameters&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;ラップされたモジュールの &lt;code&gt;forward&lt;/code&gt; 関数の戻り値に含まれるすべてのテンソルからautogradグラフをトラバースします。このグラフの一部としてグラデーションを受け取らないパラメーターは、削減の準備ができているものとして先制的にマークされます。モジュールパラメータから導出されるすべての &lt;code&gt;forward&lt;/code&gt; 出力は、損失の計算とその後の勾配計算に参加する必要があることに注意してください。そうでない場合、このラッパーは、autogradがこれらのパラメーターのグラデーションを生成するのを待ってハングします。そうでない場合は使用されないモジュールのパラメータから得られる任意の出力を用いてautogradグラフから取り外すことができる &lt;code&gt;torch.Tensor.detach&lt;/code&gt; を。 （デフォルト： &lt;code&gt;False&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="355396a4b2ab68b82f509c2d4c2526e2d2ba215d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;flush_secs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; How often, in seconds, to flush the pending events and summaries to disk. Default is every two minutes.</source>
          <target state="translated">&lt;strong&gt;flush_secs&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;保留中のイベントと要約をディスクにフラッシュする&lt;strong&gt;頻度&lt;/strong&gt;（秒単位）。デフォルトは2分ごとです。</target>
        </trans-unit>
        <trans-unit id="d5d2a692102bf116db7e2c67b38b6073ed5ae103" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fn&lt;/strong&gt; (&lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; -&amp;gt; None) &amp;ndash; function to be applied to each submodule</source>
          <target state="translated">&lt;strong&gt;fn&lt;/strong&gt;（&lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;-&amp;gt;なし）&amp;ndash;各サブモジュールに適用される関数</target>
        </trans-unit>
        <trans-unit id="ad87b9b261bd0e692ec81745b0251ab69e0c920a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fn&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; -&amp;gt; None) &amp;ndash; function to be applied to each submodule</source>
          <target state="translated">&lt;strong&gt;fn&lt;/strong&gt;（&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;-&amp;gt;なし）&amp;ndash;各サブモジュールに適用される関数</target>
        </trans-unit>
        <trans-unit id="053dcf87d7e84dd407dec8695847c296ae6c0c48" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fn&lt;/strong&gt; (&lt;code&gt;Module&lt;/code&gt; -&amp;gt; None) &amp;ndash; function to be applied to each submodule</source>
          <target state="translated">&lt;strong&gt;fn&lt;/strong&gt;（ &lt;code&gt;Module&lt;/code&gt; -&amp;gt;なし）&amp;ndash;各サブモジュールに適用される関数</target>
        </trans-unit>
        <trans-unit id="8ef37f45041ae6f49569ea35c632856c2fd1d15d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;force_reload&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to discard the existing cache and force a fresh download. Default is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;force_reload&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;既存のキャッシュを破棄して新しいダウンロードを強制するかどうか。デフォルトは &lt;code&gt;False&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="dc70a2bb2f4ed4bcf5ecccaecd8c2e734de9391d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;force_reload&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to force a fresh download of the github repo unconditionally. Does not have any effect if &lt;code&gt;source = 'local'&lt;/code&gt;. Default is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;force_reload&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;githubリポジトリの新規ダウンロードを無条件に強制するかどうか。 &lt;code&gt;source = 'local'&lt;/code&gt; 場合、効果はありません。デフォルトは &lt;code&gt;False&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="83c137929bf36e8aef1d6a6cc8426a81216a787f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;fps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Frames per second</source>
          <target state="translated">&lt;strong&gt;fps&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;フレーム/秒</target>
        </trans-unit>
        <trans-unit id="bde664f21923dc747ecb17124d60a563c8fc2b2c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;freeze&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, the tensor does not get updated in the learning process. Equivalent to &lt;code&gt;embedding.weight.requires_grad = False&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;フリーズ&lt;/strong&gt;（&lt;em&gt;ブール値&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、テンソルは学習プロセスで更新されません。 &lt;code&gt;embedding.weight.requires_grad = False&lt;/code&gt; 同等です。デフォルト： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d3fd0568363b2aa7dd7b7c7e651d3c4c84a69bed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;freeze&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, the tensor does not get updated in the learning process. Equivalent to &lt;code&gt;embeddingbag.weight.requires_grad = False&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;フリーズ&lt;/strong&gt;（&lt;em&gt;ブール値&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、テンソルは学習プロセスで更新されません。 &lt;code&gt;embeddingbag.weight.requires_grad = False&lt;/code&gt; 同等です。デフォルト： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="8ba291fa349a86b63b98c719b0de8e1e3a0b2ccd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;from&lt;/strong&gt; (&lt;em&gt;dpython:type&lt;/em&gt;) &amp;ndash; The original &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;from&lt;/strong&gt;（&lt;em&gt;dpython：type&lt;/em&gt;）&amp;ndash;元の&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="46bb4df94b1ad85e11557fbf62e5528ed469cf66" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;full&lt;/strong&gt; &amp;ndash; whether to compute full loss, i. e. to add the Stirling approximation term. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;full&lt;/strong&gt; &amp;ndash;完全な損失を計算するかどうか、つまりスターリング近似項を追加するかどうか。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="75533226793ba470c8b0b1c360399ebe87a93520" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;full&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;フル&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="65a3ef606b557f50e8baa66a8411c285745d744f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;func&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;) &amp;ndash; a callable function, such as Python callables, builtin operators (e.g. &lt;a href=&quot;generated/torch.add#torch.add&quot;&gt;&lt;code&gt;add()&lt;/code&gt;&lt;/a&gt;) and annotated TorchScript functions.</source>
          <target state="translated">&lt;strong&gt;func&lt;/strong&gt;（&lt;em&gt;callable&lt;/em&gt;）&amp;ndash; Python呼び出し可能関数、組み込み演算子（&lt;a href=&quot;generated/torch.add#torch.add&quot;&gt; &lt;code&gt;add()&lt;/code&gt; など&lt;/a&gt;）、注釈付きTorchScript関数などの呼び出し可能関数。</target>
        </trans-unit>
        <trans-unit id="eab18ced6b9533ad0e16e9973e883f1659631001" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;func&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;torch.nn.Module&lt;/a&gt;) &amp;ndash; A Python function or &lt;code&gt;torch.nn.Module&lt;/code&gt; that will be invoked. If executed in TorchScript, it will execute asynchronously, otherwise it will not. Traced invocations of fork will be captured in the IR.</source>
          <target state="translated">&lt;strong&gt;func&lt;/strong&gt;（&lt;em&gt;callable&lt;/em&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;torch.nn.Module&lt;/a&gt;）&amp;ndash;呼び出されるPython関数または &lt;code&gt;torch.nn.Module&lt;/code&gt; 。TorchScriptで実行すると、非同期で実行されます。それ以外の場合は実行されません。フォークのトレースされた呼び出しは、IRでキャプチャされます。</target>
        </trans-unit>
        <trans-unit id="1e13542d19949849cdfa6d1ca4a6a3c14a44f62f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;func&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;torch.nn.Module&lt;/a&gt;) &amp;ndash; A Python function or &lt;code&gt;torch.nn.Module&lt;/code&gt; that will be run with &lt;code&gt;example_inputs&lt;/code&gt;. &lt;code&gt;func&lt;/code&gt; arguments and return values must be tensors or (possibly nested) tuples that contain tensors. When a module is passed &lt;code&gt;torch.jit.trace&lt;/code&gt;, only the &lt;code&gt;forward&lt;/code&gt; method is run and traced (see &lt;a href=&quot;torch.jit.trace_module#torch.jit.trace_module&quot;&gt;&lt;code&gt;torch.jit.trace&lt;/code&gt;&lt;/a&gt; for details).</source>
          <target state="translated">&lt;strong&gt;FUNC&lt;/strong&gt;（&lt;em&gt;呼び出し可能&lt;/em&gt;&lt;em&gt;か&lt;/em&gt;&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;torch.nn.Module&lt;/a&gt;） - A Pythonの関数や &lt;code&gt;torch.nn.Module&lt;/code&gt; で実行されます &lt;code&gt;example_inputs&lt;/code&gt; 。 &lt;code&gt;func&lt;/code&gt; 引数と戻り値は、テンソルまたはテンソルを含む（場合によってはネストされた）タプルである必要があります。モジュールに &lt;code&gt;torch.jit.trace&lt;/code&gt; が渡されると、 &lt;code&gt;forward&lt;/code&gt; メソッドのみが実行およびトレースされます（詳細については、&lt;a href=&quot;torch.jit.trace_module#torch.jit.trace_module&quot;&gt; &lt;code&gt;torch.jit.trace&lt;/code&gt; &lt;/a&gt;を参照してください）。</target>
        </trans-unit>
        <trans-unit id="76d83916ad9f858f16111887e3f6267b2fca5fed" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;function&lt;/strong&gt; &amp;ndash; describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes &lt;code&gt;(activation, hidden)&lt;/code&gt;, &lt;code&gt;function&lt;/code&gt; should correctly use the first input as &lt;code&gt;activation&lt;/code&gt; and the second input as &lt;code&gt;hidden&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;関数&lt;/strong&gt;&amp;ndash;モデルまたはモデルの一部のフォワードパスで何を実行するかを記述します。また、タプルとして渡された入力を処理する方法も知っている必要があります。たとえば、LSTMでは、ユーザーが &lt;code&gt;(activation, hidden)&lt;/code&gt; を渡すと、 &lt;code&gt;function&lt;/code&gt; は最初の入力を &lt;code&gt;activation&lt;/code&gt; として、2番目の入力を &lt;code&gt;hidden&lt;/code&gt; として正しく使用する必要があります。</target>
        </trans-unit>
        <trans-unit id="1961467ba2825e1c559db8b59ff1bdbcbb0ec039" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;functions&lt;/strong&gt; &amp;ndash; A &lt;a href=&quot;generated/torch.nn.sequential#torch.nn.Sequential&quot;&gt;&lt;code&gt;torch.nn.Sequential&lt;/code&gt;&lt;/a&gt; or the list of modules or functions (comprising the model) to run sequentially.</source>
          <target state="translated">&lt;strong&gt;関数&lt;/strong&gt;&amp;ndash; &lt;a href=&quot;generated/torch.nn.sequential#torch.nn.Sequential&quot;&gt; &lt;code&gt;torch.nn.Sequential&lt;/code&gt; 、&lt;/a&gt;または順次実行するモジュールまたは関数（モデルを含む）のリスト。</target>
        </trans-unit>
        <trans-unit id="2fb5cc79b41c4d978a0a328aa05327cc1e203720" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;functions&lt;/strong&gt; &amp;ndash; A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).</source>
          <target state="translated">&lt;strong&gt;関数&lt;/strong&gt;&amp;ndash;関数バインディングを生成する関数名のリスト。辞書が指定されている場合は、関数名をdocstring（それ以外の場合は関数名のみ）にマップする必要があります。</target>
        </trans-unit>
        <trans-unit id="cb85d27a8748aad626d3af5de9e7af60b244f74d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;futures&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; a list of &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object.</source>
          <target state="translated">&lt;strong&gt;futures&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;）&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;オブジェクトのリスト。</target>
        </trans-unit>
        <trans-unit id="7a7221601ceeb930d343f7319c8db14fe03b3e16" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;futures&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; a list of &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects.</source>
          <target state="translated">&lt;strong&gt;futures&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;）&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;オブジェクトのリスト。</target>
        </trans-unit>
        <trans-unit id="ba355c7f4beca0e0d4bf68bb3971593069c56322" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gain&lt;/strong&gt; &amp;ndash; an optional scaling factor</source>
          <target state="translated">&lt;strong&gt;ゲイン&lt;/strong&gt;&amp;ndash;オプションの倍率</target>
        </trans-unit>
        <trans-unit id="b04565ecc2229736f50e2b26ed78404ea530d53b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gain&lt;/strong&gt; &amp;ndash; optional scaling factor</source>
          <target state="translated">&lt;strong&gt;ゲイン&lt;/strong&gt;&amp;ndash;オプションの倍率</target>
        </trans-unit>
        <trans-unit id="b03754a8da7ef008224ed892c921fb9371f0c076" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gather_list&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)</source>
          <target state="translated">&lt;strong&gt;collect_list&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list &lt;/a&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;収集されたデータに使用する適切なサイズのテンソルのリスト（デフォルトはNone、宛先ランクで指定する必要があります）</target>
        </trans-unit>
        <trans-unit id="f5579060ab76636341f5dfd97c022a89f302b7cf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;generator&lt;/strong&gt; (&lt;a href=&quot;torch.generator#torch.Generator&quot;&gt;&lt;code&gt;torch.Generator&lt;/code&gt;&lt;/a&gt;, optional) &amp;ndash; a pseudorandom number generator for sampling</source>
          <target state="translated">&lt;strong&gt;ジェネレーター&lt;/strong&gt;（&lt;a href=&quot;torch.generator#torch.Generator&quot;&gt; &lt;code&gt;torch.Generator&lt;/code&gt; &lt;/a&gt;、オプション）&amp;ndash;サンプリング用の疑似乱数ジェネレーター</target>
        </trans-unit>
        <trans-unit id="06014c6b4c4693a7e3fbaf2b2a37ca8411e8162c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;get_infos&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if set to &lt;code&gt;True&lt;/code&gt;, returns an info IntTensor. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;get_infos&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; に設定されている場合、情報IntTensorを返します。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="713f3c8adc9b7748672962092ba1c2254277918b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;github&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; a string with format &amp;ldquo;repo_owner/repo_name[:tag_name]&amp;rdquo; with an optional tag/branch. The default branch is &lt;code&gt;master&lt;/code&gt; if not specified. Example: &amp;lsquo;pytorch/vision[:hub]&amp;rsquo;</source>
          <target state="translated">&lt;strong&gt;github&lt;/strong&gt;（&lt;em&gt;string&lt;/em&gt;）&amp;ndash;「repo_owner / repo_name [：tag_name]」形式の文字列とオプションのタグ/ブランチ。指定されていない場合、デフォルトのブランチは &lt;code&gt;master&lt;/code&gt; です。例： 'pytorch / vision [：hub]'</target>
        </trans-unit>
        <trans-unit id="270e6272159af8294641789a5ae5029a0175a0a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;github&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; a string with format &amp;lt;repo_owner/repo_name[:tag_name]&amp;gt; with an optional tag/branch. The default branch is &lt;code&gt;master&lt;/code&gt; if not specified. Example: &amp;lsquo;pytorch/vision[:hub]&amp;rsquo;</source>
          <target state="translated">&lt;strong&gt;github&lt;/strong&gt;（&lt;em&gt;string&lt;/em&gt;）&amp;ndash;オプションのタグ/ブランチを含む&amp;lt;repo_owner / repo_name [：tag_name]&amp;gt;形式の文字列。指定されていない場合、デフォルトのブランチは &lt;code&gt;master&lt;/code&gt; です。例： 'pytorch / vision [：hub]'</target>
        </trans-unit>
        <trans-unit id="34a7ff872bd23abb0450fda48dac267f7a245b6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;global_step&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Global step value to record</source>
          <target state="translated">&lt;strong&gt;global_step&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;記録するグローバルステップ値</target>
        </trans-unit>
        <trans-unit id="59b52c7e39a474423f9fda9eb51f75f802a681a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;graceful&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether to do a graceful shutdown or not. If True, this will 1) wait until there is no pending system messages for &lt;code&gt;UserRRefs&lt;/code&gt; and delete them; 2) block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete.</source>
          <target state="translated">&lt;strong&gt;graceful&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;グレースフルシャットダウンを実行するかどうか。Trueの場合、これは1） &lt;code&gt;UserRRefs&lt;/code&gt; 保留中のシステムメッセージがなくなるまで待機し、それらを削除します。2）すべてのローカルおよびリモートRPCプロセスがこのメソッドに到達するまでブロックし、すべての未処理の作業が完了するのを待ちます。</target>
        </trans-unit>
        <trans-unit id="799013c067a407e423f9d42197eba6a2384dd6a4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gradient&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/constants.html#None&quot;&gt;None&lt;/a&gt;) &amp;ndash; Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless &lt;code&gt;create_graph&lt;/code&gt; is True. None values can be specified for scalar Tensors or ones that don&amp;rsquo;t require grad. If a None value would be acceptable then this argument is optional.</source>
          <target state="translated">&lt;strong&gt;勾配&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/constants.html#None&quot;&gt;なし&lt;/a&gt;）&amp;ndash;テンソルの&lt;strong&gt;勾配&lt;/strong&gt;。テンソルの場合、 &lt;code&gt;create_graph&lt;/code&gt; がTrueでない限り、gradを必要としないテンソルに自動的に変換されます。スカラーテンソルまたはgradを必要としないテンソルには値を指定できません。None値が受け入れられる場合、この引数はオプションです。</target>
        </trans-unit>
        <trans-unit id="b29df415efb0eea8f8662bee6b939588b9b78efb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;gradient_as_bucket_view&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; This is a prototype feature and subject to changes. When set to &lt;code&gt;True&lt;/code&gt;, gradients will be views pointing to different offsets of &lt;code&gt;allreduce&lt;/code&gt; communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and &lt;code&gt;allreduce&lt;/code&gt; communication buckets. When gradients are views, &lt;code&gt;detach_()&lt;/code&gt; cannot be called on the gradients. If hitting such errors, please fix it by referring to the &lt;a href=&quot;../optim#torch.optim.Optimizer.zero_grad&quot;&gt;&lt;code&gt;zero_grad()&lt;/code&gt;&lt;/a&gt; function in &lt;code&gt;torch/optim/optimizer.py&lt;/code&gt; as a solution.</source>
          <target state="translated">&lt;strong&gt;gradient_as_bucket_view&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;これはプロトタイプ機能であり、変更される可能性があります。 &lt;code&gt;True&lt;/code&gt; に設定すると、グラデーションは &lt;code&gt;allreduce&lt;/code&gt; 通信バケットのさまざまなオフセットを指すビューになります。これにより、保存されたメモリサイズがグラデーションの合計サイズと等しくなるピークメモリ使用量を減らすことができます。さらに、グラデーション間のコピーのオーバーヘッドを回避し、通信バケットを &lt;code&gt;allreduce&lt;/code&gt; ます。グラデーションがビューの場合、グラデーションで &lt;code&gt;detach_()&lt;/code&gt; を呼び出すことはできません。このようなエラーが発生した場合は、解決策として &lt;code&gt;torch/optim/optimizer.py&lt;/code&gt; &lt;a href=&quot;../optim#torch.optim.Optimizer.zero_grad&quot;&gt; &lt;code&gt;zero_grad()&lt;/code&gt; &lt;/a&gt;関数を参照して修正してください。</target>
        </trans-unit>
        <trans-unit id="b2b0bcce70df29844281ac614b0f8a39a0dfcd19" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;grid&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; flow-field of shape</source>
          <target state="translated">&lt;strong&gt;グリッド&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;形状の流れ場</target>
        </trans-unit>
        <trans-unit id="4db1041ae201d7e927245f12103bcee3afeae410" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group&lt;/strong&gt; (&lt;em&gt;ProcessGroup&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The process group to work on</source>
          <target state="translated">&lt;strong&gt;group&lt;/strong&gt;（&lt;em&gt;ProcessGroup &lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;作業するプロセスグループ</target>
        </trans-unit>
        <trans-unit id="1110361e162f3be4aa9b1a7cde1a491603596b93" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group&lt;/strong&gt; (&lt;em&gt;ProcessGroup&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The process group to work on.</source>
          <target state="translated">&lt;strong&gt;group&lt;/strong&gt;（&lt;em&gt;ProcessGroup &lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;作業するプロセスグループ。</target>
        </trans-unit>
        <trans-unit id="6db70a258153314cde5304ac47d562305e99552a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group&lt;/strong&gt; (&lt;em&gt;ProcessGroup&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of &lt;code&gt;group&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;group&lt;/strong&gt;（&lt;em&gt;ProcessGroup &lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;作業するプロセスグループ。デフォルトは、一般的なメインプロセスグループです。別の特定のグループが指定されている場合、呼び出しプロセスは &lt;code&gt;group&lt;/code&gt; 一部である必要があります。</target>
        </trans-unit>
        <trans-unit id="85aa3c312919cb9842e6de299fa1fb86a7f7ea49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;group_name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;deprecated&lt;/em&gt;) &amp;ndash; Group name.</source>
          <target state="translated">&lt;strong&gt;group_name&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;非推奨&lt;/em&gt;）&amp;ndash;グループ名。</target>
        </trans-unit>
        <trans-unit id="f26fa12d9d61e6585a36de452da97d2ffce8b34d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;groups&lt;/strong&gt; &amp;ndash; split input into groups,</source>
          <target state="translated">&lt;strong&gt;グループ&lt;/strong&gt;&amp;ndash;入力をグループに分割します。</target>
        </trans-unit>
        <trans-unit id="81c8b7e8acb12c23d521a687adf35816ef4ff39c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;groups&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Number of blocked connections from input channels to output channels. Default: 1</source>
          <target state="translated">&lt;strong&gt;groups&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;入力チャネルから出力チャネルへのブロックされた接続の数。デフォルト：1</target>
        </trans-unit>
        <trans-unit id="3936cb9ca5f3782dc3736b1eb2710d73d6c56d73" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;groups&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; number of groups in the conv layer (default: 1)</source>
          <target state="translated">&lt;strong&gt;groups&lt;/strong&gt;（&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; convレイヤー内のグループの数（デフォルト：1）</target>
        </trans-unit>
        <trans-unit id="c9313fa876c33968ef38e2bc5ed5adcaf010be99" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h&amp;rsquo;&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the next hidden state for each element in the batch</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状の&lt;strong&gt;h ' &lt;/strong&gt; &lt;code&gt;(batch, hidden_size)&lt;/code&gt; ：バッチ内の各要素の次の非表示状態を含むテンソル</target>
        </trans-unit>
        <trans-unit id="1592eedf3bb9a47887176fd782c28510508c7820" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_0&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the initial hidden state for each element in the batch.</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状の&lt;strong&gt;h_0 &lt;/strong&gt; &lt;code&gt;(batch, hidden_size)&lt;/code&gt; ：バッチ内の各要素の初期の非表示状態を含むテンソル。</target>
        </trans-unit>
        <trans-unit id="cfde5becd622ec62656a4cff95b1913fb5911358" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_0&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</source>
          <target state="translated">&lt;strong&gt;H_0&lt;/strong&gt;形状の &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt; ：テンソルは、バッチ内の各素子の初期隠れ状態を含みます。指定しない場合、デフォルトはゼロです。RNNが双方向の場合、num_directionsは2である必要があり、そうでない場合は1である必要があります。</target>
        </trans-unit>
        <trans-unit id="a088baa848abf407fdf324f68a7fc23226f3bfd9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_0&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1.</source>
          <target state="translated">&lt;strong&gt;H_0&lt;/strong&gt;形状の &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt; ：テンソルは、バッチ内の各素子の初期隠れ状態を含みます。LSTMが双方向の場合、num_directionsは2である必要があり、そうでない場合は1である必要があります。</target>
        </trans-unit>
        <trans-unit id="e0e30ccc3d1652af234e0543d06149d194f40bc0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_1&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the next hidden state for each element in the batch</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状の&lt;strong&gt;h_1 &lt;/strong&gt; &lt;code&gt;(batch, hidden_size)&lt;/code&gt; ：バッチ内の各要素の次の非表示状態を含むテンソル</target>
        </trans-unit>
        <trans-unit id="e1c9ba2b299c711fcc8d467de3dea8569b50a3fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_n&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the hidden state for &lt;code&gt;t = seq_len&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;h_n&lt;/strong&gt;形状の &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt; ：テンソルのための隠れ状態を含む &lt;code&gt;t = seq_len&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3525fd2429c6604b476c82ddd74f59209ab301ef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;h_n&lt;/strong&gt; of shape &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt;: tensor containing the hidden state for &lt;code&gt;t = seq_len&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;h_n&lt;/strong&gt;形状の &lt;code&gt;(num_layers * num_directions, batch, hidden_size)&lt;/code&gt; ：テンソルのための隠れ状態を含む &lt;code&gt;t = seq_len&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ba61e8234e80482584a34f381f7534bd9ba1d290" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hard&lt;/strong&gt; &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the returned samples will be discretized as one-hot vectors, but will be differentiated as if it is the soft sample in autograd</source>
          <target state="translated">&lt;strong&gt;ハード&lt;/strong&gt;&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、返されるサンプルはワンホットベクトルとして離散化されますが、autogradのソフトサンプルであるかのように区別されます。</target>
        </trans-unit>
        <trans-unit id="aaa96a08f23e0f6a631877636b216cb602e637fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hash_prefix&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If not None, the SHA256 downloaded file should start with &lt;code&gt;hash_prefix&lt;/code&gt;. Default: None</source>
          <target state="translated">&lt;strong&gt;hash_prefix&lt;/strong&gt;（&lt;em&gt;文字列&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; Noneでない場合、SHA256ダウンロードファイルは &lt;code&gt;hash_prefix&lt;/code&gt; で始まる必要があります。デフォルト：なし</target>
        </trans-unit>
        <trans-unit id="b66c30a2662d014769668efe291e7362c5d7d9db" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;head_bias&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, adds a bias term to the &amp;lsquo;head&amp;rsquo; of the adaptive softmax. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;head_bias&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合&lt;em&gt;、&lt;/em&gt;&lt;strong&gt;適応ソフトマックス&lt;/strong&gt;の「ヘッド」にバイアス項を追加します。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d30e56690f484d3c8d7a2b255db4bcbd77445f68" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hidden&lt;/strong&gt; of shape &lt;code&gt;(batch, hidden_size)&lt;/code&gt;: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状の&lt;strong&gt;非表示&lt;/strong&gt; &lt;code&gt;(batch, hidden_size)&lt;/code&gt; ：バッチ内の各要素の初期非表示状態を含むテンソル。指定しない場合、デフォルトはゼロです。</target>
        </trans-unit>
        <trans-unit id="e67734f0b9cf7c79bc74ff13156c666a023e5f41" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hidden_size&lt;/strong&gt; &amp;ndash; The number of features in the hidden state &lt;code&gt;h&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;hidden_​​size&lt;/strong&gt; &amp;ndash;非表示状態のフィーチャの数 &lt;code&gt;h&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3705e6bc9a6ecd869a7951f814f9fed46d9ff630" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;high&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; One above the highest integer to be drawn from the distribution.</source>
          <target state="translated">&lt;strong&gt;high&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;分布から引き出される最大の整数の1つ上。</target>
        </trans-unit>
        <trans-unit id="c19ea35377296f18df50a468ab31eff648558a09" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hop_length&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the distance between neighboring sliding window frames. Default: &lt;code&gt;None&lt;/code&gt; (treated as equal to &lt;code&gt;floor(n_fft / 4)&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;hop_length&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;隣接するスライディングウィンドウフレーム間の距離。デフォルト： &lt;code&gt;None&lt;/code&gt; （ &lt;code&gt;floor(n_fft / 4)&lt;/code&gt; と等しいものとして扱われます）</target>
        </trans-unit>
        <trans-unit id="d1396026b4c7368bc625f59b4932fadbbf084c72" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hop_length&lt;/strong&gt; (&lt;em&gt;Optional&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; The distance between neighboring sliding window frames. (Default: &lt;code&gt;n_fft // 4&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;hop_length&lt;/strong&gt;（&lt;em&gt;オプション&lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;隣接するスライディングウィンドウフレーム間の距離。（デフォルト： &lt;code&gt;n_fft // 4&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="6876910fda4d6ecccd2051aba33481c94d9368b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;host_name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The hostname or IP Address the server store should run on.</source>
          <target state="translated">&lt;strong&gt;host_name&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;サーバーストアを実行するホスト名またはIPアドレス。</target>
        </trans-unit>
        <trans-unit id="94284103cfc33f19f8a61eebc94176be0db80646" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hparam_dict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; Each key-value pair in the dictionary is the name of the hyper parameter and it&amp;rsquo;s corresponding value. The type of the value can be one of &lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;string&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt;, or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;hparam_dict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;）&amp;ndash;ディクショナリ内の各キーと値のペアは、ハイパーパラメータの名前とそれに対応する値です。値のタイプは、 &lt;code&gt;bool&lt;/code&gt; 、 &lt;code&gt;string&lt;/code&gt; 、 &lt;code&gt;float&lt;/code&gt; 、 &lt;code&gt;int&lt;/code&gt; 、または &lt;code&gt;None&lt;/code&gt; のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="a1b563f2476f046122b214d6a9600f12ff9315b6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;hparam_domain_discrete&lt;/strong&gt; &amp;ndash; (Optional[Dict[str, List[Any]]]) A dictionary that contains names of the hyperparameters and all discrete values they can hold</source>
          <target state="translated">&lt;strong&gt;hparam_domain_discrete&lt;/strong&gt; &amp;ndash;（オプション[Dict [str、List [Any]]]）ハイパーパラメーターの名前とそれらが保持できるすべての離散値を含む辞書</target>
        </trans-unit>
        <trans-unit id="98aaeaa4225400ecd738ec2048d131fe08b1d564" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;iK&lt;/strong&gt; (&lt;em&gt;tensor&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the input tensor of size</source>
          <target state="translated">&lt;strong&gt;iK&lt;/strong&gt;（&lt;em&gt;テンソル&lt;/em&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;サイズの入力テンソル</target>
        </trans-unit>
        <trans-unit id="e423e9ecf3beffc0eedb4329822c8534728b01ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ignore_index&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies a target value that is ignored and does not contribute to the input gradient. When &lt;code&gt;size_average&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the loss is averaged over non-ignored targets.</source>
          <target state="translated">&lt;strong&gt;ignore_index&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;無視され、入力勾配に寄与しないターゲット値を指定します。とき &lt;code&gt;size_average&lt;/code&gt; がある &lt;code&gt;True&lt;/code&gt; 、損失が非無視目標で平均化されます。</target>
        </trans-unit>
        <trans-unit id="607b11f999212773791595ce05953927928c08a6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ignore_index&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies a target value that is ignored and does not contribute to the input gradient. When &lt;code&gt;size_average&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the loss is averaged over non-ignored targets. Default: -100</source>
          <target state="translated">&lt;strong&gt;ignore_index&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;無視され、入力勾配に寄与しないターゲット値を指定します。とき &lt;code&gt;size_average&lt;/code&gt; がある &lt;code&gt;True&lt;/code&gt; 、損失が非無視目標で平均化されます。デフォルト：-100</target>
        </trans-unit>
        <trans-unit id="c096d38fe26de10e968164a6aecd6f913177ea6e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;imag&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The imaginary part of the complex tensor. Must be same dtype as &lt;a href=&quot;torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;imag&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;複雑なテンソルの虚数部。&lt;a href=&quot;torch.real#torch.real&quot;&gt; &lt;code&gt;real&lt;/code&gt; &lt;/a&gt;dtypeと同じである必要があります。</target>
        </trans-unit>
        <trans-unit id="c3a96d8bd554be6eb7797de17442ad6cb99464a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;img_tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;numpy.array&lt;/em&gt;&lt;em&gt;, or &lt;/em&gt;&lt;em&gt;string/blobname&lt;/em&gt;) &amp;ndash; Image data</source>
          <target state="translated">&lt;strong&gt;img_tensor&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;numpy.array &lt;/em&gt;&lt;em&gt;、または&lt;/em&gt;&lt;em&gt;string / blobname&lt;/em&gt;）&amp;ndash;画像データ</target>
        </trans-unit>
        <trans-unit id="9e075b9a8dfd668778490588e84bb0c9bbb577a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in1_features&lt;/strong&gt; &amp;ndash; size of each first input sample</source>
          <target state="translated">&lt;strong&gt;in1_features&lt;/strong&gt; &amp;ndash;各最初の入力サンプルのサイズ</target>
        </trans-unit>
        <trans-unit id="99c9afda0be0c905049d69c6c60279520769cef1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in2_features&lt;/strong&gt; &amp;ndash; size of each second input sample</source>
          <target state="translated">&lt;strong&gt;in2_features&lt;/strong&gt; &amp;ndash;各秒の入力サンプルのサイズ</target>
        </trans-unit>
        <trans-unit id="054cbce99ed259e05737255fcff3ec1a33f17b30" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in_channels&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of channels in the input image</source>
          <target state="translated">&lt;strong&gt;in_channels&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;入力画像のチャンネル数</target>
        </trans-unit>
        <trans-unit id="031258926af2ce0c91f35de69ae4829af92fe5cc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in_features&lt;/strong&gt; &amp;ndash; size of each input sample</source>
          <target state="translated">&lt;strong&gt;in_features&lt;/strong&gt; &amp;ndash;各入力サンプルのサイズ</target>
        </trans-unit>
        <trans-unit id="e1cd84155beb6cb4e4e554a0fb3f34f7def9bf3d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;in_features&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of features in the input tensor</source>
          <target state="translated">&lt;strong&gt;in_features&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;入力テンソルの特徴の数</target>
        </trans-unit>
        <trans-unit id="de92c22d2099710d576bc540cf0e439bfe62dc1b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;include_last_offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;include_last_offset&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;モジュール初期化のドキュメントを参照してください。デフォルト： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6c77074d3fbe6ed19bba6e5a032d17feba927877" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;include_last_offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;offsets&lt;/code&gt; has one additional element, where the last element is equivalent to the size of &lt;code&gt;indices&lt;/code&gt;. This matches the CSR format.</source>
          <target state="translated">&lt;strong&gt;include_last_offset&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、 &lt;code&gt;offsets&lt;/code&gt; には1つの追加要素があり、最後の要素は &lt;code&gt;indices&lt;/code&gt; のサイズに相当します。これはCSRフォーマットと一致します。</target>
        </trans-unit>
        <trans-unit id="adc8afe3a34cbeea7690cc1b8d46533e74feca80" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;include_last_offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, the size of offsets is equal to the number of bags + 1.</source>
          <target state="translated">&lt;strong&gt;include_last_offset&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;True&lt;/code&gt; の場合、オフセットのサイズはバッグの数+1に等しくなります。</target>
        </trans-unit>
        <trans-unit id="0058952dc92b6430936a12a9320099bae5295fc6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;increasing&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Order of the powers of the columns. If True, the powers increase from left to right, if False (the default) they are reversed.</source>
          <target state="translated">&lt;strong&gt;増加&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;ブール&lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;列の累乗の順序。Trueの場合、累乗は左から右に増加し、False（デフォルト）の場合、累乗は逆になります。</target>
        </trans-unit>
        <trans-unit id="d624060723fbf002e4598220b5812e19f26a17a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; index to insert.</source>
          <target state="translated">&lt;strong&gt;index&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;挿入するインデックス。</target>
        </trans-unit>
        <trans-unit id="5d2ab5af76f511772ee4673ede1ce34b868d76a5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the index to select with</source>
          <target state="translated">&lt;strong&gt;index&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;選択するインデックス</target>
        </trans-unit>
        <trans-unit id="35fb618fa2c424e1e9473beb050f9175521a3241" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; indices of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; to select from</source>
          <target state="translated">&lt;strong&gt;index&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;選択する&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;インデックス</target>
        </trans-unit>
        <trans-unit id="67e5b1c26904309bcbc72d6a5206411049f25fa6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; indices of &lt;code&gt;self&lt;/code&gt; tensor to fill in</source>
          <target state="translated">&lt;strong&gt;index&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;入力する &lt;code&gt;self&lt;/code&gt; テンソルのインデックス</target>
        </trans-unit>
        <trans-unit id="c8699c1c42412268aa01357837b44ae957ac901c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the 1-D tensor containing the indices to index</source>
          <target state="translated">&lt;strong&gt;index&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;インデックスを作成するインデックスを含む1次元テンソル</target>
        </trans-unit>
        <trans-unit id="3b1088f3894e3a3aade31aa3da2f7cbaa1c7f3c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices of elements to gather</source>
          <target state="translated">&lt;strong&gt;index&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;収集する要素のインデックス</target>
        </trans-unit>
        <trans-unit id="7246e33903dd34811bb50beca86c9f179b0ea261" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices of elements to scatter and add, can be either empty or the same size of src. When empty, the operation returns identity.</source>
          <target state="translated">&lt;strong&gt;index&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;分散および追加する要素のインデックスは、空にすることも、同じサイズのsrcにすることもできます。空の場合、操作はIDを返します。</target>
        </trans-unit>
        <trans-unit id="7891bb82e5f646ecc5571231eb40487d6adaedf7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;index&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices of elements to scatter, can be either empty or the same size of src. When empty, the operation returns identity</source>
          <target state="translated">&lt;strong&gt;index&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;分散する要素のインデックスは、空にすることも、同じサイズのsrcにすることもできます。空の場合、操作はIDを返します</target>
        </trans-unit>
        <trans-unit id="4489c58da0a616c049ed6d93d7f17fd55991d97a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices into self</source>
          <target state="translated">&lt;strong&gt;インデックス&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;自己へのインデックス</target>
        </trans-unit>
        <trans-unit id="e5be31c40930fbda8ef6fadbd69d27440a0a29e3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; the indices into tensor</source>
          <target state="translated">&lt;strong&gt;インデックス&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;テンソルへのインデックス</target>
        </trans-unit>
        <trans-unit id="4f340c71c41bf4a8b6f0ee0a633f1e707d60c41e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;array_like&lt;/em&gt;) &amp;ndash; Initial data for the tensor. Can be a list, tuple, NumPy &lt;code&gt;ndarray&lt;/code&gt;, scalar, and other types. Will be cast to a &lt;code&gt;torch.LongTensor&lt;/code&gt; internally. The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.</source>
          <target state="translated">&lt;strong&gt;index&lt;/strong&gt;（&lt;em&gt;array_like&lt;/em&gt;）&amp;ndash;テンソルの初期データ。リスト、タプル、NumPy &lt;code&gt;ndarray&lt;/code&gt; 、スカラー、およびその他のタイプにすることができます。内部で &lt;code&gt;torch.LongTensor&lt;/code&gt; にキャストされます。インデックスは行列内の非ゼロ値の座標であるため、2次元である必要があります。ここで、最初の次元はテンソル次元の数であり、2番目の次元は非ゼロ値の数です。</target>
        </trans-unit>
        <trans-unit id="bef6961ff826304ba68b7564bb99ad681fdaf697" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;indices&lt;/strong&gt; (&lt;em&gt;tuple of LongTensor&lt;/em&gt;) &amp;ndash; tensors used to index into &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;インデックス&lt;/strong&gt;（&lt;em&gt;LongTensorのタプル&lt;/em&gt;） -へのインデックスに使用テンソル &lt;code&gt;self&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d19cbf8a68fcf3b4f455172ecef9b16a565a113c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;infos&lt;/strong&gt; (&lt;em&gt;IntTensor&lt;/em&gt;, &lt;em&gt;optional&lt;/em&gt;): if &lt;code&gt;get_infos&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, this is a tensor of size</source>
          <target state="translated">&lt;strong&gt;infos&lt;/strong&gt;（&lt;em&gt;IntTensor&lt;/em&gt;、&lt;em&gt;オプション&lt;/em&gt;）： &lt;code&gt;get_infos&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; の場合、これはサイズのテンソルです</target>
        </trans-unit>
        <trans-unit id="71c1dcc99d861730834f2921fb9ea642ee26387c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;init&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the initial value of</source>
          <target state="translated">&lt;strong&gt;init&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;の初期値</target>
        </trans-unit>
        <trans-unit id="3387ec2d138470faa0b9b1e76a459e501c664a81" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;init_method&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The URL to initialize &lt;code&gt;ProcessGroupGloo&lt;/code&gt; (default: &lt;code&gt;env://&lt;/code&gt;).</source>
          <target state="translated">&lt;strong&gt;init_method&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash; &lt;code&gt;ProcessGroupGloo&lt;/code&gt; を初期化するためのURL （デフォルト： &lt;code&gt;env://&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="da9829504ae0afe64db4aa47b65791dfaa083f83" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;init_method&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The URL to initialize the distributed store used for rendezvous. It takes any value accepted for the same argument of &lt;a href=&quot;distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; (default: &lt;code&gt;env://&lt;/code&gt;).</source>
          <target state="translated">&lt;strong&gt;init_method&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;ランデブーに使用される分散ストアを初期化するためのURL。&lt;a href=&quot;distributed#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; の&lt;/a&gt;同じ引数に対して受け入れられた任意の値を取ります（デフォルト： &lt;code&gt;env://&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="f325478530b5cf14071a5c700f82577ec5ba84d8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;init_method&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; URL specifying how to initialize the process group. Default is &amp;ldquo;env://&amp;rdquo; if no &lt;code&gt;init_method&lt;/code&gt; or &lt;code&gt;store&lt;/code&gt; is specified. Mutually exclusive with &lt;code&gt;store&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;init_method&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;）&amp;ndash;プロセスグループを初期化する方法を指定するURL。 &lt;code&gt;init_method&lt;/code&gt; または &lt;code&gt;store&lt;/code&gt; が指定されていない場合、デフォルトは「env：//」です。 &lt;code&gt;store&lt;/code&gt; と相互に排他的です。</target>
        </trans-unit>
        <trans-unit id="9fa891f80017cd81f628a35707cc9e6065a280f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; (Currently not supported) can optionally do the operation in-place.</source>
          <target state="translated">&lt;strong&gt;インプレース&lt;/strong&gt;- （現在サポートされていない）は、必要に応じて動作インプレースを行うことができます。</target>
        </trans-unit>
        <trans-unit id="df1cc134d4a1280a08aae8069753847bc753ab08" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; If set to &lt;code&gt;True&lt;/code&gt;, will do this operation in-place. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;インプレース&lt;/strong&gt;-に設定されている場合 &lt;code&gt;True&lt;/code&gt; 、この操作のインプレースを行います。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="8a0af1cd98e89baa21a0964493ae5197db8d39c0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; can optionally do the operation in-place. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;インプレースは&lt;/strong&gt;-必要に応じて動作インプレースを行うことができます。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="75aed03563c53dd2d0764eb5453c6ca6e4140d83" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; &amp;ndash; perform the computation inplace</source>
          <target state="translated">&lt;strong&gt;インプレース&lt;/strong&gt;&amp;ndash;&lt;strong&gt;インプレースで&lt;/strong&gt;計算を実行します</target>
        </trans-unit>
        <trans-unit id="fdf58b63be55a4b22c46032d3fd1d61d0bd927a2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If set to &lt;code&gt;True&lt;/code&gt;, will do this operation in-place</source>
          <target state="translated">&lt;strong&gt;インプレース&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;ブール値&lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;） -に設定した場合 &lt;code&gt;True&lt;/code&gt; 、この操作のインプレースを行います</target>
        </trans-unit>
        <trans-unit id="2baabf2051ecce4e59ed7868711be86074a1121b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;inplace&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; can optionally do the operation in-place. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;インプレース&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;BOOL &lt;/a&gt;&lt;em&gt;、&lt;/em&gt;&lt;em&gt;オプション&lt;/em&gt;） -必要に応じて操作インプレースを行うことができます。デフォルト： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2ca73aff43bbd335b880e2e8b6461721261c7efd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input matrix</source>
          <target state="translated">&lt;strong&gt;input2&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;入力行列</target>
        </trans-unit>
        <trans-unit id="7cb4517ba198e148817291a746b67bb3359b3275" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the &lt;code&gt;tau&lt;/code&gt; from &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;input2&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash; &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;からの &lt;code&gt;tau&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="14436103f8610b62c5af0d3f52320c0e695e6561" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input3&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the matrix to be multiplied.</source>
          <target state="translated">&lt;strong&gt;input3&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;乗算される行列。</target>
        </trans-unit>
        <trans-unit id="189471ee9fd0ae26f96b12a2f14fabdfdb2a55c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="2d43b3b03e5e1cc0bc29a4d3ee11d035641542b7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; A Tensor that is input to &lt;code&gt;functions&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; &lt;code&gt;functions&lt;/code&gt; 入力されるテンソル</target>
        </trans-unit>
        <trans-unit id="3478ec4abd0f0da74946e0059075f92ae16838f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; Tensor of arbitrary shape</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;&amp;ndash;任意の形状のテンソル</target>
        </trans-unit>
        <trans-unit id="64075621f7b49ac8cf3466ffee84214ef27b7e0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; expectation of underlying Poisson distribution.</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;&amp;ndash;基礎となるポアソン分布の期待値。</target>
        </trans-unit>
        <trans-unit id="e5c45d37ad1ca2bfd9ba95fc0c1c9aba750297da" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; input tensor</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;&amp;ndash;入力テンソル</target>
        </trans-unit>
        <trans-unit id="fefd9a33978ca250fb3687e571631fed568e2f69" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; input tensor of any shape</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;&amp;ndash;任意の形状の入力テンソル</target>
        </trans-unit>
        <trans-unit id="32aa8fde0d3b37a5fd0bf094d226ea96d65d5b3e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; input tensor of shape</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;&amp;ndash;形状の入力テンソル</target>
        </trans-unit>
        <trans-unit id="1e71ece563e570f0828230ce4079bf1d6a1f9bfd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; quantized input</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;&amp;ndash;量子化された入力</target>
        </trans-unit>
        <trans-unit id="324d485d4d64467111174a180760e99e40ae34f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; quantized input tensor</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;&amp;ndash;量子化された入力テンソル</target>
        </trans-unit>
        <trans-unit id="1052a12b95427668122c4a5782e7d1c6ced7fbb7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; quantized input tensor of shape</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt; &amp;ndash;形状の量子化された入力テンソル</target>
        </trans-unit>
        <trans-unit id="280fc2140640a681bb0a76e2eacb285f4f40a9a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; &amp;ndash; the first input tensor</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt; &amp;ndash;最初の入力テンソル</target>
        </trans-unit>
        <trans-unit id="7c5a3515f7ca1c5139e9f3360aa15d38e06b1c58" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; an input Tensor</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力Tensor</target>
        </trans-unit>
        <trans-unit id="c0b781ff3e115b7562060e3796452984aec9e1bc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1-D input vector</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;1次元入力ベクトル</target>
        </trans-unit>
        <trans-unit id="7e381244bd054e5229323f3fd533dcf76ded11c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1-d int tensor</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash; 1-dintテンソル</target>
        </trans-unit>
        <trans-unit id="318151f5ec7778ddab17a5a32f68c531c8b678c8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1D vector.</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;1Dベクトル。</target>
        </trans-unit>
        <trans-unit id="302e5c41a337c972f93ff08bf742872e6882489e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Must be at least 1-dimensional.</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;少なくとも1次元である必要があります。</target>
        </trans-unit>
        <trans-unit id="9cff6b01a05d93e056d3e7d25e27d95e0d5b44d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Must be at least 2-dimensional.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;少なくとも2次元である必要があります。</target>
        </trans-unit>
        <trans-unit id="508cdbc779d4073e5eafa0ff8836504797cfe9a3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The input tensor of size</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;サイズの入力テンソル</target>
        </trans-unit>
        <trans-unit id="11314dbf954d397fca1f11546d1c56bacc29ebd7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The input tensor. Expected to be output of &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;stft()&lt;/code&gt;&lt;/a&gt;, can either be complex (&lt;code&gt;channel&lt;/code&gt;, &lt;code&gt;fft_size&lt;/code&gt;, &lt;code&gt;n_frame&lt;/code&gt;), or real (&lt;code&gt;channel&lt;/code&gt;, &lt;code&gt;fft_size&lt;/code&gt;, &lt;code&gt;n_frame&lt;/code&gt;, 2) where the &lt;code&gt;channel&lt;/code&gt; dimension is optional.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力テンソル。&lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;stft()&lt;/code&gt; の&lt;/a&gt;出力が期待され、複素数（ &lt;code&gt;channel&lt;/code&gt; 、 &lt;code&gt;fft_size&lt;/code&gt; 、 &lt;code&gt;n_frame&lt;/code&gt; ）、または実数（ &lt;code&gt;channel&lt;/code&gt; 、 &lt;code&gt;fft_size&lt;/code&gt; 、 &lt;code&gt;n_frame&lt;/code&gt; 、2）のいずれかで、 &lt;code&gt;channel&lt;/code&gt; 次元はオプションです。</target>
        </trans-unit>
        <trans-unit id="25819484e745b900dfb30d16e53f617750bd99ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; a minibatch of examples</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;例のミニバッチ</target>
        </trans-unit>
        <trans-unit id="c8a8643fc96ff022524bbbbcc16e1f74f4d027fc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; first tensor in the dot product. Its conjugate is used if it&amp;rsquo;s complex.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;ドット積の最初のテンソル。複雑な場合は、その共役が使用されます。</target>
        </trans-unit>
        <trans-unit id="3329ef02c74e5cee67f1ff96ed2733ed78e48c67" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; first tensor to compare</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;最初に比較するテンソル</target>
        </trans-unit>
        <trans-unit id="b6c3db01982d7f9c54ac50853d8f1beed2cda65f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; float tensor to quantize</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;量子化するフロートテンソル</target>
        </trans-unit>
        <trans-unit id="0c7042dc3bf98959f56c246765073fdcc4437d8b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input matrix</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;入力行列</target>
        </trans-unit>
        <trans-unit id="2add4041984cdf88b12d1f1e2dd5b3c7ef0a1d8c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; matrix to be added</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;追加する行列</target>
        </trans-unit>
        <trans-unit id="b95c32681c57401ab545053403e32395835243a1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; matrix to be multiplied</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;乗算される行列</target>
        </trans-unit>
        <trans-unit id="2b1dc1a962b67cc0e0f4fe6ec1e3b9ed6f855288" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; multiple right-hand sides of size</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;サイズの複数の右側</target>
        </trans-unit>
        <trans-unit id="fd44d464e1aa57b95d0330a7218514f91318409c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; padded batch of variable length sequences.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;可変長シーケンスのパディングされたバッチ。</target>
        </trans-unit>
        <trans-unit id="2df5064f26ed4ff300eeca02d8d23ff3989d592a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the &lt;code&gt;a&lt;/code&gt; from &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;） - &lt;code&gt;a&lt;/code&gt; から&lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="32b3debabd5244f6f757a0ecbfc2001f87d51061" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the dividend</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;配当</target>
        </trans-unit>
        <trans-unit id="fc63e66b77eddc23d0480985c434968ab3a3c432" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first batch of matrices to be multiplied</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;乗算される行列の最初のバッチ</target>
        </trans-unit>
        <trans-unit id="8048457126f71ad2252c07afbe21ab1ad62cc0e3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first input tensor</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;最初の入力テンソル</target>
        </trans-unit>
        <trans-unit id="40066a22db2f549d0b2b91e1d0653edab74dded7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first matrix to be multiplied</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;乗算される最初の行列</target>
        </trans-unit>
        <trans-unit id="8d03903d53b2c5ac4f518cd45d6008241190088c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first multiplicand tensor</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;最初の被乗数テンソル</target>
        </trans-unit>
        <trans-unit id="7f51f99307491138059682bbbc27c557807591a9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first tensor to be multiplied</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;最初に乗算されるテンソル</target>
        </trans-unit>
        <trans-unit id="1accceea4c5085a1b9d5fd922dec4a71a7c24ead" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input 2-D tensor</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;入力2次元テンソル</target>
        </trans-unit>
        <trans-unit id="6146e5ea09927c514d32a11ed0ac42f33556893d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input matrix</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力行列</target>
        </trans-unit>
        <trans-unit id="708c21e573ae8975fd41f843d793ad7f90dc01c9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力テンソル</target>
        </trans-unit>
        <trans-unit id="ccd5042b98d78e05ed8accf585abbd7c7cd67f8c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor containing probabilities</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;確率を含む入力テンソル</target>
        </trans-unit>
        <trans-unit id="a8678adfcf46f049849a9950535e8987135ee8c9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor containing the rates of the Poisson distribution</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;ポアソン分布のレートを含む入力テンソル</target>
        </trans-unit>
        <trans-unit id="19b4b70bbc0703888b4dd081987a664f12caeab6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of at least &lt;code&gt;signal_ndim&lt;/code&gt; dimensions</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;少なくとも &lt;code&gt;signal_ndim&lt;/code&gt; 次元の入力テンソル</target>
        </trans-unit>
        <trans-unit id="a80664bf19108f6871d4d9cc67ea5ec9d65038f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of at least &lt;code&gt;signal_ndim&lt;/code&gt;&lt;code&gt;+ 1&lt;/code&gt; dimensions</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;少なくとも &lt;code&gt;signal_ndim&lt;/code&gt; &lt;code&gt;+ 1&lt;/code&gt; 次元の入力テンソル</target>
        </trans-unit>
        <trans-unit id="80ea65a4aaf35f42f84d23c01465ca2ef54bdaad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of probability values for the Bernoulli distribution</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;ベルヌーイ分布の確率値の入力テンソル</target>
        </trans-unit>
        <trans-unit id="b642c0b47336e267e577b35820c587126ea9f39a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of size</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;サイズの入力テンソル</target>
        </trans-unit>
        <trans-unit id="9159ecc16b63f9802803f1f98dd7cfcc6322e401" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of size &lt;code&gt;(*, n, n)&lt;/code&gt; where &lt;code&gt;*&lt;/code&gt; is zero or more batch dimensions.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;サイズ &lt;code&gt;(*, n, n)&lt;/code&gt; の入力テンソル。ここで、 &lt;code&gt;*&lt;/code&gt; は0個以上のバッチ次元です。</target>
        </trans-unit>
        <trans-unit id="7caaa20ec7806018b377988dd677601f3f83e4d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力テンソル。</target>
        </trans-unit>
        <trans-unit id="121b8ceabf101922601fe3bb8a675692baf2cd6c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor. Must be at least 1-dimensional.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力テンソル。少なくとも1次元である必要があります。</target>
        </trans-unit>
        <trans-unit id="80a6967e7a1bc373747e4ceda61d8eca3bcbe71d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor. Must be at least 2-dimensional.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力テンソル。少なくとも2次元である必要があります。</target>
        </trans-unit>
        <trans-unit id="d5176c20efe2b991fb3ef27de42aad8bfff75426" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the matrix</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;行列</target>
        </trans-unit>
        <trans-unit id="a9ce71bbfa8c25d01c8c318346faac5f3f4407cd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the size of &lt;code&gt;input&lt;/code&gt; will determine size of the output tensor.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash; &lt;code&gt;input&lt;/code&gt; サイズが出力テンソルのサイズを決定します。</target>
        </trans-unit>
        <trans-unit id="bfcd8ed79796b25e26a5e670250a73272dac91fb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the source tensor</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;ソーステンソル</target>
        </trans-unit>
        <trans-unit id="b3cad4fa9c4a49fae5b079d14a2bcba4e124c2ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the square matrix of shape</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;形状の正方行列</target>
        </trans-unit>
        <trans-unit id="09c118f2812817644cc5b84fc5bc2cef3421e809" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to be added</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;追加されるテンソル</target>
        </trans-unit>
        <trans-unit id="648002a80ff2960290b93380fa5ebeba8bd25668" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to be reshaped</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;再形成されるテンソル</target>
        </trans-unit>
        <trans-unit id="20715c854d53757985d23efabade3db743a6a166" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to compare</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;比較するテンソル</target>
        </trans-unit>
        <trans-unit id="0c1865166964450f22d83f1a28613f076a2e569b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to compute the digamma function on</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;ディガンマ関数を計算するテンソル</target>
        </trans-unit>
        <trans-unit id="c2bf2493846223fdc5fbfe54614e402279de1747" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to compute the multivariate log-gamma function</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;多変量対数ガンマ関数を計算するテンソル</target>
        </trans-unit>
        <trans-unit id="7fbfb129c11e7747b44805a7b05a9ebc40147948" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to narrow</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;狭めるテンソル</target>
        </trans-unit>
        <trans-unit id="55c3c22205a1b70d592dec6666248c82e3690335" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to split</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;分割するテンソル</target>
        </trans-unit>
        <trans-unit id="3219155900c01df3eed96409ef1ed4b22f2fc99a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to unbind</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;バインドを解除するテンソル</target>
        </trans-unit>
        <trans-unit id="1a48e0ca5030fc4d6af5bdfb8605eaef2755a644" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor with the starting points</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;開始点を持つテンソル</target>
        </trans-unit>
        <trans-unit id="6cc211cd1b8495af379d1d59ebf66ec1fb368257" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; vector to be added</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;追加するベクトル</target>
        </trans-unit>
        <trans-unit id="d0f2899dfd7408899e467222cd4d0bd1c8b163e8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the dividend</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;em&gt;数値&lt;/em&gt;）&amp;ndash;配当</target>
        </trans-unit>
        <trans-unit id="3e495db7a10b097798f9ab905342091739c99080" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; N-D tensor or a Scalar containing the search value(s).</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;&lt;em&gt;Scalar&lt;/em&gt;）&amp;ndash;NDテンソル&lt;em&gt;または&lt;/em&gt;検索値を含むスカラー。</target>
        </trans-unit>
        <trans-unit id="54b5b721c1619fe54cfff53778c25cbdce11a2a0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;list of Tensors&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;&lt;em&gt;または&lt;/em&gt;テンソルの&lt;em&gt;リスト&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="e15a78cbbc8bd023145c6c784e4996207ef9b8e4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="5b1084c8300aad27ad1784e01fa04c5108343982" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; N-dimensional tensor</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;N次元テンソル</target>
        </trans-unit>
        <trans-unit id="a4ba99c5e33ab9d93f9eeb31063659728503213f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Quantized input of type &lt;code&gt;torch.quint8&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;タイプ &lt;code&gt;torch.quint8&lt;/code&gt; の量子化された入力</target>
        </trans-unit>
        <trans-unit id="e32b4df67a1c6fa976c98f26ed880fe2843dd472" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The input tensor. If dim is None, x must be 1-D or 2-D, unless &lt;code&gt;ord&lt;/code&gt; is None. If both &lt;code&gt;dim&lt;/code&gt; and &lt;code&gt;ord&lt;/code&gt; are None, the 2-norm of the input flattened to 1-D will be returned.</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力テンソル。dimがNoneの場合、 &lt;code&gt;ord&lt;/code&gt; がNoneでない限り、xは1-Dまたは2-Dである必要があります。 &lt;code&gt;dim&lt;/code&gt; と &lt;code&gt;ord&lt;/code&gt; の両方がNoneの場合、1-Dにフラット化された入力の2ノルムが返されます。</target>
        </trans-unit>
        <trans-unit id="750efbed074a69bad5c30928c16477fc2a8db885" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;入力</target>
        </trans-unit>
        <trans-unit id="fbba8e02a600f5a44e37f994cfdcf15acf5f5674" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input of shape</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;形状の入力</target>
        </trans-unit>
        <trans-unit id="71024ef201be154545b2752c7794987629860a82" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input tensor</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;入力テンソル</target>
        </trans-unit>
        <trans-unit id="cc13ec8f5ecb355720aad713d541718fbcff8ff1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; quantized input</source>
          <target state="translated">&lt;strong&gt;入力&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;テンソル&lt;/a&gt;）&amp;ndash;量子化された入力</target>
        </trans-unit>
        <trans-unit id="ca4fccfc307fc839f56fcd2935b753c3a675a514" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; quantized input tensor</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;量子化された入力テンソル</target>
        </trans-unit>
        <trans-unit id="16adc05235f86a0bdba3f8d133f8ad24c146c434" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input SparseTensor</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力SparseTensor</target>
        </trans-unit>
        <trans-unit id="f15e5e30b2c90290ac32a42b9dde72a1f212a74f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;入力テンソル</target>
        </trans-unit>
        <trans-unit id="50bc5249b9927187f00659fce46d0556e5580c69" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor representing a half-Hermitian signal</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;ハーフエルミート信号を表す入力テンソル</target>
        </trans-unit>
        <trans-unit id="01ff6678d557f24c8e79e022f907950c5a82aff9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the real input tensor</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;実際の入力テンソル</target>
        </trans-unit>
        <trans-unit id="912fbe0aeceffe41c39ef4b4f7a3bd4836fd3f31" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; Tensor containing bags of indices into the embedding matrix</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;埋め込み行列へのインデックスのバッグを含むテンソル</target>
        </trans-unit>
        <trans-unit id="062fdb3d419326f615da299be36d7f8d3afff88a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;input&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; Tensor containing indices into the embedding matrix</source>
          <target state="translated">&lt;strong&gt;input&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;埋め込み行列へのインデックスを含むテンソル</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
