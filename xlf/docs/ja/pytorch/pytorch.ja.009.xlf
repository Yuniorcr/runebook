<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="a5d4ce8f33e2308a786a65897ca2b53b361ed230" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.gcd#torch.gcd&quot;&gt;&lt;code&gt;torch.gcd()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.gcd#torch.gcd&quot;&gt; &lt;code&gt;torch.gcd()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="47e2863133f746b04949765203596816d2a569fd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt; &lt;code&gt;torch.ge()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="dc3905a5552f33afb11f303bb93b1f92da393e87" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="d0f396c7008d561476ffb59f26b17018bc84fa73" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ger#torch.ger&quot;&gt;&lt;code&gt;torch.ger()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.ger#torch.ger&quot;&gt; &lt;code&gt;torch.ger()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="29a71ad0e5b1620b27e713b79b570ee4a9882fbd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.greater#torch.greater&quot;&gt;&lt;code&gt;torch.greater()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.greater#torch.greater&quot;&gt; &lt;code&gt;torch.greater()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="2aa63e3e1cb9df927bb8622309466c088861e7c1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.greater_equal#torch.greater_equal&quot;&gt;&lt;code&gt;torch.greater_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.greater_equal#torch.greater_equal&quot;&gt; &lt;code&gt;torch.greater_equal()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="22d16c3e505067a772ddee47e68d0c5e9efdf1ea" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt; &lt;code&gt;torch.gt()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="f797b96d9a976433ee5e82f85c212bf9cc6c07aa" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.heaviside#torch.heaviside&quot;&gt;&lt;code&gt;torch.heaviside()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.heaviside#torch.heaviside&quot;&gt; &lt;code&gt;torch.heaviside()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="f4c88cbc91c3b48e1204ec2fc29b766aaa6e3c59" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.histc#torch.histc&quot;&gt;&lt;code&gt;torch.histc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.histc#torch.histc&quot;&gt; &lt;code&gt;torch.histc()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="f24053f673f97bf0cebaec0ddd1ab96fa9d420f2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.hypot#torch.hypot&quot;&gt;&lt;code&gt;torch.hypot()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.hypot#torch.hypot&quot;&gt; &lt;code&gt;torch.hypot()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="5a27de95d9ce95538faa35bfb5a5044535b471e7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.i0#torch.i0&quot;&gt;&lt;code&gt;torch.i0()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.i0#torch.i0&quot;&gt; &lt;code&gt;torch.i0()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="55baf0c2297883aeca1f8032e140f1dce62c7f54" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;torch.ifft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;torch.ifft()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="dbd53ee94265f26254d53b09c2158dbdc7c06b75" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.index_select#torch.index_select&quot;&gt;&lt;code&gt;torch.index_select()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.index_select#torch.index_select&quot;&gt; &lt;code&gt;torch.index_select()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="91cb515b356d2c52d4004829619ffcf0284a140c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.inverse#torch.inverse&quot;&gt;&lt;code&gt;torch.inverse()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.inverse#torch.inverse&quot;&gt; &lt;code&gt;torch.inverse()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="1921c02b416a3c35ca9a7616e742bedc37f85261" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.irfft#torch.irfft&quot;&gt;&lt;code&gt;torch.irfft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.irfft#torch.irfft&quot;&gt; &lt;code&gt;torch.irfft()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="0a61d72d732d7b3d2bfcfab031758abf3bd3f2bf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isclose#torch.isclose&quot;&gt;&lt;code&gt;torch.isclose()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.isclose#torch.isclose&quot;&gt; &lt;code&gt;torch.isclose()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="6d43646aed73f0e63bb4282297ddeb0953661084" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isfinite#torch.isfinite&quot;&gt;&lt;code&gt;torch.isfinite()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.isfinite#torch.isfinite&quot;&gt; &lt;code&gt;torch.isfinite()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="7f01c4969936eb94939bfba08517f9dc5d3c9b65" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isinf#torch.isinf&quot;&gt;&lt;code&gt;torch.isinf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.isinf#torch.isinf&quot;&gt; &lt;code&gt;torch.isinf()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="c15049f87e6457b76bc4c833aad4e86fc330a9a4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isnan#torch.isnan&quot;&gt;&lt;code&gt;torch.isnan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.isnan#torch.isnan&quot;&gt; &lt;code&gt;torch.isnan()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="b0c99b8516b26410ede0975c9d5e8303e6a56d72" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isneginf#torch.isneginf&quot;&gt;&lt;code&gt;torch.isneginf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.isneginf#torch.isneginf&quot;&gt; &lt;code&gt;torch.isneginf()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="c45c9bc98276b87a137a720f4ab4cbfc8bccf9d9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isposinf#torch.isposinf&quot;&gt;&lt;code&gt;torch.isposinf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.isposinf#torch.isposinf&quot;&gt; &lt;code&gt;torch.isposinf()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="bff4ba1100e12cf9a28654669a565681a03dd913" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.isreal#torch.isreal&quot;&gt;&lt;code&gt;torch.isreal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.isreal#torch.isreal&quot;&gt; &lt;code&gt;torch.isreal()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="dcf71e098da4b8c28d983651980d01f5b1a6049b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.istft#torch.istft&quot;&gt;&lt;code&gt;torch.istft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.istft#torch.istft&quot;&gt; &lt;code&gt;torch.istft()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="0f606579981b49ec0a8206d550a759168fd92964" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.kthvalue#torch.kthvalue&quot;&gt;&lt;code&gt;torch.kthvalue()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.kthvalue#torch.kthvalue&quot;&gt; &lt;code&gt;torch.kthvalue()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="0d8ba07cef0e3cd2ee953932e7401cfd0248e444" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lcm#torch.lcm&quot;&gt;&lt;code&gt;torch.lcm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.lcm#torch.lcm&quot;&gt; &lt;code&gt;torch.lcm()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="d6699a864923ddee28ba56b38069aaab083349df" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.le#torch.le&quot;&gt; &lt;code&gt;torch.le()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="32c678d238915a61f701966540f011fe2fc9cbc1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lerp#torch.lerp&quot;&gt;&lt;code&gt;torch.lerp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.lerp#torch.lerp&quot;&gt; &lt;code&gt;torch.lerp()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="e1d6db8a3f3362c5bfd61e84e72244e0ee6bf8e9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.less#torch.less&quot;&gt;&lt;code&gt;torch.less()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.less#torch.less&quot;&gt; &lt;code&gt;torch.less()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="e8eea9085bfaff6760dd93361241db5a26d61c2c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.less_equal#torch.less_equal&quot;&gt;&lt;code&gt;torch.less_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.less_equal#torch.less_equal&quot;&gt; &lt;code&gt;torch.less_equal()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="1cd2d05d4cdb6e340422b7b49a0f18ef4af47558" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lgamma#torch.lgamma&quot;&gt;&lt;code&gt;torch.lgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.lgamma#torch.lgamma&quot;&gt; &lt;code&gt;torch.lgamma()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="7caa6f91635bb60ab80ed9ca2541683e873e5da4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.log#torch.log&quot;&gt;&lt;code&gt;torch.log()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.log#torch.log&quot;&gt; &lt;code&gt;torch.log()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="73a23f7411ea79d1413155727db906f0562b2082" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.log10#torch.log10&quot;&gt;&lt;code&gt;torch.log10()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.log10#torch.log10&quot;&gt; &lt;code&gt;torch.log10()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="a8dc5bc392cc9737d55c7a288ffae377a0f80b8e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.log1p#torch.log1p&quot;&gt;&lt;code&gt;torch.log1p()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.log1p#torch.log1p&quot;&gt; &lt;code&gt;torch.log1p()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="b442c9296b5b51dc12a6d62a9e946bae4e4cace4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.log2#torch.log2&quot;&gt;&lt;code&gt;torch.log2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.log2#torch.log2&quot;&gt; &lt;code&gt;torch.log2()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="82580a23e0c28a78662f5e93ee563089d4043db8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logaddexp#torch.logaddexp&quot;&gt;&lt;code&gt;torch.logaddexp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logaddexp#torch.logaddexp&quot;&gt; &lt;code&gt;torch.logaddexp()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="7856ea9cc5c5b34b3fad25b3ceb90619e2bcf1a1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logaddexp2#torch.logaddexp2&quot;&gt;&lt;code&gt;torch.logaddexp2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logaddexp2#torch.logaddexp2&quot;&gt; &lt;code&gt;torch.logaddexp2()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="d244fcdaec339fa07bf0c8b02d8e5206a3756511" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logcumsumexp#torch.logcumsumexp&quot;&gt;&lt;code&gt;torch.logcumsumexp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logcumsumexp#torch.logcumsumexp&quot;&gt; &lt;code&gt;torch.logcumsumexp()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="e64767e412c54452d0e93a4095d3fb5a34e4a87d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logdet#torch.logdet&quot;&gt;&lt;code&gt;torch.logdet()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logdet#torch.logdet&quot;&gt; &lt;code&gt;torch.logdet()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="dae45426090891d1953ee3ad38c045f8a0fceedc" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logical_and#torch.logical_and&quot;&gt;&lt;code&gt;torch.logical_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logical_and#torch.logical_and&quot;&gt; &lt;code&gt;torch.logical_and()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="d20544f0f1149d37dc67d5b4c2ec2fcf87602aaf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logical_not#torch.logical_not&quot;&gt;&lt;code&gt;torch.logical_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logical_not#torch.logical_not&quot;&gt; &lt;code&gt;torch.logical_not()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="7f1dd2cde87434261bce6932056bcd2beea83bc0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logical_or#torch.logical_or&quot;&gt;&lt;code&gt;torch.logical_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logical_or#torch.logical_or&quot;&gt; &lt;code&gt;torch.logical_or()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="64c77ca5d503f49a2495909f4b01e01b4e55a15f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logical_xor#torch.logical_xor&quot;&gt;&lt;code&gt;torch.logical_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logical_xor#torch.logical_xor&quot;&gt; &lt;code&gt;torch.logical_xor()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="e9fec990c19512cd7cbc5c7730d590537da8787a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logit#torch.logit&quot;&gt;&lt;code&gt;torch.logit()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logit#torch.logit&quot;&gt; &lt;code&gt;torch.logit()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="7682310645eb1b3d8a10df47deccb4b498fc4809" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.logsumexp#torch.logsumexp&quot;&gt;&lt;code&gt;torch.logsumexp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.logsumexp#torch.logsumexp&quot;&gt; &lt;code&gt;torch.logsumexp()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="f577c3338ec6045533458e8c6be36ac9572aed9e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lstsq#torch.lstsq&quot;&gt;&lt;code&gt;torch.lstsq()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.lstsq#torch.lstsq&quot;&gt; &lt;code&gt;torch.lstsq()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="e411f1ff349b11bfd836c08df5971026b855e59a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt; &lt;code&gt;torch.lt()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="578639cb9fc16b15aa6b7e1967466fe9b09ff42d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lu#torch.lu&quot;&gt;&lt;code&gt;torch.lu()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.lu#torch.lu&quot;&gt; &lt;code&gt;torch.lu()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="c96a4514c11416811fe9f9b86d4dd2e1d0e4e1cd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.lu_solve#torch.lu_solve&quot;&gt;&lt;code&gt;torch.lu_solve()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.lu_solve#torch.lu_solve&quot;&gt; &lt;code&gt;torch.lu_solve()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="8c6b2883c1c8d2d0de37ea2c95e5b1fa94069b4d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.masked_select#torch.masked_select&quot;&gt;&lt;code&gt;torch.masked_select()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.masked_select#torch.masked_select&quot;&gt; &lt;code&gt;torch.masked_select()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="c4dc4224536b289cdb91dd6eb3b784c8632062f7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.matmul#torch.matmul&quot;&gt;&lt;code&gt;torch.matmul()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.matmul#torch.matmul&quot;&gt; &lt;code&gt;torch.matmul()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="18b30626e1ae4fc77dd2baf1eea08d60901e4a44" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.matrix_exp#torch.matrix_exp&quot;&gt;&lt;code&gt;torch.matrix_exp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.matrix_exp#torch.matrix_exp&quot;&gt; &lt;code&gt;torch.matrix_exp()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="8ceb6fc4ba32f1c6513652ba41e7ed5365209b9d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.matrix_power#torch.matrix_power&quot;&gt;&lt;code&gt;torch.matrix_power()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.matrix_power#torch.matrix_power&quot;&gt; &lt;code&gt;torch.matrix_power()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="e3d3a41458bccec6e7e87d98e4ad75ec5c4237fb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.max#torch.max&quot;&gt;&lt;code&gt;torch.max()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.max#torch.max&quot;&gt; &lt;code&gt;torch.max()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="689831f5600395b5cce9e382345788eac42fd2b3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.maximum#torch.maximum&quot;&gt;&lt;code&gt;torch.maximum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.maximum#torch.maximum&quot;&gt; &lt;code&gt;torch.maximum()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="75cde60dd542a6062dc76781c6f95cef4726b299" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt;&lt;code&gt;torch.mean()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt; &lt;code&gt;torch.mean()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="106c7cf7d53f462e1efa14592e3a206e193353ed" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.median#torch.median&quot;&gt;&lt;code&gt;torch.median()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.median#torch.median&quot;&gt; &lt;code&gt;torch.median()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="45e4b29591dc1fd5fe60cf3f1a2528339eb79d7a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.min#torch.min&quot;&gt;&lt;code&gt;torch.min()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.min#torch.min&quot;&gt; &lt;code&gt;torch.min()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="71ff8dd28d5ec4bba7fa90d123381bf318d57316" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.minimum#torch.minimum&quot;&gt;&lt;code&gt;torch.minimum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.minimum#torch.minimum&quot;&gt; &lt;code&gt;torch.minimum()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="f88fe431125507817f53eadcab5b7cf0486d3c3a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mm#torch.mm&quot;&gt;&lt;code&gt;torch.mm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.mm#torch.mm&quot;&gt; &lt;code&gt;torch.mm()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="89a5d4426d8c3c83f76b02d2a04a8fa74cd9c984" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mode#torch.mode&quot;&gt;&lt;code&gt;torch.mode()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.mode#torch.mode&quot;&gt; &lt;code&gt;torch.mode()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="480d2c05ebbc86790a491b5029a1882724ab97f9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.movedim#torch.movedim&quot;&gt;&lt;code&gt;torch.movedim()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.movedim#torch.movedim&quot;&gt; &lt;code&gt;torch.movedim()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="9e47d65e128408a5146c2909f05667f8df18f4de" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt; &lt;code&gt;torch.mul()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="346834db1e953738900b156dccde72730c321179" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.multinomial#torch.multinomial&quot;&gt;&lt;code&gt;torch.multinomial()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.multinomial#torch.multinomial&quot;&gt; &lt;code&gt;torch.multinomial()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="2abc253dc35a3f16344ec6f7c0d4b4f3f6f389c0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.multiply#torch.multiply&quot;&gt;&lt;code&gt;torch.multiply()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.multiply#torch.multiply&quot;&gt; &lt;code&gt;torch.multiply()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="bd83a3f33cb0434d70d1281178c10f4d4186288c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mv#torch.mv&quot;&gt;&lt;code&gt;torch.mv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.mv#torch.mv&quot;&gt; &lt;code&gt;torch.mv()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="c7ada1c677f5eab787ec7f543df6913243851e42" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.mvlgamma#torch.mvlgamma&quot;&gt;&lt;code&gt;torch.mvlgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.mvlgamma#torch.mvlgamma&quot;&gt; &lt;code&gt;torch.mvlgamma()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="96ec4016de71e2ca1618c82409f4cbcb9c612e86" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nanquantile#torch.nanquantile&quot;&gt;&lt;code&gt;torch.nanquantile()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nanquantile#torch.nanquantile&quot;&gt; &lt;code&gt;torch.nanquantile()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="eda1f1134fb78245db2a5b46d9c8dcff19e60476" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nansum#torch.nansum&quot;&gt;&lt;code&gt;torch.nansum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nansum#torch.nansum&quot;&gt; &lt;code&gt;torch.nansum()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="2aaeb0cd2a053ef536d53c256571aadbdbf969ca" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.narrow#torch.narrow&quot;&gt;&lt;code&gt;torch.narrow()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.narrow#torch.narrow&quot;&gt; &lt;code&gt;torch.narrow()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="32b6bd21755f0280c169e12a964b6d0d8f9f944d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt; &lt;code&gt;torch.ne()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="ef00c00cfc229955de1af61e47329e20da955299" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt; &lt;code&gt;torch.neg()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="c59bed38407778d4b3b65157e276bcba44fa481e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.negative#torch.negative&quot;&gt;&lt;code&gt;torch.negative()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.negative#torch.negative&quot;&gt; &lt;code&gt;torch.negative()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="ab18f54431cb99ac38d5579314003713c8aef19f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nextafter#torch.nextafter&quot;&gt;&lt;code&gt;torch.nextafter()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nextafter#torch.nextafter&quot;&gt; &lt;code&gt;torch.nextafter()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="211dc318b0ac18e597f00d3389f89e2fa69af8fd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d&quot;&gt;&lt;code&gt;AdaptiveAvgPool1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d&quot;&gt; &lt;code&gt;AdaptiveAvgPool1d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="c297648804959d7e566eb6462302a062556e0a26" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d&quot;&gt;&lt;code&gt;AdaptiveAvgPool2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d&quot;&gt; &lt;code&gt;AdaptiveAvgPool2d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="9e3e0fe3b85f801c746e8e3435b99b12af2758cb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d&quot;&gt;&lt;code&gt;AdaptiveAvgPool3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d&quot;&gt; &lt;code&gt;AdaptiveAvgPool3d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="dcb4c522bcd653244a2b0e212d2a6c1dec943575" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d&quot;&gt;&lt;code&gt;AdaptiveMaxPool1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d&quot;&gt; &lt;code&gt;AdaptiveMaxPool1d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="9197dd28b388a9ad018e4140bcb03f05bef22fce" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d&quot;&gt;&lt;code&gt;AdaptiveMaxPool2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d&quot;&gt; &lt;code&gt;AdaptiveMaxPool2d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="671fa746ed1d96f6e5c409933c093bf281ece232" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d&quot;&gt;&lt;code&gt;AdaptiveMaxPool3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d&quot;&gt; &lt;code&gt;AdaptiveMaxPool3d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="06fcc12676bdce703006b9934a46a5d88c4ea69e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.alphadropout#torch.nn.AlphaDropout&quot;&gt;&lt;code&gt;AlphaDropout&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.alphadropout#torch.nn.AlphaDropout&quot;&gt; &lt;code&gt;AlphaDropout&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="299a82bba9fc9f3c48bfd00d90ea771941bb3509" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.avgpool1d#torch.nn.AvgPool1d&quot;&gt;&lt;code&gt;AvgPool1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.avgpool1d#torch.nn.AvgPool1d&quot;&gt; &lt;code&gt;AvgPool1d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="9e431c47012b760f030d5cd2cfeebdfaaa73e3a4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.avgpool2d#torch.nn.AvgPool2d&quot;&gt;&lt;code&gt;AvgPool2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.avgpool2d#torch.nn.AvgPool2d&quot;&gt; &lt;code&gt;AvgPool2d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="5d5e8ddb26e145fc6c6ccd8ea827c9249bd3e2c2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.avgpool3d#torch.nn.AvgPool3d&quot;&gt;&lt;code&gt;AvgPool3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.avgpool3d#torch.nn.AvgPool3d&quot;&gt; &lt;code&gt;AvgPool3d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="b2f2febc074ac5ab0c26eb12dab1477a725d346c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d&quot;&gt;&lt;code&gt;BatchNorm1d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt;&lt;code&gt;BatchNorm3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d&quot;&gt; &lt;code&gt;BatchNorm1d&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt; &lt;code&gt;BatchNorm3d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="49181f1a5d0f855f350c509add5501b487b8de9a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.bceloss#torch.nn.BCELoss&quot;&gt;&lt;code&gt;BCELoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.bceloss#torch.nn.BCELoss&quot;&gt; &lt;code&gt;BCELoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="f9b2acc69b3141658463c5b48ed119f653d953fc" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss&quot;&gt;&lt;code&gt;BCEWithLogitsLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss&quot;&gt; &lt;code&gt;BCEWithLogitsLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="1e05c2119de76670ff15f68392d6126a185752ff" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.celu#torch.nn.CELU&quot;&gt;&lt;code&gt;CELU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.celu#torch.nn.CELU&quot;&gt; &lt;code&gt;CELU&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="46a2836ec87b23dd081f1f557c9d269b806abf39" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d&quot;&gt;&lt;code&gt;torch.nn.ConstantPad2d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d&quot;&gt;&lt;code&gt;torch.nn.ReflectionPad2d&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d&quot;&gt;&lt;code&gt;torch.nn.ReplicationPad2d&lt;/code&gt;&lt;/a&gt; for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Replicate padding is implemented for padding the last 3 dimensions of 5D input tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor. Reflect padding is only implemented for padding the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor.</source>
          <target state="translated">各パディングモードの動作の具体例については、&lt;a href=&quot;generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d&quot;&gt; &lt;code&gt;torch.nn.ConstantPad2d&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d&quot;&gt; &lt;code&gt;torch.nn.ReflectionPad2d&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d&quot;&gt; &lt;code&gt;torch.nn.ReplicationPad2d&lt;/code&gt; &lt;/a&gt;を参照してください。一定のパディングは、任意の次元に実装されます。複製パディングは、5D入力テンソルの最後の3次元、4D入力テンソルの最後の2次元、または3D入力テンソルの最後の次元をパディングするために実装されます。反射パディングは、4D入力テンソルの最後の2次元、または3D入力テンソルの最後の次元をパディングするためにのみ実装されます。</target>
        </trans-unit>
        <trans-unit id="92e4bf1e1580e8ab1c867f9318919922f5d69097" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt; &lt;code&gt;Conv1d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="fb3d45384af0f65dcc256fd4e526a226237028c3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; for other attributes.</source>
          <target state="translated">その他の属性については、&lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt; &lt;code&gt;Conv1d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="1d1d57fe3bdff062ac6991806c22d029393fd38a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="d165cb34672d664f9dd17ceb8fa4d5cf8092ba2a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; for other attributes.</source>
          <target state="translated">その他の属性については、&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="62b640305a360b502c1c6f7e2a7cc59219562d5b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt; &lt;code&gt;Conv3d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="827a3d0d159d0163f089c1b8b07697a3b4d204b1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; for other attributes.</source>
          <target state="translated">その他の属性については、&lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt; &lt;code&gt;Conv3d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="2c6215c6031dbc3057e738f03ebc6ccb657c175c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d&quot;&gt;&lt;code&gt;ConvTranspose1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d&quot;&gt; &lt;code&gt;ConvTranspose1d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="92a07f307c29825975eb91630c55dc6f1ef73449" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d&quot;&gt;&lt;code&gt;ConvTranspose2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d&quot;&gt; &lt;code&gt;ConvTranspose2d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="26d37ceb58b322f40fdbdfcf86ed7b27d34f3ea4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d&quot;&gt;&lt;code&gt;ConvTranspose3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">参照&lt;a href=&quot;generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d&quot;&gt; &lt;code&gt;ConvTranspose3d&lt;/code&gt; を&lt;/a&gt;詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="0ecbef13440cb7e3c72978fb3f953d0ea4a05dec" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss&quot;&gt;&lt;code&gt;CosineEmbeddingLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss&quot;&gt; &lt;code&gt;CosineEmbeddingLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="07cebd07469d5659a87059395414105b3f0a54c1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss&quot;&gt;&lt;code&gt;CrossEntropyLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss&quot;&gt; &lt;code&gt;CrossEntropyLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="ed9307322caa5510dafcec4ab6a6439e46cca0ac" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.ctcloss#torch.nn.CTCLoss&quot;&gt;&lt;code&gt;CTCLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.ctcloss#torch.nn.CTCLoss&quot;&gt; &lt;code&gt;CTCLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="90c67074e8fb1e5903e2b6ceed293a19589c2504" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.dropout#torch.nn.Dropout&quot;&gt;&lt;code&gt;Dropout&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.dropout#torch.nn.Dropout&quot;&gt; &lt;code&gt;Dropout&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="6b5e6bf0d87dc5eedbd14a70bf17165953b58726" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.dropout2d#torch.nn.Dropout2d&quot;&gt;&lt;code&gt;Dropout2d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.dropout2d#torch.nn.Dropout2d&quot;&gt; &lt;code&gt;Dropout2d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="9181ef4cdcda4e21d4c1d578bf817479c3e73473" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.dropout3d#torch.nn.Dropout3d&quot;&gt;&lt;code&gt;Dropout3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.dropout3d#torch.nn.Dropout3d&quot;&gt; &lt;code&gt;Dropout3d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="e9638040580d53d7a62b5948dfef978c66524b9c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt;&lt;code&gt;ELU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt; &lt;code&gt;ELU&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="9516b3d99ccc49e5403a54c4cd4186fa2ebff368" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.embedding#torch.nn.Embedding&quot;&gt;&lt;code&gt;torch.nn.Embedding&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.embedding#torch.nn.Embedding&quot;&gt; &lt;code&gt;torch.nn.Embedding&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="b0ef85f5e373bd3344bdb431e1a684be4a2c111f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag&quot;&gt;&lt;code&gt;torch.nn.EmbeddingBag&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag&quot;&gt; &lt;code&gt;torch.nn.EmbeddingBag&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="87b8eae451b415ea1f7e39fa3cca366d80225aa8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.fold#torch.nn.Fold&quot;&gt;&lt;code&gt;torch.nn.Fold&lt;/code&gt;&lt;/a&gt; for details</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.fold#torch.nn.Fold&quot;&gt; &lt;code&gt;torch.nn.Fold&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="a3f647c2cb1152f6be057eaec2f333adc27b7848" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.hardshrink#torch.nn.Hardshrink&quot;&gt;&lt;code&gt;Hardshrink&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.hardshrink#torch.nn.Hardshrink&quot;&gt; &lt;code&gt;Hardshrink&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="3e54bc653f24897904f35291d48d7a4d3e1b15dd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid&quot;&gt;&lt;code&gt;Hardsigmoid&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid&quot;&gt; &lt;code&gt;Hardsigmoid&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="8d8089dc9fcfe5932254ec4b47d12c1cbe02d107" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt;&lt;code&gt;Hardswish&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt; &lt;code&gt;Hardswish&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="79e7f193eb69518b7c2bc43975107d6c9f8a543d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss&quot;&gt;&lt;code&gt;HingeEmbeddingLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss&quot;&gt; &lt;code&gt;HingeEmbeddingLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="a07f37d494d50f907a7a0be59a0004e665fc42f6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt;&lt;code&gt;InstanceNorm1d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt;&lt;code&gt;InstanceNorm2d&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt;&lt;code&gt;InstanceNorm3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt; &lt;code&gt;InstanceNorm1d&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt; &lt;code&gt;InstanceNorm2d&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt; &lt;code&gt;InstanceNorm3d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="cc584ec8326a5572f69ad2a713d36f3294777bbf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.kldivloss#torch.nn.KLDivLoss&quot;&gt;&lt;code&gt;KLDivLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.kldivloss#torch.nn.KLDivLoss&quot;&gt; &lt;code&gt;KLDivLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="82170e2aa45b79e0abc6968e9d7a135946023f5a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.l1loss#torch.nn.L1Loss&quot;&gt;&lt;code&gt;L1Loss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.l1loss#torch.nn.L1Loss&quot;&gt; &lt;code&gt;L1Loss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="6d19657b2b5b68544134330ee366c7bfea1457eb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt;&lt;code&gt;LayerNorm&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt; &lt;code&gt;LayerNorm&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="2bef43e82a39e507cbf89ef28c4b26fd78760fc7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.leakyrelu#torch.nn.LeakyReLU&quot;&gt;&lt;code&gt;LeakyReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.leakyrelu#torch.nn.LeakyReLU&quot;&gt; &lt;code&gt;LeakyReLU&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="2764126c51f5530622c4b4e091ebc79776ade07c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm&quot;&gt;&lt;code&gt;LocalResponseNorm&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm&quot;&gt; &lt;code&gt;LocalResponseNorm&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="e31635636f40648e7e735d06a4392b7cebd2b258" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.logsigmoid#torch.nn.LogSigmoid&quot;&gt;&lt;code&gt;LogSigmoid&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.logsigmoid#torch.nn.LogSigmoid&quot;&gt; &lt;code&gt;LogSigmoid&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="412a8150cd54d51d6530e2d245b63346bf33130a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt;&lt;code&gt;LogSoftmax&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt; &lt;code&gt;LogSoftmax&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="7f565430e626357bd4afefcb8e556e2cb25e9480" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.lppool1d#torch.nn.LPPool1d&quot;&gt;&lt;code&gt;LPPool1d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.lppool1d#torch.nn.LPPool1d&quot;&gt; &lt;code&gt;LPPool1d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="35b0452fd507c2aa91e1da43aa355009dc166d13" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.lppool2d#torch.nn.LPPool2d&quot;&gt;&lt;code&gt;LPPool2d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.lppool2d#torch.nn.LPPool2d&quot;&gt; &lt;code&gt;LPPool2d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="2cbda61d0da4455a2876aa2b87f22199f2f1cdb3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss&quot;&gt;&lt;code&gt;MarginRankingLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss&quot;&gt; &lt;code&gt;MarginRankingLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="898e6a57e299ae9b57ccd5993abb1ef8bd55140b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt;&lt;code&gt;MaxPool1d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt; &lt;code&gt;MaxPool1d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="b0dc529ce91ee71820fbf1ed9bed0661ddc036f6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt;&lt;code&gt;MaxPool2d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt; &lt;code&gt;MaxPool2d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="b6be4f2f66ae6f0a242da37cad7e7f70f4a8e328" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt;&lt;code&gt;MaxPool3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt; &lt;code&gt;MaxPool3d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="e5de0ae10e2d087fd9d49043502233c22cd7d1ed" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d&quot;&gt;&lt;code&gt;MaxUnpool1d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d&quot;&gt; &lt;code&gt;MaxUnpool1d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="06bc4bd975e3b0441df74b337a9e03841cdc0854" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d&quot;&gt;&lt;code&gt;MaxUnpool2d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d&quot;&gt; &lt;code&gt;MaxUnpool2d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="be3c23907ec483dcbebdd45f57208e45ff569446" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d&quot;&gt;&lt;code&gt;MaxUnpool3d&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d&quot;&gt; &lt;code&gt;MaxUnpool3d&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="787cf804e853557c82df78674a398c819b20b9c8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.mseloss#torch.nn.MSELoss&quot;&gt;&lt;code&gt;MSELoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.mseloss#torch.nn.MSELoss&quot;&gt; &lt;code&gt;MSELoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="01976be184fbec1b413106ad9e281456840b581b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss&quot;&gt;&lt;code&gt;MultiLabelMarginLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss&quot;&gt; &lt;code&gt;MultiLabelMarginLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="7aa5a41923f45ec7053a3fcb00c51f4757953233" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss&quot;&gt;&lt;code&gt;MultiLabelSoftMarginLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss&quot;&gt; &lt;code&gt;MultiLabelSoftMarginLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="d1b726bdc0737f6680b51d100e20056a744ff1e6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss&quot;&gt;&lt;code&gt;MultiMarginLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss&quot;&gt; &lt;code&gt;MultiMarginLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="729c99354cfb624e609e3f72dffceaad9cd0847c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;NLLLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt; &lt;code&gt;NLLLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="8ee9817c275d3255a1242d0c82f69825ae4ec439" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance&quot;&gt;&lt;code&gt;torch.nn.PairwiseDistance&lt;/code&gt;&lt;/a&gt; for details</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance&quot;&gt; &lt;code&gt;torch.nn.PairwiseDistance&lt;/code&gt; &lt;/a&gt;を参照してください</target>
        </trans-unit>
        <trans-unit id="100c5c298a4adf254a044824d7e8f1c566ed72d4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle&quot;&gt;&lt;code&gt;PixelShuffle&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle&quot;&gt; &lt;code&gt;PixelShuffle&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="62b7addc0514a1405a79ca49e242813d820bc663" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss&quot;&gt;&lt;code&gt;PoissonNLLLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss&quot;&gt; &lt;code&gt;PoissonNLLLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="50b2df5a36c2945c6e24d5c71f419748722a3887" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.prelu#torch.nn.PReLU&quot;&gt;&lt;code&gt;PReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.prelu#torch.nn.PReLU&quot;&gt; &lt;code&gt;PReLU&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="5978d8e05fcfe3f5b05baa41628058a1104f3a4e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.relu6#torch.nn.ReLU6&quot;&gt;&lt;code&gt;ReLU6&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.relu6#torch.nn.ReLU6&quot;&gt; &lt;code&gt;ReLU6&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="7f52859f67be89a0431749df85025dfa79a6a4c5" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.rrelu#torch.nn.RReLU&quot;&gt;&lt;code&gt;RReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.rrelu#torch.nn.RReLU&quot;&gt; &lt;code&gt;RReLU&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="852d4ca82259db7394eed56f5e5c7736dab64bcf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.selu#torch.nn.SELU&quot;&gt;&lt;code&gt;SELU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.selu#torch.nn.SELU&quot;&gt; &lt;code&gt;SELU&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="13f47034bf13462a634f77a02e2bd8afda50269b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.sigmoid#torch.nn.Sigmoid&quot;&gt;&lt;code&gt;Sigmoid&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.sigmoid#torch.nn.Sigmoid&quot;&gt; &lt;code&gt;Sigmoid&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="568ac6254f20261c85c9d016122513b35b9c681e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.silu#torch.nn.SiLU&quot;&gt;&lt;code&gt;SiLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.silu#torch.nn.SiLU&quot;&gt; &lt;code&gt;SiLU&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="dd919bcb7cd8684a3301f3776235810cef864477" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss&quot;&gt;&lt;code&gt;SmoothL1Loss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss&quot;&gt; &lt;code&gt;SmoothL1Loss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="aec2f6e607f94f542eec97b84c6a6b548899b489" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss&quot;&gt;&lt;code&gt;SoftMarginLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss&quot;&gt; &lt;code&gt;SoftMarginLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="9c76fd9ca241b6dc4f7f601122680872738372dd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softmax#torch.nn.Softmax&quot;&gt;&lt;code&gt;Softmax&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.softmax#torch.nn.Softmax&quot;&gt; &lt;code&gt;Softmax&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="6858b7405b4fb52e94900af334a9f8d7d36cdcef" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softmin#torch.nn.Softmin&quot;&gt;&lt;code&gt;Softmin&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.softmin#torch.nn.Softmin&quot;&gt; &lt;code&gt;Softmin&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="3078ba39159561c5fdab61a5073beb5e6d358df8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softplus#torch.nn.Softplus&quot;&gt;&lt;code&gt;Softplus&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.softplus#torch.nn.Softplus&quot;&gt; &lt;code&gt;Softplus&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="169ff197b89985d58ad2ce1e9b54fb8b6c45231c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softshrink#torch.nn.Softshrink&quot;&gt;&lt;code&gt;Softshrink&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.softshrink#torch.nn.Softshrink&quot;&gt; &lt;code&gt;Softshrink&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="cdc172b1efd10ce5727c58366e9d3635597d6ae5" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.softsign#torch.nn.Softsign&quot;&gt;&lt;code&gt;Softsign&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.softsign#torch.nn.Softsign&quot;&gt; &lt;code&gt;Softsign&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="29958016ecc903cab7fa7360e164fabacfbf9cca" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.tanh#torch.nn.Tanh&quot;&gt;&lt;code&gt;Tanh&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.tanh#torch.nn.Tanh&quot;&gt; &lt;code&gt;Tanh&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="a254e517f98f621d085febfde8c38a66e6d36e75" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.tanhshrink#torch.nn.Tanhshrink&quot;&gt;&lt;code&gt;Tanhshrink&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.tanhshrink#torch.nn.Tanhshrink&quot;&gt; &lt;code&gt;Tanhshrink&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="35902a0c2cfa9e585fec0c3e3666693fec9dd4a9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.threshold#torch.nn.Threshold&quot;&gt;&lt;code&gt;Threshold&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.threshold#torch.nn.Threshold&quot;&gt; &lt;code&gt;Threshold&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="7ce6b129bc423f40aba54b0c2ce12c00beddaaaf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss&quot;&gt;&lt;code&gt;TripletMarginLoss&lt;/code&gt;&lt;/a&gt; for details</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss&quot;&gt; &lt;code&gt;TripletMarginLoss&lt;/code&gt; &lt;/a&gt;を参照してください</target>
        </trans-unit>
        <trans-unit id="e3fff4676883a1b11b9e2b38b57ad823ba8354c4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss&quot;&gt;&lt;code&gt;TripletMarginWithDistanceLoss&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss&quot;&gt; &lt;code&gt;TripletMarginWithDistanceLoss&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="f6ce3d99c47d2a3129d44cea56b1d906e3f87b1a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nn.unfold#torch.nn.Unfold&quot;&gt;&lt;code&gt;torch.nn.Unfold&lt;/code&gt;&lt;/a&gt; for details</source>
          <target state="translated">詳細については、&lt;a href=&quot;generated/torch.nn.unfold#torch.nn.Unfold&quot;&gt; &lt;code&gt;torch.nn.Unfold&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="9788bdbd224f6b9c0bd977e955f549886daf1bcf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.nonzero#torch.nonzero&quot;&gt;&lt;code&gt;torch.nonzero()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nonzero#torch.nonzero&quot;&gt; &lt;code&gt;torch.nonzero()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="2369f591ef4e6e16ec27b6f775edd376eeeaebaa" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.norm#torch.norm&quot;&gt;&lt;code&gt;torch.norm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.norm#torch.norm&quot;&gt; &lt;code&gt;torch.norm()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="56e7e5c8709e5c8e2f93b31866c6deb6c856253e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.not_equal#torch.not_equal&quot;&gt;&lt;code&gt;torch.not_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.not_equal#torch.not_equal&quot;&gt; &lt;code&gt;torch.not_equal()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="5b9ce8340dc0317b49aade691cfc70799f4e8a00" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.numel#torch.numel&quot;&gt;&lt;code&gt;torch.numel()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.numel#torch.numel&quot;&gt; &lt;code&gt;torch.numel()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="c4622b4f7c7ee0a0d13c8074a55141e4ff3229c8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.orgqr#torch.orgqr&quot;&gt;&lt;code&gt;torch.orgqr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.orgqr#torch.orgqr&quot;&gt; &lt;code&gt;torch.orgqr()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="d1edcc9509ea3d17ee6a647709ab6bf5071894e7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ormqr#torch.ormqr&quot;&gt;&lt;code&gt;torch.ormqr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.ormqr#torch.ormqr&quot;&gt; &lt;code&gt;torch.ormqr()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="5ec140159032ba8d0085d266f44eb6db3c1a6341" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="87e19d2ae8154d45a564a41febb06c16a86567e9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.pinverse#torch.pinverse&quot;&gt;&lt;code&gt;torch.pinverse()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.pinverse#torch.pinverse&quot;&gt; &lt;code&gt;torch.pinverse()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="7324cc21e0ad12841d92431a5e1818fae17672c9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.polygamma#torch.polygamma&quot;&gt;&lt;code&gt;torch.polygamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.polygamma#torch.polygamma&quot;&gt; &lt;code&gt;torch.polygamma()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="60b1f75853d0d1cd9cd5f0d2d089d7ee6125b0df" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.pow#torch.pow&quot;&gt;&lt;code&gt;torch.pow()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.pow#torch.pow&quot;&gt; &lt;code&gt;torch.pow()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="3b8e7716730933beddce93ceeb9531967a7b8f33" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.prod#torch.prod&quot;&gt;&lt;code&gt;torch.prod()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.prod#torch.prod&quot;&gt; &lt;code&gt;torch.prod()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="b444d47f2174704778737bc705448fc08b1ec182" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.qr#torch.qr&quot;&gt;&lt;code&gt;torch.qr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.qr#torch.qr&quot;&gt; &lt;code&gt;torch.qr()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="3c72437f61c64c17a7e0b66bf3ffe01e9afc41be" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="32fb1a354f8c66ea4a0848048572ffcf87e84e6f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.rad2deg#torch.rad2deg&quot;&gt;&lt;code&gt;torch.rad2deg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.rad2deg#torch.rad2deg&quot;&gt; &lt;code&gt;torch.rad2deg()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="bc4ab991cb3b46b0132c1ba3ff2b91cc0500fdf4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.reciprocal#torch.reciprocal&quot;&gt;&lt;code&gt;torch.reciprocal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.reciprocal#torch.reciprocal&quot;&gt; &lt;code&gt;torch.reciprocal()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="bfe7f00978e3613a7a554678c8956f4cb13251a3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.remainder#torch.remainder&quot;&gt;&lt;code&gt;torch.remainder()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.remainder#torch.remainder&quot;&gt; &lt;code&gt;torch.remainder()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="47bdb2571c3f363d76dceb91442f7ba7bc5f47c8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.renorm#torch.renorm&quot;&gt;&lt;code&gt;torch.renorm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.renorm#torch.renorm&quot;&gt; &lt;code&gt;torch.renorm()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="392bb5e2b00d5ad1124b111f4a08963b839d220b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.repeat_interleave#torch.repeat_interleave&quot;&gt;&lt;code&gt;torch.repeat_interleave()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.repeat_interleave#torch.repeat_interleave&quot;&gt; &lt;code&gt;torch.repeat_interleave()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="5d77b448fa6a312c6a3af7702ed4407f9a96ffff" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt;&lt;code&gt;torch.reshape()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt; &lt;code&gt;torch.reshape()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="64c78ad14ae1246bf4029f36289f782990d17cc2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;torch.rfft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;torch.rfft()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="edc899a0ae7fde4858b6a50cce775b0372b7653c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.roll#torch.roll&quot;&gt;&lt;code&gt;torch.roll()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.roll#torch.roll&quot;&gt; &lt;code&gt;torch.roll()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="7815d433685f994781f0429772db78da8710e65c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.rot90#torch.rot90&quot;&gt;&lt;code&gt;torch.rot90()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.rot90#torch.rot90&quot;&gt; &lt;code&gt;torch.rot90()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="0947a333f5346f54b736a4223ca159d84235f8b5" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.round#torch.round&quot;&gt;&lt;code&gt;torch.round()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.round#torch.round&quot;&gt; &lt;code&gt;torch.round()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="f3f439c8c8008d964e1bcaba78d49e70b3cb5ee0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.rsqrt#torch.rsqrt&quot;&gt;&lt;code&gt;torch.rsqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.rsqrt#torch.rsqrt&quot;&gt; &lt;code&gt;torch.rsqrt()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="8f1a0ad46bcdd4a06fd3e3e914c5990d45babeaf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sigmoid#torch.sigmoid&quot;&gt;&lt;code&gt;torch.sigmoid()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sigmoid#torch.sigmoid&quot;&gt; &lt;code&gt;torch.sigmoid()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="d03b7125b49df42263bd14ed0db4426d77994382" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sign#torch.sign&quot;&gt;&lt;code&gt;torch.sign()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sign#torch.sign&quot;&gt; &lt;code&gt;torch.sign()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="0966c7edc653b2d2d7858c36c7f0e9e1d3aea910" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.signbit#torch.signbit&quot;&gt;&lt;code&gt;torch.signbit()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.signbit#torch.signbit&quot;&gt; &lt;code&gt;torch.signbit()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="241d2b853c66fa249e8726729a4ac39a10b2e697" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sin#torch.sin&quot;&gt;&lt;code&gt;torch.sin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sin#torch.sin&quot;&gt; &lt;code&gt;torch.sin()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="89a061ad1e618c20982077437a8a87568c9307c4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sinh#torch.sinh&quot;&gt;&lt;code&gt;torch.sinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sinh#torch.sinh&quot;&gt; &lt;code&gt;torch.sinh()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="a499f1efa1ffa37cdc83e453614c6da1a94917dc" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.slogdet#torch.slogdet&quot;&gt;&lt;code&gt;torch.slogdet()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.slogdet#torch.slogdet&quot;&gt; &lt;code&gt;torch.slogdet()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="b94d091f48db8b1ed862846a551122f839916ce3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.solve#torch.solve&quot;&gt;&lt;code&gt;torch.solve()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.solve#torch.solve&quot;&gt; &lt;code&gt;torch.solve()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="a7969c92902441e24ef604b1fb7e01e1633b60a0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sort#torch.sort&quot;&gt;&lt;code&gt;torch.sort()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sort#torch.sort&quot;&gt; &lt;code&gt;torch.sort()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="ec7801816e74ec7a3d900fbf20c476c205206e5a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.split#torch.split&quot;&gt;&lt;code&gt;torch.split()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.split#torch.split&quot;&gt; &lt;code&gt;torch.split()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="39e5cf9eb721bd472b296db97e93a6ca162d7a75" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sqrt#torch.sqrt&quot;&gt;&lt;code&gt;torch.sqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sqrt#torch.sqrt&quot;&gt; &lt;code&gt;torch.sqrt()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="1e20283dbffe987cb9aa4c639a5d348b483031bd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.square#torch.square&quot;&gt;&lt;code&gt;torch.square()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.square#torch.square&quot;&gt; &lt;code&gt;torch.square()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="4537022c26f4e739fc2002bc8948554d909b1ec0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="b79329d6e4ba347ba65f0ebd150eb9025549e650" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.std#torch.std&quot;&gt;&lt;code&gt;torch.std()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.std#torch.std&quot;&gt; &lt;code&gt;torch.std()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="8072306c8cd74c97cd95675a2ff3d6b6cdfaa602" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.stft#torch.stft&quot;&gt; &lt;code&gt;torch.stft()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="18ce7dc42c983592bcaecd4a3fc5855fdfc8d9ab" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt; &lt;code&gt;torch.sub()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="af5eb1862625e7193f970d356cf7c80cbe6b8474" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.subtract#torch.subtract&quot;&gt;&lt;code&gt;torch.subtract()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.subtract#torch.subtract&quot;&gt; &lt;code&gt;torch.subtract()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="66057444812a0310739be9c55bf3b8e63f04f931" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.sum#torch.sum&quot;&gt;&lt;code&gt;torch.sum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sum#torch.sum&quot;&gt; &lt;code&gt;torch.sum()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="bcf75355dab89c53d2da96f041aaf2f0acf7b6d2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.svd#torch.svd&quot;&gt;&lt;code&gt;torch.svd()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.svd#torch.svd&quot;&gt; &lt;code&gt;torch.svd()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="dfa85538b9855279675cb6cbe04a9c51aa801f91" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.symeig#torch.symeig&quot;&gt;&lt;code&gt;torch.symeig()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.symeig#torch.symeig&quot;&gt; &lt;code&gt;torch.symeig()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="4c1d54d9efffe6c88816f66acc03d8d8dfbcb987" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.t#torch.t&quot;&gt;&lt;code&gt;torch.t()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.t#torch.t&quot;&gt; &lt;code&gt;torch.t()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="ed0de6f96e833610021fd49c3f7874b4e1addb21" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.take#torch.take&quot;&gt;&lt;code&gt;torch.take()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.take#torch.take&quot;&gt; &lt;code&gt;torch.take()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="99e605d53360cc080a69f4f5d980d95d7aa826ad" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.tan#torch.tan&quot;&gt;&lt;code&gt;torch.tan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.tan#torch.tan&quot;&gt; &lt;code&gt;torch.tan()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="e96f7967fc1e38f0303eea058aa618aef4fac2ef" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.tanh#torch.tanh&quot;&gt;&lt;code&gt;torch.tanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.tanh#torch.tanh&quot;&gt; &lt;code&gt;torch.tanh()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="777022f41fcab6d3a8b8bc1104eab2f51f7b9171" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.topk#torch.topk&quot;&gt;&lt;code&gt;torch.topk()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.topk#torch.topk&quot;&gt; &lt;code&gt;torch.topk()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="c49ff88894110989a51d17dc1b799b6a1d234266" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.trace#torch.trace&quot;&gt;&lt;code&gt;torch.trace()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.trace#torch.trace&quot;&gt; &lt;code&gt;torch.trace()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="4dd4e10bc975adcbd79cbfba2536b23c1ca9fe5d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.transpose#torch.transpose&quot;&gt;&lt;code&gt;torch.transpose()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.transpose#torch.transpose&quot;&gt; &lt;code&gt;torch.transpose()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="2bade69272d3a8a621109ac0c3cebe47f8592ccf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.triangular_solve#torch.triangular_solve&quot;&gt;&lt;code&gt;torch.triangular_solve()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.triangular_solve#torch.triangular_solve&quot;&gt; &lt;code&gt;torch.triangular_solve()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="f87e190394989d10b94b8ed6f12d07725726bba2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.tril#torch.tril&quot;&gt;&lt;code&gt;torch.tril()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.tril#torch.tril&quot;&gt; &lt;code&gt;torch.tril()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="d172b311d0d769ff2c0ed9dd6ac700f12b7519eb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.triu#torch.triu&quot;&gt;&lt;code&gt;torch.triu()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.triu#torch.triu&quot;&gt; &lt;code&gt;torch.triu()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="b7d12fec787dfa2633076e0c7bc951ad201eab36" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.true_divide#torch.true_divide&quot;&gt;&lt;code&gt;torch.true_divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.true_divide#torch.true_divide&quot;&gt; &lt;code&gt;torch.true_divide()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="2e33491c2346b21653dcb58517e74623e00bc1cf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt; &lt;code&gt;torch.trunc()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="1b23641ab143374e2f32432ecf86e4e46888de3d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.unbind#torch.unbind&quot;&gt;&lt;code&gt;torch.unbind()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.unbind#torch.unbind&quot;&gt; &lt;code&gt;torch.unbind()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="19b180c6110b55208ea1c8b65bc50f246a5ff942" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.unique#torch.unique&quot;&gt;&lt;code&gt;torch.unique()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.unique#torch.unique&quot;&gt; &lt;code&gt;torch.unique()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="54a43b1289dd07e0a5b73c772257b5c512989147" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.unique_consecutive#torch.unique_consecutive&quot;&gt; &lt;code&gt;torch.unique_consecutive()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="bd1faf4696524ab565ebe96977ff8f6d3996368c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.unsqueeze#torch.unsqueeze&quot;&gt;&lt;code&gt;torch.unsqueeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.unsqueeze#torch.unsqueeze&quot;&gt; &lt;code&gt;torch.unsqueeze()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="f4056678fae5769fa6b3f1cdb6156ce7a2397a43" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.var#torch.var&quot;&gt;&lt;code&gt;torch.var()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.var#torch.var&quot;&gt; &lt;code&gt;torch.var()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="916b508c9cb38c716d16bc8c991ff57f921295f9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.vdot#torch.vdot&quot;&gt;&lt;code&gt;torch.vdot()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.vdot#torch.vdot&quot;&gt; &lt;code&gt;torch.vdot()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="3b5ecc20fdf55d0eb2ca422795c8c3780a985baa" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1602.07868&quot;&gt;https://arxiv.org/abs/1602.07868&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1602.07868&quot;&gt;https://arxiv.org/abs/1602.07868を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="0b9412e01247af2b574511c1ccac61f158442f0f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;&gt;Gaussian Error Linear Units (GELUs)&lt;/a&gt; where the SiLU (Sigmoid Linear Unit) was originally coined, and see &lt;a href=&quot;https://arxiv.org/abs/1702.03118&quot;&gt;Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1710.05941v1&quot;&gt;Swish: a Self-Gated Activation Function&lt;/a&gt; where the SiLU was experimented with later.</source>
          <target state="translated">参照してください&lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;&gt;ガウスエラー線形単位（GELUs）&lt;/a&gt; SiLU（S字リニアユニットは）もともと鋳造された、と見る&lt;a href=&quot;https://arxiv.org/abs/1702.03118&quot;&gt;強化学習におけるニューラルネットワーク機能近似のためのシグモイド加重線形単位を&lt;/a&gt;して&lt;a href=&quot;https://arxiv.org/abs/1710.05941v1&quot;&gt;パッ：自己ゲーテッドアクティベーション機能&lt;/a&gt;SiLUがで実験しました。後で。</target>
        </trans-unit>
        <trans-unit id="14f3ddc89b63e2e7ace4c43afa51eb155bb8b073" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;&gt;Gaussian Error Linear Units (GELUs)&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;&gt;ガウス誤差線形単位（GELU）を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="bd1bf1046086c393add977bf1d0da3c9f0372ead" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1612.08083&quot;&gt;Language Modeling with Gated Convolutional Networks&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1612.08083&quot;&gt;ゲート畳み込みネットワーク&lt;/a&gt;を使用した言語モデリングを参照してください。</target>
        </trans-unit>
        <trans-unit id="db07a2a989e739b25605b6e4d0716eb84e5dae0a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://arxiv.org/abs/1802.05957&quot;&gt;Spectral Normalization for Generative Adversarial Networks&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1802.05957&quot;&gt;生成的敵対的ネットワークのスペクトル正規化を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="5a6dff82856c75a203e206e75be968948b922059" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/test/test_cpp_extensions.py&quot;&gt;the tests&lt;/a&gt; for good examples of using this function.</source>
          <target state="translated">この関数の使用例について&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/test/test_cpp_extensions.py&quot;&gt;は、テスト&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="04429ddfa22962a6b5135b0b2014ba16416b5a3a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-management&quot;&gt;Memory management&lt;/a&gt; for more details about GPU memory management.</source>
          <target state="translated">GPUメモリ管理の詳細については、&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-management&quot;&gt;メモリ管理&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="17c133c8d2a0e8bbfe7a62feb9aac8d574c244ed" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACK documentation for geqrf&lt;/a&gt; for further details.</source>
          <target state="translated">詳細&lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;については、geqrfのLAPACKドキュメントを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="21fd55c2238e14aca1f89595f1ab24153a2317e2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;jit_unsupported#jit-unsupported&quot;&gt;TorchScript Unsupported Pytorch Constructs&lt;/a&gt; for a list of unsupported PyTorch functions and modules.</source>
          <target state="translated">サポートされていないPyTorch関数とモジュールのリストについては、&lt;a href=&quot;jit_unsupported#jit-unsupported&quot;&gt;TorchScriptのサポートされていないPytorchコンストラクト&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="867beb050ab38870ddc832eb3ecef0bd481ecc8d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors operator coverage&lt;/a&gt; for a full list of the supported torch and tensor operations. We do not yet support the following that is not covered by the link:</source>
          <target state="translated">サポートされているトーチおよびテンソル操作の完全なリストについては、&lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;名前付きテンソル演算子のカバレッジ&lt;/a&gt;を参照してください。リンクでカバーされていない以下はまだサポートされていません。</target>
        </trans-unit>
        <trans-unit id="1babcf3a21d12db47391affdb4c96e1dd62be4d6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;nn.functional#torch.nn.functional.hardshrink&quot;&gt;&lt;code&gt;torch.nn.functional.hardshrink()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;nn.functional#torch.nn.functional.hardshrink&quot;&gt; &lt;code&gt;torch.nn.functional.hardshrink()&lt;/code&gt; を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="1db234df13e3a4d61ca75e6b5a3105bfac530a9e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;nn.functional#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt; for implementation details.</source>
          <target state="translated">参照&lt;a href=&quot;nn.functional#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt;実装の詳細については。</target>
        </trans-unit>
        <trans-unit id="b45b38436f82adea06622961c718c298fe2b5459" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.jit.save#torch.jit.save&quot;&gt;&lt;code&gt;torch.jit.save&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">詳細については、&lt;a href=&quot;torch.jit.save#torch.jit.save&quot;&gt; &lt;code&gt;torch.jit.save&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="906e8a62d5d0063bfb8aed76d72645065a77e807" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt;&lt;code&gt;torch.jit.trace&lt;/code&gt;&lt;/a&gt; for more information on tracing.</source>
          <target state="translated">トレースの詳細については、&lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt; &lt;code&gt;torch.jit.trace&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="777f4ad759227cc1de547b64fab825a56ffc273e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.maximum#torch.maximum&quot;&gt;&lt;code&gt;torch.maximum()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.maximum#torch.maximum&quot;&gt; &lt;code&gt;torch.maximum()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="ee5434d3ddff1ae2016c291b61569fbc56d53430" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.minimum#torch.minimum&quot;&gt;&lt;code&gt;torch.minimum()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.minimum#torch.minimum&quot;&gt; &lt;code&gt;torch.minimum()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="0be965581dce920964b073625fcdd3abb3431e9c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt; for details on conjugate symmetry.</source>
          <target state="translated">参照&lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;共役対称性の詳細については、を。</target>
        </trans-unit>
        <trans-unit id="72e0d158f46758840a4164068e9e82210cb8c350" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;AdaptiveAvgPool2d&lt;/code&gt; for details and output shape.</source>
          <target state="translated">参照 &lt;code&gt;AdaptiveAvgPool2d&lt;/code&gt; を詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="9940c6d70818d17787385090e557dd2c785cc737" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;AvgPool2d&lt;/code&gt; for details and output shape.</source>
          <target state="translated">参照 &lt;code&gt;AvgPool2d&lt;/code&gt; を詳細と出力形状のために。</target>
        </trans-unit>
        <trans-unit id="f13aa780c2216fa1121984d0068e8f5c43a24fb7" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;FeatureAlphaDropout&lt;/code&gt; for details.</source>
          <target state="translated">詳細については、 &lt;code&gt;FeatureAlphaDropout&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="d51e9069af161e98bae58fb7454beeacc6c7dd49" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;MaxPool2d&lt;/code&gt; for details.</source>
          <target state="translated">詳細については、 &lt;code&gt;MaxPool2d&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="fe5ba62143f025d44dc65bfc7175cc40e26dc5a1" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;detect_anomaly&lt;/code&gt; above for details of the anomaly detection behaviour.</source>
          <target state="translated">参照してください &lt;code&gt;detect_anomaly&lt;/code&gt; 異常検出動作の詳細については、上記の。</target>
        </trans-unit>
        <trans-unit id="f8c535644744dd427c75006813886670b5ecbefc" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;torch.sgn()&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.sgn()&lt;/code&gt; を参照してください</target>
        </trans-unit>
        <trans-unit id="2d8243a2c0e464492c9d563c4f92c56ae3421bcc" translate="yes" xml:space="preserve">
          <source>See also</source>
          <target state="translated">参照:</target>
        </trans-unit>
        <trans-unit id="cf84761b907cce8756615b37c03c0de477c6797c" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.bernoulli&quot;&gt;&lt;code&gt;bernoulli()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.bernoulli#torch.bernoulli&quot;&gt;&lt;code&gt;torch.bernoulli()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.bernoulli&quot;&gt; &lt;code&gt;bernoulli()&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/torch.bernoulli#torch.bernoulli&quot;&gt; &lt;code&gt;torch.bernoulli()&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="6f81c151298858c7ededc5c4b44ed15fc0e739e1" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.dense_dim&quot;&gt;&lt;code&gt;Tensor.dense_dim()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.dense_dim&quot;&gt; &lt;code&gt;Tensor.dense_dim()&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="a95f7e982847b8fe1f244ef9a89f823deb4ea379" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.indices&quot;&gt;&lt;code&gt;Tensor.indices()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.indices&quot;&gt; &lt;code&gt;Tensor.indices()&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="5404c54976e3ca510ce12e37b5127349f618c641" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.sparse_dim&quot;&gt;&lt;code&gt;Tensor.sparse_dim()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.sparse_dim&quot;&gt; &lt;code&gt;Tensor.sparse_dim()&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="54ff5675a07df4058683c73c3542b2001d40f9cf" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;#torch.Tensor.values&quot;&gt;&lt;code&gt;Tensor.values()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.values&quot;&gt; &lt;code&gt;Tensor.values()&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="d7cb8cf286ca9cd05efe1662c9601fe839849b83" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;https://en.wikipedia.org/wiki/One-hot&quot;&gt;One-hot on Wikipedia&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/One-hot&quot;&gt;ウィキペディアのOne-hot&lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="1b57d4272104e4b0612e05446cf15e6f3bd6c435" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss&quot;&gt;&lt;code&gt;TripletMarginLoss&lt;/code&gt;&lt;/a&gt;, which computes the triplet loss for input tensors using the</source>
          <target state="translated">&lt;a href=&quot;torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss&quot;&gt; &lt;code&gt;TripletMarginLoss&lt;/code&gt; &lt;/a&gt;も参照してください。これは、を使用して入力テンソルのトリプレット損失を計算します。</target>
        </trans-unit>
        <trans-unit id="338b9a67a86b6e51ea1d6e3591262fbb1f90e795" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss&quot;&gt;&lt;code&gt;TripletMarginWithDistanceLoss&lt;/code&gt;&lt;/a&gt;, which computes the triplet margin loss for input tensors using a custom distance function.</source>
          <target state="translated">カスタム距離関数を使用して入力テンソルのトリプレットマージン損失を計算する&lt;a href=&quot;torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss&quot;&gt; &lt;code&gt;TripletMarginWithDistanceLoss&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="be5e1942829e9e077f50ab2375a2b7c88388f6d1" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;torch.nonzero#torch.nonzero&quot;&gt;&lt;code&gt;torch.nonzero()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nonzero#torch.nonzero&quot;&gt; &lt;code&gt;torch.nonzero()&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="3944875c16171bc90184fc5b9486bebdfeaab438" translate="yes" xml:space="preserve">
          <source>See also: &lt;a href=&quot;../distributed#distributed-basics&quot;&gt;Basics&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-nn-ddp-instead&quot;&gt;Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel&lt;/a&gt;. The same constraints on input as in &lt;a href=&quot;torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;torch.nn.DataParallel&lt;/code&gt;&lt;/a&gt; apply.</source>
          <target state="translated">参照：&lt;a href=&quot;../distributed#distributed-basics&quot;&gt;基本&lt;/a&gt;および&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-nn-ddp-instead&quot;&gt;マルチプロセッシングまたはnn.DataParallelの代わりにnn.parallel.DistributedDataParallel&lt;/a&gt;を使用します。&lt;a href=&quot;torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt; &lt;code&gt;torch.nn.DataParallel&lt;/code&gt; &lt;/a&gt;と同じ入力制約が適用されます。</target>
        </trans-unit>
        <trans-unit id="57828c6fcf489090610d5b402c9d6f3f4fda9c73" translate="yes" xml:space="preserve">
          <source>See also: &lt;a href=&quot;generated/torch.multinomial#torch.multinomial&quot;&gt;&lt;code&gt;torch.multinomial()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参照：&lt;a href=&quot;generated/torch.multinomial#torch.multinomial&quot;&gt; &lt;code&gt;torch.multinomial()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7eb108ecfdf49da8d98dacb8dde1315fef208724" translate="yes" xml:space="preserve">
          <source>See also: &lt;code&gt;saving-loading-tensors&lt;/code&gt;</source>
          <target state="translated">参照： &lt;code&gt;saving-loading-tensors&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e8334cf7461d5baada537c0ffa23cfd90187f5e4" translate="yes" xml:space="preserve">
          <source>See also: &lt;code&gt;torch.distributions.Categorical()&lt;/code&gt; for specifications of &lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">参照してください： &lt;code&gt;torch.distributions.Categorical()&lt;/code&gt; の仕様について&lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6eaee759e08dae96ac3e7296ea90bee9503c008c" translate="yes" xml:space="preserve">
          <source>See below for examples.</source>
          <target state="translated">下記の例を参照してください。</target>
        </trans-unit>
        <trans-unit id="9f51e613b80a9cad0a8955652d10c4dc58a70617" translate="yes" xml:space="preserve">
          <source>See below for more details on the two behaviors.</source>
          <target state="translated">2つの行動の詳細は以下を参照してください。</target>
        </trans-unit>
        <trans-unit id="4ddb6a37aae13cb0b24c2efb3a4dab06796b836f" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html&quot;&gt;cuDNN 8 Release Notes&lt;/a&gt; for more information.</source>
          <target state="translated">詳細については、&lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html&quot;&gt;cuDNN8リリースノート&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="8eeaf16a4b3b3eff601df5e984656d59eacad769" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-examples&quot;&gt;Automatic Mixed Precision examples&lt;/a&gt; for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).</source>
          <target state="translated">より複雑なシナリオ（たとえば、勾配ペナルティ、複数のモデル/損失、カスタム自動勾配関数）での使用法（勾配スケーリングとともに）については、&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-examples&quot;&gt;自動混合精度の例&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="f064248b1c5b1747c5f3357f6ac931ae0fbb78e9" translate="yes" xml:space="preserve">
          <source>See the Note on extending the autograd engine for more details on how to use this class: &lt;a href=&quot;https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd&quot;&gt;https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd&lt;/a&gt;</source>
          <target state="translated">このクラスの使用方法の詳細については、autogradエンジンの拡張に関する注記を参照してください：&lt;a href=&quot;https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd&quot;&gt;https&lt;/a&gt;：//pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd</target>
        </trans-unit>
        <trans-unit id="a3f25968fb38aca8940c3474e1a28527b6999cfd" translate="yes" xml:space="preserve">
          <source>See the example below.</source>
          <target state="translated">下記の例を参照してください。</target>
        </trans-unit>
        <trans-unit id="81019ba57a197ef19b2f977fef5caa9379d67190" translate="yes" xml:space="preserve">
          <source>See: &lt;a href=&quot;https://arxiv.org/pdf/1505.00853.pdf&quot;&gt;https://arxiv.org/pdf/1505.00853.pdf&lt;/a&gt;</source>
          <target state="translated">参照：&lt;a href=&quot;https://arxiv.org/pdf/1505.00853.pdf&quot;&gt;https&lt;/a&gt;：//arxiv.org/pdf/1505.00853.pdf</target>
        </trans-unit>
        <trans-unit id="a0155b122fd91b7c13b804a1099ae2d088140dfb" translate="yes" xml:space="preserve">
          <source>Semantic Segmentation</source>
          <target state="translated">セマンティックセグメンテーション</target>
        </trans-unit>
        <trans-unit id="c75579406c3ae78822e3ffcc9e4cc60ff251dd2d" translate="yes" xml:space="preserve">
          <source>Sender rank -1, if not part of the group</source>
          <target state="translated">送信者ランク -1、グループに属していない場合</target>
        </trans-unit>
        <trans-unit id="17a32bd6e1dfb35f3140b1db64642f975712c5d9" translate="yes" xml:space="preserve">
          <source>Sends a tensor asynchronously.</source>
          <target state="translated">テンソルを非同期に送信します。</target>
        </trans-unit>
        <trans-unit id="4c841aadee573fffcf4298f38f9bffb444801dab" translate="yes" xml:space="preserve">
          <source>Sends a tensor synchronously.</source>
          <target state="translated">テンソルを同期的に送信します。</target>
        </trans-unit>
        <trans-unit id="0edc0112db95758e96c3f9613104687a95e00afc" translate="yes" xml:space="preserve">
          <source>Sequential</source>
          <target state="translated">Sequential</target>
        </trans-unit>
        <trans-unit id="e7d74cd050fb66a4fa3c59e667aee9ae0ee10227" translate="yes" xml:space="preserve">
          <source>Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/a&gt; manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.</source>
          <target state="translated">シーケンシャルモデルは、モジュール/関数のリストを順番に（シーケンシャルに）実行します。したがって、このようなモデルをさまざまなセグメントに分割し、各セグメントをチェックポイントすることができます。最後を除くすべてのセグメントは、&lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt; &lt;code&gt;torch.no_grad()&lt;/code&gt; の&lt;/a&gt;方法で実行されます。つまり、中間アクティベーションは保存されません。各チェックポイントセグメントの入力は、バックワードパスでセグメントを再実行するために保存されます。</target>
        </trans-unit>
        <trans-unit id="89282363da677473acdbf45f0831bde9f4d3b6c4" translate="yes" xml:space="preserve">
          <source>Serialization</source>
          <target state="translated">Serialization</target>
        </trans-unit>
        <trans-unit id="086bcd13e430cfb2d3607712e21df8c908da9204" translate="yes" xml:space="preserve">
          <source>Serialization semantics</source>
          <target state="translated">シリアライズセマンティクス</target>
        </trans-unit>
        <trans-unit id="49d31523ecd7343c1a446e5158bebd854caf4035" translate="yes" xml:space="preserve">
          <source>Set options for printing.</source>
          <target state="translated">印刷のオプションを設定します。</target>
        </trans-unit>
        <trans-unit id="da649b091863ea75252b37be60743b57866836c1" translate="yes" xml:space="preserve">
          <source>Set options for printing. Items shamelessly taken from NumPy</source>
          <target state="translated">印刷のオプションを設定します。NumPyから恥ずかしげもなくパクったアイテム</target>
        </trans-unit>
        <trans-unit id="7f8fce6ab85b47c6992c9a753d4c65a4df880bed" translate="yes" xml:space="preserve">
          <source>Set the extra representation of the module</source>
          <target state="translated">モジュールの余分な表現を設定します。</target>
        </trans-unit>
        <trans-unit id="dd4c737e6e6f369967a01656e378372e2b057847" translate="yes" xml:space="preserve">
          <source>Set the learning rate of each parameter group using a cosine annealing schedule, where</source>
          <target state="translated">コサインアニーリングスケジュールを用いて各パラメータ群の学習率を設定します。</target>
        </trans-unit>
        <trans-unit id="fb1d30c28b5d6e9bf862978770dcfc4108953994" translate="yes" xml:space="preserve">
          <source>Set the result for this &lt;code&gt;Future&lt;/code&gt;, which will mark this &lt;code&gt;Future&lt;/code&gt; as completed and trigger all attached callbacks. Note that a &lt;code&gt;Future&lt;/code&gt; cannot be marked completed twice.</source>
          <target state="translated">このため、結果セットの &lt;code&gt;Future&lt;/code&gt; このマークし、 &lt;code&gt;Future&lt;/code&gt; 完了して、すべての添付コールバックをトリガします。 &lt;code&gt;Future&lt;/code&gt; を2回完了としてマークすることはできないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="2d01a2557701031cde3d1b8c62a166dbe983b48e" translate="yes" xml:space="preserve">
          <source>Set your device to local rank using either</source>
          <target state="translated">のいずれかを使用して、デバイスをローカルランクに設定します。</target>
        </trans-unit>
        <trans-unit id="8f9ed8e87d1736694183486ee8b4775a705262ed" translate="yes" xml:space="preserve">
          <source>Sets gradients of all model parameters to zero. See similar function under &lt;a href=&quot;../optim#torch.optim.Optimizer&quot;&gt;&lt;code&gt;torch.optim.Optimizer&lt;/code&gt;&lt;/a&gt; for more context.</source>
          <target state="translated">すべてのモデルパラメータの勾配をゼロに設定します。詳細については、&lt;a href=&quot;../optim#torch.optim.Optimizer&quot;&gt; &lt;code&gt;torch.optim.Optimizer&lt;/code&gt; の&lt;/a&gt;同様の関数を参照してください。</target>
        </trans-unit>
        <trans-unit id="d404cad224614646e396f2671954c99061ff4ca5" translate="yes" xml:space="preserve">
          <source>Sets the Generator state.</source>
          <target state="translated">ジェネレータの状態を設定します。</target>
        </trans-unit>
        <trans-unit id="9111f7925b4f9859890d95207d49fc1a4c287f5d" translate="yes" xml:space="preserve">
          <source>Sets the current device.</source>
          <target state="translated">現在のデバイスを設定します。</target>
        </trans-unit>
        <trans-unit id="599ca5f4725e8b4ae5c8a8c258bedbaa0ae4f7ca" translate="yes" xml:space="preserve">
          <source>Sets the default &lt;code&gt;torch.Tensor&lt;/code&gt; type to floating point tensor type &lt;code&gt;t&lt;/code&gt;.</source>
          <target state="translated">デフォルトの &lt;code&gt;torch.Tensor&lt;/code&gt; タイプを浮動小数点テンソルタイプ &lt;code&gt;t&lt;/code&gt; に設定します。</target>
        </trans-unit>
        <trans-unit id="017579c4ce17f464667bcc54bc1a6e8591f854a8" translate="yes" xml:space="preserve">
          <source>Sets the default &lt;code&gt;torch.Tensor&lt;/code&gt; type to floating point tensor type &lt;code&gt;t&lt;/code&gt;. This type will also be used as default floating point type for type inference in &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">デフォルトの &lt;code&gt;torch.Tensor&lt;/code&gt; タイプを浮動小数点テンソルタイプ &lt;code&gt;t&lt;/code&gt; に設定します。この型は、&lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; の&lt;/a&gt;型推論のデフォルトの浮動小数点型としても使用されます。</target>
        </trans-unit>
        <trans-unit id="0194ba9213175f7befa0a141d39b82b09c76ae20" translate="yes" xml:space="preserve">
          <source>Sets the default floating point dtype to &lt;code&gt;d&lt;/code&gt;.</source>
          <target state="translated">デフォルトの浮動小数点dtypeを &lt;code&gt;d&lt;/code&gt; に設定します。</target>
        </trans-unit>
        <trans-unit id="552e257ff3a4ad554910ae569da22027db68a506" translate="yes" xml:space="preserve">
          <source>Sets the default floating point dtype to &lt;code&gt;d&lt;/code&gt;. This dtype is:</source>
          <target state="translated">デフォルトの浮動小数点dtypeを &lt;code&gt;d&lt;/code&gt; に設定します。このdtypeは次のとおりです。</target>
        </trans-unit>
        <trans-unit id="135e0facc67febcd6e6046179fa5ad6d5c7dfcb0" translate="yes" xml:space="preserve">
          <source>Sets the gradients of all optimized &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; s to zero.</source>
          <target state="translated">最適化されたすべての&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;の勾配をゼロに設定します。</target>
        </trans-unit>
        <trans-unit id="00a00aa79d426ae2b3c0c06d23ade776e4f0d645" translate="yes" xml:space="preserve">
          <source>Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper &lt;a href=&quot;https://arxiv.org/abs/1506.01186&quot;&gt;Cyclical Learning Rates for Training Neural Networks&lt;/a&gt;. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.</source>
          <target state="translated">循環学習率ポリシー（CLR）に従って、各パラメーターグループの学習率を設定します。このポリシーは、「&lt;a href=&quot;https://arxiv.org/abs/1506.01186&quot;&gt;ニューラルネットワークのトレーニングのための循環学習率」で&lt;/a&gt;詳しく説明されているように、2つの境界間で一定の頻度で学習率を循環させます。2つの境界間の距離は、反復ごとまたはサイクルごとにスケーリングできます。</target>
        </trans-unit>
        <trans-unit id="665434656af0e1a615df6cc92c6ecf52d30fbd49" translate="yes" xml:space="preserve">
          <source>Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper &lt;a href=&quot;https://arxiv.org/abs/1708.07120&quot;&gt;Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates&lt;/a&gt;.</source>
          <target state="translated">1サイクル学習率ポリシーに従って、各パラメータグループの学習率を設定します。1cycleポリシーは、学習率を初期学習率から最大学習率に、次にその最大学習率から初期学習率よりはるかに低い最小学習率にアニーリングします。このポリシーは、最初に論文「&lt;a href=&quot;https://arxiv.org/abs/1708.07120&quot;&gt;超収束：大きな学習率を使用したニューラルネットワークの非常に高速なトレーニング」で&lt;/a&gt;説明されていました。</target>
        </trans-unit>
        <trans-unit id="03d13473984ea489da3481909e502cdf0dc06a49" translate="yes" xml:space="preserve">
          <source>Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="translated">各パラメータ群の学習率を、与えられた関数の初期値lr倍に設定します。last_epoch=-1の場合、初期値lrをlrに設定します。</target>
        </trans-unit>
        <trans-unit id="03f12972542800069e5267c12cb43edb6f65202d" translate="yes" xml:space="preserve">
          <source>Sets the module in evaluation mode.</source>
          <target state="translated">モジュールを評価モードに設定します。</target>
        </trans-unit>
        <trans-unit id="bf6241075b09cc953edd0e9568c4cd5f5c88fe95" translate="yes" xml:space="preserve">
          <source>Sets the module in training mode.</source>
          <target state="translated">トレーニングモードのモジュールを設定します。</target>
        </trans-unit>
        <trans-unit id="850e5cb239a66b02ec6c1c59e0300349c7abf33d" translate="yes" xml:space="preserve">
          <source>Sets the number of threads used for interop parallelism (e.g.</source>
          <target state="translated">インターホップ並列化に使用するスレッド数を設定します(例</target>
        </trans-unit>
        <trans-unit id="be8644f2249be95fe3ec1aaa39ce51051699b0f6" translate="yes" xml:space="preserve">
          <source>Sets the number of threads used for interop parallelism (e.g. in JIT interpreter) on CPU.</source>
          <target state="translated">CPU上のInterop並列化(JITインタプリタなど)に使用するスレッド数を設定します。</target>
        </trans-unit>
        <trans-unit id="cf85722cd322795a8d83eb653874b27e33c5355c" translate="yes" xml:space="preserve">
          <source>Sets the number of threads used for intraop parallelism on CPU.</source>
          <target state="translated">CPU のイントラオップ並列化に使用するスレッド数を設定します。</target>
        </trans-unit>
        <trans-unit id="9d725baae9b356dc9c5ed1aec81420b19033493b" translate="yes" xml:space="preserve">
          <source>Sets the random number generator state of all devices.</source>
          <target state="translated">すべてのデバイスの乱数発生器の状態を設定します。</target>
        </trans-unit>
        <trans-unit id="0d3f74d91be66fe1c0dcf86ea77ec056f2a793db" translate="yes" xml:space="preserve">
          <source>Sets the random number generator state of the specified GPU.</source>
          <target state="translated">指定されたGPUの乱数発生器の状態を設定します。</target>
        </trans-unit>
        <trans-unit id="97ab6a55939177c2033f769ee905c9c0419e2d5d" translate="yes" xml:space="preserve">
          <source>Sets the random number generator state.</source>
          <target state="translated">乱数発生器の状態を設定します。</target>
        </trans-unit>
        <trans-unit id="ba56ea18f53dc2a28624660de79c9c1b82cce789" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers for the current GPU. It&amp;rsquo;s safe to call this function if CUDA is not available; in that case, it is silently ignored.</source>
          <target state="translated">現在のGPUの乱数を生成するためのシードを設定します。CUDAが利用できない場合は、この関数を呼び出しても安全です。その場合、それは黙って無視されます。</target>
        </trans-unit>
        <trans-unit id="673ea6e5e56cccf50f89c4dcf1d57a9a886467da" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers on all GPUs. It&amp;rsquo;s safe to call this function if CUDA is not available; in that case, it is silently ignored.</source>
          <target state="translated">すべてのGPUで乱数を生成するためのシードを設定します。CUDAが利用できない場合は、この関数を呼び出しても安全です。その場合、それは黙って無視されます。</target>
        </trans-unit>
        <trans-unit id="45382a699be8898ec2b2618f2defa7c97e636940" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers to a non-deterministic random number.</source>
          <target state="translated">乱数を生成するためのシードを非決定論的乱数に設定します。</target>
        </trans-unit>
        <trans-unit id="7abac63ee55826e63cd8690359844f301cb36545" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG.</source>
          <target state="translated">乱数を生成するためのシードを非決定論的乱数に設定します。RNGのシードに使用する64ビットの数値を返します。</target>
        </trans-unit>
        <trans-unit id="d8cf00796144a8bf6bcaeca0922d063540aa0555" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers to a random number for the current GPU. It&amp;rsquo;s safe to call this function if CUDA is not available; in that case, it is silently ignored.</source>
          <target state="translated">乱数を生成するためのシードを、現在のGPUの乱数に設定します。CUDAが利用できない場合は、この関数を呼び出しても安全です。その場合、それは黙って無視されます。</target>
        </trans-unit>
        <trans-unit id="29567c11f6e9503a646d0f30b8e295f5e5873021" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers to a random number on all GPUs. It&amp;rsquo;s safe to call this function if CUDA is not available; in that case, it is silently ignored.</source>
          <target state="translated">乱数を生成するためのシードを、すべてのGPUで乱数に設定します。CUDAが利用できない場合は、この関数を呼び出しても安全です。その場合、それは黙って無視されます。</target>
        </trans-unit>
        <trans-unit id="82d234e8fcede91a904aeab469b5635a53af28cb" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers.</source>
          <target state="translated">乱数を生成するためのシードを設定します。</target>
        </trans-unit>
        <trans-unit id="90135ab68f94754348a83b2e9a672e458fc27687" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers. Returns a &lt;code&gt;torch.Generator&lt;/code&gt; object.</source>
          <target state="translated">乱数を生成するためのシードを設定します。 &lt;code&gt;torch.Generator&lt;/code&gt; オブジェクトを返します。</target>
        </trans-unit>
        <trans-unit id="ec7c3df03637450d6927de0719c85e704fa2352b" translate="yes" xml:space="preserve">
          <source>Sets the seed for generating random numbers. Returns a &lt;code&gt;torch.Generator&lt;/code&gt; object. It is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits. Avoid having many 0 bits in the seed.</source>
          <target state="translated">乱数を生成するためのシードを設定します。 &lt;code&gt;torch.Generator&lt;/code&gt; オブジェクトを返します。大きなシード、つまり0ビットと1ビットのバランスが良い数値を設定することをお勧めします。シードに多くの0ビットを含めることは避けてください。</target>
        </trans-unit>
        <trans-unit id="1f7618c30e9d279fa17b2c4bdef606d408ddac63" translate="yes" xml:space="preserve">
          <source>Sets the store&amp;rsquo;s default timeout. This timeout is used during initialization and in &lt;code&gt;wait()&lt;/code&gt; and &lt;code&gt;get()&lt;/code&gt;.</source>
          <target state="translated">ストアのデフォルトのタイムアウトを設定します。このタイムアウトは、初期化中、および &lt;code&gt;wait()&lt;/code&gt; と &lt;code&gt;get()&lt;/code&gt; で使用されます。</target>
        </trans-unit>
        <trans-unit id="403c562468c4aed511a578286850fe13a038a13a" translate="yes" xml:space="preserve">
          <source>Sets the strategy for sharing CPU tensors.</source>
          <target state="translated">CPUテンソルを共有するための戦略を設定します。</target>
        </trans-unit>
        <trans-unit id="2fdbbd0a94629feee695b40b70eaf4cd936d83f2" translate="yes" xml:space="preserve">
          <source>Sets the underlying storage, size, and strides. If &lt;code&gt;source&lt;/code&gt; is a tensor, &lt;code&gt;self&lt;/code&gt; tensor will share the same storage and have the same size and strides as &lt;code&gt;source&lt;/code&gt;. Changes to elements in one tensor will be reflected in the other.</source>
          <target state="translated">基になるストレージ、サイズ、およびストライドを設定します。 &lt;code&gt;source&lt;/code&gt; がテンソルの場合、 &lt;code&gt;self&lt;/code&gt; テンソルは同じストレージを共有し、 &lt;code&gt;source&lt;/code&gt; と同じサイズとストライドを持ちます。一方のテンソルの要素への変更は、もう一方のテンソルに反映されます。</target>
        </trans-unit>
        <trans-unit id="0bcd71b693d5fcc4b6051caa2db49bd3be5ace5c" translate="yes" xml:space="preserve">
          <source>Sets whether PyTorch operations must use &amp;ldquo;deterministic&amp;rdquo; algorithms.</source>
          <target state="translated">PyTorch操作で「決定論的」アルゴリズムを使用する必要があるかどうかを設定します。</target>
        </trans-unit>
        <trans-unit id="1e8c135ec7a3af0dd0ffe1fe388882479c98805f" translate="yes" xml:space="preserve">
          <source>Sets whether PyTorch operations must use &amp;ldquo;deterministic&amp;rdquo; algorithms. That is, algorithms which, given the same input, and when run on the same software and hardware, always produce the same output. When True, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw a :class:RuntimeError when called.</source>
          <target state="translated">PyTorch操作で「決定論的」アルゴリズムを使用する必要があるかどうかを設定します。つまり、同じ入力が与えられ、同じソフトウェアとハ​​ードウェアで実行された場合、常に同じ出力を生成するアルゴリズムです。Trueの場合、操作は使用可能な場合は決定論的アルゴリズムを使用し、非決定論的アルゴリズムのみが使用可能な場合は、呼び出されたときに：class：RuntimeErrorをスローします。</target>
        </trans-unit>
        <trans-unit id="8145a6b5d6477fdcd3f4ae8c4f2e54d4aea089cc" translate="yes" xml:space="preserve">
          <source>Sets whether to materialize output grad tensors. Default is true.</source>
          <target state="translated">出力階調テンソルを実体化するかどうかを設定します。デフォルトはtrueです。</target>
        </trans-unit>
        <trans-unit id="9f66b4449eb38b9763427d77c0a12d7806005f6b" translate="yes" xml:space="preserve">
          <source>Setting &lt;code&gt;return_complex&lt;/code&gt; explicitly will be required in a future PyTorch release. Set it to False to preserve the current behavior or True to return a complex output.</source>
          <target state="translated">&lt;code&gt;return_complex&lt;/code&gt; を明示的に設定することは、将来のPyTorchリリースで必要になります。現在の動作を保持するにはFalseに設定し、複雑な出力を返すにはTrueに設定します。</target>
        </trans-unit>
        <trans-unit id="50bf9fd4e5b67c599d58e411062b855bf1acfd69" translate="yes" xml:space="preserve">
          <source>Setting the argument &lt;code&gt;num_workers&lt;/code&gt; as a positive integer will turn on multi-process data loading with the specified number of loader worker processes.</source>
          <target state="translated">引数 &lt;code&gt;num_workers&lt;/code&gt; を正の整数に設定すると、指定された数のローダーワーカープロセスでのマルチプロセスデータの読み込みがオンになります。</target>
        </trans-unit>
        <trans-unit id="cee805f166fb37a702f66bc6ef5dafcc4636a19c" translate="yes" xml:space="preserve">
          <source>Setting the environment variable &lt;code&gt;PYTORCH_JIT=0&lt;/code&gt; will disable all script and tracing annotations. If there is hard-to-debug error in one of your TorchScript models, you can use this flag to force everything to run using native Python. Since TorchScript (scripting and tracing) is disabled with this flag, you can use tools like &lt;code&gt;pdb&lt;/code&gt; to debug the model code. For example:</source>
          <target state="translated">環境変数 &lt;code&gt;PYTORCH_JIT=0&lt;/code&gt; を設定すると、すべてのスクリプトとトレースの注釈が無効になります。TorchScriptモデルの1つにデバッグが難しいエラーがある場合は、このフラグを使用して、ネイティブPythonを使用してすべてを強制的に実行できます。TorchScript（スクリプトとトレース）はこのフラグで無効になっているため、 &lt;code&gt;pdb&lt;/code&gt; などのツールを使用してモデルコードをデバッグできます。例えば：</target>
        </trans-unit>
        <trans-unit id="14edb2c4ec235464bf2c2b8b3e2ce45c04e83a80" translate="yes" xml:space="preserve">
          <source>Shape:</source>
          <target state="translated">Shape:</target>
        </trans-unit>
        <trans-unit id="f03987f78c9ae43379f61461ae0064f0d5607ece" translate="yes" xml:space="preserve">
          <source>Shared file system, &lt;code&gt;init_method=&quot;file://////{machine_name}/{share_folder_name}/some_file&quot;&lt;/code&gt;</source>
          <target state="translated">共有ファイルシステム、 &lt;code&gt;init_method=&quot;file://////{machine_name}/{share_folder_name}/some_file&quot;&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="931e6a72bc22b9c481f30ac82befe23983a1c7aa" translate="yes" xml:space="preserve">
          <source>Shared file-system initialization</source>
          <target state="translated">共有ファイルシステムの初期化</target>
        </trans-unit>
        <trans-unit id="e27a0384e6eefcbae31d9ee40a6788234f717210" translate="yes" xml:space="preserve">
          <source>Sharing CUDA tensors</source>
          <target state="translated">CUDAテンソルの共有</target>
        </trans-unit>
        <trans-unit id="f1970dc9e6498a19bd933b2843bc57e2084c8cc4" translate="yes" xml:space="preserve">
          <source>Sharing CUDA tensors between processes is supported only in Python 3, using a &lt;code&gt;spawn&lt;/code&gt; or &lt;code&gt;forkserver&lt;/code&gt; start methods.</source>
          <target state="translated">プロセス間でのCUDAテンソルの共有は、 &lt;code&gt;spawn&lt;/code&gt; または &lt;code&gt;forkserver&lt;/code&gt; startメソッドを使用するPython3でのみサポートされます。</target>
        </trans-unit>
        <trans-unit id="8ca95d11aaeb17762db51cad4433dcf3e2614300" translate="yes" xml:space="preserve">
          <source>Sharing strategies</source>
          <target state="translated">戦略の共有</target>
        </trans-unit>
        <trans-unit id="fe846e8cff1382c632019e0d7be670d40fca4949" translate="yes" xml:space="preserve">
          <source>Short-time Fourier transform (STFT).</source>
          <target state="translated">短時間フーリエ変換(STFT)。</target>
        </trans-unit>
        <trans-unit id="ba0ad92a63c363484c20c75c1280fe0b91b17b2c" translate="yes" xml:space="preserve">
          <source>Should be overridden by all subclasses.</source>
          <target state="translated">すべてのサブクラスでオーバーライドされるべきです。</target>
        </trans-unit>
        <trans-unit id="a37ae8778cd1020f5c30499fa7ea9a6181211f61" translate="yes" xml:space="preserve">
          <source>Show the docstring of entrypoint &lt;code&gt;model&lt;/code&gt;.</source>
          <target state="translated">Show the docstring of entrypoint &lt;code&gt;model&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7504f2364dcd93257f97f33b05a38d51614bd7fa" translate="yes" xml:space="preserve">
          <source>ShuffleNet V2</source>
          <target state="translated">シャッフルネットV2</target>
        </trans-unit>
        <trans-unit id="2f25ac460d78b88009ddd1aa94f79739535ca8b5" translate="yes" xml:space="preserve">
          <source>ShuffleNet v2</source>
          <target state="translated">シャッフルネット v2</target>
        </trans-unit>
        <trans-unit id="f2cac7f225122d558eb3c0591bdc2ae980f1b0b1" translate="yes" xml:space="preserve">
          <source>SiLU</source>
          <target state="translated">SiLU</target>
        </trans-unit>
        <trans-unit id="40cbe2acb9d6a109dfc4fb28692090f839183dea" translate="yes" xml:space="preserve">
          <source>Sigmoid</source>
          <target state="translated">Sigmoid</target>
        </trans-unit>
        <trans-unit id="5b7359698c3ea62e0fe6f50fc802ad6bb7e51d4a" translate="yes" xml:space="preserve">
          <source>Similar to &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt;, with FakeQuantize modules initialized to default.</source>
          <target state="translated">Similar to &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; &lt;/a&gt;, with FakeQuantize modules initialized to default.</target>
        </trans-unit>
        <trans-unit id="7d16e4a78bd1e379e62b250638417164ecf26a8e" translate="yes" xml:space="preserve">
          <source>Similar to &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt;&lt;code&gt;Linear&lt;/code&gt;&lt;/a&gt;, attributes will be randomly initialized at module creation time and will be overwritten later</source>
          <target state="translated">Similar to &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt; &lt;code&gt;Linear&lt;/code&gt; &lt;/a&gt;, attributes will be randomly initialized at module creation time and will be overwritten later</target>
        </trans-unit>
        <trans-unit id="cb2775c2f284c5797e72dc4b9b56c7b83a742605" translate="yes" xml:space="preserve">
          <source>Similar to &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt;&lt;code&gt;torch.nn.Linear&lt;/code&gt;&lt;/a&gt;, attributes will be randomly initialized at module creation time and will be overwritten later</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt; &lt;code&gt;torch.nn.Linear&lt;/code&gt; &lt;/a&gt;と同様に、属性はモジュールの作成時にランダムに初期化され、後で上書きされます</target>
        </trans-unit>
        <trans-unit id="7c8d9b9716eb5e6f4a29b36c1d72924a394fd995" translate="yes" xml:space="preserve">
          <source>Similar to &lt;code&gt;torch.nn.Conv2d&lt;/code&gt;, with FakeQuantize modules initialized to default.</source>
          <target state="translated">Similar to &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; , with FakeQuantize modules initialized to default.</target>
        </trans-unit>
        <trans-unit id="d7172942fcac61201e57b0f8f8151549517ac9d8" translate="yes" xml:space="preserve">
          <source>Similar to &lt;code&gt;torch.nn.Linear&lt;/code&gt;, with FakeQuantize modules initialized to default.</source>
          <target state="translated">Similar to &lt;code&gt;torch.nn.Linear&lt;/code&gt; , with FakeQuantize modules initialized to default.</target>
        </trans-unit>
        <trans-unit id="eb262cc11e95ba6ae1040b6fdf77b3aabdee1e50" translate="yes" xml:space="preserve">
          <source>Similar to &lt;code&gt;torch.nn.intrinsic.LinearReLU&lt;/code&gt;, with FakeQuantize modules initialized to default.</source>
          <target state="translated">Similar to &lt;code&gt;torch.nn.intrinsic.LinearReLU&lt;/code&gt; , with FakeQuantize modules initialized to default.</target>
        </trans-unit>
        <trans-unit id="088b19f4865d3ae00ef9ed3545c87c4c39a038f5" translate="yes" xml:space="preserve">
          <source>Similar to the function above, but the means and standard deviations are shared among all drawn elements. The resulting tensor has size given by &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">Similar to the function above, but the means and standard deviations are shared among all drawn elements. The resulting tensor has size given by &lt;code&gt;size&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="144ca09471d4e86a370dcd5e499624c907232699" translate="yes" xml:space="preserve">
          <source>Similar to the function above, but the means are shared among all drawn elements.</source>
          <target state="translated">上記の関数と似ていますが、描画されたすべての要素の間で手段が共有されます。</target>
        </trans-unit>
        <trans-unit id="d7b2ad151fdc2daf79483df18949d1a193fac72e" translate="yes" xml:space="preserve">
          <source>Similar to the function above, but the standard-deviations are shared among all drawn elements.</source>
          <target state="translated">上記の関数と似ていますが、標準偏差はすべての描画要素で共有されます。</target>
        </trans-unit>
        <trans-unit id="35e4478885a3d3de0a23fd41d27cf9e8b3fd2526" translate="yes" xml:space="preserve">
          <source>Similarly, a variable is not allowed to be used if it is only &lt;em&gt;defined&lt;/em&gt; along some paths through the function.</source>
          <target state="translated">Similarly, a variable is not allowed to be used if it is only &lt;em&gt;defined&lt;/em&gt; along some paths through the function.</target>
        </trans-unit>
        <trans-unit id="0ffdace347411406995002bee149d93b4c8d5097" translate="yes" xml:space="preserve">
          <source>Similarly, if you directly pass in a &lt;code&gt;store&lt;/code&gt; argument, it must be a &lt;code&gt;FileStore&lt;/code&gt; instance.</source>
          <target state="translated">Similarly, if you directly pass in a &lt;code&gt;store&lt;/code&gt; argument, it must be a &lt;code&gt;FileStore&lt;/code&gt; instance.</target>
        </trans-unit>
        <trans-unit id="9be5c47bd8d9e298dbec515a79abc57fb63bd041" translate="yes" xml:space="preserve">
          <source>Similarly, the directions can be separated in the packed case.</source>
          <target state="translated">同様に、パックされたケースの中で方向性を分離することができます。</target>
        </trans-unit>
        <trans-unit id="02a822df6b8073596762077fbb97fadeba9b1cf8" translate="yes" xml:space="preserve">
          <source>Simple Assignments</source>
          <target state="translated">簡単な課題</target>
        </trans-unit>
        <trans-unit id="b5faf41d66cbc35e2d464a122a6e9520effb835d" translate="yes" xml:space="preserve">
          <source>Simple end to end example</source>
          <target state="translated">シンプルな終わり方の例</target>
        </trans-unit>
        <trans-unit id="605569f81b2cecc08a1e8d975481f24ac3831063" translate="yes" xml:space="preserve">
          <source>Simple example, using &lt;a href=&quot;#torch.cuda.amp.GradScaler.unscale_&quot;&gt;&lt;code&gt;unscale_()&lt;/code&gt;&lt;/a&gt; to enable clipping of unscaled gradients:</source>
          <target state="translated">&lt;a href=&quot;#torch.cuda.amp.GradScaler.unscale_&quot;&gt; &lt;code&gt;unscale_()&lt;/code&gt; &lt;/a&gt;を使用して、スケーリングされていないグラデーションのクリッピングを有効にする簡単な例：</target>
        </trans-unit>
        <trans-unit id="4f6256b1a68c342ed7c517637989544d7f9e875b" translate="yes" xml:space="preserve">
          <source>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</source>
          <target state="translated">剪定されるパラメータと生成されたマスクとの乗算を単純に処理します。モジュールからマスクと元のテンソルを取得し、刈り込み済みのテンソルを返します。</target>
        </trans-unit>
        <trans-unit id="e9d02a1900249db21bb923f01100461e477f4a2a" translate="yes" xml:space="preserve">
          <source>Simulate the quantize and dequantize operations in training time. The output of this module is given by</source>
          <target state="translated">学習時間内に量子化と量子化解除の操作をシミュレートします。このモジュールの出力は</target>
        </trans-unit>
        <trans-unit id="d60674760461d218ee6d7f02184c2ead492c87d0" translate="yes" xml:space="preserve">
          <source>Since &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;stft()&lt;/code&gt;&lt;/a&gt; discards elements at the end of the signal if they do not fit in a frame, &lt;code&gt;istft&lt;/code&gt; may return a shorter signal than the original signal (can occur if &lt;code&gt;center&lt;/code&gt; is False since the signal isn&amp;rsquo;t padded).</source>
          <target state="translated">Since &lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;stft()&lt;/code&gt; &lt;/a&gt; discards elements at the end of the signal if they do not fit in a frame, &lt;code&gt;istft&lt;/code&gt; may return a shorter signal than the original signal (can occur if &lt;code&gt;center&lt;/code&gt; is False since the signal isn&amp;rsquo;t padded).</target>
        </trans-unit>
        <trans-unit id="b57ff46579e1243d4e5f8600a0a9241de5ac5737" translate="yes" xml:space="preserve">
          <source>Since SparseTensor._indices() is always a 2D tensor, the smallest sparse_dim = 1. Therefore, representation of a SparseTensor of sparse_dim = 0 is simply a dense tensor.</source>
          <target state="translated">SparseTensor._indices()は常に2次元テンソルなので、最小のsparse_dim=1となります。したがって、sparse_dim=0のスパーステンソルの表現は、単に密なテンソルです。</target>
        </trans-unit>
        <trans-unit id="1e55bc14205c3b61b63018bcb70eae3ef124b8ad" translate="yes" xml:space="preserve">
          <source>Since eigenvalues and eigenvectors might be complex, backward pass is supported only for &lt;a href=&quot;torch.symeig#torch.symeig&quot;&gt;&lt;code&gt;torch.symeig()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Since eigenvalues and eigenvectors might be complex, backward pass is supported only for &lt;a href=&quot;torch.symeig#torch.symeig&quot;&gt; &lt;code&gt;torch.symeig()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7cc899b4bce705836850ca654589c887f5fd6278" translate="yes" xml:space="preserve">
          <source>Since global structured pruning doesn&amp;rsquo;t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods.</source>
          <target state="translated">Since global structured pruning doesn&amp;rsquo;t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods.</target>
        </trans-unit>
        <trans-unit id="67bd75f4f8766a48c7b96bda1b055c053f15aefe" translate="yes" xml:space="preserve">
          <source>Since the input matrix &lt;code&gt;input&lt;/code&gt; is supposed to be symmetric, only the upper triangular portion is used by default.</source>
          <target state="translated">Since the input matrix &lt;code&gt;input&lt;/code&gt; is supposed to be symmetric, only the upper triangular portion is used by default.</target>
        </trans-unit>
        <trans-unit id="9a190d7c173d1800801bb5236c42d4c18532c731" translate="yes" xml:space="preserve">
          <source>Since workers rely on Python &lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt;&lt;code&gt;multiprocessing&lt;/code&gt;&lt;/a&gt;, worker launch behavior is different on Windows compared to Unix.</source>
          <target state="translated">ワーカーはPython&lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt; &lt;code&gt;multiprocessing&lt;/code&gt; &lt;/a&gt;に依存しているため、ワーカーの起動動作はWindowsとUnixでは異なります。</target>
        </trans-unit>
        <trans-unit id="da8a1ea02e7c7de523fde6a9a1ed49139987cfb8" translate="yes" xml:space="preserve">
          <source>Single- and Multi-process Data Loading</source>
          <target state="translated">シングルプロセスおよびマルチプロセスのデータローディング</target>
        </trans-unit>
        <trans-unit id="21a1b9cf02c75eb1f1e22759af089802524d644f" translate="yes" xml:space="preserve">
          <source>Single-Node multi-process distributed training</source>
          <target state="translated">シングルノード・マルチプロセス分散トレーニング</target>
        </trans-unit>
        <trans-unit id="1d2ab9ec1f053a991bd56f9ddd052665f52de541" translate="yes" xml:space="preserve">
          <source>Single-process data loading (default)</source>
          <target state="translated">シングルプロセスデータローディング(デフォルト</target>
        </trans-unit>
        <trans-unit id="f8df21c36569ccf0ecf10173294bd3bcb1ac958d" translate="yes" xml:space="preserve">
          <source>Slices the &lt;code&gt;self&lt;/code&gt; tensor along the selected dimension at the given index. This function returns a view of the original tensor with the given dimension removed.</source>
          <target state="translated">Slices the &lt;code&gt;self&lt;/code&gt; tensor along the selected dimension at the given index. This function returns a view of the original tensor with the given dimension removed.</target>
        </trans-unit>
        <trans-unit id="1388fcaebbfe7bba9af50141f343d91cd5010970" translate="yes" xml:space="preserve">
          <source>SmoothL1Loss</source>
          <target state="translated">SmoothL1Loss</target>
        </trans-unit>
        <trans-unit id="cf592edcfbdc177c81b474cbd6b83f6e29d0f7cc" translate="yes" xml:space="preserve">
          <source>So, it is recommended to always pass the signal length &lt;code&gt;n&lt;/code&gt;:</source>
          <target state="translated">So, it is recommended to always pass the signal length &lt;code&gt;n&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="ec836ce94eae96d1e5118d150d4218a7f447e2e4" translate="yes" xml:space="preserve">
          <source>So, it is recommended to always pass the signal shape &lt;code&gt;s&lt;/code&gt;.</source>
          <target state="translated">So, it is recommended to always pass the signal shape &lt;code&gt;s&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5fdb4ccd6a62db91e946fc4d593f43a0eb0c2b97" translate="yes" xml:space="preserve">
          <source>SobolEngine</source>
          <target state="translated">SobolEngine</target>
        </trans-unit>
        <trans-unit id="5780e85860e3ef851155673787aae34395fdcb14" translate="yes" xml:space="preserve">
          <source>SoftMarginLoss</source>
          <target state="translated">SoftMarginLoss</target>
        </trans-unit>
        <trans-unit id="bace6f3ab809aca48a2c72d1f9a972334f596086" translate="yes" xml:space="preserve">
          <source>SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.</source>
          <target state="translated">SoftPlusはReLU関数の平滑な近似であり、機械の出力が常に正であるように制約するために使用することができます。</target>
        </trans-unit>
        <trans-unit id="4db38f97f455fc816850ffbe57a25de4b9b124c8" translate="yes" xml:space="preserve">
          <source>SoftShrinkage</source>
          <target state="translated">SoftShrinkage</target>
        </trans-unit>
        <trans-unit id="a2852636b9c2b8ea655bec2aa6ef18c0c81bce34" translate="yes" xml:space="preserve">
          <source>SoftSign</source>
          <target state="translated">SoftSign</target>
        </trans-unit>
        <trans-unit id="06a2c3db74f0f43566dbc7efd5bedd81dfa46888" translate="yes" xml:space="preserve">
          <source>Softmax</source>
          <target state="translated">Softmax</target>
        </trans-unit>
        <trans-unit id="38b81da3b6ab3b9be6104d3e4c422885eca96901" translate="yes" xml:space="preserve">
          <source>Softmax is defined as:</source>
          <target state="translated">Softmaxは次のように定義されています。</target>
        </trans-unit>
        <trans-unit id="0444b47cdcbeed809978f61fc222d1069a142d9c" translate="yes" xml:space="preserve">
          <source>Softmax2d</source>
          <target state="translated">Softmax2d</target>
        </trans-unit>
        <trans-unit id="29b7eb80efd3b1d65f2835df414e4222cab21ee1" translate="yes" xml:space="preserve">
          <source>Softmin</source>
          <target state="translated">Softmin</target>
        </trans-unit>
        <trans-unit id="af86abb7b472b1e8a9af01b4180064b037b78bf8" translate="yes" xml:space="preserve">
          <source>Softmin is defined as:</source>
          <target state="translated">ソフトミンの定義は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="90ba42633b716cf810a9a32d70553b14c097c177" translate="yes" xml:space="preserve">
          <source>Softplus</source>
          <target state="translated">Softplus</target>
        </trans-unit>
        <trans-unit id="7bb40d05bf436ff810963d7ee11a5bc95644a2f9" translate="yes" xml:space="preserve">
          <source>Softshrink</source>
          <target state="translated">Softshrink</target>
        </trans-unit>
        <trans-unit id="2b10804bb8f17cdf24b14774ca1d70aa3ba213d6" translate="yes" xml:space="preserve">
          <source>Softsign</source>
          <target state="translated">Softsign</target>
        </trans-unit>
        <trans-unit id="73a65eb2cc67f9395fdddf23ed853dd71a26bf17" translate="yes" xml:space="preserve">
          <source>Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix</source>
          <target state="translated">正の半定値行列を持つ一次方程式系を解き、そのコレスキー因子行列が与えられたときに反転させます。</target>
        </trans-unit>
        <trans-unit id="ab02cfb3b2fcd0c1b4621d3c7cc39a469375c3ec" translate="yes" xml:space="preserve">
          <source>Solves a system of equations with a triangular coefficient matrix</source>
          <target state="translated">三角係数行列を持つ方程式系を解く</target>
        </trans-unit>
        <trans-unit id="5773ea0d4a0b0567ea2d9a3b38579ffd7e09e6ff" translate="yes" xml:space="preserve">
          <source>Some functions (for example, &lt;a href=&quot;https://docs.python.org/3/library/functions.html#zip&quot;&gt;&lt;code&gt;zip&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://docs.python.org/3/library/functions.html#enumerate&quot;&gt;&lt;code&gt;enumerate&lt;/code&gt;&lt;/a&gt;) can only operate on iterable types. Iterable types in TorchScript include &lt;code&gt;Tensor&lt;/code&gt;s, lists, tuples, dictionaries, strings, &lt;a href=&quot;generated/torch.nn.modulelist#torch.nn.ModuleList&quot;&gt;&lt;code&gt;torch.nn.ModuleList&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.moduledict#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;torch.nn.ModuleDict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Some functions (for example, &lt;a href=&quot;https://docs.python.org/3/library/functions.html#zip&quot;&gt; &lt;code&gt;zip&lt;/code&gt; &lt;/a&gt; and &lt;a href=&quot;https://docs.python.org/3/library/functions.html#enumerate&quot;&gt; &lt;code&gt;enumerate&lt;/code&gt; &lt;/a&gt;) can only operate on iterable types. Iterable types in TorchScript include &lt;code&gt;Tensor&lt;/code&gt; s, lists, tuples, dictionaries, strings, &lt;a href=&quot;generated/torch.nn.modulelist#torch.nn.ModuleList&quot;&gt; &lt;code&gt;torch.nn.ModuleList&lt;/code&gt; &lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.moduledict#torch.nn.ModuleDict&quot;&gt; &lt;code&gt;torch.nn.ModuleDict&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="cbbdc83ebf4a1410688a2167e77428899715f68d" translate="yes" xml:space="preserve">
          <source>Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.</source>
          <target state="translated">いくつかの入力周波数は、ハーミッ ト特性を満たすために実数値でなければなりません。このような場合、虚数成分は無視されます。例えば、ゼロ周波数項の虚数成分は、実数出力では表現できないため、常に無視されます。</target>
        </trans-unit>
        <trans-unit id="f5a5860f652baaa71e759f95e6546fb5a30df1fa" translate="yes" xml:space="preserve">
          <source>Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, use &lt;code&gt;model.train()&lt;/code&gt; or &lt;code&gt;model.eval()&lt;/code&gt; as appropriate. See &lt;a href=&quot;../generated/torch.nn.module#torch.nn.Module.train&quot;&gt;&lt;code&gt;train()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../generated/torch.nn.module#torch.nn.Module.eval&quot;&gt;&lt;code&gt;eval()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, use &lt;code&gt;model.train()&lt;/code&gt; or &lt;code&gt;model.eval()&lt;/code&gt; as appropriate. See &lt;a href=&quot;../generated/torch.nn.module#torch.nn.Module.train&quot;&gt; &lt;code&gt;train()&lt;/code&gt; &lt;/a&gt; or &lt;a href=&quot;../generated/torch.nn.module#torch.nn.Module.eval&quot;&gt; &lt;code&gt;eval()&lt;/code&gt; &lt;/a&gt; for details.</target>
        </trans-unit>
        <trans-unit id="7b9a08f765d29fc2a0904578fe32193591816e48" translate="yes" xml:space="preserve">
          <source>Some ops not listed here (e.g., binary ops like &lt;code&gt;add&lt;/code&gt;) natively promote inputs without autocasting&amp;rsquo;s intervention. If inputs are a mixture of &lt;code&gt;float16&lt;/code&gt; and &lt;code&gt;float32&lt;/code&gt;, these ops run in &lt;code&gt;float32&lt;/code&gt; and produce &lt;code&gt;float32&lt;/code&gt; output, regardless of whether autocast is enabled.</source>
          <target state="translated">ここにリストされていない一部の操作（たとえば、 &lt;code&gt;add&lt;/code&gt; などのバイナリ操作）は、自動キャストの介入なしに入力をネイティブにプロモートします。入力が混合している場合 &lt;code&gt;float16&lt;/code&gt; と &lt;code&gt;float32&lt;/code&gt; 、これらのOPSはで実行 &lt;code&gt;float32&lt;/code&gt; と農産物 &lt;code&gt;float32&lt;/code&gt; にかかわらず、自動キャストが有効かどうかの、出力。</target>
        </trans-unit>
        <trans-unit id="72e33a4ea3969c9936cfbf0b72aacf8cc360fedb" translate="yes" xml:space="preserve">
          <source>Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.</source>
          <target state="translated">共役勾配やLBFGSのような最適化アルゴリズムの中には、関数を何度も再評価する必要があるものがあります。クロージャは勾配をクリアし、損失を計算し、それを返す必要があります。</target>
        </trans-unit>
        <trans-unit id="ae4df2df1ef3b4db7bd9154e0c9f55f5890f0c7a" translate="yes" xml:space="preserve">
          <source>Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as &lt;code&gt;float32&lt;/code&gt;</source>
          <target state="translated">Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as &lt;code&gt;float32&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="78039ee874a9a5bb0eb3d37a1f02f096ee61bda5" translate="yes" xml:space="preserve">
          <source>Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as &lt;code&gt;float32&lt;/code&gt;</source>
          <target state="translated">Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as &lt;code&gt;float32&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ba80efc106bf3fdda88df0da58461971d3468eca" translate="yes" xml:space="preserve">
          <source>Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important at the expense of range.</source>
          <target state="translated">binary16と呼ばれることもあります:1 符号、5 指数、10 符号ビットを使用します。範囲を犠牲にして精度が重要な場合に便利です。</target>
        </trans-unit>
        <trans-unit id="4633684bcdc43c53e1cfb801845c2325140c26e1" translate="yes" xml:space="preserve">
          <source>Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important.</source>
          <target state="translated">binary16と呼ばれることもあります:1 符号、5 指数、10 符号ビットを使用します。精度が重要な場合に便利です。</target>
        </trans-unit>
        <trans-unit id="3900f0b870b5719788e2d6646aafed691f38941e" translate="yes" xml:space="preserve">
          <source>Sorts the elements of the &lt;code&gt;input&lt;/code&gt; tensor along a given dimension in ascending order by value.</source>
          <target state="translated">Sorts the elements of the &lt;code&gt;input&lt;/code&gt; tensor along a given dimension in ascending order by value.</target>
        </trans-unit>
        <trans-unit id="6a085f98a6ae4bb57c97870bfd419ca83f17e143" translate="yes" xml:space="preserve">
          <source>Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to &lt;code&gt;cpp_sources&lt;/code&gt; are first concatenated into a single &lt;code&gt;.cpp&lt;/code&gt; file. This file is then prepended with &lt;code&gt;#include
&amp;lt;torch/extension.h&amp;gt;&lt;/code&gt;.</source>
          <target state="translated">Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to &lt;code&gt;cpp_sources&lt;/code&gt; are first concatenated into a single &lt;code&gt;.cpp&lt;/code&gt; file. This file is then prepended with &lt;code&gt;#include &amp;lt;torch/extension.h&amp;gt;&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="38317d08ad25b33e290bef6bc76c581c87c8620f" translate="yes" xml:space="preserve">
          <source>Sparse Layers</source>
          <target state="translated">疎なレイヤー</target>
        </trans-unit>
        <trans-unit id="0e3084d96416ea178eaf45c018b3d6a666f048ab" translate="yes" xml:space="preserve">
          <source>Sparse functions</source>
          <target state="translated">疎な関数</target>
        </trans-unit>
        <trans-unit id="ca9492470122675a4d013eba5c404de6d984fba7" translate="yes" xml:space="preserve">
          <source>SparseTensor has the following invariants:</source>
          <target state="translated">SparseTensorは次のような不変量を持っています。</target>
        </trans-unit>
        <trans-unit id="ad7f19daae9a456b6857c9e6506b86f3b2ab9c8d" translate="yes" xml:space="preserve">
          <source>SparseTensor._indices().shape = (sparse_dim, nnz)</source>
          <target state="translated">SparseTensor._indices().shape=(sparse_dim,nnz)</target>
        </trans-unit>
        <trans-unit id="018f02659b563c79f98ce5dc06e6c8ead57c92f3" translate="yes" xml:space="preserve">
          <source>SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])</source>
          <target state="translated">SparseTensor._values().shape=(nnz,SparseTensor.shape[sparse_dim:])</target>
        </trans-unit>
        <trans-unit id="4ca5e4706f1046fd29382939145a442a4ce0eeb1" translate="yes" xml:space="preserve">
          <source>Spawn utility</source>
          <target state="translated">スポーンユーティリティ</target>
        </trans-unit>
        <trans-unit id="762ee18552488ab27f7d553d5bb224a9478ed209" translate="yes" xml:space="preserve">
          <source>Spawning a number of subprocesses to perform some function can be done by creating &lt;code&gt;Process&lt;/code&gt; instances and calling &lt;code&gt;join&lt;/code&gt; to wait for their completion. This approach works fine when dealing with a single subprocess but presents potential issues when dealing with multiple processes.</source>
          <target state="translated">いくつかの機能を実行するためにいくつかのサブプロセスを生成するには、 &lt;code&gt;Process&lt;/code&gt; インスタンスを作成し、 &lt;code&gt;join&lt;/code&gt; を呼び出してそれらの完了を待機します。このアプローチは、単一のサブプロセスを処理する場合は正常に機能しますが、複数のプロセスを処理する場合は潜在的な問題が発生します。</target>
        </trans-unit>
        <trans-unit id="62f14e7f69f0c928f3521a52d4471e369748ed2e" translate="yes" xml:space="preserve">
          <source>Spawning subprocesses</source>
          <target state="translated">サブプロセスの産卵</target>
        </trans-unit>
        <trans-unit id="c66cb2e22db24d665900e10960b23a14a873c2d2" translate="yes" xml:space="preserve">
          <source>Spawns &lt;code&gt;nprocs&lt;/code&gt; processes that run &lt;code&gt;fn&lt;/code&gt; with &lt;code&gt;args&lt;/code&gt;.</source>
          <target state="translated">スポーン &lt;code&gt;nprocs&lt;/code&gt; 実行されるプロセス &lt;code&gt;fn&lt;/code&gt; と &lt;code&gt;args&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a2019fa476b609b73dfcc70b82b7767a098a7dbd" translate="yes" xml:space="preserve">
          <source>Specifically, in the forward pass, &lt;code&gt;function&lt;/code&gt; will run in &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/a&gt; manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the &lt;code&gt;function&lt;/code&gt; parameter. In the backwards pass, the saved inputs and &lt;code&gt;function&lt;/code&gt; is retrieved, and the forward pass is computed on &lt;code&gt;function&lt;/code&gt; again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.</source>
          <target state="translated">Specifically, in the forward pass, &lt;code&gt;function&lt;/code&gt; will run in &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt; &lt;code&gt;torch.no_grad()&lt;/code&gt; &lt;/a&gt; manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the &lt;code&gt;function&lt;/code&gt; parameter. In the backwards pass, the saved inputs and &lt;code&gt;function&lt;/code&gt; is retrieved, and the forward pass is computed on &lt;code&gt;function&lt;/code&gt; again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.</target>
        </trans-unit>
        <trans-unit id="9fb7ff759e9a0b4f8cf21fa63ff4032c4ca275d1" translate="yes" xml:space="preserve">
          <source>Specify &lt;code&gt;init_method&lt;/code&gt; (a URL string) which indicates where/how to discover peers. Optionally specify &lt;code&gt;rank&lt;/code&gt; and &lt;code&gt;world_size&lt;/code&gt;, or encode all required parameters in the URL and omit them.</source>
          <target state="translated">Specify &lt;code&gt;init_method&lt;/code&gt; (a URL string) which indicates where/how to discover peers. Optionally specify &lt;code&gt;rank&lt;/code&gt; and &lt;code&gt;world_size&lt;/code&gt; , or encode all required parameters in the URL and omit them.</target>
        </trans-unit>
        <trans-unit id="aaa6b0a968f0861818a7c60f3b1e30636bcfe285" translate="yes" xml:space="preserve">
          <source>Specify &lt;code&gt;store&lt;/code&gt;, &lt;code&gt;rank&lt;/code&gt;, and &lt;code&gt;world_size&lt;/code&gt; explicitly.</source>
          <target state="translated">Specify &lt;code&gt;store&lt;/code&gt; , &lt;code&gt;rank&lt;/code&gt; , and &lt;code&gt;world_size&lt;/code&gt; explicitly.</target>
        </trans-unit>
        <trans-unit id="729466c776095dc7f1dabe6031157b446a5540f2" translate="yes" xml:space="preserve">
          <source>Spectral Ops</source>
          <target state="translated">スペクトルオペレーション</target>
        </trans-unit>
        <trans-unit id="2edd2d0e89ea6a8fb45d2a4b97947c90f7836312" translate="yes" xml:space="preserve">
          <source>Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm</source>
          <target state="translated">スペクトル正規化は、重みテンソルをスペクトルノルムで再スケーリングすることにより、生成的逆境ネットワーク(GAN)における識別器(批評家)の訓練を安定化する。</target>
        </trans-unit>
        <trans-unit id="7da9517b7a5b4892d14a9dd4e7bfc3bd2215ddfe" translate="yes" xml:space="preserve">
          <source>Splits a tensor into a specific number of chunks.</source>
          <target state="translated">テンソルを特定の数のチャンクに分割します。</target>
        </trans-unit>
        <trans-unit id="15060a11c482f2d0d29c0ea280b19cfd35770c5a" translate="yes" xml:space="preserve">
          <source>Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor.</source>
          <target state="translated">テンソルを特定の数のチャンクに分割します。各チャンクは入力テンソルのビューです。</target>
        </trans-unit>
        <trans-unit id="9e2be38fc2259b1dc48a946fa30b9de28430d8f6" translate="yes" xml:space="preserve">
          <source>Splits the tensor into chunks.</source>
          <target state="translated">テンソルをチャンクに分割します。</target>
        </trans-unit>
        <trans-unit id="3a4c888de6c076a07c14ac6f6526916b6b64d375" translate="yes" xml:space="preserve">
          <source>Splits the tensor into chunks. Each chunk is a view of the original tensor.</source>
          <target state="translated">テンソルをチャンクに分割します。各チャンクは元のテンソルのビューです。</target>
        </trans-unit>
        <trans-unit id="6bb9feffd05f6ed4721f33c4d4bcd3050fc0acce" translate="yes" xml:space="preserve">
          <source>SqueezeNet</source>
          <target state="translated">SqueezeNet</target>
        </trans-unit>
        <trans-unit id="8e3b43524d2794940e994fbdb21d126a6c23a892" translate="yes" xml:space="preserve">
          <source>SqueezeNet 1.0</source>
          <target state="translated">SqueezeNet 1.0</target>
        </trans-unit>
        <trans-unit id="83d7283f405d03fc00d84f279e5aa4a9ad100122" translate="yes" xml:space="preserve">
          <source>SqueezeNet 1.1</source>
          <target state="translated">SqueezeNet 1.1</target>
        </trans-unit>
        <trans-unit id="e8ffcf5b4a1dac24c72399125fdd5187787b0df5" translate="yes" xml:space="preserve">
          <source>SqueezeNet 1.1 model from the &lt;a href=&quot;https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1&quot;&gt;official SqueezeNet repo&lt;/a&gt;. SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters than SqueezeNet 1.0, without sacrificing accuracy.</source>
          <target state="translated">SqueezeNet 1.1 model from the &lt;a href=&quot;https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1&quot;&gt;official SqueezeNet repo&lt;/a&gt;. SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters than SqueezeNet 1.0, without sacrificing accuracy.</target>
        </trans-unit>
        <trans-unit id="5fe53423313b926c6b9706c2a81b42a519942948" translate="yes" xml:space="preserve">
          <source>SqueezeNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;&amp;ldquo;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &amp;lt;0.5MB model size&amp;rdquo;&lt;/a&gt; paper.</source>
          <target state="translated">SqueezeNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;&amp;ldquo;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &amp;lt;0.5MB model size&amp;rdquo;&lt;/a&gt; paper.</target>
        </trans-unit>
        <trans-unit id="d061769f4cfb05ab146ba458ef5636f865a72a82" translate="yes" xml:space="preserve">
          <source>Stack tensors in sequence depthwise (along third axis).</source>
          <target state="translated">テンソルを深さ方向(第3軸に沿って)に順に積み上げていきます。</target>
        </trans-unit>
        <trans-unit id="46e9e1e8c6989a91e6a1a0548e571225be43cbbf" translate="yes" xml:space="preserve">
          <source>Stack tensors in sequence horizontally (column wise).</source>
          <target state="translated">テンソルを水平方向(列方向)に順番に積み上げていきます。</target>
        </trans-unit>
        <trans-unit id="cd5277dc6eddb37c391be9a3629b9809479f61ec" translate="yes" xml:space="preserve">
          <source>Stack tensors in sequence vertically (row wise).</source>
          <target state="translated">テンソルを縦に並べる(行単位)。</target>
        </trans-unit>
        <trans-unit id="7911bbae7aa66ed740b8a6ebf74df4fbffeb0338" translate="yes" xml:space="preserve">
          <source>State collector class for float operations.</source>
          <target state="translated">浮動小数点演算のためのステートコレクタクラス。</target>
        </trans-unit>
        <trans-unit id="5653cebc057d4791ce07031ad9286e729de6d691" translate="yes" xml:space="preserve">
          <source>Statements</source>
          <target state="translated">Statements</target>
        </trans-unit>
        <trans-unit id="19abb71cdc8040d0d69069e805acbb331fb10491" translate="yes" xml:space="preserve">
          <source>Step between two slices is given by &lt;code&gt;step&lt;/code&gt;.</source>
          <target state="translated">Step between two slices is given by &lt;code&gt;step&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="14f4390fa64c3d61f311b3d728a02a53224437ed" translate="yes" xml:space="preserve">
          <source>Step could be called after every batch update</source>
          <target state="translated">バッチ更新のたびにステップを呼び出すことができます。</target>
        </trans-unit>
        <trans-unit id="800f66cc7b49914c33ebebf4faf976e88aaac6fe" translate="yes" xml:space="preserve">
          <source>Stochastic Weight Averaging</source>
          <target state="translated">確率的重み平均法</target>
        </trans-unit>
        <trans-unit id="a93d86688c50b7db5df06afe98e12c7eb843b764" translate="yes" xml:space="preserve">
          <source>Stores names for each of this tensor&amp;rsquo;s dimensions.</source>
          <target state="translated">Stores names for each of this tensor&amp;rsquo;s dimensions.</target>
        </trans-unit>
        <trans-unit id="9a96f51db31dc0b324375deea02a8cc1a060aba6" translate="yes" xml:space="preserve">
          <source>Strategy management</source>
          <target state="translated">戦略マネジメント</target>
        </trans-unit>
        <trans-unit id="b0f4aaebf484b2f55d024a76952eb003a0d189f2" translate="yes" xml:space="preserve">
          <source>Streams and events</source>
          <target state="translated">ストリームとイベント</target>
        </trans-unit>
        <trans-unit id="c3036c584463ccfc0fa687352917db10956640ab" translate="yes" xml:space="preserve">
          <source>Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.</source>
          <target state="translated">ストリームはデバイスごとのものです。選択されたストリームが現在のデバイスにない場合、この関数はまた、ストリームに合わせて現在のデバイスを変更します。</target>
        </trans-unit>
        <trans-unit id="8c7ec55740528fa4d2910cfad7b5e1cae6e7da08" translate="yes" xml:space="preserve">
          <source>Stride is the jump necessary to go from one element to the next one in the specified dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt;. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Stride is the jump necessary to go from one element to the next one in the specified dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim&lt;/code&gt; &lt;/a&gt;. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="aaae0d5e6cf55aab12950842f08c8cf9355bcef4" translate="yes" xml:space="preserve">
          <source>StudentT</source>
          <target state="translated">StudentT</target>
        </trans-unit>
        <trans-unit id="8234b01c9735ef16f8e122a86ac6b13bee795314" translate="yes" xml:space="preserve">
          <source>Submodules assigned in this way will be registered, and will have their parameters converted too when you call &lt;a href=&quot;#torch.nn.Module.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;, etc.</source>
          <target state="translated">Submodules assigned in this way will be registered, and will have their parameters converted too when you call &lt;a href=&quot;#torch.nn.Module.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt;, etc.</target>
        </trans-unit>
        <trans-unit id="82a36013ba03c5404268ec4092f5e0e3210ace06" translate="yes" xml:space="preserve">
          <source>Subscripts and Slicing</source>
          <target state="translated">添え字とスライシング</target>
        </trans-unit>
        <trans-unit id="a822d1e4dad94b5aa9ace547ed1affaadebbf09a" translate="yes" xml:space="preserve">
          <source>Subset of a dataset at specified indices.</source>
          <target state="translated">指定されたインデックスのデータセットのサブセット.</target>
        </trans-unit>
        <trans-unit id="ae017f86a36a573c987d535dde17fe71055f3774" translate="yes" xml:space="preserve">
          <source>Subsystems</source>
          <target state="translated">Subsystems</target>
        </trans-unit>
        <trans-unit id="fb5f075e6679b4e2fffcc3ea921e3be776764746" translate="yes" xml:space="preserve">
          <source>Subtracts &lt;code&gt;other&lt;/code&gt;, scaled by &lt;code&gt;alpha&lt;/code&gt;, from &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Subtracts &lt;code&gt;other&lt;/code&gt; , scaled by &lt;code&gt;alpha&lt;/code&gt; , from &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="61228272a4a7f9d4dd0cc2a12ad63a807870bdc6" translate="yes" xml:space="preserve">
          <source>Sum &lt;code&gt;this&lt;/code&gt; tensor to &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; must be broadcastable to &lt;code&gt;this&lt;/code&gt; tensor size.</source>
          <target state="translated">Sum &lt;code&gt;this&lt;/code&gt; tensor to &lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt;. &lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt; must be broadcastable to &lt;code&gt;this&lt;/code&gt; tensor size.</target>
        </trans-unit>
        <trans-unit id="5bbc7a7ed0ef03a5bd96699fe5d02cbbd0777b92" translate="yes" xml:space="preserve">
          <source>Sums tensors from multiple GPUs.</source>
          <target state="translated">複数のGPUからテンソルを合計します。</target>
        </trans-unit>
        <trans-unit id="a2a1c18ad4d1dc4a9dbab6435947b2aca87f5bcb" translate="yes" xml:space="preserve">
          <source>SuperResolution</source>
          <target state="translated">SuperResolution</target>
        </trans-unit>
        <trans-unit id="fe6c7185d6335e53d7798880272887b9e595250b" translate="yes" xml:space="preserve">
          <source>Supported constant Python types are</source>
          <target state="translated">サポートされているPythonの定数型は</target>
        </trans-unit>
        <trans-unit id="6c9970da0829cd2d77d97494b716ecac47fb4272" translate="yes" xml:space="preserve">
          <source>Supported inputs are dense, sparse, and batches of dense matrices.</source>
          <target state="translated">サポートされる入力は,密な行列,疎な行列,密な行列のバッチです.</target>
        </trans-unit>
        <trans-unit id="a72acca9638828e6fcd59dd9d5146b8ffaac77cb" translate="yes" xml:space="preserve">
          <source>Supported operators</source>
          <target state="translated">サポートされている演算子</target>
        </trans-unit>
        <trans-unit id="d2212b71de8fd24dec99bf8599144400e7f7e95d" translate="yes" xml:space="preserve">
          <source>Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd&amp;rsquo;s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you&amp;rsquo;re operating under heavy memory pressure, you might never need to use them.</source>
          <target state="translated">autogradでインプレース操作をサポートすることは難しい問題であり、ほとんどの場合、それらの使用はお勧めしません。Autogradの積極的なバッファの解放と再利用により、非常に効率的になり、インプレース操作によって実際にメモリ使用量が大幅に減少することはほとんどありません。大きなメモリプレッシャーの下で操作しているのでない限り、それらを使用する必要はないかもしれません。</target>
        </trans-unit>
        <trans-unit id="7d88be7203e6e438cd32536d295a5ed587dd4a76" translate="yes" xml:space="preserve">
          <source>Supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcasting to a common shape&lt;/a&gt;, &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;type promotion&lt;/a&gt;, and integer, float, and complex inputs.</source>
          <target state="translated">Supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcasting to a common shape&lt;/a&gt;, &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;type promotion&lt;/a&gt;, and integer, float, and complex inputs.</target>
        </trans-unit>
        <trans-unit id="12a17f409a6267d331bbba640e585b7dceceb728" translate="yes" xml:space="preserve">
          <source>Supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcasting to a common shape&lt;/a&gt;, &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;type promotion&lt;/a&gt;, and integer, float, and complex inputs. Always promotes integer types to the default scalar type.</source>
          <target state="translated">Supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcasting to a common shape&lt;/a&gt;, &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;type promotion&lt;/a&gt;, and integer, float, and complex inputs. Always promotes integer types to the default scalar type.</target>
        </trans-unit>
        <trans-unit id="c515e39b10e558f93fb5480918ab7b6f80dd1a7c" translate="yes" xml:space="preserve">
          <source>Supports broadcasting to a common shape, type promotion, and integer and float inputs.</source>
          <target state="translated">共通形状へのブロードキャスト、型のプロモーション、整数と浮動小数点の入力をサポートしています。</target>
        </trans-unit>
        <trans-unit id="776e5bf297164498f6e5ef24869ccff8d9d007bf" translate="yes" xml:space="preserve">
          <source>Swaps the module if it has a quantized counterpart and it has an &lt;code&gt;observer&lt;/code&gt; attached.</source>
          <target state="translated">モジュールに量子化された対応物があり、 &lt;code&gt;observer&lt;/code&gt; 接続されている場合は、モジュールを交換します。</target>
        </trans-unit>
        <trans-unit id="2b5fa2f5e7f20411e614b5e4239c9394a99b2a3f" translate="yes" xml:space="preserve">
          <source>Symbolic functions should be implemented in Python. All of these functions interact with Python methods which are implemented via C++-Python bindings, but intuitively the interface they provide looks like this:</source>
          <target state="translated">シンボリック関数はPythonで実装されている必要があります。これらの関数はすべてC++-Pythonバインディングを介して実装されたPythonメソッドと相互作用しますが、直感的にはこれらの関数が提供するインターフェースは次のようになります。</target>
        </trans-unit>
        <trans-unit id="dd8b7a9758c34d0bf6248878c673e14fd8037607" translate="yes" xml:space="preserve">
          <source>SyncBatchNorm</source>
          <target state="translated">SyncBatchNorm</target>
        </trans-unit>
        <trans-unit id="1dc553edb93ee2799123f5bdebb8f2d61aa9771e" translate="yes" xml:space="preserve">
          <source>Synchronizes all processes.</source>
          <target state="translated">すべてのプロセスを同期させます。</target>
        </trans-unit>
        <trans-unit id="e32e1c40401ff0c5bb5e89fba4cab573a0ddefc7" translate="yes" xml:space="preserve">
          <source>Synchronizes with another stream.</source>
          <target state="translated">別のストリームと同期します。</target>
        </trans-unit>
        <trans-unit id="68f18accfbfac81fef2a2bc99a682b9a686206f1" translate="yes" xml:space="preserve">
          <source>Synchronous and asynchronous collective operations</source>
          <target state="translated">同期および非同期の集団演算</target>
        </trans-unit>
        <trans-unit id="c2c53d66948214258a26ca9ca845d7ac0c17f8e7" translate="yes" xml:space="preserve">
          <source>T</source>
          <target state="translated">T</target>
        </trans-unit>
        <trans-unit id="a55d4194696253efb0ce8eaeea351140c88a65da" translate="yes" xml:space="preserve">
          <source>T = \text{input length}</source>
          <target state="translated">T=ﾃｷｽﾄ</target>
        </trans-unit>
        <trans-unit id="26aecc925a999c69c46a16510ab79a1145705de8" translate="yes" xml:space="preserve">
          <source>T+X</source>
          <target state="translated">T+X</target>
        </trans-unit>
        <trans-unit id="a614078b58d53b0cc0d0d780950744eb6f8ae5a6" translate="yes" xml:space="preserve">
          <source>TCP initialization</source>
          <target state="translated">TCP初期化</target>
        </trans-unit>
        <trans-unit id="ae0cb888e5b3c4b34090835d26990b761f9663d6" translate="yes" xml:space="preserve">
          <source>T_{cur}</source>
          <target state="translated">T_{cur}</target>
        </trans-unit>
        <trans-unit id="728548804a0da830440ca32e7488e5d512871f39" translate="yes" xml:space="preserve">
          <source>T_{cur}=0</source>
          <target state="translated">T_{cur}=0</target>
        </trans-unit>
        <trans-unit id="3f5463cfd9bd0758a96ea8db117479b2314fff2b" translate="yes" xml:space="preserve">
          <source>T_{cur}=T_{i}</source>
          <target state="translated">T_{cur}=T_{i}</target>
        </trans-unit>
        <trans-unit id="97af24691624857deb13ae1ab91e6164cd26f2be" translate="yes" xml:space="preserve">
          <source>T_{i}</source>
          <target state="translated">T_{i}</target>
        </trans-unit>
        <trans-unit id="e80c75f58bfc00b9ae732035ef70cb00333f4a1c" translate="yes" xml:space="preserve">
          <source>Take in and process masked source/target sequences.</source>
          <target state="translated">マスクされたソース/ターゲットシーケンスを取り込み、処理します。</target>
        </trans-unit>
        <trans-unit id="945a1e1f48dcf43605fff2c03f1828ea5952eb79" translate="yes" xml:space="preserve">
          <source>Take the instruction &lt;code&gt;%rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10&lt;/code&gt; for example.</source>
          <target state="translated">Take the instruction &lt;code&gt;%rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10&lt;/code&gt; for example.</target>
        </trans-unit>
        <trans-unit id="e94daa7e8a28b957c758019bcfa728a98c04545f" translate="yes" xml:space="preserve">
          <source>Takes LongTensor with index values of shape &lt;code&gt;(*)&lt;/code&gt; and returns a tensor of shape &lt;code&gt;(*, num_classes)&lt;/code&gt; that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.</source>
          <target state="translated">Takes LongTensor with index values of shape &lt;code&gt;(*)&lt;/code&gt; and returns a tensor of shape &lt;code&gt;(*, num_classes)&lt;/code&gt; that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.</target>
        </trans-unit>
        <trans-unit id="7d3331f3bb652437bd8045860b4a682532d6d90a" translate="yes" xml:space="preserve">
          <source>Takes the inverse of the square matrix &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Takes the inverse of the square matrix &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="537dfa1e7ebc0ec78b53314930b233f489d3f1c5" translate="yes" xml:space="preserve">
          <source>Takes the inverse of the square matrix &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; can be batches of 2D square tensors, in which case this function would return a tensor composed of individual inverses.</source>
          <target state="translated">Takes the inverse of the square matrix &lt;code&gt;input&lt;/code&gt; . &lt;code&gt;input&lt;/code&gt; can be batches of 2D square tensors, in which case this function would return a tensor composed of individual inverses.</target>
        </trans-unit>
        <trans-unit id="b0ad657474446cfbcb32a233330ba630a430b967" translate="yes" xml:space="preserve">
          <source>Takes the power of each element in &lt;code&gt;input&lt;/code&gt; with &lt;code&gt;exponent&lt;/code&gt; and returns a tensor with the result.</source>
          <target state="translated">Takes the power of each element in &lt;code&gt;input&lt;/code&gt; with &lt;code&gt;exponent&lt;/code&gt; and returns a tensor with the result.</target>
        </trans-unit>
        <trans-unit id="a9d3154243fedb184083b9dcec0135d9da418872" translate="yes" xml:space="preserve">
          <source>Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output:</source>
          <target state="translated">実値の周波数信号を時間領域に持ってくることで、ハーミッティアン・シンメトリックな出力が得られます。</target>
        </trans-unit>
        <trans-unit id="7ffacc7470da0fe6c6a8dae8d1ef37e2319cadeb" translate="yes" xml:space="preserve">
          <source>Taking an optimization step</source>
          <target state="translated">最適化のステップを取る</target>
        </trans-unit>
        <trans-unit id="fd4e923c14f3ac2ab68ed029691caa5081fb04f7" translate="yes" xml:space="preserve">
          <source>Taking care of batch normalization</source>
          <target state="translated">バッチ正規化のお世話</target>
        </trans-unit>
        <trans-unit id="19bff9fbbbadd7b339e0ee0ae1715d62fea9cae0" translate="yes" xml:space="preserve">
          <source>Tanh</source>
          <target state="translated">Tanh</target>
        </trans-unit>
        <trans-unit id="fa7acfd0630e85ee187c290a39277eaf9f5359da" translate="yes" xml:space="preserve">
          <source>Tanhshrink</source>
          <target state="translated">Tanhshrink</target>
        </trans-unit>
        <trans-unit id="652ac2cbbafccc62d55637f20bfa949ef565ffbd" translate="yes" xml:space="preserve">
          <source>Target:</source>
          <target state="translated">Target:</target>
        </trans-unit>
        <trans-unit id="3032d3d269195493b60b2b5232d8a35ccf1cf179" translate="yes" xml:space="preserve">
          <source>Target_lengths: Tuple or tensor of size</source>
          <target state="translated">Target_lengths:サイズのタプルまたはテンソル</target>
        </trans-unit>
        <trans-unit id="cce3202a79fe1345bd15442aee9a15d0666fb25f" translate="yes" xml:space="preserve">
          <source>Targets: Tensor of size</source>
          <target state="translated">ターゲット。サイズのテンソル</target>
        </trans-unit>
        <trans-unit id="680cb2d3fe397d144ed6bcd0531961c3a05ca6c0" translate="yes" xml:space="preserve">
          <source>Tensor</source>
          <target state="translated">Tensor</target>
        </trans-unit>
        <trans-unit id="7d374fa02b51c2a0bd017c85ffdbaad8aa59c748" translate="yes" xml:space="preserve">
          <source>Tensor Attributes</source>
          <target state="translated">テンソル属性</target>
        </trans-unit>
        <trans-unit id="47b8aa0b5113cdb5b15d1487280d5b3f9b245998" translate="yes" xml:space="preserve">
          <source>Tensor Views</source>
          <target state="translated">テンソルビュー</target>
        </trans-unit>
        <trans-unit id="a7ecaeb20f986ea84c795703082ccf954c1b6a60" translate="yes" xml:space="preserve">
          <source>Tensor autograd functions</source>
          <target state="translated">テンソルのオートグラッド機能</target>
        </trans-unit>
        <trans-unit id="7d5ce15cb1a252917b40c643e7a2eb6d5463df19" translate="yes" xml:space="preserve">
          <source>Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.</source>
          <target state="translated">テンソルは、より多くの寸法に拡張することも可能で、新しい寸法は先頭に追加されます。新しい次元については、サイズを-1に設定することはできません。</target>
        </trans-unit>
        <trans-unit id="44e4866b0b5a5220d0057d20a02b4c4e9ccd49d7" translate="yes" xml:space="preserve">
          <source>Tensor iterating over dimension 0.</source>
          <target state="translated">次元0を反復するテンソル。</target>
        </trans-unit>
        <trans-unit id="968efb5c209c67b8c5e5cb36ab67b3d4543380ee" translate="yes" xml:space="preserve">
          <source>Tensor of shape batch_shape.</source>
          <target state="translated">形状のテンソル batch_shape.</target>
        </trans-unit>
        <trans-unit id="1f4edfd7fe26e893eda2475a21249dda3127731b" translate="yes" xml:space="preserve">
          <source>Tensor of size &lt;code&gt;T x B x *&lt;/code&gt; if &lt;code&gt;batch_first&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;. Tensor of size &lt;code&gt;B x T x *&lt;/code&gt; otherwise</source>
          <target state="translated">Tensor of size &lt;code&gt;T x B x *&lt;/code&gt; if &lt;code&gt;batch_first&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; . Tensor of size &lt;code&gt;B x T x *&lt;/code&gt; otherwise</target>
        </trans-unit>
        <trans-unit id="1f64e01f2c0bfc00c617fa5cfe062d17dd7881fa" translate="yes" xml:space="preserve">
          <source>TensorPipe Backend</source>
          <target state="translated">TensorPipeバックエンド</target>
        </trans-unit>
        <trans-unit id="6fa30b4e43ae7ab6c13e5a62747d73bd4f6a50ff" translate="yes" xml:space="preserve">
          <source>Tensors</source>
          <target state="translated">Tensors</target>
        </trans-unit>
        <trans-unit id="3f3793b3bb16961a91ba80319c433489d751557f" translate="yes" xml:space="preserve">
          <source>Tensors may not have two named dimensions with the same name.</source>
          <target state="translated">テンソルは、同じ名前を持つ2つの名前付き寸法を持つことはできません。</target>
        </trans-unit>
        <trans-unit id="7a4ff50c90d4ae49393e7ddb58e4182969af3c75" translate="yes" xml:space="preserve">
          <source>Ternary Expressions</source>
          <target state="translated">三項式</target>
        </trans-unit>
        <trans-unit id="3aa34ed971590db58293e74b01ccf27489043c07" translate="yes" xml:space="preserve">
          <source>Tests if each element of &lt;code&gt;input&lt;/code&gt; has its sign bit set (is less than zero) or not.</source>
          <target state="translated">Tests if each element of &lt;code&gt;input&lt;/code&gt; has its sign bit set (is less than zero) or not.</target>
        </trans-unit>
        <trans-unit id="a5de15b26e6a795a7480af2f5882e1811d35f284" translate="yes" xml:space="preserve">
          <source>Tests if each element of &lt;code&gt;input&lt;/code&gt; is infinite (positive or negative infinity) or not.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 各要素が無限大（正または負の無限大）であるかどうかをテストします。</target>
        </trans-unit>
        <trans-unit id="a0285b21693b5a12974f50c902b26299554e6465" translate="yes" xml:space="preserve">
          <source>Tests if each element of &lt;code&gt;input&lt;/code&gt; is negative infinity or not.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 各要素が負の無限大であるかどうかをテストします。</target>
        </trans-unit>
        <trans-unit id="69536d45bf03029aac4041aa441f6044c68432e4" translate="yes" xml:space="preserve">
          <source>Tests if each element of &lt;code&gt;input&lt;/code&gt; is positive infinity or not.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 各要素が正の無限大であるかどうかをテストします。</target>
        </trans-unit>
        <trans-unit id="93ef0dd827103681fcee453b78be2ff14e1a261d" translate="yes" xml:space="preserve">
          <source>The</source>
          <target state="translated">The</target>
        </trans-unit>
        <trans-unit id="2312d5bb8d4ccdb5a8d40c96ff98fa570d836574" translate="yes" xml:space="preserve">
          <source>The 1-dimensional dot product version of this function does not support an &lt;code&gt;out&lt;/code&gt; parameter.</source>
          <target state="translated">この関数の1次元内積バージョンは、 &lt;code&gt;out&lt;/code&gt; パラメーターをサポートしていません。</target>
        </trans-unit>
        <trans-unit id="f0763c643a0c9e64fc2e1987a16313347a5b7157" translate="yes" xml:space="preserve">
          <source>The 1.6 release of PyTorch switched &lt;code&gt;torch.save&lt;/code&gt; to use a new zipfile-based file format. &lt;code&gt;torch.load&lt;/code&gt; still retains the ability to load files in the old format. If for any reason you want &lt;code&gt;torch.save&lt;/code&gt; to use the old format, pass the kwarg &lt;code&gt;_use_new_zipfile_serialization=False&lt;/code&gt;.</source>
          <target state="translated">PyTorchの1.6リリースでは、torch.saveが新しい &lt;code&gt;torch.save&lt;/code&gt; ファイルベースのファイル形式を使用するように切り替えられました。 &lt;code&gt;torch.load&lt;/code&gt; は、古い形式でファイルをロードする機能を引き続き保持します。何らかの理由で &lt;code&gt;torch.save&lt;/code&gt; に古い形式を使用させたい場合は、kwarg &lt;code&gt;_use_new_zipfile_serialization=False&lt;/code&gt; を渡します。</target>
        </trans-unit>
        <trans-unit id="539ba0cb4053409de4820d1ae27b5ebc56acc073" translate="yes" xml:space="preserve">
          <source>The 1cycle learning rate policy changes the learning rate after every batch. &lt;code&gt;step&lt;/code&gt; should be called after a batch has been used for training.</source>
          <target state="translated">1cycle学習率ポリシーは、バッチごとに学習率を変更します。バッチがトレーニングに使用された後、 &lt;code&gt;step&lt;/code&gt; を呼び出す必要があります。</target>
        </trans-unit>
        <trans-unit id="a532bc29eaf3d25b0af52da2857f32bdfeed457d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt;th dimension of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; must have the same size as the length of &lt;code&gt;index&lt;/code&gt; (which must be a vector), and all other dimensions must match &lt;code&gt;self&lt;/code&gt;, or an error will be raised.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim&lt;/code&gt; &lt;/a&gt;番目の次元の&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;の長さと同じ大きされている必要があり &lt;code&gt;index&lt;/code&gt; （ベクトルでなければなりません）、および他のすべての寸法が一致しなければならない &lt;code&gt;self&lt;/code&gt; 、またはエラーが発生します。</target>
        </trans-unit>
        <trans-unit id="cbb84ae310dddae342bffdaeaa243633f2a53249" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.quasirandom.SobolEngine&quot;&gt;&lt;code&gt;torch.quasirandom.SobolEngine&lt;/code&gt;&lt;/a&gt; is an engine for generating (scrambled) Sobol sequences. Sobol sequences are an example of low discrepancy quasi-random sequences.</source>
          <target state="translated">&lt;a href=&quot;#torch.quasirandom.SobolEngine&quot;&gt; &lt;code&gt;torch.quasirandom.SobolEngine&lt;/code&gt; は&lt;/a&gt;（スクランブル）ソボル配列を生成するためのエンジンです。ソボル列は、不一致の少ない準ランダムシーケンスの例です。</target>
        </trans-unit>
        <trans-unit id="d2e5c436ae6a7b841286261fcf013451bf51db2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; argument in functions can generally be substituted with a string. This allows for fast prototyping of code.</source>
          <target state="translated">関数の&lt;a href=&quot;#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;引数は、通常、文字列に置き換えることができます。これにより、コードの高速プロトタイピングが可能になります。</target>
        </trans-unit>
        <trans-unit id="f98c90dfa427c3d98ac8ee0cbb86e967d2daadab" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; contains a device type (&lt;code&gt;'cpu'&lt;/code&gt; or &lt;code&gt;'cuda'&lt;/code&gt;) and optional device ordinal for the device type. If the device ordinal is not present, this object will always represent the current device for the device type, even after &lt;a href=&quot;cuda#torch.cuda.set_device&quot;&gt;&lt;code&gt;torch.cuda.set_device()&lt;/code&gt;&lt;/a&gt; is called; e.g., a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; constructed with device &lt;code&gt;'cuda'&lt;/code&gt; is equivalent to &lt;code&gt;'cuda:X'&lt;/code&gt; where X is the result of &lt;a href=&quot;cuda#torch.cuda.current_device&quot;&gt;&lt;code&gt;torch.cuda.current_device()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; は、&lt;/a&gt;デバイスタイプ（含ま &lt;code&gt;'cpu'&lt;/code&gt; または &lt;code&gt;'cuda'&lt;/code&gt; ）とデバイスタイプの任意のデバイス順序を。デバイスの順序が存在しない場合、&lt;a href=&quot;cuda#torch.cuda.set_device&quot;&gt; &lt;code&gt;torch.cuda.set_device()&lt;/code&gt; &lt;/a&gt;が呼び出された後でも、このオブジェクトは常にデバイスタイプの現在のデバイスを表します。たとえば、デバイス &lt;code&gt;'cuda'&lt;/code&gt; 構築された&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;は &lt;code&gt;'cuda:X'&lt;/code&gt; と同等です。ここで、Xは&lt;a href=&quot;cuda#torch.cuda.current_device&quot;&gt; &lt;code&gt;torch.cuda.current_device()&lt;/code&gt; の&lt;/a&gt;結果です。</target>
        </trans-unit>
        <trans-unit id="fb07fddc9a5d6fb2f9f535ebf9278f5523df3bef" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.</source>
          <target state="translated">&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;ロード順序および任意の自動バッチ処理（照合）とメモリピニングのカスタマイズ、シングルまたはマルチプロセス負荷でマップスタイルおよび反復可能スタイルデータセットの両方をサポートします。</target>
        </trans-unit>
        <trans-unit id="36716fd31e682e7596ade3d0fc033b9418b25151" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; annotation&amp;rsquo;s behavior changes in PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function or method callable from code that is exported. To get this functionality back, use &lt;code&gt;@torch.jit.unused()&lt;/code&gt;. &lt;code&gt;@torch.jit.ignore&lt;/code&gt; is now equivalent to &lt;code&gt;@torch.jit.ignore(drop=False)&lt;/code&gt;. See &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; PyTorch 1.2での注釈の動作の変更。PyTorch 1.2より前は、@ ignoreデコレータを使用して、エクスポートされたコードから関数またはメソッドを呼び出し可能にしました。この機能を元に戻すには、 &lt;code&gt;@torch.jit.unused()&lt;/code&gt; 使用します。 &lt;code&gt;@torch.jit.ignore&lt;/code&gt; は、 &lt;code&gt;@torch.jit.ignore(drop=False)&lt;/code&gt; と同等になりました。詳細については、&lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="629d16d00f1d4af587a0e984e39f346b9c762a9d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine&quot;&gt;&lt;code&gt;torch.quasirandom.SobolEngine&lt;/code&gt;&lt;/a&gt; is an engine for generating (scrambled) Sobol sequences.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine&quot;&gt; &lt;code&gt;torch.quasirandom.SobolEngine&lt;/code&gt; は&lt;/a&gt;（スクランブル）ソボル配列を生成するためのエンジンです。</target>
        </trans-unit>
        <trans-unit id="b29b06dcd5a85e7f1ec30bf4cac512a58a2b2613" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback-Leibler_divergence&quot;&gt;Kullback-Leibler divergence Loss&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback-Leibler_divergence&quot;&gt;カルバック・ライブラー情報量損失&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9f5b85ebb4e0453454d4b99cfd4ca555eb171ffc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;multiprocessing#multiprocessing-doc&quot;&gt;Multiprocessing package - torch.multiprocessing&lt;/a&gt; package also provides a &lt;code&gt;spawn&lt;/code&gt; function in &lt;a href=&quot;multiprocessing#torch.multiprocessing.spawn&quot;&gt;&lt;code&gt;torch.multiprocessing.spawn()&lt;/code&gt;&lt;/a&gt;. This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well.</source>
          <target state="translated">&lt;a href=&quot;multiprocessing#multiprocessing-doc&quot;&gt;マルチプロセッシングパッケージ- torch.multiprocessing&lt;/a&gt;パッケージも提供して &lt;code&gt;spawn&lt;/code&gt; で関数を&lt;a href=&quot;multiprocessing#torch.multiprocessing.spawn&quot;&gt; &lt;code&gt;torch.multiprocessing.spawn()&lt;/code&gt; &lt;/a&gt;。このヘルパー関数は、複数のプロセスを生成するために使用できます。実行する関数を渡すことで機能し、N個のプロセスを生成して実行します。これは、マルチプロセス分散トレーニングにも使用できます。</target>
        </trans-unit>
        <trans-unit id="9e1098da0df12b2fcc4a3fa80eabfd00f062a3e8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; is a tensor with the mean of each output element&amp;rsquo;s normal distribution</source>
          <target state="translated">&lt;a href=&quot;torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt;は、各出力要素の正規分布の平均を持つテンソルです。</target>
        </trans-unit>
        <trans-unit id="235814295d4fe94a904ed9a1a782c8eebf3b58c5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt; is a tensor with the standard deviation of each output element&amp;rsquo;s normal distribution</source>
          <target state="translated">&lt;a href=&quot;torch.std#torch.std&quot;&gt; &lt;code&gt;std&lt;/code&gt; &lt;/a&gt;各出力要素の正規分布の標準偏差テンソルであります</target>
        </trans-unit>
        <trans-unit id="2fc3044dc4c4ace9a26403a7a793e46904198525" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;@torch.jit.script&lt;/code&gt; decorator will construct a &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; by compiling the body of the function.</source>
          <target state="translated">&lt;code&gt;@torch.jit.script&lt;/code&gt; デコレータが構築されます&lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; を&lt;/a&gt;関数の本体をコンパイルして。</target>
        </trans-unit>
        <trans-unit id="b8a401f5a86feecc05362977ef403b66b5efedab" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;@torch.jit.script_method&lt;/code&gt; decorator</source>
          <target state="translated">&lt;code&gt;@torch.jit.script_method&lt;/code&gt; デコレータ</target>
        </trans-unit>
        <trans-unit id="9011f0cf08e406be11c8a00cbbbc2003dbdb187f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;Final&lt;/code&gt; type constructor can be used to mark members as &lt;code&gt;constant&lt;/code&gt;. If members are not marked constant, they will be copied to the resulting &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; as an attribute. Using &lt;code&gt;Final&lt;/code&gt; opens opportunities for optimization if the value is known to be fixed and gives additional type safety.</source>
          <target state="translated">&lt;code&gt;Final&lt;/code&gt; 型コンストラクタは、としてメンバーをマークするために使用することができます &lt;code&gt;constant&lt;/code&gt; 。メンバーが定数としてマークされていない場合、それらは属性として結果の&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; に&lt;/a&gt;コピーされます。 &lt;code&gt;Final&lt;/code&gt; を使用すると、値が固定されていることがわかっている場合に最適化の機会が開かれ、タイプの安全性が向上します。</target>
        </trans-unit>
        <trans-unit id="94d670ae23dc45d9238050515c460edb409b464f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;MixtureSameFamily&lt;/code&gt; distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a &lt;code&gt;Categorical&lt;/code&gt; &amp;ldquo;selecting distribution&amp;rdquo; (over &lt;code&gt;k&lt;/code&gt; component) and a component distribution, i.e., a &lt;code&gt;Distribution&lt;/code&gt; with a rightmost batch shape (equal to &lt;code&gt;[k]&lt;/code&gt;) which indexes each (batch of) component.</source>
          <target state="translated">&lt;code&gt;MixtureSameFamily&lt;/code&gt; 分配用具すべてのコンポーネントが同じ分布型の異なるパラメータ化に由来する混合分布（バッチ）。それによってパラメータ化された &lt;code&gt;Categorical&lt;/code&gt; （オーバー「分布の選択」 &lt;code&gt;k&lt;/code&gt; 成分）と成分分布、すなわち、 &lt;code&gt;Distribution&lt;/code&gt; 右端バッチ状の（に等しい &lt;code&gt;[k]&lt;/code&gt; 成分インデックス各（バッチ）を。</target>
        </trans-unit>
        <trans-unit id="681a06025ef48d63b48a8f0ade5d434c9ed8e4d8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;None&lt;/code&gt; check must be within the if-statement&amp;rsquo;s condition; assigning a &lt;code&gt;None&lt;/code&gt; check to a variable and using it in the if-statement&amp;rsquo;s condition will not refine the types of variables in the check. Only local variables will be refined, an attribute like &lt;code&gt;self.x&lt;/code&gt; will not and must assigned to a local variable to be refined.</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; チェックは、if文の条件の範囲内でなければなりません。変数に &lt;code&gt;None&lt;/code&gt; チェックを割り当て、それをifステートメントの条件で使用しても、チェック内の変数のタイプは絞り込まれません。ローカル変数のみがリファインされます &lt;code&gt;self.x&lt;/code&gt; ような属性はリファインされません。また、リファインするにはローカル変数に割り当てる必要があります。</target>
        </trans-unit>
        <trans-unit id="c57db6f6aeb13e4c4808482871b866cd32a08ce8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;SummaryWriter&lt;/code&gt; class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.</source>
          <target state="translated">&lt;code&gt;SummaryWriter&lt;/code&gt; のクラスは、指定されたディレクトリ内のイベントファイルを作成し、それを要約してイベントを追加するための高レベルAPIを提供します。このクラスは、ファイルの内容を非同期的に更新します。これにより、トレーニングプログラムは、トレーニングを遅くすることなく、メソッドを呼び出してトレーニングループから直接ファイルにデータを追加できます。</target>
        </trans-unit>
        <trans-unit id="965753cbd79a45739bc9ad1bc6ad8aab3c55f336" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;__constants__&lt;/code&gt; array</source>
          <target state="translated">&lt;code&gt;__constants__&lt;/code&gt; 配列</target>
        </trans-unit>
        <trans-unit id="f242923c2659bf7bc5a464857040425ba312d13f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;__len__()&lt;/code&gt; method isn&amp;rsquo;t strictly required by &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;, but is expected in any calculation involving the length of a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;__len__()&lt;/code&gt; メソッドは、厳密には必要ありません&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;が、長さの伴う任意の計算には期待されている&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="eba7c266175279603efbf76b860806dfd23d6385" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;batch_size&lt;/code&gt; and &lt;code&gt;drop_last&lt;/code&gt; arguments essentially are used to construct a &lt;code&gt;batch_sampler&lt;/code&gt; from &lt;code&gt;sampler&lt;/code&gt;. For map-style datasets, the &lt;code&gt;sampler&lt;/code&gt; is either provided by user or constructed based on the &lt;code&gt;shuffle&lt;/code&gt; argument. For iterable-style datasets, the &lt;code&gt;sampler&lt;/code&gt; is a dummy infinite one. See &lt;a href=&quot;#data-loading-order-and-sampler&quot;&gt;this section&lt;/a&gt; on more details on samplers.</source>
          <target state="translated">&lt;code&gt;batch_size&lt;/code&gt; と &lt;code&gt;drop_last&lt;/code&gt; 引数は、本質的に構築するために使用されている &lt;code&gt;batch_sampler&lt;/code&gt; から &lt;code&gt;sampler&lt;/code&gt; 。マップスタイルのデータセットの場合、 &lt;code&gt;sampler&lt;/code&gt; はユーザーによって提供されるか、 &lt;code&gt;shuffle&lt;/code&gt; 引数に基づいて構築されます。反復可能なスタイルのデータセットの場合、 &lt;code&gt;sampler&lt;/code&gt; はダミーの無限のデータセットです。サンプラーの詳細については、&lt;a href=&quot;#data-loading-order-and-sampler&quot;&gt;このセクション&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="a26eee2b4f8c5fa56dc0e1b6d51769e2674252ff" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;biject_to()&lt;/code&gt; registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained &lt;code&gt;.support&lt;/code&gt; are propagated in an unconstrained space, and algorithms are typically rotation invariant.:</source>
          <target state="translated">&lt;code&gt;biject_to()&lt;/code&gt; レジストリは、制約付き確率分布からサンプルハミルトニアンモンテカルロ、に有用である &lt;code&gt;.support&lt;/code&gt; が非拘束空間に伝播され、およびアルゴリズムは、典型的には回転不変です：</target>
        </trans-unit>
        <trans-unit id="6d56e7b5edb09cb0ed4ea4637fa714f0dc8503dd" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;biject_to&lt;/code&gt; and &lt;code&gt;transform_to&lt;/code&gt; objects can be extended by user-defined constraints and transforms using their &lt;code&gt;.register()&lt;/code&gt; method either as a function on singleton constraints:</source>
          <target state="translated">&lt;code&gt;biject_to&lt;/code&gt; と &lt;code&gt;transform_to&lt;/code&gt; オブジェクトは、使用してユーザー定義の制約と変換することによって拡張することができる &lt;code&gt;.register()&lt;/code&gt; シングルトン制約の関数としての方法のいずれかを：</target>
        </trans-unit>
        <trans-unit id="44a5102b3b32f58d5c9c3eb95af7ab62d791ac38" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;callable&lt;/code&gt; should have the signature:</source>
          <target state="translated">&lt;code&gt;callable&lt;/code&gt; 署名を持っている必要があります。</target>
        </trans-unit>
        <trans-unit id="c4cd295b30506f6711ba85f58a42cc9115e197d2" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;delete_key&lt;/code&gt; API is only supported by the &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;. Using this API with the &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt; will result in an exception.</source>
          <target state="translated">&lt;code&gt;delete_key&lt;/code&gt; APIのみでサポートされてい&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt;。このAPIを&lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; で使用&lt;/a&gt;すると、例外が発生します。</target>
        </trans-unit>
        <trans-unit id="79ae973263b72787a35d19a21f1d5ebc7a491f12" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;distributions&lt;/code&gt; package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the &lt;a href=&quot;https://arxiv.org/abs/1711.10604&quot;&gt;TensorFlow Distributions&lt;/a&gt; package.</source>
          <target state="translated">&lt;code&gt;distributions&lt;/code&gt; パッケージには、パラメータ化確率分布とサンプリング機能が含まれています。これにより、最適化のための確率的計算グラフと確率的勾配推定量の構築が可能になります。このパッケージは通常、&lt;a href=&quot;https://arxiv.org/abs/1711.10604&quot;&gt;TensorFlowディストリビューション&lt;/a&gt;パッケージの設計に従います。</target>
        </trans-unit>
        <trans-unit id="3e75f8845977efa943a8e24244efa0158e93fd55" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of &lt;code&gt;grad_input&lt;/code&gt; in subsequent computations. &lt;code&gt;grad_input&lt;/code&gt; will only correspond to the inputs given as positional arguments.</source>
          <target state="translated">&lt;code&gt;grad_input&lt;/code&gt; と &lt;code&gt;grad_output&lt;/code&gt; は、モジュールは、複数の入力または出力を有する場合タプルであってもよいです。フックは引数を変更するべきではありませんが、オプションで、後続の計算で &lt;code&gt;grad_input&lt;/code&gt; の代わりに使用される入力に関して新しい勾配を返すことができます。 &lt;code&gt;grad_input&lt;/code&gt; は、位置引数として指定された入力にのみ対応します。</target>
        </trans-unit>
        <trans-unit id="c418e4ba558348daf084f57cfc5ba8f635661eb0" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;gradient_as_bucket_view&lt;/code&gt; mode does not yet work with Automatic Mixed Precision (AMP). AMP maintains stashed gradients that are used for unscaling gradients. With &lt;code&gt;gradient_as_bucket_view=True&lt;/code&gt;, these stashed gradients will point to communication buckets in the first iteration. In the next iteration, the communication buckets are mutated and thus these stashed gradients will be unexpectedly mutated as well, which might lead to wrong results.</source>
          <target state="translated">&lt;code&gt;gradient_as_bucket_view&lt;/code&gt; のモードはまだ自動混合プレシジョン（AMP）と仕事をしません。AMPは、グラデーションのスケーリングを解除するために使用される隠しグラデーションを維持します。 &lt;code&gt;gradient_as_bucket_view=True&lt;/code&gt; 、これらの隠し勾配は最初の反復で通信バケットを指します。次の反復では、通信バケットが変更されるため、これらの隠されたグラデーションも予期せず変更され、誤った結果が生じる可能性があります。</target>
        </trans-unit>
        <trans-unit id="1fd870aae016b2c00935c2fbc6c52d9270bae088" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;input&lt;/code&gt; given through a forward call is expected to contain log-probabilities of each class. &lt;code&gt;input&lt;/code&gt; has to be a Tensor of size either</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; コール転送を介して与えられた各クラスの対数確率を含むことが期待されます。 &lt;code&gt;input&lt;/code&gt; はいずれかのサイズのテンソルである必要があります</target>
        </trans-unit>
        <trans-unit id="60f14a9d856acc735af342914f0ff8281b2528f6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;input&lt;/code&gt; is expected to contain raw, unnormalized scores for each class.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 各クラスの生、非正規化スコアを含むことが期待されます。</target>
        </trans-unit>
        <trans-unit id="6624e1abebb827c38014a043499dfb4fc7c338b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;input&lt;/code&gt; tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in &lt;code&gt;input&lt;/code&gt; have to be in the range:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; テンソルは、バイナリ乱数を描画するために使用される確率を含むテンソルであるべきです。したがって、 &lt;code&gt;input&lt;/code&gt; すべての値は次の範囲内にある必要があります。</target>
        </trans-unit>
        <trans-unit id="fb4a0ff705e3f1a46ebc8daed33e4bce289527c8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;mask&lt;/code&gt; operates on the &lt;code&gt;self&lt;/code&gt; tensor, not on the given &lt;code&gt;source&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;mask&lt;/code&gt; 上で動作し &lt;code&gt;self&lt;/code&gt; ではない与えられた上で、テンソル &lt;code&gt;source&lt;/code&gt; テンソル。</target>
        </trans-unit>
        <trans-unit id="ad5a92992bc736ff5322b797aa0cf7673e44901d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;n_fft&lt;/code&gt;, &lt;code&gt;hop_length&lt;/code&gt;, &lt;code&gt;win_length&lt;/code&gt; are all the same which prevents the calculation of right padding. These additional values could be zeros or a reflection of the signal so providing &lt;code&gt;length&lt;/code&gt; could be useful. If &lt;code&gt;length&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; then padding will be aggressively removed (some loss of signal).</source>
          <target state="translated">&lt;code&gt;n_fft&lt;/code&gt; 、 &lt;code&gt;hop_length&lt;/code&gt; 、 &lt;code&gt;win_length&lt;/code&gt; は右パディングの計算を防止する全て同じです。これらの追加値はゼロまたは信号の反射である可能性があるため、 &lt;code&gt;length&lt;/code&gt; すると便利です。 &lt;code&gt;length&lt;/code&gt; が &lt;code&gt;None&lt;/code&gt; の場合、パディングは積極的に削除されます（信号がいくらか失われます）。</target>
        </trans-unit>
        <trans-unit id="e0ace9222bb0e305cbf096e49fb16c8bd736635b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;num_keys&lt;/code&gt; API is only supported by the &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;. Using this API with the &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt; will result in an exception.</source>
          <target state="translated">&lt;code&gt;num_keys&lt;/code&gt; のAPIのみでサポートされてい&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt;。このAPIを&lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; で使用&lt;/a&gt;すると、例外が発生します。</target>
        </trans-unit>
        <trans-unit id="8bd6f3dbf9c9f10c4e3b624b6d33b29b1f55e1ac" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;padding&lt;/code&gt; argument effectively adds &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; amount of zero padding to both sizes of the input. This is set so that when a &lt;a href=&quot;torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; and a &lt;a href=&quot;#torch.nn.ConvTranspose1d&quot;&gt;&lt;code&gt;ConvTranspose1d&lt;/code&gt;&lt;/a&gt; are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when &lt;code&gt;stride &amp;gt; 1&lt;/code&gt;, &lt;a href=&quot;torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; maps multiple input shapes to the same output shape. &lt;code&gt;output_padding&lt;/code&gt; is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that &lt;code&gt;output_padding&lt;/code&gt; is only used to find output shape, but does not actually add zero-padding to output.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; 引数が有効追加 &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; 入力の両方のサイズにゼロパディングの量。これは、&lt;a href=&quot;torch.nn.conv1d#torch.nn.Conv1d&quot;&gt; &lt;code&gt;Conv1d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#torch.nn.ConvTranspose1d&quot;&gt; &lt;code&gt;ConvTranspose1d&lt;/code&gt; &lt;/a&gt;が同じパラメーターで初期化されたときに、入力と出力の形状に関して互いに逆になるように設定されています。ただし、 &lt;code&gt;stride &amp;gt; 1&lt;/code&gt; &lt;a href=&quot;torch.nn.conv1d#torch.nn.Conv1d&quot;&gt; &lt;code&gt;Conv1d&lt;/code&gt; &lt;/a&gt;場合、Conv1dは複数の入力形状を同じ出力形状にマッピングします。 &lt;code&gt;output_padding&lt;/code&gt; は、計算された出力形状を片側で効果的に増やすことにより、このあいまいさを解決するために提供されています。 &lt;code&gt;output_padding&lt;/code&gt; は出力形状を見つけるためにのみ使用され、実際には出力にゼロパディングを追加しないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="5440e08e1ef68605bef85def7ab3c93778452300" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;padding&lt;/code&gt; argument effectively adds &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; amount of zero padding to both sizes of the input. This is set so that when a &lt;a href=&quot;torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; and a &lt;a href=&quot;#torch.nn.ConvTranspose2d&quot;&gt;&lt;code&gt;ConvTranspose2d&lt;/code&gt;&lt;/a&gt; are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when &lt;code&gt;stride &amp;gt; 1&lt;/code&gt;, &lt;a href=&quot;torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; maps multiple input shapes to the same output shape. &lt;code&gt;output_padding&lt;/code&gt; is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that &lt;code&gt;output_padding&lt;/code&gt; is only used to find output shape, but does not actually add zero-padding to output.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; 引数が有効追加 &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; 入力の両方のサイズにゼロパディングの量。これは、&lt;a href=&quot;torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#torch.nn.ConvTranspose2d&quot;&gt; &lt;code&gt;ConvTranspose2d&lt;/code&gt; &lt;/a&gt;が同じパラメーターで初期化されたときに、入力形状と出力形状に関して互いに逆になるように設定されています。ただし、 &lt;code&gt;stride &amp;gt; 1&lt;/code&gt; &lt;a href=&quot;torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt;場合、Conv2dは複数の入力形状を同じ出力形状にマッピングします。 &lt;code&gt;output_padding&lt;/code&gt; は、計算された出力形状を片側で効果的に増やすことにより、このあいまいさを解決するために提供されています。 &lt;code&gt;output_padding&lt;/code&gt; は出力形状を見つけるためにのみ使用され、実際には出力にゼロパディングを追加しないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="624a36cdb030302abd92f10a870506676fb58830" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;padding&lt;/code&gt; argument effectively adds &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; amount of zero padding to both sizes of the input. This is set so that when a &lt;a href=&quot;torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; and a &lt;a href=&quot;#torch.nn.ConvTranspose3d&quot;&gt;&lt;code&gt;ConvTranspose3d&lt;/code&gt;&lt;/a&gt; are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when &lt;code&gt;stride &amp;gt; 1&lt;/code&gt;, &lt;a href=&quot;torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; maps multiple input shapes to the same output shape. &lt;code&gt;output_padding&lt;/code&gt; is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that &lt;code&gt;output_padding&lt;/code&gt; is only used to find output shape, but does not actually add zero-padding to output.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; 引数が有効追加 &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; 入力の両方のサイズにゼロパディングの量。これは、&lt;a href=&quot;torch.nn.conv3d#torch.nn.Conv3d&quot;&gt; &lt;code&gt;Conv3d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#torch.nn.ConvTranspose3d&quot;&gt; &lt;code&gt;ConvTranspose3d&lt;/code&gt; &lt;/a&gt;が同じパラメーターで初期化されたときに、入力と出力の形状に関して互いに逆になるように設定されています。ただし、 &lt;code&gt;stride &amp;gt; 1&lt;/code&gt; &lt;a href=&quot;torch.nn.conv3d#torch.nn.Conv3d&quot;&gt; &lt;code&gt;Conv3d&lt;/code&gt; &lt;/a&gt;場合、Conv3dは複数の入力形状を同じ出力形状にマッピングします。 &lt;code&gt;output_padding&lt;/code&gt; は、計算された出力形状を片側で効果的に増やすことにより、このあいまいさを解決するために提供されています。 &lt;code&gt;output_padding&lt;/code&gt; は出力形状を見つけるためにのみ使用され、実際には出力にゼロパディングを追加しないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="ac2a8aee0768d2dee229b10ae182a5bbe1013563" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; and &lt;code&gt;dilation&lt;/code&gt; arguments specify how the sliding blocks are retrieved.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; と &lt;code&gt;dilation&lt;/code&gt; の引数は、スライディングブロックを取得する方法を指定します。</target>
        </trans-unit>
        <trans-unit id="9a86ce4705a9d8461e1968c13331ef942f95a281" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remote&lt;/code&gt; API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the &lt;a href=&quot;#torch.distributed.rpc.RRef.confirmed_by_owner&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.confirmed_by_owner()&lt;/code&gt;&lt;/a&gt; API.</source>
          <target state="translated">&lt;code&gt;remote&lt;/code&gt; APIは、RPCのバックエンドの種類に応じて、別のスレッドによって行うことができるワイヤー、上にそれらを送信するまで、引数のテンソルのストレージをコピーしません。呼び出し元は、返されたRRefが所有者によって確認されるまで、これらのテンソルの内容がそのまま残っていることを確認する必要があります。これは、&lt;a href=&quot;#torch.distributed.rpc.RRef.confirmed_by_owner&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.confirmed_by_owner()&lt;/code&gt; &lt;/a&gt; APIを使用して確認できます。</target>
        </trans-unit>
        <trans-unit id="6d9f2b4532f5542083c83b1d9834f5f944e782fe" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;rpc_async&lt;/code&gt; API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; completes.</source>
          <target state="translated">&lt;code&gt;rpc_async&lt;/code&gt; のAPIは、RPCのバックエンドの種類に応じて、別のスレッドによって行うことができるワイヤー、上にそれらを送信するまで、引数のテンソルのストレージをコピーしません。呼び出し元は、返された&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; が&lt;/a&gt;完了するまで、これらのテンソルの内容がそのままであることを確認する必要があります。</target>
        </trans-unit>
        <trans-unit id="3596722c9e70073957aeadba20c33b436e025428" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;spawn&lt;/code&gt; function below addresses these concerns and takes care of error propagation, out of order termination, and will actively terminate processes upon detecting an error in one of them.</source>
          <target state="translated">以下の &lt;code&gt;spawn&lt;/code&gt; 関数は、これらの懸念に対処し、エラーの伝播、順不同の終了を処理し、そのうちの1つでエラーを検出するとプロセスをアクティブに終了します。</target>
        </trans-unit>
        <trans-unit id="8a7cdd56e507990bc132c8770c06d1f57641cbc2" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;src&lt;/code&gt; tensor must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the &lt;code&gt;self&lt;/code&gt; tensor. It may be of a different data type or reside on a different device.</source>
          <target state="translated">&lt;code&gt;src&lt;/code&gt; テンソルでなければなりません&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;と &lt;code&gt;self&lt;/code&gt; テンソル。データタイプが異なる場合もあれば、別のデバイスに存在する場合もあります。</target>
        </trans-unit>
        <trans-unit id="17a168b9f78b1ca48e7a2120cd057ee2f894411e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;target&lt;/code&gt; that this loss expects should be a class index in the range</source>
          <target state="translated">&lt;code&gt;target&lt;/code&gt; この損失を期待範囲内のクラスのインデックスでなければなりません</target>
        </trans-unit>
        <trans-unit id="c1785ac75c5e40c0724177cfc6462e25d84d9b18" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.distributed&lt;/code&gt; package also provides a launch utility in &lt;code&gt;torch.distributed.launch&lt;/code&gt;. This helper utility can be used to launch multiple processes per node for distributed training.</source>
          <target state="translated">&lt;code&gt;torch.distributed&lt;/code&gt; パッケージはまた、打ち上げユーティリティを提供 &lt;code&gt;torch.distributed.launch&lt;/code&gt; を。このヘルパーユーティリティを使用して、分散トレーニングのためにノードごとに複数のプロセスを起動できます。</target>
        </trans-unit>
        <trans-unit id="72f50afcb54f84e9f4aac792b1e96785232d670d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.distributed&lt;/code&gt; package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel()&lt;/code&gt;&lt;/a&gt; builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by &lt;a href=&quot;multiprocessing&quot;&gt;Multiprocessing package - torch.multiprocessing&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;torch.nn.DataParallel()&lt;/code&gt;&lt;/a&gt; in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process.</source>
          <target state="translated">&lt;code&gt;torch.distributed&lt;/code&gt; パッケージは、一つまたは複数のマシン上で実行されているいくつかの計算ノード間でマルチ平行ためPyTorchサポートと通信プリミティブを提供します。クラス&lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel()&lt;/code&gt; &lt;/a&gt;は、この機能に基づいて構築されており、PyTorchモデルのラッパーとして同期分散トレーニングを提供します。これは、&lt;a href=&quot;multiprocessing&quot;&gt;マルチプロセッシングパッケージ&lt;/a&gt;&lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt; &lt;code&gt;torch.nn.DataParallel()&lt;/code&gt; &lt;/a&gt; torch.multiprocessingおよびtorch.nn.DataParallel（）によって提供される並列処理の種類）とは異なり、ネットワークに接続された複数のマシンをサポートし、ユーザーはそれぞれのメイントレーニングスクリプトの個別のコピーを明示的に起動する必要があります。処理する。</target>
        </trans-unit>
        <trans-unit id="f1ce33b04ab70eff6a70899c7f72e87b970b8ffd" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.futures&lt;/code&gt; package is a &lt;strong&gt;Prototype&lt;/strong&gt; feature and subject to change.</source>
          <target state="translated">&lt;code&gt;torch.futures&lt;/code&gt; のパッケージがある&lt;strong&gt;プロトタイプ&lt;/strong&gt;機能と変更されること。</target>
        </trans-unit>
        <trans-unit id="4ae3c23dcd7aaa1724a7bbfd85a5d075a7d8913b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.jit.Attribute&lt;/code&gt; wrapper class</source>
          <target state="translated">&lt;code&gt;torch.jit.Attribute&lt;/code&gt; のラッパークラス</target>
        </trans-unit>
        <trans-unit id="cd6ab112f221ddabf781ca4bc22d2c58fdee3a44" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.jit.annotate&lt;/code&gt; function</source>
          <target state="translated">&lt;code&gt;torch.jit.annotate&lt;/code&gt; の機能</target>
        </trans-unit>
        <trans-unit id="addf386bae846dc433dd4589519a4e51ea1626e9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.layout&lt;/code&gt; class is in beta and subject to change.</source>
          <target state="translated">&lt;code&gt;torch.layout&lt;/code&gt; のクラスでは、ベータ版や変更の対象です。</target>
        </trans-unit>
        <trans-unit id="69af7e2fc3132014bb438bfe3f8c1bbddaee2ce2" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;torch.nn.Parameter&lt;/code&gt; wrapper and &lt;code&gt;register_buffer&lt;/code&gt; can be used to assign tensors to a module. Other values assigned to a module that is compiled will be added to the compiled module if their types can be inferred. All &lt;a href=&quot;#types&quot;&gt;types&lt;/a&gt; available in TorchScript can be used as module attributes. Tensor attributes are semantically the same as buffers. The type of empty lists and dictionaries and &lt;code&gt;None&lt;/code&gt; values cannot be inferred and must be specified via &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations. If a type cannot be inferred and is not explicilty annotated, it will not be added as an attribute to the resulting &lt;code&gt;ScriptModule&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;torch.nn.Parameter&lt;/code&gt; のラッパーと &lt;code&gt;register_buffer&lt;/code&gt; はモジュールに割り当てるテンソルに使用することができます。コンパイルされたモジュールに割り当てられた他の値は、それらのタイプが推測できる場合、コンパイルされたモジュールに追加されます。TorchScriptで使用可能なすべての&lt;a href=&quot;#types&quot;&gt;タイプは&lt;/a&gt;、モジュール属性として使用できます。テンソル属性は、意味的にはバッファーと同じです。空のリストと辞書のタイプおよび &lt;code&gt;None&lt;/code&gt; 値は推測できないため、&lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP526スタイルの&lt;/a&gt;クラス注釈を介して指定する必要があります。タイプを推測できず、明示的に注釈が付けられていない場合、そのタイプは結果の &lt;code&gt;ScriptModule&lt;/code&gt; に属性として追加されません。</target>
        </trans-unit>
        <trans-unit id="dd928d04d4959114feb5d458396348030d86627d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;transform_to()&lt;/code&gt; registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution&amp;rsquo;s &lt;code&gt;.arg_constraints&lt;/code&gt; dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam:</source>
          <target state="translated">&lt;code&gt;transform_to()&lt;/code&gt; レジストリは、各配布者によって示されている確率分布の制約パラメータに制約なし最適化を実行するために有用である &lt;code&gt;.arg_constraints&lt;/code&gt; の辞書。これらの変換は、回転を回避するためにスペースのパラメーターを過剰に設定することがよくあります。したがって、これらはAdamのような座標ごとの最適化アルゴリズムにより適しています。</target>
        </trans-unit>
        <trans-unit id="1cee34c614aefb2bdef2c172499b9302c8a00b05" translate="yes" xml:space="preserve">
          <source>The API is 100% compatible with the original module - it&amp;rsquo;s enough to change &lt;code&gt;import multiprocessing&lt;/code&gt; to &lt;code&gt;import torch.multiprocessing&lt;/code&gt; to have all the tensors sent through the queues or shared via other mechanisms, moved to shared memory.</source>
          <target state="translated">APIは元のモジュールと100％互換性があります- &lt;code&gt;import multiprocessing&lt;/code&gt; を &lt;code&gt;import torch.multiprocessing&lt;/code&gt; に変更して、すべてのテンソルをキューを介して送信するか、他のメカニズムを介して共有し、共有メモリに移動するだけで十分です。</target>
        </trans-unit>
        <trans-unit id="fc46cba00894331ffa02531912b869b31d53f5a1" translate="yes" xml:space="preserve">
          <source>The Connectionist Temporal Classification loss.</source>
          <target state="translated">コネクショニストの時間的分類の損失</target>
        </trans-unit>
        <trans-unit id="68d2c2ff975df64fa09115de4df3b92bf60905da" translate="yes" xml:space="preserve">
          <source>The FFT of a real signal is Hermitian-symmetric, &lt;code&gt;X[i] = conj(X[-i])&lt;/code&gt; so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">実際の信号のFFTはエルミート対称であり、 &lt;code&gt;X[i] = conj(X[-i])&lt;/code&gt; ため、出力にはナイキスト周波数より下の正の周波数のみが含まれます。完全な出力を計算するには、&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; を&lt;/a&gt;使用します</target>
        </trans-unit>
        <trans-unit id="554abbeeb98f672613a7ec23a19dc9e48ad28309" translate="yes" xml:space="preserve">
          <source>The FFT of a real signal is Hermitian-symmetric, &lt;code&gt;X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])&lt;/code&gt; so the full &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt; output contains redundant information. &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt; instead omits the negative frequencies in the last dimension.</source>
          <target state="translated">実信号のFFTはエルミート対称、 &lt;code&gt;X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])&lt;/code&gt; ため、完全な&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt;出力には冗長な情報が含まれます。代わりに、&lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;は最後の次元の負の周波数を省略します。</target>
        </trans-unit>
        <trans-unit id="7e13169ea1b51d49a8b92e863c0bf3126ef56123" translate="yes" xml:space="preserve">
          <source>The Fourier domain representation of any real signal satisfies the Hermitian property: &lt;code&gt;X[i] = conj(X[-i])&lt;/code&gt;. This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt; returns the more compact one-sided representation where only the positive frequencies are returned.</source>
          <target state="translated">実際の信号のフーリエ領域表現は、エルミート特性 &lt;code&gt;X[i] = conj(X[-i])&lt;/code&gt; ます。この関数は、実際の入力の場合、負の周波数が冗長であっても、常に正と負の両方の周波数項を返します。&lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;は、正の周波数のみが返される、よりコンパクトな片側表現を返します。</target>
        </trans-unit>
        <trans-unit id="c9fe021ce10d1cbbed575b9284c6f59da576001a" translate="yes" xml:space="preserve">
          <source>The Fourier domain representation of any real signal satisfies the Hermitian property: &lt;code&gt;X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])&lt;/code&gt;. This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt; returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.</source>
          <target state="translated">実際の信号のフーリエ領域表現は、エルミート特性 &lt;code&gt;X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])&lt;/code&gt; 。この関数は、実際の入力の場合、これらの値の半分が冗長であっても、常にすべての正と負の周波数項を返します。&lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;は、最後の次元の正の周波数のみが返される、よりコンパクトな片側表現を返します。</target>
        </trans-unit>
        <trans-unit id="7ad0a191a9fee1e60e241897ad6f1ce3ecbb965d" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler divergence loss measure</source>
          <target state="translated">Kullback-Leibler発散損失尺度</target>
        </trans-unit>
        <trans-unit id="d60b9fe818a7402cdbe31175747d27cd34ed024b" translate="yes" xml:space="preserve">
          <source>The Nesterov version is analogously modified.</source>
          <target state="translated">ネステロフ版は類推的に修正されています。</target>
        </trans-unit>
        <trans-unit id="79342dbd5370ca7ac1b96027ceb5844371ecbc95" translate="yes" xml:space="preserve">
          <source>The ONNX exporter can be both &lt;em&gt;trace-based&lt;/em&gt; and &lt;em&gt;script-based&lt;/em&gt; exporter.</source>
          <target state="translated">ONNXエクスポーターは、&lt;em&gt;トレースベース&lt;/em&gt;と&lt;em&gt;スクリプト&lt;/em&gt;&lt;em&gt;ベースの&lt;/em&gt;両方のエクスポーターにすることができます。</target>
        </trans-unit>
        <trans-unit id="c11a05ce23a6c40119d293ee087abf34796175d7" translate="yes" xml:space="preserve">
          <source>The ONNX graph C++ definition is in &lt;code&gt;torch/csrc/jit/ir/ir.h&lt;/code&gt;.</source>
          <target state="translated">ONNXグラフのC ++定義は、 &lt;code&gt;torch/csrc/jit/ir/ir.h&lt;/code&gt; ます。</target>
        </trans-unit>
        <trans-unit id="0a8c3a370e79a6afd160a058933e30e88f62a863" translate="yes" xml:space="preserve">
          <source>The Process Group Backend will be deprecated soon, we recommend using the TensorPipe Backend instead.</source>
          <target state="translated">プロセスグループバックエンドはまもなく非推奨となりますので、代わりにTensorPipeバックエンドを使用することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="c611b23c7d2afdcd9cdf21ee774e5b2b21bffaa1" translate="yes" xml:space="preserve">
          <source>The Process Group agent instantiates a process group from the &lt;a href=&quot;distributed#module-torch.distributed&quot;&gt;&lt;code&gt;distributed&lt;/code&gt;&lt;/a&gt; module and utilizes its point-to-point communication capabilities to send RPC messages. Internally, the process group uses &lt;a href=&quot;https://github.com/facebookincubator/gloo/&quot;&gt;the Gloo library&lt;/a&gt;.</source>
          <target state="translated">プロセスグループエージェントは、&lt;a href=&quot;distributed#module-torch.distributed&quot;&gt; &lt;code&gt;distributed&lt;/code&gt; &lt;/a&gt;モジュールからプロセスグループをインスタンス化し、そのポイントツーポイント通信機能を利用してRPCメッセージを送信します。内部的には、プロセスグループは&lt;a href=&quot;https://github.com/facebookincubator/gloo/&quot;&gt;Glooライブラリを&lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="a48f9bb3889d63ba5122c625f2532d0293bf1e9f" translate="yes" xml:space="preserve">
          <source>The RPC module can leverage different backends to perform the communication between the nodes. The backend to be used can be specified in the &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; function, by passing a certain value of the &lt;a href=&quot;#torch.distributed.rpc.BackendType&quot;&gt;&lt;code&gt;BackendType&lt;/code&gt;&lt;/a&gt; enum. Regardless of what backend is used, the rest of the RPC API won&amp;rsquo;t change. Each backend also defines its own subclass of the &lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;&lt;code&gt;RpcBackendOptions&lt;/code&gt;&lt;/a&gt; class, an instance of which can also be passed to &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; to configure the backend&amp;rsquo;s behavior.</source>
          <target state="translated">RPCモジュールは、さまざまなバックエンドを活用してノード間の通信を実行できます。使用するバックエンドは、&lt;a href=&quot;#torch.distributed.rpc.BackendType&quot;&gt; &lt;code&gt;BackendType&lt;/code&gt; &lt;/a&gt;列挙型の特定の値を渡すことにより、&lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt;関数で指定できます。使用されているバックエンドに関係なく、RPCAPIの残りの部分は変更されません。各バックエンドは、&lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt; &lt;code&gt;RpcBackendOptions&lt;/code&gt; &lt;/a&gt;クラスの独自のサブクラスも定義します。このサブクラスのインスタンスを&lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt;に渡して、バックエンドの動作を構成することもできます。</target>
        </trans-unit>
        <trans-unit id="14ee5344d06a238323bb82692b3ba5195a694523" translate="yes" xml:space="preserve">
          <source>The RPC package also provides decorators which allow applications to specify how a given function should be treated on the callee side.</source>
          <target state="translated">RPC パッケージは、アプリケーションが着呼側で与えられた関数をどのように扱うかを指定できるようにするデコレータも提供します。</target>
        </trans-unit>
        <trans-unit id="d97e45dd1ab295237092f3a7f655e6e94838339b" translate="yes" xml:space="preserve">
          <source>The RPC tutorials introduce users to the RPC framework, provide several example applications using &lt;a href=&quot;#distributed-rpc-framework&quot;&gt;torch.distributed.rpc&lt;/a&gt; APIs, and demonstrate how to use &lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;the profiler&lt;/a&gt; to profile RPC-based workloads.</source>
          <target state="translated">RPCチュートリアルでは、ユーザーにRPCフレームワークを紹介し、&lt;a href=&quot;#distributed-rpc-framework&quot;&gt;torch.distributed.rpc&lt;/a&gt; APIを使用したいくつかのサンプルアプリケーションを提供し&lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;、プロファイラー&lt;/a&gt;を使用してRPCベースのワークロードをプロファイリングする方法を示します。</target>
        </trans-unit>
        <trans-unit id="1362003f90d57874aad22424c41e2c89849947f1" translate="yes" xml:space="preserve">
          <source>The RRef design note covers the design of the &lt;a href=&quot;#rref&quot;&gt;RRef&lt;/a&gt; (Remote REFerence) protocol used to refer to values on remote workers by the framework.</source>
          <target state="translated">RRefデザインノートは、フレームワークによってリモートワーカーの値を参照するために使用される&lt;a href=&quot;#rref&quot;&gt;RRef&lt;/a&gt;（リモートREFerence）プロトコルの設計をカバーしています。</target>
        </trans-unit>
        <trans-unit id="e195940e5abd6ebbfad00ac76917c59c49c9651e" translate="yes" xml:space="preserve">
          <source>The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time. The interface of this function is modeled after the &lt;a href=&quot;https://librosa.org/doc/latest/generated/librosa.stft.html&quot;&gt;librosa&lt;/a&gt; stft function.</source>
          <target state="translated">STFTは、入力の短いオーバーラップウィンドウのフーリエ変換を計算します。これにより、時間の経過とともに変化する信号の周波数成分が得られます。この関数のインターフェースは、&lt;a href=&quot;https://librosa.org/doc/latest/generated/librosa.stft.html&quot;&gt;librosastft&lt;/a&gt;関数をモデルにしています。</target>
        </trans-unit>
        <trans-unit id="56e279d91a2e6e157edb87095248831fbd79e513" translate="yes" xml:space="preserve">
          <source>The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example:</source>
          <target state="translated">SummaryWriterクラスは、TensorBoardで消費したり可視化したりするためのデータをログに記録するためのメインエントリです。例えば、以下のようになります。</target>
        </trans-unit>
        <trans-unit id="e88b194ff4e6feceb2bfaaa1ec38472db3a49b02" translate="yes" xml:space="preserve">
          <source>The TensorPipe agent, which is the default, leverages &lt;a href=&quot;https://github.com/pytorch/tensorpipe&quot;&gt;the TensorPipe library&lt;/a&gt;, which provides a natively point-to-point communication primitive specifically suited for machine learning that fundamentally addresses some of the limitations of Gloo. Compared to Gloo, it has the advantage of being asynchronous, which allows a large number of transfers to occur simultaneously, each at their own speed, without blocking each other. It will only open pipes between pairs of nodes when needed, on demand, and when one node fails only its incident pipes will be closed, while all other ones will keep working as normal. In addition, it is able to support multiple different transports (TCP, of course, but also shared memory, NVLink, InfiniBand, &amp;hellip;) and can automatically detect their availability and negotiate the best transport to use for each pipe.</source>
          <target state="translated">デフォルトであるTensorPipeエージェントは&lt;a href=&quot;https://github.com/pytorch/tensorpipe&quot;&gt;、TensorPipeライブラリを&lt;/a&gt;活用します、これは、Glooのいくつかの制限に根本的に対処する、機械学習に特に適したネイティブのポイントツーポイント通信プリミティブを提供します。 Glooと比較すると、非同期であるという利点があります。これにより、相互にブロックすることなく、それぞれが独自の速度で多数の転送を同時に実行できます。必要に応じて、必要に応じてノードのペア間のパイプを開くだけで、1つのノードに障害が発生すると、インシデントパイプのみが閉じられ、他のすべてのパイプは通常どおり機能し続けます。さらに、複数の異なるトランスポート（TCPはもちろん、共有メモリ、NVLink、InfiniBandなど）もサポートでき、それらの可用性を自動的に検出して、各パイプに使用する最適なトランスポートをネゴシエートできます。</target>
        </trans-unit>
        <trans-unit id="d362a2db0f7abde4a8479b2b7feada60ead120fd" translate="yes" xml:space="preserve">
          <source>The TensorPipe backend has been introduced in PyTorch v1.6 and is being actively developed. At the moment, it only supports CPU tensors, with GPU support coming soon. It comes with a TCP-based transport, just like Gloo. It is also able to automatically chunk and multiplex large tensors over multiple sockets and threads in order to achieve very high bandwidths. The agent will be able to pick the best transport on its own, with no intervention required.</source>
          <target state="translated">TensorPipeバックエンドはPyTorch v1.6で導入され、活発に開発が進められています。現時点ではCPUテンソルのみをサポートしていますが、GPUのサポートは近日中に予定されています。Glooと同様にTCPベースのトランスポートを搭載しています。また、非常に高い帯域幅を実現するために、大規模なテンソルを複数のソケットやスレッドで自動的にチャンクして多重化することができます。エージェントは、介入を必要とせず、最適なトランスポートを独自に選択することができるようになります。</target>
        </trans-unit>
        <trans-unit id="fe34bfd2269a025104eb8b4a08ae444d0c47f764" translate="yes" xml:space="preserve">
          <source>The TorchScript compiler needs to know the types of &lt;code&gt;module attributes&lt;/code&gt;. Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">TorchScriptコンパイラは、 &lt;code&gt;module attributes&lt;/code&gt; タイプを知る必要があります。ほとんどのタイプは、メンバーの値から推測できます。空のリストとdictは、タイプを推測することはできず、PEP526&lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;スタイルの&lt;/a&gt;クラスアノテーションでタイプにアノテーションを付ける必要があります。タイプを推測できず、明示的に注釈が付けられていない場合、結果の&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; に&lt;/a&gt;属性として追加されません。</target>
        </trans-unit>
        <trans-unit id="108ed42a68e399da0a184bd6d4468cb53c249801" translate="yes" xml:space="preserve">
          <source>The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with &lt;code&gt;requires_grad&lt;/code&gt; set to &lt;code&gt;True&lt;/code&gt;. Below please find a quick guide on what has changed:</source>
          <target state="translated">変数APIは非推奨になりました：テンソルでautogradを使用するために変数は不要になりました。Autogradは、 &lt;code&gt;requires_grad&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されたテンソルを自動的にサポートします。以下に、変更点に関するクイックガイドを示します。</target>
        </trans-unit>
        <trans-unit id="d526d524819b552e08647a3450928e08c0be85f0" translate="yes" xml:space="preserve">
          <source>The accuracies of the pre-trained models evaluated on COCO val2017 are as follows</source>
          <target state="translated">COCO val2017で評価した事前学習モデルの精度は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="fd6ae7d26b925d9ddede1463aabdca8219c04ca1" translate="yes" xml:space="preserve">
          <source>The algorithm used for interpolation is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="translated">補間に使用されるアルゴリズムは、 &lt;code&gt;mode&lt;/code&gt; によって決定されます。</target>
        </trans-unit>
        <trans-unit id="50402614be52e24dbcb07aaaf1f84990228308aa" translate="yes" xml:space="preserve">
          <source>The algorithm used for upsampling is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="translated">アップサンプリングに使用されるアルゴリズムは、 &lt;code&gt;mode&lt;/code&gt; によって決定されます。</target>
        </trans-unit>
        <trans-unit id="9dbbacc61ab643f749eabb1318de7d252379ec8c" translate="yes" xml:space="preserve">
          <source>The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.</source>
          <target state="translated">アップサンプリングに利用可能なアルゴリズムは、3D、4D、5Dの入力テンソルに対して、それぞれ最近傍探索、線形、バイリニア、バイリニア、バイビック、トリリニアである。</target>
        </trans-unit>
        <trans-unit id="bdd6673496c6fd17d43bfb63694d67b1a23e7074" translate="yes" xml:space="preserve">
          <source>The approximate decimal resolution of this type, i.e., &lt;code&gt;10**-precision&lt;/code&gt;.</source>
          <target state="translated">このタイプのおおよその10進分解能、つまり &lt;code&gt;10**-precision&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="11e64a47029713e51445813986a3824ac2548079" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;引数は、どの対角を考慮するかを制御します。&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;= 0の場合、主対角線以上のすべての要素が保持されます。正の値は主対角線より上の対角線と同じ数を除外し、同様に負の値は主対角線より下の対角線と同じ数を含みます。主対角線はインデックスのセットです</target>
        </trans-unit>
        <trans-unit id="2720b921eb619ce89ed4f91beabc1047581eca82" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;引数は、どの対角を考慮するかを制御します。&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;= 0の場合、主対角線の上下のすべての要素が保持されます。正の値には主対角線より上の対角線の数が含まれ、同様に負の値には主対角線より下の対角線の数が含まれます。主対角線はインデックスのセットです</target>
        </trans-unit>
        <trans-unit id="eff44d81484d8a3edd9c1b438e09ac7f19f85991" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider:</source>
          <target state="translated">&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;引数は、どの対角を考慮するかを制御します。</target>
        </trans-unit>
        <trans-unit id="5cfcdff9c158bfd56a616516f4454be040717f37" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">引数 &lt;code&gt;offset&lt;/code&gt; は、考慮する対角線を制御します。 &lt;code&gt;offset&lt;/code&gt; = 0の場合、主対角線以上のすべての要素が保持されます。正の値は主対角線より上の対角線と同じ数を除外し、同様に負の値は主対角線より下の対角線と同じ数を含みます。主対角線はインデックスのセットです</target>
        </trans-unit>
        <trans-unit id="d114863c5d0c0b07094ca0c20e0cd672c5d3f900" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">引数 &lt;code&gt;offset&lt;/code&gt; は、考慮する対角線を制御します。 &lt;code&gt;offset&lt;/code&gt; = 0の場合、主対角線の上下のすべての要素が保持されます。正の値には主対角線より上の対角線の数が含まれ、同様に負の値には主対角線より下の対角線の数が含まれます。主対角線はインデックスのセットです</target>
        </trans-unit>
        <trans-unit id="19b889e5a4344dcdd3129928b40b0f46befb2501" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider:</source>
          <target state="translated">引数 &lt;code&gt;offset&lt;/code&gt; は、考慮する対角線を制御します。</target>
        </trans-unit>
        <trans-unit id="54707c812db6f29dfe9835066cfce0408bd514d9" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;. However, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this instead returns the results multiplied by</source>
          <target state="translated">引数の仕様は&lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;とほぼ同じです。ただし、 &lt;code&gt;normalized&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、これは代わりに結果に乗算された結果を返します。</target>
        </trans-unit>
        <trans-unit id="3bf649a8dfc260fcc9e9737948bc65802a397444" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;. Similar to &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by multiplying it with</source>
          <target state="translated">引数の仕様は&lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;とほぼ同じです。&lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;と同様に、 &lt;code&gt;normalized&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、これは結果に乗算することによって結果を正規化します。</target>
        </trans-unit>
        <trans-unit id="dcab133c96e453facb95db0b7ee0dc8c72d699d6" translate="yes" xml:space="preserve">
          <source>The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects &lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;torch.nn.DataParallel&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt; when used with more than one GPU per process (see &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-multigpu&quot;&gt;Working with Multiple GPUs&lt;/a&gt;).</source>
          <target state="translated">自動キャストの状態はスレッドローカルです。新しいスレッドで有効にする場合は、そのスレッドでコンテキストマネージャーまたはデコレーターを呼び出す必要があります。これは、プロセスごとに&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-multigpu&quot;&gt;複数のGPUで&lt;/a&gt;使用される場合、&lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt; &lt;code&gt;torch.nn.DataParallel&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt; &lt;/a&gt;に影響します（複数のGPUの操作を参照）。</target>
        </trans-unit>
        <trans-unit id="cf4c102338f979040fb6ff061754881df7061681" translate="yes" xml:space="preserve">
          <source>The backend of the given process group as a lower case string.</source>
          <target state="translated">指定されたプロセスグループのバックエンドを小文字の文字列で表します。</target>
        </trans-unit>
        <trans-unit id="e19263dbfb8f739b55183f8eb835a3eac6ee2887" translate="yes" xml:space="preserve">
          <source>The backend options class for &lt;code&gt;ProcessGroupAgent&lt;/code&gt;, which is derived from &lt;code&gt;RpcBackendOptions&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;ProcessGroupAgent&lt;/code&gt; から派生した &lt;code&gt;RpcBackendOptions&lt;/code&gt; のバックエンドオプションクラス。</target>
        </trans-unit>
        <trans-unit id="ced0f1f27d8dec6f68219ca478db12401a16eb9d" translate="yes" xml:space="preserve">
          <source>The backend options for &lt;code&gt;TensorPipeAgent&lt;/code&gt;, derived from &lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;&lt;code&gt;RpcBackendOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;TensorPipeAgent&lt;/code&gt; から派生した&lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt; &lt;code&gt;RpcBackendOptions&lt;/code&gt; &lt;/a&gt;のバックエンドオプション。</target>
        </trans-unit>
        <trans-unit id="502ce5e4564798f5189d3ade0d0a77ccd7a4b686" translate="yes" xml:space="preserve">
          <source>The backward method does not support sparse and complex inputs. It works only when &lt;code&gt;B&lt;/code&gt; is not provided (i.e. &lt;code&gt;B == None&lt;/code&gt;). We are actively working on extensions, and the details of the algorithms are going to be published promptly.</source>
          <target state="translated">後方メソッドは、スパースおよび複雑な入力をサポートしていません。それは場合にのみ機能 &lt;code&gt;B&lt;/code&gt; が（つまり設けられていないされていない &lt;code&gt;B == None&lt;/code&gt; ）。私たちは積極的に拡張に取り組んでおり、アルゴリズムの詳細はすぐに公開される予定です。</target>
        </trans-unit>
        <trans-unit id="55caae9ff73829418d2cf1f4b542673bb0f801f6" translate="yes" xml:space="preserve">
          <source>The backward passes of &lt;a href=&quot;nn.functional#torch.nn.functional.binary_cross_entropy&quot;&gt;&lt;code&gt;torch.nn.functional.binary_cross_entropy()&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/torch.nn.bceloss#torch.nn.BCELoss&quot;&gt;&lt;code&gt;torch.nn.BCELoss&lt;/code&gt;&lt;/a&gt;, which wraps it) can produce gradients that aren&amp;rsquo;t representable in &lt;code&gt;float16&lt;/code&gt;. In autocast-enabled regions, the forward input may be &lt;code&gt;float16&lt;/code&gt;, which means the backward gradient must be representable in &lt;code&gt;float16&lt;/code&gt; (autocasting &lt;code&gt;float16&lt;/code&gt; forward inputs to &lt;code&gt;float32&lt;/code&gt; doesn&amp;rsquo;t help, because that cast must be reversed in backward). Therefore, &lt;code&gt;binary_cross_entropy&lt;/code&gt; and &lt;code&gt;BCELoss&lt;/code&gt; raise an error in autocast-enabled regions.</source>
          <target state="translated">逆方向のパス&lt;a href=&quot;nn.functional#torch.nn.functional.binary_cross_entropy&quot;&gt; &lt;code&gt;torch.nn.functional.binary_cross_entropy()&lt;/code&gt; &lt;/a&gt;（および&lt;a href=&quot;generated/torch.nn.bceloss#torch.nn.BCELoss&quot;&gt; &lt;code&gt;torch.nn.BCELoss&lt;/code&gt; &lt;/a&gt;それをラップ、）で表現できない勾配生成することができる &lt;code&gt;float16&lt;/code&gt; を。自動キャストが有効な領域では、順方向入力は &lt;code&gt;float16&lt;/code&gt; である可能性があります。つまり、逆方向の勾配は &lt;code&gt;float16&lt;/code&gt; で表現可能である必要があります（ &lt;code&gt;float16&lt;/code&gt; の順方向入力を &lt;code&gt;float32&lt;/code&gt; に自動キャストしても、キャストを逆方向に逆にする必要があるため、役に立ちません）。したがって、 &lt;code&gt;binary_cross_entropy&lt;/code&gt; と &lt;code&gt;BCELoss&lt;/code&gt; は、自動キャストが有効な領域でエラーを発生させます。</target>
        </trans-unit>
        <trans-unit id="977219b95a63d49a9a83d568761b5dea94f71247" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used locally.</source>
          <target state="translated">バッチサイズは、ローカルで使用するGPUの数よりも大きくする必要があります。</target>
        </trans-unit>
        <trans-unit id="23b97210326ef320e2d1633f1e69b26cc12a15dc" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used.</source>
          <target state="translated">バッチサイズは、使用するGPUの数よりも大きくする必要があります。</target>
        </trans-unit>
        <trans-unit id="1822f0d582e28f7680fb7e40c87465297eaceb93" translate="yes" xml:space="preserve">
          <source>The behavior depends on the dimensionality of the tensors as follows:</source>
          <target state="translated">挙動は以下のようにテンソルの次元性に依存します。</target>
        </trans-unit>
        <trans-unit id="2f9d3a99640c5733b646d772d2660cd0aaa3e59f" translate="yes" xml:space="preserve">
          <source>The behavior of the model changes depending if it is in training or evaluation mode.</source>
          <target state="translated">トレーニングモードなのか評価モードなのかで、モデルの挙動が変化します。</target>
        </trans-unit>
        <trans-unit id="671bda2b5e08e92d9c6a9bee65efdef513a603fd" translate="yes" xml:space="preserve">
          <source>The boolean argument &lt;code&gt;eigenvectors&lt;/code&gt; defines computation of both eigenvectors and eigenvalues or eigenvalues only.</source>
          <target state="translated">ブール引数 &lt;code&gt;eigenvectors&lt;/code&gt; は、固有ベクトルと固有値または固有値のみの両方の計算を定義します。</target>
        </trans-unit>
        <trans-unit id="a803e781482443f16488bb7ddd92d4c8599d93d8" translate="yes" xml:space="preserve">
          <source>The boolean option &lt;code&gt;sorted&lt;/code&gt; if &lt;code&gt;True&lt;/code&gt;, will make sure that the returned &lt;code&gt;k&lt;/code&gt; elements are themselves sorted</source>
          <target state="translated">ブールオプション &lt;code&gt;sorted&lt;/code&gt; あれば &lt;code&gt;True&lt;/code&gt; が、返されたことを確認します &lt;code&gt;k&lt;/code&gt; 個の要素自体がソートされています</target>
        </trans-unit>
        <trans-unit id="7230c4dcc10bf6edcaa500d739b7ece219bab321" translate="yes" xml:space="preserve">
          <source>The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.</source>
          <target state="translated">キャッシングアロケータは、テンソルが割り当てられたストリームのみを認識しています。この認識により、すでに1つのストリームのみでテンソルのライフサイクルを正しく管理しています。しかし、テンソルが割り当て元のストリームとは異なるストリームで使用された場合、アロケータは予期せずメモリを再利用してしまう可能性があります。このメソッドを呼び出すことで、アロケータはどのストリームでテンソルが使用されたかを知ることができます。</target>
        </trans-unit>
        <trans-unit id="750a4192b0cf8d28e81899b152edaaa9a146f0c9" translate="yes" xml:space="preserve">
          <source>The case when</source>
          <target state="translated">となった場合のケース</target>
        </trans-unit>
        <trans-unit id="40367078c772777ce16b0a9ed2c98fb39bfc9303" translate="yes" xml:space="preserve">
          <source>The centered version first appears in &lt;a href=&quot;https://arxiv.org/pdf/1308.0850v5.pdf&quot;&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt;.</source>
          <target state="translated">中央に配置されたバージョンは、&lt;a href=&quot;https://arxiv.org/pdf/1308.0850v5.pdf&quot;&gt;リカレントニューラルネットワークを使用したシーケンスの生成で&lt;/a&gt;最初に表示されます。</target>
        </trans-unit>
        <trans-unit id="cae8914c78d70dfbfebc7267cb87ec8d54626689" translate="yes" xml:space="preserve">
          <source>The check between numerical and analytical gradients uses &lt;a href=&quot;generated/torch.allclose#torch.allclose&quot;&gt;&lt;code&gt;allclose()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">数値勾配と分析勾配の間のチェックでは、&lt;a href=&quot;generated/torch.allclose#torch.allclose&quot;&gt; &lt;code&gt;allclose()&lt;/code&gt; を&lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="3dfe4da87960d20e13c57512ebae8f18f5cb935c" translate="yes" xml:space="preserve">
          <source>The checkpoint can be later loaded and inspected under &lt;code&gt;chrome://tracing&lt;/code&gt; URL.</source>
          <target state="translated">チェックポイントは、後で &lt;code&gt;chrome://tracing&lt;/code&gt; で読み込んで検査できます。</target>
        </trans-unit>
        <trans-unit id="1a15a1c2561662d55e6adec5b59058d1e70d3911" translate="yes" xml:space="preserve">
          <source>The columns of the output matrix are elementwise powers of the input vector</source>
          <target state="translated">出力行列の列は,入力ベクトルの要素毎の累乗である</target>
        </trans-unit>
        <trans-unit id="21a375cae9200e37dd0602ab95738f8e9f8a4429" translate="yes" xml:space="preserve">
          <source>The computation for determinant and inverse of covariance matrix is avoided when &lt;code&gt;cov_factor.shape[1] &amp;lt;&amp;lt; cov_factor.shape[0]&lt;/code&gt; thanks to &lt;a href=&quot;https://en.wikipedia.org/wiki/Woodbury_matrix_identity&quot;&gt;Woodbury matrix identity&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_determinant_lemma&quot;&gt;matrix determinant lemma&lt;/a&gt;. Thanks to these formulas, we just need to compute the determinant and inverse of the small size &amp;ldquo;capacitance&amp;rdquo; matrix:</source>
          <target state="translated">&lt;code&gt;cov_factor.shape[1] &amp;lt;&amp;lt; cov_factor.shape[0]&lt;/code&gt; 、&lt;a href=&quot;https://en.wikipedia.org/wiki/Woodbury_matrix_identity&quot;&gt;Woodbury行列の同一性&lt;/a&gt;と&lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_determinant_lemma&quot;&gt;行列式の行列式の補題の&lt;/a&gt;おかげで、行列式と逆行列式の計算は回避されます。これらの式のおかげで、小さなサイズの「静電容量」行列の行列式と逆行列を計算する必要があります。</target>
        </trans-unit>
        <trans-unit id="ce6592c2eac0498a5cc7f983f7cee34a07c28d68" translate="yes" xml:space="preserve">
          <source>The constructor of &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; can be called without argument, in which case the class is created for the pytorch default dtype (as returned by &lt;a href=&quot;generated/torch.get_default_dtype#torch.get_default_dtype&quot;&gt;&lt;code&gt;torch.get_default_dtype()&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.finfo&quot;&gt; &lt;code&gt;torch.finfo&lt;/code&gt; &lt;/a&gt;のコンストラクターは、引数なしで呼び出すことができます。その場合、クラスはpytorchのデフォルトのdtype（&lt;a href=&quot;generated/torch.get_default_dtype#torch.get_default_dtype&quot;&gt; &lt;code&gt;torch.get_default_dtype()&lt;/code&gt; &lt;/a&gt;によって返される）用に作成されます。</target>
        </trans-unit>
        <trans-unit id="c4ae76d927f18889b6dd2cecc1dcf754fe0ab549" translate="yes" xml:space="preserve">
          <source>The contents of a tensor can be accessed and modified using Python&amp;rsquo;s indexing and slicing notation:</source>
          <target state="translated">テンソルの内容は、Pythonのインデックス作成とスライス表記を使用してアクセスおよび変更できます。</target>
        </trans-unit>
        <trans-unit id="c51b27c3ede61443058e137258f2eb8e4fb6bb72" translate="yes" xml:space="preserve">
          <source>The context can be used to retrieve tensors saved during the forward pass. It also has an attribute &lt;code&gt;ctx.needs_input_grad&lt;/code&gt; as a tuple of booleans representing whether each input needs gradient. E.g., &lt;a href=&quot;#torch.autograd.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; will have &lt;code&gt;ctx.needs_input_grad[0] = True&lt;/code&gt; if the first input to &lt;a href=&quot;#torch.autograd.Function.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; needs gradient computated w.r.t. the output.</source>
          <target state="translated">コンテキストを使用して、フォワードパス中に保存されたテンソルを取得できます。また、各入力にグラデーションが必要かどうかを表すブール値のタプルとして、属性 &lt;code&gt;ctx.needs_input_grad&lt;/code&gt; があります。たとえば、&lt;a href=&quot;#torch.autograd.Function.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;への最初の入力が出力に対して勾配計算を必要とする場合、&lt;a href=&quot;#torch.autograd.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt;は &lt;code&gt;ctx.needs_input_grad[0] = True&lt;/code&gt; なります。</target>
        </trans-unit>
        <trans-unit id="d18c64aa54a70cac0b6ada6c6ee8aec0ef1f0917" translate="yes" xml:space="preserve">
          <source>The context can be used to store tensors that can be then retrieved during the backward pass.</source>
          <target state="translated">コンテキストはテンソルを格納するために使用され、そのテンソルは後方パスの間に取得されます。</target>
        </trans-unit>
        <trans-unit id="68f6bd6745cd407cbd762333af1947bd80a473cd" translate="yes" xml:space="preserve">
          <source>The context managers &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.enable_grad#torch.enable_grad&quot;&gt;&lt;code&gt;torch.enable_grad()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt;&lt;code&gt;torch.set_grad_enabled()&lt;/code&gt;&lt;/a&gt; are helpful for locally disabling and enabling gradient computation. See &lt;a href=&quot;autograd#locally-disable-grad&quot;&gt;Locally disabling gradient computation&lt;/a&gt; for more details on their usage. These context managers are thread local, so they won&amp;rsquo;t work if you send work to another thread using the &lt;code&gt;threading&lt;/code&gt; module, etc.</source>
          <target state="translated">コンテキストマネージャー&lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt; &lt;code&gt;torch.no_grad()&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/torch.enable_grad#torch.enable_grad&quot;&gt; &lt;code&gt;torch.enable_grad()&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;generated/torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt; &lt;code&gt;torch.set_grad_enabled()&lt;/code&gt; &lt;/a&gt;は、勾配計算をローカルで無効化および有効化するのに役立ちます。使用法の詳細については、&lt;a href=&quot;autograd#locally-disable-grad&quot;&gt;勾配計算&lt;/a&gt;をローカルで無効にするを参照してください。これらのコンテキストマネージャーはスレッドローカルであるため、 &lt;code&gt;threading&lt;/code&gt; モジュールなどを使用して別のスレッドに作業を送信すると機能しません。</target>
        </trans-unit>
        <trans-unit id="6ca40d5babc478c27564467be749f0f09c316678" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;n&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length &lt;code&gt;n&lt;/code&gt;.</source>
          <target state="translated">エルミート入力の正しい解釈は、 &lt;code&gt;n&lt;/code&gt; で与えられる元のデータの長さに依存します。これは、各入力形状が奇数または偶数の長さの信号に対応する可能性があるためです。デフォルトでは、信号は偶数の長さであると想定され、奇数の信号は適切にラウンドトリップしません。したがって、常に信号長 &lt;code&gt;n&lt;/code&gt; を渡すことをお勧めします。</target>
        </trans-unit>
        <trans-unit id="15f3e882aab04b4ad9da40b79636691c5dd75e80" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;s&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape &lt;code&gt;s&lt;/code&gt;.</source>
          <target state="translated">エルミート入力の正しい解釈は、 &lt;code&gt;s&lt;/code&gt; で与えられるように、元のデータの長さに依存します。これは、各入力形状が奇数または偶数の長さの信号に対応する可能性があるためです。デフォルトでは、信号は偶数の長さであると想定され、奇数の信号は適切にラウンドトリップしません。したがって、常に信号形状 &lt;code&gt;s&lt;/code&gt; を渡すことをお勧めします。</target>
        </trans-unit>
        <trans-unit id="6651537fc75b50914667cdbdb45bc29550b4b400" translate="yes" xml:space="preserve">
          <source>The corresponding quantized module of &lt;code&gt;mod&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;mod&lt;/code&gt; の対応する量子化モジュール</target>
        </trans-unit>
        <trans-unit id="b2f2b30024847cce229c4bda1e1b5ab964a9eaf4" translate="yes" xml:space="preserve">
          <source>The criterion only considers a contiguous block of non-negative targets that starts at the front.</source>
          <target state="translated">この基準では、前面から始まる非負のターゲットの連続ブロックのみを考慮します。</target>
        </trans-unit>
        <trans-unit id="e11b9195cb3521aafd237a69ce124b8d788f6886" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="translated">現在の実装では、多くの操作を実行する複雑な&lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;に対して提示された動作はありません。一部の障害の場合、 &lt;code&gt;grad_input&lt;/code&gt; と &lt;code&gt;grad_output&lt;/code&gt; には、入力と出力のサブセットのグラデーションのみが含まれます。このような&lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;場合、特定の入力または出力で直接&lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt; &lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt; &lt;/a&gt;を使用して、必要なグラデーションを取得する必要があります。</target>
        </trans-unit>
        <trans-unit id="e2bf74abb2f8760663c776314dd933141deb8a40" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="translated">現在の実装では、多くの操作を実行する複雑な&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;に対して提示された動作はありません。一部の障害の場合、 &lt;code&gt;grad_input&lt;/code&gt; と &lt;code&gt;grad_output&lt;/code&gt; には、入力と出力のサブセットのグラデーションのみが含まれます。このような&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;場合、特定の入力または出力で直接&lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt; &lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt; &lt;/a&gt;を使用して、必要なグラデーションを取得する必要があります。</target>
        </trans-unit>
        <trans-unit id="41f3fc6117e6365cbfdd81a5124ee1886a82a466" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;code&gt;Module&lt;/code&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;code&gt;Module&lt;/code&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="translated">現在の実装では、多くの操作を実行する複雑な &lt;code&gt;Module&lt;/code&gt; に対して提示された動作はありません。一部の障害の場合、 &lt;code&gt;grad_input&lt;/code&gt; と &lt;code&gt;grad_output&lt;/code&gt; には、入力と出力のサブセットのグラデーションのみが含まれます。このような &lt;code&gt;Module&lt;/code&gt; 場合、特定の入力または出力で直接&lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt; &lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt; &lt;/a&gt;を使用して、必要なグラデーションを取得する必要があります。</target>
        </trans-unit>
        <trans-unit id="c92e6421025a94114fff69d31efad27a7d187bee" translate="yes" xml:space="preserve">
          <source>The default behavior (letting &lt;code&gt;.grad&lt;/code&gt;s be &lt;code&gt;None&lt;/code&gt; before the first &lt;code&gt;backward()&lt;/code&gt;, such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to &lt;code&gt;model.zero_grad()&lt;/code&gt; or &lt;code&gt;optimizer.zero_grad()&lt;/code&gt; will not affect &lt;code&gt;.grad&lt;/code&gt; layouts.</source>
          <target state="translated">&lt;code&gt;.grad&lt;/code&gt; のパフォーマンスを得るには、デフォルトの動作（最初の &lt;code&gt;backward()&lt;/code&gt; の前に.gradを &lt;code&gt;None&lt;/code&gt; にして、レイアウトが1または2に従って作成され、3または4に従って長期間保持されるようにする）をお勧めします。 &lt;code&gt;model.zero_grad()&lt;/code&gt; または &lt;code&gt;optimizer.zero_grad()&lt;/code&gt; を呼び出しても、 &lt;code&gt;.grad&lt;/code&gt; レイアウトには影響しません。</target>
        </trans-unit>
        <trans-unit id="0fcbf7f37e10aae7ed7b4feee2a1e7bd2fefc4c8" translate="yes" xml:space="preserve">
          <source>The default floating point dtype is initially &lt;code&gt;torch.float32&lt;/code&gt;.</source>
          <target state="translated">デフォルトの浮動小数点dtypeは、最初は &lt;code&gt;torch.float32&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="fed344676665a74468953d310a0df7afafaa24e3" translate="yes" xml:space="preserve">
          <source>The default floating point tensor type is initially &lt;code&gt;torch.FloatTensor&lt;/code&gt;.</source>
          <target state="translated">デフォルトの浮動小数点テンソルタイプは、最初は &lt;code&gt;torch.FloatTensor&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="c712a8331c0e06f199c3cd7a844a65de36cd9e42" translate="yes" xml:space="preserve">
          <source>The default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a &lt;code&gt;collate_fn&lt;/code&gt; that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a &lt;code&gt;pin_memory()&lt;/code&gt; method on your custom type(s).</source>
          <target state="translated">デフォルトのメモリピン留めロジックは、テンソルと、テンソルを含むマップおよびイテラブルのみを認識します。デフォルトでは、ピン留めロジックがカスタムタイプのバッチを検出する場合（カスタムバッチタイプを返す &lt;code&gt;collate_fn&lt;/code&gt; がある場合に発生します）、またはバッチの各要素がカスタムタイプである場合、ピン留めロジックは認識しませんそれら、そしてそれはメモリを固定せずにそのバッチ（またはそれらの要素）を返します。カスタムバッチまたはデータ型のメモリ固定を有効にするには、カスタム型で &lt;code&gt;pin_memory()&lt;/code&gt; メソッドを定義します。</target>
        </trans-unit>
        <trans-unit id="0dec0a713cccbf6f2a025c718d1183b7f17ecc48" translate="yes" xml:space="preserve">
          <source>The default values are designed for &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;grad_outputs&lt;/code&gt; of double precision. This check will likely fail if they are of less precision, e.g., &lt;code&gt;FloatTensor&lt;/code&gt;.</source>
          <target state="translated">デフォルト値は、 &lt;code&gt;grad_outputs&lt;/code&gt; の &lt;code&gt;input&lt;/code&gt; およびgrad_outputs用に設計されています。 &lt;code&gt;FloatTensor&lt;/code&gt; のように精度が低い場合、このチェックは失敗する可能性があります。</target>
        </trans-unit>
        <trans-unit id="7e78814d77915278208123d7230c026ea150426b" translate="yes" xml:space="preserve">
          <source>The default values are designed for &lt;code&gt;input&lt;/code&gt; of double precision. This check will likely fail if &lt;code&gt;input&lt;/code&gt; is of less precision, e.g., &lt;code&gt;FloatTensor&lt;/code&gt;.</source>
          <target state="translated">デフォルト値は、倍精度の &lt;code&gt;input&lt;/code&gt; 用に設計されています。 &lt;code&gt;FloatTensor&lt;/code&gt; のように &lt;code&gt;input&lt;/code&gt; 精度が低い場合、このチェックは失敗する可能性があります。</target>
        </trans-unit>
        <trans-unit id="a80bd362bedb3fcb6747d9fd75b4961eb0cdb1f6" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="translated">離散フーリエ変換は分離可能であるため、ここでの&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt;は、2つの1次元&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;呼び出しと同等です。</target>
        </trans-unit>
        <trans-unit id="68e62e57e835a4d91bc43ca48c95e3efa63535c9" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="translated">離散フーリエ変換は分離可能であるため、ここでの&lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt;は、2つの1次元&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;呼び出しと同等です。</target>
        </trans-unit>
        <trans-unit id="1984f4c44a0a732d169d5d6a5c5f08cad523bc50" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to a combination of &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">離散フーリエ変換分離可能であるので、&lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;ここでは、の組み合わせと等価である&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="6def4e7a63f51a47c9f6b51fb1bb8c54875ce717" translate="yes" xml:space="preserve">
          <source>The distance swap is described in detail in the paper &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;Learning shallow convolutional feature descriptors with triplet losses&lt;/a&gt; by V. Balntas, E. Riba et al.</source>
          <target state="translated">距離スワップは、紙に詳細に記載されている&lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;三重項損失で学習浅い畳み込み特徴記述子&lt;/a&gt;V. Balntas、E. Ribaのらによる。</target>
        </trans-unit>
        <trans-unit id="e0ab87bc03e5a5294329af8c45b84a5da57986c5" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework makes it easy to run functions remotely, supports referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backward and update parameters across RPC boundaries. These features can be categorized into four sets of APIs.</source>
          <target state="translated">分散 RPC フレームワークは、リモートでの関数実行を容易にし、実際のデータをコピーすることなくリモートオブジェクトの参照をサポートし、RPC の境界を越えて透過的に後方に実行してパラメータを更新するための autograd およびオプティマイザ API を提供します。これらの機能は、4つのAPIセットに分類することができます。</target>
        </trans-unit>
        <trans-unit id="58cfec2b510135f93e0701fc204e248929b783f2" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework provides mechanisms for multi-machine model training through a set of primitives to allow for remote communication, and a higher-level API to automatically differentiate models split across several machines.</source>
          <target state="translated">分散型RPCフレームワークは、リモート通信を可能にする一連のプリミティブと、複数のマシンに分割されたモデルを自動的に区別するための高レベルAPIを介して、マルチマシンモデルのトレーニングのためのメカニズムを提供します。</target>
        </trans-unit>
        <trans-unit id="10060f95e0764042598d146fc06132ab3dfae090" translate="yes" xml:space="preserve">
          <source>The distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.</source>
          <target state="translated">分散オートグラッドのデザインノートでは、モデル並列訓練などのアプリケーションに有用なRPCベースの分散オートグラッドフレームワークのデザインを取り上げています。</target>
        </trans-unit>
        <trans-unit id="dafbad2c73a3de536f4d90e5d5fa2fe3c91b9950" translate="yes" xml:space="preserve">
          <source>The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed pacakge in &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; (by explicitly creating the store as an alternative to specifying &lt;code&gt;init_method&lt;/code&gt;.) There are 3 choices for Key-Value Stores: &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">分散パッケージには分散Key-Valueストアが付属しており、グループ内のプロセス間で情報を共有したり、&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; で&lt;/a&gt;分散パッケージを初期化したりできます（代わりにストアを明示的に作成することにより） &lt;code&gt;init_method&lt;/code&gt; を指定します。）Key-Valueストアには、&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; の&lt;/a&gt;3つの選択肢があります。</target>
        </trans-unit>
        <trans-unit id="0856f15ea144836b8b8d8118fc0a896821ee6c9d" translate="yes" xml:space="preserve">
          <source>The distribution is supported in [0, 1] and parameterized by &amp;lsquo;probs&amp;rsquo; (in (0,1)) or &amp;lsquo;logits&amp;rsquo; (real-valued). Note that, unlike the Bernoulli, &amp;lsquo;probs&amp;rsquo; does not correspond to a probability and &amp;lsquo;logits&amp;rsquo; does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.</source>
          <target state="translated">分布は[0、1]でサポートされ、「probs」（（0,1）内）または「logits」（実数値）によってパラメーター化されます。ベルヌーイとは異なり、「probs」は確率に対応せず、「logits」は対数オッズに対応しませんが、ベルヌーイとの類似性のために同じ名前が使用されていることに注意してください。詳細については、[1]を参照してください。</target>
        </trans-unit>
        <trans-unit id="6db33b80979f7c8f4114d1e3c176aabac7ef279b" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">被除数と除数には、整数と浮動小数点数の両方を含めることができます。余りは、配当 &lt;code&gt;input&lt;/code&gt; と同じ符号を持ちます。</target>
        </trans-unit>
        <trans-unit id="77ad1382d4823bfe1261c0964c88050a7ad3d117" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the divisor &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">被除数と除数には、整数と浮動小数点数の両方を含めることができます。余りは除数 &lt;code&gt;other&lt;/code&gt; と同じ符号を持ちます。</target>
        </trans-unit>
        <trans-unit id="3c8c32198d8c41dc6ec53302edb97fbbfa15ce0d" translate="yes" xml:space="preserve">
          <source>The division by</source>
          <target state="translated">による分割</target>
        </trans-unit>
        <trans-unit id="d4d19d774aea3434987d4b4cece2b5f99cec9991" translate="yes" xml:space="preserve">
          <source>The dlpack shares the tensors memory. Note that each dlpack can only be consumed once.</source>
          <target state="translated">dlpackはテンソルメモリを共有します。各dlpackは一度しか消費できないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="a37e21cd1a669630ae636a9ca39104d114477d0e" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic cosine is &lt;code&gt;[1, inf)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for &lt;code&gt;+ INF&lt;/code&gt; for which the output is mapped to &lt;code&gt;+ INF&lt;/code&gt;.</source>
          <target state="translated">逆双曲線余弦のドメインである &lt;code&gt;[1, inf)&lt;/code&gt; 及びこの範囲外の値にマッピングする &lt;code&gt;NaN&lt;/code&gt; を除いて、 &lt;code&gt;+ INF&lt;/code&gt; 出力にマッピングされている &lt;code&gt;+ INF&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b9eae4d9c836ec727b1fd15ef73922995c7e82e1" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic tangent is &lt;code&gt;(-1, 1)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for the values &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;-1&lt;/code&gt; for which the output is mapped to &lt;code&gt;+/-INF&lt;/code&gt; respectively.</source>
          <target state="translated">逆双曲線正接の定義域は &lt;code&gt;(-1, 1)&lt;/code&gt; あり、この範囲外の値は &lt;code&gt;NaN&lt;/code&gt; にマップされます。ただし、出力がそれぞれ &lt;code&gt;+/-INF&lt;/code&gt; にマップされる値 &lt;code&gt;1&lt;/code&gt; と &lt;code&gt;-1&lt;/code&gt; は除きます。</target>
        </trans-unit>
        <trans-unit id="e24f7a4d115e62a42f56d551f1451d37a5e25266" translate="yes" xml:space="preserve">
          <source>The dynamic control flow is captured correctly. We can verify in backends with different loop range.</source>
          <target state="translated">動的制御の流れを正しく捉えています。ループ範囲の異なるバックエンドでの検証が可能です。</target>
        </trans-unit>
        <trans-unit id="a418864112d90d162f2d2d68264a78b5661b0c49" translate="yes" xml:space="preserve">
          <source>The eigenvalues are returned in ascending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order.</source>
          <target state="translated">固有値は昇順で返されます。場合 &lt;code&gt;input&lt;/code&gt; 行列のバッチは、次いで、バッチ内の各行列の固有値を昇順に戻されます。</target>
        </trans-unit>
        <trans-unit id="70c72529d7b55dc5ab2b127843a9a42c7179e611" translate="yes" xml:space="preserve">
          <source>The elements are sorted into equal width bins between &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;. If &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt; are both zero, the minimum and maximum values of the data are used.</source>
          <target state="translated">要素は、&lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt;間の等しい幅のビンにソートされます。場合&lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt;及び&lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; は&lt;/a&gt;両方ともゼロであり、データの最小値と最大値が使用されます。</target>
        </trans-unit>
        <trans-unit id="ef67a3a1e1ceed752fed1303fe693fb7bb4731f9" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;Backend.UNDEFINED&lt;/code&gt; is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.</source>
          <target state="translated">エントリ &lt;code&gt;Backend.UNDEFINED&lt;/code&gt; は存在しますが、一部のフィールドの初期値としてのみ使用されます。ユーザーはそれを直接使用したり、その存在を想定したりしないでください。</target>
        </trans-unit>
        <trans-unit id="c0ed2b956d8f2913276ea868cc2cc1c195e28811" translate="yes" xml:space="preserve">
          <source>The example script above produces the graph:</source>
          <target state="translated">上のスクリプト例では、グラフを生成しています。</target>
        </trans-unit>
        <trans-unit id="8850a69d0e77cfdf6128ac9dcc486c371b38e541" translate="yes" xml:space="preserve">
          <source>The export fails because PyTorch does not support exporting &lt;code&gt;elu&lt;/code&gt; operator. We find &lt;code&gt;virtual Tensor elu(const Tensor &amp;amp; input, Scalar alpha, bool inplace) const override;&lt;/code&gt; in &lt;code&gt;VariableType.h&lt;/code&gt;. This means &lt;code&gt;elu&lt;/code&gt; is an ATen operator. We check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;, and confirm that &lt;code&gt;Elu&lt;/code&gt; is standardized in ONNX. We add the following lines to &lt;code&gt;symbolic_opset9.py&lt;/code&gt;:</source>
          <target state="translated">PyTorchはエクスポートをサポートしていないため、エクスポートは失敗し &lt;code&gt;elu&lt;/code&gt; 演算子を。 &lt;code&gt;virtual Tensor elu(const Tensor &amp;amp; input, Scalar alpha, bool inplace) const override;&lt;/code&gt; を見つけます。で &lt;code&gt;VariableType.h&lt;/code&gt; 。これは、 &lt;code&gt;elu&lt;/code&gt; ATEN演算子です。&lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNXオペレーターリスト&lt;/a&gt;をチェックし、 &lt;code&gt;Elu&lt;/code&gt; がONNXで標準化されていることを確認します。次の行を &lt;code&gt;symbolic_opset9.py&lt;/code&gt; に追加します。</target>
        </trans-unit>
        <trans-unit id="eaaaa8926b11cbb1c679608a1678277381f47dca" translate="yes" xml:space="preserve">
          <source>The fact that gradients need to be computed for a Tensor do not mean that the &lt;a href=&quot;#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; attribute will be populated, see &lt;a href=&quot;#torch.Tensor.is_leaf&quot;&gt;&lt;code&gt;is_leaf&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">テンソルの勾配を計算する必要があるという事実は、&lt;a href=&quot;#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt;属性が入力されることを意味するわけではありません。詳細については、&lt;a href=&quot;#torch.Tensor.is_leaf&quot;&gt; &lt;code&gt;is_leaf&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="f9a2121d113f583a300c4eeb7ced28dd018af26f" translate="yes" xml:space="preserve">
          <source>The fact that gradients need to be computed for a Tensor do not mean that the &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; attribute will be populated, see &lt;a href=&quot;autograd#torch.Tensor.is_leaf&quot;&gt;&lt;code&gt;is_leaf&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">テンソルの勾配を計算する必要があるという事実は、&lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt;属性が入力されることを意味するわけではありません。詳細については、&lt;a href=&quot;autograd#torch.Tensor.is_leaf&quot;&gt; &lt;code&gt;is_leaf&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="221c8b91c0e460724cf31fb2005c0c9e54071204" translate="yes" xml:space="preserve">
          <source>The first call to add for a given &lt;code&gt;key&lt;/code&gt; creates a counter associated with &lt;code&gt;key&lt;/code&gt; in the store, initialized to &lt;code&gt;amount&lt;/code&gt;. Subsequent calls to add with the same &lt;code&gt;key&lt;/code&gt; increment the counter by the specified &lt;code&gt;amount&lt;/code&gt;. Calling &lt;code&gt;add()&lt;/code&gt; with a key that has already been set in the store by &lt;code&gt;set()&lt;/code&gt; will result in an exception.</source>
          <target state="translated">特定の &lt;code&gt;key&lt;/code&gt; を追加する最初の呼び出しにより、ストア内の &lt;code&gt;key&lt;/code&gt; に関連付けられたカウンターが作成され、 &lt;code&gt;amount&lt;/code&gt; に初期化されます。同じ &lt;code&gt;key&lt;/code&gt; 追加する後続の呼び出しは、指定された &lt;code&gt;amount&lt;/code&gt; カウンターをインクリメントします。 &lt;code&gt;set()&lt;/code&gt; によってストアにすでに設定されているキーを使用して &lt;code&gt;add()&lt;/code&gt; を呼び出すと、例外が発生します。</target>
        </trans-unit>
        <trans-unit id="d04a7a0c088c359fab554cd1a6049882b564ff2e" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph.</source>
          <target state="translated">最初のパラメータは常にエクスポートされたONNXグラフです。</target>
        </trans-unit>
        <trans-unit id="26a3da676366966fcc7fbfc997f2f64e6ee31c9d" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in &lt;code&gt;VariableType.h&lt;/code&gt;, because dispatch is done with keyword arguments.</source>
          <target state="translated">最初のパラメータは常にエクスポートされたONNXグラフです。ディスパッチはキーワード引数を使用して行われるため、パラメータ名は &lt;code&gt;VariableType.h&lt;/code&gt; の名前と完全に一致する必要があります。</target>
        </trans-unit>
        <trans-unit id="5964988cdbc48443556dc471facbf25fa24c6e63" translate="yes" xml:space="preserve">
          <source>The following APIs allow users to remotely execute functions as well as create references (RRefs) to remote data objects. In these APIs, when passing a &lt;code&gt;Tensor&lt;/code&gt; as an argument or a return value, the destination worker will try to create a &lt;code&gt;Tensor&lt;/code&gt; with the same meta (i.e., shape, stride, etc.). We intentionally disallow transmitting CUDA tensors because it might crash if the device lists on source and destination workers do not match. In such cases, applications can always explicitly move the input tensors to CPU on the caller and move it to the desired devices on the callee if necessary.</source>
          <target state="translated">次のAPIを使用すると、ユーザーは関数をリモートで実行したり、リモートデータオブジェクトへの参照（RRef）を作成したりできます。これらのAPIでは、 &lt;code&gt;Tensor&lt;/code&gt; を引数または戻り値として渡すときに、宛先ワーカーは同じメタ（つまり、形状、ストライドなど）を使用して &lt;code&gt;Tensor&lt;/code&gt; を作成しようとします。ソースワーカーと宛先ワーカーのデバイスリストが一致しない場合にクラッシュする可能性があるため、CUDAテンソルの送信を意図的に禁止しています。このような場合、アプリケーションは常に入力テンソルを呼び出し元のCPUに明示的に移動し、必要に応じて呼び出し先の目的のデバイスに移動できます。</target>
        </trans-unit>
        <trans-unit id="0331e44de903db51dab5949d62e369ce763fc069" translate="yes" xml:space="preserve">
          <source>The following Python Expressions are supported.</source>
          <target state="translated">以下のPython式をサポートしています。</target>
        </trans-unit>
        <trans-unit id="a58ae690d15d348f9e4dcb69eeff8937464b7b91" translate="yes" xml:space="preserve">
          <source>The following constraints are implemented:</source>
          <target state="translated">以下の制約が実装されています。</target>
        </trans-unit>
        <trans-unit id="f123fb71a763a700ae8133c3cf978d6e15341a63" translate="yes" xml:space="preserve">
          <source>The following factory functions support named tensors:</source>
          <target state="translated">以下のファクトリー関数は、名前付きテンソルをサポートしています。</target>
        </trans-unit>
        <trans-unit id="08607d57d85e6e823913a7de503035f5f9376e6f" translate="yes" xml:space="preserve">
          <source>The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a &lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;torch.nn.Module&lt;/code&gt;&lt;/a&gt;, as a function, or as a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.</source>
          <target state="translated">次のリストは、自動キャストが有効なリージョンでの適格なopsの動作を示しています。これらの操作は、&lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;torch.nn.Module&lt;/code&gt; の&lt;/a&gt;一部として呼び出されるか、関数として呼び出されるか、&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;メソッドとして呼び出されるかにかかわらず、常に自動キャストを実行します。関数が複数の名前空間で公開されている場合、名前空間に関係なく、関数は自動キャストされます。</target>
        </trans-unit>
        <trans-unit id="690193766b4768be386db05b742730b7dbdf34e9" translate="yes" xml:space="preserve">
          <source>The following methods are unique to &lt;a href=&quot;#torch.BoolTensor&quot;&gt;&lt;code&gt;torch.BoolTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">次のメソッドは、&lt;a href=&quot;#torch.BoolTensor&quot;&gt; &lt;code&gt;torch.BoolTensor&lt;/code&gt; に&lt;/a&gt;固有のものです。</target>
        </trans-unit>
        <trans-unit id="2fcc9561d17b3f2c0c3db2c65c4b45e1fd39309f" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will act deterministically when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="translated">次の通常は非決定論的な演算は、 &lt;code&gt;d=True&lt;/code&gt; の場合に決定論的に動作します。</target>
        </trans-unit>
        <trans-unit id="6c856d87fb36615ab5d3e65d2705a558ef7fc7ab" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will throw a &lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt;&lt;code&gt;RuntimeError&lt;/code&gt;&lt;/a&gt; when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="translated">次の通常は非決定的な操作は、 &lt;code&gt;d=True&lt;/code&gt; の場合に&lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt; &lt;code&gt;RuntimeError&lt;/code&gt; &lt;/a&gt;をスローします。</target>
        </trans-unit>
        <trans-unit id="10cc3c798c883edec11b18c9b813ac5a197463fe" translate="yes" xml:space="preserve">
          <source>The following operators are supported:</source>
          <target state="translated">以下の演算子に対応しています。</target>
        </trans-unit>
        <trans-unit id="116feb31ceb0c75370eb04add5159eb16382e56b" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in PyTorch 1.8. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.fftn&quot;&gt;&lt;code&gt;torch.fft.fftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">関数&lt;a href=&quot;#torch.fft&quot;&gt; &lt;code&gt;torch.fft()&lt;/code&gt; &lt;/a&gt;は非推奨になり、PyTorch1.8で削除される予定です。代わりに、&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt;をインポートして&lt;a href=&quot;../fft#torch.fft.fft&quot;&gt; &lt;code&gt;torch.fft.fft()&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;../fft#torch.fft.fftn&quot;&gt; &lt;code&gt;torch.fft.fftn()&lt;/code&gt; &lt;/a&gt;を呼び出すことにより、新しい&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt;モジュール関数を使用します。</target>
        </trans-unit>
        <trans-unit id="18bbed5d4e21fbcafc8624ec507383dc653a9737" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.ifft&quot;&gt;&lt;code&gt;torch.ifft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.ifftn&quot;&gt;&lt;code&gt;torch.fft.ifftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">関数&lt;a href=&quot;#torch.ifft&quot;&gt; &lt;code&gt;torch.ifft()&lt;/code&gt; &lt;/a&gt;は非推奨であり、将来のPyTorchリリースで削除される予定です。代わりに、&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt;をインポートし、&lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt; &lt;code&gt;torch.fft.ifft()&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;../fft#torch.fft.ifftn&quot;&gt; &lt;code&gt;torch.fft.ifftn()&lt;/code&gt; &lt;/a&gt;を呼び出すことにより、新しい&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt;モジュール関数を使用します。</target>
        </trans-unit>
        <trans-unit id="b1a40cf129fc842a7fe39cfe2023111384b05a6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;torch.irfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.irfft&quot;&gt;&lt;code&gt;torch.fft.irfft()&lt;/code&gt;&lt;/a&gt; for one-sided input, or &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; for two-sided input.</source>
          <target state="translated">関数&lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;torch.irfft()&lt;/code&gt; &lt;/a&gt;は非推奨であり、将来のPyTorchリリースで削除される予定です。新しい使用&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fftの&lt;/a&gt;代わりに、インポートすることで、モジュールの機能を&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fftを&lt;/a&gt;して呼び出す&lt;a href=&quot;../fft#torch.fft.irfft&quot;&gt; &lt;code&gt;torch.fft.irfft()&lt;/code&gt; &lt;/a&gt;、片側の入力、またはのための&lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt; &lt;code&gt;torch.fft.ifft()&lt;/code&gt; &lt;/a&gt;両面入力のために。</target>
        </trans-unit>
        <trans-unit id="a0db51d208a07883c44d7be8c9786454460ee522" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.rfft&quot;&gt;&lt;code&gt;torch.rfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.rfft&quot;&gt;&lt;code&gt;torch.fft.rfft()&lt;/code&gt;&lt;/a&gt; for one-sided output, or &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; for two-sided output.</source>
          <target state="translated">関数&lt;a href=&quot;#torch.rfft&quot;&gt; &lt;code&gt;torch.rfft()&lt;/code&gt; &lt;/a&gt;は非推奨であり、将来のPyTorchリリースで削除される予定です。新しい使用&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fftの&lt;/a&gt;代わりに、インポートすることで、モジュールの機能を&lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fftを&lt;/a&gt;して呼び出す&lt;a href=&quot;../fft#torch.fft.rfft&quot;&gt; &lt;code&gt;torch.fft.rfft()&lt;/code&gt; &lt;/a&gt;、片側の出力、またはのための&lt;a href=&quot;../fft#torch.fft.fft&quot;&gt; &lt;code&gt;torch.fft.fft()&lt;/code&gt; &lt;/a&gt;両面出力のために。</target>
        </trans-unit>
        <trans-unit id="10b975abfe7a8de3222390632564c8c39557329e" translate="yes" xml:space="preserve">
          <source>The function is called as &lt;code&gt;fn(i, *args)&lt;/code&gt;, where &lt;code&gt;i&lt;/code&gt; is the process index and &lt;code&gt;args&lt;/code&gt; is the passed through tuple of arguments.</source>
          <target state="translated">この関数は &lt;code&gt;fn(i, *args)&lt;/code&gt; と呼ばれます。ここで、 &lt;code&gt;i&lt;/code&gt; はプロセスインデックスであり、 &lt;code&gt;args&lt;/code&gt; は渡される引数のタプルです。</target>
        </trans-unit>
        <trans-unit id="78c07c2cfdea987b1b930691f54fd99adb7a7932" translate="yes" xml:space="preserve">
          <source>The function is defined as:</source>
          <target state="translated">関数として定義されています。</target>
        </trans-unit>
        <trans-unit id="81e6543a378b1e27df259854cebe693b6ae023b6" translate="yes" xml:space="preserve">
          <source>The gated linear unit. Computes:</source>
          <target state="translated">ゲーテッドリニアユニット。計算します。</target>
        </trans-unit>
        <trans-unit id="e2ac4a47b4d32b1f56737b1e1c139ea56803b6cc" translate="yes" xml:space="preserve">
          <source>The graph is differentiated using the chain rule. If any of &lt;code&gt;tensors&lt;/code&gt; are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying &lt;code&gt;grad_tensors&lt;/code&gt;. It should be a sequence of matching length, that contains the &amp;ldquo;vector&amp;rdquo; in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (&lt;code&gt;None&lt;/code&gt; is an acceptable value for all tensors that don&amp;rsquo;t need gradient tensors).</source>
          <target state="translated">グラフは連鎖律を使用して区別されます。 &lt;code&gt;tensors&lt;/code&gt; いずれかが非スカラーであり（つまり、データに複数の要素がある）、勾配が必要な場合、ヤコビアンベクトル積が計算されます。この場合、関数はさらに &lt;code&gt;grad_tensors&lt;/code&gt; を指定する必要があります。これは、ヤコビアンベクトル積の「ベクトル」、通常は対応するテンソルに対する微分関数の勾配を含む、一致する長さのシーケンスである必要があります（勾配テンソルを必要としないすべてのテンソルで許容値は &lt;code&gt;None&lt;/code&gt; です）。</target>
        </trans-unit>
        <trans-unit id="c30476379351f558b5c5fb5c388947e81c5eddf9" translate="yes" xml:space="preserve">
          <source>The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying &lt;code&gt;gradient&lt;/code&gt;. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">グラフは連鎖律を使用して区別されます。テンソルが非スカラーであり（つまり、そのデータに複数の要素がある）、勾配が必要な場合、関数はさらに &lt;code&gt;gradient&lt;/code&gt; 指定する必要があります。これは、タイプと場所が一致するテンソルである必要があります。このテンソルには、 &lt;code&gt;self&lt;/code&gt; との微分関数の勾配が含まれています。</target>
        </trans-unit>
        <trans-unit id="e635ba31c5601250c0b5605418f5a52e38b9a9ea" translate="yes" xml:space="preserve">
          <source>The histogram is computed continuously, and the ranges per bin change with every new tensor observed.</source>
          <target state="translated">ヒストグラムは連続的に計算され、新しいテンソルが観測されるたびにビンあたりの範囲が変化します。</target>
        </trans-unit>
        <trans-unit id="4fce725259bdabadb8950b8c6e8b1cc9bdca5a28" translate="yes" xml:space="preserve">
          <source>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of &lt;a href=&quot;#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">フックは引数を変更するべきではありませんが、オプションで&lt;a href=&quot;#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; の&lt;/a&gt;代わりに使用される新しいグラデーションを返すことができます。</target>
        </trans-unit>
        <trans-unit id="63a016154b7848c498c0189a691b12aee6391726" translate="yes" xml:space="preserve">
          <source>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">フックは引数を変更するべきではありませんが、オプションで&lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; の&lt;/a&gt;代わりに使用される新しいグラデーションを返すことができます。</target>
        </trans-unit>
        <trans-unit id="d3b8c649dc7d5a1913425cba9308cbaac351ab8a" translate="yes" xml:space="preserve">
          <source>The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:</source>
          <target state="translated">フックは、Tensorに対する勾配が計算されるたびに呼び出されます。フックは次のようなシグネチャを持つ必要があります。</target>
        </trans-unit>
        <trans-unit id="91b8909b6b0301d53bc827787fcd07621d744139" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; has computed an output. It should have the following signature:</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;が出力を計算した後、フックは毎回呼び出されます。次の署名が必要です。</target>
        </trans-unit>
        <trans-unit id="ea6266d19428a156d329cc252aacfffb34135bcf" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;code&gt;forward()&lt;/code&gt; has computed an output. It should have the following signature:</source>
          <target state="translated">&lt;code&gt;forward()&lt;/code&gt; が出力を計算した後、フックは毎回呼び出されます。次の署名が必要です。</target>
        </trans-unit>
        <trans-unit id="3027e825f33c77d15aef5c4a000dacdcd543639b" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is invoked. It should have the following signature:</source>
          <target state="translated">フックは、&lt;a href=&quot;#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;が呼び出される前に毎回呼び出されます。次の署名が必要です。</target>
        </trans-unit>
        <trans-unit id="658103272c0bdd847853bfee8c1acd84bd1873a4" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;code&gt;forward()&lt;/code&gt; is invoked. It should have the following signature:</source>
          <target state="translated">フックは、 &lt;code&gt;forward()&lt;/code&gt; が呼び出される前に毎回呼び出されます。次の署名が必要です。</target>
        </trans-unit>
        <trans-unit id="b7317d93b9ecea6baa18606aa1761b42a13618f5" translate="yes" xml:space="preserve">
          <source>The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:</source>
          <target state="translated">このフックは、モジュールの入力に対するグラデーションが計算されるたびに呼び出されます。フックは次のようなシグネチャを持つべきです。</target>
        </trans-unit>
        <trans-unit id="47b9323acd4c956c6840fecee45ba64f27f04f5f" translate="yes" xml:space="preserve">
          <source>The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute &amp;ndash; that is, contain a small number of assigned labels.</source>
          <target state="translated">頻繁にアクセスされるクラスター（最初のクラスターのように、最も頻繁なラベルを含む）も、計算コストが低くなる必要があります。つまり、割り当てられたラベルの数が少なくなります。</target>
        </trans-unit>
        <trans-unit id="53da9964154c69f0225ac83d2e63e3e638d7b44d" translate="yes" xml:space="preserve">
          <source>The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus</source>
          <target state="translated">ここでの実装では、イプシロンを加える前に勾配平均の平方根を取ります(TensorFlowはこれら2つの演算を入れ替えていることに注意してください)。効果的な学習率は次のようになります。</target>
        </trans-unit>
        <trans-unit id="fd1e3da6ac802ac499dbc929d3c01e7d8d1e592e" translate="yes" xml:space="preserve">
          <source>The implementation is based on the Algorithm 5.1 from Halko et al, 2009.</source>
          <target state="translated">実装は、Halko et al,2009のアルゴリズム5.1に基づいています。</target>
        </trans-unit>
        <trans-unit id="b39161781b1b7b35ca70ad2826501e304e14740d" translate="yes" xml:space="preserve">
          <source>The implementation is based on: Bader, P.; Blanes, S.; Casas, F. Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation. Mathematics 2019, 7, 1174.</source>
          <target state="translated">実装は以下に基づいています。Bader,P.;Blanes,S.;Casas,F.最適化されたテイラー多項式近似を用いた行列指数の計算.数学 2019,7,1174.</target>
        </trans-unit>
        <trans-unit id="b9e5eb34b5f671210df55afa7cf37ff9e10daa69" translate="yes" xml:space="preserve">
          <source>The implementation of SGD with Momentum/Nesterov subtly differs from Sutskever et. al. and implementations in some other frameworks.</source>
          <target state="translated">Momentum/Nesterov を用いた SGD の実装は、Sutskever らや他のいくつかのフレームワークの実装とは微妙に異なります。</target>
        </trans-unit>
        <trans-unit id="d9d89b5b4a75085b7eb81dcbff4669f9ff584606" translate="yes" xml:space="preserve">
          <source>The implementation of SVD on CPU uses the LAPACK routine &lt;code&gt;?gesdd&lt;/code&gt; (a divide-and-conquer algorithm) instead of &lt;code&gt;?gesvd&lt;/code&gt; for speed. Analogously, the SVD on GPU uses the MAGMA routine &lt;code&gt;gesdd&lt;/code&gt; as well.</source>
          <target state="translated">CPUでのSVDの実装では、速度を &lt;code&gt;?gesvd&lt;/code&gt; ために、？gesvdの代わりにLAPACKルーチンの &lt;code&gt;?gesdd&lt;/code&gt; （分割統治アルゴリズム）を使用します。同様に、GPU上のSVDもMAGMAルーチン &lt;code&gt;gesdd&lt;/code&gt; を使用します。</target>
        </trans-unit>
        <trans-unit id="c05db592ff41dfc6119d9765d1f98df7bbeac593" translate="yes" xml:space="preserve">
          <source>The implementations of the models for object detection, instance segmentation and keypoint detection are efficient.</source>
          <target state="translated">オブジェクト検出、インスタンスセグメンテーション、キーポイント検出のためのモデルの実装は効率的である。</target>
        </trans-unit>
        <trans-unit id="2ee40d087e274de07c646cdfb7f973b9dab8f9cf" translate="yes" xml:space="preserve">
          <source>The inferred dtype for python floats in &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Pythonの推定dtypeは&lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; にあり&lt;/a&gt;ます。</target>
        </trans-unit>
        <trans-unit id="6c9a1b1a8c22e228f94e607d9ba23a7a9895221c" translate="yes" xml:space="preserve">
          <source>The input &lt;code&gt;window_length&lt;/code&gt; is a positive integer controlling the returned window size. &lt;code&gt;periodic&lt;/code&gt; flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;. Therefore, if &lt;code&gt;periodic&lt;/code&gt; is true, the</source>
          <target state="translated">入力 &lt;code&gt;window_length&lt;/code&gt; は、返されるウィンドウサイズを制御する正の整数です。 &lt;code&gt;periodic&lt;/code&gt; フラグは、返されたウィンドウが対称ウィンドウから最後の重複値を削除し、&lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;torch.stft()&lt;/code&gt; &lt;/a&gt;などの関数で周期的ウィンドウとして使用する準備ができているかどうかを判別します。したがって、 &lt;code&gt;periodic&lt;/code&gt; が真の場合、</target>
        </trans-unit>
        <trans-unit id="dffb8d48e1eff38cec4958715ef4e4bea17673f9" translate="yes" xml:space="preserve">
          <source>The input channels are separated into &lt;code&gt;num_groups&lt;/code&gt; groups, each containing &lt;code&gt;num_channels / num_groups&lt;/code&gt; channels. The mean and standard-deviation are calculated separately over the each group.</source>
          <target state="translated">入力チャネルは &lt;code&gt;num_groups&lt;/code&gt; グループに分けられ、各グループには &lt;code&gt;num_channels / num_groups&lt;/code&gt; チャネルが含まれます。平均と標準偏差は、グループごとに別々に計算されます。</target>
        </trans-unit>
        <trans-unit id="24d9bce9a9af270abcd63358a34e29c31cf80c6f" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).</source>
          <target state="translated">入力には、モジュールに指定された位置引数のみが含まれます。キーワード引数はフックに渡されず、 &lt;code&gt;forward&lt;/code&gt; のみ渡されます。フックは入力を変更できます。ユーザーは、フックでタプルまたは単一の変更された値を返すことができます。単一の値が返された場合（その値がすでにタプルである場合を除く）、値をタプルにラップします。</target>
        </trans-unit>
        <trans-unit id="501f21d86b3222c931438d1d37f49ef014fe1070" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">入力には、モジュールに指定された位置引数のみが含まれます。キーワード引数はフックに渡されず、 &lt;code&gt;forward&lt;/code&gt; のみ渡されます。フックは出力を変更できます。入力をインプレースで変更できますが、&lt;a href=&quot;#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;が呼び出された後に呼び出されるため、forwardには影響しません。</target>
        </trans-unit>
        <trans-unit id="d84cfbddddc9e1c2d0a053623e51f4c9b23ed343" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;code&gt;forward()&lt;/code&gt; is called.</source>
          <target state="translated">入力には、モジュールに指定された位置引数のみが含まれます。キーワード引数はフックに渡されず、 &lt;code&gt;forward&lt;/code&gt; のみ渡されます。フックは出力を変更できます。入力をインプレースで変更できますが、 &lt;code&gt;forward()&lt;/code&gt; が呼び出された後に呼び出されるため、forwardには影響しません。</target>
        </trans-unit>
        <trans-unit id="e0bbaa069be68586598b46c411d0f2665ffc0e9d" translate="yes" xml:space="preserve">
          <source>The input data is assumed to be of the form &lt;code&gt;minibatch x channels x [optional depth] x [optional height] x width&lt;/code&gt;. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</source>
          <target state="translated">入力データは、 &lt;code&gt;minibatch x channels x [optional depth] x [optional height] x width&lt;/code&gt; の形式であると想定されます。したがって、空間入力の場合は4Dテンソルが必要であり、体積入力の場合は5Dテンソルが必要です。</target>
        </trans-unit>
        <trans-unit id="855506544cad1eeac05a21efea97a3853e2dceb9" translate="yes" xml:space="preserve">
          <source>The input dimensions are interpreted in the form: &lt;code&gt;mini-batch x channels x [optional depth] x [optional height] x width&lt;/code&gt;.</source>
          <target state="translated">入力寸法は、 &lt;code&gt;mini-batch x channels x [optional depth] x [optional height] x width&lt;/code&gt; の形式で解釈されます。</target>
        </trans-unit>
        <trans-unit id="ff7b890633934634eb44c66753eb478acaced44e" translate="yes" xml:space="preserve">
          <source>The input is assumed to be a low-rank matrix.</source>
          <target state="translated">入力は低ランクの行列を想定しています。</target>
        </trans-unit>
        <trans-unit id="aa72bb59a173177382b9660fee81f62bda47bdae" translate="yes" xml:space="preserve">
          <source>The input quantization parameters are propagated to the output.</source>
          <target state="translated">入力された量子化パラメータは出力に伝搬されます。</target>
        </trans-unit>
        <trans-unit id="7c8e7facaf470e54c3381c46dad3dd11548e58de" translate="yes" xml:space="preserve">
          <source>The input quantization parameters propagate to the output.</source>
          <target state="translated">入力された量子化パラメータは出力に伝搬します。</target>
        </trans-unit>
        <trans-unit id="9c93ac513b2302a4f83a43405b9a47bcb2309e67" translate="yes" xml:space="preserve">
          <source>The input to the model is expected to be a list of tensors, each of shape &lt;code&gt;[C, H, W]&lt;/code&gt;, one for each image, and should be in &lt;code&gt;0-1&lt;/code&gt; range. Different images can have different sizes.</source>
          <target state="translated">モデルへの入力は、形状のそれぞれ、テンソルのリストであると予想される &lt;code&gt;[C, H, W]&lt;/code&gt; 、各画像に対して1つ、及びであるべきである &lt;code&gt;0-1&lt;/code&gt; 範囲。画像が異なれば、サイズも異なります。</target>
        </trans-unit>
        <trans-unit id="33129e152019831a4d6451e1dcb0878f56c1bef2" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.&lt;/code&gt; prefix for some operations. See example usage below.</source>
          <target state="translated">このクラスのインスタンスは、 &lt;code&gt;torch.&lt;/code&gt; 代わりに使用できます。一部の操作のプレフィックス。以下の使用例を参照してください。</target>
        </trans-unit>
        <trans-unit id="351e07f8a31af7d82fc6045df4c793836de62f62" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.ops.quantized&lt;/code&gt; prefix. See example usage below.</source>
          <target state="translated">このクラスのインスタンスは、 &lt;code&gt;torch.ops.quantized&lt;/code&gt; プレフィックスの代わりに使用できます。以下の使用例を参照してください。</target>
        </trans-unit>
        <trans-unit id="6c3dcf3c5f75afd5957f352f9917b83b90088988" translate="yes" xml:space="preserve">
          <source>The interface for specifying operator definitions is a Prototype feature; adventurous users should note that the APIs will probably change in a future interface.</source>
          <target state="translated">演算子の定義を指定するインターフェースは Prototype の機能です。</target>
        </trans-unit>
        <trans-unit id="3bb29d8ff742c3f78ac5a2da01f22239940307d9" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数の逆は&lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="3ff786d8f2651e07ce1136605dc8809d77fc8a6a" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数の逆は&lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="bd26976415ff02dd321f8aabd58b78a55df77549" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.irfft#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数の逆は&lt;a href=&quot;torch.irfft#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="51e2eb96b40c3aad396917c7dd98de9a6585beea" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数の逆は&lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="6f2e4deb4f5b0c2c822ac4f786f365c2a6921f20" translate="yes" xml:space="preserve">
          <source>The jvp is currently computed by using the backward of the backward (sometimes called the double backwards trick) as we don&amp;rsquo;t have support for forward mode AD in PyTorch at the moment.</source>
          <target state="translated">現在、PyTorchではフォワードモードADがサポートされていないため、jvpはバックワードのバックワード（ダブルバックワードトリックと呼ばれることもあります）を使用して計算されています。</target>
        </trans-unit>
        <trans-unit id="4b9e7887dd437999f146f02a0f60028f96d84bf3" translate="yes" xml:space="preserve">
          <source>The largest difference between TorchScript and the full Python language is that TorchScript only supports a small set of types that are needed to express neural net models. In particular, TorchScript supports:</source>
          <target state="translated">TorchScriptとPython言語の最大の違いは、TorchScriptがニューラルネットモデルを表現するのに必要な少数の型しかサポートしていないことです。特に、TorchScriptがサポートしているのは</target>
        </trans-unit>
        <trans-unit id="3b1d6d882f09df5877abb8ef200d99a4552edbcc" translate="yes" xml:space="preserve">
          <source>The largest representable number.</source>
          <target state="translated">表現可能な最大の数。</target>
        </trans-unit>
        <trans-unit id="63b23adb215af0fe9eaa51ef9153973e0c53546f" translate="yes" xml:space="preserve">
          <source>The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.</source>
          <target state="translated">最後の項は省略することも、スターリング式で近似することもできます。近似は1以上の目標値に使用され、1以下の目標値ではゼロが損失に加算されます。</target>
        </trans-unit>
        <trans-unit id="08f3358598c31282b2114da38bcad5d74e704334" translate="yes" xml:space="preserve">
          <source>The locations are used in the order of</source>
          <target state="translated">の順で使用されています。</target>
        </trans-unit>
        <trans-unit id="9ebd3a640891ee47a9a38fbd56c89724e57e2af2" translate="yes" xml:space="preserve">
          <source>The loss can be described as:</source>
          <target state="translated">損失と表現してもいいでしょう。</target>
        </trans-unit>
        <trans-unit id="f21bd0e4e809cda5ba13223c1dfb3494e56f4f3e" translate="yes" xml:space="preserve">
          <source>The loss function for</source>
          <target state="translated">の損失関数は</target>
        </trans-unit>
        <trans-unit id="86081d5fc32148129e542f3dc9192fb2857da205" translate="yes" xml:space="preserve">
          <source>The loss function for each pair of samples in the mini-batch is:</source>
          <target state="translated">ミニバッチのサンプルの各ペアの損失関数は、次のようになります。</target>
        </trans-unit>
        <trans-unit id="bb7d036d43546cb725a6bf9bf29c5cc2076ac418" translate="yes" xml:space="preserve">
          <source>The loss function for each sample in the mini-batch is:</source>
          <target state="translated">ミニバッチの各サンプルの損失関数は、以下の通りです。</target>
        </trans-unit>
        <trans-unit id="48f8488012ce60a4d13e1aebd7fa0ba10061eaf3" translate="yes" xml:space="preserve">
          <source>The loss function for each sample is:</source>
          <target state="translated">各サンプルの損失関数は</target>
        </trans-unit>
        <trans-unit id="c43bc97db5e0527c42aa84dc0f06c5fb6d69ddbc" translate="yes" xml:space="preserve">
          <source>The loss function then becomes:</source>
          <target state="translated">すると、損失関数は次のようになります。</target>
        </trans-unit>
        <trans-unit id="dbd64b8b1e6f8a768dc1cc623e35d46784438d27" translate="yes" xml:space="preserve">
          <source>The losses are averaged across observations for each minibatch. If the &lt;code&gt;weight&lt;/code&gt; argument is specified then this is a weighted average:</source>
          <target state="translated">損失は​​、各ミニバッチの観測全体で平均化されます。 &lt;code&gt;weight&lt;/code&gt; 引数が指定されている場合、これは加重平均です。</target>
        </trans-unit>
        <trans-unit id="187e00ce183312dd132d6826870b8e0983ae12a8" translate="yes" xml:space="preserve">
          <source>The lower triangular part of the matrix is defined as the elements on and below the diagonal.</source>
          <target state="translated">行列の下三角形部分は、対角線上と対角線下の要素として定義されます。</target>
        </trans-unit>
        <trans-unit id="cba1d04dc7071338e4468867933f6f030f4f19fd" translate="yes" xml:space="preserve">
          <source>The machine with rank 0 will be used to set up all connections.</source>
          <target state="translated">ランク0のマシンで全ての接続を設定します。</target>
        </trans-unit>
        <trans-unit id="f6b0320b620362b6deb5bb70025cb38b632f6a57" translate="yes" xml:space="preserve">
          <source>The main trick for &lt;code&gt;hard&lt;/code&gt; is to do &lt;code&gt;y_hard - y_soft.detach() + y_soft&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;hard&lt;/code&gt; の主なトリックは、 &lt;code&gt;y_hard - y_soft.detach() + y_soft&lt;/code&gt; を実行することです-y_soft.detach（）+ y_soft</target>
        </trans-unit>
        <trans-unit id="61caa3335965d2f3a59a97a54b15a39417eaac38" translate="yes" xml:space="preserve">
          <source>The max-pooling operation is applied in</source>
          <target state="translated">max-pooling 操作は</target>
        </trans-unit>
        <trans-unit id="d65dca627767b86d15d82ece9fa3008acf19101f" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups.</source>
          <target state="translated">平均値と標準偏差は、同一工程群のすべてのミニバッチについて、寸法ごとに計算しています。</target>
        </trans-unit>
        <trans-unit id="c3242427612d3b3c3e0cb9845d6bf7db1596d5b9" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over the mini-batches and</source>
          <target state="translated">平均値と標準偏差は、ミニバッチと</target>
        </trans-unit>
        <trans-unit id="5661df9ad4405c387ea95d88a69a9fe1650b1552" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.</source>
          <target state="translated">平均値と標準偏差は、ミニバッチの対象物ごとに寸法ごとに個別に計算しています。</target>
        </trans-unit>
        <trans-unit id="29fab6215a0696acbd3e84d8cb442b324881c861" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by &lt;code&gt;normalized_shape&lt;/code&gt;.</source>
          <target state="translated">平均と標準偏差は、 &lt;code&gt;normalized_shape&lt;/code&gt; で指定された形状である必要がある最後の特定の数の次元に対して別々に計算されます。</target>
        </trans-unit>
        <trans-unit id="8a930aa85422123ed7ce795a3af91990ae6ee8f7" translate="yes" xml:space="preserve">
          <source>The mean operation still operates over all the elements, and divides by</source>
          <target state="translated">平均演算はまだすべての要素に対して動作しており</target>
        </trans-unit>
        <trans-unit id="619af861d215de1b87a6226cb50c28f6e623bb30" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements in the dimension &lt;code&gt;dim&lt;/code&gt;. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="translated">中央値は、次元 &lt;code&gt;dim&lt;/code&gt; に偶数の要素を持つ &lt;code&gt;input&lt;/code&gt; テンソルに固有ではありません。この場合、2つの中央値の低い方が返されます。 &lt;code&gt;input&lt;/code&gt; の両方の中央値の平均を計算するには、代わりに &lt;code&gt;q=0.5&lt;/code&gt; で&lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; &lt;/a&gt;を使用します。</target>
        </trans-unit>
        <trans-unit id="5d7cf56bfc5cedceb3ca00313d8c3d414937caef" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="translated">中央値は、要素の数が偶数の &lt;code&gt;input&lt;/code&gt; テンソルでは一意ではありません。この場合、2つの中央値の低い方が返されます。 &lt;code&gt;input&lt;/code&gt; の両方の中央値の平均を計算するには、代わりに &lt;code&gt;q=0.5&lt;/code&gt; で&lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; &lt;/a&gt;を使用します。</target>
        </trans-unit>
        <trans-unit id="3568ef228089a375953fda3fd0c97aca0ace05a3" translate="yes" xml:space="preserve">
          <source>The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.</source>
          <target state="translated">モデルは ResNet と同じですが、ボトルネックとなるチャネル数が各ブロックで 2 倍になっていることを除けば、同じです。例えば、ResNet-50 の最後のブロックには 2048-512-2048 のチャンネルがあり、Wide ResNet-50-2 には 2048-1024-2048 のチャンネルがあります。</target>
        </trans-unit>
        <trans-unit id="f15c86c9dfb63d5c112adb09bde9cb61a9fe3f2e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss.</source>
          <target state="translated">モデルは、トレーニング中に &lt;code&gt;Dict[Tensor]&lt;/code&gt; 返します。これには、RPNとR-CNNの両方の分類と回帰の損失、およびキーポイントの損失が含まれます。</target>
        </trans-unit>
        <trans-unit id="065007e637ab5dd9f6a9a41f5c632d9aa1aa3f5e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the mask loss.</source>
          <target state="translated">モデルは、トレーニング中に &lt;code&gt;Dict[Tensor]&lt;/code&gt; 返します。これには、RPNとR-CNNの両方の分類と回帰の損失、およびマスクの損失が含まれます。</target>
        </trans-unit>
        <trans-unit id="a0c3a4815182bd38a59e8d48bd1acda3676df3d3" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN.</source>
          <target state="translated">モデルは、トレーニング中に &lt;code&gt;Dict[Tensor]&lt;/code&gt; 返します。これには、RPNとR-CNNの両方の分類と回帰の損失が含まれます。</target>
        </trans-unit>
        <trans-unit id="25fbe42f5bebb67d721733ebaa470925353914ab" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses.</source>
          <target state="translated">モデルは、トレーニング中に、分類と回帰の損失を含む &lt;code&gt;Dict[Tensor]&lt;/code&gt; 返します。</target>
        </trans-unit>
        <trans-unit id="f6df7f86985b4db8720a0303772bb804adf7b58b" translate="yes" xml:space="preserve">
          <source>The model will be attached with observer or fake quant modules, and qconfig will be propagated.</source>
          <target state="translated">モデルにオブザーバーや偽のクオンツモジュールを付けて、qconfigを伝搬させます。</target>
        </trans-unit>
        <trans-unit id="32c7b059b3336b886e2737e75a7a90bf14890a11" translate="yes" xml:space="preserve">
          <source>The models expect a list of &lt;code&gt;Tensor[C, H, W]&lt;/code&gt;, in the range &lt;code&gt;0-1&lt;/code&gt;. The models internally resize the images so that they have a minimum size of &lt;code&gt;800&lt;/code&gt;. This option can be changed by passing the option &lt;code&gt;min_size&lt;/code&gt; to the constructor of the models.</source>
          <target state="translated">モデルは、のリストを期待 &lt;code&gt;Tensor[C, H, W]&lt;/code&gt; 範囲で、 &lt;code&gt;0-1&lt;/code&gt; 。モデルは、画像のサイズを内部的に変更して、最小サイズが &lt;code&gt;800&lt;/code&gt; になるようにします。このオプションは、オプション &lt;code&gt;min_size&lt;/code&gt; をモデルのコンストラクターに渡すことで変更できます。</target>
        </trans-unit>
        <trans-unit id="b8cd112b9466880b251d727f36c9b5a8099f6121" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for detection:</source>
          <target state="translated">models サブパッケージには,検出のための以下のモデルアーキテクチャの定義が含まれています。</target>
        </trans-unit>
        <trans-unit id="2fd7064c0a6dbdb00e5c05b4bcf7bb996635bfb3" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for image classification:</source>
          <target state="translated">models サブパッケージには,画像分類のための以下のモデルアーキテクチャの定義が含まれています。</target>
        </trans-unit>
        <trans-unit id="9a82a240d4b1d0867d7d72a1a3f555d48973b4c7" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for semantic segmentation:</source>
          <target state="translated">models サブパッケージには、セマンティックセグメンテーションのための以下のモデルアーキテクチャの定義が含まれています。</target>
        </trans-unit>
        <trans-unit id="18bc003911e615e5dba9603961f512f27acb297a" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification.</source>
          <target state="translated">models サブパッケージには、画像分類、ピクセル単位のセマンティックセグメンテーション、オブジェクト検出、インスタンスセグメンテーション、人物キーポイント検出、動画分類など、さまざまなタスクに対応するモデルの定義が含まれています。</target>
        </trans-unit>
        <trans-unit id="3cf46d13a21b259a13e6ce62948cca48ddbc78f6" translate="yes" xml:space="preserve">
          <source>The modes available for resizing are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only), &lt;code&gt;area&lt;/code&gt;</source>
          <target state="translated">サイズ変更に使用できるモードは、 &lt;code&gt;nearest&lt;/code&gt; 、 &lt;code&gt;linear&lt;/code&gt; （3Dのみ）、 &lt;code&gt;bilinear&lt;/code&gt; 、 &lt;code&gt;bicubic&lt;/code&gt; （4Dのみ）、 &lt;code&gt;trilinear&lt;/code&gt; 線形（5Dのみ）、 &lt;code&gt;area&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28d8cfd32d3249e2763e181ffc749144e0ea24a2" translate="yes" xml:space="preserve">
          <source>The modes available for upsampling are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only)</source>
          <target state="translated">アップサンプリングに使用できるモードは、 &lt;code&gt;nearest&lt;/code&gt; 、 &lt;code&gt;linear&lt;/code&gt; （3Dのみ）、 &lt;code&gt;bilinear&lt;/code&gt; 、 &lt;code&gt;bicubic&lt;/code&gt; （4Dのみ）、 &lt;code&gt;trilinear&lt;/code&gt; 線形（5Dのみ）です。</target>
        </trans-unit>
        <trans-unit id="18b2caf95e73a1c791145d0b79ff330ee08dd6df" translate="yes" xml:space="preserve">
          <source>The module can be accessed as an attribute using the given name.</source>
          <target state="translated">モジュールは、与えられた名前を使って属性としてアクセスすることができます。</target>
        </trans-unit>
        <trans-unit id="4279942bb80ec4b3ccf751b2de250bbb42e94404" translate="yes" xml:space="preserve">
          <source>The module is mainly for debug and records the tensor values during runtime.</source>
          <target state="translated">このモジュールは主にデバッグ用で、実行時のテンソル値を記録します。</target>
        </trans-unit>
        <trans-unit id="cbcdfca1ae0201a3feb424a59be4618b6c74eaeb" translate="yes" xml:space="preserve">
          <source>The module records the running histogram of tensor values along with min/max values. &lt;code&gt;calculate_qparams&lt;/code&gt; will calculate scale and zero_point.</source>
          <target state="translated">モジュールは、最小/最大値とともにテンソル値の実行中のヒストグラムを記録します。 &lt;code&gt;calculate_qparams&lt;/code&gt; は、規模やzero_pointを計算します。</target>
        </trans-unit>
        <trans-unit id="00f10b867e105cafb015ef514a5b43106e8a7867" translate="yes" xml:space="preserve">
          <source>The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="translated">モジュールの &lt;code&gt;forward&lt;/code&gt; はデフォルトでコンパイルされます。 &lt;code&gt;forward&lt;/code&gt; から呼び出されたメソッドは、forwardで使用される順序で遅延コンパイルされ &lt;code&gt;forward&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c8d325f814f94943441bf4453b4ce6fd4bb34555" translate="yes" xml:space="preserve">
          <source>The most important argument of &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; constructor is &lt;code&gt;dataset&lt;/code&gt;, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:</source>
          <target state="translated">&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;コンストラクターの最も重要な引数は &lt;code&gt;dataset&lt;/code&gt; です。これは、データをロードするデータセットオブジェクトを示します。PyTorchは、次の2種類のデータセットをサポートしています。</target>
        </trans-unit>
        <trans-unit id="85f2f05d4d25b4d7e3119d033b3d3b1eb5b1a272" translate="yes" xml:space="preserve">
          <source>The moving average min/max is computed as follows</source>
          <target state="translated">移動平均のmin/maxは以下のように計算されます。</target>
        </trans-unit>
        <trans-unit id="1664b05e14f7fa9291ef99d0be2e3353189a5613" translate="yes" xml:space="preserve">
          <source>The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix</source>
          <target state="translated">多変量正規分布は,正定値共分散行列のいずれかでパラメータ化することができます.</target>
        </trans-unit>
        <trans-unit id="f00cd6c1142679bd0fe1c3ebd4ca78600af96a2c" translate="yes" xml:space="preserve">
          <source>The name of the worker.</source>
          <target state="translated">作業員の名前です。</target>
        </trans-unit>
        <trans-unit id="fba68059efcba7726021d64196e7ad3e2e4fcb96" translate="yes" xml:space="preserve">
          <source>The named tensor API is a prototype feature and subject to change.</source>
          <target state="translated">名前付きテンソルAPIはプロトタイプの機能であり、変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="ed535f6a7c3020c739db9c59caec32bca959d97b" translate="yes" xml:space="preserve">
          <source>The named tensor API is experimental and subject to change.</source>
          <target state="translated">名前付きテンソルAPIは実験的なものであり、変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="36555b023721ec5738f0c183a135318d3c9a4466" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss.</source>
          <target state="translated">負の対数尤度損失。</target>
        </trans-unit>
        <trans-unit id="80880349d7cc0a615947c8d7ecadbb60eab216c2" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss. It is useful to train a classification problem with &lt;code&gt;C&lt;/code&gt; classes.</source>
          <target state="translated">負の対数尤度損失。 &lt;code&gt;C&lt;/code&gt; クラスで分類問題をトレーニングすると便利です。</target>
        </trans-unit>
        <trans-unit id="07e33a3c723549700a6c484c42061c0b83f6423c" translate="yes" xml:space="preserve">
          <source>The new backend derives from &lt;code&gt;c10d.ProcessGroup&lt;/code&gt; and registers the backend name and the instantiating interface through &lt;code&gt;torch.distributed.Backend.register_backend()&lt;/code&gt; when imported.</source>
          <target state="translated">新しいバックエンドは &lt;code&gt;c10d.ProcessGroup&lt;/code&gt; から派生し、インポート時に &lt;code&gt;torch.distributed.Backend.register_backend()&lt;/code&gt; を介してバックエンド名とインスタンス化インターフェイスを登録します。</target>
        </trans-unit>
        <trans-unit id="5ef80d57536dd4f889b111944ebd026c0916a56b" translate="yes" xml:space="preserve">
          <source>The new usage looks like this:</source>
          <target state="translated">新しい使い方はこんな感じです。</target>
        </trans-unit>
        <trans-unit id="bfa45b6345caf33366ff2d44c58d6e6325263da6" translate="yes" xml:space="preserve">
          <source>The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.</source>
          <target state="translated">ノルムは、あたかもそれらが1つのベクトルに連結されているかのように、すべてのグラデーションをまとめて計算されます。グラデーションはその場で修正されます。</target>
        </trans-unit>
        <trans-unit id="760d37d3c11fd75fa728a8e00ba020ac1e72a10c" translate="yes" xml:space="preserve">
          <source>The normalization parameters are different from the image classification ones, and correspond to the mean and std from Kinetics-400.</source>
          <target state="translated">正規化パラメータは画像分類のものとは異なり、Kinetics-400の平均値と標準値に対応しています。</target>
        </trans-unit>
        <trans-unit id="6b4b699c449424f5a0ee9c963faafecf0fd0d061" translate="yes" xml:space="preserve">
          <source>The number of bins (size 1) is one larger than the largest value in &lt;code&gt;input&lt;/code&gt; unless &lt;code&gt;input&lt;/code&gt; is empty, in which case the result is a tensor of size 0. If &lt;code&gt;minlength&lt;/code&gt; is specified, the number of bins is at least &lt;code&gt;minlength&lt;/code&gt; and if &lt;code&gt;input&lt;/code&gt; is empty, then the result is tensor of size &lt;code&gt;minlength&lt;/code&gt; filled with zeros. If &lt;code&gt;n&lt;/code&gt; is the value at position &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;out[n] += weights[i]&lt;/code&gt; if &lt;code&gt;weights&lt;/code&gt; is specified else &lt;code&gt;out[n] += 1&lt;/code&gt;.</source>
          <target state="translated">ビンの数（サイズ1）は、 &lt;code&gt;input&lt;/code&gt; が空でない限り、 &lt;code&gt;input&lt;/code&gt; の最大値より1つ大きくなります。空の場合、結果はサイズ0のテンソルになります &lt;code&gt;minlength&lt;/code&gt; が指定されている場合、ビンの数は少なくとも &lt;code&gt;minlength&lt;/code&gt; であり、 &lt;code&gt;input&lt;/code&gt; 場合が空の場合、結果はゼロで満たされた &lt;code&gt;minlength&lt;/code&gt; のサイズのテンソルになります。 &lt;code&gt;n&lt;/code&gt; が位置 &lt;code&gt;i&lt;/code&gt; の値の場合、 &lt;code&gt;out[n] += weights[i]&lt;/code&gt; &lt;code&gt;weights&lt;/code&gt; が指定されている場合）それ以外の場合は &lt;code&gt;out[n] += 1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="792cc1a58f0a3a46d19118877b6d13c493cb98ec" translate="yes" xml:space="preserve">
          <source>The number of bits occupied by the type.</source>
          <target state="translated">型が占有するビット数。</target>
        </trans-unit>
        <trans-unit id="280e9a9c6268d27aec18f9552a0a0e023b3a5d6c" translate="yes" xml:space="preserve">
          <source>The number of keys present in the store.</source>
          <target state="translated">店内に存在する鍵の数。</target>
        </trans-unit>
        <trans-unit id="cb1db44aaa6eaf0c2b9f7e2d3594ec089fe36151" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by &lt;code&gt;TensorPipeAgent&lt;/code&gt; to execute requests.</source>
          <target state="translated">&lt;code&gt;TensorPipeAgent&lt;/code&gt; がリクエストを実行するために使用するスレッドプール内のスレッドの数。</target>
        </trans-unit>
        <trans-unit id="20a68b367a08bf827b8cb29b9a7a5775403402ae" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by ProcessGroupAgent.</source>
          <target state="translated">ProcessGroupAgentが使用するスレッドプールのスレッド数。</target>
        </trans-unit>
        <trans-unit id="25f85e0ff2381c99b81c5c68406ed2a516dd1318" translate="yes" xml:space="preserve">
          <source>The numerical properties of a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; can be accessed through either the &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; or the &lt;a href=&quot;#torch.torch.iinfo&quot;&gt;&lt;code&gt;torch.iinfo&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">数値特性&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; は、&lt;/a&gt;いずれかを介してアクセスすることができる&lt;a href=&quot;#torch.torch.finfo&quot;&gt; &lt;code&gt;torch.finfo&lt;/code&gt; &lt;/a&gt;又は&lt;a href=&quot;#torch.torch.iinfo&quot;&gt; &lt;code&gt;torch.iinfo&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d8e759a2b7197a6d6914a25b2e4bccfa94711180" translate="yes" xml:space="preserve">
          <source>The operation applied is:</source>
          <target state="translated">適用される操作は</target>
        </trans-unit>
        <trans-unit id="0b61e3d8bf7f65d3c8a372be2ffe4aa181113744" translate="yes" xml:space="preserve">
          <source>The operation is defined as:</source>
          <target state="translated">として動作を定義しています。</target>
        </trans-unit>
        <trans-unit id="416029866582b442a7cfd38c944f5ccc41de6ea3" translate="yes" xml:space="preserve">
          <source>The operator set above is sufficient to export the following models:</source>
          <target state="translated">上記の演算子のセットは、以下のモデルをエクスポートするのに十分です。</target>
        </trans-unit>
        <trans-unit id="97549a235d386a02f6c874f871b2a3fc368e849a" translate="yes" xml:space="preserve">
          <source>The order of norm. inf refers to &lt;code&gt;float('inf')&lt;/code&gt;, numpy&amp;rsquo;s &lt;code&gt;inf&lt;/code&gt; object, or any equivalent object. The following norms can be calculated:</source>
          <target state="translated">規範の順序。infは、 &lt;code&gt;float('inf')&lt;/code&gt; 、numpyの &lt;code&gt;inf&lt;/code&gt; オブジェクト、または同等のオブジェクトを指します。次の基準を計算できます。</target>
        </trans-unit>
        <trans-unit id="587438e00b276c5c07c33c3b833d1847c428edb1" translate="yes" xml:space="preserve">
          <source>The original &lt;code&gt;module&lt;/code&gt; with the converted &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layers. If the original &lt;code&gt;module&lt;/code&gt; is a &lt;code&gt;BatchNorm*D&lt;/code&gt; layer, a new &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layer object will be returned instead.</source>
          <target state="translated">変換された&lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt; &lt;/a&gt;レイヤーを持つ元の &lt;code&gt;module&lt;/code&gt; 。元の &lt;code&gt;module&lt;/code&gt; が &lt;code&gt;BatchNorm*D&lt;/code&gt; レイヤーの場合、代わりに新しい&lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt; &lt;/a&gt;レイヤーオブジェクトが返されます。</target>
        </trans-unit>
        <trans-unit id="016c290cf6497163981ebcd1b0530771762971db" translate="yes" xml:space="preserve">
          <source>The original Adam algorithm was proposed in &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;. The AdamW variant was proposed in &lt;a href=&quot;https://arxiv.org/abs/1711.05101&quot;&gt;Decoupled Weight Decay Regularization&lt;/a&gt;.</source>
          <target state="translated">オリジナルのAdamアルゴリズムは、&lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam：A Method for StochasticOptimizationで&lt;/a&gt;提案されました。AdamWバリアントが提案された&lt;a href=&quot;https://arxiv.org/abs/1711.05101&quot;&gt;減結合重ディケイ正則&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1c43e721e428fefa0921064ef9d974f9d9c3710a" translate="yes" xml:space="preserve">
          <source>The original module with the spectral norm hook</source>
          <target state="translated">スペクトルノルムフックを持つオリジナルモジュール</target>
        </trans-unit>
        <trans-unit id="1c8ee61f168965198d32cd38694cc41b385bdcc7" translate="yes" xml:space="preserve">
          <source>The original module with the weight norm hook</source>
          <target state="translated">ウェイトノルムフック付きのオリジナルモジュール</target>
        </trans-unit>
        <trans-unit id="1ad8002d2700baeb804ee6b717ba471c2750abdc" translate="yes" xml:space="preserve">
          <source>The other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the &lt;code&gt;rsample()&lt;/code&gt; method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows:</source>
          <target state="translated">これらの確率的/ポリシー勾配を実装するもう1つの方法は、 &lt;code&gt;rsample()&lt;/code&gt; メソッドからの再パラメーター化トリックを使用することです。この場合、パラメーター化された確率変数は、パラメーターのない確率変数のパラメーター化された決定論的関数を介して構築できます。したがって、再パラメータ化されたサンプルは微分可能になります。パスワイズ導関数を実装するためのコードは次のようになります。</target>
        </trans-unit>
        <trans-unit id="a7c71aa7150538e239f1ed9763931d65c1aab1d1" translate="yes" xml:space="preserve">
          <source>The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="translated">出力は,任意の入力サイズに対して,サイズ D x H x W である.出力特徴量の数は入力平面の数に等しい。</target>
        </trans-unit>
        <trans-unit id="8d13bc6b68893874e15ef23c3e8bf09f29542e58" translate="yes" xml:space="preserve">
          <source>The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="translated">出力は,任意の入力サイズに対して,サイズ H x W である.出力特徴量の数は入力平面の数に等しい。</target>
        </trans-unit>
        <trans-unit id="dd1aa83704336b7c60e0ac15205527d8852065d5" translate="yes" xml:space="preserve">
          <source>The output of the &lt;code&gt;model&lt;/code&gt; callable when called with the given &lt;code&gt;*args&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt;.</source>
          <target state="translated">指定された &lt;code&gt;*args&lt;/code&gt; および &lt;code&gt;**kwargs&lt;/code&gt; で呼び出されたときに呼び出し可能な &lt;code&gt;model&lt;/code&gt; の出力。</target>
        </trans-unit>
        <trans-unit id="2624c3f93c8a5ba7f8db76475b9b2160784ad8be" translate="yes" xml:space="preserve">
          <source>The output size is H, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="translated">出力サイズは、任意の入力サイズに対してHである。出力特徴量の数は、入力面の数に等しい。</target>
        </trans-unit>
        <trans-unit id="aaed500780776d71aa7aa3beac0d23a2ad0bee52" translate="yes" xml:space="preserve">
          <source>The output tuple size must match the outputs of &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="translated">出力タプルサイズは、 &lt;code&gt;forward&lt;/code&gt; の出力と一致する必要があります。</target>
        </trans-unit>
        <trans-unit id="69fc4cd3bd84d50752da22ec0fa2da75a028b6c8" translate="yes" xml:space="preserve">
          <source>The package needs to be initialized using the &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; function before calling any other methods. This blocks until all processes have joined.</source>
          <target state="translated">パッケージは、他のメソッドを呼び出す前に、&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt;関数を使用して初期化する必要があります。これは、すべてのプロセスが参加するまでブロックされます。</target>
        </trans-unit>
        <trans-unit id="a24470d7f3dafd97610b1a97cfc4e625080f5d77" translate="yes" xml:space="preserve">
          <source>The padding size by which to pad some dimensions of &lt;code&gt;input&lt;/code&gt; are described starting from the last dimension and moving forward.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 一部の次元をパディングするためのパディングサイズは、最後の次元から始まり、前方に向かって説明されています。</target>
        </trans-unit>
        <trans-unit id="b3d894892755f58e334d42f2a6bb03ad3c9c1029" translate="yes" xml:space="preserve">
          <source>The parallelized &lt;code&gt;module&lt;/code&gt; must have its parameters and buffers on &lt;code&gt;device_ids[0]&lt;/code&gt; before running this &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt; module.</source>
          <target state="translated">並列化された &lt;code&gt;module&lt;/code&gt; は、この&lt;a href=&quot;#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; &lt;/a&gt;モジュールを実行する前に、 &lt;code&gt;device_ids[0]&lt;/code&gt; にパラメーターとバッファーを持っている必要があります。</target>
        </trans-unit>
        <trans-unit id="611179cf7c23e5fa87e86dc65fc6c2d7edcfcdbf" translate="yes" xml:space="preserve">
          <source>The parameter can be accessed as an attribute using given name.</source>
          <target state="translated">パラメータは、与えられた名前を使って属性としてアクセスすることができます。</target>
        </trans-unit>
        <trans-unit id="3673b833c2a753da5c92e7b2bd7867575cba0695" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; can either be:</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; は、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="bb60e522db72c109467c75d771e518ca64062c0c" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can each be an &lt;code&gt;int&lt;/code&gt; or a one-element tuple.</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; 、 &lt;code&gt;padding&lt;/code&gt; は、それぞれ &lt;code&gt;int&lt;/code&gt; または1要素のタプルにすることができます。</target>
        </trans-unit>
        <trans-unit id="df694fbe631671fb53535c127328a8f9b5ccfdfe" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can either be:</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; 、 &lt;code&gt;padding&lt;/code&gt; は、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="0d35449e64c1f14944b10572266aa02084f0787f" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt; can either be:</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; 、 &lt;code&gt;padding&lt;/code&gt; 、 &lt;code&gt;dilation&lt;/code&gt; は、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="16e542ef2d9e6480cf0213002d643e04ad39afd9" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;output_padding&lt;/code&gt; can either be:</source>
          <target state="translated">パラメータ &lt;code&gt;kernel_size&lt;/code&gt; 、 &lt;code&gt;stride&lt;/code&gt; 、 &lt;code&gt;padding&lt;/code&gt; 、 &lt;code&gt;output_padding&lt;/code&gt; は、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="9df0d4116a648b1a73d8fd1ad856ffa0c5b125d9" translate="yes" xml:space="preserve">
          <source>The parameters represented by a single vector</source>
          <target state="translated">単一のベクトルで表されるパラメータ</target>
        </trans-unit>
        <trans-unit id="16c253cfff05379e83f73283d75913a9d3bc5eb0" translate="yes" xml:space="preserve">
          <source>The pivots returned by the function are 1-indexed. If &lt;code&gt;pivot&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the returned pivots is a tensor filled with zeros of the appropriate size.</source>
          <target state="translated">関数によって返されるピボットは1インデックスです。 &lt;code&gt;pivot&lt;/code&gt; が &lt;code&gt;False&lt;/code&gt; の場合、返されるピボットは適切なサイズのゼロで満たされたテンソルです。</target>
        </trans-unit>
        <trans-unit id="4600b83baa50f6eee75af80c6cdb275baf5a4820" translate="yes" xml:space="preserve">
          <source>The pre-trained models for detection, instance segmentation and keypoint detection are initialized with the classification models in torchvision.</source>
          <target state="translated">事前に学習した検出、インスタンスセグメンテーション、キーポイント検出のモデルは、torchvisionの分類モデルで初期化されています。</target>
        </trans-unit>
        <trans-unit id="bacc13981715b20b015df75a9953a0a7c1d88d4c" translate="yes" xml:space="preserve">
          <source>The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in &lt;code&gt;references/segmentation/coco_utils.py&lt;/code&gt;. The classes that the pre-trained model outputs are the following, in order:</source>
          <target state="translated">事前トレーニング済みのモデルは、COCO train2017のサブセットで、PascalVOCデータセットに存在する20のカテゴリでトレーニングされています。サブセットがどのように選択されたかについての詳細は、 &lt;code&gt;references/segmentation/coco_utils.py&lt;/code&gt; ます。事前トレーニング済みモデルが出力するクラスは、次のとおりです。</target>
        </trans-unit>
        <trans-unit id="5f5bcce5c35434b7def7a2c2abecc7855a7ff6ae" translate="yes" xml:space="preserve">
          <source>The process for obtaining the values of &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;std&lt;/code&gt; is roughly equivalent to:</source>
          <target state="translated">&lt;code&gt;mean&lt;/code&gt; 値と &lt;code&gt;std&lt;/code&gt; 値を取得するプロセスは、ほぼ次のようになります。</target>
        </trans-unit>
        <trans-unit id="2eeaf80bb67fd9965166125db5e39a93b156744c" translate="yes" xml:space="preserve">
          <source>The provided mean is the circular one.</source>
          <target state="translated">提供された平均値は円形のものです。</target>
        </trans-unit>
        <trans-unit id="9a5bce0d740da57838aed1d4d747fbb1351683c7" translate="yes" xml:space="preserve">
          <source>The provided variance is the circular one.</source>
          <target state="translated">提供されたバリアンスは円形のものです。</target>
        </trans-unit>
        <trans-unit id="cb1f1b1b62d82d262fa3ee527ff2235c6fe33cf0" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse is not necessarily a continuous function in the elements of the matrix &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/0117004&quot;&gt;[1]&lt;/a&gt;. Therefore, derivatives are not always existent, and exist for a constant rank only &lt;a href=&quot;https://www.jstor.org/stable/2156365&quot;&gt;[2]&lt;/a&gt;. However, this method is backprop-able due to the implementation by using SVD results, and could be unstable. Double-backward will also be unstable due to the usage of SVD internally. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">疑似逆行列は、必ずしも行列の要素の連続関数ではありません&lt;a href=&quot;https://epubs.siam.org/doi/10.1137/0117004&quot;&gt;[1]&lt;/a&gt;。したがって、導関数は常に存在するわけではなく、一定のランクに対してのみ存在します&lt;a href=&quot;https://www.jstor.org/stable/2156365&quot;&gt;[2]&lt;/a&gt;。ただし、このメソッドは、SVD結果を使用した実装のためにバックプロパゲーション可能であり、不安定になる可能性があります。内部でSVDを使用しているため、ダブルバックワードも不安定になります。詳細については、&lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="df88bf331dfcd873fd61e2a961e2a4565dca71ab" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of &lt;code&gt;input&lt;/code&gt; of dimensions</source>
          <target state="translated">次元の &lt;code&gt;input&lt;/code&gt; の疑似逆行列</target>
        </trans-unit>
        <trans-unit id="801cd7b0289a0cab3f1164523d810bc285cdd10c" translate="yes" xml:space="preserve">
          <source>The published models should be at least in a branch/tag. It can&amp;rsquo;t be a random commit.</source>
          <target state="translated">公開されたモデルは、少なくともブランチ/タグに含まれている必要があります。ランダムコミットにすることはできません。</target>
        </trans-unit>
        <trans-unit id="5002f22d2309c888525404ef88b8a18252cac236" translate="yes" xml:space="preserve">
          <source>The quantization parameters are computed the same way as in &lt;code&gt;MinMaxObserver&lt;/code&gt;, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.</source>
          <target state="translated">量子化パラメータは、 &lt;code&gt;MinMaxObserver&lt;/code&gt; と同じ方法で計算されますが、実行中の最小/最大値がチャネルごとに保存される点が異なります。したがって、スケールとゼロ点はチャネルごとにも計算されます。</target>
        </trans-unit>
        <trans-unit id="37425d4454f834297b034206170c35abd72a213b" translate="yes" xml:space="preserve">
          <source>The quantization parameters are computed the same way as in &lt;code&gt;MovingAverageMinMaxObserver&lt;/code&gt;, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.</source>
          <target state="translated">量子化パラメータは、 &lt;code&gt;MovingAverageMinMaxObserver&lt;/code&gt; の場合と同じ方法で計算されますが、実行中の最小/最大値がチャネルごとに保存される点が異なります。したがって、スケールとゼロ点はチャネルごとにも計算されます。</target>
        </trans-unit>
        <trans-unit id="346d775a68239ce50fc9c648f8e7ac9c30934f97" translate="yes" xml:space="preserve">
          <source>The range of the linear region</source>
          <target state="translated">線形領域の範囲</target>
        </trans-unit>
        <trans-unit id="a67bc4760995b69a50ba7bb512777bd75a29786d" translate="yes" xml:space="preserve">
          <source>The rank of the process group -1, if not part of the group</source>
          <target state="translated">プロセスグループのランク -1、グループに属していない場合は</target>
        </trans-unit>
        <trans-unit id="1640b5e94a24bbde9b57f55563952bd38f53f1c7" translate="yes" xml:space="preserve">
          <source>The real-to-complex Fourier transform results follow conjugate symmetry:</source>
          <target state="translated">実数-複素フーリエ変換の結果は共役対称性に従う。</target>
        </trans-unit>
        <trans-unit id="f3326dbe4c730be437e49c0a1205659d772531b4" translate="yes" xml:space="preserve">
          <source>The regular implementation uses the (more common in PyTorch) &lt;code&gt;torch.long&lt;/code&gt; dtype.</source>
          <target state="translated">通常の実装では、（PyTorchでより一般的な） &lt;code&gt;torch.long&lt;/code&gt; dtypeを使用します。</target>
        </trans-unit>
        <trans-unit id="f42ad6925b2f350139e05854ee56333bfbf81bfa" translate="yes" xml:space="preserve">
          <source>The relation of &lt;code&gt;(U, S, V)&lt;/code&gt; to PCA is as follows:</source>
          <target state="translated">&lt;code&gt;(U, S, V)&lt;/code&gt; とPCAの関係は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="946fd419edbe7367b996132ceeb5715b2129c56a" translate="yes" xml:space="preserve">
          <source>The rest of this section concerns the case with &lt;a href=&quot;#map-style-datasets&quot;&gt;map-style datasets&lt;/a&gt;. &lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;&lt;code&gt;torch.utils.data.Sampler&lt;/code&gt;&lt;/a&gt; classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a &lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;&lt;code&gt;Sampler&lt;/code&gt;&lt;/a&gt; could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.</source>
          <target state="translated">このセクションの残りの部分は、&lt;a href=&quot;#map-style-datasets&quot;&gt;マップスタイルのデータセットの&lt;/a&gt;場合に関するものです。&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt; &lt;code&gt;torch.utils.data.Sampler&lt;/code&gt; &lt;/a&gt;クラスは、データのロードで使用されるインデックス/キーのシーケンスを指定するために使用されます。これらは、データセットへのインデックス上の反復可能なオブジェクトを表します。たとえば、確率的勾配降下法（SGD）の一般的なケースでは、&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt; &lt;code&gt;Sampler&lt;/code&gt; &lt;/a&gt;はインデックスのリストをランダムに並べ替えて一度に1つずつ生成するか、ミニバッチSGD用に少数のインデックスを生成します。</target>
        </trans-unit>
        <trans-unit id="0dee4722eb608774d930f39be3f46f18bcb0b430" translate="yes" xml:space="preserve">
          <source>The result will never require gradient.</source>
          <target state="translated">結果的にグラデーションを必要とすることはありません。</target>
        </trans-unit>
        <trans-unit id="6b0575a2f172061283c10c05f0db13db17166b95" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;alexnet.onnx&lt;/code&gt; is a binary protobuf file which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The keyword argument &lt;code&gt;verbose=True&lt;/code&gt; causes the exporter to print out a human-readable representation of the network:</source>
          <target state="translated">結果の &lt;code&gt;alexnet.onnx&lt;/code&gt; は、エクスポートしたモデル（この場合はAlexNet）のネットワーク構造とパラメーターの両方を含むバイナリprotobufファイルです。キーワード引数 &lt;code&gt;verbose=True&lt;/code&gt; を指定すると、エクスポータは人間が読める形式のネットワークを出力します。</target>
        </trans-unit>
        <trans-unit id="3c02f08f3d1024ae629cdcc31caf60626bc7a365" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;out&lt;/code&gt; tensor shares it&amp;rsquo;s underlying storage with the &lt;code&gt;input&lt;/code&gt; tensor, so changing the content of one would change the content of the other.</source>
          <target state="translated">結果の &lt;code&gt;out&lt;/code&gt; 、それはとストレージの基礎となるだテンソル株式 &lt;code&gt;input&lt;/code&gt; 1の内容を変更すると、他の内容を変更しますので、テンソル。</target>
        </trans-unit>
        <trans-unit id="6758477f8561982cc2515cdbf897a47792390375" translate="yes" xml:space="preserve">
          <source>The resulting recording of &lt;code&gt;nn.Module.forward&lt;/code&gt; or &lt;code&gt;nn.Module&lt;/code&gt; produces &lt;code&gt;ScriptModule&lt;/code&gt;.</source>
          <target state="translated">結果として得られる &lt;code&gt;nn.Module.forward&lt;/code&gt; または &lt;code&gt;nn.Module&lt;/code&gt; の記録により、ScriptModuleが生成され &lt;code&gt;ScriptModule&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="094611e344ec0108ea463a9054c21e208ca824dd" translate="yes" xml:space="preserve">
          <source>The resulting recording of a standalone function produces &lt;code&gt;ScriptFunction&lt;/code&gt;.</source>
          <target state="translated">結果として得られるスタンドアロン関数の記録により、 &lt;code&gt;ScriptFunction&lt;/code&gt; が生成されます。</target>
        </trans-unit>
        <trans-unit id="7e1976b95811fe3c223eccaa3dc30d2bb2ed6c6f" translate="yes" xml:space="preserve">
          <source>The return value of this function is a dictionary of statistics, each of which is a non-negative integer.</source>
          <target state="translated">この関数の戻り値は、統計情報の辞書であり、それぞれが非負の整数である。</target>
        </trans-unit>
        <trans-unit id="14a7d916a182796f96dea780975d504da8d70e49" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object can come from &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; constructor. The example below shows directly using the &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; returned by &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返される&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;オブジェクトは、&lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt; &lt;code&gt;then()&lt;/code&gt; &lt;/a&gt;、または&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;コンストラクターから取得できます。以下の例は、&lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt; &lt;code&gt;then()&lt;/code&gt; &lt;/a&gt;によって返される&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;を直接使用することを示しています。</target>
        </trans-unit>
        <trans-unit id="35586b86275e1afc45c84727656b22c56c0dfc4c" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;out&lt;/code&gt; tensor only has values 0 or 1 and is of the same shape as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返さ &lt;code&gt;out&lt;/code&gt; テンソルの値は0または1のみで、 &lt;code&gt;input&lt;/code&gt; と同じ形状です。</target>
        </trans-unit>
        <trans-unit id="0c1ca22658d19c73ead093d5a482f3f0748946cd" translate="yes" xml:space="preserve">
          <source>The returned Tensor&amp;rsquo;s data will be of size &lt;code&gt;T x B x *&lt;/code&gt;, where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence and &lt;code&gt;B&lt;/code&gt; is the batch size. If &lt;code&gt;batch_first&lt;/code&gt; is True, the data will be transposed into &lt;code&gt;B x T x *&lt;/code&gt; format.</source>
          <target state="translated">返されるTensorのデータのサイズは &lt;code&gt;T x B x *&lt;/code&gt; になります。ここで、 &lt;code&gt;T&lt;/code&gt; は最長のシーケンスの長さ、 &lt;code&gt;B&lt;/code&gt; はバッチサイズです。場合 &lt;code&gt;batch_first&lt;/code&gt; がTrueで、データがに移調されます &lt;code&gt;B x T x *&lt;/code&gt; 形式を。</target>
        </trans-unit>
        <trans-unit id="a86d643d348528641013d71d94cfc9c2436b5f5a" translate="yes" xml:space="preserve">
          <source>The returned matrices will always be transposed, irrespective of the strides of the input matrices. That is, they will have stride &lt;code&gt;(1, m)&lt;/code&gt; instead of &lt;code&gt;(m, 1)&lt;/code&gt;.</source>
          <target state="translated">返される行列は、入力行列のストライドに関係なく、常に転置されます。つまり、 &lt;code&gt;(m, 1)&lt;/code&gt; ではなくストライド &lt;code&gt;(1, m)&lt;/code&gt; になります。</target>
        </trans-unit>
        <trans-unit id="e874ec4d7da2420c99c0303e4f7a958324a670ae" translate="yes" xml:space="preserve">
          <source>The returned tensor and &lt;code&gt;ndarray&lt;/code&gt; share the same memory. Modifications to the tensor will be reflected in the &lt;code&gt;ndarray&lt;/code&gt; and vice versa. The returned tensor is not resizable.</source>
          <target state="translated">返されたテンソルと &lt;code&gt;ndarray&lt;/code&gt; は同じメモリを共有します。テンソルへの変更は &lt;code&gt;ndarray&lt;/code&gt; に反映され、その逆も同様です。返されたテンソルはサイズ変更できません。</target>
        </trans-unit>
        <trans-unit id="93c4d85268a064ece907fbc8a5535a6f396d9a4e" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor</source>
          <target state="translated">返されたテンソルは、元のテンソルと同じストレージを使用しませ&lt;strong&gt;ん&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="52f5aa4074a60eef379e1658572f1bb6509ef7da" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor. If &lt;code&gt;out&lt;/code&gt; has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.</source>
          <target state="translated">返されたテンソルは、元のテンソルと同じストレージを使用しませ&lt;strong&gt;ん&lt;/strong&gt;。 &lt;code&gt;out&lt;/code&gt; の形状が予想と異なる場合は、サイレントに正しい形状に変更し、必要に応じて基になるストレージを再割り当てします。</target>
        </trans-unit>
        <trans-unit id="f46100c8ee7a830bab0c89e9873612006ce16b25" translate="yes" xml:space="preserve">
          <source>The returned tensor has the same number of dimensions as the original tensor (&lt;code&gt;input&lt;/code&gt;). The &lt;code&gt;dim&lt;/code&gt;th dimension has the same size as the length of &lt;code&gt;index&lt;/code&gt;; other dimensions have the same size as in the original tensor.</source>
          <target state="translated">返されるテンソルの次元数は、元のテンソル（ &lt;code&gt;input&lt;/code&gt; ）と同じです。 &lt;code&gt;dim&lt;/code&gt; 番目の次元は長さと同じ大きさを有する &lt;code&gt;index&lt;/code&gt; ; 他の寸法は、元のテンソルと同じサイズです。</target>
        </trans-unit>
        <trans-unit id="c72a07e12ac6ea766a45eac39941a4c32ee2a049" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions</source>
          <target state="translated">返されるテンソルは同じデータを共有し、要素数は同じでなければなりませんが、サイズは異なる場合があります。テンソルが表示されるためには、新しい表示サイズは元のサイズとストライドと互換性がなければならない、つまり、各新しい表示次元は元の次元の部分空間であるか、元の次元を横切るだけでなければならない。</target>
        </trans-unit>
        <trans-unit id="e400aae54e1364ef7f0ae0e4394533a68ef83da8" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same underlying data with this tensor.</source>
          <target state="translated">返されたテンソルは、このテンソルと同じ基礎データを共有しています。</target>
        </trans-unit>
        <trans-unit id="172826726b64a692f9e7f61dbdcbf5a0b8b36f39" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.</source>
          <target state="translated">返されたテンソルは入力テンソルとストレージを共有しているので、一方のテンソルの内容を変更すると他方のテンソルの内容も変更されます。</target>
        </trans-unit>
        <trans-unit id="28c770021d54b212e8c62be1e09939a0369312ae" translate="yes" xml:space="preserve">
          <source>The rows of &lt;code&gt;input&lt;/code&gt; do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; の行を合計して1にする必要はありませんが（この場合、値を重みとして使用します）、非負で有限であり、合計がゼロ以外である必要があります。</target>
        </trans-unit>
        <trans-unit id="67ad32a77054c34abcbd23ca0d1823d1b895c487" translate="yes" xml:space="preserve">
          <source>The running minimum/maximum</source>
          <target state="translated">走行中の最小/最大</target>
        </trans-unit>
        <trans-unit id="a731d56c62ca577d4cbb49839358e51e33b8ca80" translate="yes" xml:space="preserve">
          <source>The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. &amp;ldquo;Efficient simulation of the von Mises distribution.&amp;rdquo; Applied Statistics (1979): 152-157.</source>
          <target state="translated">フォンミーゼス分布のサンプリングアルゴリズムは、Best、DJ、およびNicholas I.Fisherの論文に基づいています。「フォンミーゼス分布の効率的なシミュレーション。」応用統計（1979）：152-157。</target>
        </trans-unit>
        <trans-unit id="4a088efcbb868e84088d5a0662d97d3ed3613744" translate="yes" xml:space="preserve">
          <source>The scale</source>
          <target state="translated">スケール</target>
        </trans-unit>
        <trans-unit id="425de0e2446cb018a534e98b7c1b54edd4cbbc42" translate="yes" xml:space="preserve">
          <source>The scale and zero point are computed as follows:</source>
          <target state="translated">スケールとゼロ点は以下のように計算されます。</target>
        </trans-unit>
        <trans-unit id="0e92d877ce880170691af2f314602d7f6a781ea7" translate="yes" xml:space="preserve">
          <source>The scale and zero point are then computed as in &lt;code&gt;MinMaxObserver&lt;/code&gt;.</source>
          <target state="translated">次に、 &lt;code&gt;MinMaxObserver&lt;/code&gt; の場合と同様に、スケールとゼロ点が計算されます。</target>
        </trans-unit>
        <trans-unit id="9a5fb7f0444cbf25c6894566e5c0f368d5dfcf33" translate="yes" xml:space="preserve">
          <source>The search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.</source>
          <target state="translated">min/max値の探索は、浮動小数点モデルに対する量子化誤差の最小化を確実にします。</target>
        </trans-unit>
        <trans-unit id="1eff8f651f1c1e81084414db9ad8e86ff7d386f5" translate="yes" xml:space="preserve">
          <source>The second argument can be a number or a tensor whose shape is &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the first argument.</source>
          <target state="translated">2番目の引数は、最初の引数で形状を&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;できる数値またはテンソルにすることができます。</target>
        </trans-unit>
        <trans-unit id="8fc5e267a8ddd57a603f273d62b726d831286e07" translate="yes" xml:space="preserve">
          <source>The sections below describe in details the effects and usages of these options.</source>
          <target state="translated">以下では、これらのオプションの効果や使用方法について詳しく説明します。</target>
        </trans-unit>
        <trans-unit id="35bad0572344e9ca340bcfea7b7e08eb1fb48094" translate="yes" xml:space="preserve">
          <source>The shape of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">テンソルの形状は、可変引数 &lt;code&gt;size&lt;/code&gt; によって定義されます。</target>
        </trans-unit>
        <trans-unit id="44ad26e34a9ec24516da93c641da630606e76045" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt; don&amp;rsquo;t need to match, but the total number of elements in each tensor need to be the same.</source>
          <target state="translated">&lt;a href=&quot;torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;torch.std#torch.std&quot;&gt; &lt;code&gt;std&lt;/code&gt; &lt;/a&gt;の形状は一致する必要はありませんが、各テンソルの要素の総数は同じである必要があります。</target>
        </trans-unit>
        <trans-unit id="7ab58315d98cc62bd1c43b5269219135f914a361" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">形状&lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;、 &lt;code&gt;tensor1&lt;/code&gt; 、および &lt;code&gt;tensor2&lt;/code&gt; がなければなりません&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="472d04a7b4ef19856846b867c2431ae86dfeee09" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; や &lt;code&gt;other&lt;/code&gt; の形状は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能でなければなりません。</target>
        </trans-unit>
        <trans-unit id="4bb8d54bf4e66fa1d51318971fbd84c5dd471f64" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">形状 &lt;code&gt;input&lt;/code&gt; 、 &lt;code&gt;tensor1&lt;/code&gt; 、および &lt;code&gt;tensor2&lt;/code&gt; がなければなりません&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="3c2b5afb57ab65d651f6e8673ddc0bb16bc251d9" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;. If &lt;code&gt;weight&lt;/code&gt; is a tensor, then the shapes of &lt;code&gt;weight&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;start&lt;/code&gt; と &lt;code&gt;end&lt;/code&gt; の形状は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能である必要があります。 &lt;code&gt;weight&lt;/code&gt; がテンソルの場合、 &lt;code&gt;weight&lt;/code&gt; 、 &lt;code&gt;start&lt;/code&gt; 、および &lt;code&gt;end&lt;/code&gt; 形状は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能である必要があります。</target>
        </trans-unit>
        <trans-unit id="30ae8ec04282dfa89f820e2638cc76a4c5308051" translate="yes" xml:space="preserve">
          <source>The shapes of the &lt;code&gt;mask&lt;/code&gt; tensor and the &lt;code&gt;input&lt;/code&gt; tensor don&amp;rsquo;t need to match, but they must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;mask&lt;/code&gt; テンソルと &lt;code&gt;input&lt;/code&gt; テンソルの形状は一致する必要はありませんが、&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能である必要があります。</target>
        </trans-unit>
        <trans-unit id="80ed96242b937c96d491a6608007e2c80ef6fbfe" translate="yes" xml:space="preserve">
          <source>The singular values are returned in descending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.</source>
          <target state="translated">特異値は降順で返されます。場合 &lt;code&gt;input&lt;/code&gt; 行列のバッチは、次いで、バッチ内の各行列の特異値が降順で戻されます。</target>
        </trans-unit>
        <trans-unit id="86c71484bcb812cb0f2e7260fb264f82086dbacb" translate="yes" xml:space="preserve">
          <source>The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for &lt;code&gt;offset&lt;/code&gt; other than</source>
          <target state="translated">新しい行列のサイズは、最後の入力次元のサイズの指定された対角線になるように計算されます。 &lt;code&gt;offset&lt;/code&gt; 以外の場合は注意してください</target>
        </trans-unit>
        <trans-unit id="0739eb19e36226e98c11fd96e8291d8a209e3a7e" translate="yes" xml:space="preserve">
          <source>The smallest positive representable number.</source>
          <target state="translated">正の表現可能な最小の数。</target>
        </trans-unit>
        <trans-unit id="cf0617c3e175f01c2b40e71995108022f6b64f95" translate="yes" xml:space="preserve">
          <source>The smallest representable number (typically &lt;code&gt;-max&lt;/code&gt;).</source>
          <target state="translated">表現可能な最小の数（通常は &lt;code&gt;-max&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="27d91cd31d6556c601166e6b57bbeb8fc2cd13fe" translate="yes" xml:space="preserve">
          <source>The smallest representable number such that &lt;code&gt;1.0 + eps != 1.0&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;1.0 + eps != 1.0&lt;/code&gt; ような表現可能な最小の数。</target>
        </trans-unit>
        <trans-unit id="ff2ebfdbbf9420dca19c2e2c489305049c84e69e" translate="yes" xml:space="preserve">
          <source>The smallest representable number.</source>
          <target state="translated">表現可能な最小の数。</target>
        </trans-unit>
        <trans-unit id="6e4db462bf55509b809c59ea2298379ae2dba77f" translate="yes" xml:space="preserve">
          <source>The sources in &lt;code&gt;cuda_sources&lt;/code&gt; are concatenated into a separate &lt;code&gt;.cu&lt;/code&gt; file and prepended with &lt;code&gt;torch/types.h&lt;/code&gt;, &lt;code&gt;cuda.h&lt;/code&gt; and &lt;code&gt;cuda_runtime.h&lt;/code&gt; includes. The &lt;code&gt;.cpp&lt;/code&gt; and &lt;code&gt;.cu&lt;/code&gt; files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in &lt;code&gt;cuda_sources&lt;/code&gt; per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the &lt;code&gt;cpp_sources&lt;/code&gt; (and include its name in &lt;code&gt;functions&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;cuda_sources&lt;/code&gt; のソースは、個別の &lt;code&gt;.cu&lt;/code&gt; ファイルに連結され、 &lt;code&gt;torch/types.h&lt;/code&gt; 、 &lt;code&gt;cuda.h&lt;/code&gt; 、および &lt;code&gt;cuda_runtime.h&lt;/code&gt; インクルードが付加されます。 &lt;code&gt;.cpp&lt;/code&gt; ファイルと &lt;code&gt;.cu&lt;/code&gt; ファイルが別々にコンパイルが、最終的に1つのライブラリにリンクされています。 &lt;code&gt;cuda_sources&lt;/code&gt; 自体の関数に対してバインディングは生成されないことに注意してください。 CUDAカーネルにバインドするには、それを呼び出すC ++関数を作成し、このC ++関数を &lt;code&gt;cpp_sources&lt;/code&gt; の1つで宣言または定義する（そしてその名前を &lt;code&gt;functions&lt;/code&gt; 含める）必要があります。</target>
        </trans-unit>
        <trans-unit id="f06fdc0c2350c66f3a9d82727c760926c8016f15" translate="yes" xml:space="preserve">
          <source>The stashing logic saves and restores the RNG state for the current device and the device of all cuda Tensor arguments to the &lt;code&gt;run_fn&lt;/code&gt;. However, the logic has no way to anticipate if the user will move Tensors to a new device within the &lt;code&gt;run_fn&lt;/code&gt; itself. Therefore, if you move Tensors to a new device (&amp;ldquo;new&amp;rdquo; meaning not belonging to the set of [current device + devices of Tensor arguments]) within &lt;code&gt;run_fn&lt;/code&gt;, deterministic output compared to non-checkpointed passes is never guaranteed.</source>
          <target state="translated">スタッシュロジックが保存され、現在のデバイスとの全てのCUDAテンソル引数のデバイスのためにRNG状態を復元 &lt;code&gt;run_fn&lt;/code&gt; 。ただし、ロジックには、ユーザーが &lt;code&gt;run_fn&lt;/code&gt; 自体の中でTensorを新しいデバイスに移動するかどうかを予測する方法がありません。あなたが新しいデバイスにテンソルを移動した場合そのため、内（「新」[テンソル引数の現在のデバイス+デバイス]のセットに属していないという意味） &lt;code&gt;run_fn&lt;/code&gt; 保証されることはありません非チェックポイントのパスに比べて、決定論的出力。</target>
        </trans-unit>
        <trans-unit id="b7f3ab8a2df042d6e3025c18a145e6e14f77af25" translate="yes" xml:space="preserve">
          <source>The sum operation still operates over all the elements, and divides by</source>
          <target state="translated">和演算はまだすべての要素にわたって動作し、次のようにして除算します。</target>
        </trans-unit>
        <trans-unit id="b2531e155fe9e77a5cfae31abf7b6f59ffb9fcaa" translate="yes" xml:space="preserve">
          <source>The support of third-party backend is experimental and subject to change.</source>
          <target state="translated">サードパーティ製バックエンドのサポートは実験的なものであり、変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="2873a12c84aba239b7842369e12bb64236d9f288" translate="yes" xml:space="preserve">
          <source>The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once.</source>
          <target state="translated">テンソルはdlpackで表現されたオブジェクトとメモリを共有します。各dlpackは一度しか消費できないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="dc114e497003f25e6fca05f82fcba607f7d78078" translate="yes" xml:space="preserve">
          <source>The tensors &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">テンソル &lt;code&gt;condition&lt;/code&gt; 、 &lt;code&gt;x&lt;/code&gt; 、 &lt;code&gt;y&lt;/code&gt; は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能でなければなりません。</target>
        </trans-unit>
        <trans-unit id="ab7ff2ed427b3e162ddc9c1bc3618ade11c4bc8e" translate="yes" xml:space="preserve">
          <source>The torch package contains data structures for multi-dimensional tensors and mathematical operations over these are defined. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.</source>
          <target state="translated">torch パッケージには多次元テンソルのデータ構造が含まれており、これらに対する数学演算が定義されています。さらに、テンソルや任意の型を効率的にシリアライズするための多くのユーティリティや、その他の便利なユーティリティも提供しています。</target>
        </trans-unit>
        <trans-unit id="437652102ffae6443117ed89a3eae4a8f5b41da3" translate="yes" xml:space="preserve">
          <source>The tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor:</source>
          <target state="translated">トレーサは,トレースされた計算でいくつかの問題のあるパターンに対して警告を表示します.例として、Tensorのスライス(ビュー)上のインプレース代入を含む関数のトレースを見てみましょう。</target>
        </trans-unit>
        <trans-unit id="89850de90f112bfc18c8cdc61311b3872ebf2872" translate="yes" xml:space="preserve">
          <source>The tracer records the example inputs shape in the graph. In case the model should accept inputs of dynamic shape, you can utilize the parameter &lt;code&gt;dynamic_axes&lt;/code&gt; in export api.</source>
          <target state="translated">トレーサーは、入力形状の例をグラフに記録します。モデルが動的形状の入力を受け入れる必要がある場合は、エクスポートAPIでパラメーター &lt;code&gt;dynamic_axes&lt;/code&gt; を利用できます。</target>
        </trans-unit>
        <trans-unit id="da3842947e505833ecf56e0071c7d1669aedc528" translate="yes" xml:space="preserve">
          <source>The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.</source>
          <target state="translated">基になっているCUDAイベントは、イベントが最初に記録されたり、他のプロセスにエクスポートされたりしたときに、怠惰に初期化されます。作成後、同じデバイス上のストリームのみがイベントを記録できます。ただし、どのデバイス上のストリームでもイベントを待機させることができます。</target>
        </trans-unit>
        <trans-unit id="f2328cc72d77d5be4ac31362bf776708e1354f25" translate="yes" xml:space="preserve">
          <source>The unreduced (i.e. with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) loss can be described as:</source>
          <target state="translated">&lt;code&gt;reduction&lt;/code&gt; されて &lt;code&gt;'none'&lt;/code&gt; （つまり、削減が「なし」に設定されている）損失は、次のように説明できます。</target>
        </trans-unit>
        <trans-unit id="83a2adc9d7ff925b785017e994bf1c2dfff219e3" translate="yes" xml:space="preserve">
          <source>The unreduced loss (i.e., with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) can be described as:</source>
          <target state="translated">&lt;code&gt;reduction&lt;/code&gt; されて &lt;code&gt;'none'&lt;/code&gt; 損失（つまり、削減が「なし」に設定されている場合）は、次のように説明できます。</target>
        </trans-unit>
        <trans-unit id="55b435ee6855f1f093ca66cf72abe8bd2c30eb03" translate="yes" xml:space="preserve">
          <source>The upper triangular part of the matrix is defined as the elements on and above the diagonal.</source>
          <target state="translated">行列の上三角部分は、対角線上の要素とその上の要素として定義されます。</target>
        </trans-unit>
        <trans-unit id="04027d302afa8c6fa85d808794bef7b659457976" translate="yes" xml:space="preserve">
          <source>The use of &lt;code&gt;collate_fn&lt;/code&gt; is slightly different when automatic batching is enabled or disabled.</source>
          <target state="translated">自動バッチ処理が有効または無効の場合、 &lt;code&gt;collate_fn&lt;/code&gt; の使用は少し異なります。</target>
        </trans-unit>
        <trans-unit id="8ff79d6204f8beff86c1883967aad41f978dae9c" translate="yes" xml:space="preserve">
          <source>The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.</source>
          <target state="translated">このユーティリティは、ノードごとに1つ以上のプロセスが生成されるシングルノード分散トレーニングに使用できます。このユーティリティは、CPU トレーニングまたは GPU トレーニングのいずれかに使用することができます。ユーティリティをGPUトレーニングに使用する場合、各分散プロセスは1つのGPU上で動作します。これにより、シングルノードのトレーニング性能を十分に向上させることができます。また、マルチノード分散トレーニングにも使用することができ、マルチノード分散トレーニングの性能を向上させるために、各ノード上で複数のプロセスを生成します。特に、直接GPUをサポートしている複数のInfinibandインタフェースを持つシステムでは、それらのインタフェースを集約した通信帯域を利用することができるため、大きなメリットがあります。</target>
        </trans-unit>
        <trans-unit id="a8baea02cc4125823d2a9295d67b336747f5d060" translate="yes" xml:space="preserve">
          <source>The value held by this &lt;code&gt;Future&lt;/code&gt;. If the function (callback or RPC) creating the value has thrown an error, this &lt;code&gt;wait&lt;/code&gt; method will also throw an error.</source>
          <target state="translated">この &lt;code&gt;Future&lt;/code&gt; 持つ価値。値を作成する関数（コールバックまたはRPC）がエラーをスローした場合、この &lt;code&gt;wait&lt;/code&gt; メソッドもエラーをスローします。</target>
        </trans-unit>
        <trans-unit id="4c32d99e8bca557bc1750470ea69ae3bfb345050" translate="yes" xml:space="preserve">
          <source>The values of this class are lowercase strings, e.g., &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;. They can be accessed as attributes, e.g., &lt;code&gt;Backend.NCCL&lt;/code&gt;.</source>
          <target state="translated">このクラスの値は小文字の文字列です（例： &lt;code&gt;&quot;gloo&quot;&lt;/code&gt; )。これらは、 &lt;code&gt;Backend.NCCL&lt;/code&gt; などの属性としてアクセスできます。</target>
        </trans-unit>
        <trans-unit id="2d09c2230d5453858141229e7b0087efe234f737" translate="yes" xml:space="preserve">
          <source>The values of this class can be accessed as attributes, e.g., &lt;code&gt;ReduceOp.SUM&lt;/code&gt;. They are used in specifying strategies for reduction collectives, e.g., &lt;a href=&quot;#torch.distributed.reduce&quot;&gt;&lt;code&gt;reduce()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt;&lt;code&gt;all_reduce_multigpu()&lt;/code&gt;&lt;/a&gt;, etc.</source>
          <target state="translated">このクラスの値には、 &lt;code&gt;ReduceOp.SUM&lt;/code&gt; などの属性としてアクセスできます。これらは、&lt;a href=&quot;#torch.distributed.reduce&quot;&gt; &lt;code&gt;reduce()&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt; &lt;code&gt;all_reduce_multigpu()&lt;/code&gt; &lt;/a&gt;などの削減集合の戦略を指定する際に使用されます。</target>
        </trans-unit>
        <trans-unit id="539dcafbce59fbba03d94008a7b26e37638c0f1b" translate="yes" xml:space="preserve">
          <source>The world size of the process group -1, if not part of the group</source>
          <target state="translated">プロセスグループ -1 のワールドサイズ.</target>
        </trans-unit>
        <trans-unit id="a5d66dffec3f1eab9a22e5606deb2eaebb37f157" translate="yes" xml:space="preserve">
          <source>Then &lt;code&gt;dynamic axes&lt;/code&gt; can be defined either as:</source>
          <target state="translated">次に、 &lt;code&gt;dynamic axes&lt;/code&gt; は次のいずれかとして定義できます。</target>
        </trans-unit>
        <trans-unit id="cacf045e7460da89c00e2f44ce242ff34d557d4d" translate="yes" xml:space="preserve">
          <source>Then for any (supported) &lt;code&gt;input&lt;/code&gt; tensor the following equality holds:</source>
          <target state="translated">次に、（サポートされている） &lt;code&gt;input&lt;/code&gt; テンソルに対して、次の等式が成り立ちます。</target>
        </trans-unit>
        <trans-unit id="61c6ce8b4e502ae0f9cf574180a84ce71379a45a" translate="yes" xml:space="preserve">
          <source>Then run the following code in two different processes:</source>
          <target state="translated">次に、以下のコードを2つの異なるプロセスで実行します。</target>
        </trans-unit>
        <trans-unit id="1ce92152c376045e006c7df7e500a93b25016945" translate="yes" xml:space="preserve">
          <source>Then, you can run:</source>
          <target state="translated">ならば、走ればいい。</target>
        </trans-unit>
        <trans-unit id="9157fc1a5d0f6b3459cf4d13b451acd4e1ae988a" translate="yes" xml:space="preserve">
          <source>There are 2 main ways to initialize a process group:</source>
          <target state="translated">プロセスグループを初期化するには、主に2つの方法があります。</target>
        </trans-unit>
        <trans-unit id="989aa62e15f82179c46dd7bca86e4177e7b7bf45" translate="yes" xml:space="preserve">
          <source>There are a few main ways to create a tensor, depending on your use case.</source>
          <target state="translated">テンソルを作成するには、ユースケースに応じて主にいくつかの方法があります。</target>
        </trans-unit>
        <trans-unit id="448eaecf17e019efecc19b1280b7ffb9043292f0" translate="yes" xml:space="preserve">
          <source>There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</source>
          <target state="translated">Tensorsで定義されているインプレースランダムサンプリング関数もいくつかあります。クリックしてドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="4a50a4f02b41f0e4e7768bdf9e0d8e8e8f9ca103" translate="yes" xml:space="preserve">
          <source>There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables:</source>
          <target state="translated">cuDNNおよびCUDAの一部のバージョンでは、RNN関数に非決定論的な問題があることが知られています。以下の環境変数を設定することで、決定論的な動作を強制することができます。</target>
        </trans-unit>
        <trans-unit id="94f93e5cb61707d9a9f9f8c1e8bae87549a0661b" translate="yes" xml:space="preserve">
          <source>There are more examples in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;symbolic_opset9.py&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset10.py&quot;&gt;symbolic_opset10.py&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;symbolic_opset9.py&lt;/a&gt;、&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset10.py&quot;&gt;symbolic_opset10.pyに&lt;/a&gt;はさらに多くの例があります。</target>
        </trans-unit>
        <trans-unit id="e7b2c38d0b8818a513a12fc7c7e90b9479638b6e" translate="yes" xml:space="preserve">
          <source>There are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:</source>
          <target state="translated">与えられたPythonの関数/モジュールのトレースが基礎となるコードを代表するものではない場合、いくつかのエッジケースが存在します。これらのケースには以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="7b0666c05baae6bc9dd7801107606ece55efb258" translate="yes" xml:space="preserve">
          <source>There are two main usages:</source>
          <target state="translated">主に2つの用途があります。</target>
        </trans-unit>
        <trans-unit id="7b78c278fc5d3de5d25f299a98858079b45aa6eb" translate="yes" xml:space="preserve">
          <source>There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired &lt;code&gt;world_size&lt;/code&gt;. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.</source>
          <target state="translated">TCPを使用して初期化する方法は2つあり、どちらもすべてのプロセスから到達可能なネットワークアドレスと目的の &lt;code&gt;world_size&lt;/code&gt; が必要です。最初の方法では、ランク0プロセスに属するアドレスを指定する必要があります。この初期化方法では、すべてのプロセスが手動でランクを指定している必要があります。</target>
        </trans-unit>
        <trans-unit id="f2b97adc02fc81457ca80fb753c694becd93e274" translate="yes" xml:space="preserve">
          <source>There is a subtlety in using the &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; pattern in a &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; wrapped in &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;My recurrent network doesn&amp;rsquo;t work with data parallelism&lt;/a&gt; section in FAQ for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; で&lt;/a&gt;ラップされた&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; パターンを使用することには微妙な点があります。詳細については、FAQの&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;「リカレントネットワークがデータ並列処理で機能しない」&lt;/a&gt;セクションを参照してください。</target>
        </trans-unit>
        <trans-unit id="6bedff7a4f5803a09c69737d429b263e4a4e07c0" translate="yes" xml:space="preserve">
          <source>Therefore, indexing &lt;code&gt;output&lt;/code&gt; at the last dimension (column dimension) gives all values within a certain block.</source>
          <target state="translated">したがって、最後の次元（列次元）で &lt;code&gt;output&lt;/code&gt; にインデックスを付けると、特定のブロック内のすべての値が得られます。</target>
        </trans-unit>
        <trans-unit id="220cff043cb62e0e4cb5366907ebd80ead3bf366" translate="yes" xml:space="preserve">
          <source>Therefore, to invert an &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;normalized&lt;/code&gt; and &lt;code&gt;onesided&lt;/code&gt; arguments should be set identically for &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, and preferably a &lt;code&gt;signal_sizes&lt;/code&gt; is given to avoid size mismatch. See the example below for a case of size mismatch.</source>
          <target state="translated">したがって、&lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;を反転するには、 &lt;code&gt;normalized&lt;/code&gt; された引数と &lt;code&gt;onesided&lt;/code&gt; 引数を&lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;に対して同じように設定する必要があります。サイズの不一致を避けるために、 &lt;code&gt;signal_sizes&lt;/code&gt; を指定することをお勧めします。サイズが一致しない場合は、以下の例を参照してください。</target>
        </trans-unit>
        <trans-unit id="11de0478b61c8d1fd83658744b437989c835c987" translate="yes" xml:space="preserve">
          <source>These are the basic building block for graphs</source>
          <target state="translated">これらはグラフの基本的な構成要素です。</target>
        </trans-unit>
        <trans-unit id="8ca4969bd4e4273b5349ab2f675d16c1de44f187" translate="yes" xml:space="preserve">
          <source>These backends include:</source>
          <target state="translated">これらのバックエンドには、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="e0e365abd4e5be3113ea52d047c2f616e603945c" translate="yes" xml:space="preserve">
          <source>These ops don&amp;rsquo;t require a particular dtype for stability, but take multiple inputs and require that the inputs&amp;rsquo; dtypes match. If all of the inputs are &lt;code&gt;float16&lt;/code&gt;, the op runs in &lt;code&gt;float16&lt;/code&gt;. If any of the inputs is &lt;code&gt;float32&lt;/code&gt;, autocast casts all inputs to &lt;code&gt;float32&lt;/code&gt; and runs the op in &lt;code&gt;float32&lt;/code&gt;.</source>
          <target state="translated">これらの操作は、安定性のために特定のdtypeを必要としませんが、複数の入力を受け取り、入力のdtypeが一致する必要があります。すべての入力が &lt;code&gt;float16&lt;/code&gt; の場合、opはfloat16で実行され &lt;code&gt;float16&lt;/code&gt; 。入力のいずれかが &lt;code&gt;float32&lt;/code&gt; の場合、自動キャストはすべての入力を &lt;code&gt;float32&lt;/code&gt; にキャストし、float32でopを &lt;code&gt;float32&lt;/code&gt; ます。</target>
        </trans-unit>
        <trans-unit id="150d61b0973e4711130a967de9e4414841dbc8c3" translate="yes" xml:space="preserve">
          <source>These options are configured by the constructor arguments of a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;, which has signature:</source>
          <target state="translated">これらのオプションは、署名を持つ&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; の&lt;/a&gt;コンストラクター引数によって構成されます。</target>
        </trans-unit>
        <trans-unit id="3dd026a9fc19a074410394c15ee56c930e5686d0" translate="yes" xml:space="preserve">
          <source>These types and features from the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module are unavailble in TorchScript.</source>
          <target state="translated">&lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt;モジュールのこれらのタイプと機能は、TorchScriptでは使用できません。</target>
        </trans-unit>
        <trans-unit id="aca9bd729c973c4d3bc4eb7f3f8d4e953c82edfd" translate="yes" xml:space="preserve">
          <source>These unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member.</source>
          <target state="translated">これらはループを展開し、タプルの各メンバのボディを生成します。ボディは各メンバに対して正しくタイプチェックを行う必要があります。</target>
        </trans-unit>
        <trans-unit id="5042b3fea81612a742984a1fb55be5b675720283" translate="yes" xml:space="preserve">
          <source>Third-party backends</source>
          <target state="translated">サードパーティのバックエンド</target>
        </trans-unit>
        <trans-unit id="f9ab4d0d07c5bfd671637ba9723d67603253bcae" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;momentum&lt;/code&gt; argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is</source>
          <target state="translated">この &lt;code&gt;momentum&lt;/code&gt; 議論は、オプティマイザークラスや従来の運動量の概念で使用されているものとは異なります。数学的には、ここで統計を実行するための更新ルールは次のとおりです。</target>
        </trans-unit>
        <trans-unit id="4d057f12e63c19fa40f83b9e371a09dbaf3641ca" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;setuptools.build_ext&lt;/code&gt; subclass takes care of passing the minimum required compiler flags (e.g. &lt;code&gt;-std=c++14&lt;/code&gt;) as well as mixed C++/CUDA compilation (and support for CUDA files in general).</source>
          <target state="translated">この &lt;code&gt;setuptools.build_ext&lt;/code&gt; サブクラスは、最低限必要なコンパイラフラグ（例： &lt;code&gt;-std=c++14&lt;/code&gt; ）と、混合C ++ / CUDAコンパイル（および一般的なCUDAファイルのサポート）の受け渡しを処理します。</target>
        </trans-unit>
        <trans-unit id="48b0bfc66718d9ed75062a9f5d57a687fd7749c9" translate="yes" xml:space="preserve">
          <source>This API is in beta and may change in the near future.</source>
          <target state="translated">このAPIはベータ版であり、近い将来変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="53d04b24e9fbd24d72acb9f431a964889b4c7cf6" translate="yes" xml:space="preserve">
          <source>This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.</source>
          <target state="translated">このAPIはベータ版です。関数のシグネチャが変更される可能性は非常に低いですが、この安定版を検討する前にパフォーマンスの大幅な改善が計画されています。</target>
        </trans-unit>
        <trans-unit id="c994160251b3f47ef8b0dd185e96a7b1ad0ee446" translate="yes" xml:space="preserve">
          <source>This API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don&amp;rsquo;t have requires_grad set, you can use a lambda to capture them. For example, for a function &lt;code&gt;f&lt;/code&gt; that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as &lt;code&gt;f(input, constant, flag=flag)&lt;/code&gt; you can use it as &lt;code&gt;functional.jacobian(lambda x: f(x, constant, flag=flag), input)&lt;/code&gt;.</source>
          <target state="translated">このAPIは、入力としてTensorのみを受け取り、Tensorのみを返すユーザー提供の関数で機能します。関数がTensorsまたはrequires_gradが設定されていないTensors以外の他の引数を取る場合は、ラムダを使用してそれらをキャプチャできます。たとえば、3つの入力を受け取る関数 &lt;code&gt;f&lt;/code&gt; の場合、ヤコビアンが必要なテンソル、定数と見なされる別のテンソル、および &lt;code&gt;f(input, constant, flag=flag)&lt;/code&gt; としてのブールフラグを &lt;code&gt;functional.jacobian(lambda x: f(x, constant, flag=flag), input)&lt;/code&gt; として使用できます。 jacobian（lambda x：f（x、constant、flag = flag）、input）。</target>
        </trans-unit>
        <trans-unit id="33a32d5fcbf1c45d06b8358ae2591cd5047cff16" translate="yes" xml:space="preserve">
          <source>This allows better BC support for &lt;a href=&quot;#torch.nn.Module.load_state_dict&quot;&gt;&lt;code&gt;load_state_dict()&lt;/code&gt;&lt;/a&gt;. In &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt;, the version number will be saved as in the attribute &lt;code&gt;_metadata&lt;/code&gt; of the returned state dict, and thus pickled. &lt;code&gt;_metadata&lt;/code&gt; is a dictionary with keys that follow the naming convention of state dict. See &lt;code&gt;_load_from_state_dict&lt;/code&gt; on how to use this information in loading.</source>
          <target state="translated">これにより、&lt;a href=&quot;#torch.nn.Module.load_state_dict&quot;&gt; &lt;code&gt;load_state_dict()&lt;/code&gt; の&lt;/a&gt;BCサポートが向上します。で&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;、バージョン番号は属性のように保存されます &lt;code&gt;_metadata&lt;/code&gt; 返された状態のdictの、ひいては漬け。 &lt;code&gt;_metadata&lt;/code&gt; は、statedictの命名規則に従ったキーを持つ辞書です。ロード時にこの情報を使用する方法については、 &lt;code&gt;_load_from_state_dict&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="09b607c5ebd52b97aa3c6cf95a474cc71af1ac47" translate="yes" xml:space="preserve">
          <source>This allows for different samples to have variable amounts of target classes.</source>
          <target state="translated">これにより、異なるサンプルが可変量のターゲットクラスを持つことができます。</target>
        </trans-unit>
        <trans-unit id="dc869bc2254cbb18e88bb0a8c7e186c575d69c2e" translate="yes" xml:space="preserve">
          <source>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.</source>
          <target state="translated">これはまた、関連するパラメータやバッファを別のオブジェクトにします。そのため、最適化中にモジュールがGPU上で動作するのであれば、オプティマイザを構築する前に呼ばれるべきです。</target>
        </trans-unit>
        <trans-unit id="cd79f292616deed21e20e4449558742671a5f03e" translate="yes" xml:space="preserve">
          <source>This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set &lt;code&gt;use_external_data_format&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; to successfully export such models.</source>
          <target state="translated">この引数により、大規模モデルをONNXにエクスポートできます。protobufのサイズ制限のため、2GBを超えるモデルを1つのファイルにエクスポートすることはできません。このようなモデルを正常にエクスポートするには、ユーザーは &lt;code&gt;use_external_data_format&lt;/code&gt; を &lt;code&gt;True&lt;/code&gt; に設定する必要があります。</target>
        </trans-unit>
        <trans-unit id="37b077e70d126ef1c4f52c0e784fa4698cbe6a7d" translate="yes" xml:space="preserve">
          <source>This attribute is &lt;code&gt;None&lt;/code&gt; by default and becomes a Tensor the first time a call to &lt;a href=&quot;#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; computes gradients for &lt;code&gt;self&lt;/code&gt;. The attribute will then contain the gradients computed and future calls to &lt;a href=&quot;#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; will accumulate (add) gradients into it.</source>
          <target state="translated">この属性はデフォルトでは &lt;code&gt;None&lt;/code&gt; であり、&lt;a href=&quot;#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; の&lt;/a&gt;呼び出しが &lt;code&gt;self&lt;/code&gt; の勾配を初めて計算するときにテンソルになります。次に、属性には計算されたグラデーションが含まれ、&lt;a href=&quot;#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt;への今後の呼び出しはグラデーションを蓄積（追加）します。</target>
        </trans-unit>
        <trans-unit id="d9aed446ae3aa4dc28d8d65d1c929ff54cb2a74f" translate="yes" xml:space="preserve">
          <source>This attribute is &lt;code&gt;None&lt;/code&gt; by default and becomes a Tensor the first time a call to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; computes gradients for &lt;code&gt;self&lt;/code&gt;. The attribute will then contain the gradients computed and future calls to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; will accumulate (add) gradients into it.</source>
          <target state="translated">この属性はデフォルトでは &lt;code&gt;None&lt;/code&gt; であり、&lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; の&lt;/a&gt;呼び出しが &lt;code&gt;self&lt;/code&gt; の勾配を初めて計算するときにテンソルになります。次に、属性には計算されたグラデーションが含まれ、&lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt;への今後の呼び出しはグラデーションを蓄積（追加）します。</target>
        </trans-unit>
        <trans-unit id="27eaef81d3fd1f9e704b27f64a092d8671cfb020" translate="yes" xml:space="preserve">
          <source>This can be called as</source>
          <target state="translated">として呼び出すことができます。</target>
        </trans-unit>
        <trans-unit id="90fa23d66ed39d5f59f1b4a16b52b28d18188c72" translate="yes" xml:space="preserve">
          <source>This can be useful to display periodically during training, or when handling out-of-memory exceptions.</source>
          <target state="translated">これは、トレーニング中に定期的に表示したり、メモリ切れの例外を処理するときに便利です。</target>
        </trans-unit>
        <trans-unit id="0c207d7ed4d817a917f51b1a12e087c64e12c094" translate="yes" xml:space="preserve">
          <source>This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the &lt;a href=&quot;#torch.optim.Optimizer&quot;&gt;&lt;code&gt;Optimizer&lt;/code&gt;&lt;/a&gt; as training progresses.</source>
          <target state="translated">これは、事前にトレーニングされたネットワークを微調整するときに役立ちます。フリーズされたレイヤーをトレーニング可能にして、トレーニングの進行に合わせて&lt;a href=&quot;#torch.optim.Optimizer&quot;&gt; &lt;code&gt;Optimizer&lt;/code&gt; &lt;/a&gt;追加できるからです。</target>
        </trans-unit>
        <trans-unit id="36c9a312367d269d922b679dae091acec9f12a5e" translate="yes" xml:space="preserve">
          <source>This can be useful when there is a need to create classes with the same constructor arguments, but different instances.</source>
          <target state="translated">これは、同じコンストラクタの引数を持つクラスを作成する必要があるが、異なるインスタンスを作成する必要がある場合に便利です。</target>
        </trans-unit>
        <trans-unit id="e5eeeb2d9e42f39a239e76c72a9314e3136e5540" translate="yes" xml:space="preserve">
          <source>This can then be visualized with TensorBoard, which should be installable and runnable with:</source>
          <target state="translated">これをTensorBoardで可視化することができるので、TensorBoardでインストールして実行できるようにする必要があります。</target>
        </trans-unit>
        <trans-unit id="2306bf42c44895d1fc76396938e439c137c2dffb" translate="yes" xml:space="preserve">
          <source>This class can be directly called to parse the string, e.g., &lt;code&gt;Backend(backend_str)&lt;/code&gt; will check if &lt;code&gt;backend_str&lt;/code&gt; is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., &lt;code&gt;Backend(&quot;GLOO&quot;)&lt;/code&gt; returns &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;.</source>
          <target state="translated">このクラスを直接呼び出して文字列を解析できます。たとえば、 &lt;code&gt;Backend(backend_str)&lt;/code&gt; は、 &lt;code&gt;backend_str&lt;/code&gt; が有効かどうかを確認し、有効な場合は解析された小文字の文字列を返します。また、大文字の文字列も受け入れます。たとえば、 &lt;code&gt;Backend(&quot;GLOO&quot;)&lt;/code&gt; は &lt;code&gt;&quot;gloo&quot;&lt;/code&gt; を返します。</target>
        </trans-unit>
        <trans-unit id="5ef03a595a40d9415bbad765363aad2d964d5b16" translate="yes" xml:space="preserve">
          <source>This class does not provide a &lt;code&gt;forward&lt;/code&gt; hook. Instead, you must use one of the underlying functions (e.g. &lt;code&gt;add&lt;/code&gt;).</source>
          <target state="translated">このクラスは &lt;code&gt;forward&lt;/code&gt; フックを提供しません。代わりに、基礎となる関数の1つを使用する必要があります（例： &lt;code&gt;add&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="c851561f849ef1eb6b901d2edf564052e34c6c02" translate="yes" xml:space="preserve">
          <source>This class has three built-in policies, as put forth in the paper:</source>
          <target state="translated">このクラスは、論文で述べられているように、3つのビルトインポリシーを持っています。</target>
        </trans-unit>
        <trans-unit id="3ddd34a2492252eba3a31d1accbd978513965aba" translate="yes" xml:space="preserve">
          <source>This class is an intermediary between the &lt;code&gt;Distribution&lt;/code&gt; class and distributions which belong to an exponential family mainly to check the correctness of the &lt;code&gt;.entropy()&lt;/code&gt; and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).</source>
          <target state="translated">このクラスは、 &lt;code&gt;Distribution&lt;/code&gt; クラスと、主に &lt;code&gt;.entropy()&lt;/code&gt; および分析的なKL発散メソッドの正確さをチェックするための指数型分布族に属する分布との間の仲介です。このクラスを使用して、ADフレームワークとBregmanダイバージェンスを使用してエントロピーとKLダイバージェンスを計算します（提供：FrankNielsenとRichardNock、指数型分布族のエントロピーとクロスエントロピー）。</target>
        </trans-unit>
        <trans-unit id="dc115d83b6af36150f881200c064c4d238e34462" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;.</source>
          <target state="translated">このクラスは、 &lt;code&gt;interpolate()&lt;/code&gt; を優先して非推奨になりました。</target>
        </trans-unit>
        <trans-unit id="c13ac423de7a3c762fee4c3a9510054ffdd5d384" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;. It is equivalent to &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="translated">このクラスは、 &lt;code&gt;interpolate()&lt;/code&gt; を優先して非推奨になりました。それは同等です &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d90500fc17196cb70e7a700371ed43a1a87eeb32" translate="yes" xml:space="preserve">
          <source>This class is useful to assemble different existing dataset streams. The chainning operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.</source>
          <target state="translated">このクラスは、異なる既存のデータセットのストリームをアセンブルするのに便利です。チェイニング操作はオンザフライで行われるので、このクラスを使って大規模なデータセットを連結すると効率的です。</target>
        </trans-unit>
        <trans-unit id="af734602c7eaf1931d607624127baafea2243648" translate="yes" xml:space="preserve">
          <source>This class is useful to assemble different existing datasets.</source>
          <target state="translated">このクラスは、異なる既存のデータセットを組み立てるのに便利です。</target>
        </trans-unit>
        <trans-unit id="f63c41dd97936df60be517dad06b50e25bfe14d0" translate="yes" xml:space="preserve">
          <source>This class uses &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; in order to retrieve the gradients for specific parameters.</source>
          <target state="translated">このクラスは、特定のパラメーターのグラデーションを取得するために&lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt; &lt;code&gt;get_gradients()&lt;/code&gt; &lt;/a&gt;を使用します。</target>
        </trans-unit>
        <trans-unit id="1862d470ef8a5cd214866c60dc3b2e18ed9571b7" translate="yes" xml:space="preserve">
          <source>This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().</source>
          <target state="translated">この集合体は、グループ全体がこの関数に入るまでプロセスをブロックします。</target>
        </trans-unit>
        <trans-unit id="43ff3e72065cb78dedbfc0cb5795ef6d313a86b8" translate="yes" xml:space="preserve">
          <source>This composition also works for &lt;code&gt;nn.Module&lt;/code&gt;s as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module.</source>
          <target state="translated">この構成は &lt;code&gt;nn.Module&lt;/code&gt; でも機能し、スクリプトモジュールのメソッドから呼び出すことができるトレースを使用してサブモジュールを生成するために使用できます。</target>
        </trans-unit>
        <trans-unit id="d56b8d3c3a6cd08249ac0400c9bd4fa666010028" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given &lt;code&gt;module&lt;/code&gt; by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.</source>
          <target state="translated">このコンテナーは、バッチディメンションでチャンク化することにより、指定されたデバイス間で入力を分割することにより、指定された &lt;code&gt;module&lt;/code&gt; アプリケーションを並列化します（他のオブジェクトはデバイスごとに1回コピーされます）。フォワードパスでは、モジュールは各デバイスに複製され、各レプリカが入力の一部を処理します。後方パス中に、各レプリカからの勾配が元のモジュールに合計されます。</target>
        </trans-unit>
        <trans-unit id="3e99fa6e39b3592c4eea5075ef0e0720a6948ff1" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.</source>
          <target state="translated">このコンテナは、バッチ次元でのチャンキングによって指定されたデバイス間で入力を分割することによって、指定されたモジュールのアプリケーションを並列化します。モジュールは各マシンと各デバイス上で複製され、各複製は入力の一部を処理します。バックワードパスの間、各ノードからの勾配が平均化されます。</target>
        </trans-unit>
        <trans-unit id="c09652531ac8f4b111870bcb71b813e31279457c" translate="yes" xml:space="preserve">
          <source>This context manager is thread local; it will not affect computation in other threads.</source>
          <target state="translated">このコンテキストマネージャーはスレッドローカルで、他のスレッドでの計算には影響しません。</target>
        </trans-unit>
        <trans-unit id="26ef7031b1f75f10e86902c0f567ce05a4ea44b5" translate="yes" xml:space="preserve">
          <source>This context manager will keep track of already-joined DDP processes, and &amp;ldquo;shadow&amp;rdquo; the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes.</source>
          <target state="translated">このコンテキストマネージャーは、すでに参加しているDDPプロセスを追跡し、参加していないDDPプロセスによって作成されたものと一致するように集合的な通信操作を挿入することにより、フォワードパスとバックワードパスを「シャドウ」します。これにより、各集合呼び出しに、すでに参加しているDDPプロセスによる対応する呼び出しが確実に行われ、プロセス間で不均一な入力を使用してトレーニングするときに発生するハングやエラーが防止されます。</target>
        </trans-unit>
        <trans-unit id="09d1e555fd19e686f48ac7306fa2525b334ba962" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt;&lt;code&gt;nn.LogSoftmax()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;nn.NLLLoss()&lt;/code&gt;&lt;/a&gt; in one single class.</source>
          <target state="translated">この基準は、&lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt; &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt; &lt;code&gt;nn.NLLLoss()&lt;/code&gt; &lt;/a&gt;を1つのクラスにまとめたものです。</target>
        </trans-unit>
        <trans-unit id="925d7f6dcda18d5ae787b2ea0d4e4cf02702d590" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;log_softmax&lt;/code&gt; and &lt;code&gt;nll_loss&lt;/code&gt; in a single function.</source>
          <target state="translated">この基準は、 &lt;code&gt;log_softmax&lt;/code&gt; と &lt;code&gt;nll_loss&lt;/code&gt; を1つの関数に組み合わせたものです。</target>
        </trans-unit>
        <trans-unit id="c7ea31e3bc10ddc7b5db55c0a7c9d23739bfa2db" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; and &lt;code&gt;nn.NLLLoss()&lt;/code&gt; in one single class.</source>
          <target state="translated">この基準は、 &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; と &lt;code&gt;nn.NLLLoss()&lt;/code&gt; を1つのクラスにまとめたものです。</target>
        </trans-unit>
        <trans-unit id="4a017ebd876454e242cede3ce02b356f5f295daa" translate="yes" xml:space="preserve">
          <source>This criterion expects a &lt;code&gt;target&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; of the same size as the &lt;code&gt;input&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">この基準は、 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;Tensor&lt;/code&gt; と同じサイズの &lt;code&gt;target&lt;/code&gt; &lt;code&gt;Tensor&lt;/code&gt; を想定しています。</target>
        </trans-unit>
        <trans-unit id="c10376150f504915e5bd847798be0f3a52dc669c" translate="yes" xml:space="preserve">
          <source>This criterion expects a class index in the range</source>
          <target state="translated">この基準は、範囲内のクラス・インデックスを求めます。</target>
        </trans-unit>
        <trans-unit id="4c181079125dc2e25b97bd2d3da70b20b81a75a7" translate="yes" xml:space="preserve">
          <source>This decorator also works with RRef helpers, i.e., . &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_sync&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_sync()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_async&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_async()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.rpc.RRef.remote&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.remote()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">このデコレータは、RRefヘルパーとも連携します。&lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_sync&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.rpc_sync()&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_async&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.rpc_async()&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;#torch.distributed.rpc.RRef.remote&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.remote()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ba581390e34d5e57c67e068b92dac8e4709b3ad6" translate="yes" xml:space="preserve">
          <source>This decorator indicates that a method on an &lt;code&gt;nn.Module&lt;/code&gt; is used as an entry point into a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and should be compiled.</source>
          <target state="translated">このデコレータは、上の方法いることを示し &lt;code&gt;nn.Module&lt;/code&gt; がへのエントリポイントとして使用され&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;コンパイルされるべきです。</target>
        </trans-unit>
        <trans-unit id="174b3d4b823448bf6302b0b53449be4c6d5ad12e" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.</source>
          <target state="translated">このデコレータは、関数やメソッドを無視してPythonの関数として残すことをコンパイラに示します。</target>
        </trans-unit>
        <trans-unit id="6d10437bc80b062c46cb5ff8be0bb49322ddb59f" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function. This allows you to leave code in your model that is not yet TorchScript compatible. If called from TorchScript, ignored functions will dispatch the call to the Python interpreter. Models with ignored functions cannot be exported; use &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">このデコレータは、関数またはメソッドを無視してPython関数として残す必要があることをコンパイラに示します。これにより、TorchScriptとまだ互換性のないコードをモデルに残すことができます。TorchScriptから呼び出された場合、無視された関数はPythonインタープリターに呼び出しをディスパッチします。関数が無視されたモデルはエクスポートできません。代わりに&lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt;を使用してください。</target>
        </trans-unit>
        <trans-unit id="1508df53ef2be98f34c189e45c94ca9319ce75f2" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.</source>
          <target state="translated">このデコレータは、関数やメソッドを無視して例外を発生させることをコンパイラに指示します。</target>
        </trans-unit>
        <trans-unit id="75d6e9d0a82393986fc62b4441accfe0e14ee3a9" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.</source>
          <target state="translated">このデコレータは、関数やメソッドを無視して例外の発生に置き換えることをコンパイラに指示します。これにより、TorchScriptと互換性のないコードをモデルに残しておいても、モデルをエクスポートすることができます。</target>
        </trans-unit>
        <trans-unit id="8952c5fbe2a12cf21923d9bc22bc2c081674eb1e" translate="yes" xml:space="preserve">
          <source>This defines</source>
          <target state="translated">これは</target>
        </trans-unit>
        <trans-unit id="04bcd4a4cb1d0ee1f357d9037605979a3155b988" translate="yes" xml:space="preserve">
          <source>This depends on the &lt;code&gt;spawn&lt;/code&gt; start method in Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; package.</source>
          <target state="translated">これは、に依存 &lt;code&gt;spawn&lt;/code&gt; Pythonの中にstartメソッド &lt;code&gt;multiprocessing&lt;/code&gt; パッケージ。</target>
        </trans-unit>
        <trans-unit id="e40f466c8c675241a4989cdf735bce5f4ddce6bc" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?orgqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-orgqr&quot;&gt;LAPACK documentation for orgqr&lt;/a&gt; for further details.</source>
          <target state="translated">これにより、基盤となるLAPACK関数 &lt;code&gt;?orgqr&lt;/code&gt; が直接呼び出されます。詳細&lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-orgqr&quot;&gt;については、orgqrのLAPACKドキュメントを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="9fb3c5a1f24bdd83af6fd14c07d4d7ef76ec7527" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?ormqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-ormqr&quot;&gt;LAPACK documentation for ormqr&lt;/a&gt; for further details.</source>
          <target state="translated">これにより、基盤となるLAPACK関数 &lt;code&gt;?ormqr&lt;/code&gt; が直接呼び出されます。詳細&lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-ormqr&quot;&gt;については、ormqrのLAPACKドキュメントを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="1a44ae2c253871828f29057a792173301af10f61" translate="yes" xml:space="preserve">
          <source>This does two things: - Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function. - Any backward computation that generate &amp;ldquo;nan&amp;rdquo; value will raise an error.</source>
          <target state="translated">これは2つのことを行います。-検出を有効にしてフォワードパスを実行すると、バックワードパスは、失敗したバックワード関数を作成したフォワード操作のトレースバックを出力できます。-「nan」値を生成する後方計算はエラーを発生させます。</target>
        </trans-unit>
        <trans-unit id="d7d3fb60afba5b578d1ed91b3b2d20aba1664bb0" translate="yes" xml:space="preserve">
          <source>This error usually means that the method you are tracing uses a module&amp;rsquo;s parameters and you are passing the module&amp;rsquo;s method instead of the module instance (e.g. &lt;code&gt;my_module_instance.forward&lt;/code&gt; vs &lt;code&gt;my_module_instance&lt;/code&gt;).</source>
          <target state="translated">このエラーは通常、トレースしているメソッドがモジュールのパラメーターを使用し、モジュールインスタンスの代わりにモジュールのメソッドを渡していることを意味します（例： &lt;code&gt;my_module_instance.forward&lt;/code&gt; と &lt;code&gt;my_module_instance&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="c753cc91ef9f274f764a4ae41747b732c7fe2261" translate="yes" xml:space="preserve">
          <source>This feature is in beta, and its design and implementation may change in the future.</source>
          <target state="translated">この機能はベータ版であり、今後設計や実装が変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="b7614e828d8ac51033f6b1d436da1e7ec19a65d5" translate="yes" xml:space="preserve">
          <source>This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; object by accessing its &lt;code&gt;.data&lt;/code&gt; attribute.</source>
          <target state="translated">この関数は、少なくとも2つの次元を持つすべての入力を受け入れます。これを適用してラベルをパックし、RNNの出力をラベルとともに使用して損失を直接計算できます。Tensorは、その &lt;code&gt;.data&lt;/code&gt; 属性にアクセスすることにより、&lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; &lt;/a&gt;オブジェクトから取得できます。</target>
        </trans-unit>
        <trans-unit id="f5cfef34a5b604499ec2d99f480d0835ab4472fc" translate="yes" xml:space="preserve">
          <source>This function accumulates gradients in the leaves - you might need to zero &lt;code&gt;.grad&lt;/code&gt; attributes or set them to &lt;code&gt;None&lt;/code&gt; before calling it. See &lt;a href=&quot;#default-grad-layouts&quot;&gt;Default gradient layouts&lt;/a&gt; for details on the memory layout of accumulated gradients.</source>
          <target state="translated">この関数は、葉にグラデーションを蓄積します。呼び出す前に、 &lt;code&gt;.grad&lt;/code&gt; 属性をゼロにするか、 &lt;code&gt;None&lt;/code&gt; に設定する必要がある場合があります。参照してください。&lt;a href=&quot;#default-grad-layouts&quot;&gt;デフォルト勾配レイアウト&lt;/a&gt;累積勾配のメモリレイアウトの詳細については、を。</target>
        </trans-unit>
        <trans-unit id="97ac98bd75da52df5620120c1ca469a7f25204b3" translate="yes" xml:space="preserve">
          <source>This function accumulates gradients in the leaves - you might need to zero &lt;code&gt;.grad&lt;/code&gt; attributes or set them to &lt;code&gt;None&lt;/code&gt; before calling it. See &lt;a href=&quot;autograd#default-grad-layouts&quot;&gt;Default gradient layouts&lt;/a&gt; for details on the memory layout of accumulated gradients.</source>
          <target state="translated">この関数は、葉にグラデーションを蓄積します。呼び出す前に、 &lt;code&gt;.grad&lt;/code&gt; 属性をゼロにするか、 &lt;code&gt;None&lt;/code&gt; に設定する必要がある場合があります。参照してください。&lt;a href=&quot;autograd#default-grad-layouts&quot;&gt;デフォルト勾配レイアウト&lt;/a&gt;累積勾配のメモリレイアウトの詳細については、を。</target>
        </trans-unit>
        <trans-unit id="5919d90354daacc619c8dc367427cc669c661c6e" translate="yes" xml:space="preserve">
          <source>This function behaves exactly like &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;, but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of &lt;a href=&quot;#torch.utils.cpp_extension.load_inline&quot;&gt;&lt;code&gt;load_inline()&lt;/code&gt;&lt;/a&gt; is identical to &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数は&lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt; &lt;code&gt;load()&lt;/code&gt; &lt;/a&gt;とまったく同じように動作しますが、ソースをファイル名ではなく文字列として受け取ります。これらの文字列はビルドディレクトリ内のファイルに保存され、その後、&lt;a href=&quot;#torch.utils.cpp_extension.load_inline&quot;&gt; &lt;code&gt;load_inline()&lt;/code&gt; の&lt;/a&gt;動作はload（）と同じになり&lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt; &lt;code&gt;load()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4742c80d8253ffcd737e46fcd5dffcfb774bfb9a" translate="yes" xml:space="preserve">
          <source>This function calculates all eigenvalues (and vectors) of &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="translated">この関数は、 &lt;code&gt;input&lt;/code&gt; すべての固有値（およびベクトル）を次のように計算します。</target>
        </trans-unit>
        <trans-unit id="2e52ac392528ad63a53ea2de90d126a45274fb82" translate="yes" xml:space="preserve">
          <source>This function can be called in an interleaved way.</source>
          <target state="translated">この関数は、インターリーブで呼び出すことができます。</target>
        </trans-unit>
        <trans-unit id="f591ac1d01d2706090dd5158a48cfafbf7078fcd" translate="yes" xml:space="preserve">
          <source>This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the &lt;code&gt;ord&lt;/code&gt; parameter.</source>
          <target state="translated">この関数は、縮小次元の数と &lt;code&gt;ord&lt;/code&gt; パラメーターの値の両方に応じて、8つの異なるタイプの行列ノルムの1つ、または無限の数のベクトルノルムの1つを計算できます。</target>
        </trans-unit>
        <trans-unit id="a99e80c93a86e3189bf95a219e4fc70666128f91" translate="yes" xml:space="preserve">
          <source>This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.</source>
          <target state="translated">この関数はバージョン0.4.1でシグネチャを変更しました。以前のシグネチャで呼び出すとエラーが発生したり、正しくない結果が返ってきたりすることがあります。</target>
        </trans-unit>
        <trans-unit id="2742ba18db0e477006e0d4a879a88ee8aaa73a1c" translate="yes" xml:space="preserve">
          <source>This function checks if all &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; satisfy the condition:</source>
          <target state="translated">この関数は、すべての &lt;code&gt;input&lt;/code&gt; および &lt;code&gt;other&lt;/code&gt; 条件を満たすかどうかをチェックします。</target>
        </trans-unit>
        <trans-unit id="63c734a4df7eea8aea1bf4116fd041c2a840b41a" translate="yes" xml:space="preserve">
          <source>This function checks that backpropagating through the gradients computed to the given &lt;code&gt;grad_outputs&lt;/code&gt; are correct.</source>
          <target state="translated">この関数は、指定された &lt;code&gt;grad_outputs&lt;/code&gt; に計算された勾配を介したバックプロパゲーションが正しいことを確認します。</target>
        </trans-unit>
        <trans-unit id="4ad58a0a805b7dcf7c19d64d7ab1a5b0774c3bf4" translate="yes" xml:space="preserve">
          <source>This function does exact same thing as &lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt;&lt;code&gt;torch.addmm()&lt;/code&gt;&lt;/a&gt; in the forward, except that it supports backward for sparse matrix &lt;code&gt;mat1&lt;/code&gt;. &lt;code&gt;mat1&lt;/code&gt; need to have &lt;code&gt;sparse_dim = 2&lt;/code&gt;. Note that the gradients of &lt;code&gt;mat1&lt;/code&gt; is a coalesced sparse tensor.</source>
          <target state="translated">この関数は、スパース行列 &lt;code&gt;mat1&lt;/code&gt; の後方サポートを除いて、前方の&lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt; &lt;code&gt;torch.addmm()&lt;/code&gt; &lt;/a&gt;とまったく同じことを行います。 &lt;code&gt;mat1&lt;/code&gt; は &lt;code&gt;sparse_dim = 2&lt;/code&gt; 必要があります。 &lt;code&gt;mat1&lt;/code&gt; の勾配は、合体したスパーステンソルであることに注意してください。</target>
        </trans-unit>
        <trans-unit id="b687a9d19b48f08afdcc0c1bcf3778a2f67002f3" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;.</source>
          <target state="translated">この関数は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;しません。</target>
        </trans-unit>
        <trans-unit id="6eaaf4ac25e2420bf8da7dbd7b25891750193e61" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;. For broadcasting matrix products, see &lt;a href=&quot;torch.matmul#torch.matmul&quot;&gt;&lt;code&gt;torch.matmul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;しません。マトリックス製品のブロードキャストについては、&lt;a href=&quot;torch.matmul#torch.matmul&quot;&gt; &lt;code&gt;torch.matmul()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="1273ec3288d9debd903d3f9e04111f9b0d5a8d6b" translate="yes" xml:space="preserve">
          <source>This function does not check if the factorization was successful or not if &lt;code&gt;get_infos&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; since the status of the factorization is present in the third element of the return tuple.</source>
          <target state="translated">この関数は、因数分解のステータスが戻りタプルの3番目の要素に存在するため、因数分解が成功したかどうか、または &lt;code&gt;get_infos&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; であるかどうかをチェックしません。</target>
        </trans-unit>
        <trans-unit id="68c688ded2e6ac8d7e4721168f69f94608270e24" translate="yes" xml:space="preserve">
          <source>This function does not optimize the given expression, so a different formula for the same computation may run faster or consume less memory. Projects like opt_einsum (&lt;a href=&quot;https://optimized-einsum.readthedocs.io/en/stable/&quot;&gt;https://optimized-einsum.readthedocs.io/en/stable/&lt;/a&gt;) can optimize the formula for you.</source>
          <target state="translated">この関数は指定された式を最適化しないため、同じ計算に対して異なる式を実行すると、実行速度が速くなったり、メモリの消費量が少なくなる可能性があります。opt_einsum（&lt;a href=&quot;https://optimized-einsum.readthedocs.io/en/stable/&quot;&gt;https://optimized-einsum.readthedocs.io/en/stable/&lt;/a&gt;）のようなプロジェクトは、数式を最適化できます。</target>
        </trans-unit>
        <trans-unit id="f44824ec132a2e35c26075b94e4bfe941180a2db" translate="yes" xml:space="preserve">
          <source>This function doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="translated">この関数は、ログがSoftmaxとそれ自体の間で計算されることを期待するNLLLossでは直接機能しません。代わりにlog_softmaxを使用してください（より高速で、数値特性が優れています）。</target>
        </trans-unit>
        <trans-unit id="ba69951c62e61a879aa86b6caaae011ee277fff1" translate="yes" xml:space="preserve">
          <source>This function eagerly initializes CUDA.</source>
          <target state="translated">この関数はCUDAを熱心に初期化します。</target>
        </trans-unit>
        <trans-unit id="12a2d17eb1ae0addb52aeb2b46ee9bc80c204ede" translate="yes" xml:space="preserve">
          <source>This function insert observer module to all leaf child module that has a valid qconfig attribute.</source>
          <target state="translated">この関数は、有効なqconfig属性を持つすべてのリーフ子モジュールにオブザーバーモジュールを挿入します。</target>
        </trans-unit>
        <trans-unit id="4c40534c7583c126c2b6a9ea9f9ce426b4120e2b" translate="yes" xml:space="preserve">
          <source>This function is a front-end to the following LOBPCG algorithms selectable via &lt;code&gt;method&lt;/code&gt; argument:</source>
          <target state="translated">この関数は、 &lt;code&gt;method&lt;/code&gt; 引数を介して選択可能な次のLOBPCGアルゴリズムのフロントエンドです。</target>
        </trans-unit>
        <trans-unit id="773eb7977e3d589f5a2a946752611ffc03872d99" translate="yes" xml:space="preserve">
          <source>This function is deprecated and may be removed in a future release. It can be implemented using &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; as &lt;code&gt;alpha * torch.outer(vec1, vec2) + beta * input&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is not zero, and as &lt;code&gt;alpha * torch.outer(vec1, vec2)&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is zero.</source>
          <target state="translated">この機能は非推奨であり、将来のリリースで削除される可能性があります。これは、使用して実装することができる&lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt;のよう &lt;code&gt;alpha * torch.outer(vec1, vec2) + beta * input&lt;/code&gt; 場合 &lt;code&gt;beta&lt;/code&gt; ゼロではない、とのような &lt;code&gt;alpha * torch.outer(vec1, vec2)&lt;/code&gt; 場合 &lt;code&gt;beta&lt;/code&gt; ゼロです。</target>
        </trans-unit>
        <trans-unit id="ecdb962fda9e357b596ac207e636ee312e395193" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future PyTorch release. Use &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">この関数は非推奨であり、将来のPyTorchリリースで削除される予定です。代わりに&lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; を&lt;/a&gt;使用してください。</target>
        </trans-unit>
        <trans-unit id="32ba15aae127598880c645601e7ae7f8189c6c82" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future release because its behavior is inconsistent with Python&amp;rsquo;s range builtin. Instead, use &lt;a href=&quot;torch.arange#torch.arange&quot;&gt;&lt;code&gt;torch.arange()&lt;/code&gt;&lt;/a&gt;, which produces values in [start, end).</source>
          <target state="translated">この関数は非推奨であり、その動作がPythonの組み込みの範囲と矛盾しているため、将来のリリースで削除される予定です。代わりに、[start、end）に値を生成する&lt;a href=&quot;torch.arange#torch.arange&quot;&gt; &lt;code&gt;torch.arange()&lt;/code&gt; を&lt;/a&gt;使用してください。</target>
        </trans-unit>
        <trans-unit id="cd06f9dfe663c3caf3762160bc99450c126a39a9" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.functional.interpolate(...)&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="1632348f060a8b22bffa765800a936ba8981c455" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="e11fc8a8842739d9e68d3ea9f36d243b26e33ee2" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.functional.interpolate(..., mode='nearest')&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="409b1beff6de19c9e0fc8534a682b00b07c2667b" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.quantized.functional.interpolate(...)&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="f088ca0f34a9882f6e67d133eea7956f99a05c61" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="a1e1948ca54d505e36aa98bcd4ccf28bf3f09403" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="translated">この関数は廃止され、&lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt; &lt;/a&gt;が採用されました。これは、 &lt;code&gt;nn.quantized.functional.interpolate(..., mode='nearest')&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="6111b83606e5f76d2d19a4fd63d2df2a457f3738" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique#torch.unique&quot;&gt;&lt;code&gt;torch.unique()&lt;/code&gt;&lt;/a&gt; in the sense that this function only eliminates consecutive duplicate values. This semantics is similar to &lt;code&gt;std::unique&lt;/code&gt; in C++.</source>
          <target state="translated">この関数は、連続する重複値のみを削除するという意味で&lt;a href=&quot;torch.unique#torch.unique&quot;&gt; &lt;code&gt;torch.unique()&lt;/code&gt; &lt;/a&gt;とは異なります。このセマンティクスは、C ++の &lt;code&gt;std::unique&lt;/code&gt; に似ています。</target>
        </trans-unit>
        <trans-unit id="c84bc16a342354e3160aa386d7ad206e77f3ce71" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt; in the sense that this function also eliminates non-consecutive duplicate values.</source>
          <target state="translated">この関数は、連続していない重複値も排除するという意味で、&lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt; &lt;code&gt;torch.unique_consecutive()&lt;/code&gt; &lt;/a&gt;とは異なります。</target>
        </trans-unit>
        <trans-unit id="42a1f69345a02122fc091a770868079bb8638971" translate="yes" xml:space="preserve">
          <source>This function is differentiable, so gradients will flow back from the result of this operation to &lt;code&gt;input&lt;/code&gt;. To create a tensor without an autograd relationship to &lt;code&gt;input&lt;/code&gt; see &lt;a href=&quot;../autograd#torch.Tensor.detach&quot;&gt;&lt;code&gt;detach()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この関数は微分可能であるため、勾配はこの操作の結果から &lt;code&gt;input&lt;/code&gt; 逆流します。 &lt;code&gt;input&lt;/code&gt; へのautograd関係のないテンソルを作成するには、&lt;a href=&quot;../autograd#torch.Tensor.detach&quot;&gt; &lt;code&gt;detach()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="0fb9aa5da82ff7de2350851193131992c19c52a8" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.cdist(input,&amp;rsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="translated">この関数は、 &lt;code&gt;scipy.spatial.distance.cdist(input,&amp;rsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="cc680c0e90287ae914b5b4470b9652c693f1647a" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.pdist(input, &amp;lsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="translated">この関数は、 &lt;code&gt;scipy.spatial.distance.pdist(input, &amp;lsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="ff6a78f486235a2c4ee2bb0612d674661defb9c5" translate="yes" xml:space="preserve">
          <source>This function is here for legacy reasons, may be removed from nn.Functional in the future.</source>
          <target state="translated">この関数はレガシーな理由でここにあり、将来的にはnn.Functionalから削除される可能性があります。</target>
        </trans-unit>
        <trans-unit id="4ffbcd1ffd822d93e92f3e882d1d47ebf27a02de" translate="yes" xml:space="preserve">
          <source>This function is implemented only for nonnegative integers</source>
          <target state="translated">この関数は非負の整数に対してのみ実装されます。</target>
        </trans-unit>
        <trans-unit id="4af64360b4b0bf60d1db0c5268a216c0039ae031" translate="yes" xml:space="preserve">
          <source>This function is more accurate than &lt;a href=&quot;torch.log#torch.log&quot;&gt;&lt;code&gt;torch.log()&lt;/code&gt;&lt;/a&gt; for small values of &lt;code&gt;input&lt;/code&gt;</source>
          <target state="translated">この関数は、 &lt;code&gt;input&lt;/code&gt; 値が小さい場合、&lt;a href=&quot;torch.log#torch.log&quot;&gt; &lt;code&gt;torch.log()&lt;/code&gt; &lt;/a&gt;よりも正確です。</target>
        </trans-unit>
        <trans-unit id="f9a118660ffcea349bb4668844a1f15351a99bbb" translate="yes" xml:space="preserve">
          <source>This function is not defined for &lt;code&gt;torch.cuda.Tensor&lt;/code&gt; yet.</source>
          <target state="translated">この関数は、 &lt;code&gt;torch.cuda.Tensor&lt;/code&gt; に対してまだ定義されていません。</target>
        </trans-unit>
        <trans-unit id="69c6cf6408f0b3c6058b2d09b95171f65a52d054" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt;&lt;code&gt;affine_grid()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="translated">この関数は、&lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;空間トランスフォーマーネットワーク&lt;/a&gt;を構築するために&lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt; &lt;code&gt;affine_grid()&lt;/code&gt; &lt;/a&gt;と組み合わせて使用​​されることがよくあります。</target>
        </trans-unit>
        <trans-unit id="3bda1093b38d267fe7d744a3f96bb51ca111b18d" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="translated">この関数は、&lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt;を構築するために&lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt; &lt;code&gt;grid_sample()&lt;/code&gt; &lt;/a&gt;と組み合わせて使用​​されることがよくあります。</target>
        </trans-unit>
        <trans-unit id="ff271211b68caf9a357a20a5285a2d8ec4fb66ee" translate="yes" xml:space="preserve">
          <source>This function is significantly slower than &lt;code&gt;vhp&lt;/code&gt; due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.</source>
          <target state="translated">この関数は、バックワードモードのAD制約のため、 &lt;code&gt;vhp&lt;/code&gt; よりも大幅に低速です。関数が2回連続的に微分可能である場合、hvp = vhp.t（）。したがって、関数がこの条件を満たすことがわかっている場合は、代わりにvhpを使用する必要があります。これは、現在の実装でははるかに高速です。</target>
        </trans-unit>
        <trans-unit id="18b3cca742284c69f10463b509a01d9fefa6450b" translate="yes" xml:space="preserve">
          <source>This function is to be overridden by all subclasses.</source>
          <target state="translated">この関数は、すべてのサブクラスでオーバーライドされます。</target>
        </trans-unit>
        <trans-unit id="83aed61e5e3f24098a9d4e00feeea5da8441abc7" translate="yes" xml:space="preserve">
          <source>This function now calls &lt;code&gt;reset_peak_memory_stats()&lt;/code&gt;, which resets /all/ peak memory stats.</source>
          <target state="translated">この関数は、 &lt;code&gt;reset_peak_memory_stats()&lt;/code&gt; を呼び出すようになりました。これにより、/ all /ピークメモリ統計がリセットされます。</target>
        </trans-unit>
        <trans-unit id="d26ea75bdd03743d020bb8fd3f0b4f05e5fca44d" translate="yes" xml:space="preserve">
          <source>This function only works with CPU tensors and should not be used in code sections that require high performance.</source>
          <target state="translated">この関数はCPUテンソルのみで動作し、高性能を必要とするコード部では使用しないでください。</target>
        </trans-unit>
        <trans-unit id="7ed8f6139c0d2640189a9321e1da51e7b538d228" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;max(dim=0)&lt;/code&gt;</source>
          <target state="translated">この関数は、 &lt;code&gt;max(dim=0)&lt;/code&gt; とは異なり、決定論的な（サブ）勾配を生成します。</target>
        </trans-unit>
        <trans-unit id="49eba71d4c9b31d15ee67772326e88d1462e9393" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;median(dim=0)&lt;/code&gt;</source>
          <target state="translated">この関数は、 &lt;code&gt;median(dim=0)&lt;/code&gt; とは異なり、決定論的な（サブ）勾配を生成します</target>
        </trans-unit>
        <trans-unit id="d9251c4cd19a437c8f31a2c44be4bdf2e2282d65" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;min(dim=0)&lt;/code&gt;</source>
          <target state="translated">この関数は、 &lt;code&gt;min(dim=0)&lt;/code&gt; とは異なり、決定論的な（サブ）勾配を生成します。</target>
        </trans-unit>
        <trans-unit id="7d3e2fb10adcbb7f259d94a424b7b3b695d7efa3" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e.</source>
          <target state="translated">この関数は,多線式を計算する方法を提供します.</target>
        </trans-unit>
        <trans-unit id="6dd319ec8d2bd7cb7695497b2e2c0c7da18093d6" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.</source>
          <target state="translated">この関数は、Einsteinの和則を用いた多線式(積の和など)を計算する方法を提供します。</target>
        </trans-unit>
        <trans-unit id="464a58780f1e7ad5a78398d14f9e91d688f64e19" translate="yes" xml:space="preserve">
          <source>This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.</source>
          <target state="translated">この関数は、メイングループ内のすべてのプロセス(すなわち、分散ジョブの一部であるすべてのプロセス)が、グループのメンバーになる予定のないプロセスであっても、この関数に入る必要があります。さらに、グループはすべてのプロセスで同じ順序で作成されなければなりません。</target>
        </trans-unit>
        <trans-unit id="a52b7a21105c565c661abb413198331f8bc8a45e" translate="yes" xml:space="preserve">
          <source>This function returns a Tensor of size &lt;code&gt;T x B x *&lt;/code&gt; or &lt;code&gt;B x T x *&lt;/code&gt; where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.</source>
          <target state="translated">この関数は、サイズ &lt;code&gt;T x B x *&lt;/code&gt; または &lt;code&gt;B x T x *&lt;/code&gt; テンソルを返します。ここで、 &lt;code&gt;T&lt;/code&gt; は最長のシーケンスの長さです。この関数は、シーケンス内のすべてのテンソルの末尾の次元とタイプが同じであることを前提としています。</target>
        </trans-unit>
        <trans-unit id="c177e0164ff9028b4584fc5df4d1e3daee9e253a" translate="yes" xml:space="preserve">
          <source>This function returns a handle with a method &lt;code&gt;handle.remove()&lt;/code&gt; that removes the hook from the module.</source>
          <target state="translated">この関数は、モジュールからフックを削除するメソッド &lt;code&gt;handle.remove()&lt;/code&gt; を持つハンドルを返します。</target>
        </trans-unit>
        <trans-unit id="a0496aaa431bc80a1684ef66937522b9d485ea0d" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the nearly optimal approximation of a singular value decomposition of a centered matrix</source>
          <target state="translated">この関数は、中心行列の特異値分解のほぼ最適な近似である名前付きタプル &lt;code&gt;(U, S, V)&lt;/code&gt; を返します。</target>
        </trans-unit>
        <trans-unit id="eda5e94a6c116342560b85fa8d7ac690f3e68684" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the singular value decomposition of a input real matrix or batches of real matrices &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="translated">この関数はnamedtuple返し &lt;code&gt;(U, S, V)&lt;/code&gt; 入力実行列または実数行列のバッチの特異値分解で &lt;code&gt;input&lt;/code&gt; その結果を</target>
        </trans-unit>
        <trans-unit id="5aeb212c2592eaa5e981d2676890741ba536e05d" translate="yes" xml:space="preserve">
          <source>This function returns eigenvalues and eigenvectors of a real symmetric matrix &lt;code&gt;input&lt;/code&gt; or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).</source>
          <target state="translated">この関数は、名前付きタプル（eigenvalues、eigenvectors）で表される、実対称行列 &lt;code&gt;input&lt;/code&gt; または実対称行列のバッチの固有値と固有ベクトルを返します。</target>
        </trans-unit>
        <trans-unit id="2f955323e5a9f295f213a25f7ecc7dec98b5caa2" translate="yes" xml:space="preserve">
          <source>This function returns the solution to the system of linear equations represented by</source>
          <target state="translated">この関数は,次式で表される一次方程式系の解を返します.</target>
        </trans-unit>
        <trans-unit id="e819ccb2e6403dc786009cdee93abaa1f0eeb18b" translate="yes" xml:space="preserve">
          <source>This function returns without waiting for &lt;code&gt;event&lt;/code&gt;: only future operations are affected.</source>
          <target state="translated">この関数は &lt;code&gt;event&lt;/code&gt; を待たずに戻ります。将来の操作のみが影響を受けます。</target>
        </trans-unit>
        <trans-unit id="bf880005934990f2038c1e6b5eb0e99c0dad47ad" translate="yes" xml:space="preserve">
          <source>This function returns without waiting for currently enqueued kernels in &lt;a href=&quot;#torch.cuda.stream&quot;&gt;&lt;code&gt;stream&lt;/code&gt;&lt;/a&gt;: only future operations are affected.</source>
          <target state="translated">この関数は、現在&lt;a href=&quot;#torch.cuda.stream&quot;&gt; &lt;code&gt;stream&lt;/code&gt; &lt;/a&gt;エンキューされているカーネルを待たずに戻ります。将来の操作のみが影響を受けます。</target>
        </trans-unit>
        <trans-unit id="7d7a84c0650abce4d3f9c668ace3ede1befabda4" translate="yes" xml:space="preserve">
          <source>This function&amp;rsquo;s name is a misnomer. It actually rounds the quotient towards zero instead of taking its floor. This behavior will be deprecated in a future PyTorch release.</source>
          <target state="translated">この関数の名前は誤った名称です。実際には、商を床に置くのではなく、ゼロに向かって丸めます。この動作は、将来のPyTorchリリースで非推奨になります。</target>
        </trans-unit>
        <trans-unit id="c29a81af2b62737c72764612d782901c3f2eb4d7" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;a href=&quot;torch.nn.dropout#torch.nn.Dropout&quot;&gt;&lt;code&gt;Dropout&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="translated">これは、特定のモジュールにのみ影響します。&lt;a href=&quot;torch.nn.dropout#torch.nn.Dropout&quot;&gt; &lt;code&gt;Dropout&lt;/code&gt; &lt;/a&gt;、 &lt;code&gt;BatchNorm&lt;/code&gt; などの影響を受ける場合、トレーニング/評価モードでの動作の詳細については、特定のモジュールのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="a56c6998fe86297bbe85815fb835af9e00568e26" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;code&gt;Dropout&lt;/code&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="translated">これは、特定のモジュールにのみ影響します。 &lt;code&gt;Dropout&lt;/code&gt; 、 &lt;code&gt;BatchNorm&lt;/code&gt; などの影響を受ける場合、トレーニング/評価モードでの動作の詳細については、特定のモジュールのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="4f8efe32a81509d5ad4a6e1e586b778004252c39" translate="yes" xml:space="preserve">
          <source>This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt; .</source>
          <target state="translated">これは、論文&lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;「特徴検出器の共適応を防ぐことによるニューラルネットワークの改善」で&lt;/a&gt;説明されているように、正則化とニューロンの共適応を防ぐための効果的な手法であることが証明されています。</target>
        </trans-unit>
        <trans-unit id="16cf55bdf2b28322a9b3129eb62ae4e3fc4f60f8" translate="yes" xml:space="preserve">
          <source>This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 1111. It uses direction numbers to generate these sequences, and these numbers have been adapted from &lt;a href=&quot;https://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111&quot;&gt;here&lt;/a&gt;.</source>
          <target state="translated">Sobolシーケンス用のエンジンのこの実装は、最大次元1111までのシーケンスをサンプリングできます。方向番号を使用してこれらのシーケンスを生成し、これらの番号は&lt;a href=&quot;https://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111&quot;&gt;ここ&lt;/a&gt;から適応されています。</target>
        </trans-unit>
        <trans-unit id="695271e751e01b7a3e177cda22c6d8e9f50c985c" translate="yes" xml:space="preserve">
          <source>This implementation uses polar coordinates. The &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.</source>
          <target state="translated">この実装は極座標を使用します。 &lt;code&gt;loc&lt;/code&gt; と &lt;code&gt;value&lt;/code&gt; 引数は、（制約なし最適化を容易にするために）、任意の実数であることができるが、角度が2の&amp;pi;を法として解釈されます。</target>
        </trans-unit>
        <trans-unit id="218178424bb17bc6ad1e0c3396e3e36ddf25ff43" translate="yes" xml:space="preserve">
          <source>This implementation was adapted from the github repo: &lt;a href=&quot;https://github.com/bckenstler/CLR&quot;&gt;bckenstler/CLR&lt;/a&gt;</source>
          <target state="translated">この実装は、githubリポジトリから適応されました：&lt;a href=&quot;https://github.com/bckenstler/CLR&quot;&gt;bckenstler / CLR&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e5d17292f2576cd2e36da2cca0252f9780e064b5" translate="yes" xml:space="preserve">
          <source>This invariant is maintained throughout &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; class, and all functions that construct a &lt;code&gt;:class:PackedSequence&lt;/code&gt; in PyTorch (i.e., they only pass in tensors conforming to this constraint).</source>
          <target state="translated">この不変条件は、&lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; &lt;/a&gt;クラス全体、および &lt;code&gt;:class:PackedSequence&lt;/code&gt; で：class：PackedSequenceを構築するすべての関数で維持されます（つまり、この制約に準拠するテンソルのみを渡します）。</target>
        </trans-unit>
        <trans-unit id="0dcba97fb0267a928459453464ca8f56762a0535" translate="yes" xml:space="preserve">
          <source>This is TorchScript&amp;rsquo;s compilation of the code for the &lt;code&gt;forward&lt;/code&gt; method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.</source>
          <target state="translated">これは、TorchScriptによる &lt;code&gt;forward&lt;/code&gt; メソッドのコードのコンパイルです。これを使用して、TorchScript（トレースまたはスクリプト）がモデルコードを正しくキャプチャしたことを確認できます。</target>
        </trans-unit>
        <trans-unit id="78aa499d132d9b73782bab5e2924798d784eafb1" translate="yes" xml:space="preserve">
          <source>This is a &lt;strong&gt;Prototype&lt;/strong&gt; function.</source>
          <target state="translated">これは&lt;strong&gt;プロトタイプ&lt;/strong&gt;関数です。</target>
        </trans-unit>
        <trans-unit id="554d0b95763033953c158f9820e38365e62ff2d9" translate="yes" xml:space="preserve">
          <source>This is a generalized version of &lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt;&lt;code&gt;torch.hann_window()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt; &lt;code&gt;torch.hann_window()&lt;/code&gt; の&lt;/a&gt;一般化されたバージョンです。</target>
        </trans-unit>
        <trans-unit id="27da13545c38e0ad916bfb253add6c85dbfec69d" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly.</source>
          <target state="translated">LAPACKを直接呼び出すための低レベル関数です。</target>
        </trans-unit>
        <trans-unit id="baa9ff1557d00c12989b3535b16426973504c66e" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in &lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACK documentation for geqrf&lt;/a&gt; .</source>
          <target state="translated">これは、LAPACKを直接呼び出すための低レベルの関数です。この関数は、geqrfの&lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACKドキュメントで&lt;/a&gt;定義されているnamedtuple（a、tau）を返します。</target>
        </trans-unit>
        <trans-unit id="bed0909b657a7fcd560ee9c527be9120da241148" translate="yes" xml:space="preserve">
          <source>This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt;, which checks for contiguity, or &lt;a href=&quot;#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, which copies data if needed. To change the size in-place with custom strides, see &lt;a href=&quot;#torch.Tensor.set_&quot;&gt;&lt;code&gt;set_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは低レベルの方法です。ストレージは、現在のストライドを無視して、C連続として再解釈されます（ターゲットサイズが現在のサイズと等しい場合を除きます。この場合、テンソルは変更されません）。ほとんどの場合、代わりに、隣接性をチェックする&lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt;、または必要に応じてデータをコピー&lt;a href=&quot;#torch.Tensor.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt;reshape （）を使用することをお勧めします。カスタムストライドを使用してインプレースでサイズを変更するには、&lt;a href=&quot;#torch.Tensor.set_&quot;&gt; &lt;code&gt;set_()&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="f3b5e88eb67e346fdbdf0577eada21e1b383ed17" translate="yes" xml:space="preserve">
          <source>This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.</source>
          <target state="translated">これは、すでに共有メモリ内にあるストレージと、プロセス間で共有するために移動する必要のない CUDA ストレージに対しては、実行しません。共有メモリ内のストレージはサイズ変更できません。</target>
        </trans-unit>
        <trans-unit id="f5b9f918ec25aa03605059dcc957950d2df96994" translate="yes" xml:space="preserve">
          <source>This is a no-op if the tensor is already of the correct type. This is equivalent to &lt;code&gt;self.type(tensor.type())&lt;/code&gt;</source>
          <target state="translated">テンソルがすでに正しいタイプである場合、これはノーオペレーションです。これは &lt;code&gt;self.type(tensor.type())&lt;/code&gt; と同等です</target>
        </trans-unit>
        <trans-unit id="be9417a8a097120c57731208ecd0b2e0d9e0a45b" translate="yes" xml:space="preserve">
          <source>This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized.</source>
          <target state="translated">これは、基礎となるストレージが既に共有メモリ内にあり、CUDAテンソルの場合には実行できません。共有メモリ内のテンソルはサイズ変更できません。</target>
        </trans-unit>
        <trans-unit id="584f2eb806dc59e0bb28f52080bdd509e138734b" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 1dとBatch Norm 1dモジュールを呼び出すシーケンシャルコンテナです。量子化の際に、これは対応するフュージョンモジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="3187ce0a4f646a0f02690921fbe6d7114d95c7e3" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 1dとReLUモジュールを呼び出すシーケンシャルコンテナです。量子化の際には、これは対応する融合モジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="3a2d1424088d04fbdd7c029c2c5a864c5f132531" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 1d、Batch Norm 1d、ReLUモジュールを呼び出すシーケンシャルコンテナです。量子化の際には、これは対応するフュージョンモジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="ee8eabed86139ec26051c16d9d19973c0fd6b09c" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 2dモジュールとBatch Norm 2dモジュールを呼び出すシーケンシャルコンテナです。量子化の際に、これは対応するフュージョンモジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="12efc861b006501b2b4700d386ad4f25b9ff0f7e" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 2dモジュールとReLUモジュールを呼び出すシーケンシャルコンテナです。量子化の際には、これは対応する融合モジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="98a705e3678fe480b570b51f1f965a2f3d037e83" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">これは、Conv 2d、Batch Norm 2d、ReLUモジュールを呼び出すシーケンシャルコンテナです。量子化の際には、これは対応するフュージョンモジュールに置き換えられます。</target>
        </trans-unit>
        <trans-unit id="01f4b9cc31c6559249deff015369111df9aa6f57" translate="yes" xml:space="preserve">
          <source>This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. &lt;code&gt;backward()&lt;/code&gt;.</source>
          <target state="translated">これは、ほとんどのオプティマイザでサポートされている簡略化されたバージョンです。この関数は、たとえば &lt;code&gt;backward()&lt;/code&gt; を使用して勾配が計算されると呼び出すことができます。</target>
        </trans-unit>
        <trans-unit id="fd838137a8dd7616f8151e3b75e72da4794d6fa0" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist.</source>
          <target state="translated">これは、 &lt;code&gt;NaN&lt;/code&gt; 値を「無視」する&lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; の&lt;/a&gt;変形であり、 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;NaN&lt;/code&gt; 値が存在しないかのように分位数 &lt;code&gt;q&lt;/code&gt; を計算します。</target>
        </trans-unit>
        <trans-unit id="57134b77fa81442b10e02b41eaf3243756f7f5ee" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist. If all values in a reduced row are &lt;code&gt;NaN&lt;/code&gt; then the quantiles for that reduction will be &lt;code&gt;NaN&lt;/code&gt;. See the documentation for &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、 &lt;code&gt;NaN&lt;/code&gt; 値を「無視」する&lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; の&lt;/a&gt;変形であり、 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;NaN&lt;/code&gt; 値が存在しないかのように分位数 &lt;code&gt;q&lt;/code&gt; を計算します。減少列のすべての値である場合 &lt;code&gt;NaN&lt;/code&gt; 、その低減のための変位値になります &lt;code&gt;NaN&lt;/code&gt; 。&lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; の&lt;/a&gt;ドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="ae0e7bea21fe5aaecb615e30a54297c8a770a6eb" translate="yes" xml:space="preserve">
          <source>This is a very memory intensive optimizer (it requires additional &lt;code&gt;param_bytes * (history_size + 1)&lt;/code&gt; bytes). If it doesn&amp;rsquo;t fit in memory try reducing the history size, or use a different algorithm.</source>
          <target state="translated">これは非常にメモリを消費するオプティマイザです（追加の &lt;code&gt;param_bytes * (history_size + 1)&lt;/code&gt; バイトが必要です）。メモリに収まらない場合は、履歴サイズを減らすか、別のアルゴリズムを使用してみてください。</target>
        </trans-unit>
        <trans-unit id="994f42290c63351a1c873f9744cae43cb48288cc" translate="yes" xml:space="preserve">
          <source>This is a wrapper around &lt;code&gt;cudaEventSynchronize()&lt;/code&gt;: see &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html&quot;&gt;CUDA Event documentation&lt;/a&gt; for more info.</source>
          <target state="translated">これは &lt;code&gt;cudaEventSynchronize()&lt;/code&gt; のラッパーです。詳細については、&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html&quot;&gt;CUDAイベントのドキュメント&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="2d81e1e16ab1c2e5ec9a42a6d43cc837303bcaf0" translate="yes" xml:space="preserve">
          <source>This is a wrapper around &lt;code&gt;cudaStreamSynchronize()&lt;/code&gt;: see &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html&quot;&gt;CUDA Stream documentation&lt;/a&gt; for more info.</source>
          <target state="translated">これは &lt;code&gt;cudaStreamSynchronize()&lt;/code&gt; のラッパーです。詳細については、&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html&quot;&gt;CUDAストリームのドキュメント&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="13f3d83bdd2dfc81633c08aeb7934d3136f118c5" translate="yes" xml:space="preserve">
          <source>This is a wrapper around &lt;code&gt;cudaStreamWaitEvent()&lt;/code&gt;: see &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html&quot;&gt;CUDA Stream documentation&lt;/a&gt; for more info.</source>
          <target state="translated">これは &lt;code&gt;cudaStreamWaitEvent()&lt;/code&gt; のラッパーです。詳細については、&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html&quot;&gt;CUDAストリームのドキュメント&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="55c07d098e34194632b960dfc4bd298727835fac" translate="yes" xml:space="preserve">
          <source>This is always &lt;code&gt;True&lt;/code&gt; for CUDA tensors.</source>
          <target state="translated">これは常に &lt;code&gt;True&lt;/code&gt; CUDAテンソルのため。</target>
        </trans-unit>
        <trans-unit id="afdc3cee342017a6e1562fe587b2298feb148739" translate="yes" xml:space="preserve">
          <source>This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization.</source>
          <target state="translated">これは両義的であり、HMCでの使用に適していますが、座標を混ぜ合わせたものであり、最適化にはあまり適していません。</target>
        </trans-unit>
        <trans-unit id="16c88bb46cbf5ef3f448ca4fe755467e88b5e578" translate="yes" xml:space="preserve">
          <source>This is different from &lt;a href=&quot;../tensors#torch.Tensor.repeat&quot;&gt;&lt;code&gt;torch.Tensor.repeat()&lt;/code&gt;&lt;/a&gt; but similar to &lt;code&gt;numpy.repeat&lt;/code&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;../tensors#torch.Tensor.repeat&quot;&gt; &lt;code&gt;torch.Tensor.repeat()&lt;/code&gt; &lt;/a&gt;とは異なりますが、numpy.repeatに似てい &lt;code&gt;numpy.repeat&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="db053cd2631236023d48272222af395e026941e8" translate="yes" xml:space="preserve">
          <source>This is equivalent to &lt;code&gt;self.log_pob(input).argmax(dim=1)&lt;/code&gt;, but is more efficient in some cases.</source>
          <target state="translated">これは &lt;code&gt;self.log_pob(input).argmax(dim=1)&lt;/code&gt; と同等ですが、場合によってはより効率的です。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
