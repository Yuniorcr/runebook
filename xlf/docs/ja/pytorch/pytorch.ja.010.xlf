<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="374b7dab14fec694b4f5430f4c9e29dd3262cee0" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_2d#torch.atleast_2d&quot;&gt;&lt;code&gt;torch.atleast_2d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、すべての1-Dテンソルが&lt;a href=&quot;torch.atleast_2d#torch.atleast_2d&quot;&gt; &lt;code&gt;torch.atleast_2d()&lt;/code&gt; &lt;/a&gt;によって再形成された後の、最初の軸に沿った連結に相当します。</target>
        </trans-unit>
        <trans-unit id="b6ef108509ff368b6991089b8608a37aca821f0d" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.</source>
          <target state="translated">これは、1-D テンソルについては第 1 軸に沿って連結し、他のすべてのテンソルについては第 2 軸に沿って連結することに相当する。</target>
        </trans-unit>
        <trans-unit id="c5b9e7c481799905ba918331b66fdb7b66a8da08" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_3d#torch.atleast_3d&quot;&gt;&lt;code&gt;torch.atleast_3d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;torch.atleast_3d#torch.atleast_3d&quot;&gt; &lt;code&gt;torch.atleast_3d()&lt;/code&gt; &lt;/a&gt;によって1-Dおよび2-Dテンソルが再形成された後の3番目の軸に沿った連結に相当します。</target>
        </trans-unit>
        <trans-unit id="c60beaf37836d9a93c155f39fdfc6ecdf807a30e" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;#torch.nn.Module.train&quot;&gt; &lt;code&gt;self.train(False)&lt;/code&gt; &lt;/a&gt;と同等です。</target>
        </trans-unit>
        <trans-unit id="d6810c532f9495d317b56fc8164ab42f154df6d1" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;torch.nn.module#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;torch.nn.module#torch.nn.Module.train&quot;&gt; &lt;code&gt;self.train(False)&lt;/code&gt; &lt;/a&gt;と同等です。</target>
        </trans-unit>
        <trans-unit id="14aba227b9dce4ab34682734cfe12d5fc9a3b9e5" translate="yes" xml:space="preserve">
          <source>This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form</source>
          <target state="translated">これは、Sutskever et.al.や他のフレームワークが</target>
        </trans-unit>
        <trans-unit id="60444d425f826d3c68c2b033a78fb2f503cd684c" translate="yes" xml:space="preserve">
          <source>This is likely less than the amount shown in &lt;code&gt;nvidia-smi&lt;/code&gt; since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-management&quot;&gt;Memory management&lt;/a&gt; for more details about GPU memory management.</source>
          <target state="translated">一部の未使用メモリはキャッシングアロケータによって保持でき、一部のコンテキストはGPUで作成する必要があるため、これは &lt;code&gt;nvidia-smi&lt;/code&gt; に表示される量よりも少ない可能性があります。GPUメモリ管理の詳細については、&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-management&quot;&gt;メモリ管理&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="c37867c00b5ddfbf25d4de90fb71189dbd82b435" translate="yes" xml:space="preserve">
          <source>This is mainly useful for changing the shape of the result of &lt;a href=&quot;#torch.distributions.independent.Independent.log_prob&quot;&gt;&lt;code&gt;log_prob()&lt;/code&gt;&lt;/a&gt;. For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:</source>
          <target state="translated">これは主に、&lt;a href=&quot;#torch.distributions.independent.Independent.log_prob&quot;&gt; &lt;code&gt;log_prob()&lt;/code&gt; の&lt;/a&gt;結果の形状を変更する場合に役立ちます。たとえば、多変量正規分布と同じ形状の対角正規分布を作成するには（交換可能）、次のことができます。</target>
        </trans-unit>
        <trans-unit id="89d9637a7ca542b85a68e4470c388a54ccbbc2a4" translate="yes" xml:space="preserve">
          <source>This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms.</source>
          <target state="translated">これは両義的ではなく、HMCには使用できません。しかし、これは(最終的な正規化を除いて)ほとんどが座標的に動作するため、座標的な最適化アルゴリズムに適しています。</target>
        </trans-unit>
        <trans-unit id="18f227b9fbcb58581a48e62cc2b1e4b0a1736273" translate="yes" xml:space="preserve">
          <source>This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model &lt;em&gt;before&lt;/em&gt; saving it ensures that the tracer has the correct device information.</source>
          <target state="translated">トレーサーが特定のデバイスでテンソルの作成を目撃する可能性があるため、これをお勧めします。そのため、既にロードされているモデルをキャストすると、予期しない影響が生じる可能性があります。保存する&lt;em&gt;前&lt;/em&gt;にモデルをキャストすると、トレーサーに正しいデバイス情報が確実に含まれます。</target>
        </trans-unit>
        <trans-unit id="61139b0235e460eab7709588bf0db5fef3c1ed33" translate="yes" xml:space="preserve">
          <source>This is supported for &lt;a href=&quot;#module-attributes&quot;&gt;module attributes&lt;/a&gt; class attribute annotations but not for functions</source>
          <target state="translated">これは、&lt;a href=&quot;#module-attributes&quot;&gt;モジュール属性&lt;/a&gt;クラス属性アノテーションではサポートされていますが、関数ではサポートされていません</target>
        </trans-unit>
        <trans-unit id="5d93a3f447e3870288983d617bf3d5ca0babcaca" translate="yes" xml:space="preserve">
          <source>This is the default method, meaning that &lt;code&gt;init_method&lt;/code&gt; does not have to be specified (or can be &lt;code&gt;env://&lt;/code&gt;).</source>
          <target state="translated">これはデフォルトのメソッドです。つまり、 &lt;code&gt;init_method&lt;/code&gt; を指定する必要はありません（または &lt;code&gt;env://&lt;/code&gt; にすることができます）。</target>
        </trans-unit>
        <trans-unit id="b773fd2df4f9d314a4e612fca433c54f4b6db8e8" translate="yes" xml:space="preserve">
          <source>This is the default strategy (except for macOS and OS X where it&amp;rsquo;s not supported).</source>
          <target state="translated">これはデフォルトの戦略です（サポートされていないmacOSとOS Xを除く）。</target>
        </trans-unit>
        <trans-unit id="b58bf6260a34528b9c54af5a74ed650426bd462d" translate="yes" xml:space="preserve">
          <source>This is the functional version of the DataParallel module.</source>
          <target state="translated">これはDataParallelモジュールの機能版です。</target>
        </trans-unit>
        <trans-unit id="28e50c155d4263693358c9685af859f0b288737a" translate="yes" xml:space="preserve">
          <source>This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first).</source>
          <target state="translated">これは最も一般的なケースであり、データのミニバッチをフェッチして、それらをバッチ付きサンプルに照合すること、すなわち、1つの次元がバッチ次元(通常は最初の次元)であるテンソルを含むことに対応します。</target>
        </trans-unit>
        <trans-unit id="9f2aa90c9ddd953d8134c3b2013f0dd5a5e04793" translate="yes" xml:space="preserve">
          <source>This is the quantized equivalent of &lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt;&lt;code&gt;ELU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt; &lt;code&gt;ELU&lt;/code&gt; &lt;/a&gt;の量子化された同等物です。</target>
        </trans-unit>
        <trans-unit id="051cad7dbbb9111a95c9efe4f04cfeb331282a9e" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="ea4fe276ddec4ebc483841a1170deaf100cb0956" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt;&lt;code&gt;BatchNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt; &lt;code&gt;BatchNorm3d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="e5ce0ce0859be0cbdcd66ae4d96e1834e80fb440" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.groupnorm#torch.nn.GroupNorm&quot;&gt;&lt;code&gt;GroupNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.groupnorm#torch.nn.GroupNorm&quot;&gt; &lt;code&gt;GroupNorm&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="4fd9da5ac2b1219bc045ac6f1d4efc7962f29b18" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt;&lt;code&gt;Hardswish&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは&lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt; &lt;code&gt;Hardswish&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="81ee4b5a559a55d776be5e5ad40fea15e08c77c3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt;&lt;code&gt;InstanceNorm1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt; &lt;code&gt;InstanceNorm1d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="306614a03d85542a417f30827501dbc097b0e4e2" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt;&lt;code&gt;InstanceNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt; &lt;code&gt;InstanceNorm2d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="5c54c49dfa0b1be230f77c6acfc8832f75ff793b" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt;&lt;code&gt;InstanceNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt; &lt;code&gt;InstanceNorm3d&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="1ab22dac0481c7e9d72041caf66a8d6e6bce24a3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt;&lt;code&gt;LayerNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt; &lt;code&gt;LayerNorm&lt;/code&gt; &lt;/a&gt;の量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="1c1757bb112d3facda0c6b8d39d0c0a08d49265a" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;nn.functional#torch.nn.functional.hardswish&quot;&gt;&lt;code&gt;hardswish()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;nn.functional#torch.nn.functional.hardswish&quot;&gt; &lt;code&gt;hardswish()&lt;/code&gt; の&lt;/a&gt;量子化バージョンです。</target>
        </trans-unit>
        <trans-unit id="c6493b5bad0951f0b43e4846b14cd1a429b62cc1" translate="yes" xml:space="preserve">
          <source>This is the reverse operation of the manner described in &lt;a href=&quot;#torch.Tensor.gather&quot;&gt;&lt;code&gt;gather()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;#torch.Tensor.gather&quot;&gt; &lt;code&gt;gather()&lt;/code&gt; &lt;/a&gt;で説明されている方法の逆の操作です。</target>
        </trans-unit>
        <trans-unit id="5295faab37629f799f6e6fbb1e9825a33f998b68" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;torch.max()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">これは、&lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;torch.max()&lt;/code&gt; &lt;/a&gt;によって返される2番目の値です。このメソッドの正確なセマンティクスについては、そのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="19c006dca5dfcf6b24613e9eb4ec7d4564e3ffde" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;torch.min()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">これは、&lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;torch.min()&lt;/code&gt; &lt;/a&gt;によって返される2番目の値です。このメソッドの正確なセマンティクスについては、そのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="ea4ca9f34dc42157c431ad05a09921e13d264890" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.sort#torch.sort&quot;&gt;&lt;code&gt;torch.sort()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">これは、&lt;a href=&quot;torch.sort#torch.sort&quot;&gt; &lt;code&gt;torch.sort()&lt;/code&gt; &lt;/a&gt;によって返される2番目の値です。このメソッドの正確なセマンティクスについては、そのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="f4162fd55661c8ae3715301b7e45fb9d304f83a8" translate="yes" xml:space="preserve">
          <source>This is typically passed to an optimizer.</source>
          <target state="translated">これは通常、オプティマイザに渡されます。</target>
        </trans-unit>
        <trans-unit id="8881d58b1f068203ff9ef6d05a7b8193f5aad690" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは通常、モデルパラメータと見なされるべきではないバッファを登録するために使用されます。たとえば、BatchNormの &lt;code&gt;running_mean&lt;/code&gt; はパラメーターではありませんが、モジュールの状態の一部です。デフォルトでは、バッファは永続的であり、パラメータと一緒に保存されます。この動作は、 &lt;code&gt;persistent&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定することで変更できます。永続バッファと非永続バッファの唯一の違いは、後者はこのモジュールの&lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; の&lt;/a&gt;一部ではないということです。</target>
        </trans-unit>
        <trans-unit id="bc05416a2fcfa2651cf566b1804c2fef547cfa8c" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは通常、モデルパラメータと見なされるべきではないバッファを登録するために使用されます。たとえば、BatchNormの &lt;code&gt;running_mean&lt;/code&gt; はパラメーターではありませんが、モジュールの状態の一部です。デフォルトでは、バッファは永続的であり、パラメータと一緒に保存されます。この動作は、 &lt;code&gt;persistent&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定することで変更できます。永続バッファと非永続バッファの唯一の違いは、後者はこのモジュールの&lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; の&lt;/a&gt;一部ではないということです。</target>
        </trans-unit>
        <trans-unit id="cdf0d415e18e8e03494fb14369b3744608b2dde4" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは通常、モデルパラメータと見なされるべきではないバッファを登録するために使用されます。たとえば、BatchNormの &lt;code&gt;running_mean&lt;/code&gt; はパラメーターではありませんが、モジュールの状態の一部です。デフォルトでは、バッファは永続的であり、パラメータと一緒に保存されます。この動作は、 &lt;code&gt;persistent&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定することで変更できます。永続バッファと非永続バッファの唯一の違いは、後者はこのモジュールの&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; の&lt;/a&gt;一部ではないということです。</target>
        </trans-unit>
        <trans-unit id="387b12018432b2012dd14d16de801f3f2b4ba6be" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは通常、モデルパラメータと見なされるべきではないバッファを登録するために使用されます。たとえば、BatchNormの &lt;code&gt;running_mean&lt;/code&gt; はパラメーターではありませんが、モジュールの状態の一部です。デフォルトでは、バッファは永続的であり、パラメータと一緒に保存されます。この動作は、 &lt;code&gt;persistent&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定することで変更できます。永続バッファと非永続バッファの唯一の違いは、後者はこのモジュールの&lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; の&lt;/a&gt;一部ではないということです。</target>
        </trans-unit>
        <trans-unit id="e9103d955a0e478fd4fc8f34beda0ed1c416c5a7" translate="yes" xml:space="preserve">
          <source>This is used by the &lt;code&gt;quantization&lt;/code&gt; utility functions to add the quant and dequant modules, before &lt;code&gt;convert&lt;/code&gt; function &lt;code&gt;QuantStub&lt;/code&gt; will just be observer, it observes the input tensor, after &lt;code&gt;convert&lt;/code&gt;, &lt;code&gt;QuantStub&lt;/code&gt; will be swapped to &lt;code&gt;nnq.Quantize&lt;/code&gt; which does actual quantization. Similarly for &lt;code&gt;DeQuantStub&lt;/code&gt;.</source>
          <target state="translated">これは、 &lt;code&gt;quantization&lt;/code&gt; ユーティリティ関数が量子化モジュールと &lt;code&gt;QuantStub&lt;/code&gt; 量子化モジュールを追加するために使用します。 &lt;code&gt;convert&lt;/code&gt; 関数QuantStubがオブザーバーになる前に、入力テンソルを観測します。 &lt;code&gt;convert&lt;/code&gt; 後、 &lt;code&gt;QuantStub&lt;/code&gt; は実際の量子化を行う &lt;code&gt;nnq.Quantize&lt;/code&gt; にスワップされます。 &lt;code&gt;DeQuantStub&lt;/code&gt; についても同様です。</target>
        </trans-unit>
        <trans-unit id="7feb7536faa55e8682dba21d0a2c61b1c664ec04" translate="yes" xml:space="preserve">
          <source>This is used e.g. for indices returned from a max &lt;code&gt;Function&lt;/code&gt;.</source>
          <target state="translated">これは、たとえばmax &lt;code&gt;Function&lt;/code&gt; から返されるインデックスに使用されます。</target>
        </trans-unit>
        <trans-unit id="18ae6606a0b9fa5a1d8375a29d5adf1260f68aa6" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets</source>
          <target state="translated">これは,例えば自動エンコーダーの再構成の誤差を測定するために使用されます.ターゲット</target>
        </trans-unit>
        <trans-unit id="44bcfdd8636d4fef63e74c75c8c5e2e8049a6723" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets &lt;code&gt;t[i]&lt;/code&gt; should be numbers between 0 and 1.</source>
          <target state="translated">これは、たとえばオートエンコーダでの再構成のエラーを測定するために使用されます。ターゲット &lt;code&gt;t[i]&lt;/code&gt; は0から1までの数字でなければならないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="82ff1110b5f2f18a81804af99bb7a1033487ea4f" translate="yes" xml:space="preserve">
          <source>This is useful for implementing efficient sub-pixel convolution with a stride of</source>
          <target state="translated">のストライドで効率的なサブピクセル畳み込みを実装するのに便利です。</target>
        </trans-unit>
        <trans-unit id="055744440e1ad514287a476301fa832544763a8f" translate="yes" xml:space="preserve">
          <source>This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization.</source>
          <target state="translated">これは正定値行列をコレスキー因数分解の観点からパラメータ化するのに便利です。</target>
        </trans-unit>
        <trans-unit id="0b3e0c96ed0785872533b5ba8bff1ae227ce271e" translate="yes" xml:space="preserve">
          <source>This layer uses statistics computed from input data in both training and evaluation modes.</source>
          <target state="translated">このレイヤは、トレーニングモードと評価モードの両方で、入力データから計算された統計量を使用します。</target>
        </trans-unit>
        <trans-unit id="e61f8043e790e86849febfe55d39af2c5aac3218" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class.</source>
          <target state="translated">この損失は、 &lt;code&gt;Sigmoid&lt;/code&gt; 層と &lt;code&gt;BCELoss&lt;/code&gt; を1つのクラスにまとめたものです。</target>
        </trans-unit>
        <trans-unit id="940b923ef28b8c58cfd52fc019408d07c1f8a33f" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class. This version is more numerically stable than using a plain &lt;code&gt;Sigmoid&lt;/code&gt; followed by a &lt;code&gt;BCELoss&lt;/code&gt; as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.</source>
          <target state="translated">この損失は、 &lt;code&gt;Sigmoid&lt;/code&gt; 層と &lt;code&gt;BCELoss&lt;/code&gt; を1つのクラスにまとめたものです。このバージョンは、単純な &lt;code&gt;Sigmoid&lt;/code&gt; 後に &lt;code&gt;BCELoss&lt;/code&gt; を使用するよりも数値的に安定しています。これは、操作を1つのレイヤーに結合することで、数値安定性のためにlog-sum-expトリックを利用するためです。</target>
        </trans-unit>
        <trans-unit id="b85bf08f0c04a7b3a3773bc9f4c6784cf3720ebd" translate="yes" xml:space="preserve">
          <source>This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version &amp;lt; 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.</source>
          <target state="translated">これにより、これらのグラフを実行するバックエンド/ランタイムによるより良い最適化（定数畳み込みなど）が可能になる場合があります。指定されていない場合（デフォルトはなし）、動作は次のように自動的に選択されます。 operator_export_typeがOperatorExportTypes.ONNXの場合、動作はこの引数をFalseに設定することと同じです。 operator_export_typeの他の値の場合、動作はこの引数をTrueに設定することと同じです。 ONNX opsetバージョン&amp;lt;9の場合、初期化子はグラフ入力の一部である必要があることに注意してください。したがって、opset_version引数が8以下に設定されている場合、この引数は無視されます。</target>
        </trans-unit>
        <trans-unit id="26f1b287bc5579a8caeed012280a7b3f3a9f07cb" translate="yes" xml:space="preserve">
          <source>This means that &lt;code&gt;model.base&lt;/code&gt;&amp;rsquo;s parameters will use the default learning rate of &lt;code&gt;1e-2&lt;/code&gt;, &lt;code&gt;model.classifier&lt;/code&gt;&amp;rsquo;s parameters will use a learning rate of &lt;code&gt;1e-3&lt;/code&gt;, and a momentum of &lt;code&gt;0.9&lt;/code&gt; will be used for all parameters.</source>
          <target state="translated">つまり、 &lt;code&gt;model.base&lt;/code&gt; のパラメータはデフォルトの学習率 &lt;code&gt;1e-2&lt;/code&gt; を使用し、 &lt;code&gt;model.classifier&lt;/code&gt; のパラメータは学習率 &lt;code&gt;1e-3&lt;/code&gt; を使用し、すべてのパラメータに &lt;code&gt;0.9&lt;/code&gt; の運動量が使用されます。</target>
        </trans-unit>
        <trans-unit id="0b5f23050e1225367270ed2f80d8587f70aa2118" translate="yes" xml:space="preserve">
          <source>This message indicates to us that the computation differed between when we first traced it and when we traced it with the &lt;code&gt;check_inputs&lt;/code&gt;. Indeed, the loop within the body of &lt;code&gt;loop_in_traced_fn&lt;/code&gt; depends on the shape of the input &lt;code&gt;x&lt;/code&gt;, and thus when we try another &lt;code&gt;x&lt;/code&gt; with a different shape, the trace differs.</source>
          <target state="translated">このメッセージは、最初にトレースしたときと &lt;code&gt;check_inputs&lt;/code&gt; を使用してトレースしたときで計算が異なることを示しています。実際、 &lt;code&gt;loop_in_traced_fn&lt;/code&gt; の本体内のループは、入力 &lt;code&gt;x&lt;/code&gt; の形状に依存するため、異なる形状の別の &lt;code&gt;x&lt;/code&gt; を試すと、トレースが異なります。</target>
        </trans-unit>
        <trans-unit id="6b6071e9753ecc5843a463c244f78990925791d4" translate="yes" xml:space="preserve">
          <source>This method assumes that the file system supports locking using &lt;code&gt;fcntl&lt;/code&gt; - most local systems and NFS support it.</source>
          <target state="translated">この方法は、ファイルシステムが &lt;code&gt;fcntl&lt;/code&gt; を使用したロックをサポートしていることを前提としています。ほとんどのローカルシステムとNFSはそれをサポートしています。</target>
        </trans-unit>
        <trans-unit id="56fec11901471926f8e8051da013976d59b168bf" translate="yes" xml:space="preserve">
          <source>This method can only be called on a coalesced sparse tensor. See &lt;code&gt;Tensor.coalesce()&lt;/code&gt; for details.</source>
          <target state="translated">このメソッドは、合体したスパーステンソルでのみ呼び出すことができます。詳細については、 &lt;code&gt;Tensor.coalesce()&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="ab188eb8949a2274bed5b2cb6c64d4adfb8be156" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="translated">このメソッドは,複素-複素離散フーリエ変換を計算します.バッチ次元を無視して,次の式を計算します.</target>
        </trans-unit>
        <trans-unit id="9b6b6de7677cc3f29ad4b4bb71322ca8119f722c" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex inverse discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="translated">このメソッドは,複素-複素逆離散フーリエ変換を計算します.バッチ次元を無視して,次の式を計算します.</target>
        </trans-unit>
        <trans-unit id="07d469ff88f699ae509e6d44562a14e49142eb33" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-real inverse discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="translated">このメソッドは、複素数から実数への逆離散フーリエ変換を計算します。これは&lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;と数学的に同等ですが、入力と出力の形式のみが異なります。</target>
        </trans-unit>
        <trans-unit id="f0df83d3e836abc896e3d3ef88cddff7fe41858c" translate="yes" xml:space="preserve">
          <source>This method computes the real-to-complex discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="translated">このメソッドは、実数から複素数への離散フーリエ変換を計算します。これは&lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;と数学的に同等ですが、入力と出力の形式のみが異なります。</target>
        </trans-unit>
        <trans-unit id="cdf0174c78d429d9d5f3575a0eab21bb4e7cb40a" translate="yes" xml:space="preserve">
          <source>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</source>
          <target state="translated">この方法は、モデルの一部を個別に微調整したり、トレーニングしたりするためにモジュールの一部を凍結するのに便利です(GANトレーニングなど)。</target>
        </trans-unit>
        <trans-unit id="ca8069362a7e2b0181c1b2a91e97eec6d51986a6" translate="yes" xml:space="preserve">
          <source>This method is implemented using the Singular Value Decomposition.</source>
          <target state="translated">このメソッドは、特異値分解を用いて実装されています。</target>
        </trans-unit>
        <trans-unit id="a51d9e13859426129477705a920dac6fd0c48f90" translate="yes" xml:space="preserve">
          <source>This method modifies the module in-place.</source>
          <target state="translated">このメソッドは、その場でモジュールを変更します。</target>
        </trans-unit>
        <trans-unit id="44d70c89b433cfca3df93bd2dca81609164685a8" translate="yes" xml:space="preserve">
          <source>This method sets the parameters&amp;rsquo; &lt;code&gt;requires_grad&lt;/code&gt; attributes in-place.</source>
          <target state="translated">このメソッドは、パラメーターの &lt;code&gt;requires_grad&lt;/code&gt; 属性をインプレースで設定します。</target>
        </trans-unit>
        <trans-unit id="ba184c175718a9033afc302007ca57b3b2566fe2" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D complex-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with last dimension of size 2, representing the real and imaginary components of complex numbers, and should have at least &lt;code&gt;signal_ndim + 1&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="translated">このメソッドは、 &lt;code&gt;signal_ndim&lt;/code&gt; で示される1D、2D、および3Dの複合体から複合体への変換をサポートします。 &lt;code&gt;input&lt;/code&gt; は、複素数の実数成分と虚数成分を表すサイズ2の最後の次元のテンソルである必要があり、少なくとも &lt;code&gt;signal_ndim + 1&lt;/code&gt; 次元と、オプションで任意の数の先行バッチ次元が必要です。 &lt;code&gt;normalized&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、これは結果をで除算することによって結果を正規化します。</target>
        </trans-unit>
        <trans-unit id="4d0394dd315af559db4d9ad47ffb52f54e5c2212" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D real-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with at least &lt;code&gt;signal_ndim&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="translated">このメソッドは、 &lt;code&gt;signal_ndim&lt;/code&gt; で示される1D、2D、および3Dの実数から複素数への変換をサポートします。 &lt;code&gt;input&lt;/code&gt; は、少なくとも &lt;code&gt;signal_ndim&lt;/code&gt; 次元を持ち、オプションで任意の数の先行バッチ次元を持つテンソルである必要があります。 &lt;code&gt;normalized&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、これは結果をで除算することによって結果を正規化します。</target>
        </trans-unit>
        <trans-unit id="75583911adf2f93140c081540537180003c155f0" translate="yes" xml:space="preserve">
          <source>This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">このメソッドは常にファイルを作成し、プログラムの最後にファイルをクリーンアップして削除するために最善を尽くします。つまり、file initメソッドを使用した各初期化では、初期化を成功させるために、まったく新しい空のファイルが必要になります。以前の初期化で使用されたものと同じファイル（たまたまクリーンアップされない）が再度使用される場合、これは予期しない動作であり、デッドロックや障害を引き起こす可能性があります。したがって、この方法ではファイルのクリーンアップに最善を尽くしますが、自動削除が失敗した場合は、トレーニングの最後にファイルが削除され、同じファイルが削除されないようにする必要があります。次回は再利用。これは、&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt;を呼び出す場合に特に重要です。同じファイル名で複数回。つまり、ファイルが削除/クリーンアップされておらず、そのファイル&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt;再度呼び出すと、失敗が予想されます。ここでの経験則は、&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt;が呼び出されるたびに、ファイルが存在しないか空であることを確認することです。</target>
        </trans-unit>
        <trans-unit id="36d7188a05dab83ac9defa225d250cd47190ba2f" translate="yes" xml:space="preserve">
          <source>This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:</source>
          <target state="translated">このメソッドは環境変数から設定を読み込み、情報の取得方法を完全にカスタマイズすることができます。設定する変数は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="57da4e0220bd8e9709ab47673f34882d1449aae9" translate="yes" xml:space="preserve">
          <source>This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX. Exported falls through and exports the operator as is, as custom op. Exporting custom operators enables users to register and implement the operator as part of their runtime backend.</source>
          <target state="translated">このモードは、ONNXに登録されておらずサポートされていない演算子(ATenまたは非ATen)をエクスポートするために使用できます。Exportedは、オペレータをそのまま、カスタムオペレータとしてエクスポートします。 カスタムオペレータをエクスポートすることで、ユーザは、ランタイムバックエンドの一部としてオペレータを登録して実装することができます。</target>
        </trans-unit>
        <trans-unit id="da76922486c33377aa2791af6603164e435bb6ab" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.</source>
          <target state="translated">このモードは、すべてのオペレータをATen opsとしてエクスポートし、ONNXへの変換を避けるために使用されます。</target>
        </trans-unit>
        <trans-unit id="d9d848a6942846852bedaf5aea8e5e5bd3995b73" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as regular ONNX operators. This is the default &lt;code&gt;operator_export_type&lt;/code&gt; mode.</source>
          <target state="translated">このモードは、すべてのオペレーターを通常のONNXオペレーターとしてエクスポートするために使用されます。これはデフォルトの &lt;code&gt;operator_export_type&lt;/code&gt; モードです。</target>
        </trans-unit>
        <trans-unit id="9cf96f197fff58f45fdefadd1ad233845146b8c4" translate="yes" xml:space="preserve">
          <source>This mode should be enabled only for debugging as the different tests will slow down your program execution.</source>
          <target state="translated">このモードは、さまざまなテストがプログラムの実行を遅くするので、デバッグ用にのみ有効にしてください。</target>
        </trans-unit>
        <trans-unit id="15b7320d744a4575a9e6699705096be13d718c1b" translate="yes" xml:space="preserve">
          <source>This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose &lt;code&gt;torch.memory_format&lt;/code&gt; is &lt;code&gt;torch.contiguous_format&lt;/code&gt; and others whose format is &lt;code&gt;torch.channels_last&lt;/code&gt;. However, corresponding parameters in different processes must have the same strides.</source>
          <target state="translated">このモジュールでは、行メジャーが連続していないストライドを持つパラメーターを使用できます。たとえば、モデルには、 &lt;code&gt;torch.memory_format&lt;/code&gt; が &lt;code&gt;torch.contiguous_format&lt;/code&gt; であるパラメーターと、フォーマットが &lt;code&gt;torch.channels_last&lt;/code&gt; であるパラメーターが含まれている場合があります。ただし、異なるプロセスの対応するパラメータは、同じストライドを持っている必要があります。</target>
        </trans-unit>
        <trans-unit id="db9962266fa6ecf5ecb4337d45f21434962d46f1" translate="yes" xml:space="preserve">
          <source>This module also contains any parameters that the original module had as well.</source>
          <target state="translated">このモジュールには、元のモジュールが持っていたパラメータも含まれています。</target>
        </trans-unit>
        <trans-unit id="5790600450f4acb47de56be8f01d6569e2e765da" translate="yes" xml:space="preserve">
          <source>This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of &lt;code&gt;fp16&lt;/code&gt; and &lt;code&gt;fp32&lt;/code&gt;, the gradient reduction on these mixed types of parameters will just work fine.</source>
          <target state="translated">このモジュールは、混合精度の分散トレーニングもサポートします。これは、モデルが &lt;code&gt;fp16&lt;/code&gt; と &lt;code&gt;fp32&lt;/code&gt; の混合タイプなど、さまざまなタイプのパラメーターを持つことができることを意味します。これらの混合タイプのパラメーターの勾配の削減は問題なく機能します。</target>
        </trans-unit>
        <trans-unit id="aaad9c62985785aa5e132959be1bf299d5f15d45" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.</source>
          <target state="translated">このモジュールは、モデルが作成された時点で、すべてのパラメータがモデルに登録されていることを前提としています。後でパラメータを追加したり削除したりしてはいけません。バッファについても同様です。</target>
        </trans-unit>
        <trans-unit id="56c78b422ae4f13f13cfea0f3331ef6b5469addb" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient &lt;code&gt;allreduce&lt;/code&gt; following the reverse order of the registered parameters of the model. In other words, it is users&amp;rsquo; responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.</source>
          <target state="translated">このモジュールは、すべてのパラメーターが各分散プロセスのモデルに登録されていることを前提としています。モジュール自体は、モデルの登録されたパラメーターの逆の順序に従って勾配 &lt;code&gt;allreduce&lt;/code&gt; を実行します。言い換えると、各分散プロセスがまったく同じモデルを持ち、したがってまったく同じパラメーター登録順序を持っていることを確認するのはユーザーの責任です。</target>
        </trans-unit>
        <trans-unit id="a2fc61472d65b92ddda57c12782f0c49ee43ce07" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">このモジュールは、入力に対する Conv1d の勾配と見ることができます。これは,フラクショナルストライド畳み込みやデコンボリューションとしても知られています (実際のデコンボリューション操作ではありませんが)。</target>
        </trans-unit>
        <trans-unit id="51bd62541eadcc8d3cc881058d8a71b8bed71b7f" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">このモジュールは、入力に対するConv2dの勾配と見ることができます。これは,フラクショナルストライド畳み込みやデコンボリューションとしても知られています (実際のデコンボリューション操作ではありませんが)。</target>
        </trans-unit>
        <trans-unit id="03c2d1d66e7773d2c26345cff96e3404e927c5ec" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">このモジュールは、入力に対するConv3dの勾配と見ることができます。これは,分数的な畳み込みやデコンボリューションとしても知られています(実際のデコンボリューション操作ではありませんが).</target>
        </trans-unit>
        <trans-unit id="8a37a41f99ccf18be289e494dc942de371c38958" translate="yes" xml:space="preserve">
          <source>This module currently does not support custom distributed collective operations in the forward pass, such as &lt;code&gt;SyncBatchNorm&lt;/code&gt; or other custom defined collectives in the model&amp;rsquo;s forward pass.</source>
          <target state="translated">このモジュールは現在、 &lt;code&gt;SyncBatchNorm&lt;/code&gt; やモデルのフォワードパス内の他のカスタム定義されたコレクティブなど、フォワードパス内のカスタム分散コレクティブ操作をサポートしていません。</target>
        </trans-unit>
        <trans-unit id="e4e13a3367c83e826e07b89f6231cff61b5213ab" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use &lt;code&gt;LogSoftmax&lt;/code&gt; instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="translated">このモジュールは、ログがSoftmaxとそれ自体の間で計算されることを想定しているNLLLossとは直接連携しません。代わりに &lt;code&gt;LogSoftmax&lt;/code&gt; を使用してください（より高速で、数値特性も優れています）。</target>
        </trans-unit>
        <trans-unit id="b724d7ccdd2d4a2fad1d30f042a783b90f3e1b70" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work with &lt;a href=&quot;../autograd#torch.autograd.grad&quot;&gt;&lt;code&gt;torch.autograd.grad()&lt;/code&gt;&lt;/a&gt; (i.e. it will only work if gradients are to be accumulated in &lt;code&gt;.grad&lt;/code&gt; attributes of parameters).</source>
          <target state="translated">このモジュールは&lt;a href=&quot;../autograd#torch.autograd.grad&quot;&gt; &lt;code&gt;torch.autograd.grad()&lt;/code&gt; &lt;/a&gt;では機能しません（つまり、パラメーターの &lt;code&gt;.grad&lt;/code&gt; 属性にグラデーションが累積される場合にのみ機能します）。</target>
        </trans-unit>
        <trans-unit id="b31e8020fcc76734bdb0706e9d94b6fab41f09de" translate="yes" xml:space="preserve">
          <source>This module implements the combined (fused) modules conv + relu which can be then quantized.</source>
          <target state="translated">このモジュールは,量子化されたモジュール conv+relu を組み合わせた(融合した)モジュールを実装しています.</target>
        </trans-unit>
        <trans-unit id="abb55caf7c23cacd70b75be7b8534a2288b0ddd0" translate="yes" xml:space="preserve">
          <source>This module implements the functions you call directly to convert your model from FP32 to quantized form. For example the &lt;a href=&quot;#torch.quantization.prepare&quot;&gt;&lt;code&gt;prepare()&lt;/code&gt;&lt;/a&gt; is used in post training quantization to prepares your model for the calibration step and &lt;a href=&quot;#torch.quantization.convert&quot;&gt;&lt;code&gt;convert()&lt;/code&gt;&lt;/a&gt; actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu.</source>
          <target state="translated">このモジュールは、モデルをFP32から量子化された形式に変換するために直接呼び出す関数を実装します。たとえば、&lt;a href=&quot;#torch.quantization.prepare&quot;&gt; &lt;code&gt;prepare()&lt;/code&gt; &lt;/a&gt;は、トレーニング後の量子化で使用され、キャリブレーションステップ用にモデルを準備し、&lt;a href=&quot;#torch.quantization.convert&quot;&gt; &lt;code&gt;convert()&lt;/code&gt; は&lt;/a&gt;実際に重みをint8に変換し、操作を量子化された対応物に置き換えます。モデルへの入力を量子化したり、conv + reluのような重要な融合を実行したりするためのヘルパー関数は他にもあります。</target>
        </trans-unit>
        <trans-unit id="a5f06261e4042915626c1433e6142f8f0b72218a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized implementations of fused operations like conv + relu.</source>
          <target state="translated">このモジュールは,conv+relu のような融合演算の量子化実装を実装しています.</target>
        </trans-unit>
        <trans-unit id="fe84855e7e341d93fbb671f10f878fe740d68b6a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized versions of the nn layers such as ~`torch.nn.Conv2d` and &lt;code&gt;torch.nn.ReLU&lt;/code&gt;.</source>
          <target state="translated">このモジュールは、〜`torch.nn.Conv2d`や &lt;code&gt;torch.nn.ReLU&lt;/code&gt; などのnnレイヤーの量子化バージョンを実装します。</target>
        </trans-unit>
        <trans-unit id="1cc16ec4495bf63da75c9357c640897793fcbd84" translate="yes" xml:space="preserve">
          <source>This module implements the versions of those fused operations needed for quantization aware training.</source>
          <target state="translated">このモジュールは、量子化を意識したトレーニングに必要な融合操作のバージョンを実装しています。</target>
        </trans-unit>
        <trans-unit id="cd564c5f343a0b5d3e2281d9491b97ab89a390d6" translate="yes" xml:space="preserve">
          <source>This module implements versions of the key nn modules &lt;strong&gt;Conv2d()&lt;/strong&gt; and &lt;strong&gt;Linear()&lt;/strong&gt; which run in FP32 but with rounding applied to simulate the effect of INT8 quantization.</source>
          <target state="translated">このモジュールは、FP32で実行されるキーnnモジュール&lt;strong&gt;Conv2d（）&lt;/strong&gt;および&lt;strong&gt;Linear（）の&lt;/strong&gt;バージョンを実装しますが、INT8量子化の効果をシミュレートするために丸めが適用されます。</target>
        </trans-unit>
        <trans-unit id="2d28957208d801ad4a8275f10517753872c57b97" translate="yes" xml:space="preserve">
          <source>This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.</source>
          <target state="translated">このモジュールは、インデックスを使用して単語の埋め込みを取得するためによく使用されます。このモジュールへの入力はインデックスと埋め込み行列のリストで、出力は対応する単語の埋め込みです。</target>
        </trans-unit>
        <trans-unit id="a56a23f8f5587064fad32f9351698184c9934864" translate="yes" xml:space="preserve">
          <source>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</source>
          <target state="translated">このモジュールは、単語の埋め込みを格納し、インデックスを使用してそれらを検索するためによく使用されます。このモジュールへの入力はインデックスのリストで、出力は対応する単語の埋め込みです。</target>
        </trans-unit>
        <trans-unit id="8c517186ab821b45b2c9090ee2862c5050c7fd67" translate="yes" xml:space="preserve">
          <source>This module provides an RPC-based distributed autograd framework that can be used for applications such as model parallel training. In short, applications may send and receive gradient recording tensors over RPC. In the forward pass, we record when gradient recording tensors are sent over RPC and during the backward pass we use this information to perform a distributed backward pass using RPC. For more details see &lt;a href=&quot;rpc/distributed_autograd#distributed-autograd-design&quot;&gt;Distributed Autograd Design&lt;/a&gt;.</source>
          <target state="translated">このモジュールは、モデル並列トレーニングなどのアプリケーションに使用できるRPCベースの分散autogradフレームワークを提供します。つまり、アプリケーションはRPCを介して勾配記録テンソルを送受信できます。フォワードパスでは、勾配記録テンソルがRPCを介して送信されたときに記録し、バックワードパスでは、この情報を使用してRPCを使用して分散バックワードパスを実行します。詳細については、&lt;a href=&quot;rpc/distributed_autograd#distributed-autograd-design&quot;&gt;Distributed AutogradDesignを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="87af39490036eebaa3381d1990a65c31f6408fae" translate="yes" xml:space="preserve">
          <source>This module returns a &lt;code&gt;NamedTuple&lt;/code&gt; with &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;loss&lt;/code&gt; fields. See further documentation for details.</source>
          <target state="translated">このモジュールは、 &lt;code&gt;output&lt;/code&gt; フィールドと &lt;code&gt;loss&lt;/code&gt; フィールドを持つ &lt;code&gt;NamedTuple&lt;/code&gt; を返します。詳細については、詳細なドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="59e9905faed8adfff4eeb0ecdbe57c1cac83f3a4" translate="yes" xml:space="preserve">
          <source>This module supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="translated">このモジュールは&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32を&lt;/a&gt;サポートします。</target>
        </trans-unit>
        <trans-unit id="c9c9e1a196eef3e1f967008237f4372d9edc34d7" translate="yes" xml:space="preserve">
          <source>This module works only with the multi-process, single-device usage of &lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt;, which means that a single process works on a single GPU.</source>
          <target state="translated">このモジュールは、&lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt; &lt;/a&gt;のマルチプロセス、シングルデバイスの使用でのみ機能します。つまり、単一のプロセスが単一のGPUで機能します。</target>
        </trans-unit>
        <trans-unit id="0f30baa501fa38b25a8509424e2193e61d72c032" translate="yes" xml:space="preserve">
          <source>This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</source>
          <target state="translated">このオブザーバは、入力テンソルの最小値と最大値の移動平均に基づいて量子化パラメータを計算します。このモジュールは、入力テンソルの平均最小値と最大値を記録し、この統計量を使用して量子化パラメータを計算します。</target>
        </trans-unit>
        <trans-unit id="29203fffadc612cba8aabcec105c43163d833d44" translate="yes" xml:space="preserve">
          <source>This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</source>
          <target state="translated">このオブザーバは、テンソルの min/max 統計量を使用して、チャネルごとの量子化パラメータを計算します。このモジュールは、入力テンソルの最小値と最大値を記録し、この統計量を使用して量子化パラメータを計算します。</target>
        </trans-unit>
        <trans-unit id="b2adf96c5dd76cc8a637924002f78629974574c7" translate="yes" xml:space="preserve">
          <source>This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</source>
          <target state="translated">このオブザーバは、テンソルのmin/max統計量を使用して量子化パラメータを計算します。このモジュールは、入力テンソルの最小値と最大値を記録し、この統計量を使用して量子化パラメータを計算します。</target>
        </trans-unit>
        <trans-unit id="31368b5a919564a12c569127df416d6acb1973be" translate="yes" xml:space="preserve">
          <source>This op should be disambiguated with &lt;a href=&quot;torch.logsumexp#torch.logsumexp&quot;&gt;&lt;code&gt;torch.logsumexp()&lt;/code&gt;&lt;/a&gt; which performs a reduction on a single tensor.</source>
          <target state="translated">この操作は、単一のテンソルでリダクションを実行する&lt;a href=&quot;torch.logsumexp#torch.logsumexp&quot;&gt; &lt;code&gt;torch.logsumexp()&lt;/code&gt; &lt;/a&gt;で明確にする必要があります。</target>
        </trans-unit>
        <trans-unit id="3256e874629b9e3bf7609aa4774907058a898427" translate="yes" xml:space="preserve">
          <source>This operation is not differentiable.</source>
          <target state="translated">この操作は微分できません。</target>
        </trans-unit>
        <trans-unit id="f253d72c955897e96cbe7e655ade3b1e375b9508" translate="yes" xml:space="preserve">
          <source>This operation is useful for explicit broadcasting by names (see examples).</source>
          <target state="translated">この操作は、名前による明示的なブロードキャストに便利です(例を参照してください)。</target>
        </trans-unit>
        <trans-unit id="be6c030e3433204ea07b32a8e6ce714fff330e3a" translate="yes" xml:space="preserve">
          <source>This operator supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="translated">この演算子は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32を&lt;/a&gt;サポートします。</target>
        </trans-unit>
        <trans-unit id="f6cbc4c392cdef9bca907c34e1833c252683a82b" translate="yes" xml:space="preserve">
          <source>This optimizer doesn&amp;rsquo;t support per-parameter options and parameter groups (there can be only one).</source>
          <target state="translated">このオプティマイザーは、パラメーターごとのオプションとパラメーターグループをサポートしていません（1つしか存在できません）。</target>
        </trans-unit>
        <trans-unit id="aa1075ffa7fba27cc0d195af9a6f5444b94b97ce" translate="yes" xml:space="preserve">
          <source>This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.</source>
          <target state="translated">本パッケージでは、CPUテンソルと同じ機能を実装していますが、計算にGPUを利用するCUDAテンソル型のサポートを追加しています。</target>
        </trans-unit>
        <trans-unit id="836145aa5caebff4eaffcf9dc94e2f473ff1df15" translate="yes" xml:space="preserve">
          <source>This package provides a &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects. Currently, the &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type is primarily used by the &lt;a href=&quot;rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt;.</source>
          <target state="translated">このパッケージは、非同期実行をカプセル化する&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;タイプと、&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;オブジェクトの操作を簡素化する一連のユーティリティ関数を提供します。現在、&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;タイプは、主に&lt;a href=&quot;rpc#distributed-rpc-framework&quot;&gt;Distributed RPCFrameworkで&lt;/a&gt;使用されています。</target>
        </trans-unit>
        <trans-unit id="510d5659f81fbb2c92add3848324ac3e769287d8" translate="yes" xml:space="preserve">
          <source>This requires &lt;code&gt;scipy&lt;/code&gt; to be installed</source>
          <target state="translated">これには &lt;code&gt;scipy&lt;/code&gt; をインストールする必要があります</target>
        </trans-unit>
        <trans-unit id="70b5dd9eee5d2e3ba91077407f335c67991b22b8" translate="yes" xml:space="preserve">
          <source>This scheduler is not chainable.</source>
          <target state="translated">このスケジューラはチェーン化できません。</target>
        </trans-unit>
        <trans-unit id="a42db487ec40ca84c9d3c3617737b87da6c0e14d" translate="yes" xml:space="preserve">
          <source>This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc.</source>
          <target state="translated">このセクションでは、上記の基本的なAPIをベースに構築され、ヤコビアンやヘシアンなどを計算できるようにするautograd用の高レベルAPIが含まれています。</target>
        </trans-unit>
        <trans-unit id="7d1f42ea5265cdd17159cef860e2867af1a5fed2" translate="yes" xml:space="preserve">
          <source>This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2.</source>
          <target state="translated">このセクションでは、PyTorch 1.2 での TorchScript の変更点を説明します。TorchScript に慣れていない方は、このセクションを読み飛ばしても構いません。PyTorch 1.2 での TorchScript API の主な変更点は 2 つあります。</target>
        </trans-unit>
        <trans-unit id="65513dd85adae4947833e9b405731c9c92f7268c" translate="yes" xml:space="preserve">
          <source>This section provides a brief overview into how different sharing strategies work. Note that it applies only to CPU tensor - CUDA tensors will always use the CUDA API, as that&amp;rsquo;s the only way they can be shared.</source>
          <target state="translated">このセクションでは、さまざまな共有戦略がどのように機能するかについて簡単に説明します。これはCPUテンソルにのみ適用されることに注意してください。CUDAテンソルは常にCUDAAPIを使用します。これが、共有できる唯一の方法だからです。</target>
        </trans-unit>
        <trans-unit id="26517401ee04a7507f48917c7c6d007aa588bc0d" translate="yes" xml:space="preserve">
          <source>This separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:</source>
          <target state="translated">この別冊連載は、マルチプロセスデータの読み込みを利用しながら、Windowsとの互換性を確保するために2つのステップを踏む必要があることを意味しています。</target>
        </trans-unit>
        <trans-unit id="4a73b78954115d8ec831d20a068a6d8ccc312011" translate="yes" xml:space="preserve">
          <source>This strategy will use file descriptors as shared memory handles. Whenever a storage is moved to shared memory, a file descriptor obtained from &lt;code&gt;shm_open&lt;/code&gt; is cached with the object, and when it&amp;rsquo;s going to be sent to other processes, the file descriptor will be transferred (e.g. via UNIX sockets) to it. The receiver will also cache the file descriptor and &lt;code&gt;mmap&lt;/code&gt; it, to obtain a shared view onto the storage data.</source>
          <target state="translated">この戦略では、ファイル記述子を共有メモリハンドルとして使用します。ストレージが共有メモリに移動されるたびに、 &lt;code&gt;shm_open&lt;/code&gt; から取得されたファイル記述子がオブジェクトとともにキャッシュされ、他のプロセスに送信されるときに、ファイル記述子が（UNIXソケットなどを介して）そのストレージに転送されます。受信者はファイル記述子をキャッシュして &lt;code&gt;mmap&lt;/code&gt; し、ストレージデータの共有ビューを取得します。</target>
        </trans-unit>
        <trans-unit id="18d6cf184b4d322d9e59ed200a7c995c7651fa42" translate="yes" xml:space="preserve">
          <source>This strategy will use file names given to &lt;code&gt;shm_open&lt;/code&gt; to identify the shared memory regions. This has a benefit of not requiring the implementation to cache the file descriptors obtained from it, but at the same time is prone to shared memory leaks. The file can&amp;rsquo;t be deleted right after its creation, because other processes need to access it to open their views. If the processes fatally crash, or are killed, and don&amp;rsquo;t call the storage destructors, the files will remain in the system. This is very serious, because they keep using up the memory until the system is restarted, or they&amp;rsquo;re freed manually.</source>
          <target state="translated">この戦略では、 &lt;code&gt;shm_open&lt;/code&gt; に指定されたファイル名を使用して、共有メモリ領域を識別します。これには、実装から取得したファイル記述子をキャッシュする必要がないという利点がありますが、同時に共有メモリリークが発生しやすくなります。他のプロセスがビューを開くためにファイルにアクセスする必要があるため、ファイルは作成直後に削除できません。プロセスが致命的にクラッシュするか、強制終了され、ストレージデストラクタを呼び出さない場合、ファイルはシステムに残ります。システムが再起動されるか、手動で解放されるまでメモリを使い続けるため、これは非常に深刻です。</target>
        </trans-unit>
        <trans-unit id="792e85a6083fd7bff7900d359e6ad50e4d63f9a7" translate="yes" xml:space="preserve">
          <source>This subset is restricted:</source>
          <target state="translated">このサブセットは制限されています。</target>
        </trans-unit>
        <trans-unit id="fabd02d487546b940bc53aa8b39e79347651d21b" translate="yes" xml:space="preserve">
          <source>This transform arises as an iterated sigmoid transform in a stick-breaking construction of the &lt;code&gt;Dirichlet&lt;/code&gt; distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.</source>
          <target state="translated">この変換は、 &lt;code&gt;Dirichlet&lt;/code&gt; 分布のスティックを壊す構造で繰り返されるシグモイド変換として発生します。最初のロジットは、シグモイドを介して最初の確率と他のすべての確率に変換され、プロセスが再帰します。</target>
        </trans-unit>
        <trans-unit id="f610c707a33d251ea07e96805fc742b6f94fb773" translate="yes" xml:space="preserve">
          <source>This will call &lt;a href=&quot;optim#torch.optim.Optimizer.step&quot;&gt;&lt;code&gt;torch.optim.Optimizer.step()&lt;/code&gt;&lt;/a&gt; on each worker containing parameters to be optimized, and will block until all workers return. The provided &lt;code&gt;context_id&lt;/code&gt; will be used to retrieve the corresponding &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;context&lt;/code&gt;&lt;/a&gt; that contains the gradients that should be applied to the parameters.</source>
          <target state="translated">これにより、最適化するパラメーターを含む各ワーカーで&lt;a href=&quot;optim#torch.optim.Optimizer.step&quot;&gt; &lt;code&gt;torch.optim.Optimizer.step()&lt;/code&gt; &lt;/a&gt;が呼び出され、すべてのワーカーが戻るまでブロックされます。指定された &lt;code&gt;context_id&lt;/code&gt; は、パラメーターに適用する必要のあるグラデーションを含む対応する&lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt; &lt;code&gt;context&lt;/code&gt; &lt;/a&gt;を取得するために使用されます。</target>
        </trans-unit>
        <trans-unit id="71b21a25c294ca711bdf2a3235c7d1f311e30f3a" translate="yes" xml:space="preserve">
          <source>This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in &lt;code&gt;backward()&lt;/code&gt;, but it&amp;rsquo;s always going to be a zero tensor with the same shape as the shape of a corresponding output.</source>
          <target state="translated">これにより、出力が勾配を必要としないものとしてマークされ、後方計算の効率が向上します。それでも &lt;code&gt;backward()&lt;/code&gt; で各出力の勾配を受け入れる必要がありますが、対応する出力の形状と同じ形状のゼロテンソルになります。</target>
        </trans-unit>
        <trans-unit id="c51f7b72279e26fd7cf66d9d61e7f7204bf3b26a" translate="yes" xml:space="preserve">
          <source>Threshold</source>
          <target state="translated">Threshold</target>
        </trans-unit>
        <trans-unit id="4c8543e85c70d9042806658de48a8eae05e8c630" translate="yes" xml:space="preserve">
          <source>Threshold is defined as:</source>
          <target state="translated">しきい値は次のように定義されています。</target>
        </trans-unit>
        <trans-unit id="a2ed3cfe5a92d0e9667b7a33a4666b01b859c7a8" translate="yes" xml:space="preserve">
          <source>Thresholds each element of the input Tensor.</source>
          <target state="translated">入力テンソルの各要素にしきい値を設定します。</target>
        </trans-unit>
        <trans-unit id="880f2f41d4783f36e126ec98db6135c252954795" translate="yes" xml:space="preserve">
          <source>To achieve this, developers need to touch the source code of PyTorch. Please follow the &lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot;&gt;instructions&lt;/a&gt; for installing PyTorch from source. If the wanted operator is standardized in ONNX, it should be easy to add support for exporting such operator (adding a symbolic function for the operator). To confirm whether the operator is standardized or not, please check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;.</source>
          <target state="translated">これを実現するには、開発者はPyTorchのソースコードに触れる必要があります。ソースからPyTorchをインストールするための&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot;&gt;指示&lt;/a&gt;に従ってください。必要な演算子がONNXで標準化されている場合、そのような演算子のエクスポートのサポートを簡単に追加できます（演算子のシンボリック関数を追加します）。オペレーターが標準化されているかどうかを確認するには、&lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNXオペレーターリスト&lt;/a&gt;を確認してください。</target>
        </trans-unit>
        <trans-unit id="b5aa3a35df66cb08f05248ae2964bf5546e98afe" translate="yes" xml:space="preserve">
          <source>To align a tensor to a specific order, use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">テンソルを特定の順序に揃えるには、&lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; を&lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="42a1cb8b698578c9fe48bbb02d735034e28323f6" translate="yes" xml:space="preserve">
          <source>To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please avoid use of &lt;code&gt;torch.Tensor.item()&lt;/code&gt;. Torch supports implicit cast of single-element tensors to numbers. E.g.:</source>
          <target state="translated">ONNXモデルの一部として可変スカラーテンソルを固定値定数としてエクスポートしないようにするには、 &lt;code&gt;torch.Tensor.item()&lt;/code&gt; の使用を避けてください。Torchは、単一要素テンソルの数値への暗黙的なキャストをサポートします。例えば：</target>
        </trans-unit>
        <trans-unit id="e33cac23c601d0b5ef35b2617bd1e0b84d5ac6c7" translate="yes" xml:space="preserve">
          <source>To be able to save a module, it must not make any calls to native Python functions. This means that all submodules must be subclasses of &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; as well.</source>
          <target state="translated">モジュールを保存できるようにするには、ネイティブPython関数を呼び出さないでください。これは、すべてのサブモジュールが&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; の&lt;/a&gt;サブクラスでもある必要があることを意味します。</target>
        </trans-unit>
        <trans-unit id="517547cd8d7cdf6793257301cea557fd580f4509" translate="yes" xml:space="preserve">
          <source>To change an existing tensor&amp;rsquo;s &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; and/or &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, consider using &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt; method on the tensor.</source>
          <target state="translated">既存のテンソルの変更するには&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;および/または&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; を&lt;/a&gt;、使用することを検討してください&lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt;テンソルの方法。</target>
        </trans-unit>
        <trans-unit id="cccb194237ee7a6be8a8fca8580f1ea27dde7026" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; (and recursively compile anything it calls), add the &lt;a href=&quot;../jit#torch.jit.export&quot;&gt;&lt;code&gt;@torch.jit.export&lt;/code&gt;&lt;/a&gt; decorator to the method. To opt out of compilation use &lt;a href=&quot;torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;forward&lt;/code&gt; 以外のメソッドをコンパイルする（そしてそれが呼び出すものを再帰的にコンパイルする）には、&lt;a href=&quot;../jit#torch.jit.export&quot;&gt; &lt;code&gt;@torch.jit.export&lt;/code&gt; &lt;/a&gt;デコレータをメソッドに追加します。コンパイルをオプトアウトするには、&lt;a href=&quot;torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="13132861c718e28cce4e074280b9a030df11d21a" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; that is not called from &lt;code&gt;forward&lt;/code&gt;, add &lt;code&gt;@torch.jit.export&lt;/code&gt;.</source>
          <target state="translated">以外の方法でコンパイルするには &lt;code&gt;forward&lt;/code&gt; から呼び出されていない &lt;code&gt;forward&lt;/code&gt; 、追加 &lt;code&gt;@torch.jit.export&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1f8e9d3b1e59225987eeef2681c83b1bcdde57d0" translate="yes" xml:space="preserve">
          <source>To compile the sources, the default system compiler (&lt;code&gt;c++&lt;/code&gt;) is used, which can be overridden by setting the &lt;code&gt;CXX&lt;/code&gt; environment variable. To pass additional arguments to the compilation process, &lt;code&gt;extra_cflags&lt;/code&gt; or &lt;code&gt;extra_ldflags&lt;/code&gt; can be provided. For example, to compile your extension with optimizations, pass &lt;code&gt;extra_cflags=['-O3']&lt;/code&gt;. You can also use &lt;code&gt;extra_cflags&lt;/code&gt; to pass further include directories.</source>
          <target state="translated">ソースをコンパイルするには、デフォルトのシステムコンパイラ（ &lt;code&gt;c++&lt;/code&gt; ）を使用します。これは、 &lt;code&gt;CXX&lt;/code&gt; 環境変数を設定することで上書きできます。コンパイルプロセスに追加の引数を渡すために、 &lt;code&gt;extra_cflags&lt;/code&gt; または &lt;code&gt;extra_ldflags&lt;/code&gt; を指定できます。たとえば、最適化を使用して拡張機能をコンパイルするには、 &lt;code&gt;extra_cflags=['-O3']&lt;/code&gt; を渡します。 &lt;code&gt;extra_cflags&lt;/code&gt; を使用して、さらにインクルードディレクトリを渡すこともできます。</target>
        </trans-unit>
        <trans-unit id="e66cd2fec20ece4ce742fe906879dd31b4bc3e7a" translate="yes" xml:space="preserve">
          <source>To compute log-probabilities for all classes, the &lt;code&gt;log_prob&lt;/code&gt; method can be used.</source>
          <target state="translated">すべてのクラスの対数確率を計算するには、 &lt;code&gt;log_prob&lt;/code&gt; メソッドを使用できます。</target>
        </trans-unit>
        <trans-unit id="77b8f89ffdd6b5d01dd6f264735e5d9852b97a53" translate="yes" xml:space="preserve">
          <source>To construct an &lt;a href=&quot;#torch.optim.Optimizer&quot;&gt;&lt;code&gt;Optimizer&lt;/code&gt;&lt;/a&gt; you have to give it an iterable containing the parameters (all should be &lt;code&gt;Variable&lt;/code&gt; s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.</source>
          <target state="translated">&lt;a href=&quot;#torch.optim.Optimizer&quot;&gt; &lt;code&gt;Optimizer&lt;/code&gt; &lt;/a&gt;を作成するには、最適化するパラメーター（すべて &lt;code&gt;Variable&lt;/code&gt; 必要があります）を含むイテラブルを与える必要があります。次に、学習率、重みの減衰など、オプティマイザー固有のオプションを指定できます。</target>
        </trans-unit>
        <trans-unit id="d8b9882386102437caf895d70f81d6894ddf1097" translate="yes" xml:space="preserve">
          <source>To counter the problem of shared memory file leaks, &lt;a href=&quot;#module-torch.multiprocessing&quot;&gt;&lt;code&gt;torch.multiprocessing&lt;/code&gt;&lt;/a&gt; will spawn a daemon named &lt;code&gt;torch_shm_manager&lt;/code&gt; that will isolate itself from the current process group, and will keep track of all shared memory allocations. Once all processes connected to it exit, it will wait a moment to ensure there will be no new connections, and will iterate over all shared memory files allocated by the group. If it finds that any of them still exist, they will be deallocated. We&amp;rsquo;ve tested this method and it proved to be robust to various failures. Still, if your system has high enough limits, and &lt;code&gt;file_descriptor&lt;/code&gt; is a supported strategy, we do not recommend switching to this one.</source>
          <target state="translated">共有メモリファイルのリークの問題に対処するために、&lt;a href=&quot;#module-torch.multiprocessing&quot;&gt; &lt;code&gt;torch.multiprocessing&lt;/code&gt; &lt;/a&gt;は、現在のプロセスグループから自身を分離し、すべての共有メモリ割り当てを追跡する &lt;code&gt;torch_shm_manager&lt;/code&gt; という名前のデーモンを生成します。接続されているすべてのプロセスが終了すると、新しい接続がないことを確認するためにしばらく待機し、グループによって割り当てられたすべての共有メモリファイルを繰り返し処理します。それらのいずれかがまだ存在していることが判明した場合、それらは割り当て解除されます。この方法をテストしたところ、さまざまな障害に対して堅牢であることが証明されました。それでも、システムに十分に高い制限があり、 &lt;code&gt;file_descriptor&lt;/code&gt; がサポートされている戦略である場合は、これに切り替えることはお勧めしません。</target>
        </trans-unit>
        <trans-unit id="bcee88acc8b939fa9c28961779b7b64da43214ae" translate="yes" xml:space="preserve">
          <source>To create a tensor with pre-existing data, use &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">既存のデータを使用してテンソルを作成するには、&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; を&lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="427cce7803261872f64a384400396e0d1af0bda9" translate="yes" xml:space="preserve">
          <source>To create a tensor with similar type but different size as another tensor, use &lt;code&gt;tensor.new_*&lt;/code&gt; creation ops.</source>
          <target state="translated">別のテンソルと同様のタイプでサイズが異なるテンソルを作成するには、 &lt;code&gt;tensor.new_*&lt;/code&gt; 作成操作を使用します。</target>
        </trans-unit>
        <trans-unit id="ed5a59600ac4cfd9c95f09892fb740465f6d0959" translate="yes" xml:space="preserve">
          <source>To create a tensor with specific size, use &lt;code&gt;torch.*&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="translated">特定のサイズ、用途とテンソル作成するには &lt;code&gt;torch.*&lt;/code&gt; テンソル作成OPS（参照&lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;創造オプスを&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="a84b4146d0aefbfce9733ab7943da8a8972fc9bd" translate="yes" xml:space="preserve">
          <source>To create a tensor with the same size (and similar types) as another tensor, use &lt;code&gt;torch.*_like&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="translated">別のテンソル、使用のものと同じ大きさ（と似たタイプ）とのテンソルを作成するには &lt;code&gt;torch.*_like&lt;/code&gt; テンソル作成OPS（参照&lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;創造オプスを&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="e323c9fb04b24bbe21e136c6130c8b1036dfb2e1" translate="yes" xml:space="preserve">
          <source>To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (&amp;ldquo;CPU total time is much greater than CUDA total time&amp;rdquo;). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.</source>
          <target state="translated">どの（CPUのみモードまたはCUDAモード）autogradプロファイラー出力を確認するかを決定するには、最初にスクリプトがCPUバウンドであるかどうかを確認する必要があります（「CPU合計時間はCUDA合計時間よりはるかに長い」）。CPUにバインドされている場合は、CPUモードのautogradプロファイラーの結果を確認すると役立ちます。一方、スクリプトがほとんどの時間をGPUでの実行に費やしている場合は、CUDAモードのautogradプロファイラーの出力で責任のあるCUDAオペレーターを探し始めるのが理にかなっています。</target>
        </trans-unit>
        <trans-unit id="1d6487001340d96dfbd24e0dfe67d91d448bd378" translate="yes" xml:space="preserve">
          <source>To enable &lt;code&gt;backend == Backend.MPI&lt;/code&gt;, PyTorch needs to be built from source on a system that supports MPI.</source>
          <target state="translated">&lt;code&gt;backend == Backend.MPI&lt;/code&gt; を有効にするには、MPIをサポートするシステム上のソースからPyTorchをビルドする必要があります。</target>
        </trans-unit>
        <trans-unit id="928a8a1dba70cf38d7b64d41e9f2de79497ca1b2" translate="yes" xml:space="preserve">
          <source>To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a &lt;code&gt;Future&lt;/code&gt; object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with &lt;code&gt;@staticmethod&lt;/code&gt; or &lt;code&gt;@classmethod&lt;/code&gt;, &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt;.</source>
          <target state="translated">非同期実行を有効にするには、アプリケーションはこのデコレータによって返された関数オブジェクトをRPCAPIに渡す必要があります。RPCがこのデコレータによってインストールされた属性を検出した場合、RPCはこの関数が &lt;code&gt;Future&lt;/code&gt; オブジェクトを返し、それに応じてそれを処理することを認識しています。ただし、これは、関数を定義するときにこのデコレータが最も外側にある必要があるという意味ではありません。たとえば、 &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; &lt;code&gt;@staticmethod&lt;/code&gt; または &lt;code&gt;@classmethod&lt;/code&gt; と組み合わせる場合、ターゲット関数を静的関数またはクラス関数として認識できるようにするには、@ rpc.functions.async_executionを内部デコレータにする必要があります。静的メソッドまたはクラスメソッドは、アクセスされたときに &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; によってインストールされた属性を保持するため、このターゲット関数は引き続き非同期で実行できます。。</target>
        </trans-unit>
        <trans-unit id="16234789078f98ba40aad60005ffa95f2e9e8ac6" translate="yes" xml:space="preserve">
          <source>To ensure that the correct number of threads is used, set_num_threads must be called before running eager, JIT or autograd code.</source>
          <target state="translated">正しいスレッド数が使用されるようにするには、eager、JIT、またはautogradコードを実行する前にset_num_threadsを呼び出す必要があります。</target>
        </trans-unit>
        <trans-unit id="2cd4366390e742467d09cd376578959df8585e90" translate="yes" xml:space="preserve">
          <source>To export a raw ir.</source>
          <target state="translated">生のIRをエクスポートするには</target>
        </trans-unit>
        <trans-unit id="13a45cf8b5a671e88848c78052e09b8b9f6c109f" translate="yes" xml:space="preserve">
          <source>To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly. In the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.</source>
          <target state="translated">ONNXでサポートされていないATenオペレータにフォールバックすること。サポートされている演算子は、定期的にONNXにエクスポートされます。次の例では、aten::triuはONNXではサポートされていません。エクスポータはこの演算子にフォールバックする。</target>
        </trans-unit>
        <trans-unit id="64ba89c98e2ef72ba75fa028267bbaa3251bdd0e" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a complex data type, the property &lt;a href=&quot;generated/torch.is_complex#torch.is_complex&quot;&gt;&lt;code&gt;is_complex&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a complex data type.</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;が複素数データ型であるかどうかを確認するには、プロパティ&lt;a href=&quot;generated/torch.is_complex#torch.is_complex&quot;&gt; &lt;code&gt;is_complex&lt;/code&gt; &lt;/a&gt;を使用できます。これは、データ型が複素数データ型の場合に &lt;code&gt;True&lt;/code&gt; を返します。</target>
        </trans-unit>
        <trans-unit id="4bde254844ec10d73414a4f9097cbf8e8d5d1c09" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a floating point data type, the property &lt;a href=&quot;generated/torch.is_floating_point#torch.is_floating_point&quot;&gt;&lt;code&gt;is_floating_point&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a floating point data type.</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;が浮動小数点データ型であるかどうかを確認するには、プロパティ&lt;a href=&quot;generated/torch.is_floating_point#torch.is_floating_point&quot;&gt; &lt;code&gt;is_floating_point&lt;/code&gt; &lt;/a&gt;を使用できます。これは、データ型が浮動小数点データ型の場合に &lt;code&gt;True&lt;/code&gt; を返します。</target>
        </trans-unit>
        <trans-unit id="48e44e1c5bae8e339aa29cc00850c62cff2a0ea7" translate="yes" xml:space="preserve">
          <source>To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It&amp;rsquo;s also helpful to include a minimal working example.</source>
          <target state="translated">ユーザーがドキュメントを前後に参照せずに探索できるように、リポジトリの所有者は関数のヘルプメッセージを明確かつ簡潔にすることを強くお勧めします。最小限の実例を含めることも役立ちます。</target>
        </trans-unit>
        <trans-unit id="e334bd6959cf3a682b73059d2817327b2e249b8f" translate="yes" xml:space="preserve">
          <source>To iterate over the full Cartesian product use &lt;code&gt;itertools.product(m.enumerate_support())&lt;/code&gt;.</source>
          <target state="translated">デカルト積全体を反復処理するには、 &lt;code&gt;itertools.product(m.enumerate_support())&lt;/code&gt; を使用します。</target>
        </trans-unit>
        <trans-unit id="6e370e4d333aa52e5158136385c8ce683abfc7e1" translate="yes" xml:space="preserve">
          <source>To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.</source>
          <target state="translated">拡張機能をロードするには、Ninja ビルドファイルが発行され、与えられたソースをダイナミックライブラリにコンパイルするために使用されます。このライブラリはモジュールとして現在の Python プロセスにロードされ、この関数から返されます。</target>
        </trans-unit>
        <trans-unit id="0f1500bbb514e4eeb3d360549b7085361d0903f1" translate="yes" xml:space="preserve">
          <source>To look up what optional arguments this module offers:</source>
          <target state="translated">このモジュールが提供するオプションの引数を調べます。</target>
        </trans-unit>
        <trans-unit id="efe5a1ff1c27c51217ed9e88d6d14c167e44ae98" translate="yes" xml:space="preserve">
          <source>To make it easier to understand, here is a small example:</source>
          <target state="translated">わかりやすくするために、ちょっとした例をご紹介します。</target>
        </trans-unit>
        <trans-unit id="4861c71c5547ba6da8095cee191825540e015436" translate="yes" xml:space="preserve">
          <source>To make writing TorchScript more convenient, we allow script code to refer to Python values in the surrounding scope. For instance, any time there is a reference to &lt;code&gt;torch&lt;/code&gt;, the TorchScript compiler is actually resolving it to the &lt;code&gt;torch&lt;/code&gt; Python module when the function is declared. These Python values are not a first class part of TorchScript. Instead they are de-sugared at compile-time into the primitive types that TorchScript supports. This depends on the dynamic type of the Python valued referenced when compilation occurs. This section describes the rules that are used when accessing Python values in TorchScript.</source>
          <target state="translated">TorchScriptの記述をより便利にするために、スクリプトコードが周囲のスコープ内のPython値を参照できるようにします。たとえば、 &lt;code&gt;torch&lt;/code&gt; への参照があるときはいつでも、関数が宣言されたときにTorchScriptコンパイラが実際に &lt;code&gt;torch&lt;/code&gt; Pythonモジュールにそれを解決しています。これらのPython値は、TorchScriptのファーストクラスの部分ではありません。代わりに、コンパイル時にTorchScriptがサポートするプリミティブ型に糖抜きされます。これは、コンパイル時に参照されるPython値の動的タイプによって異なります。このセクションでは、TorchScriptでPython値にアクセスするときに使用されるルールについて説明します。</target>
        </trans-unit>
        <trans-unit id="56b4b291b8193f650ab440b4d0108041657026e3" translate="yes" xml:space="preserve">
          <source>To obtain repeatable results, reset the seed for the pseudorandom number generator</source>
          <target state="translated">再現性のある結果を得るためには、擬似乱数発生器のシードをリセットします。</target>
        </trans-unit>
        <trans-unit id="693cb0335aff5d84a8ad039de675ef1c71894064" translate="yes" xml:space="preserve">
          <source>To prevent underflow, &amp;ldquo;gradient scaling&amp;rdquo; multiplies the network&amp;rsquo;s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don&amp;rsquo;t flush to zero.</source>
          <target state="translated">アンダーフローを防ぐために、「勾配スケーリング」はネットワークの損失にスケール係数を掛け、スケーリングされた損失の逆方向パスを呼び出します。次に、ネットワークを逆方向に流れるグラデーションが同じ係数でスケーリングされます。つまり、グラデーション値の大きさが大きいため、ゼロにフラッシュされません。</target>
        </trans-unit>
        <trans-unit id="cdaaaf6ab8e173c9d7ceb41b8dea6cb46e09c870" translate="yes" xml:space="preserve">
          <source>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</source>
          <target state="translated">カスタマイズされた追加情報を印刷するには、自分のモジュールでこの方法を再実装する必要があります。一行文字列でも複数行文字列でもどちらでも構いません。</target>
        </trans-unit>
        <trans-unit id="ff43c0b33f5574b95fded09d54e409788745cf85" translate="yes" xml:space="preserve">
          <source>To run the exported script with &lt;a href=&quot;https://caffe2.ai/&quot;&gt;caffe2&lt;/a&gt;, you will need to install &lt;code&gt;caffe2&lt;/code&gt;: If you don&amp;rsquo;t have one already, Please &lt;a href=&quot;https://caffe2.ai/docs/getting-started.html&quot;&gt;follow the install instructions&lt;/a&gt;.</source>
          <target state="translated">エクスポートされたスクリプトを&lt;a href=&quot;https://caffe2.ai/&quot;&gt;caffe2&lt;/a&gt;で実行するには、 &lt;code&gt;caffe2&lt;/code&gt; をインストールする必要があります。まだインストールしていない場合&lt;a href=&quot;https://caffe2.ai/docs/getting-started.html&quot;&gt;は、インストール手順に従って&lt;/a&gt;ください。</target>
        </trans-unit>
        <trans-unit id="c62b3cc0b531750df7ee34e20beaf808665f78f8" translate="yes" xml:space="preserve">
          <source>To specify the scale, it takes either the &lt;code&gt;size&lt;/code&gt; or the &lt;code&gt;scale_factor&lt;/code&gt; as it&amp;rsquo;s constructor argument.</source>
          <target state="translated">スケールを指定するには、コンストラクター引数として &lt;code&gt;size&lt;/code&gt; または &lt;code&gt;scale_factor&lt;/code&gt; のいずれかを取ります。</target>
        </trans-unit>
        <trans-unit id="8bc8fb9e3000235214e485d36fb9186012476b04" translate="yes" xml:space="preserve">
          <source>To stop the compiler from compiling a method, add &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;@ignore&lt;/code&gt; leaves the</source>
          <target state="translated">コンパイラがメソッドをコンパイルしないようにするには、&lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt;追加します。 &lt;code&gt;@ignore&lt;/code&gt; は</target>
        </trans-unit>
        <trans-unit id="83e075942db53d26dad4b0cea801b6df02d892d5" translate="yes" xml:space="preserve">
          <source>To take a batch diagonal, pass in dim1=-2, dim2=-1.</source>
          <target state="translated">バッチの対角線を取るには、dim1=-2、dim2=-1を渡します。</target>
        </trans-unit>
        <trans-unit id="06d7e723aad8726de16a32b689408fd5f6a39bff" translate="yes" xml:space="preserve">
          <source>To trace a specific method on a module, see &lt;a href=&quot;generated/torch.jit.trace_module#torch.jit.trace_module&quot;&gt;&lt;code&gt;torch.jit.trace_module&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">モジュールの特定のメソッドをトレースするには、&lt;a href=&quot;generated/torch.jit.trace_module#torch.jit.trace_module&quot;&gt; &lt;code&gt;torch.jit.trace_module&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="513b8c269a49217eb8a34177b8e8cfe4d36acacc" translate="yes" xml:space="preserve">
          <source>To use &lt;a href=&quot;#module-torch.optim&quot;&gt;&lt;code&gt;torch.optim&lt;/code&gt;&lt;/a&gt; you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.</source>
          <target state="translated">&lt;a href=&quot;#module-torch.optim&quot;&gt; &lt;code&gt;torch.optim&lt;/code&gt; &lt;/a&gt;を使用するには、現在の状態を保持し、計算された勾配に基づいてパラメーターを更新するオプティマイザーオブジェクトを作成する必要があります。</target>
        </trans-unit>
        <trans-unit id="7cbbf69a127d5627b468752b233e45c932bef649" translate="yes" xml:space="preserve">
          <source>To use &lt;code&gt;DistributedDataParallel&lt;/code&gt; on a host with N GPUs, you should spawn up &lt;code&gt;N&lt;/code&gt; processes, ensuring that each process exclusively works on a single GPU from 0 to N-1. This can be done by either setting &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; for every process or by calling:</source>
          <target state="translated">N GPUを搭載したホストで &lt;code&gt;DistributedDataParallel&lt;/code&gt; を使用するには、 &lt;code&gt;N&lt;/code&gt; プロセスを生成し、各プロセスが0からN-1までの単一のGPUで排他的に動作するようにする必要があります。これは、すべてのプロセスに &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; を設定するか、以下を呼び出すことによって実行できます。</target>
        </trans-unit>
        <trans-unit id="55e1a9559362bb4b36c706d26afd5c7744bfa4b4" translate="yes" xml:space="preserve">
          <source>To use a &lt;code&gt;nn.ModuleList&lt;/code&gt; inside a compiled method, it must be marked constant by adding the name of the attribute to the &lt;code&gt;__constants__&lt;/code&gt; list for the type. For loops over a &lt;code&gt;nn.ModuleList&lt;/code&gt; will unroll the body of the loop at compile time, with each member of the constant module list.</source>
          <target state="translated">コンパイルされたメソッド内で &lt;code&gt;nn.ModuleList&lt;/code&gt; を使用するには、型の &lt;code&gt;__constants__&lt;/code&gt; リストに属性の名前を追加して定数としてマークする必要があります。 &lt;code&gt;nn.ModuleList&lt;/code&gt; を介したforループは、コンパイル時にループの本体を展開し、定数モジュールリストの各メンバーを使用します。</target>
        </trans-unit>
        <trans-unit id="527375a7ef1d0e262542dedd5c0253dc05250f0d" translate="yes" xml:space="preserve">
          <source>To use these functions the torch.fft module must be imported since its name conflicts with the &lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">これらの関数を使用するには、torch.fftモジュールの名前が&lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt; &lt;code&gt;torch.fft()&lt;/code&gt; &lt;/a&gt;関数と競合するため、torch.fftモジュールをインポートする必要があります。</target>
        </trans-unit>
        <trans-unit id="43427ba5c0a00d9ec53c4149e3f5de362381fec9" translate="yes" xml:space="preserve">
          <source>To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.</source>
          <target state="translated">これを使用して、プロセス間で入力が不均一なトレーニングを可能にするには、このコンテキストマネージャーをトレーニングループの周りにラップするだけです。モデルの変更やデータのロードは必要ありません。</target>
        </trans-unit>
        <trans-unit id="6239832323d9bb66e30c43f099010c4cf674fbce" translate="yes" xml:space="preserve">
          <source>To utilize &lt;em&gt;script-based&lt;/em&gt; exporter for capturing the dynamic loop, we can write the loop in script, and call it from the regular nn.Module:</source>
          <target state="translated">&lt;em&gt;スクリプトベースの&lt;/em&gt;エクスポーターを利用して動的ループをキャプチャするには、スクリプトでループを記述し、通常のnn.Moduleから呼び出します。</target>
        </trans-unit>
        <trans-unit id="111ccaae56ca75ec4b6793420aa1331efeccd7da" translate="yes" xml:space="preserve">
          <source>Top-1 error</source>
          <target state="translated">トップ1エラー</target>
        </trans-unit>
        <trans-unit id="f1240994c38def01302ee353c1f3e6ab783c6922" translate="yes" xml:space="preserve">
          <source>Top-5 error</source>
          <target state="translated">トップ5エラー</target>
        </trans-unit>
        <trans-unit id="ce0f1f9bf9ba00bef8a66791462d63e5a977f58e" translate="yes" xml:space="preserve">
          <source>Top-level quantization APIs</source>
          <target state="translated">トップレベルの量子化API</target>
        </trans-unit>
        <trans-unit id="ecebe7e89f3537ec9d48a23fefb140aef1ac7e73" translate="yes" xml:space="preserve">
          <source>Torch defines 10 tensor types with CPU and GPU variants which are as follows:</source>
          <target state="translated">Torchでは、CPUとGPUのバリアントを持つ10種類のテンソルを定義しており、以下のようになっています。</target>
        </trans-unit>
        <trans-unit id="2270c240add89c3f53429a527826adf7f8c68848" translate="yes" xml:space="preserve">
          <source>Torch hub works by importing the package as if it was installed. There&amp;rsquo;re some side effects introduced by importing in Python. For example, you can see new items in Python caches &lt;code&gt;sys.modules&lt;/code&gt; and &lt;code&gt;sys.path_importer_cache&lt;/code&gt; which is normal Python behavior.</source>
          <target state="translated">トーチハブは、パッケージがインストールされているかのようにインポートすることで機能します。Pythonにインポートすることで発生するいくつかの副作用があります。たとえば、Pythonキャッシュ &lt;code&gt;sys.modules&lt;/code&gt; および &lt;code&gt;sys.path_importer_cache&lt;/code&gt; に新しいアイテムが表示されます。これは通常のPythonの動作です。</target>
        </trans-unit>
        <trans-unit id="e678d1ba0a8c71917974dd6b809d475b07d566ed" translate="yes" xml:space="preserve">
          <source>Torch mobile supports &lt;code&gt;torch.mobile_optimizer.optimize_for_mobile&lt;/code&gt; utility to run a list of optimization pass with modules in eval mode. The method takes the following parameters: a torch.jit.ScriptModule object, a blacklisting optimization set and a preserved method list</source>
          <target state="translated">Torch mobileは、 &lt;code&gt;torch.mobile_optimizer.optimize_for_mobile&lt;/code&gt; ユーティリティをサポートして、評価モードのモジュールで最適化パスのリストを実行します。このメソッドは、次のパラメーターを取ります：torch.jit.ScriptModuleオブジェクト、ブラックリスト最適化セット、および保存されたメソッドリスト</target>
        </trans-unit>
        <trans-unit id="03340697e5da4f35bf9335b934c01e6bb269d07d" translate="yes" xml:space="preserve">
          <source>Torch supports sparse tensors in COO(rdinate) format, which can efficiently store and process tensors for which the majority of elements are zeros.</source>
          <target state="translated">TorchはCOO(rdinate)形式のスパーステンソルをサポートしており、要素の大部分が0であるテンソルを効率的に格納して処理することができます。</target>
        </trans-unit>
        <trans-unit id="efa8e2bf58145beaa70cc1139b800b0cc73441ae" translate="yes" xml:space="preserve">
          <source>TorchElastic</source>
          <target state="translated">TorchElastic</target>
        </trans-unit>
        <trans-unit id="3b90040dd3c7ea550e3ae7ebb31cbbf38c50d775" translate="yes" xml:space="preserve">
          <source>TorchScript</source>
          <target state="translated">TorchScript</target>
        </trans-unit>
        <trans-unit id="604c8e5563e16356b9e1a864727a3495aaa4b8f6" translate="yes" xml:space="preserve">
          <source>TorchScript Classes</source>
          <target state="translated">TorchScript クラス</target>
        </trans-unit>
        <trans-unit id="5d73d728706f6b5bfb8bc6488f8b896532317b47" translate="yes" xml:space="preserve">
          <source>TorchScript Enums</source>
          <target state="translated">TorchScript Enums</target>
        </trans-unit>
        <trans-unit id="2d3d498afcd6dbfb2767a5e73bfdf3c641826801" translate="yes" xml:space="preserve">
          <source>TorchScript Language</source>
          <target state="translated">トーチスクリプト言語</target>
        </trans-unit>
        <trans-unit id="655dca0ad6a50c42b950785617112aa26c2b7ea8" translate="yes" xml:space="preserve">
          <source>TorchScript Language Reference</source>
          <target state="translated">TorchScript 言語リファレンス</target>
        </trans-unit>
        <trans-unit id="4ca55dbce27b090f25e1beef5f42bc0b74a3e196" translate="yes" xml:space="preserve">
          <source>TorchScript Unsupported Pytorch Constructs</source>
          <target state="translated">TorchScript がサポートされていない Pytorch コンストラクタ</target>
        </trans-unit>
        <trans-unit id="f37751da08a275fc84f6719af858bb6fca4efcaa" translate="yes" xml:space="preserve">
          <source>TorchScript also has a representation at a lower level than the code pretty- printer, in the form of IR graphs.</source>
          <target state="translated">TorchScriptは、IRグラフの形で、コードのプリティプリンタよりも低いレベルの表現も持っています。</target>
        </trans-unit>
        <trans-unit id="1f6a6dd855f78002eaa186408d37b5a14d38fb6e" translate="yes" xml:space="preserve">
          <source>TorchScript also provides a way to use constants that are defined in Python. These can be used to hard-code hyper-parameters into the function, or to define universal constants. There are two ways of specifying that a Python value should be treated as a constant.</source>
          <target state="translated">TorchScriptは、Pythonで定義されている定数を使用する方法も提供しています。これらは、関数にハイパーパラメータをハードコーディングしたり、普遍的な定数を定義したりするために使用することができます。Pythonの値を定数として扱うことを指定するには、2つの方法があります。</target>
        </trans-unit>
        <trans-unit id="252b708f15ebb50ba55ac0b5f321607abb84f702" translate="yes" xml:space="preserve">
          <source>TorchScript can call Python functions. This functionality is very useful when incrementally converting a model to TorchScript. The model can be moved function-by-function to TorchScript, leaving calls to Python functions in place. This way you can incrementally check the correctness of the model as you go.</source>
          <target state="translated">TorchScriptはPythonの関数を呼び出すことができます。この機能は、モデルをインクリメンタルにTorchScriptに変換する際に非常に便利です。Pythonの関数を呼び出したまま、モデルを関数単位でTorchScriptに移動させることができます。このようにして、モデルが正しいかどうかをインクリメンタルにチェックすることができます。</target>
        </trans-unit>
        <trans-unit id="0e634a60c8e32dbe73734d8f74e5fc62a8bbd892" translate="yes" xml:space="preserve">
          <source>TorchScript can lookup attributes on modules. &lt;code&gt;Builtin functions&lt;/code&gt; like &lt;code&gt;torch.add&lt;/code&gt; are accessed this way. This allows TorchScript to call functions defined in other modules.</source>
          <target state="translated">TorchScriptは、モジュールの属性を検索できます。 &lt;code&gt;torch.add&lt;/code&gt; のような組み込み &lt;code&gt;Builtin functions&lt;/code&gt; はこの方法でアクセスされます。これにより、TorchScriptは他のモジュールで定義された関数を呼び出すことができます。</target>
        </trans-unit>
        <trans-unit id="05a14206c4e373e5e12c57e024877cf642631ecf" translate="yes" xml:space="preserve">
          <source>TorchScript class support is experimental. Currently it is best suited for simple record-like types (think a &lt;code&gt;NamedTuple&lt;/code&gt; with methods attached).</source>
          <target state="translated">TorchScriptクラスのサポートは実験的なものです。現在、単純なレコードのようなタイプに最適です（メソッドがアタッチされた &lt;code&gt;NamedTuple&lt;/code&gt; を考えてください）。</target>
        </trans-unit>
        <trans-unit id="40d12a53eedc122f7c1c26cc26562fe87abc6002" translate="yes" xml:space="preserve">
          <source>TorchScript classes are statically typed. Members can only be declared by assigning to self in the &lt;code&gt;__init__()&lt;/code&gt; method.</source>
          <target state="translated">TorchScriptクラスは静的に型付けされます。メンバーは、 &lt;code&gt;__init__()&lt;/code&gt; メソッドでselfに割り当てることによってのみ宣言できます。</target>
        </trans-unit>
        <trans-unit id="6fd4fa394dbd8f52548eb83a0ab865caf3d46d44" translate="yes" xml:space="preserve">
          <source>TorchScript does not support &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#bytes&quot;&gt;&lt;code&gt;bytes&lt;/code&gt;&lt;/a&gt; so this type is not used</source>
          <target state="translated">TorchScriptは&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#bytes&quot;&gt; &lt;code&gt;bytes&lt;/code&gt; &lt;/a&gt;サポートしていないため、このタイプは使用されません</target>
        </trans-unit>
        <trans-unit id="95d83676fd42ab283c45a040ec05adf9e17c4b2a" translate="yes" xml:space="preserve">
          <source>TorchScript does not support all features and types of the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module. Some of these are more fundamental things that are unlikely to be added in the future while others may be added if there is enough user demand to make it a priority.</source>
          <target state="translated">TorchScriptは、&lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt;モジュールのすべての機能とタイプをサポートしているわけではありません。これらのいくつかは、将来追加される可能性が低いより基本的なものですが、それを優先するのに十分なユーザーの要求がある場合は、他のものが追加される可能性があります。</target>
        </trans-unit>
        <trans-unit id="155bc1507dbbed5e44c922d48bd933f969683779" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python that can either be written directly (using the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; decorator) or generated automatically from Python code via tracing. When using tracing, code is automatically converted into this subset of Python by recording only the actual operators on tensors and simply executing and discarding the other surrounding Python code.</source>
          <target state="translated">TorchScriptは、静的に型指定されたPythonのサブセットであり、直接（&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt;デコレータを使用して）書き込むことも、トレースを介してPythonコードから自動的に生成することもできます。トレースを使用する場合、コードはテンソルに実際の演算子のみを記録し、周囲の他のPythonコードを実行して破棄するだけで、Pythonのこのサブセットに自動的に変換されます。</target>
        </trans-unit>
        <trans-unit id="6e3be4ae1ba105244947e696b16b923455648d05" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full &lt;a href=&quot;jit_language_reference#language-reference&quot;&gt;TorchScript Language Reference&lt;/a&gt; for details.</source>
          <target state="translated">TorchScriptは静的に型指定されたPythonのサブセットであるため、多くのPython機能がTorchScriptに直接適用されます。詳細については、完全な&lt;a href=&quot;jit_language_reference#language-reference&quot;&gt;TorchScript言語リファレンス&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="b80f73ada7844bf2ed31f70d01150d3e0f521094" translate="yes" xml:space="preserve">
          <source>TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.</source>
          <target state="translated">TorchScriptは、PyTorchコードからシリアライズ可能で最適化可能なモデルを作成する方法です。任意のTorchScriptプログラムをPythonプロセスから保存し、Pythonに依存しないプロセスにロードすることができます。</target>
        </trans-unit>
        <trans-unit id="f3730c4db0b9ee76825d3365cc7e6ee5d81c09f3" translate="yes" xml:space="preserve">
          <source>TorchScript provides a code pretty-printer for all &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; instances. This pretty-printer gives an interpretation of the script method&amp;rsquo;s code as valid Python syntax. For example:</source>
          <target state="translated">TorchScriptは、すべての&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;インスタンスにコードプリティプリンターを提供します。このプリティプリンターは、スクリプトメソッドのコードを有効なPython構文として解釈します。例えば：</target>
        </trans-unit>
        <trans-unit id="6f800d70869c5af16f03f352efad0babdc4a4cee" translate="yes" xml:space="preserve">
          <source>TorchScript support in RPC is a prototype feature and subject to change. Since v1.5.0, &lt;code&gt;torch.distributed.rpc&lt;/code&gt; supports calling TorchScript functions as RPC target functions, and this will help improve parallelism on the callee side as executing TorchScript functions does not require GIL.</source>
          <target state="translated">RPCでのTorchScriptのサポートはプロトタイプ機能であり、変更される可能性があります。 &lt;code&gt;torch.distributed.rpc&lt;/code&gt; 以降、torch.distributed.rpcはRPCターゲット関数としてのTorchScript関数の呼び出しをサポートします。これにより、TorchScript関数の実行にGILが必要ないため、呼び出し先側の並列処理が向上します。</target>
        </trans-unit>
        <trans-unit id="7d483bee6192edebb2157f5cffc7ebe0cf7bf9b5" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of Python&amp;rsquo;s variable resolution (i.e. scoping) rules. Local variables behave the same as in Python, except for the restriction that a variable must have the same type along all paths through a function. If a variable has a different type on different branches of an if statement, it is an error to use it after the end of the if statement.</source>
          <target state="translated">TorchScriptは、Pythonの可変解決（つまりスコープ）ルールのサブセットをサポートします。ローカル変数は、関数を通るすべてのパスで変数が同じ型でなければならないという制限を除いて、Pythonと同じように動作します。変数がifステートメントの異なるブランチで異なるタイプを持っている場合、ifステートメントの終了後にそれを使用することはエラーです。</target>
        </trans-unit>
        <trans-unit id="439e6a570c3e92811a193d2bb2eb0c532aac85c2" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the &lt;code&gt;torch&lt;/code&gt; namespace, all functions in &lt;code&gt;torch.nn.functional&lt;/code&gt; and most modules from &lt;code&gt;torch.nn&lt;/code&gt; are supported in TorchScript.</source>
          <target state="translated">TorchScriptは、PyTorchが提供するテンソルおよびニューラルネットワーク関数のサブセットをサポートします。Tensorのほとんどのメソッドと、 &lt;code&gt;torch&lt;/code&gt; 名前空間の関数、 &lt;code&gt;torch.nn.functional&lt;/code&gt; のすべての関数、およびtorch.nnのほとんどのモジュールが &lt;code&gt;torch.nn&lt;/code&gt; でサポートされています。</target>
        </trans-unit>
        <trans-unit id="85815975862ed9b632ab9ab2cb6516181e624194" translate="yes" xml:space="preserve">
          <source>TorchScript supports the following types of statements:</source>
          <target state="translated">TorchScriptは以下のタイプのステートメントをサポートしています。</target>
        </trans-unit>
        <trans-unit id="237a371f4a7260a3005de6488e3d8470546b5ec9" translate="yes" xml:space="preserve">
          <source>TorchScript supports the use of most PyTorch functions and many Python built-ins. See &lt;a href=&quot;jit_builtin_functions#builtin-functions&quot;&gt;TorchScript Builtins&lt;/a&gt; for a full reference of supported functions.</source>
          <target state="translated">TorchScriptは、ほとんどのPyTorch関数と多くのPython組み込み関数の使用をサポートしています。サポートされている関数の完全なリファレンスについては、&lt;a href=&quot;jit_builtin_functions#builtin-functions&quot;&gt;TorchScriptBuiltinsを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="f626090730ab6e08eee60582294fe0a9f90efe3d" translate="yes" xml:space="preserve">
          <source>TorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example:</source>
          <target state="translated">TorchScript は、計算を表現するために静的単一代入(SSA)中間表現(IR)を使用します。この形式の命令は、ATen(PyTorchのC++バックエンド)演算子と、ループや条件式の制御フロー演算子を含む他のプリミティブ演算子で構成されています。一例として</target>
        </trans-unit>
        <trans-unit id="ee07cfab05a06e606f9327817d635689c5cd9b5d" translate="yes" xml:space="preserve">
          <source>TorchScript will refine the type of a variable of type &lt;code&gt;Optional[T]&lt;/code&gt; when a comparison to &lt;code&gt;None&lt;/code&gt; is made inside the conditional of an if-statement or checked in an &lt;code&gt;assert&lt;/code&gt;. The compiler can reason about multiple &lt;code&gt;None&lt;/code&gt; checks that are combined with &lt;code&gt;and&lt;/code&gt;, &lt;code&gt;or&lt;/code&gt;, and &lt;code&gt;not&lt;/code&gt;. Refinement will also occur for else blocks of if-statements that are not explicitly written.</source>
          <target state="translated">TorchScriptは、ifステートメントの条件内で &lt;code&gt;None&lt;/code&gt; との比較が行われるか、 &lt;code&gt;assert&lt;/code&gt; でチェックされると、 &lt;code&gt;Optional[T]&lt;/code&gt; 型の変数の型を調整します。コンパイラーは、 &lt;code&gt;and&lt;/code&gt; 、 &lt;code&gt;or&lt;/code&gt; 、および &lt;code&gt;not&lt;/code&gt; と組み合わされた複数の &lt;code&gt;None&lt;/code&gt; チェックについて推論できます。明示的に記述されていないifステートメントのelseブロックに対しても、絞り込みが行われます。</target>
        </trans-unit>
        <trans-unit id="db3578de30ac281698832393e03259941ecc59a4" translate="yes" xml:space="preserve">
          <source>TorchServe</source>
          <target state="translated">TorchServe</target>
        </trans-unit>
        <trans-unit id="174786ece7d01b50c5a292bbe7c461a354fd217b" translate="yes" xml:space="preserve">
          <source>TorchVision support</source>
          <target state="translated">トーチビジョンのサポート</target>
        </trans-unit>
        <trans-unit id="eb01e02a87150ef514202bea3337f1bbd7e6a429" translate="yes" xml:space="preserve">
          <source>Total norm of the parameters (viewed as a single vector).</source>
          <target state="translated">パラメータの全ノルム(1つのベクトルとして見た場合).</target>
        </trans-unit>
        <trans-unit id="117b4c950645374477e1f2b2a96517111745fe19" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="translated">関数をトレースし、ジャストインタイムコンパイルを使用して最適化される実行可能ファイルまたは&lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;を返します。</target>
        </trans-unit>
        <trans-unit id="4cae3f60263770c81b066cebd1a77d6ae0c2a925" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. Tracing is ideal for code that operates only on &lt;code&gt;Tensor&lt;/code&gt;s and lists, dictionaries, and tuples of &lt;code&gt;Tensor&lt;/code&gt;s.</source>
          <target state="translated">関数をトレースし、ジャストインタイムコンパイルを使用して最適化される実行可能ファイルまたは&lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;を返します。トレースはのみで動作するコードのための理想的である &lt;code&gt;Tensor&lt;/code&gt; Sとリスト、辞書、タプルの &lt;code&gt;Tensor&lt;/code&gt; S。</target>
        </trans-unit>
        <trans-unit id="20f26cd87e3024b30b8eeb8b74edbe9cdd81f4b6" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="translated">モジュールをトレースし、ジャストインタイムコンパイルを使用して最適化される実行可能&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;を返します。</target>
        </trans-unit>
        <trans-unit id="3bad49a27d3b91711b9d48661ccd1bb2133d17d3" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. When a module is passed to &lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt;&lt;code&gt;torch.jit.trace&lt;/code&gt;&lt;/a&gt;, only the &lt;code&gt;forward&lt;/code&gt; method is run and traced. With &lt;code&gt;trace_module&lt;/code&gt;, you can specify a dictionary of method names to example inputs to trace (see the &lt;code&gt;inputs&lt;/code&gt;) argument below.</source>
          <target state="translated">モジュールをトレースし、ジャストインタイムコンパイルを使用して最適化される実行可能&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;を返します。モジュールが&lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt; &lt;code&gt;torch.jit.trace&lt;/code&gt; &lt;/a&gt;に渡されると、 &lt;code&gt;forward&lt;/code&gt; メソッドのみが実行されてトレースされます。では &lt;code&gt;trace_module&lt;/code&gt; 、あなたがトレースする例入力にメソッド名の辞書を指定することができます（参照 &lt;code&gt;inputs&lt;/code&gt; 以下の引数を）。</target>
        </trans-unit>
        <trans-unit id="d93ef3f27cd83786d560cf0e7159be09c5658df2" translate="yes" xml:space="preserve">
          <source>Traced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly.</source>
          <target state="translated">トレースされた関数はスクリプト関数を呼び出すことができます。これは、モデルの大部分が単なるフィードフォワード・ネットワークであるにもかかわらず、モデルのごく一部が何らかの制御フローを必要とする場合に便利です。トレース関数によって呼び出されたスクリプト関数内の制御フローは正しく保存されます。</target>
        </trans-unit>
        <trans-unit id="0fce73bbb5357ac93c61680e5f58be84db277c7b" translate="yes" xml:space="preserve">
          <source>Tracer</source>
          <target state="translated">Tracer</target>
        </trans-unit>
        <trans-unit id="d09bb53da249750b0b3b27f27eabe9dbddadc0ed" translate="yes" xml:space="preserve">
          <source>Tracer Warnings</source>
          <target state="translated">トレーサーの警告</target>
        </trans-unit>
        <trans-unit id="bf933c3083c5a6cb4cedac1b1e0c22d6553dab1d" translate="yes" xml:space="preserve">
          <source>Tracing Edge Cases</source>
          <target state="translated">エッジケースのトレース</target>
        </trans-unit>
        <trans-unit id="c5153b07a6f011eb8b3585b8b94ddfea62a7a0ce" translate="yes" xml:space="preserve">
          <source>Tracing of control flow that is dependent on inputs (e.g. tensor shapes)</source>
          <target state="translated">入力に依存する制御フローのトレース(テンソル形状など</target>
        </trans-unit>
        <trans-unit id="656eeb39a9810745d15b70b8f4cca3cf416cf8d6" translate="yes" xml:space="preserve">
          <source>Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)</source>
          <target state="translated">テンソルビューのインプレース操作のトレース(例:割り当ての左側のインデックス作成)</target>
        </trans-unit>
        <trans-unit id="e793ff03e25bf5dd4f85e642caecdeee487a3a73" translate="yes" xml:space="preserve">
          <source>Tracing only correctly records functions and modules which are not data dependent (e.g., do not have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned &lt;code&gt;ScriptModule&lt;/code&gt; will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,</source>
          <target state="translated">トレースは、データに依存せず（たとえば、テンソルのデータに条件がない）、追跡されていない外部依存関係がない（たとえば、入出力の実行やグローバル変数へのアクセス）関数とモジュールのみを正しく記録します。トレースは、指定された関数が指定されたテンソルで実行されたときに実行された操作のみを記録します。したがって、返された &lt;code&gt;ScriptModule&lt;/code&gt; は、どの入力でも常に同じトレースグラフを実行します。これは、入力やモジュールの状態に応じて、モジュールがさまざまな操作のセットを実行することが予想される場合に、いくつかの重要な意味を持ちます。例えば、</target>
        </trans-unit>
        <trans-unit id="1289e51c253ed91b441a90b66b33a5a7cc2af8cf" translate="yes" xml:space="preserve">
          <source>Tracing vs Scripting</source>
          <target state="translated">トレースとスクリプト</target>
        </trans-unit>
        <trans-unit id="4de8ec680853c82acd4d7ddcd35ef7c092a558c3" translate="yes" xml:space="preserve">
          <source>Tracing will not record any control-flow like if-statements or loops. When this control-flow is constant across your module, this is fine and it often inlines the control-flow decisions. But sometimes the control-flow is actually part of the model itself. For instance, a recurrent network is a loop over the (possibly dynamic) length of an input sequence.</source>
          <target state="translated">トレースは、ifステートメントやループのような制御フローを記録しません。このコントロールフローがモジュール全体で一定である場合、これは問題なく、コントロールフローの決定をインラインで行うことが多いです。しかし、コントロールフローが実際にはモデル自体の一部であることもあります。例えば、リカレント・ネットワークは入力シーケンスの(動的な)長さのループです。</target>
        </trans-unit>
        <trans-unit id="b6fe7f5e79177b05f6d251ecb9c162d45455045d" translate="yes" xml:space="preserve">
          <source>Training</source>
          <target state="translated">Training</target>
        </trans-unit>
        <trans-unit id="9799ca804afd728925d42f0fad8bc4ebafb48be1" translate="yes" xml:space="preserve">
          <source>Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.</source>
          <target state="translated">制約のない行列から,非負の対角項目を持つ下位三角行列に変換する.</target>
        </trans-unit>
        <trans-unit id="2028c39f00230fbfeeb2cd068176cf529f49e2ed" translate="yes" xml:space="preserve">
          <source>Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.</source>
          <target state="translated">制約のない空間から棒割り処理を経て1次元の単純化に変換します。</target>
        </trans-unit>
        <trans-unit id="69c694e67b533e546676316ef1f032d5873aac31" translate="yes" xml:space="preserve">
          <source>Transform from unconstrained space to the simplex via</source>
          <target state="translated">を介して非制約空間から単純空間への変換</target>
        </trans-unit>
        <trans-unit id="8e9519c4f24f7581be45723ce541214099b89154" translate="yes" xml:space="preserve">
          <source>Transform functor that applies a sequence of transforms &lt;code&gt;tseq&lt;/code&gt; component-wise to each submatrix at &lt;code&gt;dim&lt;/code&gt; in a way compatible with &lt;a href=&quot;generated/torch.stack#torch.stack&quot;&gt;&lt;code&gt;torch.stack()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.stack#torch.stack&quot;&gt; &lt;code&gt;torch.stack()&lt;/code&gt; &lt;/a&gt;と互換性のある方法で、一連の変換 &lt;code&gt;tseq&lt;/code&gt; をコンポーネントごとに &lt;code&gt;dim&lt;/code&gt; の各部分行列に適用する変換ファンクター。</target>
        </trans-unit>
        <trans-unit id="7f966e79d9dc111e8433e53a94b6d1fe7a4bd63c" translate="yes" xml:space="preserve">
          <source>Transform functor that applies a sequence of transforms &lt;code&gt;tseq&lt;/code&gt; component-wise to each submatrix at &lt;code&gt;dim&lt;/code&gt;, of length &lt;code&gt;lengths[dim]&lt;/code&gt;, in a way compatible with &lt;a href=&quot;generated/torch.cat#torch.cat&quot;&gt;&lt;code&gt;torch.cat()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cat#torch.cat&quot;&gt; &lt;code&gt;torch.cat()&lt;/code&gt; &lt;/a&gt;と互換性のある方法で、長 &lt;code&gt;lengths[dim]&lt;/code&gt; &lt;code&gt;dim&lt;/code&gt; で各部分行列にコンポーネントごとに一連の変換 &lt;code&gt;tseq&lt;/code&gt; を適用する変換ファンクター。</target>
        </trans-unit>
        <trans-unit id="bd1a4c11c8705b74a20ba32754ab9f1d32f26cf3" translate="yes" xml:space="preserve">
          <source>Transform via the mapping</source>
          <target state="translated">マッピングを介して変換</target>
        </trans-unit>
        <trans-unit id="186888187095080b876d7340f9103c9cc0aaf2ee" translate="yes" xml:space="preserve">
          <source>Transform via the pointwise affine mapping</source>
          <target state="translated">ポイントワイズアフィン写像による変換</target>
        </trans-unit>
        <trans-unit id="b14aa86e0c434d274e86855b383dde3e3e75a076" translate="yes" xml:space="preserve">
          <source>TransformedDistribution</source>
          <target state="translated">TransformedDistribution</target>
        </trans-unit>
        <trans-unit id="68c170c0011cf476eed353d994b12887940cfc96" translate="yes" xml:space="preserve">
          <source>Transformer</source>
          <target state="translated">Transformer</target>
        </trans-unit>
        <trans-unit id="6e42fdb55f8e197d60cafca548c1e82579acbea4" translate="yes" xml:space="preserve">
          <source>Transformer Layers</source>
          <target state="translated">変圧器の層</target>
        </trans-unit>
        <trans-unit id="ef303bb941bf717e2006e7273f0810b07b78b045" translate="yes" xml:space="preserve">
          <source>TransformerDecoder</source>
          <target state="translated">TransformerDecoder</target>
        </trans-unit>
        <trans-unit id="39490c0e215073daec405a4b72d513bc1f4fb82e" translate="yes" xml:space="preserve">
          <source>TransformerDecoder is a stack of N decoder layers</source>
          <target state="translated">TransformerDecoderは、N個のデコーダ層のスタックです。</target>
        </trans-unit>
        <trans-unit id="92f20fa7687824fdbb370a6b475f6eeb6f79194b" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer</source>
          <target state="translated">TransformerDecoderLayer</target>
        </trans-unit>
        <trans-unit id="98ae4e9bf414c9fe8b37e6dbc99426a1c7335c2f" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</source>
          <target state="translated">TransformerDecoderLayerは、自己ATTN、マルチヘッドATTN、およびフィードフォワードネットワークで構成されています。</target>
        </trans-unit>
        <trans-unit id="04a4a3c2fb795f7db70756477e2d518c4a892786" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="translated">TransformerDecoderLayerは、自己属性、マルチヘッド属性、およびフィードフォワードネットワークで構成されています。この標準デコーダーレイヤーは、「Attention Is AllYouNeed」という論文に基づいています。 Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N Gomez、Lukasz Kaiser、Illia Polosukhin 2017.必要なのは注意だけです。ニューラル情報処理システムの進歩、6000〜6010ページ。ユーザーは、アプリケーション中に別の方法で変更または実装できます。</target>
        </trans-unit>
        <trans-unit id="32d7343edd3b94812b03b1cdf834b1b16cc2a3fc" translate="yes" xml:space="preserve">
          <source>TransformerEncoder</source>
          <target state="translated">TransformerEncoder</target>
        </trans-unit>
        <trans-unit id="933db464a961834a0fb72e3b98e0d537c401c215" translate="yes" xml:space="preserve">
          <source>TransformerEncoder is a stack of N encoder layers</source>
          <target state="translated">TransformerEncoder は、N 個のエンコーダ層のスタックです。</target>
        </trans-unit>
        <trans-unit id="2a732f60462f98f99de0f8f387f8c71e6c32aba7" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer</source>
          <target state="translated">TransformerEncoderLayer</target>
        </trans-unit>
        <trans-unit id="7b3ae5e9124dac923ef7ce7bbf1287aa019083f2" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network.</source>
          <target state="translated">TransformerEncoderLayerは、自己ATTNとフィードフォワードネットワークで構成されています。</target>
        </trans-unit>
        <trans-unit id="0c8dccb3d09a0f9d0d9b17ea9825251d663f04d4" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="translated">TransformerEncoderLayerは、自己応答型ネットワークとフィードフォワードネットワークで構成されています。この標準エンコーダ層は、「注意が必要なすべて」という論文に基づいています。Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N Gomez、Lukasz Kaiser、Illia Polosukhin 2017.必要なのは注意だけです。ニューラル情報処理システムの進歩、6000〜6010ページ。ユーザーは、アプリケーション中に別の方法で変更または実装できます。</target>
        </trans-unit>
        <trans-unit id="06b62690eb29918ca38e1018a11381720478cf29" translate="yes" xml:space="preserve">
          <source>Traverse the modules and save all observers into dict. This is mainly used for quantization accuracy debug :param mod: the top module we want to save all observers :param prefix: the prefix for the current module :param target_dict: the dictionary used to save all the observers</source>
          <target state="translated">モジュールをたどって、すべてのオブザーバーをdictに保存します。これは主に量子化精度のデバッグに使われます。 :param mod:オブザーバを保存したいトップモジュール :param prefix:現在のモジュールのプレフィックス :param target_dict:オブザーバを保存するための辞書</target>
        </trans-unit>
        <trans-unit id="4cec3758c8a59b886f1bbcffd16de3c1f5af9091" translate="yes" xml:space="preserve">
          <source>Tries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting.</source>
          <target state="translated">このスポーンコンテキスト内の 1 つ以上のプロセスを結合しようとします。そのうちの 1 つが 0 以外の終了ステータスで終了した場合、この関数は残りのプロセスを kill し、最初のプロセスが終了した原因で例外を発生させます。</target>
        </trans-unit>
        <trans-unit id="a9cb51a530db6f55c97c4e0720722c1b583dafb7" translate="yes" xml:space="preserve">
          <source>TripletMarginLoss</source>
          <target state="translated">TripletMarginLoss</target>
        </trans-unit>
        <trans-unit id="858db382b48e5cc5988f431f28f1130330c1eaf1" translate="yes" xml:space="preserve">
          <source>TripletMarginWithDistanceLoss</source>
          <target state="translated">TripletMarginWithDistanceLoss</target>
        </trans-unit>
        <trans-unit id="88b33e4e12f75ac8bf792aebde41f1a090f3a612" translate="yes" xml:space="preserve">
          <source>True</source>
          <target state="translated">True</target>
        </trans-unit>
        <trans-unit id="56d8a88a6680e193e3cef0877ab543f4d2d36e12" translate="yes" xml:space="preserve">
          <source>True if all differences satisfy allclose condition</source>
          <target state="translated">すべての差分がallclose条件を満たしていれば真</target>
        </trans-unit>
        <trans-unit id="6692b9e66830b65bbf15ff9de3c8b1af8bfbc5eb" translate="yes" xml:space="preserve">
          <source>Tuple Construction</source>
          <target state="translated">タプル構造</target>
        </trans-unit>
        <trans-unit id="2c809de45543797accbb7ba7433a5877747ac289" translate="yes" xml:space="preserve">
          <source>Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to &lt;code&gt;pack_padded_sequence&lt;/code&gt; or &lt;code&gt;pack_sequence&lt;/code&gt;.</source>
          <target state="translated">パディングされたシーケンスを含むTensorのタプル、およびバッチ内の各シーケンスの長さのリストを含むTensor。バッチ要素は、バッチが &lt;code&gt;pack_padded_sequence&lt;/code&gt; または &lt;code&gt;pack_sequence&lt;/code&gt; に渡されたときに最初に順序付けられたとおりに並べ替えられます。</target>
        </trans-unit>
        <trans-unit id="654171647baa6be8557a5d627cf35c7075ebb257" translate="yes" xml:space="preserve">
          <source>Tutorials</source>
          <target state="translated">Tutorials</target>
        </trans-unit>
        <trans-unit id="19fd25590b1f3de52164e96d6a585a4ae3f02132" translate="yes" xml:space="preserve">
          <source>Two names &lt;em&gt;match&lt;/em&gt; if they are equal (string equality) or if at least one is &lt;code&gt;None&lt;/code&gt;. Nones are essentially a special &amp;ldquo;wildcard&amp;rdquo; name.</source>
          <target state="translated">2つの名前が等しい場合（文字列が等しい場合）、または少なくとも1つが &lt;code&gt;None&lt;/code&gt; の場合、2つの名前は&lt;em&gt;一致し&lt;/em&gt;ます。どれも本質的に特別な「ワイルドカード」名です。</target>
        </trans-unit>
        <trans-unit id="3deb7456519697ecf4eefc455516c969a3681bae" translate="yes" xml:space="preserve">
          <source>Type</source>
          <target state="translated">Type</target>
        </trans-unit>
        <trans-unit id="74a99ad458c9adf9d415d4bdb125da11688a37f7" translate="yes" xml:space="preserve">
          <source>Type Info</source>
          <target state="translated">タイプ情報</target>
        </trans-unit>
        <trans-unit id="58cf557846d7f65d34e8a92739eef2665164fad4" translate="yes" xml:space="preserve">
          <source>Type aliases</source>
          <target state="translated">タイプエイリアス</target>
        </trans-unit>
        <trans-unit id="c96d0a66889e478977efae5dbe9bb76dea1e1106" translate="yes" xml:space="preserve">
          <source>Type mismatch errors &lt;em&gt;in&lt;/em&gt; an autocast-enabled region are a bug; if this is what you observe, please file an issue.</source>
          <target state="translated">自動キャストが有効な領域&lt;em&gt;での&lt;/em&gt;型の不一致エラー&lt;em&gt;は&lt;/em&gt;バグです。これがあなたが観察するものであるならば、問題を提出してください。</target>
        </trans-unit>
        <trans-unit id="93b9e289e2842469d001eccf7ad5d79f3c302dc9" translate="yes" xml:space="preserve">
          <source>Types</source>
          <target state="translated">Types</target>
        </trans-unit>
        <trans-unit id="c0d285234eb927c53c2fc1dda528e04061e0d90d" translate="yes" xml:space="preserve">
          <source>Types produced by &lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt;&lt;code&gt;collections.namedtuple&lt;/code&gt;&lt;/a&gt; can be used in TorchScript.</source>
          <target state="translated">&lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt; &lt;code&gt;collections.namedtuple&lt;/code&gt; &lt;/a&gt;によって生成されたタイプは、TorchScriptで使用できます。</target>
        </trans-unit>
        <trans-unit id="e041e0fe9045d1072a9765e6e123108f2940daba" translate="yes" xml:space="preserve">
          <source>Typically, in SWA the learning rate is set to a high constant value. &lt;code&gt;SWALR&lt;/code&gt; is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group:</source>
          <target state="translated">通常、SWAでは、学習率は高い定数値に設定されます。 &lt;code&gt;SWALR&lt;/code&gt; は、学習率を固定値にアニーリングし、それを一定に保つ学習率スケジューラーです。たとえば、次のコードは、各パラメーターグループ内の5エポックで学習率を初期値から0.05まで線形にアニーリングするスケジューラーを作成します。</target>
        </trans-unit>
        <trans-unit id="b2c7c0caa10a0cca5ea7d69e54018ae0c0389dd6" translate="yes" xml:space="preserve">
          <source>U</source>
          <target state="translated">U</target>
        </trans-unit>
        <trans-unit id="4e5f249d2049283bfab86474c53e19434fabe07e" translate="yes" xml:space="preserve">
          <source>URL specifying how to initialize the process group. Default is &lt;code&gt;env://&lt;/code&gt;</source>
          <target state="translated">プロセスグループを初期化する方法を指定するURL。デフォルトは &lt;code&gt;env://&lt;/code&gt; です</target>
        </trans-unit>
        <trans-unit id="9016840b6ab501762ae42c3bc2d77284127ecd9d" translate="yes" xml:space="preserve">
          <source>Unflatten</source>
          <target state="translated">Unflatten</target>
        </trans-unit>
        <trans-unit id="22753787538a4df3368517dc3a28771d6a6bd1c2" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape.</source>
          <target state="translated">テンソルディムを任意の形状に展開してアンフラットします。</target>
        </trans-unit>
        <trans-unit id="ffd6987181a5e9a56284c36d90107b63ec0ad279" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape. For use with &lt;code&gt;Sequential&lt;/code&gt;.</source>
          <target state="translated">テンソルの薄暗い部分を平らにして、目的の形状に拡張します。 &lt;code&gt;Sequential&lt;/code&gt; で使用します。</target>
        </trans-unit>
        <trans-unit id="02d85ea4efaa0d1ca099f19843e9be07e28b954d" translate="yes" xml:space="preserve">
          <source>Unfold</source>
          <target state="translated">Unfold</target>
        </trans-unit>
        <trans-unit id="27c8f884a26740cbb923c350e9fa436fb8314b34" translate="yes" xml:space="preserve">
          <source>Unfortunately, the concrete &lt;code&gt;subset&lt;/code&gt; that was used is lost. For more information see &lt;a href=&quot;https://github.com/pytorch/vision/issues/1439&quot;&gt;this discussion&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/vision/pull/1965&quot;&gt;these experiments&lt;/a&gt;.</source>
          <target state="translated">残念ながら、使用された具体的な &lt;code&gt;subset&lt;/code&gt; は失われます。詳細については、&lt;a href=&quot;https://github.com/pytorch/vision/issues/1439&quot;&gt;このディスカッション&lt;/a&gt;または&lt;a href=&quot;https://github.com/pytorch/vision/pull/1965&quot;&gt;これらの実験を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="7a4ec9e5c0816d2c18ff0c058bf903c94688d657" translate="yes" xml:space="preserve">
          <source>Unfortunately, there&amp;rsquo;s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or &lt;a href=&quot;#torch.autograd.profiler.load_nvprof&quot;&gt;&lt;code&gt;torch.autograd.profiler.load_nvprof()&lt;/code&gt;&lt;/a&gt; can load the results for inspection e.g. in Python REPL.</source>
          <target state="translated">残念ながら、nvprofに収集したデータをディスクにフラッシュさせる方法はありません。そのため、CUDAプロファイリングでは、このコンテキストマネージャーを使用してnvprofトレースに注釈を付け、プロセスが終了するのを待ってから検査する必要があります。次に、NVIDIA Visual Profiler（nvvp）を使用してタイムラインを視覚化するか、&lt;a href=&quot;#torch.autograd.profiler.load_nvprof&quot;&gt; &lt;code&gt;torch.autograd.profiler.load_nvprof()&lt;/code&gt; &lt;/a&gt;を使用して結果を読み込み、PythonREPLなどで検査できます。</target>
        </trans-unit>
        <trans-unit id="e96b0da8b763233f21fa51e37813da0eba866185" translate="yes" xml:space="preserve">
          <source>Uniform</source>
          <target state="translated">Uniform</target>
        </trans-unit>
        <trans-unit id="2786d7c847a0723edc813cd593fabc9e06192fcd" translate="yes" xml:space="preserve">
          <source>Unless otherwise specified, this function should not modify the &lt;code&gt;.grad&lt;/code&gt; field of the parameters.</source>
          <target state="translated">特に指定がない限り、この関数はパラメーターの &lt;code&gt;.grad&lt;/code&gt; フィールドを変更しないでください。</target>
        </trans-unit>
        <trans-unit id="b69bc99e876e93b97ea0728c1acca97b14bbffef" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand()&lt;/code&gt;&lt;/a&gt;, this function copies the tensor&amp;rsquo;s data.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.expand&quot;&gt; &lt;code&gt;expand()&lt;/code&gt; &lt;/a&gt;とは異なり、この関数はテンソルのデータをコピーします。</target>
        </trans-unit>
        <trans-unit id="ef5392b685d4622304e5dfc150347ec19f0ee0af" translate="yes" xml:space="preserve">
          <source>Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the &lt;code&gt;affine&lt;/code&gt; option, Layer Normalization applies per-element scale and bias with &lt;code&gt;elementwise_affine&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;affine&lt;/code&gt; オプションを使用してチャネル/平面全体にスカラースケールとバイアスを適用するバッチ正規化とインスタンス正規化とは異なり、レイヤー正規化は &lt;code&gt;elementwise_affine&lt;/code&gt; を使用して要素ごとのスケールとバイアスを適用します。</target>
        </trans-unit>
        <trans-unit id="8607cd4c09ad49a23e91f63072337c8e4b9af158" translate="yes" xml:space="preserve">
          <source>Unlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. The refcounting is implemented under the hood but requires users to follow the next best practices.</source>
          <target state="translated">CPUテンソルとは異なり、送信プロセスは、受信プロセスがテンソルのコピーを保持している限り、元のテンソルを保持する必要があります。リファイカウントはフードの下に実装されていますが、ユーザーは次のベストプラクティスに従う必要があります。</target>
        </trans-unit>
        <trans-unit id="2498c971c849991894b17969e87a3329f48f1d2d" translate="yes" xml:space="preserve">
          <source>Unlike Python, each variable in TorchScript function must have a single static type. This makes it easier to optimize TorchScript functions.</source>
          <target state="translated">Pythonとは異なり、TorchScript関数の各変数は1つの静的型を持つ必要があります。これにより、TorchScript関数の最適化が容易になります。</target>
        </trans-unit>
        <trans-unit id="3d7bdd35d7e8961eff68645ed326f054b5482c52" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented</source>
          <target state="translated">実装されそうにない</target>
        </trans-unit>
        <trans-unit id="a5b6f222f736f9cef541160a005848fe4ef77888" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented (however &lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;&gt;&lt;code&gt;typing.Optional&lt;/code&gt;&lt;/a&gt; is supported)</source>
          <target state="translated">実装される可能性は&lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;&gt; &lt;code&gt;typing.Optional&lt;/code&gt; &lt;/a&gt;です（ただし、入力します。オプションがサポートされています）</target>
        </trans-unit>
        <trans-unit id="01773bdc2e10d2279820d7115b2522610a09e4f3" translate="yes" xml:space="preserve">
          <source>Unpacks the data and pivots from a LU factorization of a tensor.</source>
          <target state="translated">データをアンパックし、テンソルのLU因数分解からピボットします。</target>
        </trans-unit>
        <trans-unit id="a7afe79ac1105fe133bbed5120636913c111a929" translate="yes" xml:space="preserve">
          <source>Unsupported Typing Constructs</source>
          <target state="translated">サポートされていない型付けコンストラクト</target>
        </trans-unit>
        <trans-unit id="b2d1fe571301d72d9191816bf953a47985e426ec" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;ModuleDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt; &lt;code&gt;ModuleDict&lt;/code&gt; &lt;/a&gt;を、マッピングまたは反復可能な既存のキーを上書きするキーと値のペアで更新します。</target>
        </trans-unit>
        <trans-unit id="4eca5c6565cbf1cfd8391f002e24b24767bdf27d" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt;&lt;code&gt;ParameterDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="translated">マッピングまたは反復可能な既存のキーを上書きするキーと値のペアで&lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt; &lt;code&gt;ParameterDict&lt;/code&gt; &lt;/a&gt;を更新します。</target>
        </trans-unit>
        <trans-unit id="b985b6244019737115cb9e96cf752b208328d084" translate="yes" xml:space="preserve">
          <source>Updates the scale factor.</source>
          <target state="translated">スケールファクタを更新します。</target>
        </trans-unit>
        <trans-unit id="e9f4cec65260954e7a90830aa6f73b90e5c8817f" translate="yes" xml:space="preserve">
          <source>Upsample</source>
          <target state="translated">Upsample</target>
        </trans-unit>
        <trans-unit id="0b472c3013e4cd26ac1f5f84d9b57c5b6b4ca455" translate="yes" xml:space="preserve">
          <source>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</source>
          <target state="translated">与えられたマルチチャンネルの1D(時間的)、2D(空間的)、3D(体積的)データをアップサンプリングします。</target>
        </trans-unit>
        <trans-unit id="55dfe2d567c845d264928977c08cd8d377265025" translate="yes" xml:space="preserve">
          <source>Upsamples the input to either the given &lt;code&gt;size&lt;/code&gt; or the given &lt;code&gt;scale_factor&lt;/code&gt;</source>
          <target state="translated">指定 &lt;code&gt;size&lt;/code&gt; れたサイズまたは指定された &lt;code&gt;scale_factor&lt;/code&gt; のいずれかに入力をアップサンプリングします</target>
        </trans-unit>
        <trans-unit id="c9b84912595a004dd92417c63046087231bc7d36" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using bilinear upsampling.</source>
          <target state="translated">バイリニアアップサンプリングを使用して、入力をアップサンプリングします。</target>
        </trans-unit>
        <trans-unit id="a31dc0017d79484c9069d06f3ac78a206e41dbfa" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using nearest neighbours&amp;rsquo; pixel values.</source>
          <target state="translated">最も近い隣人のピクセル値を使用して、入力をアップサンプリングします。</target>
        </trans-unit>
        <trans-unit id="fef8a1cac9f9d013e57f5acb6da8a9f36cede8d1" translate="yes" xml:space="preserve">
          <source>UpsamplingBilinear2d</source>
          <target state="translated">UpsamplingBilinear2d</target>
        </trans-unit>
        <trans-unit id="2a7960d23688a71e6707ef6d16f626ed000e779c" translate="yes" xml:space="preserve">
          <source>UpsamplingNearest2d</source>
          <target state="translated">UpsamplingNearest2d</target>
        </trans-unit>
        <trans-unit id="96d06615a1d6c33a776a3eb6549cbfd22dbce234" translate="yes" xml:space="preserve">
          <source>Usage of this function is discouraged in favor of &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt;. In most cases it&amp;rsquo;s better to use &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environmental variable.</source>
          <target state="translated">この関数の使用は、&lt;a href=&quot;#torch.cuda.device&quot;&gt; &lt;code&gt;device&lt;/code&gt; &lt;/a&gt;を優先して推奨されません。ほとんどの場合、 &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; 環境変数を使用することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="680c6c9f314b68f86ff46d8fdf9a289c7fb0c343" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_as&quot;&gt;&lt;code&gt;align_as()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to align tensor dimensions by name to a specified ordering. This is useful for performing &amp;ldquo;broadcasting by names&amp;rdquo;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.align_as&quot;&gt; &lt;code&gt;align_as()&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt;を使用して、テンソル次元を名前で指定された順序に揃えます。これは、「名前によるブロードキャスト」を実行する場合に役立ちます。</target>
        </trans-unit>
        <trans-unit id="66804d0e4e83078e14bfdbaa37e395675103792c" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to permute large amounts of dimensions without mentioning all of them as in required by &lt;a href=&quot;tensors#torch.Tensor.permute&quot;&gt;&lt;code&gt;permute()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt;を使用して、permute（）で必要とされるように、すべてのディメンションに言及せずに大量のディメンションを&lt;a href=&quot;tensors#torch.Tensor.permute&quot;&gt; &lt;code&gt;permute()&lt;/code&gt; &lt;/a&gt;ます。</target>
        </trans-unit>
        <trans-unit id="ede9bff88acacf3e23cc669406c39e0d8aba75b8" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.item&quot;&gt;&lt;code&gt;torch.Tensor.item()&lt;/code&gt;&lt;/a&gt; to get a Python number from a tensor containing a single value:</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.item&quot;&gt; &lt;code&gt;torch.Tensor.item()&lt;/code&gt; &lt;/a&gt;を使用して、単一の値を含むテンソルからPython番号を取得します。</target>
        </trans-unit>
        <trans-unit id="e75b6839717cab84ecfa02664778f32a2c198780" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; to access the dimension names of a tensor and &lt;a href=&quot;#torch.Tensor.rename&quot;&gt;&lt;code&gt;rename()&lt;/code&gt;&lt;/a&gt; to rename named dimensions.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt;を使用してテンソルの次元名にアクセスし、&lt;a href=&quot;#torch.Tensor.rename&quot;&gt; &lt;code&gt;rename()&lt;/code&gt; &lt;/a&gt;を使用して名前付き次元の名前を変更します。</target>
        </trans-unit>
        <trans-unit id="b5d22900f5f0642196ebec69d6c5572874ee7c1b" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;tensors#torch.Tensor.flatten&quot;&gt;&lt;code&gt;flatten()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.Tensor.unflatten&quot;&gt;&lt;code&gt;unflatten()&lt;/code&gt;&lt;/a&gt; to flatten and unflatten dimensions, respectively. These methods are more verbose than &lt;a href=&quot;tensors#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensors#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, but have more semantic meaning to someone reading the code.</source>
          <target state="translated">&lt;a href=&quot;tensors#torch.Tensor.flatten&quot;&gt; &lt;code&gt;flatten()&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#torch.Tensor.unflatten&quot;&gt; &lt;code&gt;unflatten()&lt;/code&gt; &lt;/a&gt;を使用して、それぞれ寸法を平坦化と非平坦化します。これらのメソッドは、&lt;a href=&quot;tensors#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt;や&lt;a href=&quot;tensors#torch.Tensor.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt;よりも冗長ですが、コードを読んでいる人にとっては意味的な意味があります。</target>
        </trans-unit>
        <trans-unit id="b67226dee210381a9009b14565e54ae65d2034d5" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;torch.cuda.current_stream()&lt;/code&gt; if no stream is specified.</source>
          <target state="translated">ストリームが指定されていない場合は、 &lt;code&gt;torch.cuda.current_stream()&lt;/code&gt; を使用します。</target>
        </trans-unit>
        <trans-unit id="641cc18d21edd9671570b1c6c598d26a3e5396f4" translate="yes" xml:space="preserve">
          <source>Use Gloo, unless you have specific reasons to use MPI.</source>
          <target state="translated">MPIを使用する特別な理由がない限り、Glooを使用してください。</target>
        </trans-unit>
        <trans-unit id="4b4d173cba748d683c0210fec722f5787173d83a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)</source>
          <target state="translated">NCCLを使用してください。特にマルチプロセス・シングルノードまたはマルチノードの分散トレーニングでは、現在最高の分散GPUトレーニング性能を提供しています。NCCLで問題が発生した場合は、代替オプションとしてGlooを使用してください。(Glooは現在のところGPUではNCCLよりも動作が遅いことに注意してください)。</target>
        </trans-unit>
        <trans-unit id="9b1bf082bc9bf3ac7914e6187c00d474a91fb73a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it&amp;rsquo;s the only backend that currently supports InfiniBand and GPUDirect.</source>
          <target state="translated">NCCLは、現在InfiniBandとGPUDirectをサポートしている唯一のバックエンドであるため、NCCLを使用してください。</target>
        </trans-unit>
        <trans-unit id="5e1700010ee503b77904278f8d396bfe42da63c7" translate="yes" xml:space="preserve">
          <source>Use external data format</source>
          <target state="translated">外部データ形式を使用する</target>
        </trans-unit>
        <trans-unit id="9d0232b01e54b3a2e55b622100adbb68b4edb340" translate="yes" xml:space="preserve">
          <source>Use of Python Values</source>
          <target state="translated">Pythonの値の使用</target>
        </trans-unit>
        <trans-unit id="73922dca1fc1eb81dc09b31de16429c2bad893e0" translate="yes" xml:space="preserve">
          <source>Use the Gloo backend for distributed &lt;strong&gt;CPU&lt;/strong&gt; training.</source>
          <target state="translated">分散&lt;strong&gt;CPU&lt;/strong&gt;トレーニングにはGlooバックエンドを使用してください。</target>
        </trans-unit>
        <trans-unit id="fc3648b93e2309e15d382b23a53006e5ccb31b40" translate="yes" xml:space="preserve">
          <source>Use the NCCL backend for distributed &lt;strong&gt;GPU&lt;/strong&gt; training</source>
          <target state="translated">分散&lt;strong&gt;GPU&lt;/strong&gt;トレーニングにNCCLバックエンドを使用する</target>
        </trans-unit>
        <trans-unit id="9ca66ca0ae6872cefb7cdaba9a1b1d7c4fb946f1" translate="yes" xml:space="preserve">
          <source>Used to infer dtype for python complex numbers. The default complex dtype is set to &lt;code&gt;torch.complex128&lt;/code&gt; if default floating point dtype is &lt;code&gt;torch.float64&lt;/code&gt;, otherwise it&amp;rsquo;s set to &lt;code&gt;torch.complex64&lt;/code&gt;</source>
          <target state="translated">Pythonの複素数のdtypeを推測するために使用されます。デフォルト複雑DTYPEは次のように設定されて &lt;code&gt;torch.complex128&lt;/code&gt; デフォルトの浮動小数点DTYPEがある場合 &lt;code&gt;torch.float64&lt;/code&gt; 、とそうでない場合のセット &lt;code&gt;torch.complex64&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fac6c061fc6d273aebc0e9ab0c99f04ae81e1097" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Argument</source>
          <target state="translated">オーナーを引数に持つユーザ共有RRef</target>
        </trans-unit>
        <trans-unit id="a20f912ad88b795f826c479ed261f676c30da2f7" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Return Value</source>
          <target state="translated">オーナーとのユーザー共有RRefをリターン値として</target>
        </trans-unit>
        <trans-unit id="a7e43a6c0dede8cc47d9e15900c78d9da28530c0" translate="yes" xml:space="preserve">
          <source>User Share RRef with User</source>
          <target state="translated">ユーザーとRRefを共有する</target>
        </trans-unit>
        <trans-unit id="78d0e495aa5431e6e0a8139c0666804c17a54940" translate="yes" xml:space="preserve">
          <source>User extensions can register their own location tags and tagging and deserialization methods using &lt;code&gt;torch.serialization.register_package()&lt;/code&gt;.</source>
          <target state="translated">ユーザー拡張機能は、 &lt;code&gt;torch.serialization.register_package()&lt;/code&gt; を使用して、独自のロケーションタグとタグ付けおよび逆シリアル化メソッドを登録できます。</target>
        </trans-unit>
        <trans-unit id="a2628232b96e27d1cfe74641b5e5eef1e4a68d67" translate="yes" xml:space="preserve">
          <source>Users can force a reload by calling &lt;code&gt;hub.load(..., force_reload=True)&lt;/code&gt;. This will delete the existing github folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.</source>
          <target state="translated">ユーザーは、 &lt;code&gt;hub.load(..., force_reload=True)&lt;/code&gt; 呼び出すことにより、強制的にリロードできます。これにより、既存のgithubフォルダーとダウンロードされた重みが削除され、新しいダウンロードが再初期化されます。これは、更新が同じブランチに公開されている場合に役立ちます。ユーザーは最新のリリースについていくことができます。</target>
        </trans-unit>
        <trans-unit id="a0dfaf311b141161fd5f1c71366b90c3c330831f" translate="yes" xml:space="preserve">
          <source>Users may use customized &lt;code&gt;collate_fn&lt;/code&gt; to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types.</source>
          <target state="translated">ユーザーは、カスタマイズされた &lt;code&gt;collate_fn&lt;/code&gt; を使用して、カスタムバッチ処理を実現できます。たとえば、最初のディメンション以外のディメンションに沿った照合、さまざまな長さのシーケンスのパディング、カスタムデータ型のサポートの追加などです。</target>
        </trans-unit>
        <trans-unit id="36a0e861f968553beec79e7c334a566865bdb40a" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;torch.cuda.current_stream()&lt;/code&gt; if no stream is specified. The stream&amp;rsquo;s device must match the event&amp;rsquo;s device.</source>
          <target state="translated">ストリームが指定されていない場合は、 &lt;code&gt;torch.cuda.current_stream()&lt;/code&gt; を使用します。ストリームのデバイスは、イベントのデバイスと一致する必要があります。</target>
        </trans-unit>
        <trans-unit id="daa3e4bf690e79b2d584e60908e08290c5eeab22" translate="yes" xml:space="preserve">
          <source>Using &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt;&lt;code&gt;scale_tril&lt;/code&gt;&lt;/a&gt; will be more efficient: all computations internally are based on &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt;&lt;code&gt;scale_tril&lt;/code&gt;&lt;/a&gt;. If &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix&quot;&gt;&lt;code&gt;covariance_matrix&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix&quot;&gt;&lt;code&gt;precision_matrix&lt;/code&gt;&lt;/a&gt; is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt; &lt;code&gt;scale_tril&lt;/code&gt; &lt;/a&gt;を使用すると、より効率的に&lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt; &lt;code&gt;scale_tril&lt;/code&gt; &lt;/a&gt;ます。内部のすべての計算は、scale_trilに基づいています。代わりに&lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix&quot;&gt; &lt;code&gt;covariance_matrix&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix&quot;&gt; &lt;code&gt;precision_matrix&lt;/code&gt; &lt;/a&gt;が渡された場合、コレスキー分解を使用して対応する下三角行列を計算するためにのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="df3d36e2101db7508ee9c65bba6c79f4162eb560" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;DistributedDataParallel&lt;/code&gt; in conjunction with the &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt; is experimental and subject to change.</source>
          <target state="translated">&lt;code&gt;DistributedDataParallel&lt;/code&gt; をDistributedRPC &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Framework&lt;/a&gt;と組み合わせて使用することは実験的であり、変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="6f8c8c4de2dead884cf1cd1b1efe492df15bdbf3" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;torch.jit.trace&lt;/code&gt; and &lt;code&gt;torch.jit.trace_module&lt;/code&gt;, you can turn an existing module or Python function into a TorchScript &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.</source>
          <target state="translated">&lt;code&gt;torch.jit.trace&lt;/code&gt; および &lt;code&gt;torch.jit.trace_module&lt;/code&gt; を使用すると、既存のモジュールまたはPython関数を&lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; に変換でき&lt;/a&gt;ます。入力例を提供する必要があります。関数を実行して、すべてのテンソルで実行された操作を記録します。</target>
        </trans-unit>
        <trans-unit id="016141762b3eda50d55b6434765c656b21e2c21e" translate="yes" xml:space="preserve">
          <source>Using GPU tensors as arguments or return values of &lt;code&gt;func&lt;/code&gt; is not supported since we don&amp;rsquo;t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of &lt;code&gt;func&lt;/code&gt;.</source>
          <target state="translated">GPUテンソルをネットワーク経由で送信することはサポートされていないため、引数または &lt;code&gt;func&lt;/code&gt; の戻り値としてGPUテンソルを使用することはサポートされていません。GPUテンソルを引数または &lt;code&gt;func&lt;/code&gt; の戻り値として使用する前に、明示的にCPUにコピーする必要があります。</target>
        </trans-unit>
        <trans-unit id="40f8083d709ef1dfeba3266d73cd40bc2120c4c0" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute matrix norms:</source>
          <target state="translated">使用 &lt;code&gt;dim&lt;/code&gt; 計算行列ノルムに引数を：</target>
        </trans-unit>
        <trans-unit id="34eeb9d75aa34a25cf342ae4a398eead839c1594" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute vector norms:</source>
          <target state="translated">使用 &lt;code&gt;dim&lt;/code&gt; 計算ベクトル規範に引数を：</target>
        </trans-unit>
        <trans-unit id="46e68d726011984151416846f828c085d1a8cabd" translate="yes" xml:space="preserve">
          <source>Using this method with &lt;code&gt;create_graph=True&lt;/code&gt; will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using &lt;code&gt;autograd.grad&lt;/code&gt; when creating the graph to avoid this. If you have to use this function, make sure to reset the &lt;code&gt;.grad&lt;/code&gt; fields of your parameters to &lt;code&gt;None&lt;/code&gt; after use to break the cycle and avoid the leak.</source>
          <target state="translated">&lt;code&gt;create_graph=True&lt;/code&gt; でこのメソッドを使用すると、パラメーターとその勾配の間に参照サイクルが作成され、メモリリークが発生する可能性があります。これを回避するために、グラフを作成するときに &lt;code&gt;autograd.grad&lt;/code&gt; を使用することをお勧めします。この関数を使用する必要がある場合は、使用後にパラメータの &lt;code&gt;.grad&lt;/code&gt; フィールドを &lt;code&gt;None&lt;/code&gt; にリセットして、サイクルを中断し、リークを回避してください。</target>
        </trans-unit>
        <trans-unit id="892e242741e1c766ef8440ce7e813adfc429718b" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv2d&lt;/code&gt; modules.</source>
          <target state="translated">通常、入力は &lt;code&gt;nn.Conv2d&lt;/code&gt; モジュールから取得されます。</target>
        </trans-unit>
        <trans-unit id="62c50ebd5d624de117b146ade6f9d5575d770fc3" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv3d&lt;/code&gt; modules.</source>
          <target state="translated">通常、入力は &lt;code&gt;nn.Conv3d&lt;/code&gt; モジュールから取得されます。</target>
        </trans-unit>
        <trans-unit id="18fdc5ee8b1f8fba8dabaa933373c0483ab7fad7" translate="yes" xml:space="preserve">
          <source>Utilities</source>
          <target state="translated">Utilities</target>
        </trans-unit>
        <trans-unit id="f16cdccb3faaa5e67faa6fdb65caa5cf29d51cbb" translate="yes" xml:space="preserve">
          <source>Utility functions</source>
          <target state="translated">ユーティリティ機能</target>
        </trans-unit>
        <trans-unit id="ad96fcc3041d4053b9449ab4a648c6341e2217d7" translate="yes" xml:space="preserve">
          <source>Utility functions in other modules</source>
          <target state="translated">他のモジュールのユーティリティ機能</target>
        </trans-unit>
        <trans-unit id="1c41457151932f1c1abddf366a843f91dd5ea098" translate="yes" xml:space="preserve">
          <source>Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.</source>
          <target state="translated">単位の剪定を行わずに、1のマスクで剪定パラメタライズを生成するユーティリティ剪定メソッド。</target>
        </trans-unit>
        <trans-unit id="c9ee5681d3c59f7541c27a38b67edf46259e187b" translate="yes" xml:space="preserve">
          <source>V</source>
          <target state="translated">V</target>
        </trans-unit>
        <trans-unit id="1eafbd543ea2d8baa3df59414ec62e236836b78c" translate="yes" xml:space="preserve">
          <source>V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses: &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&lt;/a&gt;</source>
          <target state="translated">V. Balntas、et al。：三重項損失を伴う浅い畳み込み特徴記述子の学習：&lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;http&lt;/a&gt;：//www.bmva.org/bmvc/2016/papers/paper119/index.html</target>
        </trans-unit>
        <trans-unit id="2badedb167ecf8f97a1ffcdd2817401074fa728e" translate="yes" xml:space="preserve">
          <source>VGG</source>
          <target state="translated">VGG</target>
        </trans-unit>
        <trans-unit id="76dc2de0cc7c95272fc67f826f2dc5fa09c8ded2" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) from &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」の&lt;/a&gt;VGG11層モデル（構成「A」）</target>
        </trans-unit>
        <trans-unit id="efac66ad38738b85f5ef46a914f1b2bede87a54e" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">バッチ正規化を使用したVGG11層モデル（構成「A」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8ab10add11dcdf5fb182aad6d9f35aa59134c466" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 13層モデル（構成「B」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c68abe1f69acd7242c26b04b450c20879de247dc" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">バッチ正規化を使用したVGG13層モデル（構成「B」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7df2c09a84b7ee83b6d8493aa3dccf2e12cc714a" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 16層モデル（構成「D」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4979b51da2376b295925fa358ed83201a60846eb" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">バッチ正規化を使用したVGG16層モデル（構成「D」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="734385d469c0b7b16d39310aab3a223999973e9e" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;ldquo;E&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 19層モデル（構成「E」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b468de2d615302723a63a19cf8ef07ba2f87d240" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;lsquo;E&amp;rsquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">バッチ正規化を使用したVGG19層モデル（構成「E」）&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;「大規模画像認識のための非常に深い畳み込みネットワーク」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22dca5ed30facb9e021b2a416131b71d9addd5df" translate="yes" xml:space="preserve">
          <source>VGG-11</source>
          <target state="translated">VGG-11</target>
        </trans-unit>
        <trans-unit id="59c5eb8db8735266b63e11d68abfe0513f53e937" translate="yes" xml:space="preserve">
          <source>VGG-11 with batch normalization</source>
          <target state="translated">VGG-11 バッチ正規化機能付き</target>
        </trans-unit>
        <trans-unit id="275a0abbe0e323332b36f91fd1e0bbabaa98823b" translate="yes" xml:space="preserve">
          <source>VGG-13</source>
          <target state="translated">VGG-13</target>
        </trans-unit>
        <trans-unit id="00e8568346d410021e9a56f3b1b1f1983ac75021" translate="yes" xml:space="preserve">
          <source>VGG-13 with batch normalization</source>
          <target state="translated">VGG-13とバッチ正規化</target>
        </trans-unit>
        <trans-unit id="3fa5851c47708aca43af1f031237ec76652d5fb8" translate="yes" xml:space="preserve">
          <source>VGG-16</source>
          <target state="translated">VGG-16</target>
        </trans-unit>
        <trans-unit id="6794221fe94aa99e89cd4a3fc469cd560c5b1798" translate="yes" xml:space="preserve">
          <source>VGG-16 with batch normalization</source>
          <target state="translated">VGG-16 バッチ正規化機能付き</target>
        </trans-unit>
        <trans-unit id="e33f8119a3d18ad4bc21aa5913ffff22adbc62fa" translate="yes" xml:space="preserve">
          <source>VGG-19</source>
          <target state="translated">VGG-19</target>
        </trans-unit>
        <trans-unit id="6a06101d542844efc9851734dd33c0a3fcfb9071" translate="yes" xml:space="preserve">
          <source>VGG-19 with batch normalization</source>
          <target state="translated">VGG-19 バッチ正規化機能付き</target>
        </trans-unit>
        <trans-unit id="08ab4ecc000363002865da057bb07b708354e689" translate="yes" xml:space="preserve">
          <source>Valid operation names:</source>
          <target state="translated">有効な操作名。</target>
        </trans-unit>
        <trans-unit id="8ab2f6ea14647497320511c9699ad1fa98390d6e" translate="yes" xml:space="preserve">
          <source>Value associated with &lt;code&gt;key&lt;/code&gt; if &lt;code&gt;key&lt;/code&gt; is in the store.</source>
          <target state="translated">値に関連付けられた &lt;code&gt;key&lt;/code&gt; 場合 &lt;code&gt;key&lt;/code&gt; 店です。</target>
        </trans-unit>
        <trans-unit id="8f65e3050b4aa23d2b9ffeafc8ef420283f1bc31" translate="yes" xml:space="preserve">
          <source>Values looked up as attributes of a module are assumed to be constant:</source>
          <target state="translated">モジュールの属性としてルックアップされた値は、一定であると仮定します。</target>
        </trans-unit>
        <trans-unit id="766a55330fbdeceea3990ed650f19ff9d7ebc2b2" translate="yes" xml:space="preserve">
          <source>Vandermonde matrix. If increasing is False, the first column is</source>
          <target state="translated">バンデルモンデ行列。増加が False の場合、最初の列は</target>
        </trans-unit>
        <trans-unit id="2363c9d67b7ae2a0e8fa492fb3e5e19109b5b856" translate="yes" xml:space="preserve">
          <source>Variable (deprecated)</source>
          <target state="translated">変数 (非推奨)</target>
        </trans-unit>
        <trans-unit id="8993fc586517fabcc3d0fce5c92e800dc4e3de15" translate="yes" xml:space="preserve">
          <source>Variable Resolution</source>
          <target state="translated">可変分解能</target>
        </trans-unit>
        <trans-unit id="ac018db1f7b00972061adff843d37497d8ee153c" translate="yes" xml:space="preserve">
          <source>Variables</source>
          <target state="translated">Variables</target>
        </trans-unit>
        <trans-unit id="b1c39119660f33e5be726c1652639e668fcdfcd8" translate="yes" xml:space="preserve">
          <source>Verifies that the given compiler is ABI-compatible with PyTorch.</source>
          <target state="translated">与えられたコンパイラが PyTorch と ABI 互換であることを確認します。</target>
        </trans-unit>
        <trans-unit id="991eeb7d6acd3a1b90daf8e907ad4a1126fbf67e" translate="yes" xml:space="preserve">
          <source>Via a string and device ordinal:</source>
          <target state="translated">文字列とデバイス序列を介して。</target>
        </trans-unit>
        <trans-unit id="78f371ae51565c626e223c43a305351baadcdfcb" translate="yes" xml:space="preserve">
          <source>Via a string:</source>
          <target state="translated">文字列を介して。</target>
        </trans-unit>
        <trans-unit id="3eb4e2b65c3b6adcd10d477d9e1ded0579505479" translate="yes" xml:space="preserve">
          <source>Video classification</source>
          <target state="translated">動画の分類</target>
        </trans-unit>
        <trans-unit id="61ad6c7f7397fbdc6a6513e489917c12a6953f11" translate="yes" xml:space="preserve">
          <source>View this tensor as the same size as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.view_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.view(other.size())&lt;/code&gt;.</source>
          <target state="translated">このテンソルを &lt;code&gt;other&lt;/code&gt; テンソルと同じサイズとして表示します。 &lt;code&gt;self.view_as(other)&lt;/code&gt; は &lt;code&gt;self.view(other.size())&lt;/code&gt; と同等です。</target>
        </trans-unit>
        <trans-unit id="68b460da28c94fa06be12e6f2242c79c2eca8bd2" translate="yes" xml:space="preserve">
          <source>Vision Layers</source>
          <target state="translated">ビジョンレイヤー</target>
        </trans-unit>
        <trans-unit id="8cd7ed1352bc6bdd613ec698de479a989aa9357b" translate="yes" xml:space="preserve">
          <source>Vision functions</source>
          <target state="translated">ビジョン機能</target>
        </trans-unit>
        <trans-unit id="2b4b5f5947eec0464172de099ad91010e86ae87b" translate="yes" xml:space="preserve">
          <source>VonMises</source>
          <target state="translated">VonMises</target>
        </trans-unit>
        <trans-unit id="e2415cb7f63df0c9de23362326ad3c37a9adfc96" translate="yes" xml:space="preserve">
          <source>W</source>
          <target state="translated">W</target>
        </trans-unit>
        <trans-unit id="46965f2770b3f862f5982cb5b877918de164026c" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}</source>
          <target state="translated">W_{out}=(W_{in}-1)♪ ♪times \times</target>
        </trans-unit>
        <trans-unit id="5b7347a76af1465d2b26baeed335756321992f36" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06cfb3f342da0b25161bf11f15bf2d81bfb676a7" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="144094cbaa937780b12d5cb8356e1fc4a6c692aa" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23da97705d0e8d5a571139859c1eefc7a25b59f7" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}</source>
          <target state="translated">W_{out}=W_{in}+\text{padding_left}+\text{padding_right}.</target>
        </trans-unit>
        <trans-unit id="bf2c9a7a90367be7032bd4f58614c0c9ad3779f8" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} \times \text{upscale\_factor}</source>
          <target state="translated">W_{out}=W_{in}\times ﾃｷｽﾄ{upscale_factor}.</target>
        </trans-unit>
        <trans-unit id="c5a97d192d1067a74706f036b87ed2b96b8d5895" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{times ≫ ≪Text{scale_factor}≫</target>
        </trans-unit>
        <trans-unit id="dd2204cd0212ab9c7330f7c2e6a0b40cae1ae693" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]} \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+2*\times (Text{kernel_size[1]}-1)-1}{Text{stride[1]}}+1\rightrfloor</target>
        </trans-unit>
        <trans-unit id="876c8e0e601e7178678d337712548c5cc7269f61" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+2 \times ≪W_{in}≫+2 ≪W_{in}≫ ≪W_{in}≫ ≪W_{out}≫ ≪W_{out}≫ ≪W_{out}≫+2 ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫</target>
        </trans-unit>
        <trans-unit id="6e0657f1536a7a37c826480069677e9010835607" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+2 ≪W_{in}≫+2 ≪Times ≫ ≪Text{padding}[1]-≪Text{kernel_size}[1]}≪Text{stride}[1]}+1 Right\rfloor</target>
        </trans-unit>
        <trans-unit id="a11c0191402c7be2c31a4ea7dcec53e255776ef6" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+≪W_{in}≫+2 \times ≪Text{padding}[2]-≪Text{dilation}[2]≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{out}≫+2 ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫ ≪W_{in}≫</target>
        </trans-unit>
        <trans-unit id="b4191f6bab2de7c1a0f2413fa95cad94e0003ac5" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫+≪W_{in}+2 \times ≫ ≪Text{padding}[2]-≫ ≪Text{kernel_size}[2]}≪Text{stride}[2]}+1 right\rheight®floor</target>
        </trans-unit>
        <trans-unit id="90b3ee06862d0af7ed835a4f57d3b9839c234435" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_{out}=≪W_{out}≫-≪W_{in}≫-≪W_{in}≫-≪W_{in][1]}≪W_{in][1]}+1right\rfloor</target>
        </trans-unit>
        <trans-unit id="206613fe29a3beacd4aa9146df44d435eeec070f" translate="yes" xml:space="preserve">
          <source>Wait for all the kernels in this stream to complete.</source>
          <target state="translated">このストリーム内のすべてのカーネルが完了するのを待ちます。</target>
        </trans-unit>
        <trans-unit id="a3b76987c40f20668517b915def98113b24efd97" translate="yes" xml:space="preserve">
          <source>Waits for all kernels in all streams on a CUDA device to complete.</source>
          <target state="translated">CUDAデバイス上のすべてのストリームのすべてのカーネルが完了するのを待ちます。</target>
        </trans-unit>
        <trans-unit id="02d96942e51e668861c82b6ad6a1888bbaffdfc3" translate="yes" xml:space="preserve">
          <source>Waits for all provided futures to be complete, and returns the list of completed values.</source>
          <target state="translated">提供されたすべての先物が完了するのを待ち、完了した値のリストを返します。</target>
        </trans-unit>
        <trans-unit id="0a15d8c4d33065db8154fb49b0e2d2a98624af19" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store, and throws an exception if the keys have not been set by the supplied &lt;code&gt;timeout&lt;/code&gt;.</source>
          <target state="translated">各キーを待つ &lt;code&gt;keys&lt;/code&gt; ストアに追加すること、およびキーを供給することによって設定されていない場合に例外をスロー &lt;code&gt;timeout&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="24a824e1b1fcd89978ae586133c59626ca034951" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store. If not all keys are set before the &lt;code&gt;timeout&lt;/code&gt; (set during store initialization), then &lt;code&gt;wait&lt;/code&gt; will throw an exception.</source>
          <target state="translated">キーの各キーがストアに追加されるのを待ち &lt;code&gt;keys&lt;/code&gt; 。 &lt;code&gt;timeout&lt;/code&gt; （ストアの初期化中に設定）の前にすべてのキーが設定されていない場合、 &lt;code&gt;wait&lt;/code&gt; は例外をスローします。</target>
        </trans-unit>
        <trans-unit id="bf737f8268ade30b29f4e304a6f74e8dc126217a" translate="yes" xml:space="preserve">
          <source>Waits for the event to complete.</source>
          <target state="translated">イベントが完了するのを待ちます。</target>
        </trans-unit>
        <trans-unit id="ff9074c4aa9ff896546792138df1d27c0f9655c8" translate="yes" xml:space="preserve">
          <source>Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.</source>
          <target state="translated">このイベントで現在捕捉されているすべての作業が完了するまで待機します。これにより、イベントが完了するまでCPUスレッドが進行しないようにします。</target>
        </trans-unit>
        <trans-unit id="e9c45563358e813f157ba81b33143542165ba84e" translate="yes" xml:space="preserve">
          <source>Warning</source>
          <target state="translated">Warning</target>
        </trans-unit>
        <trans-unit id="322f5a9cd5158cc9dec2b3e7da850004249c9e4e" translate="yes" xml:space="preserve">
          <source>We accumulate the gradients in the appropriate &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;torch.distributed.autograd.context&lt;/code&gt;&lt;/a&gt; on each of the nodes. The autograd context to be used is looked up given the &lt;code&gt;context_id&lt;/code&gt; that is passed in when &lt;a href=&quot;#torch.distributed.autograd.backward&quot;&gt;&lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt;&lt;/a&gt; is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; API.</source>
          <target state="translated">各ノードの適切な&lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt; &lt;code&gt;torch.distributed.autograd.context&lt;/code&gt; &lt;/a&gt;にグラデーションを蓄積します。使用されるautogradコンテキストは、&lt;a href=&quot;#torch.distributed.autograd.backward&quot;&gt; &lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt; &lt;/a&gt;が呼び出されたときに渡される &lt;code&gt;context_id&lt;/code&gt; を指定して検索されます。指定されたIDに対応する有効なautogradコンテキストがない場合、エラーをスローします。&lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt; &lt;code&gt;get_gradients()&lt;/code&gt; &lt;/a&gt; APIを使用して、累積されたグラデーションを取得できます。</target>
        </trans-unit>
        <trans-unit id="db56b28279fe3aea1467a7671b69e8c448f2218c" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt;&lt;code&gt;torch.nn.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt; &lt;code&gt;torch.nn.Linear&lt;/code&gt; &lt;/a&gt;と同じインターフェースを採用しています。</target>
        </trans-unit>
        <trans-unit id="3f8d51566ad98edb597de3ba663338465830f744" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv2d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv2d&quot;&gt; &lt;code&gt;torch.nn.quantized.Conv2d&lt;/code&gt; &lt;/a&gt;と同じインターフェースを採用しています。</target>
        </trans-unit>
        <trans-unit id="2e48e5f00566ef4a4b3bcb10eab8d225acffc2e4" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv3d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv3d&quot;&gt; &lt;code&gt;torch.nn.quantized.Conv3d&lt;/code&gt; &lt;/a&gt;と同じインターフェースを採用しています。</target>
        </trans-unit>
        <trans-unit id="a3fb7d904d476590d98359b7241880b1be2f9a76" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Linear&quot;&gt;&lt;code&gt;torch.nn.quantized.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Linear&quot;&gt; &lt;code&gt;torch.nn.quantized.Linear&lt;/code&gt; &lt;/a&gt;と同じインターフェースを採用しています。</target>
        </trans-unit>
        <trans-unit id="9fb0293b1874dcfe3feb591ccd49a1edaf2d5f3f" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Conv2d&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&quot;&gt;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&lt;/a&gt; for documentation.</source>
          <target state="translated">&lt;code&gt;torch.nn.Conv2d&lt;/code&gt; と同じインターフェースを採用しています。ドキュメントについては、&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&quot;&gt;https：&lt;/a&gt;//pytorch.org/docs/stable/nn.html？highlight = conv2d＃torch.nn.Conv2dを参照してください。</target>
        </trans-unit>
        <trans-unit id="27cfadeddc9160f1498530cc3eb1f719bfd8434a" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Linear&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt; for documentation.</source>
          <target state="translated">&lt;code&gt;torch.nn.Linear&lt;/code&gt; と同じインターフェースを採用しています。ドキュメントについては、&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https：&lt;/a&gt;//pytorch.org/docs/stable/nn.html#torch.nn.Linearを参照してください。</target>
        </trans-unit>
        <trans-unit id="5105f862983064b0cf7594afba8d668c8f56e9ff" translate="yes" xml:space="preserve">
          <source>We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements of a part of a model. Checkout this example:</source>
          <target state="translated">トレーシングとスクリプトの混在が可能です。モデルの一部の特定の要件に合わせて、トレースとスクリプトを構成することができます。この例をチェックしてみてください。</target>
        </trans-unit>
        <trans-unit id="6defebafebb2e25f76bcf411ab1ec455c032f365" translate="yes" xml:space="preserve">
          <source>We also do not support the following subsystems, though some may work out of the box:</source>
          <target state="translated">また、以下のサブシステムもサポートしていません。</target>
        </trans-unit>
        <trans-unit id="3fa9a188f6de578d3bee11e42ad3ca8321060293" translate="yes" xml:space="preserve">
          <source>We can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with &lt;code&gt;torch.cat&lt;/code&gt;:</source>
          <target state="translated">インプレース更新を使用しないようにコードを変更することでこれを修正できますが、 &lt;code&gt;torch.cat&lt;/code&gt; を使用して結果テンソルをアウトオブプレースで構築します。</target>
        </trans-unit>
        <trans-unit id="7aac442ad82fddd2074ccf19e4043ee60a7cdfb0" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; の&lt;/a&gt;インターフェースを組み合わせました。</target>
        </trans-unit>
        <trans-unit id="4304f8e1f80a53c4c4788f88c99ef4dcb6d0bc02" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt;&lt;code&gt;torch.nn.ReLU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt; &lt;code&gt;torch.nn.ReLU&lt;/code&gt; &lt;/a&gt;のインターフェースを組み合わせました。</target>
        </trans-unit>
        <trans-unit id="94fba3bcda49d77ec84730c68d223d594ab23cb3" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt; の&lt;/a&gt;インターフェースを組み合わせました。</target>
        </trans-unit>
        <trans-unit id="a068c518b2d5e22e14c479569a01c9c9d06bceec" translate="yes" xml:space="preserve">
          <source>We highly recommend taking a look at the original paper for more details.</source>
          <target state="translated">詳しくは原紙をご覧になることを強くお勧めします。</target>
        </trans-unit>
        <trans-unit id="86ad4838215218ac000a7d87b4b48e01c6961c5f" translate="yes" xml:space="preserve">
          <source>We provide models for action recognition pre-trained on Kinetics-400. They have all been trained with the scripts provided in &lt;code&gt;references/video_classification&lt;/code&gt;.</source>
          <target state="translated">Kinetics-400で事前トレーニングされた行動認識のモデルを提供します。それらはすべて、 &lt;code&gt;references/video_classification&lt;/code&gt; で提供されるスクリプトでトレーニングされています。</target>
        </trans-unit>
        <trans-unit id="e921c07692e5a21fc62c5f94070c73cf433f0bb7" translate="yes" xml:space="preserve">
          <source>We provide pre-trained models, using the PyTorch &lt;a href=&quot;../model_zoo#module-torch.utils.model_zoo&quot;&gt;&lt;code&gt;torch.utils.model_zoo&lt;/code&gt;&lt;/a&gt;. These can be constructed by passing &lt;code&gt;pretrained=True&lt;/code&gt;:</source>
          <target state="translated">PyTorch &lt;a href=&quot;../model_zoo#module-torch.utils.model_zoo&quot;&gt; &lt;code&gt;torch.utils.model_zoo&lt;/code&gt; &lt;/a&gt;を使用して、事前にトレーニングされたモデルを提供します。これらは、 &lt;code&gt;pretrained=True&lt;/code&gt; を渡すことで構築できます：</target>
        </trans-unit>
        <trans-unit id="62c9fd377e2aad0f66ac5c26c69e0a5867347f36" translate="yes" xml:space="preserve">
          <source>We provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons.</source>
          <target state="translated">純粋なPythonプログラムから、スタンドアロンのC++プログラムなど、Pythonとは独立して動作するTorchScriptプログラムにモデルをインクリメンタルに移行させるためのツールを提供します。これにより、Pythonの使い慣れたツールを使ってPyTorchでモデルを学習し、Pythonプログラムがパフォーマンスやマルチスレッドの理由で不利になる可能性がある本番環境にTorchScript経由でモデルをエクスポートすることが可能になります。</target>
        </trans-unit>
        <trans-unit id="bd73422aa80da87d8ae75185414e27781227d850" translate="yes" xml:space="preserve">
          <source>We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.</source>
          <target state="translated">提供されたルートを使用して、autogradグラフを発見し、適切な依存関係を計算します。このメソッドは、autogradの計算全体が完了するまでブロックします。</target>
        </trans-unit>
        <trans-unit id="7ea805e8dac2601210cb6d8d378a9d7e2e8c1197" translate="yes" xml:space="preserve">
          <source>Weibull</source>
          <target state="translated">Weibull</target>
        </trans-unit>
        <trans-unit id="48996658259127412cd98e4a4e7b4a204e6d1b0a" translate="yes" xml:space="preserve">
          <source>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by &lt;code&gt;name&lt;/code&gt; (e.g. &lt;code&gt;'weight'&lt;/code&gt;) with two parameters: one specifying the magnitude (e.g. &lt;code&gt;'weight_g'&lt;/code&gt;) and one specifying the direction (e.g. &lt;code&gt;'weight_v'&lt;/code&gt;). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every &lt;code&gt;forward()&lt;/code&gt; call.</source>
          <target state="translated">重みの正規化は、重みテンソルの大きさをその方向から切り離す再パラメーター化です。これにより、 &lt;code&gt;name&lt;/code&gt; 指定されたパラメーター（例： &lt;code&gt;'weight'&lt;/code&gt; ）が2つのパラメーターに置き換えられます。1つは大きさを指定し（例： &lt;code&gt;'weight_g'&lt;/code&gt; ）、もう1つは方向を指定します（例： &lt;code&gt;'weight_v'&lt;/code&gt; ）。重みの正規化は、すべての &lt;code&gt;forward()&lt;/code&gt; 呼び出しの前に、大きさと方向から重みテンソルを再計算するフックを介して実装されます。</target>
        </trans-unit>
        <trans-unit id="d53879f401fe4a3643952a897298dc95fdaf8d7e" translate="yes" xml:space="preserve">
          <source>Weight:</source>
          <target state="translated">Weight:</target>
        </trans-unit>
        <trans-unit id="769bb19e615b7f8e2809e5882e2d05a18f57a531" translate="yes" xml:space="preserve">
          <source>When</source>
          <target state="translated">When</target>
        </trans-unit>
        <trans-unit id="d4a914f8dcec6b172849d1c8fb16401fbdbb7604" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when &lt;code&gt;align_corners = False&lt;/code&gt;. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at &lt;code&gt;-1&lt;/code&gt;. From version 1.3.0, under &lt;code&gt;align_corners = True&lt;/code&gt; all grid points along a unit dimension are considered to be at &lt;code&gt;`0&lt;/code&gt; (the center of the input image).</source>
          <target state="translated">&lt;code&gt;align_corners = True&lt;/code&gt; 場合、1Dデータでの2Dアフィン変換と2Dデータでの3Dアフィン変換（つまり、空間次元の1つに単位サイズがある場合）は明確に定義されておらず、意図された使用例ではありません。 &lt;code&gt;align_corners = False&lt;/code&gt; 場合、これは問題ではありません。バージョン1.2.0までは、単位寸法に沿ったすべてのグリッドポイントは任意に &lt;code&gt;-1&lt;/code&gt; であると見なされていました。バージョン1.3.0以降、 &lt;code&gt;align_corners = True&lt;/code&gt; 、単位寸法に沿ったすべてのグリッドポイントは &lt;code&gt;`0&lt;/code&gt; （入力画像の中心）にあると見なされます。</target>
        </trans-unit>
        <trans-unit id="71f1f4c1d858ca7758a6c256f77db87995af50dc" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was &lt;code&gt;align_corners = True&lt;/code&gt;. Since then, the default behavior has been changed to &lt;code&gt;align_corners = False&lt;/code&gt;, in order to bring it in line with the default for &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;interpolate()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">場合 &lt;code&gt;align_corners = True&lt;/code&gt; 、グリッド位置は、入力画像サイズに画素サイズの相対的に依存し、によってサンプリング位置に&lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt; &lt;code&gt;grid_sample()&lt;/code&gt; は、&lt;/a&gt;（アップサンプリングまたはダウンサンプリングされた後に）異なる解像度で与えられた同じ入力に対して異なるであろう。バージョン1.2.0までのデフォルトの動作は &lt;code&gt;align_corners = True&lt;/code&gt; でした。それ以降、&lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;interpolate()&lt;/code&gt; の&lt;/a&gt;デフォルトと一致させるために、デフォルトの動作が &lt;code&gt;align_corners = False&lt;/code&gt; 変更されました。</target>
        </trans-unit>
        <trans-unit id="393f1e49e22eb1a104936dd30d39047e787303b1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;batch_size&lt;/code&gt; (default &lt;code&gt;1&lt;/code&gt;) is not &lt;code&gt;None&lt;/code&gt;, the data loader yields batched samples instead of individual samples. &lt;code&gt;batch_size&lt;/code&gt; and &lt;code&gt;drop_last&lt;/code&gt; arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify &lt;code&gt;batch_sampler&lt;/code&gt;, which yields a list of keys at a time.</source>
          <target state="translated">とき &lt;code&gt;batch_size&lt;/code&gt; （デフォルト &lt;code&gt;1&lt;/code&gt; ）ではないん &lt;code&gt;None&lt;/code&gt; 、データローダ収量はサンプルの代わりに、個々のサンプルをバッチ処理しました。 &lt;code&gt;batch_size&lt;/code&gt; 引数と &lt;code&gt;drop_last&lt;/code&gt; 引数は、データローダーがデータセットキーのバッチを取得する方法を指定するために使用されます。マップスタイルのデータセットの場合、ユーザーは代わりに &lt;code&gt;batch_sampler&lt;/code&gt; を指定できます。これにより、一度にキーのリストが生成されます。</target>
        </trans-unit>
        <trans-unit id="19d86416250dfee41099689ccdc474523b9de05a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;compute_uv&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, backward cannot be performed since &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; from the forward pass is required for the backward operation.</source>
          <target state="translated">&lt;code&gt;compute_uv&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; の場合、フォワードパスからの &lt;code&gt;U&lt;/code&gt; と &lt;code&gt;V&lt;/code&gt; がバックワード操作に必要であるため、バックワードを実行できません。</target>
        </trans-unit>
        <trans-unit id="2e150d7511a776c744a811c73ef08944e9bb6434" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;dim&lt;/code&gt; is given, a squeeze operation is done only in the given dimension. If &lt;code&gt;input&lt;/code&gt; is of shape:</source>
          <target state="translated">とき &lt;code&gt;dim&lt;/code&gt; が与えられ、スクイズ操作だけ与えられた次元で行われます。 &lt;code&gt;input&lt;/code&gt; が形状の場合：</target>
        </trans-unit>
        <trans-unit id="b1120f7dfd992f23bbea9418ec57a1af2fe9af61" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a scalar value, the operation applied is:</source>
          <target state="translated">場合 &lt;code&gt;exponent&lt;/code&gt; スカラー値であり、動作は適用しました。</target>
        </trans-unit>
        <trans-unit id="66481b64a8fde71d4312aa50639d9ce101db880a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the operation applied is:</source>
          <target state="translated">とき &lt;code&gt;exponent&lt;/code&gt; テンソルで、操作は、ISを適用します：</target>
        </trans-unit>
        <trans-unit id="25f39b6b9d40581f69f611ee72f68c18106be3d1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;exponent&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">とき &lt;code&gt;exponent&lt;/code&gt; テンソルである、の形状 &lt;code&gt;input&lt;/code&gt; と &lt;code&gt;exponent&lt;/code&gt; でなければなりません&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2903dec45ead9638a2d9f01c40302928711917b1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;groups == in_channels&lt;/code&gt; and &lt;code&gt;out_channels == K * in_channels&lt;/code&gt;, where &lt;code&gt;K&lt;/code&gt; is a positive integer, this operation is also termed in literature as depthwise convolution.</source>
          <target state="translated">場合 &lt;code&gt;groups == in_channels&lt;/code&gt; と &lt;code&gt;out_channels == K * in_channels&lt;/code&gt; 、 &lt;code&gt;K&lt;/code&gt; は正の整数であり、この操作はまた、奥行き畳み込みとして文献に呼ばれます。</target>
        </trans-unit>
        <trans-unit id="c1b8e1328795e35aa33cb3682bf9e0a128163cf6" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;input&lt;/code&gt; is on CUDA, &lt;a href=&quot;#torch.nonzero&quot;&gt;&lt;code&gt;torch.nonzero()&lt;/code&gt;&lt;/a&gt; causes host-device synchronization.</source>
          <target state="translated">場合 &lt;code&gt;input&lt;/code&gt; CUDAである、&lt;a href=&quot;#torch.nonzero&quot;&gt; &lt;code&gt;torch.nonzero()&lt;/code&gt; &lt;/a&gt;ホスト・デバイスの同期化を引き起こします。</target>
        </trans-unit>
        <trans-unit id="9fd8de791261b8fe5e3fd820c55e05ef945d4e88" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;module&lt;/code&gt; returns a scalar (i.e., 0-dimensional tensor) in &lt;code&gt;forward()&lt;/code&gt;, this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</source>
          <target state="translated">場合 &lt;code&gt;module&lt;/code&gt; スカラー（すなわち、0次元テンソル）を返す &lt;code&gt;forward()&lt;/code&gt; 、このラッパーは、各デバイスからの結果を含む、データを並列に使用されるデバイスの数に等しい長さのベクトルを返します。</target>
        </trans-unit>
        <trans-unit id="14811e48d579473d679dcda9d3180b5365c2423a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shape of &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor</source>
          <target state="translated">ときに &lt;code&gt;other&lt;/code&gt; テンソルである、の形状 &lt;code&gt;other&lt;/code&gt; でなければならない&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;下地テンソルの形状に</target>
        </trans-unit>
        <trans-unit id="6d6234d9ff0cd3ff058351e570bff6a3e90e8d1e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;other&lt;/code&gt; がテンソルである場合、 &lt;code&gt;input&lt;/code&gt; および &lt;code&gt;other&lt;/code&gt; の形状は&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;ブロードキャスト&lt;/a&gt;可能でなければなりません。</target>
        </trans-unit>
        <trans-unit id="2b23daf961076c6f771f3b98153804afb65cab2e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;size&lt;/code&gt; is given, it is the output size of the image &lt;code&gt;(h, w)&lt;/code&gt;.</source>
          <target state="translated">場合 &lt;code&gt;size&lt;/code&gt; 与えられ、それは画像の出力サイズである &lt;code&gt;(h, w)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d240a83215b54cebb671916345cdadc2427a5a89" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;some&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, the gradients on &lt;code&gt;U[..., :, min(m, n):]&lt;/code&gt; and &lt;code&gt;V[..., :, min(m, n):]&lt;/code&gt; will be ignored in backward as those vectors can be arbitrary bases of the subspaces.</source>
          <target state="translated">ときに &lt;code&gt;some&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; を、上の勾配 &lt;code&gt;U[..., :, min(m, n):]&lt;/code&gt; と &lt;code&gt;V[..., :, min(m, n):]&lt;/code&gt; これらのベクターは、できる限り後方に無視されます部分空間の任意の基底である。</target>
        </trans-unit>
        <trans-unit id="85caf16edf789cb76e0cd678a0b93ec725f0105b" translate="yes" xml:space="preserve">
          <source>When a model is trained on &lt;code&gt;M&lt;/code&gt; nodes with &lt;code&gt;batch=N&lt;/code&gt;, the gradient will be &lt;code&gt;M&lt;/code&gt; times smaller when compared to the same model trained on a single node with &lt;code&gt;batch=M*N&lt;/code&gt; (because the gradients between different nodes are averaged). You should take this into consideration when you want to obtain a mathematically equivalent training process compared to the local training counterpart.</source>
          <target state="translated">モデルが &lt;code&gt;batch=N&lt;/code&gt; の &lt;code&gt;M&lt;/code&gt; ノードでトレーニングされる場合、 &lt;code&gt;batch=M*N&lt;/code&gt; の単一ノードでトレーニングされた同じモデルと比較すると、勾配は &lt;code&gt;M&lt;/code&gt; 分の1になります（異なるノード間の勾配が平均化されるため）。ローカルトレーニングの対応物と比較して数学的に同等のトレーニングプロセスを取得したい場合は、これを考慮に入れる必要があります。</target>
        </trans-unit>
        <trans-unit id="794b9d3ddf3eff59a51409ca459125a99df4910e" translate="yes" xml:space="preserve">
          <source>When a non-sparse &lt;code&gt;param&lt;/code&gt; receives a non-sparse gradient during &lt;a href=&quot;#torch.autograd.backward&quot;&gt;&lt;code&gt;torch.autograd.backward()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.Tensor.backward&quot;&gt;&lt;code&gt;torch.Tensor.backward()&lt;/code&gt;&lt;/a&gt;&lt;code&gt;param.grad&lt;/code&gt; is accumulated as follows.</source>
          <target state="translated">&lt;a href=&quot;#torch.autograd.backward&quot;&gt; &lt;code&gt;torch.autograd.backward()&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;#torch.Tensor.backward&quot;&gt; &lt;code&gt;torch.Tensor.backward()&lt;/code&gt; &lt;/a&gt;中に非スパース &lt;code&gt;param&lt;/code&gt; が非スパース勾配を受信すると、 &lt;code&gt;param.grad&lt;/code&gt; は次のように累積されます。</target>
        </trans-unit>
        <trans-unit id="746e7941b779e1590393a8195388add16038ba46" translate="yes" xml:space="preserve">
          <source>When a subclass is used with &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;, each item in the dataset will be yielded from the &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; iterator. When &lt;code&gt;num_workers &amp;gt; 0&lt;/code&gt;, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. &lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt;&lt;code&gt;get_worker_info()&lt;/code&gt;&lt;/a&gt;, when called in a worker process, returns information about the worker. It can be used in either the dataset&amp;rsquo;s &lt;code&gt;__iter__()&lt;/code&gt; method or the &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; &amp;lsquo;s &lt;code&gt;worker_init_fn&lt;/code&gt; option to modify each copy&amp;rsquo;s behavior.</source>
          <target state="translated">サブクラスが&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;で使用される場合、データセット内の各アイテムは&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;イテレーターから生成されます。 &lt;code&gt;num_workers &amp;gt; 0&lt;/code&gt; 場合、各ワーカープロセスはデータセットオブジェクトの異なるコピーを持つため、ワーカーから重複データが返されないように、各コピーを個別に構成することが望ましい場合がよくあります。&lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt; &lt;code&gt;get_worker_info()&lt;/code&gt; &lt;/a&gt;は、ワーカープロセスで呼び出されると、ワーカーに関する情報を返します。データセットの &lt;code&gt;__iter__()&lt;/code&gt; メソッドまたは&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;の &lt;code&gt;worker_init_fn&lt;/code&gt; オプションのいずれかで使用して、各コピーの動作を変更できます。</target>
        </trans-unit>
        <trans-unit id="4794503b3573f4db7c79a8a74163748fdeea10ab" translate="yes" xml:space="preserve">
          <source>When both &lt;code&gt;batch_size&lt;/code&gt; and &lt;code&gt;batch_sampler&lt;/code&gt; are &lt;code&gt;None&lt;/code&gt; (default value for &lt;code&gt;batch_sampler&lt;/code&gt; is already &lt;code&gt;None&lt;/code&gt;), automatic batching is disabled. Each sample obtained from the &lt;code&gt;dataset&lt;/code&gt; is processed with the function passed as the &lt;code&gt;collate_fn&lt;/code&gt; argument.</source>
          <target state="translated">両方の場合は &lt;code&gt;batch_size&lt;/code&gt; と &lt;code&gt;batch_sampler&lt;/code&gt; がある &lt;code&gt;None&lt;/code&gt; （のデフォルト値 &lt;code&gt;batch_sampler&lt;/code&gt; がすでにされていない &lt;code&gt;None&lt;/code&gt; ）、自動バッチ処理が無効になっています。 &lt;code&gt;dataset&lt;/code&gt; から取得された各サンプルは、 &lt;code&gt;collate_fn&lt;/code&gt; 引数として渡された関数で処理されます。</target>
        </trans-unit>
        <trans-unit id="83b686ad466f2edcefa11e742fdd44b512b0dd23" translate="yes" xml:space="preserve">
          <source>When called in a worker, this returns an object guaranteed to have the following attributes:</source>
          <target state="translated">ワーカーで呼び出された場合、これは以下の属性を持つことが保証されたオブジェクトを返します。</target>
        </trans-unit>
        <trans-unit id="b6402d4d143ce0d83ed8c31ca1e7c6b83b35fb53" translate="yes" xml:space="preserve">
          <source>When called in the main process, this returns &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">メインプロセスで呼び出されると、 &lt;code&gt;None&lt;/code&gt; が返されます。</target>
        </trans-unit>
        <trans-unit id="bc06fbb059ebdabdac77c068a81b8512376f9ddd" translate="yes" xml:space="preserve">
          <source>When called with &lt;code&gt;dims&lt;/code&gt; of the list form, the given dimensions will be contracted in place of the last</source>
          <target state="translated">呼ばれると &lt;code&gt;dims&lt;/code&gt; 、リスト形式の、与えられた寸法は、最後の代わりに収縮されます</target>
        </trans-unit>
        <trans-unit id="8f0991d5226cadceec16a5e2277a3d90d9a71503" translate="yes" xml:space="preserve">
          <source>When called with a non-negative integer argument &lt;code&gt;dims&lt;/code&gt; =</source>
          <target state="translated">負でない整数の引数で呼び出された場合 &lt;code&gt;dims&lt;/code&gt; =</target>
        </trans-unit>
        <trans-unit id="6abce014373fbb8ba1a8a6700a958f442096752d" translate="yes" xml:space="preserve">
          <source>When combined with TorchScript decorators, this decorator must be the outmost one.</source>
          <target state="translated">TorchScriptのデコレータと組み合わせる場合は、このデコレータを一番外にしなければなりません。</target>
        </trans-unit>
        <trans-unit id="01692078c2de1ab235032349659ea1bad48180a7" translate="yes" xml:space="preserve">
          <source>When combined with static or class method, this decorator must be the inner one.</source>
          <target state="translated">静的メソッドやクラスメソッドと組み合わせる場合、このデコレータは内側のものでなければなりません。</target>
        </trans-unit>
        <trans-unit id="02e71c552b6cfbf5a63e34935049e68d7d217f7f" translate="yes" xml:space="preserve">
          <source>When creating a new &lt;a href=&quot;#torch.autograd.Function&quot;&gt;&lt;code&gt;Function&lt;/code&gt;&lt;/a&gt;, the following methods are available to &lt;code&gt;ctx&lt;/code&gt;.</source>
          <target state="translated">新しい&lt;a href=&quot;#torch.autograd.Function&quot;&gt; &lt;code&gt;Function&lt;/code&gt; &lt;/a&gt;を作成する場合、 &lt;code&gt;ctx&lt;/code&gt; では次のメソッドを使用できます。</target>
        </trans-unit>
        <trans-unit id="24460dec44dc8e232523a0b3b6cffc083f18ab5b" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.Tensor.new_tensor&quot;&gt;&lt;code&gt;new_tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;tensor.new_tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;tensor.new_tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="translated">データがテンソル &lt;code&gt;x&lt;/code&gt; の場合、&lt;a href=&quot;#torch.Tensor.new_tensor&quot;&gt; &lt;code&gt;new_tensor()&lt;/code&gt; &lt;/a&gt;は渡されたものから「データ」を読み取り、リーフ変数を作成します。したがって、 &lt;code&gt;tensor.new_tensor(x)&lt;/code&gt; は &lt;code&gt;x.clone().detach()&lt;/code&gt; と &lt;code&gt;tensor.new_tensor(x, requires_grad=True)&lt;/code&gt; あり、tensor.new_tensor（x、requires_grad = True）は &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt; と同等です。 &lt;code&gt;clone()&lt;/code&gt; と &lt;code&gt;detach()&lt;/code&gt; を使用した同等のものをお勧めします。</target>
        </trans-unit>
        <trans-unit id="44313813a50d98d5898f4bddc561c8a3109b1400" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;torch.tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;torch.tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="translated">データがテンソル &lt;code&gt;x&lt;/code&gt; の場合、&lt;a href=&quot;#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt;は渡されたものから「データ」を読み取り、リーフ変数を作成します。したがって、 &lt;code&gt;torch.tensor(x)&lt;/code&gt; は &lt;code&gt;x.clone().detach()&lt;/code&gt; と &lt;code&gt;torch.tensor(x, requires_grad=True)&lt;/code&gt; あり、torch.tensor（x、requires_grad = True）は &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt; と同等です。 &lt;code&gt;clone()&lt;/code&gt; と &lt;code&gt;detach()&lt;/code&gt; を使用した同等のものをお勧めします。</target>
        </trans-unit>
        <trans-unit id="4d87a694539e44d9bb0922817749f14565db02a2" translate="yes" xml:space="preserve">
          <source>When drawn without replacement, &lt;code&gt;num_samples&lt;/code&gt; must be lower than number of non-zero elements in &lt;code&gt;input&lt;/code&gt; (or the min number of non-zero elements in each row of &lt;code&gt;input&lt;/code&gt; if it is a matrix).</source>
          <target state="translated">置換せずに描画する場合、 &lt;code&gt;num_samples&lt;/code&gt; は、 &lt;code&gt;input&lt;/code&gt; の非ゼロ要素の数（または、行列の場合は &lt;code&gt;input&lt;/code&gt; 各行の非ゼロ要素の最小数）よりも小さくする必要があります。</target>
        </trans-unit>
        <trans-unit id="0bdf0c7df4d55ef87659ffa5921e0c1608596af5" translate="yes" xml:space="preserve">
          <source>When entering an autocast-enabled region, Tensors may be any type. You should not call &lt;code&gt;.half()&lt;/code&gt; on your model(s) or inputs when using autocasting.</source>
          <target state="translated">自動キャストが有効な領域に入るとき、テンソルはどのタイプでもかまいません。 &lt;code&gt;.half()&lt;/code&gt; を使用する場合は、モデルまたは入力で.half（）を呼び出さないでください。</target>
        </trans-unit>
        <trans-unit id="52b1d0543e21755317d78418469b43a9d3b74143" translate="yes" xml:space="preserve">
          <source>When fetching from &lt;a href=&quot;#iterable-style-datasets&quot;&gt;iterable-style datasets&lt;/a&gt; with &lt;a href=&quot;#multi-process-data-loading&quot;&gt;multi-processing&lt;/a&gt;, the &lt;code&gt;drop_last&lt;/code&gt; argument drops the last non-full batch of each worker&amp;rsquo;s dataset replica.</source>
          <target state="translated">フェッチする場合は&lt;a href=&quot;#iterable-style-datasets&quot;&gt;反復可能な形式のデータセット&lt;/a&gt;と&lt;a href=&quot;#multi-process-data-loading&quot;&gt;マルチプロセッシング&lt;/a&gt;、 &lt;code&gt;drop_last&lt;/code&gt; 引数は、各労働者のデータセットのレプリカの最後の非完全なバッチを削除します。</target>
        </trans-unit>
        <trans-unit id="aeab232209859fdb56ecfdd5e27289109b3779cd" translate="yes" xml:space="preserve">
          <source>When given an image of &lt;code&gt;Channels x Height x Width&lt;/code&gt;, it will apply &lt;code&gt;Softmax&lt;/code&gt; to each location</source>
          <target state="translated">&lt;code&gt;Channels x Height x Width&lt;/code&gt; の画像が与えられると、各場所に &lt;code&gt;Softmax&lt;/code&gt; が適用されます</target>
        </trans-unit>
        <trans-unit id="722286111baf501df8f66886a8878dbc93c4e6e4" translate="yes" xml:space="preserve">
          <source>When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:</source>
          <target state="translated">last_epoch=-1の場合、初期値lrをlrに設定します。スケジュールは再帰的に定義されているため、学習率は他の演算子によってこのスケジューラの外でも同時に変更できることに注意してください。このスケジューラだけで学習率を設定すると、各ステップの学習率は次のようになります。</target>
        </trans-unit>
        <trans-unit id="9635ed5ff58b5f10f0a861bfe4da315b19a3d9ec" translate="yes" xml:space="preserve">
          <source>When manually importing this backend and invoking &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; with the corresponding backend name, the &lt;code&gt;torch.distributed&lt;/code&gt; package runs on the new backend.</source>
          <target state="translated">このバックエンドを手動でインポートし、対応するバックエンド名で&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt;を呼び出すと、 &lt;code&gt;torch.distributed&lt;/code&gt; パッケージが新しいバックエンドで実行されます。</target>
        </trans-unit>
        <trans-unit id="6c58952c2db520eb2d0c000e5b5179ffccb22124" translate="yes" xml:space="preserve">
          <source>When passed to the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;torch.jit.script&lt;/code&gt;&lt;/a&gt; function, a &lt;code&gt;torch.nn.Module&lt;/code&gt;&amp;rsquo;s data is copied to a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and the TorchScript compiler compiles the module. The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;, as well as any &lt;code&gt;@torch.jit.export&lt;/code&gt; methods.</source>
          <target state="translated">渡されたとき&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;torch.jit.script&lt;/code&gt; &lt;/a&gt;機能、 &lt;code&gt;torch.nn.Module&lt;/code&gt; のデータがコピーされ&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;とTorchScriptコンパイラは、モジュールをコンパイルします。モジュールの &lt;code&gt;forward&lt;/code&gt; はデフォルトでコンパイルされます。 &lt;code&gt;forward&lt;/code&gt; から呼び出されたメソッドは、 &lt;code&gt;@torch.jit.export&lt;/code&gt; メソッドと同様に、 &lt;code&gt;forward&lt;/code&gt; で使用される順序で遅延コンパイルされます。</target>
        </trans-unit>
        <trans-unit id="98975218294f0d2cf1d9e027d0c9f18c335eccc0" translate="yes" xml:space="preserve">
          <source>When running on CUDA, &lt;code&gt;row * col&lt;/code&gt; must be less than</source>
          <target state="translated">CUDAで実行する場合、 &lt;code&gt;row * col&lt;/code&gt; は以下でなければなりません</target>
        </trans-unit>
        <trans-unit id="6442e147505eb082b7d915b41a9300ebc0cdaab5" translate="yes" xml:space="preserve">
          <source>When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.</source>
          <target state="translated">scale_factorが指定されている場合、recompute_scale_factor=Trueの場合、scale_factorは出力サイズの計算に使用され、補間のための新しいスケールを推論するために使用されます。recompute_scale_factorのデフォルトの動作は1.6.0でFalseに変更され、補間計算ではscale_factorが使用されます。</target>
        </trans-unit>
        <trans-unit id="6134dc991dfe53224afc4b25750c288358d06db2" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;divisor&lt;/code&gt; tensor contains no zero elements, then &lt;code&gt;fold&lt;/code&gt; and &lt;code&gt;unfold&lt;/code&gt; operations are inverses of each other (up to constant divisor).</source>
          <target state="translated">場合 &lt;code&gt;divisor&lt;/code&gt; テンソルは全くゼロ要素を含まず、その後 &lt;code&gt;fold&lt;/code&gt; と &lt;code&gt;unfold&lt;/code&gt; 操作はお互い（定数除数まで）の逆数です。</target>
        </trans-unit>
        <trans-unit id="ea57553addc94dbd66c0aab4dca41d371d835f2e" translate="yes" xml:space="preserve">
          <source>When the dtypes of inputs to an arithmetic operation (&lt;code&gt;add&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;) differ, we promote by finding the minimum dtype that satisfies the following rules:</source>
          <target state="translated">算術演算（ &lt;code&gt;add&lt;/code&gt; 、 &lt;code&gt;sub&lt;/code&gt; 、 &lt;code&gt;div&lt;/code&gt; 、 &lt;code&gt;mul&lt;/code&gt; ）への入力のdtypeが異なる場合、次のルールを満たす最小のdtypeを見つけることによって促進します。</target>
        </trans-unit>
        <trans-unit id="b36d5c15d9cebdd9841a3cbecf930ef5be3e8fb3" translate="yes" xml:space="preserve">
          <source>When the input Tensor is a sparse tensor then the unspecifed values are treated as &lt;code&gt;-inf&lt;/code&gt;.</source>
          <target state="translated">入力テンソルがスパーステンソルの場合、指定されていない値は &lt;code&gt;-inf&lt;/code&gt; として扱われます。</target>
        </trans-unit>
        <trans-unit id="0306aa2f6b60a04c04ba2bb3e8bec0e8ed79e4a2" translate="yes" xml:space="preserve">
          <source>When the probability density function is differentiable with respect to its parameters, we only need &lt;code&gt;sample()&lt;/code&gt; and &lt;code&gt;log_prob()&lt;/code&gt; to implement REINFORCE:</source>
          <target state="translated">確率密度関数がそのパラメーターに関して微分可能である場合、REINFORCEを実装するために必要なのは &lt;code&gt;sample()&lt;/code&gt; と &lt;code&gt;log_prob()&lt;/code&gt; だけです。</target>
        </trans-unit>
        <trans-unit id="adc8c2e11d80dad257b6a86b35591121148d959c" translate="yes" xml:space="preserve">
          <source>When the shapes do not match, the shape of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; is used as the shape for the returned output tensor</source>
          <target state="translated">形状が一致しない場合、&lt;a href=&quot;torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt;の形状が返される出力テンソルの形状として使用されます</target>
        </trans-unit>
        <trans-unit id="0836fba3472a07dd87c4cfcc80c3a0f1b5a7eda0" translate="yes" xml:space="preserve">
          <source>When used in a &lt;code&gt;worker_init_fn&lt;/code&gt; passed over to &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;, this method can be useful to set up each worker process differently, for instance, using &lt;code&gt;worker_id&lt;/code&gt; to configure the &lt;code&gt;dataset&lt;/code&gt; object to only read a specific fraction of a sharded dataset, or use &lt;code&gt;seed&lt;/code&gt; to seed other libraries used in dataset code (e.g., NumPy).</source>
          <target state="translated">で使用される場合 &lt;code&gt;worker_init_fn&lt;/code&gt; に渡さ&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;、この方法は使用し、例えば、異なる各ワーカープロセスを設定するのに有用であり得る &lt;code&gt;worker_id&lt;/code&gt; を構成する &lt;code&gt;dataset&lt;/code&gt; のみかけらデータセットの特定の画分、または使用読み取りすることを目的とする &lt;code&gt;seed&lt;/code&gt; 他に播種しますデータセットコードで使用されるライブラリ（例：NumPy）。</target>
        </trans-unit>
        <trans-unit id="7a635c724cadbffe9cd8e217f6606360c059e6c0" translate="yes" xml:space="preserve">
          <source>When using &lt;a href=&quot;#torch.utils.cpp_extension.BuildExtension&quot;&gt;&lt;code&gt;BuildExtension&lt;/code&gt;&lt;/a&gt;, it is allowed to supply a dictionary for &lt;code&gt;extra_compile_args&lt;/code&gt; (rather than the usual list) that maps from languages (&lt;code&gt;cxx&lt;/code&gt; or &lt;code&gt;nvcc&lt;/code&gt;) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.</source>
          <target state="translated">&lt;a href=&quot;#torch.utils.cpp_extension.BuildExtension&quot;&gt; &lt;code&gt;BuildExtension&lt;/code&gt; &lt;/a&gt;を使用する場合、言語（ &lt;code&gt;cxx&lt;/code&gt; または &lt;code&gt;nvcc&lt;/code&gt; ）からコンパイラーに提供する追加のコンパイラーフラグのリストにマップする &lt;code&gt;extra_compile_args&lt;/code&gt; （通常のリストではなく）の辞書を提供できます。これにより、混合コンパイル中にC ++コンパイラとCUDAコンパイラに異なるフラグを指定できます。</target>
        </trans-unit>
        <trans-unit id="eb9c51be75e99cadf6963fd3f79e89b1c37968c6" translate="yes" xml:space="preserve">
          <source>When using an &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; with &lt;a href=&quot;#multi-process-data-loading&quot;&gt;multi-process data loading&lt;/a&gt;. The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; documentations for how to achieve this.</source>
          <target state="translated">&lt;a href=&quot;#multi-process-data-loading&quot;&gt;マルチプロセスデータの読み込みで&lt;/a&gt;&lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt;を使用する場合。同じデータセットオブジェクトが各ワーカープロセスでレプリケートされるため、データの重複を避けるためにレプリカを異なる方法で構成する必要があります。これを実現する方法については、&lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; の&lt;/a&gt;ドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="7635dd8102bfabbafa2a3b5c97436e5cb54d9ba1" translate="yes" xml:space="preserve">
          <source>When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;Reproducibility&lt;/a&gt; for background.</source>
          <target state="translated">CUDAバックエンドを使用する場合、この操作は、簡単にオフにできないバックワードパスで非決定的な動作を引き起こす可能性があります。背景については、&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;再現性&lt;/a&gt;に関する注記を参照してください。</target>
        </trans-unit>
        <trans-unit id="54dc7a473943cf2733a6d4da75adee3f4a210bcf" translate="yes" xml:space="preserve">
          <source>When viewing a profile created using &lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt;&lt;code&gt;emit_nvtx&lt;/code&gt;&lt;/a&gt; in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, &lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt;&lt;code&gt;emit_nvtx&lt;/code&gt;&lt;/a&gt; appends sequence number information to the ranges it generates.</source>
          <target state="translated">Nvidia Visual Profilerで&lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt; &lt;code&gt;emit_nvtx&lt;/code&gt; &lt;/a&gt;を使用して作成されたプロファイルを表示する場合、各バックワードパス操作を対応するフォワードパス操作と関連付けるのは難しい場合があります。このタスクを容易にするために、&lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt; &lt;code&gt;emit_nvtx&lt;/code&gt; &lt;/a&gt;は、生成する範囲にシーケンス番号情報を追加します。</target>
        </trans-unit>
        <trans-unit id="79561f3cf985a5e4719b8247ff531adef7e29706" translate="yes" xml:space="preserve">
          <source>When writing TorchScript directly using &lt;code&gt;@torch.jit.script&lt;/code&gt; decorator, the programmer must only use the subset of Python supported in TorchScript. This section documents what is supported in TorchScript as if it were a language reference for a stand alone language. Any features of Python not mentioned in this reference are not part of TorchScript. See &lt;code&gt;Builtin Functions&lt;/code&gt; for a complete reference of available Pytorch tensor methods, modules, and functions.</source>
          <target state="translated">&lt;code&gt;@torch.jit.script&lt;/code&gt; デコレータを使用してTorchScriptを直接記述する場合、プログラマはTorchScriptでサポートされているPythonのサブセットのみを使用する必要があります。このセクションでは、TorchScriptでサポートされているものを、スタンドアロン言語の言語リファレンスであるかのように説明します。このリファレンスに記載されていないPythonの機能は、TorchScriptの一部ではありません。使用可能なPytorchテンソルのメソッド、モジュール、および関数の完全なリファレンスについては、組み込み &lt;code&gt;Builtin Functions&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="50bf66ea03e8d3f7a8a4f3f33c0d423a99cbb251" translate="yes" xml:space="preserve">
          <source>When you call &lt;a href=&quot;#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt; on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call &lt;code&gt;torch.load(.., map_location='cpu')&lt;/code&gt; and then &lt;code&gt;load_state_dict()&lt;/code&gt; to avoid GPU RAM surge when loading a model checkpoint.</source>
          <target state="translated">GPUテンソルを含むファイルで&lt;a href=&quot;#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt;を呼び出すと、それらのテンソルはデフォルトでGPUにロードされます。あなたは呼び出すことができます &lt;code&gt;torch.load(.., map_location='cpu')&lt;/code&gt; 、その後 &lt;code&gt;load_state_dict()&lt;/code&gt; モデルのチェックポイントをロードするときにGPU RAMのサージを避けるために。</target>
        </trans-unit>
        <trans-unit id="d4b9a73359398be175a62d4f2cbf23eca1eb7394" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ここで、 &lt;code&gt;n = prod(s)&lt;/code&gt; は論理FFTサイズです。同じ正規化モードで後方変換（&lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt;）を呼び出すと、2つの変換間に &lt;code&gt;1/n&lt;/code&gt; の全体的な正規化が適用されます。これは、&lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; を&lt;/a&gt;完全に逆にするために必要です。</target>
        </trans-unit>
        <trans-unit id="540833bd1b024f45b94c5fe10bdcce398da41e2a" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ここで、 &lt;code&gt;n = prod(s)&lt;/code&gt; は論理FFTサイズです。同じ正規化モードで逆方向変換（&lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt;）を呼び出すと、2つの変換間に &lt;code&gt;1/n&lt;/code&gt; の全体的な正規化が適用されます。これは、&lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; を&lt;/a&gt;完全に逆にするために必要です。</target>
        </trans-unit>
        <trans-unit id="4fa609678777cb0f1d2bea5c36804f3e7f699ab3" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ここで、 &lt;code&gt;n = prod(s)&lt;/code&gt; は論理IFFTサイズです。同じ正規化モードで順方向変換（&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt;）を呼び出すと、2つの変換間に &lt;code&gt;1/n&lt;/code&gt; の全体的な正規化が適用されます。これは、&lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; を&lt;/a&gt;完全に逆にするために必要です。</target>
        </trans-unit>
        <trans-unit id="983301015460563605882b2d43f60b6f31ca13f7" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ここで、 &lt;code&gt;n = prod(s)&lt;/code&gt; は論理IFFTサイズです。同じ正規化モードで順方向変換（&lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;）を呼び出すと、2つの変換間に &lt;code&gt;1/n&lt;/code&gt; の全体的な正規化が適用されます。これは、&lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; を&lt;/a&gt;完全に逆にするために必要です。</target>
        </trans-unit>
        <trans-unit id="dcabf659c01386b96c4252ad5247bb9f50b66574" translate="yes" xml:space="preserve">
          <source>Where are my downloaded models saved?</source>
          <target state="translated">ダウンロードしたモデルはどこに保存されていますか?</target>
        </trans-unit>
        <trans-unit id="017d4a3464bb8236ca4dfe2a53c2f262d76de8ab" translate="yes" xml:space="preserve">
          <source>Which backend to use?</source>
          <target state="translated">どのバックエンドを使うか?</target>
        </trans-unit>
        <trans-unit id="3631991286d3139c1de9a41ef4982f8e5d886ce2" translate="yes" xml:space="preserve">
          <source>Which produces:</source>
          <target state="translated">どっちが産むんだ?</target>
        </trans-unit>
        <trans-unit id="a68a67a970d91d390715c4a5723442211582200e" translate="yes" xml:space="preserve">
          <source>While Loops</source>
          <target state="translated">Whileループ</target>
        </trans-unit>
        <trans-unit id="48f3d5bde3297b3b923d0f7759d754056d7f6c40" translate="yes" xml:space="preserve">
          <source>While it is assumed that &lt;code&gt;A&lt;/code&gt; is symmetric, &lt;code&gt;A.grad&lt;/code&gt; is not. To make sure that &lt;code&gt;A.grad&lt;/code&gt; is symmetric, so that &lt;code&gt;A - t * A.grad&lt;/code&gt; is symmetric in first-order optimization routines, prior to running &lt;code&gt;lobpcg&lt;/code&gt; we do the following symmetrization map: &lt;code&gt;A -&amp;gt; (A + A.t()) / 2&lt;/code&gt;. The map is performed only when the &lt;code&gt;A&lt;/code&gt; requires gradients.</source>
          <target state="translated">&lt;code&gt;A&lt;/code&gt; は対称であると想定されていますが、 &lt;code&gt;A.grad&lt;/code&gt; は対称ではありません。 &lt;code&gt;A.grad&lt;/code&gt; が対称であることを確認し、 &lt;code&gt;A - t * A.grad&lt;/code&gt; が1次最適化ルーチンで対称になるようにするには、 &lt;code&gt;lobpcg&lt;/code&gt; を実行する前に、次の対称化マップを実行します &lt;code&gt;A -&amp;gt; (A + A.t()) / 2&lt;/code&gt; 。マップは、 &lt;code&gt;A&lt;/code&gt; がグラデーションを必要とする場合にのみ実行されます。</target>
        </trans-unit>
        <trans-unit id="f69597c76f979c6f47613f7588234a4093ae500a" translate="yes" xml:space="preserve">
          <source>While it should always give you a valid decomposition, it may not give you the same one across platforms - it will depend on your LAPACK implementation.</source>
          <target state="translated">常に有効な分解を提供する必要がありますが、プラットフォームをまたいで同じものを提供するとは限りません-それはあなたのLAPACKの実装に依存します。</target>
        </trans-unit>
        <trans-unit id="90f928cf1ec6eb30bffa5d56a4ee00f33a26ccf6" translate="yes" xml:space="preserve">
          <source>While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.</source>
          <target state="translated">数学的には log(softmax(x))と同等ですが、これら2つの演算を別々に行うと遅くなり、数値的にも不安定になります。この関数は,出力と勾配を正しく計算するために別の定式化を用いています.</target>
        </trans-unit>
        <trans-unit id="adfc4c1cf043279d74d69b96592d62572f9a8648" translate="yes" xml:space="preserve">
          <source>Wide ResNet</source>
          <target state="translated">ワイドレスネット</target>
        </trans-unit>
        <trans-unit id="eec7e605ef42b4a1da3f9a4494e1414462efb313" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2</source>
          <target state="translated">ワイドResNet-101-2</target>
        </trans-unit>
        <trans-unit id="ac86db1353b797ed1c0d73605a56146ab6cb1914" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">「WideResidualNetworks &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;」の&lt;/a&gt;WideResNet-101-2モデル</target>
        </trans-unit>
        <trans-unit id="b631c349f7132ef6c2a8879b09c961b30fce8aba" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2</source>
          <target state="translated">ワイドResNet-50-2</target>
        </trans-unit>
        <trans-unit id="726f31b3e4c15ecbbc898cae3f9e8f73a3460020" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">「WideResidualNetworks &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;」の&lt;/a&gt;WideResNet-50-2モデル</target>
        </trans-unit>
        <trans-unit id="d2b826d3f7d8e2201135c671569ea283afb245af" translate="yes" xml:space="preserve">
          <source>Will result in:</source>
          <target state="translated">結果として</target>
        </trans-unit>
        <trans-unit id="8fa871c4385dea4733bc80aea73b5f5364e4ef06" translate="yes" xml:space="preserve">
          <source>Windows FAQ</source>
          <target state="translated">Windows FAQ</target>
        </trans-unit>
        <trans-unit id="8ac66d0e68a85c8d292fec91f6cbcf0bd809b4e9" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;bilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="translated">&lt;code&gt;align_corners = True&lt;/code&gt; 、直線補間モード（ &lt;code&gt;bilinear&lt;/code&gt; ）比例出力と入力画素を整列していないので、出力値は入力の大きさに依存することができます。これは、バージョン0.3.1までのこれらのモードのデフォルトの動作でした。それ以降、デフォルトの動作は &lt;code&gt;align_corners = False&lt;/code&gt; です。これが出力にどのように影響するかについての具体的な例については、&lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt; &lt;code&gt;Upsample&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="05c8bfe5d3e24898d75d1b114e7f0550313f9961" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See below for concrete examples on how this affects the outputs.</source>
          <target state="translated">&lt;code&gt;align_corners = True&lt;/code&gt; 、直線補間モード（ &lt;code&gt;linear&lt;/code&gt; 、 &lt;code&gt;bilinear&lt;/code&gt; 、 &lt;code&gt;bicubic&lt;/code&gt; 、および &lt;code&gt;trilinear&lt;/code&gt; ）比例出力と入力画素を整列していないので、出力値は入力の大きさに依存することができます。これは、バージョン0.3.1までのこれらのモードのデフォルトの動作でした。それ以降、デフォルトの動作は &lt;code&gt;align_corners = False&lt;/code&gt; です。これが出力にどのように影響するかについての具体的な例については、以下を参照してください。</target>
        </trans-unit>
        <trans-unit id="9b1242167a707b16c47db2fe5f3c0130b4361833" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="translated">&lt;code&gt;align_corners = True&lt;/code&gt; 、直線補間モード（ &lt;code&gt;linear&lt;/code&gt; 、 &lt;code&gt;bilinear&lt;/code&gt; 、および &lt;code&gt;trilinear&lt;/code&gt; ）比例出力と入力画素を整列していないので、出力値は入力の大きさに依存することができます。これは、バージョン0.3.1までのこれらのモードのデフォルトの動作でした。それ以降、デフォルトの動作は &lt;code&gt;align_corners = False&lt;/code&gt; です。これが出力にどのように影響するかについての具体的な例については、&lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt; &lt;code&gt;Upsample&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="343e3c563441d98e78b2af38a8ee16b2107d4b5b" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;mode='bicubic'&lt;/code&gt;, it&amp;rsquo;s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call &lt;code&gt;result.clamp(min=0, max=255)&lt;/code&gt; if you want to reduce the overshoot when displaying the image.</source>
          <target state="translated">で &lt;code&gt;mode='bicubic'&lt;/code&gt; 、それは言い換えれば、それはイメージのため255よりも大きな負の値または値を生成することができ、オーバーシュートが発生することが可能です。画像を表示するときのオーバーシュートを減らしたい場合は &lt;code&gt;result.clamp(min=0, max=255)&lt;/code&gt; 明示的に呼び出します。</target>
        </trans-unit>
        <trans-unit id="670f4308b71dfa44f0ce3d603252c9a80431b7a1" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;padding_idx&lt;/code&gt; set, the embedding vector at &lt;code&gt;padding_idx&lt;/code&gt; is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from &lt;a href=&quot;#torch.nn.Embedding&quot;&gt;&lt;code&gt;Embedding&lt;/code&gt;&lt;/a&gt; is always zero.</source>
          <target state="translated">&lt;code&gt;padding_idx&lt;/code&gt; セット、の埋め込みベクトル &lt;code&gt;padding_idx&lt;/code&gt; は、すべてゼロに初期化されます。ただし、このベクトルは後で変更できることに注意してください。たとえば、カスタマイズされた初期化方法を使用して、出力のパディングに使用されるベクトルを変更します。&lt;a href=&quot;#torch.nn.Embedding&quot;&gt; &lt;code&gt;Embedding&lt;/code&gt; &lt;/a&gt;からのこのベクトルの勾配は常にゼロです。</target>
        </trans-unit>
        <trans-unit id="245747fecda85d5b71d626b9e2eb04627690bcb2" translate="yes" xml:space="preserve">
          <source>With &lt;em&gt;trace-based&lt;/em&gt; exporter, we get the result ONNX graph which unrolls the for loop:</source>
          <target state="translated">で&lt;em&gt;、トレースベースの&lt;/em&gt;輸出、我々はループのアンロール結果ONNXグラフを取得します：</target>
        </trans-unit>
        <trans-unit id="19fffb59deac1debbb0ed0b605bbe04aebbddb29" translate="yes" xml:space="preserve">
          <source>With the default arguments it uses the Euclidean norm over vectors along dimension</source>
          <target state="translated">デフォルトの引数では,次元に沿ったベクトルのユークリッドノルムを使用します.</target>
        </trans-unit>
        <trans-unit id="6aa512cf7aad097706fd2890b5de1202e2c6a97c" translate="yes" xml:space="preserve">
          <source>Within a Python process, the &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;Global Interpreter Lock (GIL)&lt;/a&gt; prevents true fully parallelizing Python code across threads. To avoid blocking computation code with data loading, PyTorch provides an easy switch to perform multi-process data loading by simply setting the argument &lt;code&gt;num_workers&lt;/code&gt; to a positive integer.</source>
          <target state="translated">Pythonプロセス内で、&lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;グローバルインタープリターロック（GIL）は&lt;/a&gt;、スレッド間でPythonコードを完全に並列化することを防ぎます。データの読み込みによる計算コードのブロックを回避するために、PyTorchは、引数 &lt;code&gt;num_workers&lt;/code&gt; を正の整数に設定するだけで、マルチプロセスのデータ読み込みを実行するための簡単なスイッチを提供します。</target>
        </trans-unit>
        <trans-unit id="67793521614b6b44f958c708ad8988aeadfd2309" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length in the last dimension:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; に&lt;/a&gt;出力長を指定しないと、入力が最後の次元で奇数長であるため、出力は適切にラウンドトリップしません。</target>
        </trans-unit>
        <trans-unit id="4263d953681d9ae30565f30202d1492e42e9e8ac" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; に&lt;/a&gt;出力長を指定しないと、入力が奇数長であるため、出力は適切にラウンドトリップしません。</target>
        </trans-unit>
        <trans-unit id="2f5f94e39ac0f29eec4ef83cffba8ff93856c6cc" translate="yes" xml:space="preserve">
          <source>Workers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected.</source>
          <target state="translated">イテレータがガベージコレクタになった場合や、イテレータが終了するとワーカーはシャットダウンされます。</target>
        </trans-unit>
        <trans-unit id="275f19f1f1857e51454ed2397868a56594f54c36" translate="yes" xml:space="preserve">
          <source>Working with &lt;code&gt;collate_fn&lt;/code&gt;</source>
          <target state="translated">作業 &lt;code&gt;collate_fn&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7322e6cb1692d568f796bae37f8f8678f8c44133" translate="yes" xml:space="preserve">
          <source>Wrap most of you main script&amp;rsquo;s code within &lt;code&gt;if __name__ == '__main__':&lt;/code&gt; block, to make sure it doesn&amp;rsquo;t run again (most likely generating error) when each worker process is launched. You can place your dataset and &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; instance creation logic here, as it doesn&amp;rsquo;t need to be re-executed in workers.</source>
          <target state="translated">ほとんどのメインスクリプトのコードを &lt;code&gt;if __name__ == '__main__':&lt;/code&gt; main __'：ブロック内にラップし、各ワーカープロセスの起動時に再び実行されないようにします（エラーが発生する可能性があります）。データセットと&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;インスタンス作成ロジックは、ワーカーで再実行する必要がないため、ここに配置できます。</target>
        </trans-unit>
        <trans-unit id="1ecad6be71c469e57d25325f17d76f3b6c7e6652" translate="yes" xml:space="preserve">
          <source>Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.</source>
          <target state="translated">有効なqconfigを持っている場合は、葉の子モジュールをQuantWrapperでラップします。 この関数は、インプレースのモジュールの子モジュールを修正し、入力モジュールもラップする新しいモジュールを返すことができます。</target>
        </trans-unit>
        <trans-unit id="5375f310fdbbbaceba22e923b5eb13a5f74fab7f" translate="yes" xml:space="preserve">
          <source>Wrapper around a &lt;code&gt;torch._C.Future&lt;/code&gt; which encapsulates an asynchronous execution of a callable, e.g. &lt;a href=&quot;rpc#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;. It also exposes a set of APIs to add callback functions and set results.</source>
          <target state="translated">呼び出し可能オブジェクトの非同期実行をカプセル化する &lt;code&gt;torch._C.Future&lt;/code&gt; ラッパー。例：&lt;a href=&quot;rpc#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;。また、コールバック関数を追加して結果を設定するための一連のAPIも公開しています。</target>
        </trans-unit>
        <trans-unit id="8df13c46adfe3a050db797a249e788f92b1e60e7" translate="yes" xml:space="preserve">
          <source>Wrapper around a CUDA event.</source>
          <target state="translated">CUDAイベントのラッパー。</target>
        </trans-unit>
        <trans-unit id="821f73849dcef0ef5418016b1e5d009bb01173fa" translate="yes" xml:space="preserve">
          <source>Wrapper around a CUDA stream.</source>
          <target state="translated">CUDAストリームのラッパー。</target>
        </trans-unit>
        <trans-unit id="65acb3d4bdbad8b7e153886b6b2199ad76a756c8" translate="yes" xml:space="preserve">
          <source>Wrapper class for quantized operations.</source>
          <target state="translated">量子化処理のラッパークラス.</target>
        </trans-unit>
        <trans-unit id="8e2dfc8a3e67da8b6e1ca8fafe3e8afa2b3f0620" translate="yes" xml:space="preserve">
          <source>Wrapper that allows creation of class factories.</source>
          <target state="translated">クラス工場の作成を可能にするラッパー。</target>
        </trans-unit>
        <trans-unit id="843c52018e11fda7799dc5989f496736df1f7d7b" translate="yes" xml:space="preserve">
          <source>Wraps another sampler to yield a mini-batch of indices.</source>
          <target state="translated">インデックスのミニバッチを得るために別のサンプラーをラップします。</target>
        </trans-unit>
        <trans-unit id="2a31ffa99fd6d35b029d1f439707a5bd7c9b4da8" translate="yes" xml:space="preserve">
          <source>Writes all values from the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor. For each value in &lt;code&gt;src&lt;/code&gt;, its output index is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">テンソルからすべての値を書き込み &lt;code&gt;src&lt;/code&gt; に &lt;code&gt;self&lt;/code&gt; に指定されたインデックスの &lt;code&gt;index&lt;/code&gt; テンソル。各値について &lt;code&gt;src&lt;/code&gt; 、その出力インデックスは、そのインデックスで指定された &lt;code&gt;src&lt;/code&gt; のための &lt;code&gt;dimension != dim&lt;/code&gt; との対応する値によって &lt;code&gt;index&lt;/code&gt; のための &lt;code&gt;dimension = dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5adf3c6baa6091132b20eef0ff93f0984499afa5" translate="yes" xml:space="preserve">
          <source>Writes entries directly to event files in the log_dir to be consumed by TensorBoard.</source>
          <target state="translated">TensorBoardが消費するlog_dirのイベントファイルに直接エントリを書き込みます。</target>
        </trans-unit>
        <trans-unit id="c032adc1ff629c9b66f22749ad667e6beadf144b" translate="yes" xml:space="preserve">
          <source>X</source>
          <target state="translated">X</target>
        </trans-unit>
        <trans-unit id="29822901b44f81c6464586704f28915142f932bc" translate="yes" xml:space="preserve">
          <source>X (Tensor): tensor of eigenvectors of size</source>
          <target state="translated">X(テンソル):サイズの固有ベクトルのテンソル</target>
        </trans-unit>
        <trans-unit id="030b21a9a5ef307806a06b78be434c1972173393" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = X^*[N_1 - \omega_1, \dots, N_d - \omega_d],</source>
          <target state="translated">X[\omega_1,♪♪dots,♪omega_d]=X^*[N_1-\omega_1,♪dots,N_d-\omega_d]。</target>
        </trans-unit>
        <trans-unit id="ed1d6f505827ea7067cb8c300c0904dbfa5fbd7a" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="translated">X[\omega_1,I\dots,I\omega_d]=&quot;N_i&quot;}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}</target>
        </trans-unit>
        <trans-unit id="746d94247ac643f86d1ee17dab642515e2ff9085" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a14f6ead4d7bc2b1f74caf9a0cc9b1f3d45fbb66" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = X[m, \text{n\_fft} - \omega]^*</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e6854a6a1b525f353a271b3c9b646d1b5c28d3f" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = \sum_{k = 0}^{\text{win\_length-1}}% \text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ % \exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14ebad2cda6dcca683250b526859348822d9d5c8" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. E.g.:</source>
          <target state="translated">はい、これはONNXopsetバージョン&amp;gt; = 11で現在サポートされています。例：</target>
        </trans-unit>
        <trans-unit id="5684cedf8d1212771db72a004af08859ca152372" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. ONNX introduced the concept of Sequence in opset 11. Similar to list, Sequence is a data type that contains arbitrary number of Tensors. Associated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc. However, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace add operator. E.g.:</source>
          <target state="translated">はい、これは現在ONNXopsetバージョン&amp;gt; = 11でサポートされています。ONNXはopset11にSequenceの概念を導入しました。リストと同様に、Sequenceは任意の数のテンソルを含むデータ型です。SequenceInsert、SequenceAtなどの関連演算子もONNXに導入されています。ただし、ループ内のインプレースリスト追加はONNXにエクスポートできません。これを実装するには、インプレース追加演算子を使用してください。例えば：</target>
        </trans-unit>
        <trans-unit id="738a2b66281e5ca4973cbceebc923d1996e03dad" translate="yes" xml:space="preserve">
          <source>Yields</source>
          <target state="translated">Yields</target>
        </trans-unit>
        <trans-unit id="7c19fb2e314a3137e93fb758f531465aacc3456b" translate="yes" xml:space="preserve">
          <source>You can also construct hybrid sparse tensors, where only the first n dimensions are sparse, and the rest of the dimensions are dense.</source>
          <target state="translated">また、最初のn次元だけが疎で、残りの次元は密であるハイブリッドスパーステンソルを構築することもできます。</target>
        </trans-unit>
        <trans-unit id="d617602dcc62272743d7a795be368e5ffbe5433d" translate="yes" xml:space="preserve">
          <source>You can also run the exported model with &lt;a href=&quot;https://github.com/microsoft/onnxruntime&quot;&gt;ONNX Runtime&lt;/a&gt;, you will need to install &lt;code&gt;ONNX Runtime&lt;/code&gt;: please &lt;a href=&quot;https://github.com/microsoft/onnxruntime#installation&quot;&gt;follow these instructions&lt;/a&gt;.</source>
          <target state="translated">あなたはまたしてエクスポートされたモデルで実行することができます&lt;a href=&quot;https://github.com/microsoft/onnxruntime&quot;&gt;ONNX Runtimeを&lt;/a&gt;インストールする必要があります、 &lt;code&gt;ONNX Runtime&lt;/code&gt; を：してください&lt;a href=&quot;https://github.com/microsoft/onnxruntime#installation&quot;&gt;これらの指示に従ってください&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="18be7e0f13bf0aca1a5c2348aa103f8bb566787e" translate="yes" xml:space="preserve">
          <source>You can also use cosine annealing to a fixed value instead of linear annealing by setting &lt;code&gt;anneal_strategy=&quot;cos&quot;&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;anneal_strategy=&quot;cos&quot;&lt;/code&gt; を設定することにより、線形アニーリングの代わりにコサインアニーリングを固定値に使用することもできます。</target>
        </trans-unit>
        <trans-unit id="c8c75fa187b181f202978d8a86ab28cca3e9eb36" translate="yes" xml:space="preserve">
          <source>You can also verify the protobuf using the &lt;a href=&quot;https://github.com/onnx/onnx/&quot;&gt;ONNX&lt;/a&gt; library. You can install &lt;code&gt;ONNX&lt;/code&gt; with conda:</source>
          <target state="translated">&lt;a href=&quot;https://github.com/onnx/onnx/&quot;&gt;ONNX&lt;/a&gt;ライブラリを使用してprotobufを確認することもできます。 &lt;code&gt;ONNX&lt;/code&gt; してONNXをインストールできます。</target>
        </trans-unit>
        <trans-unit id="ec539497ce517be7d2c9354eae2288ab7aed0f7d" translate="yes" xml:space="preserve">
          <source>You can construct a model with random weights by calling its constructor:</source>
          <target state="translated">コンストラクタを呼び出すことで、ランダムな重みを持つモデルを構築することができます。</target>
        </trans-unit>
        <trans-unit id="1400fef33661543c72061ceed7d3f9e9ebc487ea" translate="yes" xml:space="preserve">
          <source>You can create your own registry by creating a new &lt;a href=&quot;#torch.distributions.constraint_registry.ConstraintRegistry&quot;&gt;&lt;code&gt;ConstraintRegistry&lt;/code&gt;&lt;/a&gt; object.</source>
          <target state="translated">新しい&lt;a href=&quot;#torch.distributions.constraint_registry.ConstraintRegistry&quot;&gt; &lt;code&gt;ConstraintRegistry&lt;/code&gt; &lt;/a&gt;オブジェクトを作成することにより、独自のレジストリを作成できます。</target>
        </trans-unit>
        <trans-unit id="decdc18078675c119e9394b80151d2e753fa566c" translate="yes" xml:space="preserve">
          <source>You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn&amp;rsquo;t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.</source>
          <target state="translated">オプションをキーワード引数として渡すことはできます。それらは、それらをオーバーライドしなかったグループで、デフォルトとして使用されます。これは、パラメータグループ間で他のすべてのオプションの一貫性を保ちながら、1つのオプションのみを変更する場合に役立ちます。</target>
        </trans-unit>
        <trans-unit id="3431a2a1d98df776e7494a8e2a5ab89115af9a01" translate="yes" xml:space="preserve">
          <source>You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.</source>
          <target state="translated">引数としてテンソルとストレージの両方を使用することができます。与えられたオブジェクトがGPU上で割り当てられていない場合、これはノーオペです。</target>
        </trans-unit>
        <trans-unit id="cc736ac07ea8c62fb5b125af6be01955a2077643" translate="yes" xml:space="preserve">
          <source>You must either provide a value for total_steps or provide a value for both epochs and steps_per_epoch.</source>
          <target state="translated">total_stepsに値を指定するか、エポックとstep_per_epochの両方に値を指定する必要があります。</target>
        </trans-unit>
        <trans-unit id="762890e45c5b225da275ae73984ed756702605c8" translate="yes" xml:space="preserve">
          <source>You should never try to change your model&amp;rsquo;s parameters after wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;. Because, when wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;, the constructor of &lt;code&gt;DistributedDataParallel&lt;/code&gt; will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model&amp;rsquo;s parameters afterwards, gradient redunction functions no longer match the correct set of parameters.</source>
          <target state="translated">モデルを &lt;code&gt;DistributedDataParallel&lt;/code&gt; でラップした後は、モデルのパラメーターを変更しようとしないでください。モデル包むとき、ので &lt;code&gt;DistributedDataParallel&lt;/code&gt; を、コンストラクタの &lt;code&gt;DistributedDataParallel&lt;/code&gt; は、建設時のモデル自体のすべてのパラメータに追加の勾配の減少関数を登録します。後でモデルのパラメーターを変更すると、勾配縮小関数が正しいパラメーターのセットと一致しなくなります。</target>
        </trans-unit>
        <trans-unit id="ccd4aee0db1253438f0c3a9d3c74816b28340f08" translate="yes" xml:space="preserve">
          <source>You&amp;rsquo;ll generally want to use &lt;a href=&quot;torch.qr#torch.qr&quot;&gt;&lt;code&gt;torch.qr()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">通常、代わりに&lt;a href=&quot;torch.qr#torch.qr&quot;&gt; &lt;code&gt;torch.qr()&lt;/code&gt; &lt;/a&gt;を使用することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="f259809289921b4be91b7ff0e29bbdb10551660f" translate="yes" xml:space="preserve">
          <source>Your models should also subclass this class.</source>
          <target state="translated">モデルもこのクラスをサブクラス化する必要があります。</target>
        </trans-unit>
        <trans-unit id="93d450611dc79948223d0cdb9f4a99610848c9d6" translate="yes" xml:space="preserve">
          <source>ZeroPad2d</source>
          <target state="translated">ZeroPad2d</target>
        </trans-unit>
        <trans-unit id="90bc5acc0a3b091bfe56eb668e9c84ac53428130" translate="yes" xml:space="preserve">
          <source>[* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1] \times \ldots \times \text{normalized\_shape}[-1]]</source>
          <target state="translated">[* \ times \ text {normalized \ _shape} [0] \ times \ text {normalized \ _shape} [1] \ times \ ldots \ times \ text {normalized \ _shape} [-1]]</target>
        </trans-unit>
        <trans-unit id="b61210c8825bb88613c060ba48aae051333bd30a" translate="yes" xml:space="preserve">
          <source>[-1, 1]</source>
          <target state="translated">[-1、1]</target>
        </trans-unit>
        <trans-unit id="ae17aa1eaf46c89eecaf929c74d8fda9a55db49b" translate="yes" xml:space="preserve">
          <source>[0, 1)</source>
          <target state="translated">[0、1）</target>
        </trans-unit>
        <trans-unit id="c49d95c97e6b97b46f0fa87c4c3d5517b2dd2ac1" translate="yes" xml:space="preserve">
          <source>[0, C-1]</source>
          <target state="translated">[0、C-1]</target>
        </trans-unit>
        <trans-unit id="c00ab2adc7412d84a0750550969f7439fdbed02f" translate="yes" xml:space="preserve">
          <source>[0] a pretty-printed representation (as valid Python syntax) of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;code&gt;code&lt;/code&gt;. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant&amp;rsquo;s values.</source>
          <target state="translated">[0] &lt;code&gt;forward&lt;/code&gt; メソッドの内部グラフの（有効なPython構文としての）きれいに印刷された表現。 &lt;code&gt;code&lt;/code&gt; 参照してください。[1] [0]の出力のCONSTANT.cN形式に従ったConstMap。[0]出力のインデックスは、基になる定数の値へのキーです。</target>
        </trans-unit>
        <trans-unit id="bc47f02dcecd076fd210f14fb47e8772c796bc8d" translate="yes" xml:space="preserve">
          <source>[1] D. W. Griffin and J. S. Lim, &amp;ldquo;Signal estimation from modified short-time Fourier transform,&amp;rdquo; IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.</source>
          <target state="translated">[1] DWGriffinおよびJSLim、「修正された短時間フーリエ変換からの信号推定」、IEEETrans。ASSP、vol.32、no.2、pp.236-243、1984年4月。</target>
        </trans-unit>
        <trans-unit id="6682e58cfacdff6355a8741ff44261abb6d30565" translate="yes" xml:space="preserve">
          <source>[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)</source>
          <target state="translated">[1]具体的な分布：離散確率変数の連続緩和（Maddison et al、2017）</target>
        </trans-unit>
        <trans-unit id="1abd3a692f41e45de17752978fa303a578688034" translate="yes" xml:space="preserve">
          <source>[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. &lt;a href=&quot;https://arxiv.org/abs/1907.06845&quot;&gt;https://arxiv.org/abs/1907.06845&lt;/a&gt;</source>
          <target state="translated">[1]継続的なBernoulli：変分オートエンコーダーの広範なエラーの修正、Loaiza-GanemGおよびCunninghamJP、NeurIPS2019。https &lt;a href=&quot;https://arxiv.org/abs/1907.06845&quot;&gt;：&lt;/a&gt; //arxiv.org/abs/1907.06845</target>
        </trans-unit>
        <trans-unit id="9d064e85a047a7b9150d33ff887f20a63a320452" translate="yes" xml:space="preserve">
          <source>[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)</source>
          <target state="translated">[2] Gumbel-Softmaxによるカテゴリの再パラメータ化（Jang et al、2017）</target>
        </trans-unit>
        <trans-unit id="9390898997c65a41ca007de79555abfa5f0ba5df" translate="yes" xml:space="preserve">
          <source>[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A Robust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/17M1129830&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/17M1129830&lt;/a&gt;</source>
          <target state="translated">[DuerschEtal2018] Jed A. Duersch、Meiyue Shao、Chao Yang、Ming Gu （2018）LOBPCGの堅牢で効率的な実装。SIAM J.Sci。計算、40（5）、C655-C676。（22ページ）&lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/17M1129830&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/17M1129830&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f08253d232026d76249c2db42c2940e24ba3bacf" translate="yes" xml:space="preserve">
          <source>[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2), 517-541. (25 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&lt;/a&gt;</source>
          <target state="translated">[Knyazev2001] Andrew V.Knyazev。 （2001）最適な前処理付き固有ソルバーに向けて：局所的に最適なブロック前処理付き共役勾配法。 SIAM J.Sci。計算、23（2）、517-541。 （25ページ）&lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ff64b4c2d43518023be089f31430ef90034f605e" translate="yes" xml:space="preserve">
          <source>[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/S1064827500370883&quot;&gt;https://epubs.siam.org/doi/10.1137/S1064827500370883&lt;/a&gt;</source>
          <target state="translated">[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/S1064827500370883&quot;&gt;https://epubs.siam.org/doi/10.1137/S1064827500370883&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f1e9b6692d359e3ebb28163afd3f06ce34d6b2df" translate="yes" xml:space="preserve">
          <source>[[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(&amp;lsquo;A&amp;rsquo;, &amp;lsquo;B1&amp;rsquo;, &amp;lsquo;B2&amp;rsquo;))</source>
          <target state="translated">[[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(&amp;lsquo;A&amp;rsquo;, &amp;lsquo;B1&amp;rsquo;, &amp;lsquo;B2&amp;rsquo;))</target>
        </trans-unit>
        <trans-unit id="38ac8c676253501b3fb6819ff9373ce6cfe5b055" translate="yes" xml:space="preserve">
          <source>\ ^*</source>
          <target state="translated">\ ^*</target>
        </trans-unit>
        <trans-unit id="b9bb5838f4fb69b64d3191210035a54a11128eeb" translate="yes" xml:space="preserve">
          <source>\Delta\theta = \alpha r \frac{\partial\log p(a|\pi^\theta(s))}{\partial\theta}</source>
          <target state="translated">\Delta\theta = \alpha r \frac{\partial\log p(a|\pi^\theta(s))}{\partial\theta}</target>
        </trans-unit>
        <trans-unit id="80a1cbf9743b017c73b23f243284395f0c518670" translate="yes" xml:space="preserve">
          <source>\Gamma(\cdot)</source>
          <target state="translated">\Gamma(\cdot)</target>
        </trans-unit>
        <trans-unit id="15e33ea71862e7b6632cf90aebb8ab938c698f9d" translate="yes" xml:space="preserve">
          <source>\Phi(x)</source>
          <target state="translated">\Phi(x)</target>
        </trans-unit>
        <trans-unit id="2a6c118642891e41137cb68e1346d34dabbd128c" translate="yes" xml:space="preserve">
          <source>\Vert x \Vert _p = \left( \sum_{i=1}^n \vert x_i \vert ^ p \right) ^ {1/p}.</source>
          <target state="translated">\Vert x \Vert _p = \left( \sum_{i=1}^n \vert x_i \vert ^ p \right) ^ {1/p}.</target>
        </trans-unit>
        <trans-unit id="f7c665b45932a814215e979bc2611080b4948e68" translate="yes" xml:space="preserve">
          <source>\alpha</source>
          <target state="translated">\alpha</target>
        </trans-unit>
        <trans-unit id="1eafe1d2e67a6ccfd6c43fcc6a7aba05feb684c2" translate="yes" xml:space="preserve">
          <source>\alpha = 1.6732632423543772848170429916717</source>
          <target state="translated">\alpha = 1.6732632423543772848170429916717</target>
        </trans-unit>
        <trans-unit id="e9b40d7ba0060d884943279a8f77be052be116fb" translate="yes" xml:space="preserve">
          <source>\alpha/(\sqrt{v} + \epsilon)</source>
          <target state="translated">\alpha/(\sqrt{v} + \epsilon)</target>
        </trans-unit>
        <trans-unit id="cbe1c5b26860bdd639e58977c2072d9b65aece25" translate="yes" xml:space="preserve">
          <source>\alpha=1.6732632423543772848170429916717</source>
          <target state="translated">\alpha=1.6732632423543772848170429916717</target>
        </trans-unit>
        <trans-unit id="553ba8e7e5ec0531ddcc7d2c4cd048f424b3f52f" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \eta_t &amp;amp; = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right), &amp;amp; T_{cur} \neq (2k+1)T_{max}; \\ \eta_{t+1} &amp;amp; = \eta_{t} + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 - \cos\left(\frac{1}{T_{max}}\pi\right)\right), &amp;amp; T_{cur} = (2k+1)T_{max}. \end{aligned}</source>
          <target state="translated">\ begin {aligned} \ eta_t＆= \ eta_ {min} + \ frac {1} {2}（\ eta_ {max}-\ eta_ {min}）\ left（1 + \ cos \ left（\ frac {T_ {cur}} {T_ {max}} \ pi \ right）\ right）、＆T_ {cur} \ neq（2k + 1）T_ {max}; \\ \ eta_ {t + 1}＆= \ eta_ {t} + \ frac {1} {2}（\ eta_ {max}-\ eta_ {min}）\ left（1- \ cos \ left（\ frac {1} {T_ {max}} \ pi \ right）\ right）、＆T_ {cur} =（2k + 1）T_ {max}。\ end {aligned}</target>
        </trans-unit>
        <trans-unit id="a6518676c0b93d25520b42e7f84e6cb06eb90098" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{if Symmetric:}&amp;amp;\\ &amp;amp;s = 2 \max(|x_\text{min}|, x_\text{max}) / \left( Q_\text{max} - Q_\text{min} \right) \\ &amp;amp;z = \begin{cases} 0 &amp;amp; \text{if dtype is qint8} \\ 128 &amp;amp; \text{otherwise} \end{cases}\\ \text{Otherwise:}&amp;amp;\\ &amp;amp;s = \left( x_\text{max} - x_\text{min} \right ) / \left( Q_\text{max} - Q_\text{min} \right ) \\ &amp;amp;z = Q_\text{min} - \text{round}(x_\text{min} / s) \end{aligned}</source>
          <target state="translated">\ begin {aligned} \ text {if Symmetric：}＆\\＆s = 2 \ max（| x_ \ text {min} |、x_ \ text {max}）/ \ left（Q_ \ text {max} -Q_ \ text {min} \ right）\\＆z = \ begin {cases} 0＆\ text {if dtype is qint8} \\ 128＆\ text {otherwise} \ end {cases} \\ \ text {Otherwise：}＆\ \＆s = \ left（x_ \ text {max} -x_ \ text {min} \ right）/ \ left（Q_ \ text {max} -Q_ \ text {min} \ right）\\＆z = Q_ \ text { min}-\ text {round}（x_ \ text {min} / s）\ end {aligned}</target>
        </trans-unit>
        <trans-unit id="eef3f6c4a900c50c6ccae139e6900e18ddc9222e" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k, \text{stride[1]} \times h + m, \text{stride[2]} \times w + n) \end{aligned}</source>
          <target state="translated">\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k, \text{stride[1]} \times h + m, \text{stride[2]} \times w + n) \end{aligned}</target>
        </trans-unit>
        <trans-unit id="3444423e4aa6a27dc2d4e17fde867139b4f23a7b" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\ &amp;amp; \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k, \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)} {kD \times kH \times kW} \end{aligned}</source>
          <target state="translated">\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\ &amp;amp; \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k, \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)} {kD \times kH \times kW} \end{aligned}</target>
        </trans-unit>
        <trans-unit id="9b5fbddfbbd75f7006abb89ad6964ec0f13538c7" translate="yes" xml:space="preserve">
          <source>\begin{aligned} out(N_i, C_j, h, w) ={} &amp;amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m, \text{stride[1]} \times w + n) \end{aligned}</source>
          <target state="translated">\begin{aligned} out(N_i, C_j, h, w) ={} &amp;amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m, \text{stride[1]} \times w + n) \end{aligned}</target>
        </trans-unit>
        <trans-unit id="68934623d4a064db7240ac0ed4e63262bd5a88d3" translate="yes" xml:space="preserve">
          <source>\begin{aligned} v_{t+1} &amp;amp; = \mu * v_{t} + \text{lr} * g_{t+1}, \\ p_{t+1} &amp;amp; = p_{t} - v_{t+1}. \end{aligned}</source>
          <target state="translated">\ begin {aligned} v_ {t + 1}＆= \ mu * v_ {t} + \ text {lr} * g_ {t + 1}、\\ p_ {t + 1}＆= p_ {t} -v_ {t +1}。\ end {aligned}</target>
        </trans-unit>
        <trans-unit id="e5d65e65e32d2c2c154e2f6af937b11f165a63e3" translate="yes" xml:space="preserve">
          <source>\begin{aligned} v_{t+1} &amp;amp; = \mu * v_{t} + g_{t+1}, \\ p_{t+1} &amp;amp; = p_{t} - \text{lr} * v_{t+1}, \end{aligned}</source>
          <target state="translated">\ begin {aligned} v_ {t + 1}＆= \ mu * v_ {t} + g_ {t + 1}、\\ p_ {t + 1}＆= p_ {t}-\ text {lr} * v_ {t + 1}、\ end {aligned}</target>
        </trans-unit>
        <trans-unit id="4ea19e5cef880f790f54322d49c339a67ef7eba6" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \\ i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\ f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\ g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\ o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\ c_t = f_t \odot c_{t-1} + i_t \odot g_t \\ h_t = o_t \odot \tanh(c_t) \\ \end{array}</source>
          <target state="translated">\begin{array}{ll} \\ i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\ f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\ g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\ o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\ c_t = f_t \odot c_{t-1} + i_t \odot g_t \\ h_t = o_t \odot \tanh(c_t) \\ \end{array}</target>
        </trans-unit>
        <trans-unit id="6b6cb34a158b8201fbf48f50465b1ece00a7eb5f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|AX-B\|_2. \end{array}</source>
          <target state="translated">\begin{array}{ll} \min_X &amp;amp; \|AX-B\|_2. \end{array}</target>
        </trans-unit>
        <trans-unit id="9312cff9d66b9a6753766d98374f8fbc6cf98e2f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|X\|_2 &amp;amp; \text{subject to} &amp;amp; AX = B. \end{array}</source>
          <target state="translated">\begin{array}{ll} \min_X &amp;amp; \|X\|_2 &amp;amp; \text{subject to} &amp;amp; AX = B. \end{array}</target>
        </trans-unit>
        <trans-unit id="930f69e97734f3784bc6b17a4919f47bb6fa5660" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\ f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\ g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\ o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\ c' = f * c + i * g \\ h' = o * \tanh(c') \\ \end{array}</source>
          <target state="translated">\begin{array}{ll} i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\ f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\ g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\ o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\ c' = f * c + i * g \\ h' = o * \tanh(c') \\ \end{array}</target>
        </trans-unit>
        <trans-unit id="e6bea3b60c648ba0107ad5a273553cd4d585a9b3" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\ z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\ n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\ h' = (1 - z) * n + z * h \end{array}</source>
          <target state="translated">\begin{array}{ll} r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\ z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\ n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\ h' = (1 - z) * n + z * h \end{array}</target>
        </trans-unit>
        <trans-unit id="e41fd097ab91cc853c05e5d0d9e0d37ff3dd7662" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\ z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\ n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \end{array}</source>
          <target state="translated">\begin{array}{ll} r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\ z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\ n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \end{array}</target>
        </trans-unit>
        <trans-unit id="083f0eacd92d5be23a46e2af20aa291d89fff082" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} x_\text{min} &amp;amp;= \begin{cases} \min(X) &amp;amp; \text{if~}x_\text{min} = \text{None} \\ \min\left(x_\text{min}, \min(X)\right) &amp;amp; \text{otherwise} \end{cases}\\ x_\text{max} &amp;amp;= \begin{cases} \max(X) &amp;amp; \text{if~}x_\text{max} = \text{None} \\ \max\left(x_\text{max}, \max(X)\right) &amp;amp; \text{otherwise} \end{cases}\\ \end{array}</source>
          <target state="translated">\ begin {array} {ll} x_ \ text {min}＆= \ begin {cases} \ min（X）＆\ text {if〜} x_ \ text {min} = \ text {None} \\ \ min \ left（x_ \ text {min}、\ min（X）\ right）＆\ text {otherwise} \ end {cases} \\ x_ \ text {max}＆= \ begin {cases} \ max（X）＆\ text {if〜} x_ \ text {max} = \ text {None} \\ \ max \ left（x_ \ text {max}、\ max（X）\ right）＆\ text {otherwise} \ end {cases} \\ \ end {array}</target>
        </trans-unit>
        <trans-unit id="afd260a0e4c76fcb26f5d6cc69c2624609e9b929" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} x_\text{min} = \begin{cases} \min(X) &amp;amp; \text{if~}x_\text{min} = \text{None} \\ (1 - c) x_\text{min} + c \min(X) &amp;amp; \text{otherwise} \end{cases}\\ x_\text{max} = \begin{cases} \max(X) &amp;amp; \text{if~}x_\text{max} = \text{None} \\ (1 - c) x_\text{max} + c \max(X) &amp;amp; \text{otherwise} \end{cases}\\ \end{array}</source>
          <target state="translated">\ begin {array} {ll} x_ \ text {min} = \ begin {cases} \ min（X）＆\ text {if〜} x_ \ text {min} = \ text {None} \\（1-c ）x_ \ text {min} + c \ min（X）＆\ text {otherwise} \ end {cases} \\ x_ \ text {max} = \ begin {cases} \ max（X）＆\ text {if〜 } x_ \ text {max} = \ text {None} \\（1-c）x_ \ text {max} + c \ max（X）＆\ text {otherwise} \ end {cases} \\ \ end {array }</target>
        </trans-unit>
        <trans-unit id="6499d503bfc00cadae1440b191c52a8632e2f8c4" translate="yes" xml:space="preserve">
          <source>\beta</source>
          <target state="translated">\beta</target>
        </trans-unit>
        <trans-unit id="6ceee6706c4f97ef457cd65c6ed19732a4a4c7e0" translate="yes" xml:space="preserve">
          <source>\delta^{(l-1)}_t</source>
          <target state="translated">\delta^{(l-1)}_t</target>
        </trans-unit>
        <trans-unit id="986673ab936df8ce1454d7dc93297bd7bbc51c4b" translate="yes" xml:space="preserve">
          <source>\ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</source>
          <target state="translated">\ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</target>
        </trans-unit>
        <trans-unit id="4d38ace96b9720e15bef838db0ce85ef5b456092" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n) + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n) + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</target>
        </trans-unit>
        <trans-unit id="ec2ad56a41af8d7cfc9a26f61f048cf6cbd7e78d" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</target>
        </trans-unit>
        <trans-unit id="de7b2379237fe388c3459990f0a508ab1bd86588" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</target>
        </trans-unit>
        <trans-unit id="f6a58890a57b18854cfa1707f659a4fd6ff6e4cb" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left( x_n - y_n \right)^2,</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left( x_n - y_n \right)^2,</target>
        </trans-unit>
        <trans-unit id="bbe073a5e1335cf9e8392124ee3bc939a82d8704" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right|,</source>
          <target state="translated">\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right|,</target>
        </trans-unit>
        <trans-unit id="295ee7beff5d61b430ac876642e47ec1ac559795" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';} \\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';} \\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</target>
        </trans-unit>
        <trans-unit id="4d5ca23779a198f768fff42eaa5060e03279277c" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</target>
        </trans-unit>
        <trans-unit id="3c1589dd2fb19090831cbb7f520baedd8d65a3e4" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{`mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{`sum'.} \end{cases}</source>
          <target state="translated">\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{`mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{`sum'.} \end{cases}</target>
        </trans-unit>
        <trans-unit id="8ba1942b6f9513bd39cc8cb1f1c52037ed6d49d3" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;amp; \text{if reduction} = \text{'mean';}\\ \sum_{n=1}^N l_n, &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;amp; \text{if reduction} = \text{'mean';}\\ \sum_{n=1}^N l_n, &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</target>
        </trans-unit>
        <trans-unit id="5a3a211a3bdfba5d642037c3946f715f2ce73187" translate="yes" xml:space="preserve">
          <source>\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c}) + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</source>
          <target state="translated">\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c}) + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</target>
        </trans-unit>
        <trans-unit id="38c29fec2dd06380b3c75f2ad0de63a4841510db" translate="yes" xml:space="preserve">
          <source>\eta_t = \eta_{min}</source>
          <target state="translated">\eta_t = \eta_{min}</target>
        </trans-unit>
        <trans-unit id="669aeacd1d773163fae893a8ca1bc6fee6ec98cc" translate="yes" xml:space="preserve">
          <source>\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)</source>
          <target state="translated">\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)</target>
        </trans-unit>
        <trans-unit id="4c0a99b42d5ab2de0c04cb285ece308fea6df103" translate="yes" xml:space="preserve">
          <source>\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)</source>
          <target state="translated">\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)</target>
        </trans-unit>
        <trans-unit id="ce41fe8b1089f12f8148fc05ab4748c0e9ba7592" translate="yes" xml:space="preserve">
          <source>\eta_t=\eta_{max}</source>
          <target state="translated">\eta_t=\eta_{max}</target>
        </trans-unit>
        <trans-unit id="2e3b3cc9e60fd68fa21c6331d4f52b4c13f61075" translate="yes" xml:space="preserve">
          <source>\eta_{max}</source>
          <target state="translated">\eta_{max}</target>
        </trans-unit>
        <trans-unit id="c04fbcdd9308d49d4c2300f2ebbb1e83c44e8b83" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target} * \text{input}</source>
          <target state="translated">\exp(\text{input}) - \text{target} * \text{input}</target>
        </trans-unit>
        <trans-unit id="cd47433a58e73b2601de69c1a9744b288212fb5f" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target}*\text{input}</source>
          <target state="translated">\exp(\text{input}) - \text{target}*\text{input}</target>
        </trans-unit>
        <trans-unit id="fe43eb66c8bdd27010ca6db1c98281c7d4c4731f" translate="yes" xml:space="preserve">
          <source>\exp^A = \sum_{k=0}^\infty A^k / k!.</source>
          <target state="translated">\exp^A = \sum_{k=0}^\infty A^k / k!.</target>
        </trans-unit>
        <trans-unit id="14f6d7074093dd4d9ef81de502fd654605e4bb67" translate="yes" xml:space="preserve">
          <source>\forall i = d, \dots, d+k-1</source>
          <target state="translated">\forall i = d, \dots, d+k-1</target>
        </trans-unit>
        <trans-unit id="6aed51b4bd21bf9aaad2932aa936dab4bdd62611" translate="yes" xml:space="preserve">
          <source>\frac{1}{1-p}</source>
          <target state="translated">\frac{1}{1-p}</target>
        </trans-unit>
        <trans-unit id="037205eaa442691656469a316bf2dff320c2f572" translate="yes" xml:space="preserve">
          <source>\frac{1}{2} N (N - 1)</source>
          <target state="translated">\frac{1}{2} N (N - 1)</target>
        </trans-unit>
        <trans-unit id="64c94d13eeb330b494061e86538db66574ad0f7d" translate="yes" xml:space="preserve">
          <source>\frac{1}{3}</source>
          <target state="translated">\frac{1}{3}</target>
        </trans-unit>
        <trans-unit id="5947a169159fe867f85f3fd8b9690019b48152f5" translate="yes" xml:space="preserve">
          <source>\frac{1}{8}</source>
          <target state="translated">\frac{1}{8}</target>
        </trans-unit>
        <trans-unit id="f81c864ea46e4c42b16663b744993f9011be95f2" translate="yes" xml:space="preserve">
          <source>\frac{300}{100}=3</source>
          <target state="translated">\frac{300}{100}=3</target>
        </trans-unit>
        <trans-unit id="436bcf2181eea8fe79cdb015a5567ca1b8d69652" translate="yes" xml:space="preserve">
          <source>\frac{5}{3}</source>
          <target state="translated">\frac{5}{3}</target>
        </trans-unit>
        <trans-unit id="a9c2bfb5b8138830fd93a3a13faef54bc4dc94a2" translate="yes" xml:space="preserve">
          <source>\frac{m}{2} \leq</source>
          <target state="translated">\frac{m}{2} \leq</target>
        </trans-unit>
        <trans-unit id="b8fb95020958e9f0b58822f35b35fb490054c02e" translate="yes" xml:space="preserve">
          <source>\frac{p - 1}{2}</source>
          <target state="translated">\frac{p - 1}{2}</target>
        </trans-unit>
        <trans-unit id="67833ee2012ec1c6254b6c009dc72bf0dc48aa6d" translate="yes" xml:space="preserve">
          <source>\gamma</source>
          <target state="translated">\gamma</target>
        </trans-unit>
        <trans-unit id="27634ea2c473bc36e59149a7f0c457ffb292325a" translate="yes" xml:space="preserve">
          <source>\hat{x}</source>
          <target state="translated">\hat{x}</target>
        </trans-unit>
        <trans-unit id="b9e47e1f86f68634e0d0a997d4ea3952fae1e892" translate="yes" xml:space="preserve">
          <source>\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t</source>
          <target state="translated">\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t</target>
        </trans-unit>
        <trans-unit id="770b7843ffee64a984041915fe8461fd98a76060" translate="yes" xml:space="preserve">
          <source>\in [0, \infty]</source>
          <target state="translated">\in [0, \infty]</target>
        </trans-unit>
        <trans-unit id="32c336e712ce8251fe81037413821c2c348ab43d" translate="yes" xml:space="preserve">
          <source>\inf</source>
          <target state="translated">\inf</target>
        </trans-unit>
        <trans-unit id="9b97f26fbeb1b84327c736de515f10980d9c7d68" translate="yes" xml:space="preserve">
          <source>\infty</source>
          <target state="translated">\infty</target>
        </trans-unit>
        <trans-unit id="b237071f96360004cf37b06340000864b59f54c3" translate="yes" xml:space="preserve">
          <source>\int y\,dx</source>
          <target state="translated">\int y\,dx</target>
        </trans-unit>
        <trans-unit id="a10251c74fceb1b1b9e9c45471b613f216beb4a9" translate="yes" xml:space="preserve">
          <source>\int_a^b f = -\int_b^a f</source>
          <target state="translated">\int_a^b f = -\int_b^a f</target>
        </trans-unit>
        <trans-unit id="b3931f1ce298c536432fd324b3a1ab4337120689" translate="yes" xml:space="preserve">
          <source>\lambda</source>
          <target state="translated">\lambda</target>
        </trans-unit>
        <trans-unit id="1b47c3b18de49455a713efe91ac03ec27aad59a5" translate="yes" xml:space="preserve">
          <source>\lbrace (i, i) \rbrace</source>
          <target state="translated">\lbrace (i, i) \rbrace</target>
        </trans-unit>
        <trans-unit id="0fa91d11567564172cce99f01e48d1e73a28bc31" translate="yes" xml:space="preserve">
          <source>\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]</source>
          <target state="translated">\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]</target>
        </trans-unit>
        <trans-unit id="89ca5f286a6835c142dd1390cd67d2698dca7bcb" translate="yes" xml:space="preserve">
          <source>\left[\text{-clip\_value}, \text{clip\_value}\right]</source>
          <target state="translated">\left[\text{-clip\_value}, \text{clip\_value}\right]</target>
        </trans-unit>
        <trans-unit id="6ceb7ce51c6d9da5e34a800614788f55dff712c0" translate="yes" xml:space="preserve">
          <source>\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</source>
          <target state="translated">\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</target>
        </trans-unit>
        <trans-unit id="a74c86baea5a7bb180947e759ccafb91eee50e79" translate="yes" xml:space="preserve">
          <source>\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</source>
          <target state="translated">\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</target>
        </trans-unit>
        <trans-unit id="ed56196dc362da2ecbc9244bc832a7311739bb3d" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="de0088faf4547f5a22dd4cd77b23209a7dd8113e" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="88061aa72baf92fe1d129c82b73d16c37a6af8f0" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="f0a9328764eddcc1a62bf8fbb7c249505ed2c539" translate="yes" xml:space="preserve">
          <source>\leq</source>
          <target state="translated">\leq</target>
        </trans-unit>
        <trans-unit id="49abff7af14753dc3cdab27a1bf7f8ed5f2c2fa1" translate="yes" xml:space="preserve">
          <source>\leq 256</source>
          <target state="translated">\leq 256</target>
        </trans-unit>
        <trans-unit id="a719b606b8fa813d6dc2397930551b66016965ba" translate="yes" xml:space="preserve">
          <source>\leq S</source>
          <target state="translated">\leq S</target>
        </trans-unit>
        <trans-unit id="f7d8821758716417ac6285ab84b0661734c3439c" translate="yes" xml:space="preserve">
          <source>\leq T</source>
          <target state="translated">\leq T</target>
        </trans-unit>
        <trans-unit id="9ca5d36772ef139985921c4d545788d48fddcf0c" translate="yes" xml:space="preserve">
          <source>\lfloor \frac{N_d}{2} \rfloor + 1</source>
          <target state="translated">\lfloor \frac{N_d}{2} \rfloor + 1</target>
        </trans-unit>
        <trans-unit id="bc0c94864255e5978415bf4d290dee47e2677194" translate="yes" xml:space="preserve">
          <source>\lfloor\frac{\text{input planes}}{sT}\rfloor</source>
          <target state="translated">\lfloor\frac{\text{input planes}}{sT}\rfloor</target>
        </trans-unit>
        <trans-unit id="9878e94e87e5bd7098242aa36f29f409bc60ed2b" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty</source>
          <target state="translated">\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty</target>
        </trans-unit>
        <trans-unit id="26c67f72ffeaf83ec8c07ff1c00c57b7b4ce05e6" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \log (x) = -\infty</source>
          <target state="translated">\lim_{x\to 0} \log (x) = -\infty</target>
        </trans-unit>
        <trans-unit id="d92b1f0baf6824880fb434c0077a618ba265ea33" translate="yes" xml:space="preserve">
          <source>\log (0) = -\infty</source>
          <target state="translated">\log (0) = -\infty</target>
        </trans-unit>
        <trans-unit id="99e33ed0cf12197cde63048ced916ea73cbc9c3e" translate="yes" xml:space="preserve">
          <source>\log(0)</source>
          <target state="translated">\log(0)</target>
        </trans-unit>
        <trans-unit id="9f7859c28e08a9c15994c8896e613dd27080b6a5" translate="yes" xml:space="preserve">
          <source>\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)</source>
          <target state="translated">\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)</target>
        </trans-unit>
        <trans-unit id="c1e3a3daacbe46fc4916de547be4049f250f3288" translate="yes" xml:space="preserve">
          <source>\log(\text{Softmax}(x))</source>
          <target state="translated">\log(\text{Softmax}(x))</target>
        </trans-unit>
        <trans-unit id="322a0b1861bac0e7bc727021b82a0313c2a2b303" translate="yes" xml:space="preserve">
          <source>\log\left(e^x + e^y\right)</source>
          <target state="translated">\log\left(e^x + e^y\right)</target>
        </trans-unit>
        <trans-unit id="8783c75c38eec2ddd68cac88f7dc5ac00e806274" translate="yes" xml:space="preserve">
          <source>\log_2\left(2^x + 2^y\right)</source>
          <target state="translated">\log_2\left(2^x + 2^y\right)</target>
        </trans-unit>
        <trans-unit id="8935b97a0f600209c234abf4d224c5fe967c9fa1" translate="yes" xml:space="preserve">
          <source>\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert</source>
          <target state="translated">\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert</target>
        </trans-unit>
        <trans-unit id="52c49baa710d814dcf054568b203f1298d137cf7" translate="yes" xml:space="preserve">
          <source>\mathbf{L}</source>
          <target state="translated">\mathbf{L}</target>
        </trans-unit>
        <trans-unit id="eafd018e3a8aa85682e4f0b33612ce6efed642ca" translate="yes" xml:space="preserve">
          <source>\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})}, \sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}</source>
          <target state="translated">\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})}, \sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}</target>
        </trans-unit>
        <trans-unit id="0b4343943b1e977c3f21f3ea5b47966a6b6ad676" translate="yes" xml:space="preserve">
          <source>\mathbf{\Sigma}</source>
          <target state="translated">\mathbf{\Sigma}</target>
        </trans-unit>
        <trans-unit id="250361cfd8c9755de3dafc641e9f127bd071fcd2" translate="yes" xml:space="preserve">
          <source>\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top</source>
          <target state="translated">\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top</target>
        </trans-unit>
        <trans-unit id="4e7e4ed1e422b1e8789d9ac447a022c6ca481196" translate="yes" xml:space="preserve">
          <source>\mathbf{\Sigma}^{-1}</source>
          <target state="translated">\mathbf{\Sigma}^{-1}</target>
        </trans-unit>
        <trans-unit id="201a130976ca261c5bc7ef67511ced9f15d7653a" translate="yes" xml:space="preserve">
          <source>\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}</source>
          <target state="translated">\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}</target>
        </trans-unit>
        <trans-unit id="df511dffcdad49a67cd5945ee0ab7aef8bb4f9fc" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 0.01)</source>
          <target state="translated">\mathcal{N}(0, 0.01)</target>
        </trans-unit>
        <trans-unit id="8f3ba18912099017c093331676f6188e744efdac" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 1)</source>
          <target state="translated">\mathcal{N}(0, 1)</target>
        </trans-unit>
        <trans-unit id="50c7a47dd01b89919d5422d0295475337aab6ace" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, \text{std}^2)</source>
          <target state="translated">\mathcal{N}(0, \text{std}^2)</target>
        </trans-unit>
        <trans-unit id="92c992916044275832483fbb98179a2d00ca4c1e" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(\text{mean}, \text{std}^2)</source>
          <target state="translated">\mathcal{N}(\text{mean}, \text{std}^2)</target>
        </trans-unit>
        <trans-unit id="87b1ef0bb4d4a0924cc95ef538f83b2bb3e53e42" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\sqrt{k}, \sqrt{k})</source>
          <target state="translated">\mathcal{U}(-\sqrt{k}, \sqrt{k})</target>
        </trans-unit>
        <trans-unit id="d7f6ed3e182dc030701ad2deb57bf7caa5b88d6f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\text{bound}, \text{bound})</source>
          <target state="translated">\mathcal{U}(-\text{bound}, \text{bound})</target>
        </trans-unit>
        <trans-unit id="325a9ed78efc110eaab4160ef2c97f9ac713df84" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-a, a)</source>
          <target state="translated">\mathcal{U}(-a, a)</target>
        </trans-unit>
        <trans-unit id="b28ca9812620dfef2c686761b7aa8334acb0312f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(0, 1)</source>
          <target state="translated">\mathcal{U}(0, 1)</target>
        </trans-unit>
        <trans-unit id="b9712b8b025515dc1dea5f46b97d3c4fe634d954" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(\text{lower}, \text{upper})</source>
          <target state="translated">\mathcal{U}(\text{lower}, \text{upper})</target>
        </trans-unit>
        <trans-unit id="e216b98083013f9a980a8176c0057ec95ff5e691" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(a, b)</source>
          <target state="translated">\mathcal{U}(a, b)</target>
        </trans-unit>
        <trans-unit id="2df7cbd2cee6b04f561cf080750aba56226af4cb" translate="yes" xml:space="preserve">
          <source>\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="translated">\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</target>
        </trans-unit>
        <trans-unit id="20a69b6b57674088939e407b3b5098441f752171" translate="yes" xml:space="preserve">
          <source>\mathrm{erfinv}(\mathrm{erf}(x)) = x</source>
          <target state="translated">\mathrm{erfinv}(\mathrm{erf}(x)) = x</target>
        </trans-unit>
        <trans-unit id="56d825970c3fb1fe7543e6c61686e2830fac7d9f" translate="yes" xml:space="preserve">
          <source>\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="translated">\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</target>
        </trans-unit>
        <trans-unit id="5cc0da2aee8ceedc2aa92a9553e40d60b942829b" translate="yes" xml:space="preserve">
          <source>\mathrm{rate}^k \frac{e^{-\mathrm{rate}}}{k!}</source>
          <target state="translated">\mathrm{rate}^k \frac{e^{-\mathrm{rate}}}{k!}</target>
        </trans-unit>
        <trans-unit id="3e9c0d2c84e11d8338d1dac942d5dede72bd1acf" translate="yes" xml:space="preserve">
          <source>\min(input.size(-1), input.size(-2))</source>
          <target state="translated">\min(input.size(-1), input.size(-2))</target>
        </trans-unit>
        <trans-unit id="3a4e56595df1d02f21e5c3ff7b0e9d80921858ff" translate="yes" xml:space="preserve">
          <source>\mu</source>
          <target state="translated">\mu</target>
        </trans-unit>
        <trans-unit id="c8e2d1a0bf50a27d43ade30cfb048d99feb31ad1" translate="yes" xml:space="preserve">
          <source>\odot</source>
          <target state="translated">\odot</target>
        </trans-unit>
        <trans-unit id="73b077a63e22815fe5c8ee82dab9894be842b19c" translate="yes" xml:space="preserve">
          <source>\omega</source>
          <target state="translated">\omega</target>
        </trans-unit>
        <trans-unit id="72166555a6db55785fc6fbe9a7c5bbe72be28db8" translate="yes" xml:space="preserve">
          <source>\otimes</source>
          <target state="translated">\otimes</target>
        </trans-unit>
        <trans-unit id="bd84e490efa5f32ec0f75c3ea25a6efc19cfaa88" translate="yes" xml:space="preserve">
          <source>\pi^\theta</source>
          <target state="translated">\pi^\theta</target>
        </trans-unit>
        <trans-unit id="6dc2ada78a76bad95d3b921558b1195549985eab" translate="yes" xml:space="preserve">
          <source>\prod(\text{kernel\_size})</source>
          <target state="translated">\prod(\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="02cea321dbc3a3edfc9cc1780dfe84f694c585f0" translate="yes" xml:space="preserve">
          <source>\psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}</source>
          <target state="translated">\psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}</target>
        </trans-unit>
        <trans-unit id="b30c154a70c78f1c5dcecce58dde2e9ff4ec56aa" translate="yes" xml:space="preserve">
          <source>\psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)</source>
          <target state="translated">\psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)</target>
        </trans-unit>
        <trans-unit id="69c15416b63fd933850978302bcda59da879774e" translate="yes" xml:space="preserve">
          <source>\sigma</source>
          <target state="translated">\sigma</target>
        </trans-unit>
        <trans-unit id="bfe16f27ebc966df6f10ba356a1547b6e7242dd7" translate="yes" xml:space="preserve">
          <source>\sqrt{2}</source>
          <target state="translated">\sqrt{2}</target>
        </trans-unit>
        <trans-unit id="358161536f25000be6a773604fcf4a28afd7b7bd" translate="yes" xml:space="preserve">
          <source>\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}</source>
          <target state="translated">\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}</target>
        </trans-unit>
        <trans-unit id="64126f7d8d3d661d4e29a191268f98bf759903a3" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^K N_i}</source>
          <target state="translated">\sqrt{\prod_{i=1}^K N_i}</target>
        </trans-unit>
        <trans-unit id="976e9a0789eb95323325b92e6edc3b432c6754b8" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^d N_i}</source>
          <target state="translated">\sqrt{\prod_{i=1}^d N_i}</target>
        </trans-unit>
        <trans-unit id="9312c2748ccad7c25f8d92bc855a5a0b38989a51" translate="yes" xml:space="preserve">
          <source>\star</source>
          <target state="translated">\star</target>
        </trans-unit>
        <trans-unit id="3df82d7a797b96797b79d72f235bc018cb8155c9" translate="yes" xml:space="preserve">
          <source>\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0</source>
          <target state="translated">\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0</target>
        </trans-unit>
        <trans-unit id="a12be8b8923c0e92cce3f35968e39960bc665833" translate="yes" xml:space="preserve">
          <source>\tanh</source>
          <target state="translated">\tanh</target>
        </trans-unit>
        <trans-unit id="053658991aeb9a94a57c16cbe979538b3eca3b46" translate="yes" xml:space="preserve">
          <source>\texttt{n\_classes}</source>
          <target state="translated">\texttt{n\_classes}</target>
        </trans-unit>
        <trans-unit id="79df4825cae48291f72d91cc9840f2adcb2716c4" translate="yes" xml:space="preserve">
          <source>\texttt{result[i]}</source>
          <target state="translated">\texttt{result[i]}</target>
        </trans-unit>
        <trans-unit id="8a01c6904694a6040cbbe4d31b35e67c5fb8c5bc" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p\_tensor[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p\_tensor[i]})</target>
        </trans-unit>
        <trans-unit id="248f05fb27cede89d993bc3bb9750c4fde2467c2" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p})</target>
        </trans-unit>
        <trans-unit id="15ca5fa99d2411cc7a336f5df78382fd090360c9" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{self[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{self[i]})</target>
        </trans-unit>
        <trans-unit id="f4308a3c6285dab040bdf907914e0906e5918d43" translate="yes" xml:space="preserve">
          <source>\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</source>
          <target state="translated">\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</target>
        </trans-unit>
        <trans-unit id="1c5fbaf107a1bd80dcb32b25718623eef6cabd9f" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; 0\\ \alpha * (\exp(x) - 1), &amp;amp; \text{ if } x \leq 0 \end{cases}</source>
          <target state="translated">\text{ELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; 0\\ \alpha * (\exp(x) - 1), &amp;amp; \text{ if } x \leq 0 \end{cases}</target>
        </trans-unit>
        <trans-unit id="ee2c000c326328286929b5abe78d5f28ecc355ca" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))</source>
          <target state="translated">\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))</target>
        </trans-unit>
        <trans-unit id="1ae2cb648d379e90a17c82c0bca8393180166a13" translate="yes" xml:space="preserve">
          <source>\text{GELU}(x) = x * \Phi(x)</source>
          <target state="translated">\text{GELU}(x) = x * \Phi(x)</target>
        </trans-unit>
        <trans-unit id="98d15f980c451db5bf1f8450a31ea12ac80b261b" translate="yes" xml:space="preserve">
          <source>\text{GLU}(a, b) = a \otimes \sigma(b)</source>
          <target state="translated">\text{GLU}(a, b) = a \otimes \sigma(b)</target>
        </trans-unit>
        <trans-unit id="2b0fd43cdc620c274316d22c100c5a1cedf8758f" translate="yes" xml:space="preserve">
          <source>\text{HardShrink}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\text{HardShrink}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</target>
        </trans-unit>
        <trans-unit id="905f89b6d5fe72acb1c8befdbd1fbd394ccc9c79" translate="yes" xml:space="preserve">
          <source>\text{HardTanh}(x) = \begin{cases} 1 &amp;amp; \text{ if } x &amp;gt; 1 \\ -1 &amp;amp; \text{ if } x &amp;lt; -1 \\ x &amp;amp; \text{ otherwise } \\ \end{cases}</source>
          <target state="translated">\text{HardTanh}(x) = \begin{cases} 1 &amp;amp; \text{ if } x &amp;gt; 1 \\ -1 &amp;amp; \text{ if } x &amp;lt; -1 \\ x &amp;amp; \text{ otherwise } \\ \end{cases}</target>
        </trans-unit>
        <trans-unit id="c0b2885acfe7c1d78f13cbb375808a8a70622e9c" translate="yes" xml:space="preserve">
          <source>\text{Hardsigmoid}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ 1 &amp;amp; \text{if~} x \ge +3, \\ x / 6 + 1 / 2 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="translated">\text{Hardsigmoid}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ 1 &amp;amp; \text{if~} x \ge +3, \\ x / 6 + 1 / 2 &amp;amp; \text{otherwise} \end{cases}</target>
        </trans-unit>
        <trans-unit id="3707baa731a2a1d93fe314a962f04a7440710fa1" translate="yes" xml:space="preserve">
          <source>\text{Hardswish}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ x &amp;amp; \text{if~} x \ge +3, \\ x \cdot (x + 3) /6 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="translated">\text{Hardswish}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ x &amp;amp; \text{if~} x \ge +3, \\ x \cdot (x + 3) /6 &amp;amp; \text{otherwise} \end{cases}</target>
        </trans-unit>
        <trans-unit id="f4d15ebefe7344cee7608a90993d80b6ed04f631" translate="yes" xml:space="preserve">
          <source>\text{LeakyRELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ \text{negative\_slope} \times x, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\text{LeakyRELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ \text{negative\_slope} \times x, &amp;amp; \text{ otherwise } \end{cases}</target>
        </trans-unit>
        <trans-unit id="68988527b0b6237352fc1f66e3f8116ea81dd84a" translate="yes" xml:space="preserve">
          <source>\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</source>
          <target state="translated">\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</target>
        </trans-unit>
        <trans-unit id="51a86607b1de32af3fd6faa8dfe21281f15bc36b" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)</source>
          <target state="translated">\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)</target>
        </trans-unit>
        <trans-unit id="a4939da8f3a870de43d45395bdbb72f415fd6a5c" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</source>
          <target state="translated">\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</target>
        </trans-unit>
        <trans-unit id="a58af29fea78e99ffe2b4d0392c259d952558379" translate="yes" xml:space="preserve">
          <source>\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</source>
          <target state="translated">\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</target>
        </trans-unit>
        <trans-unit id="96135669cb29b74a14e76462df05bf97eef0ca84" translate="yes" xml:space="preserve">
          <source>\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</source>
          <target state="translated">\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</target>
        </trans-unit>
        <trans-unit id="ccbab99b824f6a91f1e1bf82a1a0fdaf575b1bb6" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ ax, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\text{PReLU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ ax, &amp;amp; \text{ otherwise } \end{cases}</target>
        </trans-unit>
        <trans-unit id="caa4e9686c6c337eb30002e3bdcb6a130c86d148" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</source>
          <target state="translated">\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</target>
        </trans-unit>
        <trans-unit id="5b347087764b6a93bb6f3ca448de0809ea32466b" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + a * \min(0,x)</source>
          <target state="translated">\text{PReLU}(x) = \max(0,x) + a * \min(0,x)</target>
        </trans-unit>
        <trans-unit id="8ab73665e19333454b7dcdadd14407c6f4eaf162" translate="yes" xml:space="preserve">
          <source>\text{RReLU}(x) = \begin{cases} x &amp;amp; \text{if } x \geq 0 \\ ax &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\text{RReLU}(x) = \begin{cases} x &amp;amp; \text{if } x \geq 0 \\ ax &amp;amp; \text{ otherwise } \end{cases}</target>
        </trans-unit>
        <trans-unit id="ca059eb696d6eaad86cfafe12af59ddef8aeb677" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(0,x), 6)</source>
          <target state="translated">\text{ReLU6}(x) = \min(\max(0,x), 6)</target>
        </trans-unit>
        <trans-unit id="be84129fb353e250c9169d66a9b03062f62f4151" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(x_0, x), q(6))</source>
          <target state="translated">\text{ReLU6}(x) = \min(\max(x_0, x), q(6))</target>
        </trans-unit>
        <trans-unit id="3bfab1dc2c67446ab1cfb128a9c2bc89afb1f336" translate="yes" xml:space="preserve">
          <source>\text{ReLU}</source>
          <target state="translated">\text{ReLU}</target>
        </trans-unit>
        <trans-unit id="08bf65cb7cffc00d2f4597ea4d30a22a63976e74" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x) = (x)^+ = \max(0, x)</source>
          <target state="translated">\text{ReLU}(x) = (x)^+ = \max(0, x)</target>
        </trans-unit>
        <trans-unit id="836fcdcc8abf1c798414718b995cf2a357c6c5a8" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x)= \max(x_0, x)</source>
          <target state="translated">\text{ReLU}(x)= \max(x_0, x)</target>
        </trans-unit>
        <trans-unit id="cab6677fddc14bf3ddd26b8fe4fe9e7d772b3259" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="translated">\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</target>
        </trans-unit>
        <trans-unit id="0eacb9a4c9b22d795a671d9085d3576f65f88226" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="translated">\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</target>
        </trans-unit>
        <trans-unit id="14b5832965c3a723de26aea464be1a7e1207b0e7" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="translated">\ text {Sigmoid}（x）= \ frac {1} {1 + \ exp（-x）}</target>
        </trans-unit>
        <trans-unit id="dd6633f7f328cd88d5fded389f4a69b2a11f8bed" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="translated">\ text {Sigmoid}（x）= \ sigma（x）= \ frac {1} {1 + \ exp（-x）}</target>
        </trans-unit>
        <trans-unit id="fa68ac444b17ef606bcb6fc0587fbb0c08130721" translate="yes" xml:space="preserve">
          <source>\text{SoftShrinkage}(x) = \begin{cases} x - \lambda, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x + \lambda, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {SoftShrinkage}（x）= \ begin {cases} x- \ lambda、＆\ text {if} x&amp;gt; \ lambda \\ x + \ lambda、＆\ text {if} x &amp;lt;-\ lambda \\ 0 、＆​​\ text {それ以外の場合} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="16ab4540ec544f6557be83896c80fd5ae2348d4a" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{ 1 + |x|}</source>
          <target state="translated">\ text {SoftSign}（x）= \ frac {x} {1 + | x |}</target>
        </trans-unit>
        <trans-unit id="b4e8d4178c0a5a0115f54178dd9083d36fddd624" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{1 + |x|}</source>
          <target state="translated">\ text {SoftSign}（x）= \ frac {x} {1 + | x |}</target>
        </trans-unit>
        <trans-unit id="a2f536bf3a8bd1033dd978de393c848edde7120d" translate="yes" xml:space="preserve">
          <source>\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</source>
          <target state="translated">\ text {Softmax}（x_ {i}）= \ frac {\ exp（x_i）} {\ sum_j \ exp（x_j）}</target>
        </trans-unit>
        <trans-unit id="5b81ade15ec338b468467f739fb0a7a9ffbb69c9" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x) = \text{Softmax}(-x)</source>
          <target state="translated">\ text {Softmin}（x）= \ text {Softmax}（-x）</target>
        </trans-unit>
        <trans-unit id="f07f1bdd8c02f9e56e3ec0dac4ae00ebee846aae" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}</source>
          <target state="translated">\ text {Softmin}（x_ {i}）= \ frac {\ exp（-x_i）} {\ sum_j \ exp（-x_j）}</target>
        </trans-unit>
        <trans-unit id="2b624f5a535d4c37714f3d81447e39c785f806f7" translate="yes" xml:space="preserve">
          <source>\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))</source>
          <target state="translated">\ text {Softplus}（x）= \ frac {1} {\ beta} * \ log（1 + \ exp（\ beta * x））</target>
        </trans-unit>
        <trans-unit id="c0fd3a2a1e0641ec623557f2f43fda5f7c89dc6c" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \tanh(x)</source>
          <target state="translated">\ text {Tanhshrink}（x）= x- \ tanh（x）</target>
        </trans-unit>
        <trans-unit id="995fcea13ed41fcda67e8f62153baff1abf82a33" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \text{Tanh}(x)</source>
          <target state="translated">\ text {Tanhshrink}（x）= x- \ text {Tanh}（x）</target>
        </trans-unit>
        <trans-unit id="d0334b6a3fe1d679df6ff4bfb438db0b64230682" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}</source>
          <target state="translated">\ text {Tanh}（x）= \ tanh（x）= \ frac {\ exp（x）-\ exp（-x）} {\ exp（x）+ \ exp（-x）}</target>
        </trans-unit>
        <trans-unit id="75a10715f4ce8ac09932b779e24b1a0a5468348f" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}</source>
          <target state="translated">\ text {Tanh}（x）= \ tanh（x）= \ frac {\ exp（x）-\ exp（-x）} {\ exp（x）+ \ exp（-x）}</target>
        </trans-unit>
        <trans-unit id="f169080e8d1eb07929eead4219c50bde74fb23da" translate="yes" xml:space="preserve">
          <source>\text{batch1} \mathbin{@} \text{batch2}</source>
          <target state="translated">\ text {batch1} \ mathbin {@} \ text {batch2}</target>
        </trans-unit>
        <trans-unit id="229245bb08fb0569d7993b5cea15398607e69900" translate="yes" xml:space="preserve">
          <source>\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}}</source>
          <target state="translated">\ text {bound} = \ text {gain} \ times \ sqrt {\ frac {3} {\ text {fan \ _mode}}}</target>
        </trans-unit>
        <trans-unit id="31ee1b43627512c3204fd0ea19131e5c35c60dc9" translate="yes" xml:space="preserve">
          <source>\text{gamma}^{\text{cycle iterations}}</source>
          <target state="translated">\ text {gamma} ^ {\ text {サイクルの反復}}</target>
        </trans-unit>
        <trans-unit id="3e2f5fb306be83947e6aad79d83c918b084bbee4" translate="yes" xml:space="preserve">
          <source>\text{in\_channels}</source>
          <target state="translated">\text{in\_channels}</target>
        </trans-unit>
        <trans-unit id="26c469932ffffddf5378f023f97a627052bcde45" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;gt; \text{other}</source>
          <target state="translated">\ text {input}&amp;gt; \ text {other}</target>
        </trans-unit>
        <trans-unit id="c15cd245b2e6c1904e5b0a29dc06bb37cb23bb5e" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;lt; \text{other}</source>
          <target state="translated">\ text {input} &amp;lt;\ text {other}</target>
        </trans-unit>
        <trans-unit id="f395bedc77d1db475982b0b075906a247b84c2eb" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target} * \log(\text{input}+\text{eps})</source>
          <target state="translated">\ text {input}-\ text {target} * \ log（\ text {input} + \ text {eps}）</target>
        </trans-unit>
        <trans-unit id="686d55b3f5e0392f4e8fcfd75b77c03037718ecf" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target}*\log(\text{input}+\text{eps})</source>
          <target state="translated">\ text {input}-\ text {target} * \ log（\ text {input} + \ text {eps}）</target>
        </trans-unit>
        <trans-unit id="6da1a408fc1384eb95e3208a559d3a3e0f03e99f" translate="yes" xml:space="preserve">
          <source>\text{input} = Q R</source>
          <target state="translated">\ text {input} = QR</target>
        </trans-unit>
        <trans-unit id="c4dd474d77793c7a92038a0755b05fe0ba4adf17" translate="yes" xml:space="preserve">
          <source>\text{input} = V \text{diag}(e) V^T</source>
          <target state="translated">\ text {input} = V \ text {diag}（e）V ^ T</target>
        </trans-unit>
        <trans-unit id="e721787652f4fc0b6f66950d386917c38d8957f8" translate="yes" xml:space="preserve">
          <source>\text{input} \geq \text{other}</source>
          <target state="translated">\ text {input} \ geq \ text {other}</target>
        </trans-unit>
        <trans-unit id="3cbe8a936fc7cb3a9fdf09ad329713fbe61317e4" translate="yes" xml:space="preserve">
          <source>\text{input} \leq \text{other}</source>
          <target state="translated">\ text {input} \ leq \ text {other}</target>
        </trans-unit>
        <trans-unit id="3901704e8ee508e9522d87ef1f88b52ef027532b" translate="yes" xml:space="preserve">
          <source>\text{input} \neq \text{other}</source>
          <target state="translated">\ text {input} \ neq \ text {other}</target>
        </trans-unit>
        <trans-unit id="ccf6648a02673a0bbbd4f96889ba48888a0e40f4" translate="yes" xml:space="preserve">
          <source>\text{input}[i, j]</source>
          <target state="translated">\ text {input} [i、j]</target>
        </trans-unit>
        <trans-unit id="47eeab2292dd874946bd7e66f27cc66b11eafc5e" translate="yes" xml:space="preserve">
          <source>\text{input}_{i}</source>
          <target state="translated">\text{input}_{i}</target>
        </trans-unit>
        <trans-unit id="dd2a901c455f604c6af98c920667ce74d801ef8c" translate="yes" xml:space="preserve">
          <source>\text{input}_{i} / \text{other}_{i}</source>
          <target state="translated">\ text {input} _ {i} / \ text {other} _ {i}</target>
        </trans-unit>
        <trans-unit id="c920fc92a2f279db08ab80d5217f4a1714bd40d6" translate="yes" xml:space="preserve">
          <source>\text{i}^{th}</source>
          <target state="translated">\text{i}^{th}</target>
        </trans-unit>
        <trans-unit id="6ca0545c1ccedc4ff77a3d4acb74e3770c0fbbdd" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]})</source>
          <target state="translated">\ text {kernel \ _size [0]}、\ text {kernel \ _size [1]}）</target>
        </trans-unit>
        <trans-unit id="9229970eeb833df18b71461a132cd3b0ce98ef3a" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})</source>
          <target state="translated">\ text {kernel \ _size [0]}、\ text {kernel \ _size [1]}、\ text {kernel \ _size [2]}）</target>
        </trans-unit>
        <trans-unit id="15869cb2da85e3f8dc3a924d02c0d13f080d42d1" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size})</source>
          <target state="translated">\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="80b2d9d0a1aff139a212f622b578c27a2cb6c126" translate="yes" xml:space="preserve">
          <source>\text{k}^{th}</source>
          <target state="translated">\text{k}^{th}</target>
        </trans-unit>
        <trans-unit id="0f84b2b9c4507f4c38935799e7145beab872e3ff" translate="yes" xml:space="preserve">
          <source>\text{logcumsumexp}(x)_{ij} = \log \sum\limits_{j=0}^{i} \exp(x_{ij})</source>
          <target state="translated">\ text {logcumsumexp}（x）_ {ij} = \ log \ sum \ limits_ {j = 0} ^ {i} \ exp（x_ {ij}）</target>
        </trans-unit>
        <trans-unit id="e4fff2f19ba639fc2ee13a3cda790993ad87fdfe" translate="yes" xml:space="preserve">
          <source>\text{logsumexp}(x)_{i} = \log \sum_j \exp(x_{ij})</source>
          <target state="translated">\ text {logsumexp}（x）_ {i} = \ log \ sum_j \ exp（x_ {ij}）</target>
        </trans-unit>
        <trans-unit id="e990f63d798552bb3d1da254e1d0c3c11424c266" translate="yes" xml:space="preserve">
          <source>\text{loss} = \frac{\sum^{N}_{i=1} loss(i, class[i])}{\sum^{N}_{i=1} weight[class[i]]}</source>
          <target state="translated">\ text {loss} = \ frac {\ sum ^ {N} _ {i = 1} loss（i、class [i]）} {\ sum ^ {N} _ {i = 1} weight [class [i] ]}</target>
        </trans-unit>
        <trans-unit id="65ca80a02b47d73c67e5a3e63c9b314787018d8d" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) = -x[class] + \log\left(\sum_j \exp(x[j])\right)</source>
          <target state="translated">\ text {loss}（x、class）=-\ log \ left（\ frac {\ exp（x [class]）} {\ sum_j \ exp（x [j]）} \ right）= -x [class] + \ log \ left（\ sum_j \ exp（x [j]）\ right）</target>
        </trans-unit>
        <trans-unit id="019a711532dd32707cd8b1b52a85f9dcf92bf993" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</source>
          <target state="translated">\ text {loss}（x、class）= weight [class] \ left（-x [class] + \ log \ left（\ sum_j \ exp（x [j]）\ right）\ right）</target>
        </trans-unit>
        <trans-unit id="ffd0b22341cc6c018e0b5728d553ab798985dbfc" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \begin{cases} 1 - \cos(x_1, x_2), &amp;amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp;amp; \text{if } y = -1 \end{cases}</source>
          <target state="translated">\ text {loss}（x、y）= \ begin {cases} 1- \ cos（x_1、x_2）、＆\ text {if} y = 1 \\ \ max（0、\ cos（x_1、x_2）- \ text {margin}）、＆\ text {if} y = -1 \ end {cases}</target>
        </trans-unit>
        <trans-unit id="c59e75cdc56fc3f83b35cdb9f628a1f6dc39d344" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}</source>
          <target state="translated">\ text {loss}（x、y）= \ frac {1} {n} \ sum_ {i} z_ {i}</target>
        </trans-unit>
        <trans-unit id="b06e3acda76e096ac3266b690a1f15f3c86a036c" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}</source>
          <target state="translated">\ text {loss}（x、y）= \ frac {\ sum_i \ max（0、\ text {margin} --x [y] + x [i]））^ p} {\ text {x.size}（ 0）}</target>
        </trans-unit>
        <trans-unit id="932b5db2aa0a39e3cde91ee0926767654acd252f" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}</source>
          <target state="translated">\ text {loss}（x、y）= \ frac {\ sum_i \ max（0、w [y] *（\ text {margin} --x [y] + x [i]））^ p）} {\ text {x.size}（0）}</target>
        </trans-unit>
        <trans-unit id="1be1a15f43a5fe29e30cefc8a77697b52665cc11" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</source>
          <target state="translated">\ text {loss}（x、y）= \ sum_i \ frac {\ log（1 + \ exp（-y [i] * x [i]））} {\ text {x.nelement}（）}</target>
        </trans-unit>
        <trans-unit id="be8b3997d164f84206bbdb59fc30b16e4df28e7b" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</source>
          <target state="translated">\ text {loss}（x、y）= \ sum_ {ij} \ frac {\ max（0、1-（x [y [j]]-x [i]））} {\ text {x.size} （0）}</target>
        </trans-unit>
        <trans-unit id="ed773e5223ed4e8e7a2085c415f1e22caa9ee61a" translate="yes" xml:space="preserve">
          <source>\text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})</source>
          <target state="translated">\ text {loss}（x1、x2、y）= \ max（0、-y *（x1-x2）+ \ text {margin}）</target>
        </trans-unit>
        <trans-unit id="1a70593a5153f0c40ba1299b72f0b57903179f3e" translate="yes" xml:space="preserve">
          <source>\text{other}_{i}</source>
          <target state="translated">\text{other}_{i}</target>
        </trans-unit>
        <trans-unit id="17f7519844253d6013b5ffbe16cec0f08dcc6ff0" translate="yes" xml:space="preserve">
          <source>\text{out} = -1 \times \text{input}</source>
          <target state="translated">\ text {out} = -1 \ times \ text {input}</target>
        </trans-unit>
        <trans-unit id="90a41651feeb08c3eedd09a310ec016d8b14ad0c" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \（\ text {mat1} _i \ mathbin {@} \ text {mat2} _i）</target>
        </trans-unit>
        <trans-unit id="bb3213012476cae8440fbe19a6e07cd71a7793bd" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \（\ text {mat} \ mathbin {@} \ text {vec}）</target>
        </trans-unit>
        <trans-unit id="e949532f9deb6bd6d752ab7f572576835541d193" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \（\ text {vec1} \ otimes \ text {vec2}）</target>
        </trans-unit>
        <trans-unit id="0358e27d2d15a38fee22f3930f0704830355a8e2" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{abs} \cdot \cos(\text{angle}) + \text{abs} \cdot \sin(\text{angle}) \cdot j</source>
          <target state="translated">\ text {out} = \ text {abs} \ cdot \ cos（\ text {angle}）+ \ text {abs} \ cdot \ sin（\ text {angle}）\ cdot j</target>
        </trans-unit>
        <trans-unit id="36eb11cd26daa728869e9dc0ff4a77afceb36308" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{alpha} \times \text{other}</source>
          <target state="translated">\ text {out} = \ text {input} + \ text {alpha} \ times \ text {other}</target>
        </trans-unit>
        <trans-unit id="256d0e45774d81cdb6896827af46bd880155747c" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{other}</source>
          <target state="translated">\ text {out} = \ text {input} + \ text {other}</target>
        </trans-unit>
        <trans-unit id="b32a02ba7ef69d1c7496b06c7c87e64e5599ad7d" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1} \text{input}(N_i, C_j, \text{stride} \times l + m)</source>
          <target state="translated">\ text {out}（N_i、C_j、l）= \ frac {1} {k} \ sum_ {m = 0} ^ {k-1} \ text {input}（N_i、C_j、\ text {stride} \ l + mの倍）</target>
        </trans-unit>
        <trans-unit id="6ffc018e43b40a0d8b659cfa4eb86a17bb9a021a" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="translated">\ text {out}（N_i、C _ {\ text {out} _j}）= \ text {bias}（C _ {\ text {out} _j}）+ \ sum_ {k = 0} ^ {C _ {\ text { in}}-1} \ text {weight}（C _ {\ text {out} _j}、k）\ star \ text {input}（N_i、k）</target>
        </trans-unit>
        <trans-unit id="1329f06a718ae43bb8cf775c59ab239d1fb3b8cd" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="translated">\ text {out}（N_i、C _ {\ text {out} _j}）= \ text {bias}（C _ {\ text {out} _j}）+ \ sum_ {k = 0} ^ {C_ {in}- 1} \ text {weight}（C _ {\ text {out} _j}、k）\ star \ text {input}（N_i、k）</target>
        </trans-unit>
        <trans-unit id="43e8084d30aefee2bbf06206dd0a8ab14834163f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \begin{cases} \text{x}_i &amp;amp; \text{if } \text{condition}_i \\ \text{y}_i &amp;amp; \text{otherwise} \\ \end{cases}</source>
          <target state="translated">\ text {out} _i = \ begin {cases} \ text {x} _i＆\ text {if} \ text {condition} _i \\ \ text {y} _i＆\ text {otherwise} \\ \ end {cases }</target>
        </trans-unit>
        <trans-unit id="887bfc6ee868fe88195f2b1a697c2be49d6ee9a3" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)</source>
          <target state="translated">\ text {out} _i = \ beta \ \ text {input} _i + \ alpha \（\ text {batch1} _i \ mathbin {@} \ text {batch2} _i）</target>
        </trans-unit>
        <trans-unit id="65617250468b820b83705d0c5a443bdbe573e367" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \frac{\text{input}_i}{\text{other}_i}</source>
          <target state="translated">\ text {out} _i = \ frac {\ text {input} _i} {\ text {other} _i}</target>
        </trans-unit>
        <trans-unit id="07bf2f98a46000c9d46f464978022925a2bd7e3f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}</source>
          <target state="translated">\ text {out} _i = \ text {input} _i + \ text {value} \ times \ frac {\ text {tensor1} _i} {\ text {tensor2} _i}</target>
        </trans-unit>
        <trans-unit id="3da52d8260a7cb2e43645767f57dc80dcb7dfe10" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i + \ text {value} \ times \ text {tensor1} _i \ times \ text {tensor2} _i</target>
        </trans-unit>
        <trans-unit id="62f87f35ab6f8ad57797570d5573c2164ad3b265" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i \ mathbin {@} \ text {mat2} _i</target>
        </trans-unit>
        <trans-unit id="7a9589802a74e5f4ebdc7b3bf9edf66a572b8b2d" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \times \text{other}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i \ times \ text {other} _i</target>
        </trans-unit>
        <trans-unit id="598abd50041125894fc9d56c763e555dc53626ff" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{other} \times \text{input}_i</source>
          <target state="translated">\ text {out} _i = \ text {other} \ times \ text {input} _i</target>
        </trans-unit>
        <trans-unit id="f19bf9db2f65318c8d3279e96d9b9001602fb09a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{self} ^ {\text{exponent}_i}</source>
          <target state="translated">\ text {out} _i = \ text {self} ^ {\ text {exponent} _i}</target>
        </trans-unit>
        <trans-unit id="a7a2515cc1e93998b3a18636546dddfa38daeec0" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)</source>
          <target state="translated">\ text {out} _i = \ text {start} _i + \ text {weight} _i \ times（\ text {end} _i- \ text {start} _i）</target>
        </trans-unit>
        <trans-unit id="0cb57186f10cc53449c5a3667b3e9e3171f06840" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ \text{exponent}</source>
          <target state="translated">\ text {out} _i = x_i ^ \ text {exponent}</target>
        </trans-unit>
        <trans-unit id="fc2ce21ca05c23e8481f3ba74137e106e861116a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ {\text{exponent}_i}</source>
          <target state="translated">\ text {out} _i = x_i ^ {\ text {exponent} _i}</target>
        </trans-unit>
        <trans-unit id="199c9bafa2de3cddd25186555e3341b49eafa3be" translate="yes" xml:space="preserve">
          <source>\text{out}_i \sim \text{Poisson}(\text{input}_i)</source>
          <target state="translated">\ text {out} _i \ sim \ text {Poisson}（\ text {input} _i）</target>
        </trans-unit>
        <trans-unit id="ec5b93008f32dc9277b22a2479b439d557449a63" translate="yes" xml:space="preserve">
          <source>\text{out}_{i+1} = \text{out}_i + \text{step}.</source>
          <target state="translated">\ text {out} _ {i + 1} = \ text {out} _i + \ text {step}。</target>
        </trans-unit>
        <trans-unit id="ebe2d70cc579db08dc5e67ffe414031300d09f4c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = I_0(\text{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2}</source>
          <target state="translated">\ text {out} _ {i} = I_0（\ text {input} _ {i}）= \ sum_ {k = 0} ^ {\ infty} \ frac {（\ text {input} _ {i} ^ 2 / 4）^ k} {（k！）^ 2}</target>
        </trans-unit>
        <trans-unit id="6b1003c5523ecc63d8ba6aacefacf06606afa128" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cos（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="f24a159b70e10e5357a57a49e58cd32d187c432a" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cos ^ {-1}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="21103602f770a5f7ee7fc1da1e89140f56599227" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cosh（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="84def1c39988847c83528f8fb8f04adcd967d458" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cosh ^ {-1}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="0f2819fdd8d29dbcba0965986f63a77a316be87c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}</source>
          <target state="translated">\ text {out} _ {i} = \ frac {1} {1 + e ^ {-\ text {input} _ {i}}}</target>
        </trans-unit>
        <trans-unit id="e639a53f131fc76fecc291100aefd605f12a0eda" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \frac{1}{\sqrt{\text{input}_{i}}}</source>
          <target state="translated">\ text {out} _ {i} = \ frac {1} {\ sqrt {\ text {input} _ {i}}}</target>
        </trans-unit>
        <trans-unit id="46ce5ca183590faeb94bd046c494af519bde768a" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \frac{1}{\text{input}_{i}}</source>
          <target state="translated">\ text {out} _ {i} = \ frac {1} {\ text {input} _ {i}}</target>
        </trans-unit>
        <trans-unit id="4427bc0696c242442c6213bd5a82d54aee1a2769" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil = \left\lfloor \text{input}_{i} \right\rfloor + 1</source>
          <target state="translated">\ text {out} _ {i} = \ left \ lceil \ text {input} _ {i} \ right \ rceil = \ left \ lfloor \ text {input} _ {i} \ right \ rfloor + 1</target>
        </trans-unit>
        <trans-unit id="15280a01ee3fbee59abcbcd2395fe2dc53197b84" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \left\lfloor \text{input}_{i} \right\rfloor</source>
          <target state="translated">\ text {out} _ {i} = \ left \ lfloor \ text {input} _ {i} \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="8713f714ca9f011825a27f3a64b5103537d7c3a8" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \log \Gamma(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ log \ Gamma（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="f106314a05b463158c27ca8e8540a802938b4b7b" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \operatorname{sgn}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ operatorname {sgn}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="6d66baddaec726fdba3a80fb59890298bae46f57" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sin(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ sin（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="2029dbca4086f039d8d89647025660d63e959250" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sin^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ sin ^ {-1}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="29f2eee43262fe3393a73b473efb38b2970a7d4d" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sinh(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ sinh（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="4f9ac0337247d9d45d980c107144e23646dfd988" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sinh^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ sinh ^ {-1}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="a48b925f8709a2a6a8ef052050d18b589d5123a2" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sqrt{\text{input}_{i}^{2} + \text{other}_{i}^{2}}</source>
          <target state="translated">\ text {out} _ {i} = \ sqrt {\ text {input} _ {i} ^ {2} + \ text {other} _ {i} ^ {2}}</target>
        </trans-unit>
        <trans-unit id="a6adf034e3ba47318aad9958d92aa5caaeae39a3" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sqrt{\text{input}_{i}}</source>
          <target state="translated">\ text {out} _ {i} = \ sqrt {\ text {input} _ {i}}</target>
        </trans-unit>
        <trans-unit id="3563e2ffc1ebfbc800c1dd7b12d56f17e2f84012" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \tan(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ tan（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="c84ff0d5cfb4e3368d70e9d3853ac2f7da77df34" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \tan^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ tan ^ {-1}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="339ee2ac3b749c3430c326bb801e434eb7dd88eb" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \tanh(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ tanh（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="0bbbd60b143b37df1c047a994b490f440b3b4a12" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \tanh^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ tanh ^ {-1}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="b2b840e12a553c229a2511c216662f4659dc95dc" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \text{input}_{i} - \left\lfloor |\text{input}_{i}| \right\rfloor * \operatorname{sgn}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ text {input} _ {i}-\ left \ lfloor | \ text {input} _ {i} | \ right \ rfloor * \ operatorname {sgn}（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="07e6f589bfadaeb684ef52c979559ff70dd53446" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = angle(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = angle（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="3208732ebcf3371420c5abc0afe1728e365bcaab" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = conj(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = conj（\ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="b2990a218cc833a6ec80d5667b3951afb3dfc0ec" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = |\text{input}_{i}|</source>
          <target state="translated">\ text {out} _ {i} = | \ text {input} _ {i} |</target>
        </trans-unit>
        <trans-unit id="c6aa4724db682413c4a0d9e7da789301bde04767" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} \sim \mathcal{N}(0, 1)</source>
          <target state="translated">\ text {out} _ {i} \ sim \ mathcal {N}（0、1）</target>
        </trans-unit>
        <trans-unit id="19b54f107e367ae800728c182219b0a6d113a55d" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} \ sim \ mathrm {Bernoulli}（p = \ text {input} _ {i}）</target>
        </trans-unit>
        <trans-unit id="0ff6f3181c37c49b168dc5bb2c5512278c4d8cdd" translate="yes" xml:space="preserve">
          <source>\text{out}_{{i+1}} = \text{out}_{i} + \text{step}</source>
          <target state="translated">\ text {out} _ {{i + 1}} = \ text {out} _ {i} + \ text {step}</target>
        </trans-unit>
        <trans-unit id="22972795246817e25ed6cd9e7f9534bb009c4be5" translate="yes" xml:space="preserve">
          <source>\text{padding\_back}</source>
          <target state="translated">\text{padding\_back}</target>
        </trans-unit>
        <trans-unit id="8f171d7dcd7097c55030a1f2855ec480075028b7" translate="yes" xml:space="preserve">
          <source>\text{padding\_bottom}</source>
          <target state="translated">\text{padding\_bottom}</target>
        </trans-unit>
        <trans-unit id="c61e815e96b628670818ef2117895748429c8232" translate="yes" xml:space="preserve">
          <source>\text{padding\_front}</source>
          <target state="translated">\text{padding\_front}</target>
        </trans-unit>
        <trans-unit id="db98a807cd5008387cbab3fe6e2aec41450b4604" translate="yes" xml:space="preserve">
          <source>\text{padding\_front}, \text{padding\_back})</source>
          <target state="translated">\ text {padding \ _front}、\ text {padding \ _back}）</target>
        </trans-unit>
        <trans-unit id="71d180502f18ec5d773aee314a7a4111878c2b75" translate="yes" xml:space="preserve">
          <source>\text{padding\_left}</source>
          <target state="translated">\text{padding\_left}</target>
        </trans-unit>
        <trans-unit id="d6ccab1e4d0ca305991dad9a66b238c71e38726d" translate="yes" xml:space="preserve">
          <source>\text{padding\_right}</source>
          <target state="translated">\text{padding\_right}</target>
        </trans-unit>
        <trans-unit id="822c531daaae9a83e8cb9610863aa10968e367cb" translate="yes" xml:space="preserve">
          <source>\text{padding\_top}</source>
          <target state="translated">\text{padding\_top}</target>
        </trans-unit>
        <trans-unit id="c863b416ef996ee31809e50a0bf7888a3fdd7ad8" translate="yes" xml:space="preserve">
          <source>\text{padding\_top}, \text{padding\_bottom}</source>
          <target state="translated">\ text {padding \ _top}、\ text {padding \ _bottom}</target>
        </trans-unit>
        <trans-unit id="4dfb970fd31b03f94d4e4f1e23a1347628359b17" translate="yes" xml:space="preserve">
          <source>\text{padding\_top}, \text{padding\_bottom})</source>
          <target state="translated">\ text {padding \ _top}、\ text {padding \ _bottom}）</target>
        </trans-unit>
        <trans-unit id="2940a7cd2f158af987520c8f34af379096300028" translate="yes" xml:space="preserve">
          <source>\text{scale} = 1.0507009873554804934193349852946</source>
          <target state="translated">\ text {scale} = 1.0507009873554804934193349852946</target>
        </trans-unit>
        <trans-unit id="509bb45bc88eeba17ab4e18a7b0718847be009ba" translate="yes" xml:space="preserve">
          <source>\text{silu}(x) = x * \sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}</source>
          <target state="translated">\ text {silu}（x）= x * \ sigma（x）、\ text {where} \ sigma（x）\ text {はロジスティックシグモイドです。}</target>
        </trans-unit>
        <trans-unit id="19a93b01d26977b01f0022e9ad1395871a79c6d2" translate="yes" xml:space="preserve">
          <source>\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}</source>
          <target state="translated">\ text {similarity} = \ dfrac {x_1 \ cdot x_2} {\ max（\ Vert x_1 \ Vert _2 \ cdot \ Vert x_2 \ Vert _2、\ epsilon）}</target>
        </trans-unit>
        <trans-unit id="64fe2ce749a0723d9a9566b76783f0dec79a7cd9" translate="yes" xml:space="preserve">
          <source>\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}.</source>
          <target state="translated">\ text {similarity} = \ dfrac {x_1 \ cdot x_2} {\ max（\ Vert x_1 \ Vert _2 \ cdot \ Vert x_2 \ Vert _2、\ epsilon）}。</target>
        </trans-unit>
        <trans-unit id="1e184ebb3380e76e50f3e3cc8c8ba0cdbf171178" translate="yes" xml:space="preserve">
          <source>\text{spatial\_size}</source>
          <target state="translated">\text{spatial\_size}</target>
        </trans-unit>
        <trans-unit id="30d98357c0cf5aadd64e4c5685312bfb74e6ff43" translate="yes" xml:space="preserve">
          <source>\text{std} = \frac{\text{gain}}{\sqrt{\text{fan\_mode}}}</source>
          <target state="translated">\text{std} = \frac{\text{gain}}{\sqrt{\text{fan\_mode}}}</target>
        </trans-unit>
        <trans-unit id="0bba3586f43350ca4e47d9af0d460338cdf45084" translate="yes" xml:space="preserve">
          <source>\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in} + \text{fan\_out}}}</source>
          <target state="translated">\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in} + \text{fan\_out}}}</target>
        </trans-unit>
        <trans-unit id="8ee7a6294338ef4ec43db737a79e7a5c5827ad5d" translate="yes" xml:space="preserve">
          <source>\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]</source>
          <target state="translated">\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]</target>
        </trans-unit>
        <trans-unit id="9ae213028d9116087df1c48df38a24906070e968" translate="yes" xml:space="preserve">
          <source>\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})</source>
          <target state="translated">\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})</target>
        </trans-unit>
        <trans-unit id="40e87ce770ee226cb8037a67d055f5046fdf9036" translate="yes" xml:space="preserve">
          <source>\text{target} \sim \mathrm{Poisson}(\text{input}) \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input}) + \log(\text{target!})</source>
          <target state="translated">\text{target} \sim \mathrm{Poisson}(\text{input}) \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input}) + \log(\text{target!})</target>
        </trans-unit>
        <trans-unit id="ee61c683da63904db4a3e121bdf113933043f4d7" translate="yes" xml:space="preserve">
          <source>\text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).</source>
          <target state="translated">\text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).</target>
        </trans-unit>
        <trans-unit id="0f6bd28cfcaee12c0d20055b4b4afd7bf6b48113" translate="yes" xml:space="preserve">
          <source>\text{tensor1} / \text{tensor2}</source>
          <target state="translated">\text{tensor1} / \text{tensor2}</target>
        </trans-unit>
        <trans-unit id="075aa43f8957c1474a6931e41c96dcb7c5d686d6" translate="yes" xml:space="preserve">
          <source>\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \times eigenvectors[:, j + 1]</source>
          <target state="translated">\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \times eigenvectors[:, j + 1]</target>
        </trans-unit>
        <trans-unit id="dbbcfbb1d0a265eefc03e882be62153c1a959077" translate="yes" xml:space="preserve">
          <source>\text{true eigenvector}[j] = eigenvectors[:, j] + i \times eigenvectors[:, j + 1]</source>
          <target state="translated">\text{true eigenvector}[j] = eigenvectors[:, j] + i \times eigenvectors[:, j + 1]</target>
        </trans-unit>
        <trans-unit id="63386bdc7be9c10bbab18ac478e13f97470e863e" translate="yes" xml:space="preserve">
          <source>\text{val}</source>
          <target state="translated">\text{val}</target>
        </trans-unit>
        <trans-unit id="d6db479a087e6e41362768b25784ba255c6716a3" translate="yes" xml:space="preserve">
          <source>\text{vec1} \otimes \text{vec2}</source>
          <target state="translated">\text{vec1} \otimes \text{vec2}</target>
        </trans-unit>
        <trans-unit id="64e1f89a912ee4486e65f62e989e6b10c06342f9" translate="yes" xml:space="preserve">
          <source>\text{win\_length} &amp;lt; \text{n\_fft}</source>
          <target state="translated">\text{win\_length} &amp;lt; \text{n\_fft}</target>
        </trans-unit>
        <trans-unit id="18afc597cccde66cf5c2fa4706aeb0646f3723b2" translate="yes" xml:space="preserve">
          <source>\text{window\_length} + 1</source>
          <target state="translated">\text{window\_length} + 1</target>
        </trans-unit>
        <trans-unit id="848da5fb0b7bd079124401e74232d55ffe67400b" translate="yes" xml:space="preserve">
          <source>\text{{heaviside}}(input, values) = \begin{cases} 0, &amp;amp; \text{if input &amp;lt; 0}\\ values, &amp;amp; \text{if input == 0}\\ 1, &amp;amp; \text{if input &amp;gt; 0} \end{cases}</source>
          <target state="translated">\text{{heaviside}}(input, values) = \begin{cases} 0, &amp;amp; \text{if input &amp;lt; 0}\\ values, &amp;amp; \text{if input == 0}\\ 1, &amp;amp; \text{if input &amp;gt; 0} \end{cases}</target>
        </trans-unit>
        <trans-unit id="f6688d2b60281ad8fefcc0fea715a9f936e08052" translate="yes" xml:space="preserve">
          <source>\text{{out}}_i = \text{trunc} \left( \frac{{\text{{input}}_i}}{{\text{{other}}_i}} \right)</source>
          <target state="translated">\text{{out}}_i = \text{trunc} \left( \frac{{\text{{input}}_i}}{{\text{{other}}_i}} \right)</target>
        </trans-unit>
        <trans-unit id="b390b7373d30b8df651a5b7c8366caf960e3ef34" translate="yes" xml:space="preserve">
          <source>\text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i</source>
          <target state="translated">\text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i</target>
        </trans-unit>
        <trans-unit id="cb005d76f9f2e394a770c2562c2e150a413b3216" translate="yes" xml:space="preserve">
          <source>\theta</source>
          <target state="translated">\theta</target>
        </trans-unit>
        <trans-unit id="48eecc0a7bfa64d8c32f97e0fc816b2205b04bb8" translate="yes" xml:space="preserve">
          <source>\{0, \ldots, K-1\}</source>
          <target state="translated">\{0, \ldots, K-1\}</target>
        </trans-unit>
        <trans-unit id="86f7e437faa5a7fce15d1ddcb9eaeaea377667b8" translate="yes" xml:space="preserve">
          <source>a</source>
          <target state="translated">a</target>
        </trans-unit>
        <trans-unit id="985ea09fbb51e15e4429dc741d64e71fe6b0efab" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; object</source>
          <target state="translated">a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; &lt;/a&gt; object</target>
        </trans-unit>
        <trans-unit id="d162d7e8bd1d5dda8190b029ffd491226ce6290d" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;tuple&lt;/code&gt; of three ints &amp;ndash; in which case, the first &lt;code&gt;int&lt;/code&gt; is used for the depth dimension, the second &lt;code&gt;int&lt;/code&gt; for the height dimension and the third &lt;code&gt;int&lt;/code&gt; for the width dimension</source>
          <target state="translated">a &lt;code&gt;tuple&lt;/code&gt; of three ints &amp;ndash; in which case, the first &lt;code&gt;int&lt;/code&gt; is used for the depth dimension, the second &lt;code&gt;int&lt;/code&gt; for the height dimension and the third &lt;code&gt;int&lt;/code&gt; for the width dimension</target>
        </trans-unit>
        <trans-unit id="5e87ba64be191e5fb677cabf542708af2183633d" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;tuple&lt;/code&gt; of two ints &amp;ndash; in which case, the first &lt;code&gt;int&lt;/code&gt; is used for the height dimension, and the second &lt;code&gt;int&lt;/code&gt; for the width dimension</source>
          <target state="translated">a &lt;code&gt;tuple&lt;/code&gt; of two ints &amp;ndash; in which case, the first &lt;code&gt;int&lt;/code&gt; is used for the height dimension, and the second &lt;code&gt;int&lt;/code&gt; for the width dimension</target>
        </trans-unit>
        <trans-unit id="9af60a637080c119529287cf450a17dc51cdd322" translate="yes" xml:space="preserve">
          <source>a = \text{gain} \times \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}}</source>
          <target state="translated">a=\text{gain}◦◦times \sqrtrt{\frac{6}{\text{fan\_in}+◦text{fan\_out}}}}}</target>
        </trans-unit>
        <trans-unit id="a18d36abd55ff2f8af389fa82ac5cf243f710013" translate="yes" xml:space="preserve">
          <source>a Tensor containing the result of module(input) located on output_device</source>
          <target state="translated">output_device上のmodule(input)の結果を含むテンソル</target>
        </trans-unit>
        <trans-unit id="890f00f76ee1d37642ba29392aeb8c4a40b22625" translate="yes" xml:space="preserve">
          <source>a Tensor of the same dimension and shape as the input with values in the range [-inf, 0)</source>
          <target state="translated">入力と同じ次元と形状のテンソルで,値は [-inf,0]の範囲にあります.</target>
        </trans-unit>
        <trans-unit id="2003464995dec65fd9f3cf1a1961d2e3b1715419" translate="yes" xml:space="preserve">
          <source>a Tensor of the same dimension and shape as the input with values in the range [0, 1]</source>
          <target state="translated">入力と同じ次元と形状のテンソルで,値は [0,1]の範囲にあります.</target>
        </trans-unit>
        <trans-unit id="6fae14cf3c01e74728e043358ac48d17d6d7ff1f" translate="yes" xml:space="preserve">
          <source>a Tensor of the same dimension and shape as the input, with values in the range [0, 1]</source>
          <target state="translated">入力と同じ次元と形状のテンソルで,値は [0,1]の範囲にあります.</target>
        </trans-unit>
        <trans-unit id="0fa1afd2e30272a3a23ae385fdbd6176134150f6" translate="yes" xml:space="preserve">
          <source>a class with the highest probability for each example</source>
          <target state="translated">各例に対して最高の確率を持つクラス</target>
        </trans-unit>
        <trans-unit id="45f2f8dde4763de997a5fc5976cf3a9430e5dc9f" translate="yes" xml:space="preserve">
          <source>a dictionary containing a whole state of the module</source>
          <target state="translated">モジュールの全体の状態を含む辞書</target>
        </trans-unit>
        <trans-unit id="00a9b884880389883482dc751408a90f0cb9ee68" translate="yes" xml:space="preserve">
          <source>a dictionary to specify dynamic axes of input/output, such that: - KEY: input and/or output names - VALUE: index of dynamic axes for given key and potentially the name to be used for exported dynamic axes. In general the value is defined according to one of the following ways or a combination of both: (1). A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes of provided input/output during export. OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on such axis of such input/output during export.</source>
          <target state="translated">入力/出力の動体軸を指定するための辞書。-VALUE:与えられたキーの動体軸のインデックスと、エクスポートされた動体軸に使用される名前の可能性があります。一般的に、値は以下の方法のいずれか、または両方の組み合わせに従って定義されます。(1).(1)指定された入力の動軸を指定する整数のリスト。このシナリオでは、自動化された名前が生成され、エクスポート中に提供された入出力の動体軸に適用されます。OR (2)。対応する入出力の動的軸のインデックスから、エクスポート中にその入出力のその軸に適用されたい名前へのマッピングを指定する内部辞書。</target>
        </trans-unit>
        <trans-unit id="fc0ea09618fde72378f33c4a6ddb59c0e4510cd0" translate="yes" xml:space="preserve">
          <source>a function for tracing the iteration process. When specified, it is called at each iteration step with LOBPCG instance as an argument. The LOBPCG instance holds the full state of the iteration process in the following attributes:</source>
          <target state="translated">反復処理をトレースするための関数です。指定された場合は,LOBPCG インスタンスを引数として,各反復処理ステップで呼び出されます.LOBPCGインスタンスは、以下の属性で反復処理の全状態を保持します。</target>
        </trans-unit>
        <trans-unit id="f41119365c981a28ba7e837a621a43e97595a3e6" translate="yes" xml:space="preserve">
          <source>a handle that can be used to remove the added hook by calling &lt;code&gt;handle.remove()&lt;/code&gt;</source>
          <target state="translated">a handle that can be used to remove the added hook by calling &lt;code&gt;handle.remove()&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ae26d64a3b3e80ec8246fb0560aeac16291f28f8" translate="yes" xml:space="preserve">
          <source>a list of available entrypoint names</source>
          <target state="translated">利用可能なエントリポイント名のリスト</target>
        </trans-unit>
        <trans-unit id="9f1c5e47605b815eb437073ec1603759477ea676" translate="yes" xml:space="preserve">
          <source>a reference to the execution of &lt;code&gt;func&lt;/code&gt;. The value &lt;code&gt;T&lt;/code&gt; can only be accessed by forcing completion of &lt;code&gt;func&lt;/code&gt; through &lt;code&gt;torch.jit.wait&lt;/code&gt;.</source>
          <target state="translated">a reference to the execution of &lt;code&gt;func&lt;/code&gt; . The value &lt;code&gt;T&lt;/code&gt; can only be accessed by forcing completion of &lt;code&gt;func&lt;/code&gt; through &lt;code&gt;torch.jit.wait&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="33218b2df65cce0092049e58dd8bb1cb81e976cb" translate="yes" xml:space="preserve">
          <source>a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the depth, height and width dimension</source>
          <target state="translated">a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the depth, height and width dimension</target>
        </trans-unit>
        <trans-unit id="850561b0cef31ac05a0c3f4be1e02622fadced9d" translate="yes" xml:space="preserve">
          <source>a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the depth, height and width dimensions</source>
          <target state="translated">a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the depth, height and width dimensions</target>
        </trans-unit>
        <trans-unit id="a73b9f710ff83671961055a44ffd5cbbcae7d407" translate="yes" xml:space="preserve">
          <source>a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the height and width dimension</source>
          <target state="translated">a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the height and width dimension</target>
        </trans-unit>
        <trans-unit id="13675467768e46f60e5b4830e00591213f355dd1" translate="yes" xml:space="preserve">
          <source>a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the height and width dimensions</source>
          <target state="translated">a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the height and width dimensions</target>
        </trans-unit>
        <trans-unit id="779476176932b5394791dab09ad1d1763eee6c5b" translate="yes" xml:space="preserve">
          <source>a tensor located on &lt;code&gt;destination&lt;/code&gt; device, that is a result of concatenating &lt;code&gt;tensors&lt;/code&gt; along &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">上に位置テンソル &lt;code&gt;destination&lt;/code&gt; デバイスは、それが連結した結果である &lt;code&gt;tensors&lt;/code&gt; 沿って &lt;code&gt;dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="db8d55dadf017c53a5cb22f260f3267b482ce8b0" translate="yes" xml:space="preserve">
          <source>a tensor of shape &lt;code&gt;Size([max(input) + 1])&lt;/code&gt; if &lt;code&gt;input&lt;/code&gt; is non-empty, else &lt;code&gt;Size(0)&lt;/code&gt;</source>
          <target state="translated">a tensor of shape &lt;code&gt;Size([max(input) + 1])&lt;/code&gt; if &lt;code&gt;input&lt;/code&gt; is non-empty, else &lt;code&gt;Size(0)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a1ce48932d1afaa2c16e715f71dcc93e8712b7a3" translate="yes" xml:space="preserve">
          <source>a tuple containing &lt;code&gt;out&lt;/code&gt; tensors, each containing a chunk of &lt;code&gt;tensor&lt;/code&gt;.</source>
          <target state="translated">タプル &lt;code&gt;out&lt;/code&gt; テンソルは、それぞれ、のチャンクを含む &lt;code&gt;tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ab2e28106a5b9ebc089b8e609fd50c57563064f9" translate="yes" xml:space="preserve">
          <source>a tuple containing &lt;code&gt;out&lt;/code&gt; tensors, each containing a copy of &lt;code&gt;tensor&lt;/code&gt;.</source>
          <target state="translated">タプル &lt;code&gt;out&lt;/code&gt; テンソルは、それぞれのコピー含む &lt;code&gt;tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3a204de89b91437261225e8faf7e8b031ac8eb66" translate="yes" xml:space="preserve">
          <source>a tuple containing chunks of &lt;code&gt;tensor&lt;/code&gt;, placed on &lt;code&gt;devices&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;devices&lt;/code&gt; に配置された &lt;code&gt;tensor&lt;/code&gt; チャンクを含むタプル。</target>
        </trans-unit>
        <trans-unit id="fc7fda381188f8a86d844f77a91f9092c0f7ea9a" translate="yes" xml:space="preserve">
          <source>a tuple containing copies of &lt;code&gt;tensor&lt;/code&gt;, placed on &lt;code&gt;devices&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;devices&lt;/code&gt; に配置された &lt;code&gt;tensor&lt;/code&gt; コピーを含むタプル。</target>
        </trans-unit>
        <trans-unit id="3cffaf9b9bcbb730e4c1694aa1ce58215c898bc4" translate="yes" xml:space="preserve">
          <source>above), and</source>
          <target state="translated">上記)、そして</target>
        </trans-unit>
        <trans-unit id="82451b41fd7878180b6aa2b54e369cbec4e8032c" translate="yes" xml:space="preserve">
          <source>abs</source>
          <target state="translated">abs</target>
        </trans-unit>
        <trans-unit id="da4dbfbc4fdc56237451cf5e9f9933b2b64f1ba6" translate="yes" xml:space="preserve">
          <source>absolute</source>
          <target state="translated">absolute</target>
        </trans-unit>
        <trans-unit id="8379b4f061ef5249954793884b3e333722e3a581" translate="yes" xml:space="preserve">
          <source>according to the</source>
          <target state="translated">によると</target>
        </trans-unit>
        <trans-unit id="328f0742cf74e8ffdb253376b1803615055e215b" translate="yes" xml:space="preserve">
          <source>acos</source>
          <target state="translated">acos</target>
        </trans-unit>
        <trans-unit id="cae11b2912fc7a669ec8e6fd1ef1797da899527b" translate="yes" xml:space="preserve">
          <source>acosh() -&amp;gt; Tensor</source>
          <target state="translated">acosh() -&amp;gt; Tensor</target>
        </trans-unit>
        <trans-unit id="43b9ab0810a93aacba6924ceb0586f3a5ded5277" translate="yes" xml:space="preserve">
          <source>acosh_() -&amp;gt; Tensor</source>
          <target state="translated">acosh_() -&amp;gt; Tensor</target>
        </trans-unit>
        <trans-unit id="188c404e728c43b5646acbca5ba76bfe689b737f" translate="yes" xml:space="preserve">
          <source>across all input channels. If called with &lt;code&gt;nn.PReLU(nChannels)&lt;/code&gt;, a separate</source>
          <target state="translated">across all input channels. If called with &lt;code&gt;nn.PReLU(nChannels)&lt;/code&gt; , a separate</target>
        </trans-unit>
        <trans-unit id="6bb3255719c068be2327cb70a2b8e43a35888f5b" translate="yes" xml:space="preserve">
          <source>adaptive_avg_pool1d</source>
          <target state="translated">adaptive_avg_pool1d</target>
        </trans-unit>
        <trans-unit id="c61fa142ae65a4935debc614087fb58ff3a6df54" translate="yes" xml:space="preserve">
          <source>adaptive_avg_pool2d</source>
          <target state="translated">adaptive_avg_pool2d</target>
        </trans-unit>
        <trans-unit id="14ce4a9f26f8969b6890e407e6f51988efbeea4c" translate="yes" xml:space="preserve">
          <source>adaptive_avg_pool3d</source>
          <target state="translated">adaptive_avg_pool3d</target>
        </trans-unit>
        <trans-unit id="3aaf9eadafea10463817e8d201b7f07b7265e014" translate="yes" xml:space="preserve">
          <source>adaptive_max_pool1d</source>
          <target state="translated">adaptive_max_pool1d</target>
        </trans-unit>
        <trans-unit id="d3d98fd5af00d45465427af1e01b6955b7cc0483" translate="yes" xml:space="preserve">
          <source>adaptive_max_pool2d</source>
          <target state="translated">adaptive_max_pool2d</target>
        </trans-unit>
        <trans-unit id="a3a21897a04cb4aa98f34167bb5bef9a6168c500" translate="yes" xml:space="preserve">
          <source>adaptive_max_pool3d</source>
          <target state="translated">adaptive_max_pool3d</target>
        </trans-unit>
        <trans-unit id="58d1bbce297de3c304a9fefc3b483181872a5c6b" translate="yes" xml:space="preserve">
          <source>add</source>
          <target state="translated">add</target>
        </trans-unit>
        <trans-unit id="5a96c9fe756bb4bf51deba8e7d88183c67947c4a" translate="yes" xml:space="preserve">
          <source>add (nonzero alpha not supported)</source>
          <target state="translated">追加</target>
        </trans-unit>
        <trans-unit id="d8cab2345ee84bff05fb1af92dc904a85a08122e" translate="yes" xml:space="preserve">
          <source>add_relu</source>
          <target state="translated">add_relu</target>
        </trans-unit>
        <trans-unit id="be4a7c50ca01a7fc792facb9bd955bb98f5efd0f" translate="yes" xml:space="preserve">
          <source>add_scalar</source>
          <target state="translated">add_scalar</target>
        </trans-unit>
        <trans-unit id="d22b6f87e06eb3844a28e850f32f0a7a889d92fe" translate="yes" xml:space="preserve">
          <source>addmm</source>
          <target state="translated">addmm</target>
        </trans-unit>
        <trans-unit id="015bd815072238008dba22c71b1f12d4a0e9ce46" translate="yes" xml:space="preserve">
          <source>affine_grid</source>
          <target state="translated">affine_grid</target>
        </trans-unit>
        <trans-unit id="8bde722d88ff10934184bb1cc67a307b1553065a" translate="yes" xml:space="preserve">
          <source>after a restart. Default: 1.</source>
          <target state="translated">を再起動した後に使用します。デフォルト:1.</target>
        </trans-unit>
        <trans-unit id="186f4cfeddd96df3f01de118db573839422f502a" translate="yes" xml:space="preserve">
          <source>after restart, set</source>
          <target state="translated">再起動後に</target>
        </trans-unit>
        <trans-unit id="5dc5329726e33c5c40a0fde2a6492a766965bdc3" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._Cat&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._Cat&lt;/code&gt; のエイリアス</target>
        </trans-unit>
        <trans-unit id="cff8a9de6aa7fa5fc4f28c651af38f742f1fcebf" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._DependentProperty&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._DependentProperty&lt;/code&gt; のエイリアス</target>
        </trans-unit>
        <trans-unit id="c6de7425cb1a6932219fdf4542beb8455ae1f2ce" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._GreaterThan&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._GreaterThan&lt;/code&gt; のエイリアス</target>
        </trans-unit>
        <trans-unit id="27d8f16c0fa611de1e5b1506b6023adc7a8d2f87" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._GreaterThanEq&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._GreaterThanEq&lt;/code&gt; のエイリアス</target>
        </trans-unit>
        <trans-unit id="4e2f7434e9b90fc50cc6286977410f04d80e91a9" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._HalfOpenInterval&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._HalfOpenInterval&lt;/code&gt; のエイリアス</target>
        </trans-unit>
        <trans-unit id="91d112cab3026dc93f2178836f77313f2d104de5" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._IntegerInterval&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._IntegerInterval&lt;/code&gt; のエイリアス</target>
        </trans-unit>
        <trans-unit id="6d32f64a8596e02e3341c04582c89898ba004bc0" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._Interval&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._Interval&lt;/code&gt; のエイリアス</target>
        </trans-unit>
        <trans-unit id="6faf50bed58cc08be17966a6d90c9e06a9764c5f" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._LessThan&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._LessThan&lt;/code&gt; のエイリアス</target>
        </trans-unit>
        <trans-unit id="5120abef070748d281f11bc2b98b4ec00cc7739e" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._Stack&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._Stack&lt;/code&gt; のエイリアス</target>
        </trans-unit>
        <trans-unit id="a4f813ae14b83732a4864c611af5b3566fb3ec62" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;typing.Tuple&lt;/code&gt;</source>
          <target state="translated">alias of &lt;code&gt;typing.Tuple&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="068aa03cad6f28f79ba1111c747f21ad5b02ebe5" translate="yes" xml:space="preserve">
          <source>all_gather</source>
          <target state="translated">all_gather</target>
        </trans-unit>
        <trans-unit id="d02036fc361150eb3b815512675dec0cfae0d0f3" translate="yes" xml:space="preserve">
          <source>all_reduce</source>
          <target state="translated">all_reduce</target>
        </trans-unit>
        <trans-unit id="d26207411b688464018bef94d89557e73d634e59" translate="yes" xml:space="preserve">
          <source>all_to_all</source>
          <target state="translated">all_to_all</target>
        </trans-unit>
        <trans-unit id="a690d72de3b70e796e4e33522d5b7688578c24c7" translate="yes" xml:space="preserve">
          <source>allowable values are torch.qint8 and torch.quint8. The values of quant_min and quant_max should be chosen to be consistent with the dtype</source>
          <target state="translated">の値は torch.qint8 と torch.quint8 です。quant_min と quant_max の値は、dtype</target>
        </trans-unit>
        <trans-unit id="d554177a50129be7b474c04ec4d6704a4b3ab4b7" translate="yes" xml:space="preserve">
          <source>along &lt;code&gt;dim&lt;/code&gt;, using the trapezoid rule.</source>
          <target state="translated">along &lt;code&gt;dim&lt;/code&gt; , using the trapezoid rule.</target>
        </trans-unit>
        <trans-unit id="76cd6d3a1710d97fef46a230a4cc6240eb2a1380" translate="yes" xml:space="preserve">
          <source>along &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">along &lt;code&gt;dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6554d1a5e11c7976a696c18e79f0814b9396668a" translate="yes" xml:space="preserve">
          <source>along dimension &lt;code&gt;dim&lt;/code&gt; is transformed as</source>
          <target state="translated">along dimension &lt;code&gt;dim&lt;/code&gt; is transformed as</target>
        </trans-unit>
        <trans-unit id="be76331b95dfc399cd776d2fc68021e0db03cc4f" translate="yes" xml:space="preserve">
          <source>alpha</source>
          <target state="translated">alpha</target>
        </trans-unit>
        <trans-unit id="4b077706cb8a0c2ebeb6471729485bac1a733acf" translate="yes" xml:space="preserve">
          <source>alpha_dropout</source>
          <target state="translated">alpha_dropout</target>
        </trans-unit>
        <trans-unit id="cffa50a32cb13a240d705317bcec65dd1f31b6ad" translate="yes" xml:space="preserve">
          <source>and</source>
          <target state="translated">and</target>
        </trans-unit>
        <trans-unit id="ea11961808d60532cba9bb74eca2efc9028432b5" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;L&lt;/code&gt; represents a sequence length.</source>
          <target state="translated">and &lt;code&gt;L&lt;/code&gt; represents a sequence length.</target>
        </trans-unit>
        <trans-unit id="51be83d9e5138cf7da772101995cb9cef885eb19" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;dim = i&lt;/code&gt;, then &lt;code&gt;index&lt;/code&gt; must be an</source>
          <target state="translated">and &lt;code&gt;dim = i&lt;/code&gt; , then &lt;code&gt;index&lt;/code&gt; must be an</target>
        </trans-unit>
        <trans-unit id="7274e61465c020300a30dd75f2b7c77288b1872f" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;grid&lt;/code&gt; with shape</source>
          <target state="translated">and &lt;code&gt;grid&lt;/code&gt; with shape</target>
        </trans-unit>
        <trans-unit id="c3e9497e8d92dbb47e859e474f20c5ff68bc011d" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;kernel_size&lt;/code&gt;</source>
          <target state="translated">and &lt;code&gt;kernel_size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="69906b9a4a5287952e8b1f59e1b066d2de6ab17e" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;out&lt;/code&gt; will be a matrix of size</source>
          <target state="translated">and &lt;code&gt;out&lt;/code&gt; will be a matrix of size</target>
        </trans-unit>
        <trans-unit id="bfbe811b15a7be917e54a7bd6d548093f12e6464" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;out&lt;/code&gt; will have the same size as &lt;code&gt;index&lt;/code&gt;.</source>
          <target state="translated">and &lt;code&gt;out&lt;/code&gt; will have the same size as &lt;code&gt;index&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e8884a6fb0ff6189ded0963ddb98c79c7d9aeb69" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;solution&lt;/code&gt; is the solution</source>
          <target state="translated">and &lt;code&gt;solution&lt;/code&gt; is the solution</target>
        </trans-unit>
        <trans-unit id="deb10848a6c3d0e28f26841e04fd508759c1568a" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;vec2&lt;/code&gt; is a vector of size</source>
          <target state="translated">and &lt;code&gt;vec2&lt;/code&gt; is a vector of size</target>
        </trans-unit>
        <trans-unit id="8516d1daf3054c5229e8207676fe21041fa9e2b0" translate="yes" xml:space="preserve">
          <source>and a &lt;code&gt;Tensor&lt;/code&gt; label</source>
          <target state="translated">and a &lt;code&gt;Tensor&lt;/code&gt; label</target>
        </trans-unit>
        <trans-unit id="7a32db4530d76c7069a29b591cae084dc90bcd99" translate="yes" xml:space="preserve">
          <source>and a labels tensor</source>
          <target state="translated">とラベルテンソル</target>
        </trans-unit>
        <trans-unit id="d9a100e0a04720d0e08f421ee521b4c07271e73c" translate="yes" xml:space="preserve">
          <source>and a margin with a value greater than</source>
          <target state="translated">よりも大きな値を持つマージンが</target>
        </trans-unit>
        <trans-unit id="9bef7a9ca9492d76a989ba472a5134335b532cc1" translate="yes" xml:space="preserve">
          <source>and a matrix</source>
          <target state="translated">と行列</target>
        </trans-unit>
        <trans-unit id="994f7a70392302396190552425356c44897b163a" translate="yes" xml:space="preserve">
          <source>and all but the last dimension are the same shape as the input.</source>
          <target state="translated">と、最後の次元以外はすべて入力と同じ形状になります。</target>
        </trans-unit>
        <trans-unit id="d3efa22a01fb352021085e134597a06af8fdcd21" translate="yes" xml:space="preserve">
          <source>and assumes</source>
          <target state="translated">と仮定して</target>
        </trans-unit>
        <trans-unit id="02e70de252e8382255cf467699d2e8c35f5e1033" translate="yes" xml:space="preserve">
          <source>and loading from an iterable-style dataset is roughly equivalent with:</source>
          <target state="translated">とほぼ同等であり、イテレート可能なスタイルのデータセットからのロードは</target>
        </trans-unit>
        <trans-unit id="44fe67e18a7b0c61be3bcd65d4873d81ff165fe4" translate="yes" xml:space="preserve">
          <source>and multiple right-hand sides</source>
          <target state="translated">と複数の右辺</target>
        </trans-unit>
        <trans-unit id="6b843bb82aff68f312d79ab6cf03a56ace808f2d" translate="yes" xml:space="preserve">
          <source>and one of the following modes is used: - &lt;code&gt;linear&lt;/code&gt; - &lt;code&gt;bilinear&lt;/code&gt; - &lt;code&gt;bicubic&lt;/code&gt; - &lt;code&gt;trilinear&lt;/code&gt;</source>
          <target state="translated">and one of the following modes is used: - &lt;code&gt;linear&lt;/code&gt; - &lt;code&gt;bilinear&lt;/code&gt; - &lt;code&gt;bicubic&lt;/code&gt; - &lt;code&gt;trilinear&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="34c169d448724e62adaa41082c03ac52ab54fff0" translate="yes" xml:space="preserve">
          <source>and output</source>
          <target state="translated">と出力</target>
        </trans-unit>
        <trans-unit id="422022e7df4fc1e7e88e8f2a905326d532895a06" translate="yes" xml:space="preserve">
          <source>and restarts at step</source>
          <target state="translated">で再起動します。</target>
        </trans-unit>
        <trans-unit id="14b190aab2ad2654706b74c2be95acb6da7f2850" translate="yes" xml:space="preserve">
          <source>and scalar output</source>
          <target state="translated">とスカラー出力</target>
        </trans-unit>
        <trans-unit id="b34851d4c502e28d96eafba7331ab90e1898bc8d" translate="yes" xml:space="preserve">
          <source>and so forth. If increasing is True, the columns are</source>
          <target state="translated">といったようなことが考えられます。増加がTrueの場合、列は</target>
        </trans-unit>
        <trans-unit id="146cd69984268e3dc94fa6880d4aabdda441d4be" translate="yes" xml:space="preserve">
          <source>and standard deviation</source>
          <target state="translated">と標準偏差</target>
        </trans-unit>
        <trans-unit id="5bba86a9b83c5aa670a1f3d4c691ac866067b19d" translate="yes" xml:space="preserve">
          <source>and target</source>
          <target state="translated">とターゲット</target>
        </trans-unit>
        <trans-unit id="9124e7eb2a80bd8e99f0ee162bc6c10019f76e56" translate="yes" xml:space="preserve">
          <source>and target tensor</source>
          <target state="translated">とターゲットテンソル</target>
        </trans-unit>
        <trans-unit id="adaffd67d43c3d4bd29c2d4e89911618672f35ec" translate="yes" xml:space="preserve">
          <source>and the LU factorization of A, in order as a namedtuple &lt;code&gt;solution, LU&lt;/code&gt;.</source>
          <target state="translated">and the LU factorization of A, in order as a namedtuple &lt;code&gt;solution, LU&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3c8e722a8235feec4386550dd50bdb353aabf9b3" translate="yes" xml:space="preserve">
          <source>and the elements of</source>
          <target state="translated">の要素と</target>
        </trans-unit>
        <trans-unit id="6058dacca42ad9e7a4c0849aef311147f2ca8455" translate="yes" xml:space="preserve">
          <source>and the total loss functions is</source>
          <target state="translated">と全損失関数は</target>
        </trans-unit>
        <trans-unit id="0f0f37057f618e969e7c100dbe104d0a83309871" translate="yes" xml:space="preserve">
          <source>and vector</source>
          <target state="translated">とベクトル</target>
        </trans-unit>
        <trans-unit id="f12fb281ea5401324cfef7c940cff938f57fc26a" translate="yes" xml:space="preserve">
          <source>and we will be able to step into the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function as a normal Python function. To disable the TorchScript compiler for a specific function, see &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">and we will be able to step into the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; function as a normal Python function. To disable the TorchScript compiler for a specific function, see &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="29748c4ec2f5a11f19b122c91c764f2c283a295d" translate="yes" xml:space="preserve">
          <source>and x2 has shape</source>
          <target state="translated">であり、x2は形状</target>
        </trans-unit>
        <trans-unit id="abe7ad204a8ebc99e055716a1499632a1e21a7ef" translate="yes" xml:space="preserve">
          <source>and zero point</source>
          <target state="translated">とゼロ点</target>
        </trans-unit>
        <trans-unit id="11a9215ec620218c8e8d409263469091431ff0d4" translate="yes" xml:space="preserve">
          <source>angle</source>
          <target state="translated">angle</target>
        </trans-unit>
        <trans-unit id="4ad4ad16aa212e130ee9bd5ac32ae35842a975fe" translate="yes" xml:space="preserve">
          <source>arange</source>
          <target state="translated">arange</target>
        </trans-unit>
        <trans-unit id="a538840857b54aaa96ea8ec3ac8fb4a13708f87e" translate="yes" xml:space="preserve">
          <source>arbitrary shapes with a total of</source>
          <target state="translated">の合計を持つ任意の形状</target>
        </trans-unit>
        <trans-unit id="86c852d13e59fa71b388491fc4ce6baa16edb892" translate="yes" xml:space="preserve">
          <source>are assumed to be 1 and not referenced from</source>
          <target state="translated">は1であると仮定して</target>
        </trans-unit>
        <trans-unit id="e4876ebe475fcdaae7801e2c6555b7737319caff" translate="yes" xml:space="preserve">
          <source>are computed as:</source>
          <target state="translated">として計算されます。</target>
        </trans-unit>
        <trans-unit id="f920c82e9a5117ed628937a9379bf48075b0898c" translate="yes" xml:space="preserve">
          <source>are learnable affine transform parameters of &lt;code&gt;normalized_shape&lt;/code&gt; if &lt;code&gt;elementwise_affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">are learnable affine transform parameters of &lt;code&gt;normalized_shape&lt;/code&gt; if &lt;code&gt;elementwise_affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; . The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e538aee8f67210d75bc513a9c556cb3f2c22456c" translate="yes" xml:space="preserve">
          <source>are learnable parameter vectors of size &lt;code&gt;C&lt;/code&gt; (where &lt;code&gt;C&lt;/code&gt; is the input size) if &lt;code&gt;affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">are learnable parameter vectors of size &lt;code&gt;C&lt;/code&gt; (where &lt;code&gt;C&lt;/code&gt; is the input size) if &lt;code&gt;affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; . The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="425eb1a701280f929d1aa755a3bd9afffe042bd8" translate="yes" xml:space="preserve">
          <source>are learnable parameter vectors of size &lt;code&gt;C&lt;/code&gt; (where &lt;code&gt;C&lt;/code&gt; is the input size). By default, the elements of</source>
          <target state="translated">are learnable parameter vectors of size &lt;code&gt;C&lt;/code&gt; (where &lt;code&gt;C&lt;/code&gt; is the input size). By default, the elements of</target>
        </trans-unit>
        <trans-unit id="86f4f24ef6637d895465729e95b5618cb38c12be" translate="yes" xml:space="preserve">
          <source>are learnable parameter vectors of size C (where C is the input size) if &lt;code&gt;affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">are learnable parameter vectors of size C (where C is the input size) if &lt;code&gt;affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; . The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8ff6a70926b2f700851f7fc8c20c594a8e76e9c3" translate="yes" xml:space="preserve">
          <source>are learnable per-channel affine transform parameter vectors of size &lt;code&gt;num_channels&lt;/code&gt; if &lt;code&gt;affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">are learnable per-channel affine transform parameter vectors of size &lt;code&gt;num_channels&lt;/code&gt; if &lt;code&gt;affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; . The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="75bf5f4b4eb9c04180b6f81312057902bb428ee6" translate="yes" xml:space="preserve">
          <source>are returned because the real-to-complex Fourier transform satisfies the conjugate symmetry, i.e.,</source>
          <target state="translated">は実数-複素フーリエ変換が共役対称性を満たすために返されます。</target>
        </trans-unit>
        <trans-unit id="b9344438a3a3542ca4e36035afaf2dc5ecaf2be7" translate="yes" xml:space="preserve">
          <source>are sampled from</source>
          <target state="translated">をサンプリングしています。</target>
        </trans-unit>
        <trans-unit id="2865bbee8d449f1d185c550ec6b590c22b6957d4" translate="yes" xml:space="preserve">
          <source>are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8ee644b4632f03c7ed8cd56a6d8d3b0a6de1f9ba" translate="yes" xml:space="preserve">
          <source>are set to 1 and the elements of</source>
          <target state="translated">の要素を1とし</target>
        </trans-unit>
        <trans-unit id="07c77f1bda818de0208d523f8899b3173d0eae6a" translate="yes" xml:space="preserve">
          <source>are tensors of arbitrary shapes with a total of</source>
          <target state="translated">の合計値を持つ任意の形状のテンソルです。</target>
        </trans-unit>
        <trans-unit id="d1792693a82797275a29dc724b1fdb08fe5cef51" translate="yes" xml:space="preserve">
          <source>are the dimensions of the matrix.</source>
          <target state="translated">は行列の次元です.</target>
        </trans-unit>
        <trans-unit id="eb06c7309fb6a43a67a1cdf5bde8c81dbe8d348d" translate="yes" xml:space="preserve">
          <source>are the input, forget, cell, and output gates, respectively.</source>
          <target state="translated">はそれぞれ入力ゲート、忘却ゲート、セルゲート、出力ゲートである。</target>
        </trans-unit>
        <trans-unit id="1ab04c3136643cd3b0d7e5db6c93dfabb34510d0" translate="yes" xml:space="preserve">
          <source>are the minimum and maximum of the quantized data type.</source>
          <target state="translated">は量子化されたデータ型の最小値と最大値です。</target>
        </trans-unit>
        <trans-unit id="1d1f1537fbffe574251e7df78749efa3f5feee9a" translate="yes" xml:space="preserve">
          <source>are the only supported values.</source>
          <target state="translated">がサポートされている唯一の値です。</target>
        </trans-unit>
        <trans-unit id="8ef8bf39209d2f2fce9681f8054b40e4806ff7f6" translate="yes" xml:space="preserve">
          <source>are the parameters,</source>
          <target state="translated">はパラメータです。</target>
        </trans-unit>
        <trans-unit id="9b2535542a411be559b351451bde244b4f033b2f" translate="yes" xml:space="preserve">
          <source>are the reset, update, and new gates, respectively.</source>
          <target state="translated">はそれぞれリセット、更新、新規ゲートです。</target>
        </trans-unit>
        <trans-unit id="05b98babf9d8d0da7974d92dffc5a3f4ea246a9c" translate="yes" xml:space="preserve">
          <source>are then computed as:</source>
          <target state="translated">として計算されます。</target>
        </trans-unit>
        <trans-unit id="d4665f1e2ffa54be784f9b474fe768613b821365" translate="yes" xml:space="preserve">
          <source>argmax</source>
          <target state="translated">argmax</target>
        </trans-unit>
        <trans-unit id="4538ebc5d50e576bcdd8095d274394500be1d3e0" translate="yes" xml:space="preserve">
          <source>argmin</source>
          <target state="translated">argmin</target>
        </trans-unit>
        <trans-unit id="0a960f0a485739de9ec7e0394dd5e46da6afde09" translate="yes" xml:space="preserve">
          <source>as below</source>
          <target state="translated">以下のように</target>
        </trans-unit>
        <trans-unit id="908b35dcf789b151ab044796ae0006bab35551e6" translate="yes" xml:space="preserve">
          <source>as described above</source>
          <target state="translated">上述の通り</target>
        </trans-unit>
        <trans-unit id="98d3a6b6e515d500e9df2fee1f684162fcc532ee" translate="yes" xml:space="preserve">
          <source>as explicit separate matrices.</source>
          <target state="translated">を明示的な別個の行列として使用することができます。</target>
        </trans-unit>
        <trans-unit id="196302a3f9515657b0c55dab8c474613624309d3" translate="yes" xml:space="preserve">
          <source>as the &lt;code&gt;target&lt;/code&gt; for each value of a 1D tensor of size &lt;code&gt;minibatch&lt;/code&gt;; if &lt;code&gt;ignore_index&lt;/code&gt; is specified, this criterion also accepts this class index (this index may not necessarily be in the class range).</source>
          <target state="translated">as the &lt;code&gt;target&lt;/code&gt; for each value of a 1D tensor of size &lt;code&gt;minibatch&lt;/code&gt; ; if &lt;code&gt;ignore_index&lt;/code&gt; is specified, this criterion also accepts this class index (this index may not necessarily be in the class range).</target>
        </trans-unit>
        <trans-unit id="c3e04b10767441eb9b4b0a57a2d62cbb77d5cdad" translate="yes" xml:space="preserve">
          <source>as vec norm when dim is None</source>
          <target state="translated">dim が None の場合は vec norm として</target>
        </trans-unit>
        <trans-unit id="73688580af35401988ca63c730bbff475def44d3" translate="yes" xml:space="preserve">
          <source>as:</source>
          <target state="translated">as:</target>
        </trans-unit>
        <trans-unit id="6480588dae849f3cd69c8be01b990f84ce70a375" translate="yes" xml:space="preserve">
          <source>as_strided</source>
          <target state="translated">as_strided</target>
        </trans-unit>
        <trans-unit id="f6b3e9a1435d3f432e7a6b9003583871fcb26de3" translate="yes" xml:space="preserve">
          <source>asin</source>
          <target state="translated">asin</target>
        </trans-unit>
        <trans-unit id="0217022a2cbba3beab97d37baf8f0e78d8404ee5" translate="yes" xml:space="preserve">
          <source>asynchronous operation - when &lt;code&gt;async_op&lt;/code&gt; is set to True. The collective operation function returns a distributed request object. In general, you don&amp;rsquo;t need to create it manually and it is guaranteed to support two methods:</source>
          <target state="translated">asynchronous operation - when &lt;code&gt;async_op&lt;/code&gt; is set to True. The collective operation function returns a distributed request object. In general, you don&amp;rsquo;t need to create it manually and it is guaranteed to support two methods:</target>
        </trans-unit>
        <trans-unit id="0366fd6a49c84920645a67d8439d5e2e6075abe4" translate="yes" xml:space="preserve">
          <source>at each cycle iteration.</source>
          <target state="translated">を各サイクルの繰り返しで使用することができます。</target>
        </trans-unit>
        <trans-unit id="f077504d04fa1bc96b042497b434fd33fabdf43d" translate="yes" xml:space="preserve">
          <source>atan</source>
          <target state="translated">atan</target>
        </trans-unit>
        <trans-unit id="12dad2bbce60ee2bc83c285aa39dca395465f305" translate="yes" xml:space="preserve">
          <source>atol</source>
          <target state="translated">atol</target>
        </trans-unit>
        <trans-unit id="cfacbf356c29ecafbd9f709c475179843abdd31e" translate="yes" xml:space="preserve">
          <source>attn_mask: 2D mask</source>
          <target state="translated">attn_mask.2次元マスク</target>
        </trans-unit>
        <trans-unit id="cc13d40f3fdefe8666e6601dc54517fbb0641b8a" translate="yes" xml:space="preserve">
          <source>attn_output:</source>
          <target state="translated">attn_output:</target>
        </trans-unit>
        <trans-unit id="4cac88fb84e0dd512ca9b04f9392aa70146ed77a" translate="yes" xml:space="preserve">
          <source>attn_output_weights:</source>
          <target state="translated">attn_output_weights:</target>
        </trans-unit>
        <trans-unit id="87792e7cfa548e657093e5c536428e801de627fe" translate="yes" xml:space="preserve">
          <source>avg_pool1d</source>
          <target state="translated">avg_pool1d</target>
        </trans-unit>
        <trans-unit id="8454dbf5a788a4fc7b69874c4fe549fff6d01e4a" translate="yes" xml:space="preserve">
          <source>avg_pool2d</source>
          <target state="translated">avg_pool2d</target>
        </trans-unit>
        <trans-unit id="99a1901d4567084ff3e326ca553c35a49cbcc708" translate="yes" xml:space="preserve">
          <source>avg_pool3d</source>
          <target state="translated">avg_pool3d</target>
        </trans-unit>
        <trans-unit id="e9d71f5ee7c92d6dc9e92ffdad17b8bd49418f98" translate="yes" xml:space="preserve">
          <source>b</source>
          <target state="translated">b</target>
        </trans-unit>
        <trans-unit id="ab598131b37324cb1420d88178e195226078e8ec" translate="yes" xml:space="preserve">
          <source>b_{c} = a_{c}\left(k + \frac{\alpha}{n} \sum_{c'=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c'}^2\right)^{-\beta}</source>
          <target state="translated">b_{c}=a_{c}\left(k+\frac{a+n/2)}^{min(N-1,c+n/2)}a_{c'}^2right)^{-beta}</target>
        </trans-unit>
        <trans-unit id="07949b7c55cab1a81db23f348c3409b06fc9ee75" translate="yes" xml:space="preserve">
          <source>baddbmm</source>
          <target state="translated">baddbmm</target>
        </trans-unit>
        <trans-unit id="779455ee3bde8494d9629b353e17b19e92357ba8" translate="yes" xml:space="preserve">
          <source>barrier</source>
          <target state="translated">barrier</target>
        </trans-unit>
        <trans-unit id="1405df66cbe219b0bf6355bc3d60361a8376b6b4" translate="yes" xml:space="preserve">
          <source>base</source>
          <target state="translated">base</target>
        </trans-unit>
        <trans-unit id="115bac8f958d7eaf610aede34fad2562c346e505" translate="yes" xml:space="preserve">
          <source>batch size</source>
          <target state="translated">バッチサイズ</target>
        </trans-unit>
        <trans-unit id="31442bf4302dd7b34e796318f57912b743ccccf0" translate="yes" xml:space="preserve">
          <source>batch1</source>
          <target state="translated">batch1</target>
        </trans-unit>
        <trans-unit id="1d1b6777e9c5a97a6a039ea824d68ee915cc1491" translate="yes" xml:space="preserve">
          <source>batch2</source>
          <target state="translated">batch2</target>
        </trans-unit>
        <trans-unit id="b30c0788a62514c66ada13cce11022313c9e5df0" translate="yes" xml:space="preserve">
          <source>batch_norm</source>
          <target state="translated">batch_norm</target>
        </trans-unit>
        <trans-unit id="793787e7648c00d5d9f56d9f3ee2a1d2314216d8" translate="yes" xml:space="preserve">
          <source>being an orthogonal matrix or batch of orthogonal matrices and</source>
          <target state="translated">直交行列または直交行列の集合であり</target>
        </trans-unit>
        <trans-unit id="b50848b7bdcbc01d3b72f5bb1ce5b7a00a5aad61" translate="yes" xml:space="preserve">
          <source>being an upper triangular matrix or batch of upper triangular matrices.</source>
          <target state="translated">上三角行列または上三角行列のバッチであること。</target>
        </trans-unit>
        <trans-unit id="4928df7f06c7ee461e27540fb51dac9bf4b670ad" translate="yes" xml:space="preserve">
          <source>beta is an optional parameter that defaults to 1.</source>
          <target state="translated">beta はオプションのパラメータで、デフォルトは 1 です。</target>
        </trans-unit>
        <trans-unit id="736c1fbfd886e877040ee4134960bc63ce05262d" translate="yes" xml:space="preserve">
          <source>between two distributions.</source>
          <target state="translated">2つの分布の間に</target>
        </trans-unit>
        <trans-unit id="e1e8289afa6b54e410bc2304308fe48e83a8ef02" translate="yes" xml:space="preserve">
          <source>bias</source>
          <target state="translated">bias</target>
        </trans-unit>
        <trans-unit id="5b74ed2accf3a37671aad25b7b91bed84cad9a6e" translate="yes" xml:space="preserve">
          <source>bias:</source>
          <target state="translated">bias:</target>
        </trans-unit>
        <trans-unit id="a1ceb3e5387c09cb212ce6ee3c3c1a476297155d" translate="yes" xml:space="preserve">
          <source>bilinear</source>
          <target state="translated">bilinear</target>
        </trans-unit>
        <trans-unit id="2e1157f96f8af393a261e4d71a43aa637f30cb60" translate="yes" xml:space="preserve">
          <source>binary answer to whether &lt;code&gt;module&lt;/code&gt; is pruned.</source>
          <target state="translated">&lt;code&gt;module&lt;/code&gt; がプルーニングされているかどうかに対するバイナリ回答。</target>
        </trans-unit>
        <trans-unit id="74c77efd9714f8e8eecdc286f7d29c392001890c" translate="yes" xml:space="preserve">
          <source>binary_cross_entropy</source>
          <target state="translated">binary_cross_entropy</target>
        </trans-unit>
        <trans-unit id="b6c72f603883e86a6cb8021afc246ea5ec1382f5" translate="yes" xml:space="preserve">
          <source>binary_cross_entropy_with_logits</source>
          <target state="translated">binary_cross_entropy_with_logits</target>
        </trans-unit>
        <trans-unit id="0569ae912e387dbc988dee842bf17cdd4e437f63" translate="yes" xml:space="preserve">
          <source>bits</source>
          <target state="translated">bits</target>
        </trans-unit>
        <trans-unit id="b3334e38fd49d5a88ca98cd30878d427a4ddbf06" translate="yes" xml:space="preserve">
          <source>bitshift</source>
          <target state="translated">bitshift</target>
        </trans-unit>
        <trans-unit id="e4e1043a1b984700e67896c1396be3aa962489d5" translate="yes" xml:space="preserve">
          <source>blank=0</source>
          <target state="translated">blank=0</target>
        </trans-unit>
        <trans-unit id="4d46077c78ecf87402848fc9f920b72cde953b02" translate="yes" xml:space="preserve">
          <source>bound</source>
          <target state="translated">bound</target>
        </trans-unit>
        <trans-unit id="7717af821763c86c1249c0045aa20cfbed718b66" translate="yes" xml:space="preserve">
          <source>box AP</source>
          <target state="translated">ボックスAP</target>
        </trans-unit>
        <trans-unit id="aa06eab1f6ebf63aafea56f4652eca504ae7b58f" translate="yes" xml:space="preserve">
          <source>boxes (&lt;code&gt;FloatTensor[N, 4]&lt;/code&gt;): the ground-truth boxes in &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; format, with values between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt;</source>
          <target state="translated">ボックス（ &lt;code&gt;FloatTensor[N, 4]&lt;/code&gt; ）： &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; 形式のグラウンドトゥルースボックス。値は &lt;code&gt;0&lt;/code&gt; から &lt;code&gt;H&lt;/code&gt; 、 &lt;code&gt;0&lt;/code&gt; から &lt;code&gt;W&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="bf13e93de62c26c00d7b1d7846899f810d8062c3" translate="yes" xml:space="preserve">
          <source>boxes (&lt;code&gt;FloatTensor[N, 4]&lt;/code&gt;): the ground-truth boxes in &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; format, with values of &lt;code&gt;x&lt;/code&gt; between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; and values of &lt;code&gt;y&lt;/code&gt; between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt;</source>
          <target state="translated">ボックス（ &lt;code&gt;FloatTensor[N, 4]&lt;/code&gt; ）：グランドトゥルースボックスで &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; の値を有するフォーマット、 &lt;code&gt;x&lt;/code&gt; との間の &lt;code&gt;0&lt;/code&gt; と &lt;code&gt;W&lt;/code&gt; の値と &lt;code&gt;y&lt;/code&gt; の間で &lt;code&gt;0&lt;/code&gt; と &lt;code&gt;H&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4523bad4432d4b614f44a286645e98445b5fdb92" translate="yes" xml:space="preserve">
          <source>boxes (&lt;code&gt;FloatTensor[N, 4]&lt;/code&gt;): the predicted boxes in &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; format, with values between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt;</source>
          <target state="translated">ボックス（ &lt;code&gt;FloatTensor[N, 4]&lt;/code&gt; ）： &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; 形式の予測ボックス、値は &lt;code&gt;0&lt;/code&gt; から &lt;code&gt;H&lt;/code&gt; 、 &lt;code&gt;0&lt;/code&gt; から &lt;code&gt;W&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="99db80efa8d4fb3770cb46ce8d01afcb432bd25a" translate="yes" xml:space="preserve">
          <source>boxes (&lt;code&gt;FloatTensor[N, 4]&lt;/code&gt;): the predicted boxes in &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; format, with values of &lt;code&gt;x&lt;/code&gt; between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; and values of &lt;code&gt;y&lt;/code&gt; between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt;</source>
          <target state="translated">ボックス（ &lt;code&gt;FloatTensor[N, 4]&lt;/code&gt; ）：予測されたボックスに &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; の値を有するフォーマット、 &lt;code&gt;x&lt;/code&gt; との間の &lt;code&gt;0&lt;/code&gt; と &lt;code&gt;W&lt;/code&gt; の値と &lt;code&gt;y&lt;/code&gt; の間で &lt;code&gt;0&lt;/code&gt; と &lt;code&gt;H&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b58ccb7871e0261dfe6de9e4f5274543ebbebeb8" translate="yes" xml:space="preserve">
          <source>broadcast</source>
          <target state="translated">broadcast</target>
        </trans-unit>
        <trans-unit id="8ab478517af17507ee846089a366bb3e5480b24d" translate="yes" xml:space="preserve">
          <source>but slower. Default: &lt;em&gt;False&lt;/em&gt;. See &lt;a href=&quot;https://arxiv.org/pdf/1707.06990.pdf&quot;&gt;&amp;ldquo;paper&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">しかし、遅いです。デフォルト：&lt;em&gt;False&lt;/em&gt;。&lt;a href=&quot;https://arxiv.org/pdf/1707.06990.pdf&quot;&gt;「紙」を&lt;/a&gt;参照</target>
        </trans-unit>
        <trans-unit id="408158643ed564c72fa0921826f8294d71ccbf7c" translate="yes" xml:space="preserve">
          <source>by</source>
          <target state="translated">by</target>
        </trans-unit>
        <trans-unit id="3e8a60d5f4e9887048f1c82dc39ab1673602aa1e" translate="yes" xml:space="preserve">
          <source>by summing the overlapping values. Similar to &lt;a href=&quot;torch.nn.unfold#torch.nn.Unfold&quot;&gt;&lt;code&gt;Unfold&lt;/code&gt;&lt;/a&gt;, the arguments must satisfy</source>
          <target state="translated">重複する値を合計することによって。&lt;a href=&quot;torch.nn.unfold#torch.nn.Unfold&quot;&gt; &lt;code&gt;Unfold&lt;/code&gt; &lt;/a&gt;と同様に、引数は満たす必要があります</target>
        </trans-unit>
        <trans-unit id="84a516841ba77a5b4648de2cd0dfcb30ea46dbb4" translate="yes" xml:space="preserve">
          <source>c</source>
          <target state="translated">c</target>
        </trans-unit>
        <trans-unit id="7ba642b78cae54611d3ce19d3a3b9315a889b623" translate="yes" xml:space="preserve">
          <source>c &amp;gt; 1</source>
          <target state="translated">c&amp;gt; 1</target>
        </trans-unit>
        <trans-unit id="a695701dc8bcc878e55225bb88a2be5e0250b7f9" translate="yes" xml:space="preserve">
          <source>c = (u u^T)^{{-1}} b</source>
          <target state="translated">c=(u u^T)^{{-1}}b</target>
        </trans-unit>
        <trans-unit id="325406a6e81b2954fe93c1849e849987a40ad22c" translate="yes" xml:space="preserve">
          <source>c = (u^T u)^{{-1}} b</source>
          <target state="translated">c=(u^T u)^{{-1}}b</target>
        </trans-unit>
        <trans-unit id="9db33fa0cd63d5bcc994a2d10aee7aa1c6cf5234" translate="yes" xml:space="preserve">
          <source>c = 1</source>
          <target state="translated">c=1</target>
        </trans-unit>
        <trans-unit id="75e4c1ce919ca4246ccdfcf0f90143995bdcf09e" translate="yes" xml:space="preserve">
          <source>c_t</source>
          <target state="translated">c_t</target>
        </trans-unit>
        <trans-unit id="52a612a982a939b9f63856d2f4f4f8c12f2509f8" translate="yes" xml:space="preserve">
          <source>can be adjusted using &lt;code&gt;min_val&lt;/code&gt; and &lt;code&gt;max_val&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;min_val&lt;/code&gt; および &lt;code&gt;max_val&lt;/code&gt; を使用して調整できます。</target>
        </trans-unit>
        <trans-unit id="4d83e8bdc70262378c30c2dddd5537deeb8a96c5" translate="yes" xml:space="preserve">
          <source>can be avoided if one sets &lt;code&gt;reduction = 'sum'&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;reduction = 'sum'&lt;/code&gt; 設定すれば回避できます。</target>
        </trans-unit>
        <trans-unit id="90bd565efaf349a2fc8e4dc779050d0325ec50a1" translate="yes" xml:space="preserve">
          <source>can be avoided if sets &lt;code&gt;reduction = 'sum'&lt;/code&gt;.</source>
          <target state="translated">セット &lt;code&gt;reduction = 'sum'&lt;/code&gt; 場合、回避できます。</target>
        </trans-unit>
        <trans-unit id="3c65ddc9ba0e2d191602d3c173ef6a188ce419ee" translate="yes" xml:space="preserve">
          <source>can be precisely described as:</source>
          <target state="translated">と正確に記述することができます。</target>
        </trans-unit>
        <trans-unit id="8634627c53b593a107a2ce1c2b724bc47ea5e431" translate="yes" xml:space="preserve">
          <source>can be used to compute normalized (unit length) eigenvectors of corresponding eigenvalues as follows. If the corresponding &lt;code&gt;eigenvalues[j]&lt;/code&gt; is a real number, column &lt;code&gt;eigenvectors[:, j]&lt;/code&gt; is the eigenvector corresponding to &lt;code&gt;eigenvalues[j]&lt;/code&gt;. If the corresponding &lt;code&gt;eigenvalues[j]&lt;/code&gt; and &lt;code&gt;eigenvalues[j + 1]&lt;/code&gt; form a complex conjugate pair, then the true eigenvectors can be computed as</source>
          <target state="translated">次のように、対応する固有値の正規化された（単位長）固有ベクトルを計算するために使用できます。対応する &lt;code&gt;eigenvalues[j]&lt;/code&gt; が実数の場合、列 &lt;code&gt;eigenvectors[:, j]&lt;/code&gt; は &lt;code&gt;eigenvalues[j]&lt;/code&gt; 対応する固有ベクトルです。対応する &lt;code&gt;eigenvalues[j]&lt;/code&gt; と &lt;code&gt;eigenvalues[j + 1]&lt;/code&gt; が複素共役ペアを形成する場合、真の固有ベクトルは次のように計算できます。</target>
        </trans-unit>
        <trans-unit id="9d989e8d27dc9e0ec3389fc855f142c3d40f0c50" translate="yes" xml:space="preserve">
          <source>cat</source>
          <target state="translated">cat</target>
        </trans-unit>
        <trans-unit id="613af80c25dfbc9ef75ce605280571b9a518d632" translate="yes" xml:space="preserve">
          <source>ceil</source>
          <target state="translated">ceil</target>
        </trans-unit>
        <trans-unit id="8cc51bda059df43f07da456966946b85e1e39fed" translate="yes" xml:space="preserve">
          <source>celu</source>
          <target state="translated">celu</target>
        </trans-unit>
        <trans-unit id="a4ab4f21160cf5b0a8b70fd62605f209cd8eb378" translate="yes" xml:space="preserve">
          <source>clamp</source>
          <target state="translated">clamp</target>
        </trans-unit>
        <trans-unit id="0b03af5027f5174c31ba5792d65334dc4b86d0c4" translate="yes" xml:space="preserve">
          <source>clamp_max</source>
          <target state="translated">clamp_max</target>
        </trans-unit>
        <trans-unit id="c71e6f6650b2c00996a576acfebb2e5c2c9133bc" translate="yes" xml:space="preserve">
          <source>clamp_min</source>
          <target state="translated">clamp_min</target>
        </trans-unit>
        <trans-unit id="af6ab064a94fb3e9fd590a99e474c2ed6fe59e97" translate="yes" xml:space="preserve">
          <source>clear()</source>
          <target state="translated">clear()</target>
        </trans-unit>
        <trans-unit id="dde02ec247dcb158d65d0da1bdd9301502952f4f" translate="yes" xml:space="preserve">
          <source>clip_value</source>
          <target state="translated">clip_value</target>
        </trans-unit>
        <trans-unit id="5dc3fded50a44a781130cc74e32147cfb27fb2b1" translate="yes" xml:space="preserve">
          <source>colors:</source>
          <target state="translated">colors:</target>
        </trans-unit>
        <trans-unit id="ec3d37cfb39e6ddfa966aa2a0e1cc364f09c1aac" translate="yes" xml:space="preserve">
          <source>columns (when specified) or &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">列（指定されている場合）または &lt;code&gt;1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d8f2c317f524276e700bfcb20a9ddc27b06eedc5" translate="yes" xml:space="preserve">
          <source>columns of &lt;code&gt;input&lt;/code&gt; are linearly independent. This behavior will propably change once QR supports pivoting.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 列は線形独立です。QRがピボットをサポートすると、この動作は適切に変更されます。</target>
        </trans-unit>
        <trans-unit id="a3ed4cce473629e46512b31377e0c26f8c611bf9" translate="yes" xml:space="preserve">
          <source>columns represent the principal directions</source>
          <target state="translated">柱は主な方向を表します。</target>
        </trans-unit>
        <trans-unit id="805682fdf2befd0f72b7d16894fd58f0d033c065" translate="yes" xml:space="preserve">
          <source>columns.</source>
          <target state="translated">columns.</target>
        </trans-unit>
        <trans-unit id="49ba358c3272c2db40fc6ab2c103669678628b68" translate="yes" xml:space="preserve">
          <source>concat</source>
          <target state="translated">concat</target>
        </trans-unit>
        <trans-unit id="afad9a69767e7c24ecda5af9187202e1b4b070b2" translate="yes" xml:space="preserve">
          <source>condition</source>
          <target state="translated">condition</target>
        </trans-unit>
        <trans-unit id="f607ae1e89eb5c8e88a5312b0347f07cd629f09e" translate="yes" xml:space="preserve">
          <source>conduct; niter must be a nonnegative integer, and defaults to 2</source>
          <target state="translated">niter は非負の整数でなければならず、デフォルトは 2 です。</target>
        </trans-unit>
        <trans-unit id="0713af050ff5d1185e252e762af4f2837f6692ed" translate="yes" xml:space="preserve">
          <source>containing the window</source>
          <target state="translated">ウィンドウを含む</target>
        </trans-unit>
        <trans-unit id="d75270e5e6de30e18e4c2d9c2c4833c4b41b71ea" translate="yes" xml:space="preserve">
          <source>contains the eigenvalues of</source>
          <target state="translated">の固有値を含みます。</target>
        </trans-unit>
        <trans-unit id="edf1afc933ef93b4de4eecc542db49d0c5ac72fd" translate="yes" xml:space="preserve">
          <source>contains the solution. If</source>
          <target state="translated">が解決策を含んでいることを示しています。もし</target>
        </trans-unit>
        <trans-unit id="963bb6a82e1a90627d6c03382ec4d1cf449f21be" translate="yes" xml:space="preserve">
          <source>conv1d</source>
          <target state="translated">conv1d</target>
        </trans-unit>
        <trans-unit id="6c6a564ef490d5000c80c05e48c88c09ac2644d2" translate="yes" xml:space="preserve">
          <source>conv2d</source>
          <target state="translated">conv2d</target>
        </trans-unit>
        <trans-unit id="37994178ccdc1cb8e941708bda9ab3d1d68ab700" translate="yes" xml:space="preserve">
          <source>conv3d</source>
          <target state="translated">conv3d</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
