<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="d4e46a5f1bb78af76b3523ee33b19eb9c786ce08" translate="yes" xml:space="preserve">
          <source>A common practice for evaluating the results of image denoising is by looking at the difference between the reconstruction and the original image. If the reconstruction is perfect this will look like Gaussian noise.</source>
          <target state="translated">画像のノイズ除去の結果を評価するための一般的な方法は、再構成と元の画像の違いを見ることです。再構成が完璧であれば、これはガウスノイズのように見えます。</target>
        </trans-unit>
        <trans-unit id="4ed0a0436668c33a0351f627732f0587ac7983ed" translate="yes" xml:space="preserve">
          <source>A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.</source>
          <target state="translated">合成データセット上での scikit-learn のいくつかの分類器の比較。この例のポイントは、異なる分類器の決定境界の性質を説明することです。これらの例で伝えられた直感が必ずしも実際のデータセットに反映されるとは限らないので、これは大目に見てください。</target>
        </trans-unit>
        <trans-unit id="08896462e97fc3765d56051f9cf494df2d13585e" translate="yes" xml:space="preserve">
          <source>A comparison of different values for regularization parameter &amp;lsquo;alpha&amp;rsquo; on synthetic datasets. The plot shows that different alphas yield different decision functions.</source>
          <target state="translated">合成データセットの正則化パラメーター「alpha」のさまざまな値の比較。プロットは、異なるアルファが異なる決定関数を生成することを示しています。</target>
        </trans-unit>
        <trans-unit id="5ffdb4122629b0408fc648de5eb2e8856b001ee3" translate="yes" xml:space="preserve">
          <source>A comparison of the clustering algorithms in scikit-learn</source>
          <target state="translated">scikit-learn のクラスタリングアルゴリズムの比較</target>
        </trans-unit>
        <trans-unit id="3aba9c608b9764b22ebdea01be3ad9e7396b0707" translate="yes" xml:space="preserve">
          <source>A comparison of the outlier detection algorithms in scikit-learn. Local Outlier Factor (LOF) does not show a decision boundary in black as it has no predict method to be applied on new data when it is used for outlier detection.</source>
          <target state="translated">scikit-learnにおける外れ値検出アルゴリズムの比較です。Local Outlier Factor (LOF)は、外れ値検出に使用した場合、新しいデータに適用する予測方法がないため、決定境界が黒く表示されません。</target>
        </trans-unit>
        <trans-unit id="d90a43f9bc5a8cfdf7c44591b758b4103cd4b78a" translate="yes" xml:space="preserve">
          <source>A complete example of this classification problem is available as an example that you can run and study: &lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;Recognizing hand-written digits&lt;/a&gt;.</source>
          <target state="translated">この分類問題の完全な例は、実行して学習できる例として利用でき&lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;ます&lt;/a&gt;。手書き数字の認識。</target>
        </trans-unit>
        <trans-unit id="5fa8afb91b45355653b6f013837f985ec55a809c" translate="yes" xml:space="preserve">
          <source>A constant prediction baseline</source>
          <target state="translated">一定の予測ベースライン</target>
        </trans-unit>
        <trans-unit id="4711e9024cec2c4a1383670cb0218b599982c096" translate="yes" xml:space="preserve">
          <source>A context object for caching a function&amp;rsquo;s return value each time it is called with the same input arguments.</source>
          <target state="translated">同じ入力引数で呼び出されるたびに関数の戻り値をキャッシュするためのコンテキストオブジェクト。</target>
        </trans-unit>
        <trans-unit id="198c09de283f0c6001f8ae5be18eb83356d81a3a" translate="yes" xml:space="preserve">
          <source>A contiguous slice of distance matrix, optionally processed by &lt;code&gt;reduce_func&lt;/code&gt;.</source>
          <target state="translated">オプションで &lt;code&gt;reduce_func&lt;/code&gt; によって処理される、距離行列の連続スライス。</target>
        </trans-unit>
        <trans-unit id="2ac4a68a26c3c2eae7c82c7be7376e0b1206d26c" translate="yes" xml:space="preserve">
          <source>A contingency matrix given by the &lt;code&gt;contingency_matrix&lt;/code&gt; function. If value is &lt;code&gt;None&lt;/code&gt;, it will be computed, otherwise the given value is used, with &lt;code&gt;labels_true&lt;/code&gt; and &lt;code&gt;labels_pred&lt;/code&gt; ignored.</source>
          <target state="translated">&lt;code&gt;contingency_matrix&lt;/code&gt; 関数によって与えられる偶発的な行列。値が &lt;code&gt;None&lt;/code&gt; の場合は計算されます。それ以外の場合は、指定された値が使用され、 &lt;code&gt;labels_true&lt;/code&gt; と &lt;code&gt;labels_pred&lt;/code&gt; は無視されます。</target>
        </trans-unit>
        <trans-unit id="e9c011a87c5032bcc5be043ba28885f86dd1572a" translate="yes" xml:space="preserve">
          <source>A continuous log-uniform random variable is available through &lt;code&gt;loguniform&lt;/code&gt;. This is a continuous version of log-spaced parameters. For example to specify &lt;code&gt;C&lt;/code&gt; above, &lt;code&gt;loguniform(1,
100)&lt;/code&gt; can be used instead of &lt;code&gt;[1, 10, 100]&lt;/code&gt; or &lt;code&gt;np.logspace(0, 2,
num=1000)&lt;/code&gt;. This is an alias to SciPy&amp;rsquo;s &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html&quot;&gt;stats.reciprocal&lt;/a&gt;.</source>
          <target state="translated">連続対数一様確率変数は、 &lt;code&gt;loguniform&lt;/code&gt; を通じて利用できます。これは、対数間隔のパラメーターの連続バージョンです。たとえば、上記の &lt;code&gt;C&lt;/code&gt; を指定するには、 &lt;code&gt;[1, 10, 100]&lt;/code&gt; &lt;code&gt;np.logspace(0, 2, num=1000)&lt;/code&gt; ]またはnp.logspace（0、2、num = 1000）の代わりに &lt;code&gt;loguniform(1, 100)&lt;/code&gt; を使用できます。これは、SciPyの&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html&quot;&gt;stats.reciprocal&lt;/a&gt;のエイリアスです。</target>
        </trans-unit>
        <trans-unit id="c54ec3e229cd991b7b39939aa120ad2133da3723" translate="yes" xml:space="preserve">
          <source>A copy of the &lt;code&gt;classes&lt;/code&gt; parameter where provided, or otherwise, the sorted set of classes found when fitting.</source>
          <target state="translated">提供されている場合は、 &lt;code&gt;classes&lt;/code&gt; パラメーターのコピー。それ以外の場合は、フィッティング時に見つかったソートされたクラスのセット。</target>
        </trans-unit>
        <trans-unit id="0460778fea705652baf3b5d397ac145e2c8d559f" translate="yes" xml:space="preserve">
          <source>A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.</source>
          <target state="translated">したがって、文書のコーパスは、文書ごとに1行、コーパスに含まれるトークン(単語など)ごとに1列の行列で表すことができます。</target>
        </trans-unit>
        <trans-unit id="b1d91a72335bd05329f277d8169a200f3a7e7461" translate="yes" xml:space="preserve">
          <source>A cross-validation generator splits the whole dataset k times in training and test data. Subsets of the training set with varying sizes will be used to train the estimator and a score for each training subset size and the test set will be computed. Afterwards, the scores will be averaged over all k runs for each training subset size.</source>
          <target state="translated">クロスバリデーション生成器は、データセット全体をk回に分けて、訓練データとテストデータに分割する。サイズの異なる訓練集合の部分集合が、推定器を訓練するために使用され、各訓練部分集合のサイズとテスト集合のスコアが計算される。その後、スコアは、各訓練サブセットサイズについて、すべてのk回の実行にわたって平均化される。</target>
        </trans-unit>
        <trans-unit id="2e60520122aededd78427759b8d5e2ce36a7e309" translate="yes" xml:space="preserve">
          <source>A dataset is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the &lt;code&gt;.data&lt;/code&gt; member, which is a &lt;code&gt;n_samples, n_features&lt;/code&gt; array. In the case of supervised problem, one or more response variables are stored in the &lt;code&gt;.target&lt;/code&gt; member. More details on the different datasets can be found in the &lt;a href=&quot;../../datasets/index#datasets&quot;&gt;dedicated section&lt;/a&gt;.</source>
          <target state="translated">データセットは、すべてのデータとデータに関する一部のメタデータを保持する辞書のようなオブジェクトです。このデータは、 &lt;code&gt;n_samples, n_features&lt;/code&gt; 配列である &lt;code&gt;.data&lt;/code&gt; メンバーに格納されます。監視対象の問題の場合、1つ以上の応答変数が &lt;code&gt;.target&lt;/code&gt; メンバーに格納されます。さまざまなデータセットの詳細については、&lt;a href=&quot;../../datasets/index#datasets&quot;&gt;専用セクションを&lt;/a&gt;ご覧ください。</target>
        </trans-unit>
        <trans-unit id="c689c278ed2c402419ef82b79e626634acb9d7dd" translate="yes" xml:space="preserve">
          <source>A dataset is uniquely specified by its &lt;code&gt;data_id&lt;/code&gt;, but not necessarily by its name. Several different &amp;ldquo;versions&amp;rdquo; of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that &lt;code&gt;fetch_openml(name=&quot;miceprotein&quot;)&lt;/code&gt; can yield different results at different times if earlier versions become inactive. You can see that the dataset with &lt;code&gt;data_id&lt;/code&gt; 40966 that we fetched above is the version 1 of the &amp;ldquo;miceprotein&amp;rdquo; dataset:</source>
          <target state="translated">データセットは &lt;code&gt;data_id&lt;/code&gt; によって一意に指定されますが、必ずしも名前で指定されるとは限りません。まったく異なるデータセットを含むことができる、同じ名前のデータセットのいくつかの異なる「バージョン」が存在する可能性があります。データセットの特定のバージョンに重大な問題が含まれていることが判明した場合、それは非アクティブ化されている可能性があります。名前を使用してデータセットを指定すると、まだアクティブなデータセットの最も古いバージョンが生成されます。つまり、以前のバージョンが非アクティブになった場合、 &lt;code&gt;fetch_openml(name=&quot;miceprotein&quot;)&lt;/code&gt; は異なる時点で異なる結果を生成する可能性があります。上記でフェッチした &lt;code&gt;data_id&lt;/code&gt; 40966 のデータセットは、「マウスタンパク質」データセットのバージョン1であることがわかります。</target>
        </trans-unit>
        <trans-unit id="015b04be42703b2ddb8ba533bada1a317c896a28" translate="yes" xml:space="preserve">
          <source>A decision tree classifier.</source>
          <target state="translated">決定木分類器。</target>
        </trans-unit>
        <trans-unit id="2630b9dcfdbd7604846f7a233491e190f45a9740" translate="yes" xml:space="preserve">
          <source>A decision tree is boosted using the AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail.</source>
          <target state="translated">決定木はAdaBoost.R2用いて昇圧される&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;ガウスノイズの少量1D正弦波データセットにアルゴリズム。299ブースト（300デシジョンツリー）は、単一のデシジョンツリーリグレッサと比較されます。ブーストの数が増えると、リグレッサはより詳細にフィットするようになります。</target>
        </trans-unit>
        <trans-unit id="c1457482037e3da549c4d599a20c71477d87290a" translate="yes" xml:space="preserve">
          <source>A decision tree is boosted using the AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail.</source>
          <target state="translated">AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;アルゴリズムを使用して、少量のガウスノイズのある1D正弦波データセットで決定木がブーストされます。 299のブースト（300の決定木）が単一の決定木リグレッサと比較されます。ブーストの数が増えると、リグレッサはより詳細に適合します。</target>
        </trans-unit>
        <trans-unit id="98423fef8caf500459be69ac8cff91bec1b557c8" translate="yes" xml:space="preserve">
          <source>A decision tree regressor.</source>
          <target state="translated">決定木回帰器。</target>
        </trans-unit>
        <trans-unit id="fac54dbce7b4a9529cb6b07cf132d857e0fb7a2e" translate="yes" xml:space="preserve">
          <source>A demo of K-Means clustering on the handwritten digits data</source>
          <target state="translated">手書きの数字データを用いたK-Meansクラスタリングのデモ</target>
        </trans-unit>
        <trans-unit id="985d8495d6de7661e6b4b5a0919641f87e6bc3f6" translate="yes" xml:space="preserve">
          <source>A demo of structured Ward hierarchical clustering on an image of coins</source>
          <target state="translated">コインの画像上での構造化されたWard階層的クラスタリングのデモ</target>
        </trans-unit>
        <trans-unit id="728da4fce5aa5c78dc45e16c348781662170d8aa" translate="yes" xml:space="preserve">
          <source>A demo of the Spectral Biclustering algorithm</source>
          <target state="translated">Spectral Biclustering アルゴリズムのデモ</target>
        </trans-unit>
        <trans-unit id="9435014b37ab8720ce8e8dead6831ddaa2609878" translate="yes" xml:space="preserve">
          <source>A demo of the Spectral Co-Clustering algorithm</source>
          <target state="translated">Spectral Co-Clustering アルゴリズムのデモ</target>
        </trans-unit>
        <trans-unit id="35f2c0aaf840683641795d09aa0a1e201f37f9de" translate="yes" xml:space="preserve">
          <source>A demo of the mean-shift clustering algorithm</source>
          <target state="translated">平均シフトクラスタリングアルゴリズムのデモ</target>
        </trans-unit>
        <trans-unit id="d3781231daa22a497eef6f674b9f3cd7b216bf2a" translate="yes" xml:space="preserve">
          <source>A demonstration of feature discretization on synthetic classification datasets. Feature discretization decomposes each feature into a set of bins, here equally distributed in width. The discrete values are then one-hot encoded, and given to a linear classifier. This preprocessing enables a non-linear behavior even though the classifier is linear.</source>
          <target state="translated">合成分類データに対する特徴量離散化のデモです。特徴離散化では、各特徴をビンの集合(ここでは幅が等しく分散しています)に分解します。そして、離散的な値をワンショット符号化し、線形分類器に与えます。この前処理により、線形分類器が線形であっても非線形な振る舞いが可能になります。</target>
        </trans-unit>
        <trans-unit id="95eced0791e7dfa2447624723a21d4f3bf963a74" translate="yes" xml:space="preserve">
          <source>A detailed description of the algorithm can be found in the documentation of the &lt;code&gt;linear_model&lt;/code&gt; sub-package.</source>
          <target state="translated">アルゴリズムの詳細な説明は、 &lt;code&gt;linear_model&lt;/code&gt; サブパッケージのドキュメントに記載されています。</target>
        </trans-unit>
        <trans-unit id="85ef0d1a4425c2a7d76f392327f60a1b7513d0f1" translate="yes" xml:space="preserve">
          <source>A dict of arrays containing the score/time arrays for each scorer is returned. The possible keys for this &lt;code&gt;dict&lt;/code&gt; are:</source>
          <target state="translated">各スコアラーのスコア/時間配列を含む配列のdictが返されます。この &lt;code&gt;dict&lt;/code&gt; の可能なキーは次のとおりです。</target>
        </trans-unit>
        <trans-unit id="111521e1b62dc92a5d55cc421f659d37509b3d83" translate="yes" xml:space="preserve">
          <source>A dict with keys as column headers and values as columns, that can be imported into a pandas &lt;code&gt;DataFrame&lt;/code&gt;.</source>
          <target state="translated">列ヘッダーとしてのキーと列としての値を持つdict 。これは &lt;code&gt;DataFrame&lt;/code&gt; インポートできます。</target>
        </trans-unit>
        <trans-unit id="c8159c56c705a647167c0db6b7eec6aec52babc5" translate="yes" xml:space="preserve">
          <source>A dictionary mapping feature names to feature indices.</source>
          <target state="translated">特徴名と特徴インデックスを対応付けた辞書。</target>
        </trans-unit>
        <trans-unit id="d69a0d863960291435690723c91aee6afd547cfb" translate="yes" xml:space="preserve">
          <source>A dictionary of {dataset_name: data_dict}, or {dataset_name: (data_dict, ordering). &lt;code&gt;data_dict&lt;/code&gt; itself is a dictionary of {column_name: data_array}, and &lt;code&gt;ordering&lt;/code&gt; is a list of column_names to determine the ordering in the data set (see &lt;code&gt;fake_mldata&lt;/code&gt; for details).</source>
          <target state="translated">{dataset_name：data_dict}、または{dataset_name：（data_dict、ordering）の辞書。 &lt;code&gt;data_dict&lt;/code&gt; 自体は{column_name：data_array}のディクショナリであり、 &lt;code&gt;ordering&lt;/code&gt; はデータセット内の順序を決定するcolumn_namesのリストです（詳細については、 &lt;code&gt;fake_mldata&lt;/code&gt; を参照してください）。</target>
        </trans-unit>
        <trans-unit id="f0df6656a799b1b490376f17c246c83878449378" translate="yes" xml:space="preserve">
          <source>A different approach for approximating an additive variant of the chi squared kernel.</source>
          <target state="translated">カイ2乗カーネルの加法変量を近似するための異なるアプローチ。</target>
        </trans-unit>
        <trans-unit id="183fe0e3f615bfbf70ad6cec7cbf647c2be26b19" translate="yes" xml:space="preserve">
          <source>A discrepancy between the number of terms reported for DictVectorizer and for FeatureHasher is to be expected due to hash collisions.</source>
          <target state="translated">DictVectorizerで報告された項数とFeatureHasherで報告された項数の間には、ハッシュの衝突による不一致が予想されます。</target>
        </trans-unit>
        <trans-unit id="5359a8a92b18a35ca1595de7bb20b2a0c7e2882d" translate="yes" xml:space="preserve">
          <source>A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then D_{i, j} is the distance between the ith array from X and the jth array from Y.</source>
          <target state="translated">Y が None でない場合,D_{i,j}が与えられた行列 X の i 番目のベクトルと j 番目のベクトルの間の距離であるような距離行列 D.Y が None でない場合,D_{i,j}は,X からの i 番目の配列と Y からの j 番目の配列の間の距離である.</target>
        </trans-unit>
        <trans-unit id="91104d203f36a74818ec42e8962e98d2f258acd0" translate="yes" xml:space="preserve">
          <source>A document is a sequence of \(N\) words.</source>
          <target state="translated">A document is a sequence of \(N\)words.</target>
        </trans-unit>
        <trans-unit id="ce7ab60e9677f7708dd8a607942d2c0cccce5364" translate="yes" xml:space="preserve">
          <source>A feature array, or array of distances between samples if &lt;code&gt;metric='precomputed'&lt;/code&gt;.</source>
          <target state="translated">機能配列、または &lt;code&gt;metric='precomputed'&lt;/code&gt; の場合はサンプル間の距離の配列。</target>
        </trans-unit>
        <trans-unit id="b2db61a5e11f04d01b2535f105cf88de339d5444" translate="yes" xml:space="preserve">
          <source>A feature array, or array of distances between samples if metric=&amp;rsquo;precomputed&amp;rsquo;</source>
          <target state="translated">特徴配列、またはmetric = 'precomputed'の場合はサンプル間の距離の配列</target>
        </trans-unit>
        <trans-unit id="eb559e312d64d98c335a9c4be5e943c082d9b127" translate="yes" xml:space="preserve">
          <source>A feature array, or array of distances between samples if metric=&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="translated">特徴配列、またはmetric = 'precomputed'の場合はサンプル間の距離の配列。</target>
        </trans-unit>
        <trans-unit id="798664f5f45f763ed33c1b77b763944574aa4f4f" translate="yes" xml:space="preserve">
          <source>A few definitions:</source>
          <target state="translated">いくつかの定義があります。</target>
        </trans-unit>
        <trans-unit id="86908273dab820320f981714d4826279093534cf" translate="yes" xml:space="preserve">
          <source>A few definitions: a &lt;em&gt;claim&lt;/em&gt; is the request made by a policyholder to the insurer to compensate for a loss covered by the insurance. The &lt;em&gt;claim amount&lt;/em&gt; is the amount of money that the insurer must pay. The &lt;em&gt;exposure&lt;/em&gt; is the duration of the insurance coverage of a given policy, in years.</source>
          <target state="translated">いくつかの定義：&lt;em&gt;請求&lt;/em&gt;とは、保険の対象となる損失を補償するために保険契約者が保険会社に対して行う要求です。&lt;em&gt;請求額は、&lt;/em&gt;保険者が支払わなければならない金額です。&lt;em&gt;露出は&lt;/em&gt;年間で、与えられた政策の保険の期間です。</target>
        </trans-unit>
        <trans-unit id="24d02e76c875ea70e2d1746a2ae2b7543f98dedf" translate="yes" xml:space="preserve">
          <source>A few features available in this model:</source>
          <target state="translated">このモデルに搭載されている機能をいくつかご紹介します。</target>
        </trans-unit>
        <trans-unit id="a596b46f1e97e6f01add8ad1641f2aa816638c4f" translate="yes" xml:space="preserve">
          <source>A figure object onto which the plots will be drawn, after the figure has been cleared. By default, a new one is created.</source>
          <target state="translated">図がクリアされた後、その上にプロットが描かれる図オブジェクト。デフォルトでは、新しい図が作成されます。</target>
        </trans-unit>
        <trans-unit id="24026427c908afb044a16ec627f1f7bf5b040d61" translate="yes" xml:space="preserve">
          <source>A fitted estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;, or &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;. Multioutput-multiclass classifiers are not supported.</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;予測&lt;/a&gt;、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;、または&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_functionを&lt;/a&gt;実装する近似推定量オブジェクト。マルチ出力-マルチクラス分類子はサポートされていません。</target>
        </trans-unit>
        <trans-unit id="06304e4581de12feb43d312c564ae9a70106b9ba" translate="yes" xml:space="preserve">
          <source>A fitted gradient boosting model.</source>
          <target state="translated">フィットした勾配昇圧モデル。</target>
        </trans-unit>
        <trans-unit id="ad78d890244343cb481ec0d46fb95ae01e19e178" translate="yes" xml:space="preserve">
          <source>A function to handle preprocessing, tokenization and n-grams generation.</source>
          <target state="translated">前処理、トークン化、n-gram生成を処理する関数。</target>
        </trans-unit>
        <trans-unit id="5c6e87ebdac3e8a70f2ac508aaddfaa8df544da7" translate="yes" xml:space="preserve">
          <source>A function to preprocess the text before tokenization.</source>
          <target state="translated">トークン化の前にテキストを前処理する関数。</target>
        </trans-unit>
        <trans-unit id="004f6b369bbbaaeac6e581b99b63af10868df162" translate="yes" xml:space="preserve">
          <source>A function to split a string into a sequence of tokens.</source>
          <target state="translated">文字列をトークンの列に分割する関数。</target>
        </trans-unit>
        <trans-unit id="70c3f4d0379ae967616e2ce28baef3bbea4be9cf" translate="yes" xml:space="preserve">
          <source>A generator over parameter settings, constructed from param_distributions.</source>
          <target state="translated">param_distributionsから構築されたパラメータ設定のジェネレーター。</target>
        </trans-unit>
        <trans-unit id="ab5af31ec50522e8f84b5fd320b5185d1e649aae" translate="yes" xml:space="preserve">
          <source>A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning</source>
          <target state="translated">ベイズ法の良い紹介は、C.Bishopに記載されています。パターン認識と機械学習</target>
        </trans-unit>
        <trans-unit id="2abfd97b78d2efa9a468ce9e53e4e40c6a740a0c" translate="yes" xml:space="preserve">
          <source>A good value reported by this method does not imply the best information retrieval.</source>
          <target state="translated">この方法で報告された良好な値は、最良の情報検索を意味するものではありません。</target>
        </trans-unit>
        <trans-unit id="ee0139a4c757902f8a15e776dbb86ec75b8b58ee" translate="yes" xml:space="preserve">
          <source>A graphical overview of basic areas of machine learning, and guidance which kind of algorithms to use in a given situation.</source>
          <target state="translated">機械学習の基本的な領域をグラフィカルに概観し、与えられた状況でどのようなアルゴリズムを使用するべきかをガイダンスする。</target>
        </trans-unit>
        <trans-unit id="09adb6eadfd2176464d83487c19995d3ec4f7162" translate="yes" xml:space="preserve">
          <source>A histogram is a simple visualization of data where bins are defined, and the number of data points within each bin is tallied. An example of a histogram can be seen in the upper-left panel of the following figure:</source>
          <target state="translated">ヒストグラムは、ビンが定義され、各ビン内のデータ・ポイントの数が集計されるデータの単純な可視化です。ヒストグラムの例は、次の図の左上のパネルで見ることができます。</target>
        </trans-unit>
        <trans-unit id="7bef6839991533f8c855219658630fb1d97c3e13" translate="yes" xml:space="preserve">
          <source>A kernel between the gene sequences is defined using R-convolution &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; by integrating a binary letter-wise kernel over all pairs of letters among a pair of strings.</source>
          <target state="translated">遺伝子配列間のカーネルは、R-convolution &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;を使用して、文字列のペアの中の文字のすべてのペアにわたってバイナリ文字ごとのカーネルを統合することによって定義されます。</target>
        </trans-unit>
        <trans-unit id="9d7d25ec6ddbab84b2f2e99ec27b1c5e75159286" translate="yes" xml:space="preserve">
          <source>A kernel hyperparameter&amp;rsquo;s specification in form of a namedtuple.</source>
          <target state="translated">名前付きタプルの形式でのカーネルハイパーパラメータの仕様。</target>
        </trans-unit>
        <trans-unit id="9dd2e21d515f945fa766366930e3f2f884774833" translate="yes" xml:space="preserve">
          <source>A kernel matrix K such that K_{i, j} is the kernel between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then K_{i, j} is the kernel between the ith array from X and the jth array from Y.</source>
          <target state="translated">Y が None でない場合、K_{i,j}が与えられた行列 X の i 番目のベクトルと j 番目のベクトルの間のカーネルであるようなカーネル行列 K。Y が None でない場合,K_{i,j}は,X からの i 番目の配列と Y からの j 番目の配列の間のカーネルである.</target>
        </trans-unit>
        <trans-unit id="1c832d0ad2e131abf1539cfeafad1048954835e4" translate="yes" xml:space="preserve">
          <source>A larger &lt;code&gt;leaf_size&lt;/code&gt; leads to a faster tree construction time, because fewer nodes need to be created</source>
          <target state="translated">&lt;code&gt;leaf_size&lt;/code&gt; を大きくすると、作成する必要のあるノードが少なくなるため、ツリーの構築時間が短縮されます。</target>
        </trans-unit>
        <trans-unit id="5116c45ec5d86e0d701daa6fab158df33c292bf3" translate="yes" xml:space="preserve">
          <source>A larger number of split will provide no benefits if the number of training samples is large enough. Indeed, the training time will increase. &lt;code&gt;cv&lt;/code&gt; is not used for model evaluation but for prediction.</source>
          <target state="translated">トレーニングサンプルの数が十分に多い場合、分割数を増やしてもメリットはありません。確かに、トレーニング時間は増加します。 &lt;code&gt;cv&lt;/code&gt; はモデルの評価ではなく、予測に使用されます。</target>
        </trans-unit>
        <trans-unit id="9f498b24974153e962b445930a2302cf78d89cfc" translate="yes" xml:space="preserve">
          <source>A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode.</source>
          <target state="translated">最後の主要なパラメータは、バルクまたはワンアットタイムモードでの予測が可能であることです。</target>
        </trans-unit>
        <trans-unit id="b50d9ba875cfa3e1bbfa88757252d5cc5b663f5e" translate="yes" xml:space="preserve">
          <source>A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. Consider the following example where we plot the learning curve of a naive Bayes classifier and an SVM.</source>
          <target state="translated">学習曲線は、訓練サンプル数を変化させた場合の推定器の検証スコアと訓練スコアを示します。これは、より多くの訓練データを追加することでどれだけの利益が得られるか、また、推定器が分散誤差やバイアス誤差のどちらに苦しんでいるかを知るためのツールです。ナイーブベイズ分類器とSVMの学習曲線をプロットした次の例を考えてみましょう。</target>
        </trans-unit>
        <trans-unit id="7ca8b138a43ce55d825b3e612cada7898d228b4c" translate="yes" xml:space="preserve">
          <source>A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data. In the following plot you can see an example: naive Bayes roughly converges to a low score.</source>
          <target state="translated">学習曲線は、訓練サンプル数を変化させた場合の推定器の検証スコアと訓練スコアを示します。これは、訓練データを追加することでどの程度の利益が得られるか、また、推定器が分散誤差またはバイアス誤差のどちらに苦しむかを調べるためのツールです。検証スコアと訓練スコアの両方が、訓練セットのサイズが大きくなるにつれて低すぎる値に収束する場合、訓練データを増やしてもあまりメリットはありません。次のプロットでは、ナイーブベイズが大体低いスコアに収束する例を見ることができます。</target>
        </trans-unit>
        <trans-unit id="2cc6d3a8d1e5bdfb3693da69e5fd147a58a0255e" translate="yes" xml:space="preserve">
          <source>A list of arguments name to ignore in the hashing</source>
          <target state="translated">ハッシュで無視する引数名のリスト。</target>
        </trans-unit>
        <trans-unit id="977999f316fb9d6c637d2852021b1d8363940c27" translate="yes" xml:space="preserve">
          <source>A list of arrays of length &lt;code&gt;len(estimators_)&lt;/code&gt; containing the class labels for each estimator in the chain.</source>
          <target state="translated">チェーン内の各推定量のクラスラベルを含む、長さ &lt;code&gt;len(estimators_)&lt;/code&gt; の配列のリスト。</target>
        </trans-unit>
        <trans-unit id="8b21b4fe65d64da1a4d1d84446f550bb55759446" translate="yes" xml:space="preserve">
          <source>A list of class labels known to the classifier.</source>
          <target state="translated">分類器が知っているクラスラベルのリスト。</target>
        </trans-unit>
        <trans-unit id="c5eaed1c27ab8dfc53ee38efebd6c102a71098c5" translate="yes" xml:space="preserve">
          <source>A list of classes or column indices to select some (or to force inclusion of classes absent from the data)</source>
          <target state="translated">いくつかを選択する(またはデータにないクラスを強制的に含める)ためのクラスまたは列のインデックスのリスト</target>
        </trans-unit>
        <trans-unit id="a8fb99b6c7b15c5d0f9079fcee9eb1e89cb6ab1c" translate="yes" xml:space="preserve">
          <source>A list of clones of base_estimator.</source>
          <target state="translated">base_estimatorのクローンのリスト。</target>
        </trans-unit>
        <trans-unit id="53426e5188a28c01da334e8bfdbe7c9957c50862" translate="yes" xml:space="preserve">
          <source>A list of feature names.</source>
          <target state="translated">機能名のリスト。</target>
        </trans-unit>
        <trans-unit id="371683d834fcd55b1b47a55ccdff2980b0102eb8" translate="yes" xml:space="preserve">
          <source>A list of length n_features containing the feature names (e.g., &amp;ldquo;f=ham&amp;rdquo; and &amp;ldquo;f=spam&amp;rdquo;).</source>
          <target state="translated">機能名を含む長さn_featuresのリスト（たとえば、「f = ham」および「f = spam」）。</target>
        </trans-unit>
        <trans-unit id="3e9dc6d758a5816faa5f49abd255ba444796f1a7" translate="yes" xml:space="preserve">
          <source>A list of length n_features containing the feature names. If None generic names will be used (&amp;ldquo;feature_0&amp;rdquo;, &amp;ldquo;feature_1&amp;rdquo;, &amp;hellip;).</source>
          <target state="translated">機能名を含む長さn_featuresのリスト。Noneの場合、一般名が使用されます（&amp;ldquo; feature_0&amp;rdquo;、&amp;ldquo; feature_1&amp;rdquo;、&amp;hellip;）。</target>
        </trans-unit>
        <trans-unit id="7951c01a96e3650048b499324860f7763c837156" translate="yes" xml:space="preserve">
          <source>A list of stop words.</source>
          <target state="translated">ストップワードの一覧です。</target>
        </trans-unit>
        <trans-unit id="c84a50cbe409f7bcbe9aef60e47724b3d0374c13" translate="yes" xml:space="preserve">
          <source>A logistic regression with L1 penalty yields sparse models, and can thus be used to perform feature selection, as detailed in &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1-based feature selection&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1ベースの特徴選択で&lt;/a&gt;詳述されているように、L1ペナルティによるロジスティック回帰はスパースモデルを生成するため、特徴選択の実行に使用できます。</target>
        </trans-unit>
        <trans-unit id="2e8f6767450aa8f965fa6ba15414ee47e53d061d" translate="yes" xml:space="preserve">
          <source>A logistic regression with \(\ell_1\) penalty yields sparse models, and can thus be used to perform feature selection, as detailed in &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1-based feature selection&lt;/a&gt;.</source>
          <target state="translated">\（\ ell_1 \）ペナルティを伴うロジスティック回帰では、スパースモデルが生成されるため、&lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1ベースの特徴選択で&lt;/a&gt;詳しく説明されているように、特徴選択を実行するために使用できます。</target>
        </trans-unit>
        <trans-unit id="ed6a763f542c3b72714c079a2432308cb03a943b" translate="yes" xml:space="preserve">
          <source>A major difference is that GPR can choose the kernel&amp;rsquo;s hyperparameters based on gradient-ascent on the marginal likelihood function while KRR needs to perform a grid search on a cross-validated loss function (mean-squared error loss). A further difference is that GPR learns a generative, probabilistic model of the target function and can thus provide meaningful confidence intervals and posterior samples along with the predictions while KRR only provides predictions.</source>
          <target state="translated">主な違いは、GPRが限界尤度関数の勾配上昇に基づいてカーネルのハイパーパラメーターを選択できるのに対し、KRRは交差検定損失関数（平均二乗誤差損失）でグリッド検索を実行する必要があることです。さらなる違いは、GPRはターゲット関数の生成的で確率的なモデルを学習するため、KRRが予測のみを提供するのに対して、有意な信頼区間と事後サンプルを予測とともに提供できることです。</target>
        </trans-unit>
        <trans-unit id="413fd0e2f5ff27577a8397f2320f65ec5d748528" translate="yes" xml:space="preserve">
          <source>A major motivation of this method is F1-scoring, when the positive class is in the minority.</source>
          <target state="translated">この方法の大きな動機は、正のクラスが少数派の場合のF1スコアリングです。</target>
        </trans-unit>
        <trans-unit id="ef86aa8565e00b57d2c91be8664b6a11d798a158" translate="yes" xml:space="preserve">
          <source>A major problem with histograms, however, is that the choice of binning can have a disproportionate effect on the resulting visualization. Consider the upper-right panel of the above figure. It shows a histogram over the same data, with the bins shifted right. The results of the two visualizations look entirely different, and might lead to different interpretations of the data.</source>
          <target state="translated">しかし、ヒストグラムの大きな問題は、ビニングの選択が結果の可視化に不釣り合いな影響を与える可能性があることです。上の図の右上のパネルを考えてみましょう。これは、ビンを右にずらした同じデータのヒストグラムを示しています。2つの可視化の結果は全く違ったものに見え、データの異なる解釈につながるかもしれません。</target>
        </trans-unit>
        <trans-unit id="b4945c81d17c82b3985167f2fe21eac206551aeb" translate="yes" xml:space="preserve">
          <source>A mapping of terms to feature indices.</source>
          <target state="translated">用語の特徴指標へのマッピング。</target>
        </trans-unit>
        <trans-unit id="d80b1965676bd061b195356f9e0d7d581d27bcda" translate="yes" xml:space="preserve">
          <source>A mask of the observations that have been used to compute the raw robust estimates of location and shape, before correction and re-weighting.</source>
          <target state="translated">補正と再重み付けの前に、位置と形状の生のロバスト推定値を計算するために使用されたオブザベーションのマスク。</target>
        </trans-unit>
        <trans-unit id="0ec3f28dc02dc03ba22d583507907375d9c62b5f" translate="yes" xml:space="preserve">
          <source>A mask of the observations that have been used to compute the re-weighted robust location and covariance estimates.</source>
          <target state="translated">再重み付けされたロバストな位置と共分散推定値を計算するために使用されたオブザベーションのマスク。</target>
        </trans-unit>
        <trans-unit id="adea06a15a23aac7c9d9327f52e8025add2d08fa" translate="yes" xml:space="preserve">
          <source>A mask of the observations that have been used to compute the robust estimates of location and shape.</source>
          <target state="translated">位置と形状のロバスト推定値を計算するために使用されたオブザベーションのマスク。</target>
        </trans-unit>
        <trans-unit id="8be3be97461376c787b398256771e5f8ab1ed15d" translate="yes" xml:space="preserve">
          <source>A matrix containing only 1s ands 0s.</source>
          <target state="translated">1sと0sのみを含む行列。</target>
        </trans-unit>
        <trans-unit id="957a75d1eeb1c72e470b64334da958786ce3e979" translate="yes" xml:space="preserve">
          <source>A matrix of shape (n_samples, n_samples) will be created from this.</source>
          <target state="translated">これを元に形状の行列(n_samples,n_samples)を作成します。</target>
        </trans-unit>
        <trans-unit id="259daba30c783e777c062485d929bc6df3c76ec7" translate="yes" xml:space="preserve">
          <source>A matrix of term/token counts.</source>
          <target state="translated">項数/トークン数の行列。</target>
        </trans-unit>
        <trans-unit id="151a7f165ba08edb6c7ba1bd07841827b86e653b" translate="yes" xml:space="preserve">
          <source>A matrix such that &lt;code&gt;y_indicator[i, j] = 1&lt;/code&gt; iff &lt;code&gt;classes_[j]&lt;/code&gt; is in &lt;code&gt;y[i]&lt;/code&gt;, and 0 otherwise.</source>
          <target state="translated">マトリックスよう &lt;code&gt;y_indicator[i, j] = 1&lt;/code&gt; IFF &lt;code&gt;classes_[j]&lt;/code&gt; はである &lt;code&gt;y[i]&lt;/code&gt; 、およびそうでなければ0。</target>
        </trans-unit>
        <trans-unit id="6ac1281f3fac07a890a7987a1940a19d553f0540" translate="yes" xml:space="preserve">
          <source>A model is trained using \(k-1\) of the folds as training data;</source>
          <target state="translated">ひだの「ひだ」を学習データとして用いて、モデルを学習します。</target>
        </trans-unit>
        <trans-unit id="ca1ea4d381438e8c7ee31dbdc82c2c3f3b443e43" translate="yes" xml:space="preserve">
          <source>A more detailed summary of the search is available at &lt;code&gt;gs_clf.cv_results_&lt;/code&gt;.</source>
          <target state="translated">検索のより詳細な要約は、 &lt;code&gt;gs_clf.cv_results_&lt;/code&gt; で入手できます。</target>
        </trans-unit>
        <trans-unit id="0abf01b95edac52233877bfdb4963759d68aad5b" translate="yes" xml:space="preserve">
          <source>A more sophisticated approach is to use the &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output &lt;code&gt;y&lt;/code&gt; and the other feature columns are treated as inputs &lt;code&gt;X&lt;/code&gt;. A regressor is fit on &lt;code&gt;(X,
y)&lt;/code&gt; for known &lt;code&gt;y&lt;/code&gt;. Then, the regressor is used to predict the missing values of &lt;code&gt;y&lt;/code&gt;. This is done for each feature in an iterative fashion, and then is repeated for &lt;code&gt;max_iter&lt;/code&gt; imputation rounds. The results of the final imputation round are returned.</source>
          <target state="translated">より洗練されたアプローチは、&lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;IterativeImputer&lt;/code&gt; &lt;/a&gt;クラスを使用することです。このクラスは、欠測値を持つ各機能を他の機能の関数としてモデル化し、その推定値を代入に使用します。これは、ラウンドロビン方式で繰り返されます。各ステップで、特徴列は出力 &lt;code&gt;y&lt;/code&gt; として指定され、他の特徴列は入力 &lt;code&gt;X&lt;/code&gt; として扱われます。リグレッサは、既知の &lt;code&gt;y&lt;/code&gt; に対して &lt;code&gt;(X, y)&lt;/code&gt; 適合します。次に、リグレッサを使用して、 &lt;code&gt;y&lt;/code&gt; の欠落値を予測します。これは、各機能に対して反復的に実行され、 &lt;code&gt;max_iter&lt;/code&gt; 代入ラウンドに対して繰り返されます。最終代入ラウンドの結果が返されます。</target>
        </trans-unit>
        <trans-unit id="e1a254f5635c860c720fd64108216f6d8c4b1064" translate="yes" xml:space="preserve">
          <source>A more traditional (and possibly better) way to predict on a sparse subset of input features would be to use univariate feature selection followed by a traditional (l2-penalised) logistic regression model.</source>
          <target state="translated">入力特徴の疎なサブセットで予測するより伝統的な(そしておそらくより良い)方法は、一変量特徴選択に続いて伝統的な(l2ペナライズされた)ロジスティック回帰モデルを使用することであろう。</target>
        </trans-unit>
        <trans-unit id="fc3b3b1809b0fddd0d084276f73f16c23ecf93ac" translate="yes" xml:space="preserve">
          <source>A multi-label model that arranges binary classifiers into a chain.</source>
          <target state="translated">二値分類器を連鎖的に配置したマルチラベルモデル。</target>
        </trans-unit>
        <trans-unit id="07157eeb8d3c41fd6ae5f1ce4e96d98c52fa44ce" translate="yes" xml:space="preserve">
          <source>A multi-label model that arranges regressions into a chain.</source>
          <target state="translated">回帰を連鎖的に配置したマルチラベルモデル。</target>
        </trans-unit>
        <trans-unit id="9e05b7bdec595d3e973dea3540db250a47de0ecd" translate="yes" xml:space="preserve">
          <source>A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt;.</source>
          <target state="translated">多出力問題は、予測するいくつかの出力がある教師あり学習問題です。 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; Yがサイズ[n_samples、n_outputs]の 2次元配列の場合です。</target>
        </trans-unit>
        <trans-unit id="0d0f038bfe45a99c12fb61542c6de1b37df0ed3c" translate="yes" xml:space="preserve">
          <source>A new plotting API is available for creating visualizations. This new API allows for quickly adjusting the visuals of a plot without involving any recomputation. It is also possible to add different plots to the same figure. The following example illustrates &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt;&lt;code&gt;plot_roc_curve&lt;/code&gt;&lt;/a&gt;, but other plots utilities are supported like &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt;&lt;code&gt;plot_precision_recall_curve&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt;&lt;code&gt;plot_confusion_matrix&lt;/code&gt;&lt;/a&gt;. Read more about this new API in the &lt;a href=&quot;https://scikit-learn.org/0.23/visualizations.html#visualizations&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">ビジュアライゼーションを作成するための新しいプロットAPIが利用可能です。この新しいAPIを使用すると、再計算を行わなくても、プロットのビジュアルをすばやく調整できます。同じ図に異なるプロットを追加することも可能です。次の例は示して&lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt; &lt;code&gt;plot_roc_curve&lt;/code&gt; を&lt;/a&gt;、他のプロットユーティリティは次のようにサポートされています&lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt; &lt;code&gt;plot_precision_recall_curve&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt; &lt;code&gt;plot_confusion_matrix&lt;/code&gt; &lt;/a&gt;。この新しいAPIの詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/visualizations.html#visualizations&quot;&gt;ユーザーガイド&lt;/a&gt;をご覧ください。</target>
        </trans-unit>
        <trans-unit id="df611f66ef7be3083c893378a9603e78fdf46201" translate="yes" xml:space="preserve">
          <source>A new sample is inserted into the root of the CF Tree which is a CF Node. It is then merged with the subcluster of the root, that has the smallest radius after merging, constrained by the threshold and branching factor conditions. If the subcluster has any child node, then this is done repeatedly till it reaches a leaf. After finding the nearest subcluster in the leaf, the properties of this subcluster and the parent subclusters are recursively updated.</source>
          <target state="translated">CF ノードである CF ツリーのルートに新しいサンプルを挿入します。そして,閾値と分岐係数の条件で制約されたルートのサブクラスターとマージします.サブクラスタに子ノードがある場合は,リーフに到達するまで繰り返し行われます.リーフに最も近いサブクラスタを見つけた後、このサブクラスタと親サブクラスタの特性は再帰的に更新されます。</target>
        </trans-unit>
        <trans-unit id="06b4df58df49d31652c1e508653814db2c7c4b45" translate="yes" xml:space="preserve">
          <source>A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</source>
          <target state="translated">ノードは、この値以上の不純物が減少した場合に分割されます。</target>
        </trans-unit>
        <trans-unit id="f2f0f4cbbbaee87b8b7904e37994705d9bd8774b" translate="yes" xml:space="preserve">
          <source>A noise-free case</source>
          <target state="translated">ノイズのないケース</target>
        </trans-unit>
        <trans-unit id="a9448dea0ed4384de356358aa919a44c7d032339" translate="yes" xml:space="preserve">
          <source>A noisy case with known noise-level per datapoint</source>
          <target state="translated">データポイントごとに既知のノイズレベルを持つノイジーケース</target>
        </trans-unit>
        <trans-unit id="b96ec23e53414db9b11937cf799cd6c1f6432a32" translate="yes" xml:space="preserve">
          <source>A non-negative floating point value (the best value is 0.0), or an array of floating point values, one for each individual target.</source>
          <target state="translated">非負の浮動小数点値(最良値は0.0)、または個々のターゲットごとに1つずつの浮動小数点値の配列。</target>
        </trans-unit>
        <trans-unit id="2ced62a814e481bd22a6aa7667bcac7ac64675cc" translate="yes" xml:space="preserve">
          <source>A non-negative floating point value (the best value is 0.0).</source>
          <target state="translated">非負の浮動小数点値(最良値は0.0)。</target>
        </trans-unit>
        <trans-unit id="5ef179d616825b29b180ecdb28f9b1e0bfb64faf" translate="yes" xml:space="preserve">
          <source>A non-parametric supervised learning method used for classification. Creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.</source>
          <target state="translated">分類に用いられるノンパラメトリック教師付き学習法。データの特徴から推測される単純な決定規則を学習することで、対象変数の値を予測するモデルを作成します。</target>
        </trans-unit>
        <trans-unit id="e9c1cbeaf3f9c4d5469e9c590dbc955ea1a377cb" translate="yes" xml:space="preserve">
          <source>A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, &lt;code&gt;log2(n_classes) / n_classes&lt;/code&gt; is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since &lt;code&gt;log2(n_classes)&lt;/code&gt; is much smaller than n_classes.</source>
          <target state="translated">0と1の間の数は、1対残りの数よりも少ない分類子を必要とします。理論的には、 &lt;code&gt;log2(n_classes) / n_classes&lt;/code&gt; で各クラスを明確に表すのに十分です。ただし、実際には、 &lt;code&gt;log2(n_classes)&lt;/code&gt; はn_classesよりもはるかに小さいため、精度は高くありません。</target>
        </trans-unit>
        <trans-unit id="75b2970d65b634b9e917fd730d5f1ac86be5fb84" translate="yes" xml:space="preserve">
          <source>A number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name &amp;ldquo;error-correcting&amp;rdquo;. In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging.</source>
          <target state="translated">1より大きい数値には、1対残りよりも多くの分類子が必要になります。この場合、一部の分類子は理論的には他の分類子が犯した間違いを修正するため、「エラー修正」という名前が付けられます。ただし、実際には、分類子の間違いは通常相関しているため、これは発生しない可能性があります。エラー訂正出力コードは、バギングと同様の効果があります。</target>
        </trans-unit>
        <trans-unit id="303cdccb0a0b9ef8ee02f659c78579cbfb972892" translate="yes" xml:space="preserve">
          <source>A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a &lt;code&gt;score&lt;/code&gt; function, or &lt;code&gt;scoring&lt;/code&gt; must be passed.</source>
          <target state="translated">そのタイプのオブジェクトは、グリッドポイントごとにインスタンス化されます。これは、scikit-learn Estimatorインターフェースを実装すると想定されています。推定器が &lt;code&gt;score&lt;/code&gt; 関数を提供するか、スコア &lt;code&gt;scoring&lt;/code&gt; 渡す必要があります。</target>
        </trans-unit>
        <trans-unit id="56be8330a991d6aa887f39d43a91aa1ac6760e20" translate="yes" xml:space="preserve">
          <source>A one-dimensional array of distances</source>
          <target state="translated">距離の一次元配列</target>
        </trans-unit>
        <trans-unit id="77da2f75f17e6ea0f8d3b4beab5a8c65d7801934" translate="yes" xml:space="preserve">
          <source>A paragraph describing the characteristic of the dataset: its source, reference, etc.</source>
          <target state="translated">データセットの特徴を記述した段落.</target>
        </trans-unit>
        <trans-unit id="8831c41b4fb10cdce1530c46ecbd5c89f650c66c" translate="yes" xml:space="preserve">
          <source>A parameter can be given to allow K-means to be run in parallel, called &lt;code&gt;n_jobs&lt;/code&gt;. Giving this parameter a positive value uses that many processors (default: 1). A value of -1 uses all available processors, with -2 using one less, and so on. Parallelization generally speeds up computation at the cost of memory (in this case, multiple copies of centroids need to be stored, one for each job).</source>
          <target state="translated">K-meansを並行して実行できるようにするパラメーターを指定できます。これは &lt;code&gt;n_jobs&lt;/code&gt; と呼ばれます。このパラメーターに正の値を指定すると、その数のプロセッサーが使用されます（デフォルト：1）。値が-1の場合、使用可能なすべてのプロセッサが使用され、-2の場合は1つ少なく使用されます。並列化は通常、メモリを犠牲にして計算を高速化します（この場合、重心の複数のコピーを、ジョブごとに1つずつ保存する必要があります）。</target>
        </trans-unit>
        <trans-unit id="c76c7128eb10e519c4cde4416a2b5ebd1e864530" translate="yes" xml:space="preserve">
          <source>A plot that compares the various Beta-divergence loss functions supported by the Multiplicative-Update (&amp;lsquo;mu&amp;rsquo;) solver in &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt; &lt;/a&gt;のMultiplicative-Update（ 'mu'）ソルバーでサポートされているさまざまなベータ発散損失関数を比較するプロット。</target>
        </trans-unit>
        <trans-unit id="c428b2107a14d30575de38f7fc951c176630f469" translate="yes" xml:space="preserve">
          <source>A plot that compares the various convex loss functions supported by &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; で&lt;/a&gt;サポートされているさまざまな凸損失関数を比較するプロット。</target>
        </trans-unit>
        <trans-unit id="9c371e4431820ea4253b6da8f883638e2996e440" translate="yes" xml:space="preserve">
          <source>A plot will appear showing the top 5 most uncertain digits for each iteration of training. These may or may not contain mistakes, but we will train the next model with their true labels.</source>
          <target state="translated">プロットは、訓練の各反復について、最も不確実な上位5桁の数字を表示します。これらには間違いが含まれているかもしれませんし、含まれていないかもしれませんが、次のモデルはそれらの真のラベルで学習します。</target>
        </trans-unit>
        <trans-unit id="d0a927be776ddcc1a3b0a115e5ff9bf99815db03" translate="yes" xml:space="preserve">
          <source>A positive floating point value (the best value is 0.0).</source>
          <target state="translated">正の浮動小数点値(最良値は0.0)。</target>
        </trans-unit>
        <trans-unit id="2e47d08f91be7ad2360165e81b098e1d32db277c" translate="yes" xml:space="preserve">
          <source>A positive monotonic constraint is a constraint of the form:</source>
          <target state="translated">正の単調制約は、形式の制約である。</target>
        </trans-unit>
        <trans-unit id="9fa1e2038dd5a4a82d42f5d5bcbe0df63da34a6c" translate="yes" xml:space="preserve">
          <source>A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge&amp;rsquo;s stability under rotation.</source>
          <target state="translated">LassoとRidgeの間のトレードオフの実用的な利点は、Elastic-Netが回転時のRidgeの安定性の一部を継承できることです。</target>
        </trans-unit>
        <trans-unit id="fd73c2da7375a102550eb9b6f798b301111f7f0e" translate="yes" xml:space="preserve">
          <source>A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge&amp;rsquo;s stability under rotation.</source>
          <target state="translated">LassoとRidgeの間のトレードオフの実際的な利点は、Elastic-Netが回転中のRidgeの安定性の一部を継承できることです。</target>
        </trans-unit>
        <trans-unit id="0644879ffb5a73e0cac4122d0a74647c32667b9b" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator object or a seed for it if int. If &lt;code&gt;init='random'&lt;/code&gt;, &lt;code&gt;random_state&lt;/code&gt; is used to initialize the random transformation. If &lt;code&gt;init='pca'&lt;/code&gt;, &lt;code&gt;random_state&lt;/code&gt; is passed as an argument to PCA when initializing the transformation. Pass an int for reproducible results across multiple function calls. See :term: &lt;code&gt;Glossary &amp;lt;random_state&amp;gt;&lt;/code&gt;.</source>
          <target state="translated">疑似乱数ジェネレータオブジェクトまたはintの場合はそのシード。 &lt;code&gt;init='random'&lt;/code&gt; の場合、 &lt;code&gt;random_state&lt;/code&gt; を使用してランダム変換を初期化します。場合 &lt;code&gt;init='pca'&lt;/code&gt; 、 &lt;code&gt;random_state&lt;/code&gt; はPCAに引数として渡される変換を初期化するとき。複数の関数呼び出しにわたって再現可能な結果を​​得るためにintを渡します。：term： &lt;code&gt;Glossary &amp;lt;random_state&amp;gt;&lt;/code&gt; 参照してください。</target>
        </trans-unit>
        <trans-unit id="4370bf36aea658b96c83523c6e4601f6e89a2b33" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when &lt;code&gt;eigen_solver='amg'&lt;/code&gt; and by the K-Means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;eigen_solver='amg'&lt;/code&gt; の場合、およびK-Means初期化によって、lobpcg固有ベクトル分解の初期化に使用される疑似乱数ジェネレーター。intを使用して、ランダム性を決定論的にします。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="5ce137982e9454ec15b3a9266e29af4a861559f3" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == &amp;lsquo;amg&amp;rsquo; and by the K-Means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">eigen_solver == 'amg'の場合にlobpcg固有ベクトル分解の初期化に使用される疑似乱数ジェネレーターと、K平均法による初期化。intを使用して、ランダム性を決定論的にします。&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="4da70ee0892b2024f8c0c33c157ae6f4ed05fab9" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == &amp;lsquo;amg&amp;rsquo; and by the K-Means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">eigen_solver == 'amg'の場合、およびK-Means初期化によって、lobpcg固有ベクトル分解の初期化に使用される疑似乱数ジェネレーター。intを使用して、ランダム性を決定論的にします。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="0c41ce7b7d007fb54c5cb75f35d5c711d3e908a9" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigenvectors decomposition. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;amg&amp;rsquo;.</source>
          <target state="translated">lobpcg固有ベクトル分解の初期化に使用される疑似乱数ジェネレーター。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;solver&lt;/code&gt; == 'amg'の場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="8d91d68f1ffad3cd242144da0cec28854cf478b1" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigenvectors. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;amg&amp;rsquo;.</source>
          <target state="translated">lobpcg固有ベクトルの初期化に使用される疑似乱数ジェネレーター。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;solver&lt;/code&gt; == 'amg'の場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="ee2bf152e538d224d6fde70d9b0dd7a1cdbb5755" translate="yes" xml:space="preserve">
          <source>A random forest classifier.</source>
          <target state="translated">ランダムフォレスト分類器。</target>
        </trans-unit>
        <trans-unit id="e60aacdcc2887bf4d3963c64796dccc1910a3c9c" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if &lt;code&gt;bootstrap=True&lt;/code&gt; (default).</source>
          <target state="translated">ランダムフォレストは、データセットのさまざまなサブサンプルに分類ツリーの数を当てはめるメタ推定器であり、平均を使用して予測精度を向上させ、過剰適合を制御します。サブサンプルサイズは常に元の入力サンプルサイズと同じですが、 &lt;code&gt;bootstrap=True&lt;/code&gt; （デフォルト）の場合、サンプルは置き換えて描画されます。</target>
        </trans-unit>
        <trans-unit id="6d7fe13f3eb79eb708ba336fe6e8fed09167f0c0" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the &lt;code&gt;max_samples&lt;/code&gt; parameter if &lt;code&gt;bootstrap=True&lt;/code&gt; (default), otherwise the whole dataset is used to build each tree.</source>
          <target state="translated">ランダムフォレストは、データセットのさまざまなサブサンプルに多数の分類決定木を適合させ、平均化を使用して予測精度を向上させ、過剰適合を制御するメタ推定量です。サブサンプルサイズは、 &lt;code&gt;bootstrap=True&lt;/code&gt; （デフォルト）の場合は &lt;code&gt;max_samples&lt;/code&gt; パラメーターで制御されます。それ以外の場合は、データセット全体が各ツリーの構築に使用されます。</target>
        </trans-unit>
        <trans-unit id="6afddbc046b0606e01c6a10f7c579e615a468bc2" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if &lt;code&gt;bootstrap=True&lt;/code&gt; (default).</source>
          <target state="translated">ランダムフォレストはメタエスティメータであり、データセットのさまざまなサブサンプルにいくつかの決定木分類器を適合させ、平均を使用して予測精度を向上させ、過剰適合を制御します。サブサンプルサイズは常に元の入力サンプルサイズと同じですが、 &lt;code&gt;bootstrap=True&lt;/code&gt; （デフォルト）の場合、サンプルは置き換えて描画されます。</target>
        </trans-unit>
        <trans-unit id="8049315f314116153326eb819ff277cc1b244e8c" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the &lt;code&gt;max_samples&lt;/code&gt; parameter if &lt;code&gt;bootstrap=True&lt;/code&gt; (default), otherwise the whole dataset is used to build each tree.</source>
          <target state="translated">ランダムフォレストは、データセットのさまざまなサブサンプルに多数の決定木分類器を適合させ、平均化を使用して予測精度を向上させ、過剰適合を制御するメタ推定量です。サブサンプルサイズは、 &lt;code&gt;bootstrap=True&lt;/code&gt; （デフォルト）の場合は &lt;code&gt;max_samples&lt;/code&gt; パラメーターで制御されます。それ以外の場合は、データセット全体が各ツリーの構築に使用されます。</target>
        </trans-unit>
        <trans-unit id="4c30f02c56b168ad378f278a145ee7ca666d2b4b" translate="yes" xml:space="preserve">
          <source>A random forest regressor.</source>
          <target state="translated">ランダムフォレスト回帰器。</target>
        </trans-unit>
        <trans-unit id="517674d6fed45c030e35daafc36d33d7e1cb17fe" translate="yes" xml:space="preserve">
          <source>A random number generator instance to define the state of the random permutations generator. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator.</source>
          <target state="translated">乱数の順列生成器の状態を定義するための乱数生成器インスタンス。整数が与えられた場合、シードを固定します。デフォルトはグローバルな numpy 乱数発生器です。</target>
        </trans-unit>
        <trans-unit id="ad42d75eb3232815220e50c42e13b1c5a583a979" translate="yes" xml:space="preserve">
          <source>A random number generator instance to define the state of the random permutations generator. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">ランダム置換ジェネレーターの状態を定義する乱数ジェネレーターインスタンス。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="282455615e66e12f3be6646a3e4853abf573194f" translate="yes" xml:space="preserve">
          <source>A random number generator instance to define the state of the random permutations generator. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;</source>
          <target state="translated">ランダム順列ジェネレーターの状態を定義する乱数ジェネレーターインスタンス。複数の関数呼び出しにわたって再現可能な出力のためにintを渡します。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="3de0a58d62c3c2bcc234bd4e43f83d387ef4775e" translate="yes" xml:space="preserve">
          <source>A random order for each round.</source>
          <target state="translated">各ラウンドのランダムな順番。</target>
        </trans-unit>
        <trans-unit id="4166975533e11bd5bc32126880dea04c49362bfe" translate="yes" xml:space="preserve">
          <source>A randomized algorithm for the decomposition of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</source>
          <target state="translated">行列分解のためのランダム化アルゴリズム Per-Gunnar Martinsson,Vladimir Rokhlin,Mark Tygert</target>
        </trans-unit>
        <trans-unit id="1bef857d80e5f022acdf94565f064371e78877b1" translate="yes" xml:space="preserve">
          <source>A recursive feature elimination example showing the relevance of pixels in a digit classification task.</source>
          <target state="translated">デジタル分類タスクにおけるピクセルの関連性を示す再帰的特徴除去の例。</target>
        </trans-unit>
        <trans-unit id="59abf2d80f7d73a7cc0db592c8bcede424657b87" translate="yes" xml:space="preserve">
          <source>A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.</source>
          <target state="translated">クロスバリデーションで選択された特徴の数を自動チューニングした再帰的特徴除去の例。</target>
        </trans-unit>
        <trans-unit id="dcc2f34db796875ac21f4e9f9639246afd97a032" translate="yes" xml:space="preserve">
          <source>A reference (and not a copy) of the first argument in the &lt;code&gt;fit()&lt;/code&gt; method is stored for future reference. If that array changes between the use of &lt;code&gt;fit()&lt;/code&gt; and &lt;code&gt;predict()&lt;/code&gt; you will have unexpected results.</source>
          <target state="translated">&lt;code&gt;fit()&lt;/code&gt; メソッドの最初の引数の（コピーではなく）参照は、将来の参照のために保存されます。その配列が &lt;code&gt;fit()&lt;/code&gt; と &lt;code&gt;predict()&lt;/code&gt; の使用の間に変更されると、予期しない結果になります。</target>
        </trans-unit>
        <trans-unit id="d5fde252cc168f1a7ffd9faadab134fe0513cbda" translate="yes" xml:space="preserve">
          <source>A regressor which will be used to combine the base estimators. The default regressor is a &lt;code&gt;RidgeCV&lt;/code&gt;.</source>
          <target state="translated">基本推定量を組み合わせるために使用されるリグレッサ。デフォルトの &lt;code&gt;RidgeCV&lt;/code&gt; はRidgeCVです。</target>
        </trans-unit>
        <trans-unit id="b815536c2d161fd6acf2bcd856fde57736561c74" translate="yes" xml:space="preserve">
          <source>A representation of the full diabetes dataset would involve 11 dimensions (10 feature dimensions and one of the target variable). It is hard to develop an intuition on such representation, but it may be useful to keep in mind that it would be a fairly &lt;em&gt;empty&lt;/em&gt; space.</source>
          <target state="translated">完全な糖尿病データセットの表現には、11の次元が含まれます（10の特徴次元と1つのターゲット変数）。そのような表現について直感を養うことは難しいですが、それはかなり&lt;em&gt;空の&lt;/em&gt;スペースになることを覚えておくと役に立ちます。</target>
        </trans-unit>
        <trans-unit id="fb7d808e9a808e07d251ec0ab6593856a70a9b8b" translate="yes" xml:space="preserve">
          <source>A scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">シグニチャー &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 備えたスコアラー呼び出し可能オブジェクト/関数。</target>
        </trans-unit>
        <trans-unit id="cd458b3d90a800480ecc75cb9b07374a934f767b" translate="yes" xml:space="preserve">
          <source>A search consists of:</source>
          <target state="translated">検索は次のようなもので構成されています。</target>
        </trans-unit>
        <trans-unit id="4faa007b7ff82792cbf9f6b91017a8816af3cc88" translate="yes" xml:space="preserve">
          <source>A second feature array only if X has shape [n_samples_a, n_features].</source>
          <target state="translated">Xが形状[n_samples_a,n_features]を持っている場合にのみ、2番目の特徴量の配列。</target>
        </trans-unit>
        <trans-unit id="b3743bb79cbeb6292d7788299c27fa0302da1677" translate="yes" xml:space="preserve">
          <source>A selection of dtypes to exclude. For more details, see &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt;&lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">除外するdtypeの選択。詳細については、&lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt; &lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="9ed7392ea2bcc5742fc5a21d5c3ccda544426c7d" translate="yes" xml:space="preserve">
          <source>A selection of dtypes to include. For more details, see &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt;&lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">含めるdtypeの選択。詳細については、&lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt; &lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="ee9834606bdd77c4a06472befa3674177dd91a7e" translate="yes" xml:space="preserve">
          <source>A seq of Axis objects, one for each subplot.</source>
          <target state="translated">サブプロットごとに1つのAxisオブジェクトのseq。</target>
        </trans-unit>
        <trans-unit id="09a68a5ad2629329af5cff530eb7ba2ddf7ad320" translate="yes" xml:space="preserve">
          <source>A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below.</source>
          <target state="translated">シーケンスオブディクトは、検索するグリッドのシーケンスを意味し、意味のない、または効果のないパラメータの組み合わせを探索しないようにするのに便利です。以下の例を参照してください。</target>
        </trans-unit>
        <trans-unit id="5ad6cb6da544247e185919510fbbf9bbac2dfc37" translate="yes" xml:space="preserve">
          <source>A set of labels (any orderable and hashable object) for each sample. If the &lt;code&gt;classes&lt;/code&gt; parameter is set, &lt;code&gt;y&lt;/code&gt; will not be iterated.</source>
          <target state="translated">各サンプルのラベルのセット（順序付け可能でハッシュ可能なオブジェクト）。 &lt;code&gt;classes&lt;/code&gt; パラメータが設定されている場合、 &lt;code&gt;y&lt;/code&gt; は反復されません。</target>
        </trans-unit>
        <trans-unit id="7b19a5a4863d81ac862d0e9be41e4683721f5dd5" translate="yes" xml:space="preserve">
          <source>A similar clustering at multiple values of eps. Our implementation is optimized for memory usage.</source>
          <target state="translated">epsの複数の値で同様のクラスタリングを行う。我々の実装はメモリ使用量のために最適化されています。</target>
        </trans-unit>
        <trans-unit id="eb593a41404fe5b14fb797c942b194b27f9badb7" translate="yes" xml:space="preserve">
          <source>A similar clustering for a specified neighborhood radius (eps). Our implementation is optimized for runtime.</source>
          <target state="translated">指定された近傍半径(eps)の類似クラスタリング。我々の実装は実行時に最適化されています。</target>
        </trans-unit>
        <trans-unit id="134a4703b97e34542f6c5ec20a8b6025ee87dd8f" translate="yes" xml:space="preserve">
          <source>A simple choice to construct \(R_ij\) so that it is nonnegative and symmetric is:</source>
          <target state="translated">負ではない対称性を持つように構成するのは簡単な選択です。</target>
        </trans-unit>
        <trans-unit id="11b768780ec203929a5802877f9f67a2310663fd" translate="yes" xml:space="preserve">
          <source>A simple example shipped with scikit-learn: iris dataset</source>
          <target state="translated">scikit-learnに同梱されている簡単な例:虹彩データセット</target>
        </trans-unit>
        <trans-unit id="8e839688e01ba01e9dfe145feb09335f1aa963d0" translate="yes" xml:space="preserve">
          <source>A simple example:</source>
          <target state="translated">簡単な例です。</target>
        </trans-unit>
        <trans-unit id="2a7be91a45219aa7eb5e6e5584cdc7ab00269326" translate="yes" xml:space="preserve">
          <source>A simple graphical frontend for Libsvm mainly intended for didactic purposes. You can create data points by point and click and visualize the decision region induced by different kernels and parameter settings.</source>
          <target state="translated">Libsvm用のシンプルなグラフィカル・フロントエンドです。ポイント&amp;クリックでデータポイントを作成し、異なるカーネルやパラメータ設定によって引き起こされる決定領域を視覚化することができます。</target>
        </trans-unit>
        <trans-unit id="08a70c5b3e130b5b3a3556a3a9d846d499b809af" translate="yes" xml:space="preserve">
          <source>A simple linear generative model with Gaussian latent variables.</source>
          <target state="translated">ガウスの潜在変数を用いた単純な線形生成モデル。</target>
        </trans-unit>
        <trans-unit id="7ed72a91acd5901b2715fbe7249ca1b4b882cab1" translate="yes" xml:space="preserve">
          <source>A simple one-dimensional regression example computed in two different ways:</source>
          <target state="translated">2つの異なる方法で計算された単純な1次元回帰の例。</target>
        </trans-unit>
        <trans-unit id="d0a2166f9acbabe38964ccee31d9e7127ebb3bd0" translate="yes" xml:space="preserve">
          <source>A simple toy dataset to visualize clustering and classification algorithms.</source>
          <target state="translated">クラスタリングと分類アルゴリズムを可視化するためのシンプルなおもちゃのデータセット。</target>
        </trans-unit>
        <trans-unit id="47127c0ac17be6b42bd52820b6c53e975ec33072" translate="yes" xml:space="preserve">
          <source>A simple toy dataset to visualize clustering and classification algorithms. Read more in the &lt;a href=&quot;../../datasets/index#sample-generators&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">クラスタリングと分類のアルゴリズムを視覚化するための単純なおもちゃのデータセット。詳細については、&lt;a href=&quot;../../datasets/index#sample-generators&quot;&gt;ユーザーガイド&lt;/a&gt;をご覧ください。</target>
        </trans-unit>
        <trans-unit id="0963c917bb15b907590115ef7d8ee882d54970b8" translate="yes" xml:space="preserve">
          <source>A single str (see &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt;) or a callable (see &lt;a href=&quot;../model_evaluation#scoring&quot;&gt;Defining your scoring strategy from metric functions&lt;/a&gt;) to evaluate the predictions on the test set.</source>
          <target state="translated">テストセットの予測を評価するための単一のstr（&lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;スコアリングパラメーター：モデル評価ルールの&lt;/a&gt;&lt;a href=&quot;../model_evaluation#scoring&quot;&gt;定義を&lt;/a&gt;参照）または呼び出し可能（メトリック関数からのスコアリング戦略の定義を参照）。</target>
        </trans-unit>
        <trans-unit id="6cc426ce246dcb8426ecbbb6a60a4ff7a07e84ce" translate="yes" xml:space="preserve">
          <source>A single string (see &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt;) or a callable (see &lt;a href=&quot;../model_evaluation#scoring&quot;&gt;Defining your scoring strategy from metric functions&lt;/a&gt;) to evaluate the predictions on the test set.</source>
          <target state="translated">テストセットの予測を評価するための単一の文字列（&lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;スコアリングパラメーター：モデル評価ルールの&lt;/a&gt;&lt;a href=&quot;../model_evaluation#scoring&quot;&gt;定義を&lt;/a&gt;参照）または呼び出し可能（メトリック関数からのスコアリング戦略の定義を参照）。</target>
        </trans-unit>
        <trans-unit id="2e53707c3f177aad6668f2e995ecf10221437949" translate="yes" xml:space="preserve">
          <source>A small value of &lt;code&gt;C&lt;/code&gt; includes more/all the observations, allowing the margins to be calculated using all the data in the area.</source>
          <target state="translated">&lt;code&gt;C&lt;/code&gt; の小さな値には、より多くの/すべての観測が含まれるため、エリア内のすべてのデータを使用してマージンを計算できます。</target>
        </trans-unit>
        <trans-unit id="e33601e97cbb479e0128e302bee052e1e2030a89" translate="yes" xml:space="preserve">
          <source>A solution in high-dimensional statistical learning is to &lt;em&gt;shrink&lt;/em&gt; the regression coefficients to zero: any two randomly chosen set of observations are likely to be uncorrelated. This is called &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; regression:</source>
          <target state="translated">高次元統計学習の解決策は、回帰係数をゼロに&lt;em&gt;縮小&lt;/em&gt;することです。ランダムに選択された2つの観測値のセットはいずれも無相関である可能性があります。これは&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;回帰と呼ばれます。</target>
        </trans-unit>
        <trans-unit id="dcd1f54984c6dffbbcff2667b694f4185f6ec040" translate="yes" xml:space="preserve">
          <source>A solution to this problem is a procedure called &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot;&gt;cross-validation&lt;/a&gt; (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called &lt;em&gt;k&lt;/em&gt;-fold CV, the training set is split into &lt;em&gt;k&lt;/em&gt; smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the &lt;em&gt;k&lt;/em&gt; &amp;ldquo;folds&amp;rdquo;:</source>
          <target state="translated">この問題の解決策は、&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot;&gt;相互検証&lt;/a&gt;（CV）と呼ばれる手順です。テストセットは最終的な評価のために引き続き保留する必要がありますが、CVを実行するときに検証セットは必要ありません。&lt;em&gt;k&lt;/em&gt;フォールドCV と呼ばれる基本的なアプローチでは、トレーニングセットが&lt;em&gt;k個の&lt;/em&gt;小さなセットに分割されます（他のアプローチについては以下で説明しますが、一般的に同じ原則に従います）。以下の手順は、&lt;em&gt;k個の&lt;/em&gt;「折り目」のそれぞれについて行われます。</target>
        </trans-unit>
        <trans-unit id="538e1506057d7a3220a186af08ab5489676c59df" translate="yes" xml:space="preserve">
          <source>A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with &lt;code&gt;metric='precomputed'&lt;/code&gt;. See &lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">スパース半径近傍グラフ（欠落しているエントリはepsの外にあると推定される）は、メモリ効率の良い方法で事前計算でき、dbscanは &lt;code&gt;metric='precomputed'&lt;/code&gt; を使用してこれに対して実行できます。&lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="5434739c8a85282e43e6d41eec6550c57895597f" translate="yes" xml:space="preserve">
          <source>A star marks the expected sample for each class; its size reflects the probability of selecting that class label.</source>
          <target state="translated">星印は,各クラスの期待標本をマークします.</target>
        </trans-unit>
        <trans-unit id="a226eb090c390674deacec5886c6aefd3ca76b60" translate="yes" xml:space="preserve">
          <source>A str (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; which should return only a single value.</source>
          <target state="translated">str（モデル評価のドキュメントを参照）または単一の値のみを返す必要があるシグネチャ &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; を持つスコアラー呼び出し可能オブジェクト/関数。</target>
        </trans-unit>
        <trans-unit id="7313aa5f13cd15f69aa6d50563adc57324fb0eb6" translate="yes" xml:space="preserve">
          <source>A str (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">str（モデル評価ドキュメントを参照）または署名 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 持つスコアラー呼び出し可能オブジェクト/関数。</target>
        </trans-unit>
        <trans-unit id="d85d389496ba61c19ce84f015d0fceafa4c84844" translate="yes" xml:space="preserve">
          <source>A str, giving an expression as a function of n_jobs, as in &amp;lsquo;2*n_jobs&amp;rsquo;</source>
          <target state="translated">'2 * n_jobs'のように、n_jobsの関数として式を与えるstr</target>
        </trans-unit>
        <trans-unit id="31ecf6c64ee79cd0b8c2879dbc516ad7450905ee" translate="yes" xml:space="preserve">
          <source>A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.</source>
          <target state="translated">ラウンドロビン方式で他の特徴の関数として欠落値を持つ各特徴をモデル化することにより、欠落値を入力するための戦略。</target>
        </trans-unit>
        <trans-unit id="9cc68f40152c74e41efd28d7122ce96e71e57739" translate="yes" xml:space="preserve">
          <source>A strategy to implement out-of-core scaling is to stream data to the estimator in mini-batches. Each mini-batch is vectorized using &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; so as to guarantee that the input space of the estimator has always the same dimensionality. The amount of memory used at any time is thus bounded by the size of a mini-batch. Although there is no limit to the amount of data that can be ingested using such an approach, from a practical point of view the learning time is often limited by the CPU time one wants to spend on the task.</source>
          <target state="translated">コア外スケーリングを実装する戦略は、ミニバッチで推定器にデータをストリーミングすることです。各ミニバッチは&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt;を使用してベクトル化され、推定器の入力空間が常に同じ次元であることを保証します。したがって、常に使用されるメモリの量は、ミニバッチのサイズによって制限されます。このようなアプローチを使用して取り込むことができるデータの量に制限はありませんが、実際の観点から、学習時間は、多くの場合、タスクに費やしたいCPU時間によって制限されます。</target>
        </trans-unit>
        <trans-unit id="7c0e35a146ea0efee84f42e6ee454306c5da4827" translate="yes" xml:space="preserve">
          <source>A string (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">文字列（モデル評価のドキュメントを参照）またはシグニチャー &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 備えたスコアラー呼び出し可能オブジェクト/関数。</target>
        </trans-unit>
        <trans-unit id="fad6fe3853f8abecb73cc5e995f9131ee653eef0" translate="yes" xml:space="preserve">
          <source>A string (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;. For a list of scoring functions that can be used, look at &lt;a href=&quot;../classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt;. The default scoring option used is &amp;lsquo;accuracy&amp;rsquo;.</source>
          <target state="translated">文字列（モデル評価のドキュメントを参照）またはシグニチャー &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 備えたスコアラー呼び出し可能オブジェクト/関数。使用できるスコアリング関数のリストについては、&lt;a href=&quot;../classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; をご覧ください&lt;/a&gt;。使用されるデフォルトのスコアリングオプションは「精度」です。</target>
        </trans-unit>
        <trans-unit id="1963bcb955646804f10ec7ad2d71cbad666ea477" translate="yes" xml:space="preserve">
          <source>A string (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;. If None, the negative mean squared error if cv is &amp;lsquo;auto&amp;rsquo; or None (i.e. when using generalized cross-validation), and r2 score otherwise.</source>
          <target state="translated">文字列（モデル評価ドキュメントを参照）または署名 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 持つスコアラー呼び出し可能オブジェクト/関数。 Noneの場合、cvが「auto」またはNoneの場合（つまり、一般化された交差検定を使用する場合）の負の平均二乗誤差、それ以外の場合はr2スコア。</target>
        </trans-unit>
        <trans-unit id="e94d51ba3f9b9473065d98a0a11ca48ef9ac3a04" translate="yes" xml:space="preserve">
          <source>A string of unicode symbols.</source>
          <target state="translated">ユニコードシンボルの文字列。</target>
        </trans-unit>
        <trans-unit id="93b243cc3849b7302c5dad12c7987dfe501650ad" translate="yes" xml:space="preserve">
          <source>A string, giving an expression as a function of n_jobs, as in &amp;lsquo;2*n_jobs&amp;rsquo;</source>
          <target state="translated">「2 * n_jobs」のように、n_jobsの関数として式を与える文字列</target>
        </trans-unit>
        <trans-unit id="82f0cd7ca226a86963abc9437ca213dfd9c32ccc" translate="yes" xml:space="preserve">
          <source>A sub-pipeline can also be extracted using the slicing notation commonly used for Python Sequences such as lists or strings (although only a step of 1 is permitted). This is convenient for performing only some of the transformations (or their inverse):</source>
          <target state="translated">また、リストや文字列などのPythonシーケンスで一般的に使われているスライシング記法を使ってサブパイプラインを抽出することもできます(ただし、1ステップのみ許可されています)。これは一部の変換(またはその逆)だけを行うのに便利です。</target>
        </trans-unit>
        <trans-unit id="84dd7b04896fc19b0373bd22f46ebced4bcbae51" translate="yes" xml:space="preserve">
          <source>A supervised learning estimator with a &lt;code&gt;fit&lt;/code&gt; method that provides information about feature importance either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute.</source>
          <target state="translated">&lt;code&gt;coef_&lt;/code&gt; 属性または &lt;code&gt;feature_importances_&lt;/code&gt; 属性のいずれかを使用して、機能の重要性に関する情報を提供する &lt;code&gt;fit&lt;/code&gt; メソッドを備えた教師あり学習推定量。</target>
        </trans-unit>
        <trans-unit id="be778589a42569621d412c8d143e2c4ecab0280f" translate="yes" xml:space="preserve">
          <source>A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.</source>
          <target state="translated">サポートベクターマシンは,高次元空間または無限次元空間に超平面または超平面の集合を構築し,分類,回帰,その他のタスクに使用することができる.直感的には、良い分離は、任意のクラスの最も近い訓練データ点までの距離が最も大きい超平面(いわゆる関数的マージン)によって達成されます(一般的に、マージンが大きいほど、分類器の一般化誤差が低くなるため)。</target>
        </trans-unit>
        <trans-unit id="b0f8a884660f9ed7117b59341660ac6dff079372" translate="yes" xml:space="preserve">
          <source>A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called &amp;ldquo;support vectors&amp;rdquo;:</source>
          <target state="translated">サポートベクターマシンは、高次元または無限次元の空間に超平面または超平面のセットを構築します。これは、分類、回帰、またはその他のタスクに使用できます。直感的には、任意のクラスの最も近いトレーニングデータポイントまでの距離が最大の超平面（いわゆる機能マージン）によって良好な分離が達成されます。これは、一般にマージンが大きいほど、分類器の汎化誤差が小さくなるためです。次の図は、線形分離可能問題の決定関数を示しています。マージン境界には、「サポートベクター」と呼ばれる3つのサンプルがあります。</target>
        </trans-unit>
        <trans-unit id="3a7e8b3dcde624d3d8f97ed6de9c3edf28cc3212" translate="yes" xml:space="preserve">
          <source>A synthetic random regression problem is generated. The targets &lt;code&gt;y&lt;/code&gt; are modified by: (i) translating all targets such that all entries are non-negative and (ii) applying an exponential function to obtain non-linear targets which cannot be fitted using a simple linear model.</source>
          <target state="translated">合成ランダム回帰問題が生成されます。ターゲット &lt;code&gt;y&lt;/code&gt; は、（i）すべてのエントリが非負になるようにすべてのターゲットを変換し、（ii）単純な線形モデルを使用してフィットできない非線形ターゲットを取得するために指数関数を適用することによって変更されます。</target>
        </trans-unit>
        <trans-unit id="0903abbf1faeca209d35b8701b7569a1792a87c6" translate="yes" xml:space="preserve">
          <source>A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.</source>
          <target state="translated">リコールが高くても精度が低いシステムは、多くの結果を返しますが、訓練ラベルと比較すると、予測されたラベルのほとんどが正しくありません。高精度でありながら低リコールのシステムはその逆で、結果は非常に少ないですが、訓練ラベルと比較すると、予測されたラベルのほとんどが正しくなります。高精度で高リコールの理想的なシステムは、多くの結果を返し、すべての結果が正しくラベル付けされます。</target>
        </trans-unit>
        <trans-unit id="173f587454933c3828450ea8ca8b53629bab1434" translate="yes" xml:space="preserve">
          <source>A thin wrapper around the functionality of the kernels in sklearn.metrics.pairwise.</source>
          <target state="translated">sklearn.metrics.pairwiseのカーネルの機能の薄いラッパーです。</target>
        </trans-unit>
        <trans-unit id="66676f8254f2529112cd062a4d4d85367f781237" translate="yes" xml:space="preserve">
          <source>A trivial solution to this problem is to set all the points on the origin. In order to avoid that, the disparities \(\hat{d}_{ij}\) are normalized.</source>
          <target state="translated">この問題の簡単な解決策は、すべての点を原点に設定することである。それを避けるために、格差は正規化されます。</target>
        </trans-unit>
        <trans-unit id="92110b9d440ed0f7743804ea18583b68d039b315" translate="yes" xml:space="preserve">
          <source>A tutorial exercise for using different SVM kernels.</source>
          <target state="translated">異なるSVMカーネルを使用するためのチュートリアル演習。</target>
        </trans-unit>
        <trans-unit id="b027711ce35320d3d1f8472a3216da00739d6438" translate="yes" xml:space="preserve">
          <source>A tutorial exercise regarding the use of classification techniques on the Digits dataset.</source>
          <target state="translated">Digitsデータセットでの分類技術の使用に関するチュートリアル演習です。</target>
        </trans-unit>
        <trans-unit id="f2776a357a1de5e5a81f9ce6aa4ab14f87148c15" translate="yes" xml:space="preserve">
          <source>A tutorial exercise using Cross-validation with an SVM on the Digits dataset.</source>
          <target state="translated">DigitsデータセットでSVMを用いたクロスバリデーションを使用したチュートリアル演習。</target>
        </trans-unit>
        <trans-unit id="b2628903b486483a05c00dab06ec0310f6a300cc" translate="yes" xml:space="preserve">
          <source>A tutorial exercise which uses cross-validation with linear models.</source>
          <target state="translated">線形モデルを用いたクロスバリデーションを使用したチュートリアル演習。</target>
        </trans-unit>
        <trans-unit id="cd8b8f382c4d69a8c061676d971b4b4296bfd8b7" translate="yes" xml:space="preserve">
          <source>A tutorial on statistical-learning for scientific data processing</source>
          <target state="translated">科学的データ処理のための統計学習のチュートリアル</target>
        </trans-unit>
        <trans-unit id="59f7f31eba85f0c9b2945699e7dc03221dcc0732" translate="yes" xml:space="preserve">
          <source>A two-dimensional classification example showing iso-probability lines for the predicted probabilities.</source>
          <target state="translated">予測された確率の等確率線を示す二次元分類例。</target>
        </trans-unit>
        <trans-unit id="027a04b42d0e6eeb4156be0ce184aaf749b2c919" translate="yes" xml:space="preserve">
          <source>A typical &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_sparsify.py&quot;&gt;benchmark&lt;/a&gt; on synthetic data yields a &amp;gt;30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers.</source>
          <target state="translated">合成データの典型的な&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_sparsify.py&quot;&gt;ベンチマーク&lt;/a&gt;では、モデルと入力の両方がスパースな場合（それぞれ0.000024と0.027400の非ゼロ係数比）、レイテンシが30％以上減少します。データとモデルのスパース性とサイズによって、走行距離は異なる場合があります。さらに、スパース化は、運用サーバーに展開された予測モデルのメモリ使用量を減らすのに非常に役立ちます。</target>
        </trans-unit>
        <trans-unit id="ec7bb89c7f50ac1476a296b31de1d976e3adfdc7" translate="yes" xml:space="preserve">
          <source>A valid representation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel&quot;&gt;multilabel&lt;/a&gt;&lt;code&gt;y&lt;/code&gt; is an either dense or sparse &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-binary&quot;&gt;binary&lt;/a&gt; matrix of shape &lt;code&gt;(n_samples, n_classes)&lt;/code&gt;. Each column represents a class. The &lt;code&gt;1&lt;/code&gt;&amp;rsquo;s in each row denote the positive classes a sample has been labelled with. An example of a dense matrix &lt;code&gt;y&lt;/code&gt; for 3 samples:</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel&quot;&gt;マルチラベル&lt;/a&gt; &lt;code&gt;y&lt;/code&gt; の有効な表現は、形状の密または疎の&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-binary&quot;&gt;バイナリ&lt;/a&gt;行列 &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; です。各列はクラスを表します。各行の &lt;code&gt;1&lt;/code&gt; は、サンプルにラベルが付けられたポジティブクラスを示します。3つのサンプルの密行列 &lt;code&gt;y&lt;/code&gt; の例：</target>
        </trans-unit>
        <trans-unit id="cd911da25d74dfd6322d9ef6e738de82cbe5f6c0" translate="yes" xml:space="preserve">
          <source>A valid representation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;multioutput&lt;/a&gt;&lt;code&gt;y&lt;/code&gt; is a dense matrix of shape &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; of class labels. A column wise concatenation of 1d &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;multiclass&lt;/a&gt; variables. An example of &lt;code&gt;y&lt;/code&gt; for 3 samples:</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;マルチ出力&lt;/a&gt; &lt;code&gt;y&lt;/code&gt; の有効な表現は、クラスラベルの形状 &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; の密行列です。1d&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;マルチクラス&lt;/a&gt;変数の列ごとの連結。3つのサンプルの &lt;code&gt;y&lt;/code&gt; の例：</target>
        </trans-unit>
        <trans-unit id="9b5488b6a26a1d957fe479981670f261e01fd257" translate="yes" xml:space="preserve">
          <source>A valid representation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;multioutput&lt;/a&gt;&lt;code&gt;y&lt;/code&gt; is a dense matrix of shape &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; of floats. A column wise concatenation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-continuous&quot;&gt;continuous&lt;/a&gt; variables. An example of &lt;code&gt;y&lt;/code&gt; for 3 samples:</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;マルチ出力&lt;/a&gt; &lt;code&gt;y&lt;/code&gt; の有効な表現は、floatの形状 &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; の密行列です。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-continuous&quot;&gt;連続&lt;/a&gt;変数の列ごとの連結。3つのサンプルの &lt;code&gt;y&lt;/code&gt; の例：</target>
        </trans-unit>
        <trans-unit id="f7b09774b632b4a2428bc3dfc2607f68bc8654b3" translate="yes" xml:space="preserve">
          <source>A value ranges from 0 to 1. Radius neighbors will be searched until the ratio between total neighbors within the radius and the total candidates becomes less than this value unless it is terminated by hash length reaching &lt;code&gt;min_hash_match&lt;/code&gt;.</source>
          <target state="translated">値の範囲は0〜1です。半径の長さ内の全近傍と全候補の間の比率が &lt;code&gt;min_hash_match&lt;/code&gt; に到達するハッシュ長で終了しない限り、半径近傍の比率がこの値より小さくなるまで、半径近傍が検索されます。</target>
        </trans-unit>
        <trans-unit id="2582a27b00b61e2b9522f341f9a346d9f6709d01" translate="yes" xml:space="preserve">
          <source>A vector of size n_samples with the values of Xred assigned to each of the cluster of samples.</source>
          <target state="translated">サイズ n_samples のベクトルで,Xred の値がサンプルのクラスタのそれぞれに割り当てられています.</target>
        </trans-unit>
        <trans-unit id="d6682290692f64a366a7947dddf0230094bb4014" translate="yes" xml:space="preserve">
          <source>A very short introduction into machine learning problems and how to solve them using scikit-learn. Introduced basic concepts and conventions.</source>
          <target state="translated">機械学習の問題とscikit-learnを使った解決方法をごく短時間で紹介。基本的な概念と規約を紹介しました。</target>
        </trans-unit>
        <trans-unit id="c7690c5e442eb70039d5b5e2448652422a789b71" translate="yes" xml:space="preserve">
          <source>A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction.</source>
          <target state="translated">投票リレグレッサーは、データセット全体に対して複数の基本レグレッサーをそれぞれ適合させるアンサンブルメタ推定器である。そして、個々の予測値を平均化して最終的な予測値を形成する。</target>
        </trans-unit>
        <trans-unit id="f36937969f4f8fe7fe83a3d1b4c3ed127cd62323" translate="yes" xml:space="preserve">
          <source>A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction. We will use three different regressors to predict the data: &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt;&lt;code&gt;LinearRegression&lt;/code&gt;&lt;/a&gt;). Then the above 3 regressors will be used for the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">投票リグレッサは、それぞれがデータセット全体にある複数のベースリグレッサに適合するアンサンブルメタ推定量です。次に、個々の予測を平均して、最終的な予測を作成します。私たちは、データを予測するために3つの異なる説明変数を使用します。&lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt; &lt;code&gt;RandomForestRegressor&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;../../modules/generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt; &lt;code&gt;LinearRegression&lt;/code&gt; &lt;/a&gt;）。次に、上記の3つの&lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt; &lt;code&gt;VotingRegressor&lt;/code&gt; &lt;/a&gt;使用されます。</target>
        </trans-unit>
        <trans-unit id="9e5f7b45681bf1702c76ccf69a63504aee5a5896" translate="yes" xml:space="preserve">
          <source>A {n_samples by n_samples} size matrix will be created from this</source>
          <target state="translated">この行列から {n_samples by n_samples}サイズの行列が作成されます。</target>
        </trans-unit>
        <trans-unit id="fcc549a5a2b9c6ffeadeffd9e820de04a6f70e50" translate="yes" xml:space="preserve">
          <source>A. Kraskov, H. Stogbauer and P. Grassberger, &amp;ldquo;Estimating mutual information&amp;rdquo;. Phys. Rev. E 69, 2004.</source>
          <target state="translated">A. Kraskov、H。Stogbauer、P。Grassberger、「相互情報量の推定」。物理学 Rev. E 69、2004。</target>
        </trans-unit>
        <trans-unit id="2f18974816a09bbd22c4a7050c6bc6030b7596e3" translate="yes" xml:space="preserve">
          <source>A. McCallum and K. Nigam (1998). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529&quot;&gt;A comparison of event models for Naive Bayes text classification.&lt;/a&gt; Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.</source>
          <target state="translated">A.マッカラムとK.ニガム（1998）。&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529&quot;&gt;Naive Bayesテキスト分類のイベントモデルの比較。&lt;/a&gt;手続き AAAI / ICML-98テキスト分類の学習に関するワークショップ、41〜48ページ。</target>
        </trans-unit>
        <trans-unit id="09ff19e0224a8e064521a1f035498a8575f6b1a4" translate="yes" xml:space="preserve">
          <source>A. McCallum and K. Nigam (1998). A comparison of event models for naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.</source>
          <target state="translated">A.McCallum and K.Nigam (1998).ナイーブベイズテキスト分類のためのイベントモデルの比較.議論の中心となっているのは、「テキスト分類のための学習に関するワークショップ」である。AAAI/ICML-98 Workshop on Learning for Text Categorization,pp.</target>
        </trans-unit>
        <trans-unit id="664157aad39c267bf285cc58317f2c271aa8f944" translate="yes" xml:space="preserve">
          <source>A. Noll, R. Salzmann and M.V. Wuthrich, Case Study: French Motor Third-Party Liability Claims (November 8, 2018). &lt;a href=&quot;http://dx.doi.org/10.2139/ssrn.3164764&quot;&gt;doi:10.2139/ssrn.3164764&lt;/a&gt;</source>
          <target state="translated">A. Noll、R。Salzmann、MV Wuthrich、ケーススタディ：フランスのモーターサードパーティの責任請求（2018年11月8日）。&lt;a href=&quot;http://dx.doi.org/10.2139/ssrn.3164764&quot;&gt;doi：10.2139 / ssrn.3164764&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4aaae5b2c1983dccf44b8ac2ef0c2ae6d4b3ffdb" translate="yes" xml:space="preserve">
          <source>AGE</source>
          <target state="translated">AGE</target>
        </trans-unit>
        <trans-unit id="4aecbad800270fae47218d640b45ace7ade395c4" translate="yes" xml:space="preserve">
          <source>AGE proportion of owner-occupied units built prior to 1940</source>
          <target state="translated">1940年以前に建てられた持ち家の割合</target>
        </trans-unit>
        <trans-unit id="80cd3c2daea500fa1de49e5116e93d233f023eef" translate="yes" xml:space="preserve">
          <source>AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful to select the value of the regularization parameter by making a trade-off between the goodness of fit and the complexity of the model. A good model should explain well the data while being simple.</source>
          <target state="translated">AICはアカイケ情報基準、BICはベイズ情報基準である。このような基準は、適合性の良さとモデルの複雑さとの間でトレードオフを行うことにより、正則化パラメータの値を選択するのに有用である。良いモデルは、単純でありながらデータをよく説明するべきである。</target>
        </trans-unit>
        <trans-unit id="fedf36066038cc2bbe4c1f8d6f37bca1d4a271f3" translate="yes" xml:space="preserve">
          <source>AMI</source>
          <target state="translated">AMI</target>
        </trans-unit>
        <trans-unit id="977d221308080b656b11030badda34761fa1379d" translate="yes" xml:space="preserve">
          <source>ANOVA F-value between label/feature for classification tasks.</source>
          <target state="translated">ANOVA 分類タスクのラベル/特徴量間のF値。</target>
        </trans-unit>
        <trans-unit id="500622cc8c5af556f9bf30d45e5a5dc4d38771d9" translate="yes" xml:space="preserve">
          <source>AP and the trapezoidal area under the operating points (&lt;a href=&quot;../../modules/generated/sklearn.metrics.auc#sklearn.metrics.auc&quot;&gt;&lt;code&gt;sklearn.metrics.auc&lt;/code&gt;&lt;/a&gt;) are common ways to summarize a precision-recall curve that lead to different results. Read more in the &lt;a href=&quot;../../modules/model_evaluation#precision-recall-f-measure-metrics&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">APと操作点の下の台形領域（&lt;a href=&quot;../../modules/generated/sklearn.metrics.auc#sklearn.metrics.auc&quot;&gt; &lt;code&gt;sklearn.metrics.auc&lt;/code&gt; &lt;/a&gt;）は、異なる結果につながる精度-再現率曲線を要約する一般的な方法です。詳細については、&lt;a href=&quot;../../modules/model_evaluation#precision-recall-f-measure-metrics&quot;&gt;ユーザーガイド&lt;/a&gt;をご覧ください。</target>
        </trans-unit>
        <trans-unit id="7ce6548ae70272727979635a979ae4f9ea8a3a93" translate="yes" xml:space="preserve">
          <source>AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:</source>
          <target state="translated">APは、各閾値で達成された精度の加重平均として精度-リコール曲線を要約し、前の閾値からのリコールの増加を重みとして使用します。</target>
        </trans-unit>
        <trans-unit id="d93d10ff0fbef1b4aa0ddc24e10e907746d3c85a" translate="yes" xml:space="preserve">
          <source>API</source>
          <target state="translated">API</target>
        </trans-unit>
        <trans-unit id="b276f94cd8d0e74a21de6e5939b8c10ca9a975d6" translate="yes" xml:space="preserve">
          <source>API Reference</source>
          <target state="translated">API リファレンス</target>
        </trans-unit>
        <trans-unit id="044c42df47a473e04593a603631e399678960367" translate="yes" xml:space="preserve">
          <source>ARD is also known in the literature as &lt;em&gt;Sparse Bayesian Learning&lt;/em&gt; and &lt;em&gt;Relevance Vector Machine&lt;/em&gt;&lt;a href=&quot;#id16&quot; id=&quot;id12&quot;&gt;3&lt;/a&gt;&lt;a href=&quot;#id18&quot; id=&quot;id13&quot;&gt;4&lt;/a&gt;.</source>
          <target state="translated">ARDはまた、文献で知られている&lt;em&gt;疎ベイズ学習&lt;/em&gt;と&lt;em&gt;関連ベクトルマシン&lt;/em&gt;&lt;a href=&quot;#id16&quot; id=&quot;id12&quot;&gt;3 &lt;/a&gt;&lt;a href=&quot;#id18&quot; id=&quot;id13&quot;&gt;4&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="fc664a8aed1b7702083559e78f4a1b25e63a7739" translate="yes" xml:space="preserve">
          <source>ARD is also known in the literature as &lt;em&gt;Sparse Bayesian Learning&lt;/em&gt; and &lt;em&gt;Relevance Vector Machine&lt;/em&gt;&lt;a href=&quot;#id20&quot; id=&quot;id16&quot;&gt;[3]&lt;/a&gt;&lt;a href=&quot;#id21&quot; id=&quot;id17&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">ARDは、&lt;em&gt;スパースベイジアン学習&lt;/em&gt;および&lt;em&gt;関連性ベクトルマシン&lt;/em&gt;として文献でも知られています&lt;a href=&quot;#id20&quot; id=&quot;id16&quot;&gt;[3] &lt;/a&gt;&lt;a href=&quot;#id21&quot; id=&quot;id17&quot;&gt;[4]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a1df128dfacd3f460cbb61bb4087bb92287d3fcb" translate="yes" xml:space="preserve">
          <source>ARI</source>
          <target state="translated">ARI</target>
        </trans-unit>
        <trans-unit id="8c442cbb4124477c731be288798913c884c906af" translate="yes" xml:space="preserve">
          <source>ARI is a symmetric measure:</source>
          <target state="translated">ARIは対称的な尺度です。</target>
        </trans-unit>
        <trans-unit id="b6cdde34aa4cbbe354d4a8fe12fbb26711ad6710" translate="yes" xml:space="preserve">
          <source>ARI is symmetric, so labelings that have pure clusters with members coming from the same classes but unnecessary splits are penalized:</source>
          <target state="translated">ARIは対称性があるので、同じクラスから来たメンバーを持つ純粋なクラスタを持ち、不必要な分割があるラベリングはペナルティを受けます。</target>
        </trans-unit>
        <trans-unit id="5045d88e766edf44d9736d9751b0aa0b647864ac" translate="yes" xml:space="preserve">
          <source>A[i, j] is assigned the weight of edge that connects i to j.</source>
          <target state="translated">A[i,j]には、iとjを結ぶ辺の重みが割り当てられている。</target>
        </trans-unit>
        <trans-unit id="9f4a63ae0bf1594deea5ea12c509b27f409ce3db" translate="yes" xml:space="preserve">
          <source>Aaron Defazio, Francis Bach, Simon Lacoste-Julien: &lt;a href=&quot;https://arxiv.org/abs/1407.0202&quot;&gt;SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.&lt;/a&gt;</source>
          <target state="translated">Aaron Defazio、Francis Bach、Simon Lacoste-Julien：&lt;a href=&quot;https://arxiv.org/abs/1407.0202&quot;&gt;SAGA：非強く凸の複合目的をサポートする高速インクリメンタルグラデーション法。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e82c46cc189803ea687c96da2038982e9dcb6a4b" translate="yes" xml:space="preserve">
          <source>Ability to use shared memory efficiently with worker processes for large numpy-based datastructures.</source>
          <target state="translated">大規模なnumpyベースのデータ構造のためのワーカープロセスで共有メモリを効率的に使用する能力。</target>
        </trans-unit>
        <trans-unit id="54d1141ddccb7851744d76084cc8611f93e2cfc3" translate="yes" xml:space="preserve">
          <source>Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See &lt;a href=&quot;#tree-algorithms&quot;&gt;algorithms&lt;/a&gt; for more information.</source>
          <target state="translated">数値データとカテゴリーデータの両方を処理できます。他の手法は通常、1種類の変数しか持たないデータセットの分析に特化しています。詳細については、&lt;a href=&quot;#tree-algorithms&quot;&gt;アルゴリズム&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="2367c2ca7022225c20205571d9433b5b6b84cd68" translate="yes" xml:space="preserve">
          <source>Able to handle multi-output problems.</source>
          <target state="translated">複数出力の問題に対応できる。</target>
        </trans-unit>
        <trans-unit id="da22d4ef976c68e8fef4ef4e1a2681784cdddf3a" translate="yes" xml:space="preserve">
          <source>Above, we limited this regularization to a very little amount. Regularization improves the conditioning of the problem and reduces the variance of the estimates. RidgeCV applies cross validation in order to determine which value of the regularization parameter (&lt;code&gt;alpha&lt;/code&gt;) is best suited for prediction.</source>
          <target state="translated">上記では、この正則化をごくわずかな量に制限しました。正則化は、問題の条件付けを改善し、推定値の分散を減らします。RidgeCVは、正則化パラメーター（ &lt;code&gt;alpha&lt;/code&gt; ）のどの値が予測に最も適しているかを判断するために交差検定を適用します。</target>
        </trans-unit>
        <trans-unit id="1c09e3c38ba7261734dc4ab72dcf1ef3efd8a152" translate="yes" xml:space="preserve">
          <source>Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. Dimensions whose singular values are non-significant are discarded. Only used if solver is &amp;lsquo;svd&amp;rsquo;.</source>
          <target state="translated">Xの特異値が有意であると見なされるための絶対しきい値。Xのランクを推定するために使用されます。特異値が有意でない次元は破棄されます。ソルバーが「svd」の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="2aa749baee7b6d0eb7f8d7395643fd7e673c649b" translate="yes" xml:space="preserve">
          <source>Absolute threshold for a singular value to be considered significant, used to estimate the rank of &lt;code&gt;Xk&lt;/code&gt; where &lt;code&gt;Xk&lt;/code&gt; is the centered matrix of samples in class k. This parameter does not affect the predictions. It only controls a warning that is raised when features are considered to be colinear.</source>
          <target state="translated">&lt;code&gt;Xk&lt;/code&gt; のランクを推定するために使用される、有意と見なされる特異値の絶対しきい値。ここで、 &lt;code&gt;Xk&lt;/code&gt; はクラスkのサンプルの中心行列です。このパラメーターは予測に影響しません。フィーチャが同一線上にあると見なされた場合に発生する警告のみを制御します。</target>
        </trans-unit>
        <trans-unit id="f587c3583da9a191a72d5ff01fbfedde272f2898" translate="yes" xml:space="preserve">
          <source>Absolute tolerance for equivalence of arrays. Default = 1E-10.</source>
          <target state="translated">配列の等価性に対する絶対的な許容範囲。デフォルト=1E-10。</target>
        </trans-unit>
        <trans-unit id="d79faf207bac3f682107ababce293b4eceadf34c" translate="yes" xml:space="preserve">
          <source>Acceptable data types for the parameter.</source>
          <target state="translated">パラメータに使用可能なデータ型。</target>
        </trans-unit>
        <trans-unit id="a62a0f9649f4eb6fd92f87e85b475e87182d6e46" translate="yes" xml:space="preserve">
          <source>Access the fitted transformer by name.</source>
          <target state="translated">装着されている変圧器に名前でアクセスします。</target>
        </trans-unit>
        <trans-unit id="9e0e7377700ae6d6f91649710d89b9dd54aaa033" translate="yes" xml:space="preserve">
          <source>According to the JL lemma, projecting 500 samples without too much distortion will require at least several thousands dimensions, irrespective of the number of features of the original dataset.</source>
          <target state="translated">JLのリーマによれば,あまり歪みのない500サンプルを投影するには,元のデータセットの特徴量に関係なく,少なくとも数千次元の次元を必要とする.</target>
        </trans-unit>
        <trans-unit id="caff4ed631e2a64d90ad9e75e695e4cb077ff929" translate="yes" xml:space="preserve">
          <source>According to the model above, the log of the posterior is:</source>
          <target state="translated">上記のモデルによれば、事後処理の対数は</target>
        </trans-unit>
        <trans-unit id="1e10c7e434a3850cc1ce980a85ce2c40965ebbd1" translate="yes" xml:space="preserve">
          <source>According to the observed data, the frequency of accidents is higher for drivers younger than 30 years old, and is positively correlated with the &lt;code&gt;BonusMalus&lt;/code&gt; variable. Our model is able to mostly correctly model this behaviour.</source>
          <target state="translated">観測データによると、事故の頻度は30歳未満のドライバーの方が高く、 &lt;code&gt;BonusMalus&lt;/code&gt; 変数と正の相関があります。私たちのモデルは、この振る舞いをほぼ正しくモデル化することができます。</target>
        </trans-unit>
        <trans-unit id="7b168ce1b26b0ed19f0857c9a9e0275bc2a86de2" translate="yes" xml:space="preserve">
          <source>Accuracy classification score.</source>
          <target state="translated">精度の分類スコア。</target>
        </trans-unit>
        <trans-unit id="ed1b27307b9c829fdd35ed1ccf294d2e82a6dfcb" translate="yes" xml:space="preserve">
          <source>Accuracy of the Model</source>
          <target state="translated">モデルの精度</target>
        </trans-unit>
        <trans-unit id="b171ff92fbbc89c347ecf11b61ad701495ae9495" translate="yes" xml:space="preserve">
          <source>Accuracy vs alpha for training and testing sets</source>
          <target state="translated">トレーニングセットとテストセットの精度対アルファ</target>
        </trans-unit>
        <trans-unit id="3159fc0c287c6fa355557d0a32e1e4fbdd60d32b" translate="yes" xml:space="preserve">
          <source>Across the module, we designate the vector \(w = (w_1, ..., w_p)\) as &lt;code&gt;coef_&lt;/code&gt; and \(w_0\) as &lt;code&gt;intercept_&lt;/code&gt;.</source>
          <target state="translated">モジュール全体で、ベクトル\（w =（w_1、...、w_p）\）を &lt;code&gt;coef_&lt;/code&gt; として、\（w_0 \）を &lt;code&gt;intercept_&lt;/code&gt; として指定します。</target>
        </trans-unit>
        <trans-unit id="1e74068a27362b129f4808ec6412c747359a90d1" translate="yes" xml:space="preserve">
          <source>Activation function for the hidden layer.</source>
          <target state="translated">隠れ層の活性化関数。</target>
        </trans-unit>
        <trans-unit id="13466b0d67f826e470f7f662a0fa9b98771bd57d" translate="yes" xml:space="preserve">
          <source>Actual class (observation)</source>
          <target state="translated">実際の授業(見学</target>
        </trans-unit>
        <trans-unit id="4bda103291a01841e5d33c04e072de1679753a3f" translate="yes" xml:space="preserve">
          <source>Actual number of iteration for each Cs.</source>
          <target state="translated">各 Cs の実際の反復回数。</target>
        </trans-unit>
        <trans-unit id="b9e108d70e5d6e1ab362876cb4abfe8b3ea0d61c" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1.</source>
          <target state="translated">すべてのクラス、ひだ、Csの反復の実際の数。2進法または多項式の場合、1次元は1に等しい。</target>
        </trans-unit>
        <trans-unit id="e15b0921ef0e026565cb96038a661f88e504a7d1" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1. If &lt;code&gt;penalty='elasticnet'&lt;/code&gt;, the shape is &lt;code&gt;(n_classes, n_folds,
n_cs, n_l1_ratios)&lt;/code&gt; or &lt;code&gt;(1, n_folds, n_cs, n_l1_ratios)&lt;/code&gt;.</source>
          <target state="translated">すべてのクラス、フォールド、およびCの実際の反復回数。バイナリまたは多項の場合、最初の次元は1に等しくなります。 &lt;code&gt;penalty='elasticnet'&lt;/code&gt; 場合、形状は &lt;code&gt;(n_classes, n_folds, n_cs, n_l1_ratios)&lt;/code&gt; または &lt;code&gt;(1, n_folds, n_cs, n_l1_ratios)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="e7c00438aea9f14670d9e5ce3ad69eadb9e56fa7" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given.</source>
          <target state="translated">すべてのクラスの実際の反復回数。2値または多項式の場合は1つの要素のみを返します。liblinearソルバーの場合は、全クラスにわたる最大反復回数のみが与えられます。</target>
        </trans-unit>
        <trans-unit id="41eb0ffb0b86a2b7fa1c30caad1eab7979f038bb" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.</source>
          <target state="translated">各ターゲットに対する実際の反復回数.sag および lsqr ソルバでのみ利用可能。他のソルバは None を返します。</target>
        </trans-unit>
        <trans-unit id="5f183a64b01f4bbfef50fc734f02625115726f0b" translate="yes" xml:space="preserve">
          <source>Actual number of iterations used in the solver.</source>
          <target state="translated">ソルバーで使用された実際の反復回数。</target>
        </trans-unit>
        <trans-unit id="a5f57bad41ea8a1cc3f7ad65dabd66fae549152b" translate="yes" xml:space="preserve">
          <source>Actual number of iterations.</source>
          <target state="translated">実際の反復回数。</target>
        </trans-unit>
        <trans-unit id="e88dbeae46a975e549a3e1b429ac335f1717a327" translate="yes" xml:space="preserve">
          <source>AdaBoost can be used both for classification and regression problems:</source>
          <target state="translated">AdaBoostは、分類問題と回帰問題の両方に使用することができます。</target>
        </trans-unit>
        <trans-unit id="1c3d2a8d1e7ee52688369f7b268da1f090f613a4" translate="yes" xml:space="preserve">
          <source>Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update parameters based on adaptive estimates of lower-order moments.</source>
          <target state="translated">Adamは確率的なオプティマイザという意味でSGDに似ていますが、低次モーメントの適応的な推定値に基づいてパラメータを更新する量を自動的に調整することができます。</target>
        </trans-unit>
        <trans-unit id="37193f0def1de066166921b64d73e81a4207486f" translate="yes" xml:space="preserve">
          <source>Add plots</source>
          <target state="translated">圃場の追加</target>
        </trans-unit>
        <trans-unit id="71042ebf81b88cec72926e5e2da3d76737a8fc09" translate="yes" xml:space="preserve">
          <source>Adding a constant kernel is equivalent to adding a constant:</source>
          <target state="translated">定数カーネルを追加することは、定数を追加することと同等です。</target>
        </trans-unit>
        <trans-unit id="57edf9b3cf9cc602f38d55f2f3497cc8e4458efe" translate="yes" xml:space="preserve">
          <source>Adding parameters that do not influence the performance does not decrease efficiency.</source>
          <target state="translated">性能に影響しないパラメータを追加しても効率が下がることはありません。</target>
        </trans-unit>
        <trans-unit id="d8b82f8ac1fd93a8c5863d25cc6558cb656b7f40" translate="yes" xml:space="preserve">
          <source>Additional Resources</source>
          <target state="translated">追加リソース</target>
        </trans-unit>
        <trans-unit id="28c6073bb46543d76ed56428ba51e1f98f703736" translate="yes" xml:space="preserve">
          <source>Additional fit parameters.</source>
          <target state="translated">適合パラメータを追加しました。</target>
        </trans-unit>
        <trans-unit id="3bc09d37c7749b14dd88f92786b449d978558dbf" translate="yes" xml:space="preserve">
          <source>Additional keyword arguments for the metric function.</source>
          <target state="translated">メトリック関数の追加キーワード引数。</target>
        </trans-unit>
        <trans-unit id="448e0ea40ef1f0816d1ba3c00c7fd9818ee1579c" translate="yes" xml:space="preserve">
          <source>Additional keyword arguments for the metric function. For most metrics will be same with &lt;code&gt;metric_params&lt;/code&gt; parameter, but may also contain the &lt;code&gt;p&lt;/code&gt; parameter value if the &lt;code&gt;effective_metric_&lt;/code&gt; attribute is set to &amp;lsquo;minkowski&amp;rsquo;.</source>
          <target state="translated">メトリック関数の追加のキーワード引数。ほとんどのメトリックは &lt;code&gt;metric_params&lt;/code&gt; パラメーターと同じですが、 &lt;code&gt;effective_metric_&lt;/code&gt; 属性が 'minkowski'に設定されている場合は、 &lt;code&gt;p&lt;/code&gt; パラメーター値も含まれる場合があります。</target>
        </trans-unit>
        <trans-unit id="9a84da277628ee33f1ee92b82eb0c5a6af6d0524" translate="yes" xml:space="preserve">
          <source>Additional number of random vectors to sample the range of M so as to ensure proper conditioning. The total number of random vectors used to find the range of M is n_components + n_oversamples. Smaller number can improve speed but can negatively impact the quality of approximation of singular vectors and singular values.</source>
          <target state="translated">適切な条件付けを確実にするために、Mの範囲をサンプリングするためのランダムベクトルの追加数。M の範囲を求めるために使用されるランダムベクトルの総数は n_components+n_oversamples です。この数を少なくすると速度が向上しますが、特異ベクトルや特異値の近似の品質に悪影響を与える可能性があります。</target>
        </trans-unit>
        <trans-unit id="7cc2d5826a730e9f6abc3aeeee28797fe00b8570" translate="yes" xml:space="preserve">
          <source>Additional parameter passed to the fit function of the estimator.</source>
          <target state="translated">推定子のはめ込み関数に渡される追加のパラ メータ。</target>
        </trans-unit>
        <trans-unit id="df9b29356cfac6bb58ec5dca9394f2d9dfc4703b" translate="yes" xml:space="preserve">
          <source>Additional parameters (keyword arguments) for kernel function passed as callable object.</source>
          <target state="translated">呼び出し可能なオブジェクトとして渡されるカーネル関数の追加パラメータ(キーワード引数)。</target>
        </trans-unit>
        <trans-unit id="39a26d6ec93b4bd557ff0d3e3333c06661f787d5" translate="yes" xml:space="preserve">
          <source>Additional parameters to be passed to score_func.</source>
          <target state="translated">score_func に渡す追加のパラメータ。</target>
        </trans-unit>
        <trans-unit id="71247cd4c08f9e15e8cf580ef4b1b215b5be4541" translate="yes" xml:space="preserve">
          <source>Additional parameters to be passed to the tree for use with the metric. For more information, see the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">メトリックで使用するためにツリーに渡される追加のパラメーター。詳細については、&lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; の&lt;/a&gt;ドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="36da43716a16aade125d9759679637e39761ccc9" translate="yes" xml:space="preserve">
          <source>Additionally compute class covariance matrix (default False), used only in &amp;lsquo;svd&amp;rsquo; solver.</source>
          <target state="translated">さらに、「svd」ソルバーでのみ使用されるクラス共分散行列（デフォルトはFalse）を計算します。</target>
        </trans-unit>
        <trans-unit id="cb731300b5471fde45bf8790ae9ce49720693175" translate="yes" xml:space="preserve">
          <source>Additionally, &lt;code&gt;Pipeline&lt;/code&gt; can be instantiated with the &lt;code&gt;memory&lt;/code&gt; argument to memoize the transformers within the pipeline, avoiding to fit again the same transformers over and over.</source>
          <target state="translated">さらに、 &lt;code&gt;Pipeline&lt;/code&gt; を &lt;code&gt;memory&lt;/code&gt; 引数でインスタンス化して、パイプライン内のトランスフォーマーをメモし、同じトランスフォーマーを何度も何度も適合させることを回避できます。</target>
        </trans-unit>
        <trans-unit id="4fcf36ef35c92f01311334b57dea32bb115ca779" translate="yes" xml:space="preserve">
          <source>Additionally, latent semantic analysis can also be used to reduce dimensionality and discover latent patterns in the data.</source>
          <target state="translated">さらに、潜在的な意味分析は、次元を減らし、データの潜在的なパターンを発見するために使用することもできます。</target>
        </trans-unit>
        <trans-unit id="c13231af636098d832d9fddca4838809fd525e6e" translate="yes" xml:space="preserve">
          <source>Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).</source>
          <target state="translated">加法(ラプラス/リドストーン)平滑化パラメータ(平滑化なしの場合は0)。</target>
        </trans-unit>
        <trans-unit id="b8c1aaec1a2a62d157d744d0faee837709b2772c" translate="yes" xml:space="preserve">
          <source>Adjacency matrix of the graph</source>
          <target state="translated">グラフの隣接行列</target>
        </trans-unit>
        <trans-unit id="250995e59d0465859871db4d7d37821f2e9e268c" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information</source>
          <target state="translated">調整済み相互情報</target>
        </trans-unit>
        <trans-unit id="11e5941af4a261b664e9f0491f0ecd18e8412b46" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. It accounts for the fact that the MI is generally higher for two clusterings with a larger number of clusters, regardless of whether there is actually more information shared. For two clusterings \(U\) and \(V\), the AMI is given as:</source>
          <target state="translated">調整済み相互情報 (AMI)は、相互情報 (MI)スコアを偶然性を考慮して調整したものです。これは、実際により多くの情報が共有されているかどうかに関係なく、より多くのクラスター数を持つ2つのクラスターの方が一般的にMIが高いという事実を説明する。2つのクラスター化について \(U)と\(V)の場合、AMIは次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="4a1b6ee1509942e0e22b1880790459ca848319f4" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information (adjusted against chance)</source>
          <target state="translated">調整済み相互情報(偶然性に対して調整済み</target>
        </trans-unit>
        <trans-unit id="4630f03a5a119c76517981dad27161c68958ec63" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information between two clusterings.</source>
          <target state="translated">2つのクラスタリング間の調整された相互情報。</target>
        </trans-unit>
        <trans-unit id="1802b8bdeb7acaeda57735feed31e0ee765d7139" translate="yes" xml:space="preserve">
          <source>Adjusted Rand Index</source>
          <target state="translated">調整済みランド指数</target>
        </trans-unit>
        <trans-unit id="31e214397fb70765d3e9e011e4c9138d3d446804" translate="yes" xml:space="preserve">
          <source>Adjusted against chance Mutual Information</source>
          <target state="translated">確率に対して調整済み 相互情報</target>
        </trans-unit>
        <trans-unit id="a058b0be3c60201e595de35ba7261933ab6a3173" translate="yes" xml:space="preserve">
          <source>Adjusted for chance measure such as ARI display some random variations centered around a mean score of 0.0 for any number of samples and clusters.</source>
          <target state="translated">ARIのような偶然性測定のために調整されており、任意の数のサンプルやクラスターに対して0.0の平均スコアを中心としたいくつかのランダムな変動を表示します。</target>
        </trans-unit>
        <trans-unit id="b80cf8c9df3b30a05f03bffeb2976e09c960b38a" translate="yes" xml:space="preserve">
          <source>Adjustment for chance in clustering performance evaluation</source>
          <target state="translated">クラスタリング性能評価における偶然性の調整</target>
        </trans-unit>
        <trans-unit id="7a26e45b68e65e5d705e3f16c6dd31629c5f071a" translate="yes" xml:space="preserve">
          <source>Advanced Plotting With Partial Dependence</source>
          <target state="translated">部分依存性を利用した高度なプロッティング</target>
        </trans-unit>
        <trans-unit id="c22815e21882a0aa7a6a9b8a93ee5e8e17ca7614" translate="yes" xml:space="preserve">
          <source>Affects shape of transform output only when voting=&amp;rsquo;soft&amp;rsquo; If voting=&amp;rsquo;soft&amp;rsquo; and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes).</source>
          <target state="translated">voting = 'soft'の場合にのみ、変換出力の形状に影響します。flatten_transform = Falseの場合、（n_classifiers、n_samples、n_classes）を返します。</target>
        </trans-unit>
        <trans-unit id="6be196ba0996dde7d9ed4c8d27213a288c55819b" translate="yes" xml:space="preserve">
          <source>Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the &lt;em&gt;preference&lt;/em&gt;, which controls how many exemplars are used, and the &lt;em&gt;damping factor&lt;/em&gt; which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages.</source>
          <target state="translated">アフィニティ伝播は、提供されたデータに基づいてクラスターの数を選択するため、興味深い場合があります。この目的のために、2つの重要なパラメーターは、使用される見本の数を制御する&lt;em&gt;設定&lt;/em&gt;と、これらのメッセージを更新するときに数値の変動を回避するために責任メッセージと可用性メッセージを減衰させる&lt;em&gt;減衰係数&lt;/em&gt;です。</target>
        </trans-unit>
        <trans-unit id="f0a35e8a6a6b08571acff68f4d3ca53b52b812c7" translate="yes" xml:space="preserve">
          <source>Affinity matrix used for clustering. Available only if after calling &lt;code&gt;fit&lt;/code&gt;.</source>
          <target state="translated">クラスタリングに使用されるアフィニティマトリックス。 &lt;code&gt;fit&lt;/code&gt; を呼び出した後にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="63246ae129dc66d8916faa22ac2026722fe870df" translate="yes" xml:space="preserve">
          <source>Affinity propagation</source>
          <target state="translated">親和性の伝播</target>
        </trans-unit>
        <trans-unit id="50cd6f7778a2976318486216f7be1102ea3c2dbd" translate="yes" xml:space="preserve">
          <source>Affinity_matrix constructed from samples or precomputed.</source>
          <target state="translated">サンプルから構築されたAffinity_matrix、または事前に計算されたAffinity_matrix。</target>
        </trans-unit>
        <trans-unit id="9d6ec54ac3f9f07d1c79bc7828709b2a1ebe0b76" translate="yes" xml:space="preserve">
          <source>After being fitted, the model can then be used to predict new values:</source>
          <target state="translated">フィットした後、モデルは新しい値を予測するために使用することができます。</target>
        </trans-unit>
        <trans-unit id="56033184bfd4d9eb6d5cedda898ff603a56457b5" translate="yes" xml:space="preserve">
          <source>After being fitted, the model can then be used to predict the class of samples:</source>
          <target state="translated">フィットされた後、モデルはサンプルのクラスを予測するために使用することができます。</target>
        </trans-unit>
        <trans-unit id="d1a9437feb3595025967367029f3b25aca08ed48" translate="yes" xml:space="preserve">
          <source>After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.</source>
          <target state="translated">このメソッドを呼び出した後、partial_fitメソッド(もしあれば)を使ってさらにフィッティングを行うと、densifyを呼び出すまで動作しません。</target>
        </trans-unit>
        <trans-unit id="1ac4b13a36333967e31b510f39d76728b1ade858" translate="yes" xml:space="preserve">
          <source>After discretization, linear regression and decision tree make exactly the same prediction. As features are constant within each bin, any model must predict the same value for all points within a bin. Compared with the result before discretization, linear model become much more flexible while decision tree gets much less flexible. Note that binning features generally has no beneficial effect for tree-based models, as these models can learn to split up the data anywhere.</source>
          <target state="translated">離散化後、線形回帰と決定木は全く同じ予測を行います。特徴量は各ビン内で一定なので、どのモデルもビン内のすべての点で同じ値を予測しなければなりません。離散化前の結果と比較すると、線形モデルの方が柔軟性が高く、決定木の方が柔軟性が低くなります。特徴をビン分けすることは、一般的にツリーベースのモデルにとって有益な効果がないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="eef177bc3be0e883eb6e86188495b80899ebaefb" translate="yes" xml:space="preserve">
          <source>After fitting (training), the model can predict labels for new samples:</source>
          <target state="translated">フィッティング(訓練)後、モデルは新しいサンプルのラベルを予測することができます。</target>
        </trans-unit>
        <trans-unit id="5e6425357ee2c6e34c7ccf4e621fb9b2e0d28394" translate="yes" xml:space="preserve">
          <source>After fitting a model, row and column cluster membership can be found in the &lt;code&gt;rows_&lt;/code&gt; and &lt;code&gt;columns_&lt;/code&gt; attributes. &lt;code&gt;rows_[i]&lt;/code&gt; is a binary vector with nonzero entries corresponding to rows that belong to bicluster &lt;code&gt;i&lt;/code&gt;. Similarly, &lt;code&gt;columns_[i]&lt;/code&gt; indicates which columns belong to bicluster &lt;code&gt;i&lt;/code&gt;.</source>
          <target state="translated">モデルをフィッティングした後、行と列のクラスターメンバーシップは、 &lt;code&gt;rows_&lt;/code&gt; および &lt;code&gt;columns_&lt;/code&gt; 属性で確認できます。 &lt;code&gt;rows_[i]&lt;/code&gt; は、バイクラスター &lt;code&gt;i&lt;/code&gt; に属する行に対応するゼロ以外のエントリを持つバイナリベクトルです。同様に、 &lt;code&gt;columns_[i]&lt;/code&gt; は、bicluster &lt;code&gt;i&lt;/code&gt; に属する列を示します。</target>
        </trans-unit>
        <trans-unit id="71ae50af31deda9eff4041050d01116db2fe0b76" translate="yes" xml:space="preserve">
          <source>After normalizing, the first few singular vectors are computed, just as in the Spectral Co-Clustering algorithm.</source>
          <target state="translated">正規化の後、Spectral Co-Clustering アルゴリズムと同様に、最初の数個の特異ベクトルが計算されます。</target>
        </trans-unit>
        <trans-unit id="c326685e6de3fbd40d347b380da45cf33d03051c" translate="yes" xml:space="preserve">
          <source>After this operation, \(U_k \Sigma_k^\top\) is the transformed training set with \(k\) features (called &lt;code&gt;n_components&lt;/code&gt; in the API).</source>
          <target state="translated">この操作の後、\（U_k \ Sigma_k ^ \ top \）は\（k \）機能（APIでは &lt;code&gt;n_components&lt;/code&gt; と呼ばれます）を使用して変換されたトレーニングセットです。</target>
        </trans-unit>
        <trans-unit id="d10a093c92eea73e29249b26fe409d51c485f495" translate="yes" xml:space="preserve">
          <source>After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. The following section gives you an example of how to persist a model with pickle. We&amp;rsquo;ll also review a few security and maintainability issues when working with pickle serialization.</source>
          <target state="translated">scikit-learnモデルをトレーニングした後、再トレーニングせずにモデルを永続的に使用できるようにする方法が望ましいです。次のセクションでは、pickleを使用してモデルを永続化する方法の例を示します。また、ピクルシリアライゼーションを使用する際のセキュリティと保守性の問題についても確認します。</target>
        </trans-unit>
        <trans-unit id="24449d2cd36cc7fedaa85e80b22b4a39b7f8e2cc" translate="yes" xml:space="preserve">
          <source>After using such a procedure to fit the dictionary, the transform is simply a sparse coding step that shares the same implementation with all dictionary learning objects (see &lt;a href=&quot;#sparsecoder&quot;&gt;Sparse coding with a precomputed dictionary&lt;/a&gt;).</source>
          <target state="translated">このような手順を使用してディクショナリを適合させた後の変換は、すべてのディクショナリ学習オブジェクトと同じ実装を共有するスパースコーディングステップです（&lt;a href=&quot;#sparsecoder&quot;&gt;事前計算済みディクショナリを使用したスパースコーディングを&lt;/a&gt;参照）。</target>
        </trans-unit>
        <trans-unit id="2b35bdc816ab119e837a0f526e39f92fef72f3ba" translate="yes" xml:space="preserve">
          <source>Again please see the &lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;reference documentation&lt;/a&gt; for the details on all the parameters.</source>
          <target state="translated">ここでも、すべてのパラメーターの詳細については、&lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;リファレンスドキュメント&lt;/a&gt;をご覧ください。</target>
        </trans-unit>
        <trans-unit id="99e1009ea0e8d7b9e4e16a4193c8696d68c4efff" translate="yes" xml:space="preserve">
          <source>Again, we check the performance of the computed model using, for example, the median absolute error of the model and the R squared coefficient.</source>
          <target state="translated">ここでも,例えば,モデルの中央値の絶対誤差やRの2乗係数を用いて,計算されたモデルの性能をチェックします.</target>
        </trans-unit>
        <trans-unit id="ff9f1ff32120d8b893c1ded522d49590353b29a6" translate="yes" xml:space="preserve">
          <source>Age</source>
          <target state="translated">Age</target>
        </trans-unit>
        <trans-unit id="e869aecf975cf3b336fce1699b0d4503e42564de" translate="yes" xml:space="preserve">
          <source>Agglomerate features.</source>
          <target state="translated">凝集体の特徴。</target>
        </trans-unit>
        <trans-unit id="dbb914491e4e2790ae24eaabcabbe0b9aa470c77" translate="yes" xml:space="preserve">
          <source>Agglomerative Clustering</source>
          <target state="translated">凝集型クラスタリング</target>
        </trans-unit>
        <trans-unit id="35bb423e043758761d2f3e248257c1ca705a5ac8" translate="yes" xml:space="preserve">
          <source>Agglomerative cluster has a &amp;ldquo;rich get richer&amp;rdquo; behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes. However, the affinity (or distance used in clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative. Single linkage, while not robust to noisy data, can be computed very efficiently and can therefore be useful to provide hierarchical clustering of larger datasets. Single linkage can also perform well on non-globular data.</source>
          <target state="translated">凝集クラスターには、「リッチゲットリッチ」動作があり、クラスターサイズが不均一になります。この点で、単一リンケージは最悪の戦略であり、ウォードは最も規則的なサイズを提供します。ただし、アフィニティ（またはクラスタリングで使用される距離）はWardで変更できないため、非ユークリッドメトリックの場合、平均リンケージが適切な代替手段となります。単一のリンケージは、ノイズの多いデータに対して堅牢ではありませんが、非常に効率的に計算できるため、より大きなデータセットの階層的クラスタリングを提供するのに役立ちます。単一リンケージは、非グローバルデータに対しても適切に実行できます。</target>
        </trans-unit>
        <trans-unit id="890d73cb8666b8b11b8fc128eb14fe1823b0fbff" translate="yes" xml:space="preserve">
          <source>Agglomerative clustering</source>
          <target state="translated">凝集型クラスタリング</target>
        </trans-unit>
        <trans-unit id="21c09abace100a2c8351e8288af87cf02023a3a7" translate="yes" xml:space="preserve">
          <source>Agglomerative clustering with and without structure</source>
          <target state="translated">構造の有無にかかわらず凝集型クラスタリング</target>
        </trans-unit>
        <trans-unit id="06e589f9bfed18f64327ae7d57413054b3c6f8de" translate="yes" xml:space="preserve">
          <source>Agglomerative clustering with different metrics</source>
          <target state="translated">異なるメトリクスを用いた凝集型クラスタリング</target>
        </trans-unit>
        <trans-unit id="4d6b62f2cefed7f7111116c687eea9e3676aa5b0" translate="yes" xml:space="preserve">
          <source>Agnostic</source>
          <target state="translated">Agnostic</target>
        </trans-unit>
        <trans-unit id="ac9f2566d02b5e4600cd56af348af55d70c023a7" translate="yes" xml:space="preserve">
          <source>Agnostic:</source>
          <target state="translated">Agnostic:</target>
        </trans-unit>
        <trans-unit id="a679479691140b63a2247391cf7941ca13e56589" translate="yes" xml:space="preserve">
          <source>Agriculture / weather modeling: number of rain events per year (Poisson), amount of rainfall per event (Gamma), total rainfall per year (Tweedie / Compound Poisson Gamma).</source>
          <target state="translated">農業・気象モデル:年間降雨回数(ポアソン)、1回あたりの降雨量(ガンマ)、年間総降雨量(ツイーディー/複合ポアソンガンマ)。</target>
        </trans-unit>
        <trans-unit id="a88029fe8eca09e8e53e31e892c06950617c42cc" translate="yes" xml:space="preserve">
          <source>Akaike information criterion for the current model on the input X.</source>
          <target state="translated">入力X上の現在のモデルの赤池情報基準。</target>
        </trans-unit>
        <trans-unit id="471a24dc120a414840a5a390be002dd1fb20dce7" translate="yes" xml:space="preserve">
          <source>Alcalinity of Ash:</source>
          <target state="translated">灰のアルカリ度。</target>
        </trans-unit>
        <trans-unit id="1c996292fc0f34e9abb2ed1bce253f665b2889f0" translate="yes" xml:space="preserve">
          <source>Alcalinity of ash</source>
          <target state="translated">灰のアルコール度</target>
        </trans-unit>
        <trans-unit id="35fa6a7b518a822ef300d3ac95936a0d4551ed5f" translate="yes" xml:space="preserve">
          <source>Alcohol</source>
          <target state="translated">Alcohol</target>
        </trans-unit>
        <trans-unit id="4537230d988c507a37af5da4b6f068cb25fae3fd" translate="yes" xml:space="preserve">
          <source>Alcohol:</source>
          <target state="translated">Alcohol:</target>
        </trans-unit>
        <trans-unit id="a755c4c6a00484a10fdd39c78f5316741e33717a" translate="yes" xml:space="preserve">
          <source>Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good Probabilities With Supervised Learning, in Proceedings of the 22nd International Conference on Machine Learning (ICML). See section 4 (Qualitative Analysis of Predictions).</source>
          <target state="translated">Alexandru Niculescu-Mizil and Rich Caruana (2005)Predicting Good Probabilities With Supervised Learning,Proceedings of the 22nd International Conference on Machine Learning (ICML).第4節(予測の質的分析)を参照のこと。</target>
        </trans-unit>
        <trans-unit id="e00df3c495c9c818a9560b719b9887130853cc0a" translate="yes" xml:space="preserve">
          <source>Algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors instance.</source>
          <target state="translated">近傍探索に使用するアルゴリズムは、neighbors.NearestNeighborsインスタンスに渡されます。</target>
        </trans-unit>
        <trans-unit id="c6ad136737477e6b0db26cf9286713a801e6cd45" translate="yes" xml:space="preserve">
          <source>Algorithm to use in the optimization problem.</source>
          <target state="translated">最適化問題で使用するアルゴリズム。</target>
        </trans-unit>
        <trans-unit id="c739143784c9537893f5a8383165eeec97c8efbf" translate="yes" xml:space="preserve">
          <source>Algorithm used to compute the nearest neighbors:</source>
          <target state="translated">最も近い隣人を計算するために使用されるアルゴリズム。</target>
        </trans-unit>
        <trans-unit id="cb151b633578d2167df8c911cd7019b2a2913090" translate="yes" xml:space="preserve">
          <source>Algorithm used to transform the data lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection &lt;code&gt;dictionary * X'&lt;/code&gt;</source>
          <target state="translated">データlarsの変換に使用されるアルゴリズム：最小角度回帰法（linear_model.lars_path）を使用lasso_lars：Larsを使用してLasso解を計算lasso_cd：座標降下法を使用してLasso解（linear_model.Lasso）を計算推定コンポーネントがスパースである場合、lasso_larsはより高速になります。 omp：直交マッチング追跡を使用して、スパースソリューションのしきい値を推定します：射影 &lt;code&gt;dictionary * X'&lt;/code&gt; からのアルファ未満のすべての係数をゼロに圧縮します* X '</target>
        </trans-unit>
        <trans-unit id="59c5f2e6c851221e36e62f9314e77aa5b776cd59" translate="yes" xml:space="preserve">
          <source>Algorithm used to transform the data. lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X&amp;rsquo;</source>
          <target state="translated">データの変換に使用されるアルゴリズム。 lars：最小角度回帰法（linear_model.lars_path）を使用します。lasso_lars：Larsを使用してLasso解を計算しますlasso_cd：座標降下法を使用してLasso解（linear_model.Lasso）を計算します。推定コンポーネントがスパースである場合、lasso_larsはより高速になります。 omp：直交マッチング追跡を使用して、スパースソリューションのしきい値を推定します：射影辞書からのアルファより小さいすべての係数をゼロに押しつぶします* X '</target>
        </trans-unit>
        <trans-unit id="d57ea62b7e17ad495e3491babf97d100e18a1702" translate="yes" xml:space="preserve">
          <source>Algorithm used to transform the data: lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection &lt;code&gt;dictionary * X'&lt;/code&gt;</source>
          <target state="translated">データの変換に使用されるアルゴリズム：lars：最小角度回帰法（linear_model.lars_path）を使用lasso_lars：Larsを使用してLasso解を計算lasso_cd：座標降下法を使用してLasso解（linear_model.Lasso）を計算推定コンポーネントがスパースである場合、lasso_larsはより高速になります。 omp：直交マッチング追跡を使用して、スパースソリューションのしきい値を推定します：射影 &lt;code&gt;dictionary * X'&lt;/code&gt; からのアルファ未満のすべての係数をゼロに圧縮します* X '</target>
        </trans-unit>
        <trans-unit id="6dd589ff391e3fe9203a6e5ffaf1bc46cb8b74f7" translate="yes" xml:space="preserve">
          <source>Algorithms also differ in how rows and columns may be assigned to biclusters, which leads to different bicluster structures. Block diagonal or checkerboard structures occur when rows and columns are divided into partitions.</source>
          <target state="translated">アルゴ リ ズ ムはまた、 行 と 列のバイ ク ラ ス タ ーへの割 り 当て方が異な る ため、 バイ ク ラ ス タ ー構造が異な り ます。ブロック対角線構造またはチェッカーボード構造は、行と列がパーティションに分割されている場合に発生します。</target>
        </trans-unit>
        <trans-unit id="06560884189da8b18b9514c9f15fcee660a6560e" translate="yes" xml:space="preserve">
          <source>Algorithms differ in how they define biclusters. Some of the common types include:</source>
          <target state="translated">アルゴリズムは、バイクラスターをどのように定義するかによって異なります。一般的なタイプには、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="f7b4372e3de7ce07bac66d661704328db6f955a4" translate="yes" xml:space="preserve">
          <source>Alias for field number 0</source>
          <target state="translated">フィールド番号0のエイリアス</target>
        </trans-unit>
        <trans-unit id="bbef6b362c3d3509157f18014e4e5a25eb4e07ea" translate="yes" xml:space="preserve">
          <source>Alias for field number 1</source>
          <target state="translated">フィールド番号1のエイリアス</target>
        </trans-unit>
        <trans-unit id="cb7d09e2006a3aec07c13d82496b6f2adb24a1f7" translate="yes" xml:space="preserve">
          <source>Alias for field number 2</source>
          <target state="translated">フィールド番号2の別名</target>
        </trans-unit>
        <trans-unit id="2116d748feb69a3af8d3d3f32852bff649bb421e" translate="yes" xml:space="preserve">
          <source>Alias for field number 3</source>
          <target state="translated">フィールド番号3の別名</target>
        </trans-unit>
        <trans-unit id="8ba0e524b527e040b1c69a39c8c3727540cd8735" translate="yes" xml:space="preserve">
          <source>Alias for field number 4</source>
          <target state="translated">フィールド番号4のエイリアス</target>
        </trans-unit>
        <trans-unit id="e972f77e5bbab5ce66decc0654515ea9c4cce33b" translate="yes" xml:space="preserve">
          <source>All Gaussian process kernels are interoperable with &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; and vice versa: instances of subclasses of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; can be passed as &lt;code&gt;metric&lt;/code&gt; to &lt;code&gt;pairwise_kernels&lt;/code&gt; from &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt;. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt;&lt;code&gt;PairwiseKernel&lt;/code&gt;&lt;/a&gt;. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter &lt;code&gt;gamma&lt;/code&gt; is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed.</source>
          <target state="translated">すべてのガウス過程カーネルは&lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt;と相互運用可能であり、その逆も同様です。&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;のサブクラスのインスタンスは、&lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt;から &lt;code&gt;pairwise_kernels&lt;/code&gt; に &lt;code&gt;metric&lt;/code&gt; として渡すことができます。さらに、ペアワイズのカーネル関数は、ラッパークラス&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt; &lt;code&gt;PairwiseKernel&lt;/code&gt; &lt;/a&gt;を使用してGPカーネルとして使用できます。唯一の注意点は、ハイパーパラメータの勾配は分析的ではなく数値であり、これらのカーネルはすべて等方性距離のみをサポートしていることです。パラメータ &lt;code&gt;gamma&lt;/code&gt; はハイパーパラメータと見なされ、最適化できます。他のカーネルパラメータは初期化時に直接設定され、固定されたままになります。</target>
        </trans-unit>
        <trans-unit id="c99cb316a9285e1cd15f4f7521535d0751148726" translate="yes" xml:space="preserve">
          <source>All Gaussian process kernels are interoperable with &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; and vice versa: instances of subclasses of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; can be passed as &lt;code&gt;metric&lt;/code&gt; to pairwise_kernels`` from &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt;. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt;&lt;code&gt;PairwiseKernel&lt;/code&gt;&lt;/a&gt;. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter &lt;code&gt;gamma&lt;/code&gt; is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed.</source>
          <target state="translated">すべてのガウスプロセスカーネルは&lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt;と相互運用でき、その逆も同様です。&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;のサブクラスのインスタンスは、 &lt;code&gt;metric&lt;/code&gt; として&lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt;からpairwise_kernelsに渡すことができます。さらに、ラッパークラス&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt; &lt;code&gt;PairwiseKernel&lt;/code&gt; &lt;/a&gt;を使用することにより、pairwiseのカーネル関数をGPカーネルとして使用できます。唯一の注意点は、ハイパーパラメーターの勾配が分析的ではなく数値的であり、これらのすべてのカーネルが等方性距離のみをサポートすることです。パラメータ &lt;code&gt;gamma&lt;/code&gt; はハイパーパラメータと見なされ、最適化される場合があります。他のカーネルパラメータは初期化時に直接設定され、固定されたままになります。</target>
        </trans-unit>
        <trans-unit id="facb53030a78059078bd7255fa4340b830781f6e" translate="yes" xml:space="preserve">
          <source>All above functions (i.e. &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt;&lt;code&gt;minmax_scale&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt;&lt;code&gt;maxabs_scale&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt;&lt;code&gt;robust_scale&lt;/code&gt;&lt;/a&gt;) accept 1D array which can be useful in some specific case.</source>
          <target state="translated">上記のすべての関数（&lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt; &lt;code&gt;minmax_scale&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt; &lt;code&gt;maxabs_scale&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt; &lt;code&gt;robust_scale&lt;/code&gt; &lt;/a&gt;）は、特定のケースで役立つ1D配列を受け入れます。</target>
        </trans-unit>
        <trans-unit id="e88245fc777e7533587c2f8e718d50cd7116a3f1" translate="yes" xml:space="preserve">
          <source>All available versions</source>
          <target state="translated">利用可能なすべてのバージョン</target>
        </trans-unit>
        <trans-unit id="03af622cdb4aac726985802a2b9d3765c7a41749" translate="yes" xml:space="preserve">
          <source>All bins in each feature have identical widths.</source>
          <target state="translated">各機能のビンはすべて同じ幅を持っています。</target>
        </trans-unit>
        <trans-unit id="59c634de3245b210109c9f97187eafdfb201b388" translate="yes" xml:space="preserve">
          <source>All bins in each feature have the same number of points.</source>
          <target state="translated">各機能のビンはすべて同数のポイントを持っています。</target>
        </trans-unit>
        <trans-unit id="8e7d5dca10b34ad961d839250513b9c8a0893f0e" translate="yes" xml:space="preserve">
          <source>All classifiers in scikit-learn do multiclass classification out-of-the-box. You don&amp;rsquo;t need to use the &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; module unless you want to experiment with different multiclass strategies.</source>
          <target state="translated">scikit-learnのすべての分類子は、すぐにマルチクラス分類を実行します。異なるマルチクラス戦略を試したい場合を除いて、&lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; &lt;/a&gt;モジュールを使用する必要はありません。</target>
        </trans-unit>
        <trans-unit id="026ccd1f86687bfe3e5f08bd5747feaae826b1dc" translate="yes" xml:space="preserve">
          <source>All classifiers in scikit-learn implement multiclass classification; you only need to use this module if you want to experiment with custom multiclass strategies.</source>
          <target state="translated">scikit-learn のすべての分類器は多クラス分類を実装しています。</target>
        </trans-unit>
        <trans-unit id="c78fdb80201033cef297ce6fb5232c2383bd7521" translate="yes" xml:space="preserve">
          <source>All decision trees use &lt;code&gt;np.float32&lt;/code&gt; arrays internally. If training data is not in this format, a copy of the dataset will be made.</source>
          <target state="translated">すべての決定木は内部で &lt;code&gt;np.float32&lt;/code&gt; 配列を使用します。トレーニングデータがこの形式でない場合、データセットのコピーが作成されます。</target>
        </trans-unit>
        <trans-unit id="41b549b6eacc2f621d683cbb4d8d7de1b7ed3ca8" translate="yes" xml:space="preserve">
          <source>All entries of this dict (if any) are passed as keyword arguments to the pairwise kernel function.</source>
          <target state="translated">このディクトのすべてのエントリ (もしあれば)は、ペアワイズカーネル関数のキーワード引数として渡されます。</target>
        </trans-unit>
        <trans-unit id="ff969c4b66fd28ef01340faf70c8e905bbf72131" translate="yes" xml:space="preserve">
          <source>All estimator objects expose a &lt;code&gt;fit&lt;/code&gt; method that takes a dataset (usually a 2-d array):</source>
          <target state="translated">すべてのestimatorオブジェクトは、データセット（通常は2次元配列）を取る &lt;code&gt;fit&lt;/code&gt; メソッドを公開します。</target>
        </trans-unit>
        <trans-unit id="8781771bbb6ededabb6ec83fb1b33a34b860476e" translate="yes" xml:space="preserve">
          <source>All estimators in a pipeline, except the last one, must be transformers (i.e. must have a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt; method). The last estimator may be any type (transformer, classifier, etc.).</source>
          <target state="translated">最後の推定量を除いて、パイプライン内のすべての推定量はトランスフォーマーである必要があります（つまり、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;変換&lt;/a&gt;メソッドが必要です）。最後の推定量は、任意のタイプ（トランスフォーマー、分類子など）にすることができます。</target>
        </trans-unit>
        <trans-unit id="e9616c63898ea085d32d1415c7461f768e275753" translate="yes" xml:space="preserve">
          <source>All estimators in a pipeline, except the last one, must be transformers (i.e. must have a &lt;code&gt;transform&lt;/code&gt; method). The last estimator may be any type (transformer, classifier, etc.).</source>
          <target state="translated">パイプライン内のすべての推定器は、最後のものを除いて、 &lt;code&gt;transform&lt;/code&gt; なければなりません（つまり、変換メソッドが必要です）。最後の推定量は、任意のタイプ（変圧器、分類器など）にすることができます。</target>
        </trans-unit>
        <trans-unit id="7c3fa432791090758c929f832210b21538e1fa2f" translate="yes" xml:space="preserve">
          <source>All estimators in the pipeline must support &lt;code&gt;inverse_transform&lt;/code&gt;.</source>
          <target state="translated">パイプライン内のすべての推定量は、 &lt;code&gt;inverse_transform&lt;/code&gt; をサポートする必要があります。</target>
        </trans-unit>
        <trans-unit id="b2ef621d2a7c2b25a3be25bb6ebef0201032958b" translate="yes" xml:space="preserve">
          <source>All estimators should specify all the parameters that can be set at the class level in their &lt;code&gt;__init__&lt;/code&gt; as explicit keyword arguments (no &lt;code&gt;*args&lt;/code&gt; or &lt;code&gt;**kwargs&lt;/code&gt;).</source>
          <target state="translated">すべての推定者は、クラスレベルで設定できるすべてのパラメーターを &lt;code&gt;__init__&lt;/code&gt; の明示的なキーワード引数として指定する必要があります（ &lt;code&gt;*args&lt;/code&gt; または &lt;code&gt;**kwargs&lt;/code&gt; なし）。</target>
        </trans-unit>
        <trans-unit id="24e3691f195d78233bed4d7732d29903c7a0bfe6" translate="yes" xml:space="preserve">
          <source>All last five solvers support both dense and sparse data. However, only &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; supports sparse input when &lt;code&gt;fit_intercept&lt;/code&gt; is True.</source>
          <target state="translated">最後の5つのソルバーはすべて、密データと疎データの両方をサポートしています。ただし、 &lt;code&gt;fit_intercept&lt;/code&gt; がTrueの場合、「sag」と「saga」のみがスパース入力をサポートします。</target>
        </trans-unit>
        <trans-unit id="61f9c2a4b604c0e088d2123f4269900a4c32a338" translate="yes" xml:space="preserve">
          <source>All last five solvers support both dense and sparse data. However, only &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; supports sparse input when`fit_intercept` is True.</source>
          <target state="translated">最後の5つのソルバーはすべて、密データと疎データの両方をサポートしています。ただし、 `fit_intercept`がTrueの場合、「sag」と「saga」のみがスパース入力をサポートします。</target>
        </trans-unit>
        <trans-unit id="f728f573fb469cf76a885b25e7cd8d2d6661b65d" translate="yes" xml:space="preserve">
          <source>All last five solvers support both dense and sparse data. However, only &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;sparse_cg&amp;rsquo; supports sparse input when &lt;code&gt;fit_intercept&lt;/code&gt; is True.</source>
          <target state="translated">最後の5つのソルバーはすべて、密なデータと疎なデータの両方をサポートします。ただし、 &lt;code&gt;fit_intercept&lt;/code&gt; がTrueの場合、スパース入力をサポートするのは「sag」と「sparse_cg」のみです。</target>
        </trans-unit>
        <trans-unit id="9b477d9d300bf316e6d73adbb21d261c724739c5" translate="yes" xml:space="preserve">
          <source>All of X is processed as a single batch. This is intended for cases when &lt;a href=&quot;#sklearn.preprocessing.MaxAbsScaler.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is not feasible due to very large number of &lt;code&gt;n_samples&lt;/code&gt; or because X is read from a continuous stream.</source>
          <target state="translated">Xはすべて単一のバッチとして処理されます。これは、 &lt;code&gt;n_samples&lt;/code&gt; の数が非常に多いため、またはXが連続ストリームから読み取られるために、&lt;a href=&quot;#sklearn.preprocessing.MaxAbsScaler.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;が実行できない場合を対象としています。</target>
        </trans-unit>
        <trans-unit id="b3fc8e75fa02a8bfaeaedb8d116922428a0fe800" translate="yes" xml:space="preserve">
          <source>All of X is processed as a single batch. This is intended for cases when &lt;a href=&quot;#sklearn.preprocessing.MinMaxScaler.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is not feasible due to very large number of &lt;code&gt;n_samples&lt;/code&gt; or because X is read from a continuous stream.</source>
          <target state="translated">Xはすべて単一のバッチとして処理されます。これは、 &lt;code&gt;n_samples&lt;/code&gt; の数が非常に多いため、またはXが連続ストリームから読み取られるために、&lt;a href=&quot;#sklearn.preprocessing.MinMaxScaler.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;が実行できない場合を対象としています。</target>
        </trans-unit>
        <trans-unit id="0771ca4e648606c13a5db59d2963430f3dd6f313" translate="yes" xml:space="preserve">
          <source>All of X is processed as a single batch. This is intended for cases when &lt;a href=&quot;#sklearn.preprocessing.StandardScaler.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is not feasible due to very large number of &lt;code&gt;n_samples&lt;/code&gt; or because X is read from a continuous stream.</source>
          <target state="translated">Xはすべて単一のバッチとして処理されます。これは、 &lt;code&gt;n_samples&lt;/code&gt; の数が非常に多いため、またはXが連続ストリームから読み取られるために、&lt;a href=&quot;#sklearn.preprocessing.StandardScaler.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;が実行できない場合を対象としています。</target>
        </trans-unit>
        <trans-unit id="8e879aa8d08a0dbe80a902ae3611f1bc99c99eb6" translate="yes" xml:space="preserve">
          <source>All of the above are supported by &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">上記のすべては、&lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;によってサポートされています。</target>
        </trans-unit>
        <trans-unit id="fe669fffadfc5710970ab03a9fde631f4966ca91" translate="yes" xml:space="preserve">
          <source>All of the above are supported by &lt;code&gt;sklearn.linear_model.stochastic_gradient&lt;/code&gt;.</source>
          <target state="translated">上記のすべてが &lt;code&gt;sklearn.linear_model.stochastic_gradient&lt;/code&gt; によってサポートされています。</target>
        </trans-unit>
        <trans-unit id="b5ed2e5e9ebf405638935350a19afaab363efb49" translate="yes" xml:space="preserve">
          <source>All of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below.</source>
          <target state="translated">上記の損失関数はいずれも、下の図に示すように、誤分類誤差(ゼロワン損失)の上限とみなすことができる。</target>
        </trans-unit>
        <trans-unit id="0050a0a20cce6934eea13e5fb5f70c03d8a3e6cd" translate="yes" xml:space="preserve">
          <source>All penalization parameters explored.</source>
          <target state="translated">すべてのペナルティパラメータを探索しました。</target>
        </trans-unit>
        <trans-unit id="955d9d437b529cc06e590b033bee6c6f48038cb1" translate="yes" xml:space="preserve">
          <source>All scikit-learn classifiers are capable of multiclass classification, but the meta-estimators offered by &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; permit changing the way they handle more than two classes because this may have an effect on classifier performance (either in terms of generalization error or required computational resources).</source>
          <target state="translated">すべてのscikit-learn分類子はマルチクラス分類が可能ですが、&lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; が&lt;/a&gt;提供するメタ推定子は、2つ以上のクラスを処理する方法を変更できます。リソース）。</target>
        </trans-unit>
        <trans-unit id="196b45b70e9cf2661d4f75a2029c5a0c8898e090" translate="yes" xml:space="preserve">
          <source>All settings, not just those presently modified, will be returned to their previous values when the context manager is exited. This is not thread-safe.</source>
          <target state="translated">現在変更されているものだけでなく、すべての設定は、コンテキストマネージャを終了すると以前の値に戻ります。これはスレッドセーフではありません。</target>
        </trans-unit>
        <trans-unit id="aa66fd6d2b23c29862eb90bc96dba9c2ff577f39" translate="yes" xml:space="preserve">
          <source>All supervised &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;estimators&lt;/a&gt; in scikit-learn implement a &lt;code&gt;fit(X, y)&lt;/code&gt; method to fit the model and a &lt;code&gt;predict(X)&lt;/code&gt; method that, given unlabeled observations &lt;code&gt;X&lt;/code&gt;, returns the predicted labels &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">scikit-learnのすべての教師付き&lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;推定量&lt;/a&gt;は、モデルにフィットする &lt;code&gt;fit(X, y)&lt;/code&gt; メソッドと、ラベルなしの観測値 &lt;code&gt;X&lt;/code&gt; が指定された場合に予測ラベル &lt;code&gt;y&lt;/code&gt; を返すpredict &lt;code&gt;predict(X)&lt;/code&gt; メソッドを実装します。</target>
        </trans-unit>
        <trans-unit id="5fd3433668568da7d84a79ee4e0518a15dd72a27" translate="yes" xml:space="preserve">
          <source>All the input data is provided matrix X (labeled and unlabeled) and corresponding label matrix y with a dedicated marker value for unlabeled samples.</source>
          <target state="translated">すべての入力データには、ラベル付きとラベルなしの行列Xと、ラベルなしサンプルのための専用のマーカー値を持つ対応するラベル行列yが提供されます。</target>
        </trans-unit>
        <trans-unit id="799926dcab2e28e1fbc27d0dc0ec06e58ac91465" translate="yes" xml:space="preserve">
          <source>All these estimators can compute internally the nearest neighbors, but most of them also accept precomputed nearest neighbors &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;sparse graph&lt;/a&gt;, as given by &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt;&lt;code&gt;radius_neighbors_graph&lt;/code&gt;&lt;/a&gt;. With mode &lt;code&gt;mode='connectivity'&lt;/code&gt;, these functions return a binary adjacency sparse graph as required, for instance, in &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;SpectralClustering&lt;/code&gt;&lt;/a&gt;. Whereas with &lt;code&gt;mode='distance'&lt;/code&gt;, they return a distance sparse graph as required, for instance, in &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt;. To include these functions in a scikit-learn pipeline, one can also use the corresponding classes &lt;a href=&quot;generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt;&lt;code&gt;KNeighborsTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt;&lt;code&gt;RadiusNeighborsTransformer&lt;/code&gt;&lt;/a&gt;. The benefits of this sparse graph API are multiple.</source>
          <target state="translated">これらの推定量はすべて、内部で最近傍を計算できますが、それらのほとんどは、&lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;kneighbors_graph&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt; &lt;code&gt;radius_neighbors_graph&lt;/code&gt; で&lt;/a&gt;指定されているように、事前に計算された最近傍&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;スパースグラフ&lt;/a&gt;も受け入れます。 mode &lt;code&gt;mode='connectivity'&lt;/code&gt; の場合、これらの関数は、たとえば&lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;SpectralClustering&lt;/code&gt; で&lt;/a&gt;、必要に応じてバイナリ隣接スパースグラフを返します。一方、 &lt;code&gt;mode='distance'&lt;/code&gt; の場合、たとえば&lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; で&lt;/a&gt;、必要に応じて距離スパースグラフを返します。これらの関数をscikit-learnパイプラインに含めるには、対応するクラス&lt;a href=&quot;generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt; &lt;code&gt;KNeighborsTransformer&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt; &lt;code&gt;RadiusNeighborsTransformer&lt;/code&gt; を&lt;/a&gt;使用することもできます。。このスパースグラフAPIの利点は複数あります。</target>
        </trans-unit>
        <trans-unit id="f3103275737fc127cec8d65804a5477574232497" translate="yes" xml:space="preserve">
          <source>All three models are significantly better than chance but also very far from making perfect predictions.</source>
          <target state="translated">3つのモデルはすべて偶然よりも有意に優れていますが、完璧な予測をすることからは程遠いものです。</target>
        </trans-unit>
        <trans-unit id="68201d9ddadd1b10424b1028599271aea0a81e03" translate="yes" xml:space="preserve">
          <source>All values are cached on the filesystem, in a deep directory structure.</source>
          <target state="translated">すべての値は、ファイルシステム上の深いディレクトリ構造にキャッシュされます。</target>
        </trans-unit>
        <trans-unit id="ad92361405543cadf3390872502b0368070801b1" translate="yes" xml:space="preserve">
          <source>All, &lt;a href=&quot;generated/sklearn.metrics.mutual_info_score#sklearn.metrics.mutual_info_score&quot;&gt;&lt;code&gt;mutual_info_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt;&lt;code&gt;adjusted_mutual_info_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt;&lt;code&gt;normalized_mutual_info_score&lt;/code&gt;&lt;/a&gt; are symmetric: swapping the argument does not change the score. Thus they can be used as a &lt;strong&gt;consensus measure&lt;/strong&gt;:</source>
          <target state="translated">すべて、&lt;a href=&quot;generated/sklearn.metrics.mutual_info_score#sklearn.metrics.mutual_info_score&quot;&gt; &lt;code&gt;mutual_info_score&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt; &lt;code&gt;adjusted_mutual_info_score&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt; &lt;code&gt;normalized_mutual_info_score&lt;/code&gt; は&lt;/a&gt;対称である：引数を交換すると、スコアを変更しません。したがって、&lt;strong&gt;コンセンサス指標&lt;/strong&gt;として使用できます。</target>
        </trans-unit>
        <trans-unit id="9cc3cbd54e82f76756e09725006efed713dfb14b" translate="yes" xml:space="preserve">
          <source>Allow to bypass several input checking. Don&amp;rsquo;t use this parameter unless you know what you do.</source>
          <target state="translated">複数の入力チェックをバイパスできるようにします。何をするかわからない場合は、このパラメータを使用しないでください。</target>
        </trans-unit>
        <trans-unit id="b2988d20e10f74030eeb38e913a183de75189dbb" translate="yes" xml:space="preserve">
          <source>Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.</source>
          <target state="translated">許容される入力は、リスト、numpy配列、scipy-sparse行列、またはpandasデータフレームです。</target>
        </trans-unit>
        <trans-unit id="246073e83f7bacaf2b27a7cb44f4ce78f91064b5" translate="yes" xml:space="preserve">
          <source>Allows NaN in the input.</source>
          <target state="translated">入力中のNaNを許可します。</target>
        </trans-unit>
        <trans-unit id="7d1bd9e91706d2b4396d936c8102ad0f0e779fcd" translate="yes" xml:space="preserve">
          <source>Allows NaN/Inf in the input if the underlying estimator does as well.</source>
          <target state="translated">基礎となる推定子が同様の場合、入力のNaN/Infを許可する。</target>
        </trans-unit>
        <trans-unit id="b369263cc488ea5fca6946112c8afe7517b680b5" translate="yes" xml:space="preserve">
          <source>Allows simple indexing of lists or arrays.</source>
          <target state="translated">リストや配列のシンプルなインデックスを作成できます。</target>
        </trans-unit>
        <trans-unit id="bd0b505b007b0a8de2703b572d4c9c27c39f3415" translate="yes" xml:space="preserve">
          <source>Allows to examine the spread of each true cluster across predicted clusters and vice versa.</source>
          <target state="translated">予測されたクラスタ間の各真のクラスタの広がりを調べることができます。</target>
        </trans-unit>
        <trans-unit id="036fd67d51f9b182736132cac7fb44d0406f98ef" translate="yes" xml:space="preserve">
          <source>Almost every group is distinguished by whether headers such as &lt;code&gt;NNTP-Posting-Host:&lt;/code&gt; and &lt;code&gt;Distribution:&lt;/code&gt; appear more or less often.</source>
          <target state="translated">ほとんどすべてのグループは、 &lt;code&gt;NNTP-Posting-Host:&lt;/code&gt; や &lt;code&gt;Distribution:&lt;/code&gt; などのヘッダーが多かれ少なかれ表示されるかどうかによって区別されます。</target>
        </trans-unit>
        <trans-unit id="54076ee073b95c8e2227a7529bbd3c040278da4f" translate="yes" xml:space="preserve">
          <source>Alpaydin (alpaydin &amp;lsquo;@&amp;rsquo; boun.edu.tr)</source>
          <target state="translated">アルパイディン（alpaydin '@' boun.edu.tr）</target>
        </trans-unit>
        <trans-unit id="78d537ced7a0f9450c0b7f5b446829da6458dfc9" translate="yes" xml:space="preserve">
          <source>Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.</source>
          <target state="translated">Alpaydin,C.Kaynak (1998)Cascading Classifiers,Kybernetika.</target>
        </trans-unit>
        <trans-unit id="bc0f2eb3e0375e2eb0ff510eb47611a2b3ce02af" translate="yes" xml:space="preserve">
          <source>Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary.</source>
          <target state="translated">アルファは、正則化項、別名ペナルティ項のパラメータで、重みのサイズを制限することでオーバーフィッティングに対抗します。アルファを大きくすると、より小さなウェイトを奨励することで、高い分散(オーバーフィットの兆候)を修正することができ、結果として、より小さな曲率で表示される決定境界プロットが得られます。同様に、アルファを減少させると、より大きな重みを奨励することで、高いバイアス(アンダーフィッティングの兆候)を修正することができ、結果として、より複雑な決定境界が得られる可能性があります。</target>
        </trans-unit>
        <trans-unit id="a01b7375f883c68c86f4064f9c436e0a18c14d5d" translate="yes" xml:space="preserve">
          <source>Alpha is again treated as a random variable that is to be estimated from the data.</source>
          <target state="translated">アルファは再びデータから推定するランダム変数として扱われます。</target>
        </trans-unit>
        <trans-unit id="9bc4ba22610a89d08abe53892ec649b1e8a46ce9" translate="yes" xml:space="preserve">
          <source>Also for multiple metric evaluation, the attributes &lt;code&gt;best_index_&lt;/code&gt;, &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; will only be available if &lt;code&gt;refit&lt;/code&gt; is set and all of them will be determined w.r.t this specific scorer.</source>
          <target state="translated">また、複数のメトリック評価の場合、属性 &lt;code&gt;best_index_&lt;/code&gt; 、 &lt;code&gt;best_score_&lt;/code&gt; 、および &lt;code&gt;best_params_&lt;/code&gt; は、 &lt;code&gt;refit&lt;/code&gt; が設定されている場合にのみ使用でき、それらすべてがこの特定のスコアラーに対して決定されます。</target>
        </trans-unit>
        <trans-unit id="7c913c796a2efc191b66ad3c6a4c672e085ca88e" translate="yes" xml:space="preserve">
          <source>Also from the thickness of the silhouette plot the cluster size can be visualized. The silhouette plot for cluster 0 when &lt;code&gt;n_clusters&lt;/code&gt; is equal to 2, is bigger in size owing to the grouping of the 3 sub clusters into one big cluster. However when the &lt;code&gt;n_clusters&lt;/code&gt; is equal to 4, all the plots are more or less of similar thickness and hence are of similar sizes as can be also verified from the labelled scatter plot on the right.</source>
          <target state="translated">また、シルエットプロットの太さから、クラスターサイズを視覚化できます。 &lt;code&gt;n_clusters&lt;/code&gt; が2に等しい場合のクラスター0のシルエットプロットは、3つのサブクラスターが1つの大きなクラスターにグループ化されているため、サイズが大きくなっています。ただし、 &lt;code&gt;n_clusters&lt;/code&gt; が4に等しい場合、すべてのプロットはほぼ同じ厚さであり、したがって同様のサイズであり、右側のラベル付き散布図からも確認できます。</target>
        </trans-unit>
        <trans-unit id="11074bab0d4cf60477f10f484031e0877ac22e6b" translate="yes" xml:space="preserve">
          <source>Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only &lt;code&gt;n_classes&lt;/code&gt; classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.</source>
          <target state="translated">one-vs-allとも呼ばれるこの戦略は、クラスごとに1つの分類子を当てはめることで構成されます。各分類子について、クラスは他のすべてのクラスに対して適合されます。その計算効率に加えて（ &lt;code&gt;n_classes&lt;/code&gt; 分類器のみが必要です）、このアプローチの1つの利点はその解釈可能性です。各クラスは1つと1つの分類子のみで表されるため、対応する分類子を調べることにより、クラスに関する知識を得ることができます。これは、マルチクラス分類で最も一般的に使用される戦略であり、デフォルトの選択です。</target>
        </trans-unit>
        <trans-unit id="cc34e41c8fa23138f4c5476496b3bc4a71c58fb4" translate="yes" xml:space="preserve">
          <source>Also note that both random features have very low importances (close to 0) as expected.</source>
          <target state="translated">また、両方のランダム特徴は、予想通り、非常に低いインポータンス(0に近い)であることにも注意してください。</target>
        </trans-unit>
        <trans-unit id="6f4d07ff4d50db829057964fee42d298814f3483" translate="yes" xml:space="preserve">
          <source>Also note that even though Box-Cox seems to perform better than Yeo-Johnson for lognormal and chi-squared distributions, keep in mind that Box-Cox does not support inputs with negative values.</source>
          <target state="translated">また、Box-Coxは、対数正規分布やカイ二乗分布に対してYeo-Johnsonよりも優れた性能を発揮するようですが、Box-Coxは負の値を持つ入力をサポートしないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="0af9b79bdfeacc61c23bfadd6f95c644ff726db7" translate="yes" xml:space="preserve">
          <source>Also note that for the linear case, the algorithm used in &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; by the &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; implementation is much more efficient than its &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;-based &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; counterpart and can scale almost linearly to millions of samples and/or features.</source>
          <target state="translated">また、線形の場合、&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;実装によって&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; で&lt;/a&gt;使用されるアルゴリズムは、&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;ベースの&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; の&lt;/a&gt;対応するアルゴリズムよりもはるかに効率的であり、数百万のサンプルや機能にほぼ線形にスケーリングできます。</target>
        </trans-unit>
        <trans-unit id="5de10864fc42a72508361320516be4ba6cf282ee" translate="yes" xml:space="preserve">
          <source>Also note that the digits labels roughly match the natural grouping found by t-SNE while the linear 2D projection of the PCA model yields a representation where label regions largely overlap. This is a strong clue that this data can be well separated by non linear methods that focus on the local structure (e.g. an SVM with a Gaussian RBF kernel). However, failing to visualize well separated homogeneously labeled groups with t-SNE in 2D does not necessarily imply that the data cannot be correctly classified by a supervised model. It might be the case that 2 dimensions are not low enough to accurately represents the internal structure of the data.</source>
          <target state="translated">また、PCAモデルの線形2D投影では、ラベル領域が大きく重なる表現が得られる一方で、桁のラベルはt-SNEによって発見された自然なグルーピングとほぼ一致していることにも注意してください。これは、このデータが局所構造に焦点を当てた非線形手法(例えば、ガウスRBFカーネルを持つSVM)によって十分に分離できることを示す強い手がかりです。しかし、よく分離された均質にラベル付けされたグループを2次元でt-SNEで可視化できないことは、必ずしもそのデータが教師付きモデルによって正しく分類できないことを意味するものではありません。2次元ではデータの内部構造を正確に表現するには十分に低くないということかもしれません。</target>
        </trans-unit>
        <trans-unit id="ab26a7a4f86bc166b4ac9642686ddf7141b2b207" translate="yes" xml:space="preserve">
          <source>Also note that we set a low value for the tolerance to make sure that the model has converged before collecting the coefficients.</source>
          <target state="translated">また、係数を収集する前にモデルが収束したことを確認するために、許容値を低く設定していることにも注意してください。</target>
        </trans-unit>
        <trans-unit id="7d37adf37d9aef6924cdccc898bbb4ca10bb0758" translate="yes" xml:space="preserve">
          <source>Also useful for lower-level tasks is the function &lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt;&lt;code&gt;lasso_path&lt;/code&gt;&lt;/a&gt; that computes the coefficients along the full path of possible values.</source>
          <target state="translated">下位レベルのタスクにも役立つのは、可能な値のフルパスに沿って係数を計算する関数&lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt; &lt;code&gt;lasso_path&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="2587ad1f75524017fb1ecdf6376d89726a0f6825" translate="yes" xml:space="preserve">
          <source>Also, by evaluating log marginal likelihood (L) of these models, we can determine which one is better. It can be concluded that the model with larger L is more likely.</source>
          <target state="translated">また、これらのモデルの対数限界尤度(L)を評価することで、どちらが優れているかを判断することができます。Lが大きいモデルの方が可能性が高いと結論づけられます。</target>
        </trans-unit>
        <trans-unit id="0b5de16e36a8364053840b53f5f6fe23ce35a1bc" translate="yes" xml:space="preserve">
          <source>Also, the EXPERIENCE and AGE are strongly linearly correlated.</source>
          <target state="translated">また、experienceとageは強い線形相関があります。</target>
        </trans-unit>
        <trans-unit id="4a2b5f07f08a470a2b2e7ef1ae5a0e4e195af3e0" translate="yes" xml:space="preserve">
          <source>Also, these routines have not been tested for graphs with negative distances. Negative distances can lead to infinite cycles that must be handled by specialized algorithms.</source>
          <target state="translated">また、これらのルーチンは負の距離を持つグラフについてはテストされていません。負の距離は、特殊なアルゴリズムで処理しなければならない無限サイクルを引き起こす可能性があります。</target>
        </trans-unit>
        <trans-unit id="9ba49a974ea195b74ff8d485a45574a6c2262ea1" translate="yes" xml:space="preserve">
          <source>Also, this estimator is different from the R implementation of Robust Regression (&lt;a href=&quot;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&quot;&gt;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&lt;/a&gt;) because the R implementation does a weighted least squares implementation with weights given to each sample on the basis of how much the residual is greater than a certain threshold.</source>
          <target state="translated">また、この推定は、ロバスト回帰のR実装（&lt;a href=&quot;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&quot;&gt;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&lt;/a&gt;）とは異なります。R実装は、各サンプルは、残差が特定のしきい値よりもどれだけ大きいかに基づいています。</target>
        </trans-unit>
        <trans-unit id="8c3843c6d1de204ea6ff77660712728c7734da35" translate="yes" xml:space="preserve">
          <source>Alternate label propagation strategy more robust to noise</source>
          <target state="translated">代替ラベル伝搬ストラテジーはノイズに対してよりロバスト</target>
        </trans-unit>
        <trans-unit id="a1df7e48a8694bc511c33f4fdb83c6299c349bb3" translate="yes" xml:space="preserve">
          <source>Alternate output array in which to place the result. The default is &lt;code&gt;None&lt;/code&gt;; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See &lt;code&gt;doc.ufuncs&lt;/code&gt; for details.</source>
          <target state="translated">結果を配置する代替出力配列。デフォルトは &lt;code&gt;None&lt;/code&gt; です。指定する場合は、予想される出力と同じ形状にする必要がありますが、必要に応じて型がキャストされます。詳細については、 &lt;code&gt;doc.ufuncs&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="1444a36563243cd516125e47e643f6c86af2b2d3" translate="yes" xml:space="preserve">
          <source>Alternating Least Squares (Fast HALS).</source>
          <target state="translated">交互最小二乗(高速HALS)。</target>
        </trans-unit>
        <trans-unit id="16e8b22da1997a9e7a46f4b2f3065cb731917af6" translate="yes" xml:space="preserve">
          <source>Alternative implementation that does incremental updates of the centers&amp;rsquo; positions using mini-batches.</source>
          <target state="translated">ミニバッチを使用してセンターの位置を段階的に更新する代替実装。</target>
        </trans-unit>
        <trans-unit id="9319ffecfc2401bba3b77152ec4f1cf3bfc1dbce" translate="yes" xml:space="preserve">
          <source>Alternative online implementation that does incremental updates of the centers positions using mini-batches. For large scale learning (say n_samples &amp;gt; 10k) MiniBatchKMeans is probably much faster than the default batch implementation.</source>
          <target state="translated">ミニバッチを使用してセンターの位置を段階的に更新する代替のオンライン実装。大規模な学習（n_samples&amp;gt; 10kなど）の場合、MiniBatchKMeansはおそらくデフォルトのバッチ実装よりもはるかに高速です。</target>
        </trans-unit>
        <trans-unit id="203de1a7976ebc427fc76b372cb16da457115e31" translate="yes" xml:space="preserve">
          <source>Alternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with &lt;code&gt;pip install graphviz&lt;/code&gt;.</source>
          <target state="translated">あるいは、graphvizのバイナリをgraphvizプロジェクトのホームページからダウンロードし、Pythonラッパーをpypiから &lt;code&gt;pip install graphviz&lt;/code&gt; を使用してインストールすることもできます。</target>
        </trans-unit>
        <trans-unit id="a86f6ee74cd1a30708099bd1d22a36b2f08e9a0f" translate="yes" xml:space="preserve">
          <source>Alternatively the backend can be passed directly as an instance.</source>
          <target state="translated">あるいは、バックエンドを直接インスタンスとして渡すこともできます。</target>
        </trans-unit>
        <trans-unit id="76c8a1ac67e02525fd024c6509339aad00aeced5" translate="yes" xml:space="preserve">
          <source>Alternatively, it can be set by the &amp;lsquo;SCIKIT_LEARN_DATA&amp;rsquo; environment variable or programmatically by giving an explicit folder path. The &amp;lsquo;~&amp;rsquo; symbol is expanded to the user home folder.</source>
          <target state="translated">または、「SCIKIT_LEARN_DATA」環境変数で設定するか、明示的にフォルダーパスを指定してプログラムで設定できます。「〜」記号はユーザーのホームフォルダーに展開されます。</target>
        </trans-unit>
        <trans-unit id="adf895b415f65b1116cbe9c2579f72b5380f5476" translate="yes" xml:space="preserve">
          <source>Alternatively, one can directly model the total loss with a unique Compound Poisson Gamma generalized linear model (with a log link function). This model is a special case of the Tweedie GLM with a &amp;ldquo;power&amp;rdquo; parameter \(p \in (1, 2)\). Here, we fix apriori the &lt;code&gt;power&lt;/code&gt; parameter of the Tweedie model to some arbitrary value (1.9) in the valid range. Ideally one would select this value via grid-search by minimizing the negative log-likelihood of the Tweedie model, but unfortunately the current implementation does not allow for this (yet).</source>
          <target state="translated">または、独自の複合ポアソンガンマ一般化線形モデル（対数リンク関数を使用）を使用して、総損失を直接モデル化することもできます。このモデルは、「パワー」パラメーター\（p \ in（1、2）\）を持つTweedieGLMの特殊なケースです。ここでは、Tweedieモデルの &lt;code&gt;power&lt;/code&gt; パラメーターを有効範囲内の任意の値（1.9）に事前に固定します。理想的には、Tweedieモデルの負の対数尤度を最小化することにより、グリッド検索を介してこの値を選択しますが、残念ながら、現在の実装ではこれは（まだ）許可されていません。</target>
        </trans-unit>
        <trans-unit id="5dd953719fa9152bd6124ae49c16c66b8c780a34" translate="yes" xml:space="preserve">
          <source>Alternatively, one can use the &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; classes directly to find nearest neighbors. This is the functionality wrapped by the &lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors&quot;&gt;&lt;code&gt;NearestNeighbors&lt;/code&gt;&lt;/a&gt; class used above. The Ball Tree and KD Tree have the same interface; we&amp;rsquo;ll show an example of using the KD Tree here:</source>
          <target state="translated">あるいは、使用することができます&lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;最寄りの隣人を見つけるために、直接クラスを。これは、上記で使用した&lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors&quot;&gt; &lt;code&gt;NearestNeighbors&lt;/code&gt; &lt;/a&gt;クラスによってラップされた機能です。Ball TreeとKD Treeのインターフェースは同じです。ここでは、KDツリーの使用例を示します。</target>
        </trans-unit>
        <trans-unit id="a2b3bf63690d9fdf9c988aa4c9e184e0fbef9897" translate="yes" xml:space="preserve">
          <source>Alternatively, orthogonal matching pursuit can target a specific error instead of a specific number of non-zero coefficients. This can be expressed as:</source>
          <target state="translated">代替的に、直交マッチングの追求は、特定の数の非ゼロ係数の代わりに、特定の誤差を対象とすることができる。これは、次のように表すことができる。</target>
        </trans-unit>
        <trans-unit id="0f7ec2ff37de8d3d768baf118074df7d6149a11f" translate="yes" xml:space="preserve">
          <source>Alternatively, the &lt;code&gt;scoring&lt;/code&gt; argument can be provided to specify an alternative scoring method.</source>
          <target state="translated">または、 &lt;code&gt;scoring&lt;/code&gt; 引数を指定して、別のスコアリング方法を指定することもできます。</target>
        </trans-unit>
        <trans-unit id="ea61f1eddfe969d509258a2bd8bcd0672a722400" translate="yes" xml:space="preserve">
          <source>Alternatively, the estimator &lt;a href=&quot;generated/sklearn.linear_model.lassolarsic#sklearn.linear_model.LassoLarsIC&quot;&gt;&lt;code&gt;LassoLarsIC&lt;/code&gt;&lt;/a&gt; proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to find the optimal value of alpha as the regularization path is computed only once instead of k+1 times when using k-fold cross-validation. However, such criteria needs a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).</source>
          <target state="translated">または、推定器&lt;a href=&quot;generated/sklearn.linear_model.lassolarsic#sklearn.linear_model.LassoLarsIC&quot;&gt; &lt;code&gt;LassoLarsIC&lt;/code&gt; &lt;/a&gt;は、赤池情報量基準（AIC）およびベイズ情報量基準（BIC）の使用を提案します。k分割交差検証を使用する場合、正則化パスはk + 1回ではなく1回だけ計算されるため、alphaの最適値を見つける方が計算コストが低くなります。ただし、このような基準では、解の自由度を適切に推定する必要があり、大きなサンプル（漸近的な結果）に対して導出され、モデルが正しい、つまりデータがこのモデルによって実際に生成されると仮定します。また、問題の条件が悪い場合（サンプルよりも機能が多い場合）は、壊れる傾向があります。</target>
        </trans-unit>
        <trans-unit id="f454663104fe93624afb99e1fe4ea6ef9bcd01cf" translate="yes" xml:space="preserve">
          <source>Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:</source>
          <target state="translated">あるいは,各クラスの確率を予測することも可能であり,これは葉の中の同じクラスの訓練サンプルの割合である.</target>
        </trans-unit>
        <trans-unit id="5c74d35687e3587d17adc94ee7b35af54787e26a" translate="yes" xml:space="preserve">
          <source>Alternatively, the tree can also be exported in textual format with the function &lt;a href=&quot;generated/sklearn.tree.export_text#sklearn.tree.export_text&quot;&gt;&lt;code&gt;export_text&lt;/code&gt;&lt;/a&gt;. This method doesn&amp;rsquo;t require the installation of external libraries and is more compact:</source>
          <target state="translated">または、関数&lt;a href=&quot;generated/sklearn.tree.export_text#sklearn.tree.export_text&quot;&gt; &lt;code&gt;export_text&lt;/code&gt; を使用&lt;/a&gt;して、ツリーをテキスト形式でエクスポートすることもできます。この方法では、外部ライブラリをインストールする必要がなく、よりコンパクトです。</target>
        </trans-unit>
        <trans-unit id="3d7768459c8a27625c6b1edd47cdb7cdd42a8dbd" translate="yes" xml:space="preserve">
          <source>Alternatively, using &lt;code&gt;precomputed&lt;/code&gt;, a user-provided affinity matrix can be used.</source>
          <target state="translated">または、 &lt;code&gt;precomputed&lt;/code&gt; たを使用して、ユーザー指定のアフィニティーマトリックスを使用できます。</target>
        </trans-unit>
        <trans-unit id="408564450de584fdcd4c6ffe287273935ea61be3" translate="yes" xml:space="preserve">
          <source>Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter &lt;code&gt;max_leaf_nodes&lt;/code&gt;. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; has &lt;code&gt;k - 1&lt;/code&gt; split nodes and thus can model interactions of up to order &lt;code&gt;max_leaf_nodes - 1&lt;/code&gt; .</source>
          <target state="translated">または、パラメーター &lt;code&gt;max_leaf_nodes&lt;/code&gt; を使用してリーフノードの数を指定することにより、ツリーサイズを制御できます。この場合、ツリーはベストファーストサーチを使用して成長します。不純物が最も改善されたノードが最初に拡張されます。 &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; のツリーは &lt;code&gt;k - 1&lt;/code&gt; 分割ノードを持つため、最大 &lt;code&gt;max_leaf_nodes - 1&lt;/code&gt; 相互作用をモデル化できます。</target>
        </trans-unit>
        <trans-unit id="9d8e5a012d3af6b5916c5e0e392c2cf43112fb2f" translate="yes" xml:space="preserve">
          <source>Although GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid.</source>
          <target state="translated">クラスタリングにはGMMがよく使われますが、得られたクラスタをデータセットの実際のクラスと比較することができます。この比較を有効にするために、ガウシアンの平均値を学習集合のクラスの平均値で初期化します。</target>
        </trans-unit>
        <trans-unit id="9e3f75157b99771eea902cdfb247d8150fcbbb65" translate="yes" xml:space="preserve">
          <source>Although a list of sets or tuples is a very intuitive format for multilabel data, it is unwieldy to process. This transformer converts between this intuitive format and the supported multilabel format: a (samples x classes) binary matrix indicating the presence of a class label.</source>
          <target state="translated">セットやタプルのリストはマルチラベルデータのための非常に直感的なフォーマットですが、処理するには扱いにくいものです。この変換器は,この直感的なフォーマットとサポートされているマルチラベルフォーマットの間で変換します:クラスラベルの存在を示す(サンプル x クラス)バイナリ行列.</target>
        </trans-unit>
        <trans-unit id="6b97831f2bf852269cd56a6950af72a787322f90" translate="yes" xml:space="preserve">
          <source>Although online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting.</source>
          <target state="translated">オンライン法は局所的な最適点への収束が保証されていますが、最適点の品質や収束速度は、ミニバッチサイズや学習率設定に関連する属性に依存する場合があります。</target>
        </trans-unit>
        <trans-unit id="be1a3f200b8c31300cb53ca0638c27940f9fd773" translate="yes" xml:space="preserve">
          <source>Although the online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting.</source>
          <target state="translated">オンライン法は局所最適点に収束することが保証されているが、最適点の品質や収束速度は、ミニバッチサイズや学習率設定に関連する属性に依存する可能性がある。</target>
        </trans-unit>
        <trans-unit id="b824f979746fff21942ed5a8526c30831b67aa67" translate="yes" xml:space="preserve">
          <source>Always ignored, exists for compatibility.</source>
          <target state="translated">常に無視され、互換性のために存在します。</target>
        </trans-unit>
        <trans-unit id="f671e33354d7ef61d0b19d9b0bec50200bf9529c" translate="yes" xml:space="preserve">
          <source>Always ignored, exists for compatibility. &lt;code&gt;np.zeros(n_samples)&lt;/code&gt; may be used as a placeholder.</source>
          <target state="translated">常に無視され、互換性のために存在します。 &lt;code&gt;np.zeros(n_samples)&lt;/code&gt; はプレースホルダーとして使用できます。</target>
        </trans-unit>
        <trans-unit id="86f5a3adc76607b6ec083bd7a2c05aeeca017cf3" translate="yes" xml:space="preserve">
          <source>Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method.</source>
          <target state="translated">変換メソッドを呼び出す際の条件付けを改善するために適用するリッジの収縮量。</target>
        </trans-unit>
        <trans-unit id="a7500e59c81a09edaea684ef869962960b125989" translate="yes" xml:space="preserve">
          <source>Amount of ridge shrinkage to apply in order to improve conditioning.</source>
          <target state="translated">コンディショニングを改善するために適用するリッジの収縮量。</target>
        </trans-unit>
        <trans-unit id="a93d54da303ddad51202c963e4858505c997f8d0" translate="yes" xml:space="preserve">
          <source>Amount of verbosity.</source>
          <target state="translated">冗長性の量。</target>
        </trans-unit>
        <trans-unit id="40cf2c52a2e789ddb84ca29dcc3ddee6f4122f49" translate="yes" xml:space="preserve">
          <source>An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.</source>
          <target state="translated">AdaBoost [1]の分類器は,元のデータセットに分類器を適合させることから始まり,その後,同じデータセットに分類器の追加のコピーを適合させるが,後続の分類器がより困難なケースに焦点を当てるように,誤って分類されたインスタンスの重みを調整するメタ推定器である.</target>
        </trans-unit>
        <trans-unit id="043d85ce90b3af29bb38c14a359edb987b27e86d" translate="yes" xml:space="preserve">
          <source>An AdaBoost [1] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.</source>
          <target state="translated">AdaBoost [1]のレグレッサーは、元のデータセットにレグレッサーをフィッティングすることから始まり、その後、同じデータセットにレグレッサーの追加のコピーをフィッティングするが、現在の予測の誤差に応じてインスタンスの重みが調整されるメタ推定器である。このように,後続のリグレグレッサーは,より困難なケースに焦点を当てている.</target>
        </trans-unit>
        <trans-unit id="a0726965169fa1a924e6755570036e923d55d00c" translate="yes" xml:space="preserve">
          <source>An AdaBoost classifier.</source>
          <target state="translated">AdaBoostの分類器。</target>
        </trans-unit>
        <trans-unit id="3388a3be3246d31996f2f786da9e288b6011901a" translate="yes" xml:space="preserve">
          <source>An AdaBoost regressor that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction.</source>
          <target state="translated">元のデータセットにレグレッサーをフィッティングすることから始まり、その後、同じデータセットにレグレッサーの追加コピーをフィッティングするが、現在の予測の誤差に応じてインスタンスの重みが調整されるAdaBoostレグレッサー。</target>
        </trans-unit>
        <trans-unit id="0f161ccb66d6dfc0409a9a54a8f62d08c472ddf6" translate="yes" xml:space="preserve">
          <source>An AdaBoost regressor.</source>
          <target state="translated">AdaBoostのレグレッサー。</target>
        </trans-unit>
        <trans-unit id="6356fae027d92d258a26309d7a3f256df2c9cc55" translate="yes" xml:space="preserve">
          <source>An Exception object.</source>
          <target state="translated">例外オブジェクト。</target>
        </trans-unit>
        <trans-unit id="91b401262ec54e41ccc28fc97e3dca77764edee1" translate="yes" xml:space="preserve">
          <source>An already fitted classifier can be calibrated by setting &lt;code&gt;cv=&quot;prefit&quot;&lt;/code&gt;. In this case, the data is only used to fit the regressor. It is up to the user make sure that the data used for fitting the classifier is disjoint from the data used for fitting the regressor.</source>
          <target state="translated">すでに適合している分類器は、 &lt;code&gt;cv=&quot;prefit&quot;&lt;/code&gt; を設定することで較正できます。この場合、データはリグレッサを適合させるためにのみ使用されます。分類器のフィッティングに使用されるデータが、リグレッサーのフィッティングに使用されるデータと互いに素であることを確認するのはユーザーの責任です。</target>
        </trans-unit>
        <trans-unit id="96b2243b1ac08495762e9c93f82c1e26829c8a97" translate="yes" xml:space="preserve">
          <source>An alternative and recommended approach is to use &lt;code&gt;StandardScaler&lt;/code&gt; in a &lt;code&gt;Pipeline&lt;/code&gt;</source>
          <target state="translated">別の推奨されるアプローチは、 &lt;code&gt;Pipeline&lt;/code&gt; &lt;code&gt;StandardScaler&lt;/code&gt; を使用することです</target>
        </trans-unit>
        <trans-unit id="6bcd8e2a5d780cc52692b09904c485c359805e94" translate="yes" xml:space="preserve">
          <source>An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt;&lt;code&gt;MinMaxScaler&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.preprocessing.maxabsscaler#sklearn.preprocessing.MaxAbsScaler&quot;&gt;&lt;code&gt;MaxAbsScaler&lt;/code&gt;&lt;/a&gt;, respectively.</source>
          <target state="translated">代替の標準化は、特定の最小値と最大値の間、多くの場合0と1の間にあるように機能をスケーリングするか、各機能の最大絶対値をユニットサイズにスケーリングすることです。これは、それぞれ&lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt; &lt;code&gt;MinMaxScaler&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;generated/sklearn.preprocessing.maxabsscaler#sklearn.preprocessing.MaxAbsScaler&quot;&gt; &lt;code&gt;MaxAbsScaler&lt;/code&gt; &lt;/a&gt;を使用して実現できます。</target>
        </trans-unit>
        <trans-unit id="dc1db7ec650725e3eb3f215f1e02c7a5ab5d599a" translate="yes" xml:space="preserve">
          <source>An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.</source>
          <target state="translated">別のタスク、顔認識または顔の識別は、未知の人の顔の写真を与えられ、識別された人の以前に見た写真のギャラリーを参照して、その人の名前を識別します。</target>
        </trans-unit>
        <trans-unit id="9f1396eaada67d4771525d15c3b9982306c357b5" translate="yes" xml:space="preserve">
          <source>An alternative to pickling is to export the model to another format using one of the model export tools listed under &lt;a href=&quot;https://scikit-learn.org/0.23/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;. Unlike pickling, once exported you cannot recover the full Scikit-learn estimator object, but you can deploy the model for prediction, usually by using tools supporting open model interchange formats such as &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX&lt;/a&gt; or &lt;a href=&quot;http://dmg.org/pmml/v4-4/GeneralStructure.html&quot;&gt;PMML&lt;/a&gt;.</source>
          <target state="translated">酸洗いの代わりに、&lt;a href=&quot;https://scikit-learn.org/0.23/related_projects.html#related-projects&quot;&gt;関連プロジェクトに&lt;/a&gt;リストされているモデルエクスポートツールの1つを使用して、モデルを別の形式にエクスポートすることもできます。ピクルスとは異なり、エクスポートすると、Scikit-learn推定オブジェクト全体を復元することはできませんが、通常は&lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX&lt;/a&gt;や&lt;a href=&quot;http://dmg.org/pmml/v4-4/GeneralStructure.html&quot;&gt;PMML&lt;/a&gt;などのオープンモデル交換形式をサポートするツールを使用して、予測用にモデルをデプロイできます。</target>
        </trans-unit>
        <trans-unit id="63983f26490d9d0dd330cb65d368efc57d509991" translate="yes" xml:space="preserve">
          <source>An application of the different &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold learning&lt;/a&gt; techniques on a spherical data-set. Here one can see the use of dimensionality reduction in order to gain some intuition regarding the manifold learning methods. Regarding the dataset, the poles are cut from the sphere, as well as a thin slice down its side. This enables the manifold learning techniques to &amp;lsquo;spread it open&amp;rsquo; whilst projecting it onto two dimensions.</source>
          <target state="translated">さまざまな&lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;多様体学習&lt;/a&gt;手法の球面データセットへの適用。ここで、多様性学習法に関する直感を得るために、次元削減の使用を見ることができます。データセットに関しては、極は球から切り取られており、その横の薄いスライスも切り取られています。これにより、多様な学習手法で2次元に投影しながら、「広げて開く」ことができます。</target>
        </trans-unit>
        <trans-unit id="7255375783faf14b00594786271748afe79b54ea" translate="yes" xml:space="preserve">
          <source>An approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of the Laplacian of the graph. Usually this would mean working directly with the Laplacian matrix. If the original data matrix \(A\) has shape \(m \times n\), the Laplacian matrix for the corresponding bipartite graph has shape \((m + n) \times (m + n)\). However, in this case it is possible to work directly with \(A\), which is smaller and more efficient.</source>
          <target state="translated">最適な正規化カットの近似解は,グラフのラプラシアンの一般化された固有値分解を介して見つかるかもしれません.通常,これはラプラシアン行列を直接扱うことを意味する.もし、元のデータ行列が\(A\)has shape \(m \times n)なら、対応する二部グラフのラプラシアン行列は shape \((m+n)&quot;\times (m+n)A\)となる。しかし、この場合は、直接、「A\(A\)」を使った方が、より小さくて効率が良いのです。</target>
        </trans-unit>
        <trans-unit id="abda993d7d851644a2cce6e5b3201fc8e649cfdf" translate="yes" xml:space="preserve">
          <source>An approximation to the RBF kernel using random Fourier features.</source>
          <target state="translated">ランダムフーリエ特徴量を用いたRBFカーネルの近似.</target>
        </trans-unit>
        <trans-unit id="56451d3e9b79dc1326101c93a26155d78a7c4638" translate="yes" xml:space="preserve">
          <source>An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size &lt;code&gt;radius&lt;/code&gt; around the query points.</source>
          <target state="translated">クエリポイントの周りのサイズ &lt;code&gt;radius&lt;/code&gt; のボール内にある、母集団マトリックスからの近似最近傍ポイントのインデックスの配列の配列。</target>
        </trans-unit>
        <trans-unit id="69a7cca6f3965cdbc1f2a0f6316fc86cbe697d1b" translate="yes" xml:space="preserve">
          <source>An array of norms along given axis for X. When X is sparse, a NotImplementedError will be raised for norm &amp;lsquo;l1&amp;rsquo; or &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="translated">Xの指定された軸に沿ったノルムの配列。Xがスパースの場合、ノルム 'l1'または 'l2'に対してNotImplementedErrorが発生します。</target>
        </trans-unit>
        <trans-unit id="9eb7e49edfb49a7576a2bc3ab22563bf222603f7" translate="yes" xml:space="preserve">
          <source>An array of points to query</source>
          <target state="translated">クエリを行うポイントの配列</target>
        </trans-unit>
        <trans-unit id="deb131dede53f65a17272e64b8336596d48d67e7" translate="yes" xml:space="preserve">
          <source>An array of points to query. Last dimension should match dimension of training data (n_features).</source>
          <target state="translated">問い合わせを行う点の配列.最後の次元は,学習データの次元 (n_features)と一致する必要があります.</target>
        </trans-unit>
        <trans-unit id="ce1fbc6a1ce43e2f3a3c1ddf094fbdf1907a173f" translate="yes" xml:space="preserve">
          <source>An array of points to query. Last dimension should match dimension of training data.</source>
          <target state="translated">問い合わせを行う点の配列.最後の次元は学習データの次元と一致する必要があります。</target>
        </trans-unit>
        <trans-unit id="f7729a5fd61fb2317d29018126beb8ea2ea03054" translate="yes" xml:space="preserve">
          <source>An array of type np.float</source>
          <target state="translated">np.float 型の配列.</target>
        </trans-unit>
        <trans-unit id="9febb5eef4fe704c0e2f76fc17196ae84e5bb310" translate="yes" xml:space="preserve">
          <source>An array with shape (n_samples_X, n_features).</source>
          <target state="translated">形状(n_samples_X,n_features)を持つ配列。</target>
        </trans-unit>
        <trans-unit id="43ecc2546079acafbd51894522d368c0004da034" translate="yes" xml:space="preserve">
          <source>An array with shape (n_samples_X, n_samples_Y).</source>
          <target state="translated">形状(n_samples_X,n_samples_Y)を持つ配列。</target>
        </trans-unit>
        <trans-unit id="4e6f0a1fd0fa00a29e1c148218c8f262ec65e6a9" translate="yes" xml:space="preserve">
          <source>An array with shape (n_samples_Y, n_features).</source>
          <target state="translated">形状(n_samples_Y,n_features)を持つ配列。</target>
        </trans-unit>
        <trans-unit id="55ae6635fc7f12f40e0f75b8c113526f67ceb18f" translate="yes" xml:space="preserve">
          <source>An axis object onto which the plots will be drawn.</source>
          <target state="translated">プロットが描かれる軸オブジェクト。</target>
        </trans-unit>
        <trans-unit id="25ca61d5a0ba319e8f5dd67899dd1b75491f14b8" translate="yes" xml:space="preserve">
          <source>An early approach to taking advantage of this aggregate information was the &lt;em&gt;KD tree&lt;/em&gt; data structure (short for &lt;em&gt;K-dimensional tree&lt;/em&gt;), which generalizes two-dimensional &lt;em&gt;Quad-trees&lt;/em&gt; and 3-dimensional &lt;em&gt;Oct-trees&lt;/em&gt; to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no \(D\)-dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only \(O[\log(N)]\) distance computations. Though the KD tree approach is very fast for low-dimensional (\(D &amp;lt; 20\)) neighbors searches, it becomes inefficient as \(D\) grows very large: this is one manifestation of the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;. In scikit-learn, KD tree neighbors searches are specified using the keyword &lt;code&gt;algorithm = 'kd_tree'&lt;/code&gt;, and are computed using the class &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">この集約情報を利用することへの早期のアプローチであった&lt;em&gt;KDツリー&lt;/em&gt;データ構造（ショート&lt;em&gt;K次元の木&lt;/em&gt;二次元の一般化）、&lt;em&gt;クワッドツリー&lt;/em&gt;と3次元&lt;em&gt;のOct-ツリーを&lt;/em&gt;次元の任意の数に。 KDツリーは、データ空間に沿ってパラメータ空間を再帰的に分割し、データポイントが格納されるネストされた直交異方性領域に分割するバイナリツリー構造です。 KDツリーの構築は非常に高速です。分割はデータ軸に沿ってのみ実行されるため、\（D \）次元の距離を計算する必要はありません。作成されたクエリポイントの最近傍は、\（O [\ log（N）] \）の距離計算のみで決定できます。 KDツリーアプローチは、低次元（\（D &amp;lt;20 \））の近傍検索に対して非常に高速ですが、\（D \）が非常に大きくなると非効率になります。これは、いわゆる「次元の呪い」の1つの兆候です」。 scikit-learnでは、KDツリーネイバー検索はキーワード &lt;code&gt;algorithm = 'kd_tree'&lt;/code&gt; を使用して指定されます、およびクラス&lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;を使用して計算されます。</target>
        </trans-unit>
        <trans-unit id="099567709025ab0aa48e57d57280b0443b9a24ed" translate="yes" xml:space="preserve">
          <source>An empty dict signifies default parameters.</source>
          <target state="translated">空の dict はデフォルトのパラメータを示します。</target>
        </trans-unit>
        <trans-unit id="ffa5ed2f9779625aa20adf682c3351ed0a53fb7f" translate="yes" xml:space="preserve">
          <source>An encoding can also be called a &amp;lsquo;character set&amp;rsquo;, but this term is less accurate: several encodings can exist for a single character set.</source>
          <target state="translated">エンコーディングは「文字セット」と呼ぶこともできますが、この用語はあまり正確ではありません。単一の文字セットに対して複数のエンコーディングが存在する可能性があります。</target>
        </trans-unit>
        <trans-unit id="15c84c56e7d6dd7f29f03b3419a89a1fa5861864" translate="yes" xml:space="preserve">
          <source>An ensemble of totally random trees.</source>
          <target state="translated">完全にランダムな木のアンサンブル。</target>
        </trans-unit>
        <trans-unit id="372a9ef151ce0e02d102211d2e14fc577127d782" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt;と&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predictを&lt;/a&gt;実装する推定オブジェクト。</target>
        </trans-unit>
        <trans-unit id="b0193485700595d6695b40a0a211b9e29e959231" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and one of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; or &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt;と&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;または&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_probaの&lt;/a&gt;いずれかを実装する推定オブジェクト。</target>
        </trans-unit>
        <trans-unit id="215970b6b504c6bfb744566538a8accc41cd92e5" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-score&quot;&gt;score&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt;、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-score&quot;&gt;score&lt;/a&gt;、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_probaを&lt;/a&gt;実装する推定オブジェクト。</target>
        </trans-unit>
        <trans-unit id="ed51316796470f5285f1ea5227efd05cf75b44c0" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; と &lt;code&gt;predict&lt;/code&gt; 実装する推定オブジェクト。</target>
        </trans-unit>
        <trans-unit id="2c736b1e847c7ee4aecf59b3bb14cb64e436dee2" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;code&gt;fit&lt;/code&gt; and one of &lt;code&gt;decision_function&lt;/code&gt; or &lt;code&gt;predict_proba&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; を実装するestimatorオブジェクトと、 &lt;code&gt;decision_function&lt;/code&gt; または &lt;code&gt;predict_proba&lt;/code&gt; のいずれか。</target>
        </trans-unit>
        <trans-unit id="016774ede32e1f200ea316bb37e829ebc5d620e2" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;score&lt;/code&gt; and &lt;code&gt;predict_proba&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 、 &lt;code&gt;score&lt;/code&gt; 、および &lt;code&gt;predict_proba&lt;/code&gt; を実装する推定子オブジェクト。</target>
        </trans-unit>
        <trans-unit id="bc6d2e5beaf0995cbcda2b429dbc0e874dd47ce4" translate="yes" xml:space="preserve">
          <source>An estimator object that is used to compute the initial predictions. &lt;code&gt;init&lt;/code&gt; has to provide &lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.predict_proba&quot;&gt;&lt;code&gt;predict_proba&lt;/code&gt;&lt;/a&gt;. If &amp;lsquo;zero&amp;rsquo;, the initial raw predictions are set to zero. By default, a &lt;code&gt;DummyEstimator&lt;/code&gt; predicting the classes priors is used.</source>
          <target state="translated">初期予測を計算するために使用される推定オブジェクト。 &lt;code&gt;init&lt;/code&gt; は&lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.predict_proba&quot;&gt; &lt;code&gt;predict_proba&lt;/code&gt; &lt;/a&gt;を提供する必要があります。'zero'の場合、初期の生の予測はゼロに設定されます。デフォルトでは、クラスの &lt;code&gt;DummyEstimator&lt;/code&gt; 予測を予測するDummyEstimatorが使用されます。</target>
        </trans-unit>
        <trans-unit id="cd1e7319f04194cc6e7a0ff87c048ca39034bdd7" translate="yes" xml:space="preserve">
          <source>An estimator object that is used to compute the initial predictions. &lt;code&gt;init&lt;/code&gt; has to provide &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;. If &amp;lsquo;zero&amp;rsquo;, the initial raw predictions are set to zero. By default a &lt;code&gt;DummyEstimator&lt;/code&gt; is used, predicting either the average target value (for loss=&amp;rsquo;ls&amp;rsquo;), or a quantile for the other losses.</source>
          <target state="translated">初期予測を計算するために使用される推定オブジェクト。 &lt;code&gt;init&lt;/code&gt; は、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;適合&lt;/a&gt;と&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;予測&lt;/a&gt;を提供する必要があります。'zero'の場合、初期の生の予測はゼロに設定されます。デフォルトでは、 &lt;code&gt;DummyEstimator&lt;/code&gt; が使用され、平均目標値（loss = 'ls'の場合​​）または他の損失の分位数が予測されます。</target>
        </trans-unit>
        <trans-unit id="3e3cbd1a80e427709b1e224e290f485edaa6e701" translate="yes" xml:space="preserve">
          <source>An estimator object that is used to compute the initial predictions. &lt;code&gt;init&lt;/code&gt; has to provide &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. If None it uses &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">初期予測を計算するために使用される推定オブジェクト。 &lt;code&gt;init&lt;/code&gt; は &lt;code&gt;fit&lt;/code&gt; と &lt;code&gt;predict&lt;/code&gt; を提供する必要があります。Noneの場合は &lt;code&gt;loss.init_estimator&lt;/code&gt; を使用します。</target>
        </trans-unit>
        <trans-unit id="cf6861479f811af12e4fa2074ef55363e1ab1784" translate="yes" xml:space="preserve">
          <source>An estimator that has already been &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fitted&quot;&gt;fitted&lt;/a&gt; and is compatible with &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-scorer&quot;&gt;scorer&lt;/a&gt;.</source>
          <target state="translated">すでに&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fitted&quot;&gt;適合&lt;/a&gt;されており、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-scorer&quot;&gt;スコアラー&lt;/a&gt;と互換性のある推定量。</target>
        </trans-unit>
        <trans-unit id="8add3305e3de95c19db62ec2f9ba60a9992f6c8c" translate="yes" xml:space="preserve">
          <source>An estimator to inspect.</source>
          <target state="translated">検査するための見積もり屋さん。</target>
        </trans-unit>
        <trans-unit id="0c14b451345a9bebab4624cdfc1eb448ab003b65" translate="yes" xml:space="preserve">
          <source>An example comparing nearest neighbors classification with and without Neighborhood Components Analysis.</source>
          <target state="translated">近傍成分分析を用いた場合と用いない場合の最近傍分類を比較した例。</target>
        </trans-unit>
        <trans-unit id="0583622ca0751c993f08e9d98041286805dcca3a" translate="yes" xml:space="preserve">
          <source>An example comparing the effect of reconstructing noisy fragments of a raccoon face image using firstly online &lt;a href=&quot;../../modules/decomposition#dictionarylearning&quot;&gt;Dictionary Learning&lt;/a&gt; and various transform methods.</source>
          <target state="translated">最初にオンラインの&lt;a href=&quot;../../modules/decomposition#dictionarylearning&quot;&gt;辞書学習&lt;/a&gt;とさまざまな変換方法を使用して、アライグマの顔画像のノイズの多いフラグメントを再構築する効果を比較する例。</target>
        </trans-unit>
        <trans-unit id="d3ed7be079cbbb4b66d35ed08e7318047e508129" translate="yes" xml:space="preserve">
          <source>An example illustrating the approximation of the feature map of an RBF kernel.</source>
          <target state="translated">RBF カーネルの特徴量マップの近似の例。</target>
        </trans-unit>
        <trans-unit id="5f84ab4970b6751b484f351a6364e917ee9e2e2e" translate="yes" xml:space="preserve">
          <source>An example of a chunked operation adhering to this setting is &lt;code&gt;metric.pairwise_distances_chunked&lt;/code&gt;, which facilitates computing row-wise reductions of a pairwise distance matrix.</source>
          <target state="translated">この設定に準拠したチャンク操作の例は、 &lt;code&gt;metric.pairwise_distances_chunked&lt;/code&gt; です。これにより、ペアワイズ距離行列の行単位の削減の計算が容易になります。</target>
        </trans-unit>
        <trans-unit id="d30da83c344e58d7fad635964b30a7c96a578d3f" translate="yes" xml:space="preserve">
          <source>An example of an estimator is the class &lt;code&gt;sklearn.svm.SVC&lt;/code&gt;, which implements &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;support vector classification&lt;/a&gt;. The estimator&amp;rsquo;s constructor takes as arguments the model&amp;rsquo;s parameters.</source>
          <target state="translated">推定器の例は、&lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;サポートベクター分類&lt;/a&gt;を実装するクラス &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; です。推定器のコンストラクターは、引数としてモデルのパラメーターを受け取ります。</target>
        </trans-unit>
        <trans-unit id="2a28c2a5262858339108957d662b2d9ad76c60fe" translate="yes" xml:space="preserve">
          <source>An example of biclusters formed by partitioning rows and columns.</source>
          <target state="translated">行と列を分割して形成されたバイクラスターの例。</target>
        </trans-unit>
        <trans-unit id="4ec3a210a521488a01f06a7254c4cbcafb1acb47" translate="yes" xml:space="preserve">
          <source>An example of checkerboard biclusters.</source>
          <target state="translated">市松模様のバイクラスターの例。</target>
        </trans-unit>
        <trans-unit id="263d7481d0f6606e447078322b3482f64ed58b85" translate="yes" xml:space="preserve">
          <source>An example of estimating sources from noisy data.</source>
          <target state="translated">ノイズの多いデータからソースを推定する例。</target>
        </trans-unit>
        <trans-unit id="ba8a60e5ae0a34a0a9e8c9683b9f2c1dae098cc6" translate="yes" xml:space="preserve">
          <source>An example of reshaping data would be the digits dataset</source>
          <target state="translated">データのリシェイプの例としては、数字のデータセットがあります。</target>
        </trans-unit>
        <trans-unit id="fbe3a4b0f4df558e025b4b6c278ce0b8f2e24f89" translate="yes" xml:space="preserve">
          <source>An example of the HTML output can be seen in the &lt;strong&gt;HTML representation of Pipeline&lt;/strong&gt; section of &lt;a href=&quot;../auto_examples/compose/plot_column_transformer_mixed_types#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py&quot;&gt;Column Transformer with Mixed Types&lt;/a&gt;. As an alternative, the HTML can be written to a file using &lt;a href=&quot;generated/sklearn.utils.estimator_html_repr#sklearn.utils.estimator_html_repr&quot;&gt;&lt;code&gt;estimator_html_repr&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">HTML出力の例は、&lt;a href=&quot;../auto_examples/compose/plot_column_transformer_mixed_types#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py&quot;&gt;混合タイプの列トランスフォーマーの&lt;/a&gt;&lt;strong&gt;パイプライン&lt;/strong&gt;セクションの&lt;strong&gt;HTML表現で&lt;/strong&gt;確認できます。別の方法として、&lt;a href=&quot;generated/sklearn.utils.estimator_html_repr#sklearn.utils.estimator_html_repr&quot;&gt; &lt;code&gt;estimator_html_repr&lt;/code&gt; &lt;/a&gt;を使用してHTMLをファイルに書き込むことができます。</target>
        </trans-unit>
        <trans-unit id="1fbe1528edfe5c2ef6e2542ba5d3a7a740cb4806" translate="yes" xml:space="preserve">
          <source>An example of the same &lt;code&gt;y&lt;/code&gt; in sparse matrix form:</source>
          <target state="translated">スパース行列形式の同じ &lt;code&gt;y&lt;/code&gt; の例：</target>
        </trans-unit>
        <trans-unit id="d2d2bef04bd500898cd35385a3db44c487620221" translate="yes" xml:space="preserve">
          <source>An example showing how different online solvers perform on the hand-written digits dataset.</source>
          <target state="translated">手書きの数字データセットで、異なるオンラインソルバーがどのように動作するかを示す例。</target>
        </trans-unit>
        <trans-unit id="2aae52ed09d5204233cf6faccc3b62129219ab9a" translate="yes" xml:space="preserve">
          <source>An example showing how the scikit-learn can be used to recognize images of hand-written digits.</source>
          <target state="translated">scikit-learnを用いて、手書きの数字の画像を認識する方法を示す例。</target>
        </trans-unit>
        <trans-unit id="dc6fbf3a5e9982b266901310af78a9acc8d8b83c" translate="yes" xml:space="preserve">
          <source>An example showing univariate feature selection.</source>
          <target state="translated">一変量特徴選択を示す例。</target>
        </trans-unit>
        <trans-unit id="a63d45f44a041fe05e157072e07ec905aa6168f0" translate="yes" xml:space="preserve">
          <source>An example to compare multi-output regression with random forest and the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt; meta-estimator.</source>
          <target state="translated">多出力回帰をランダムフォレストと&lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt;メタ推定器と比較する例。</target>
        </trans-unit>
        <trans-unit id="d69827da4fe888278b671529418570646583bc79" translate="yes" xml:space="preserve">
          <source>An example to illustrate multi-output regression with decision tree.</source>
          <target state="translated">決定木を用いた多出力回帰を説明する例。</target>
        </trans-unit>
        <trans-unit id="278a8759739f9d084f6d58de5359105935a98d5a" translate="yes" xml:space="preserve">
          <source>An example to show covariance estimation with the Mahalanobis distances on Gaussian distributed data.</source>
          <target state="translated">ガウス分布データのマハラノビス距離を用いた共分散推定を示す例。</target>
        </trans-unit>
        <trans-unit id="79be1ff44d921e5597feeeddf48473c82478df46" translate="yes" xml:space="preserve">
          <source>An example using &lt;a href=&quot;../../modules/generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;sklearn.ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; for anomaly detection.</source>
          <target state="translated">異常検出に&lt;a href=&quot;../../modules/generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt; &lt;code&gt;sklearn.ensemble.IsolationForest&lt;/code&gt; &lt;/a&gt;を使用する例。</target>
        </trans-unit>
        <trans-unit id="c015cad72773b21af2994319c7e4935c0925533a" translate="yes" xml:space="preserve">
          <source>An example using a one-class SVM for novelty detection.</source>
          <target state="translated">新規性検出に1クラスSVMを用いた例。</target>
        </trans-unit>
        <trans-unit id="e3e6ce179701e2cc84a27da4032032e9c54e1a80" translate="yes" xml:space="preserve">
          <source>An extra-trees classifier.</source>
          <target state="translated">余分な木の分類器。</target>
        </trans-unit>
        <trans-unit id="402fd1d845155a85adc98aea2772c11c414eb821" translate="yes" xml:space="preserve">
          <source>An extra-trees regressor.</source>
          <target state="translated">エクストラツリーの回帰器。</target>
        </trans-unit>
        <trans-unit id="36e1ed7c225c31bf0ba3eb4dff97aa286624cf9d" translate="yes" xml:space="preserve">
          <source>An extremely randomized tree classifier.</source>
          <target state="translated">極端にランダム化された木の分類器。</target>
        </trans-unit>
        <trans-unit id="2963ff7051490c77b458af39546e6bfce2134347" translate="yes" xml:space="preserve">
          <source>An extremely randomized tree regressor.</source>
          <target state="translated">極めてランダム化された木の回帰器。</target>
        </trans-unit>
        <trans-unit id="2e305a3161f1f3a1a75895f3bf89737b2fa1110e" translate="yes" xml:space="preserve">
          <source>An illustration of Swiss Roll reduction with locally linear embedding</source>
          <target state="translated">局所的な線形埋め込みを用いたスイスロール縮小の図解</target>
        </trans-unit>
        <trans-unit id="ff57c22861af0b46c7c064d86f712f1102614de2" translate="yes" xml:space="preserve">
          <source>An illustration of dimensionality reduction on the S-curve dataset with various manifold learning methods.</source>
          <target state="translated">様々な多様な学習法を用いたS字型データセットの次元削減の説明。</target>
        </trans-unit>
        <trans-unit id="461c40d4fafb15fd1dbf5dddb7defd80f66220bc" translate="yes" xml:space="preserve">
          <source>An illustration of t-SNE on the two concentric circles and the S-curve datasets for different perplexity values.</source>
          <target state="translated">2つの同心円上のt-SNEと、異なるペルプレキシシティ値のS字データセットの図。</target>
        </trans-unit>
        <trans-unit id="4217a4a05133f009315b7adf6df3894014f6eea5" translate="yes" xml:space="preserve">
          <source>An illustration of the isotonic regression on generated data. The isotonic regression finds a non-decreasing approximation of a function while minimizing the mean squared error on the training data. The benefit of such a model is that it does not assume any form for the target function such as linearity. For comparison a linear regression is also presented.</source>
          <target state="translated">生成されたデータでのアイソトニック回帰の図。アイソトニック回帰は、学習データの平均2乗誤差を最小化しながら、関数の非減少近似を見つける。このようなモデルの利点は、線形性のような目的関数のためにいかなる形式も仮定しないことです。比較のために線形回帰も提示されている。</target>
        </trans-unit>
        <trans-unit id="95f191130b61aa88bf4d9f0c56ed2e78e6bd9498" translate="yes" xml:space="preserve">
          <source>An illustration of the metric and non-metric MDS on generated noisy data.</source>
          <target state="translated">生成されたノイズの多いデータに対するメトリックと非メトリックのデータシートの図。</target>
        </trans-unit>
        <trans-unit id="4ed4596d6f2bc0049548fe72a3bd1528d6ff1a95" translate="yes" xml:space="preserve">
          <source>An illustration of various embeddings on the digits dataset.</source>
          <target state="translated">桁データセットの様々なエンベッディングの説明図。</target>
        </trans-unit>
        <trans-unit id="5349a7af33dbb0da98921a0576af67237bdedc22" translate="yes" xml:space="preserve">
          <source>An illustration of various linkage option for agglomerative clustering on a 2D embedding of the digits dataset.</source>
          <target state="translated">数字データセットの2次元埋め込み上での凝集型クラスタリングのための様々なリンケージオプションの説明図。</target>
        </trans-unit>
        <trans-unit id="9431c782d6d1945d9109d26f9bdc51c8ca5b4cc9" translate="yes" xml:space="preserve">
          <source>An implementation of a randomized algorithm for principal component analysis A. Szlam et al. 2014</source>
          <target state="translated">主成分分析のためのランダム化アルゴリズムの実装 A.Szlam 他 2014年</target>
        </trans-unit>
        <trans-unit id="97eec2318d7e6e3549c069ff7f108d7733cec0af" translate="yes" xml:space="preserve">
          <source>An important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones.</source>
          <target state="translated">性能最適化の重要な側面は、予測精度を損なう可能性があるということです。実際、より単純なモデル(例えば、非線形ではなく線形であったり、パラメータが少ない)は、しばしば高速に実行されますが、より複雑なモデルと同じようにデータの正確な特性を考慮に入れることができない場合があります。</target>
        </trans-unit>
        <trans-unit id="44f31261fda2a48216b1472f9c206633a0f349f1" translate="yes" xml:space="preserve">
          <source>An important notion of robust fitting is that of breakdown point: the fraction of data that can be outlying for the fit to start missing the inlying data.</source>
          <target state="translated">ロバストフィットの重要な概念は、ブレークダウンポイントです。</target>
        </trans-unit>
        <trans-unit id="98153081fcfad9474299ea23738f0ec83b423b23" translate="yes" xml:space="preserve">
          <source>An important question is how can the Dirichlet process use an infinite, unbounded number of clusters and still be consistent. While a full explanation doesn&amp;rsquo;t fit this manual, one can think of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process&quot;&gt;stick breaking process&lt;/a&gt; analogy to help understanding it. The stick breaking process is a generative story for the Dirichlet process. We start with a unit-length stick and in each step we break off a portion of the remaining stick. Each time, we associate the length of the piece of the stick to the proportion of points that falls into a group of the mixture. At the end, to represent the infinite mixture, we associate the last remaining piece of the stick to the proportion of points that don&amp;rsquo;t fall into all the other groups. The length of each piece is a random variable with probability proportional to the concentration parameter. Smaller value of the concentration will divide the unit-length into larger pieces of the stick (defining more concentrated distribution). Larger concentration values will create smaller pieces of the stick (increasing the number of components with non zero weights).</source>
          <target state="translated">重要な質問は、ディリクレプロセスが無限の無限の数のクラスターをどのように使用し、それでも一貫性を保つことができるかです。完全な説明はこのマニュアルに適合しませんが、&lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process&quot;&gt;スティックの破壊プロセスを&lt;/a&gt;考えることができますそれを理解するのに役立つアナロジー。スティック破壊プロセスは、ディリクレプロセスの生成ストーリーです。単位長のスティックから始め、各ステップで残りのスティックの一部を切り離します。毎回、スティックのピースの長さを、混合のグループに分類されるポイントの比率に関連付けます。最後に、無限の混合を表すために、スティックの最後に残った部分を、他のすべてのグループに分類されないポイントの比率に関連付けます。各ピースの長さは確率変数であり、確率は濃度パラメーターに比例します。濃度の値が小さいほど、単位長はスティックのより大きな部分に分割されます（より集中した分布を定義します）。濃度の値が大きいほど、スティックの断片が小さくなります（重みがゼロでないコンポーネントの数が増えます）。</target>
        </trans-unit>
        <trans-unit id="627c954c961a3acd43edae0e16b57eae5630ce43" translate="yes" xml:space="preserve">
          <source>An improvement of the Ledoit-Wolf shrinkage, the &lt;a href=&quot;../../modules/generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;sklearn.covariance.OAS&lt;/code&gt;&lt;/a&gt;, proposed by Chen et al. Its convergence is significantly better under the assumption that the data are Gaussian, in particular for small samples.</source>
          <target state="translated">Ledoit -ウルフ収縮の改善、&lt;a href=&quot;../../modules/generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;sklearn.covariance.OAS&lt;/code&gt; &lt;/a&gt; Chenらによって提案されました。特に小さなサンプルの場合、データがガウスであるという仮定の下で、その収束は大幅に向上します。</target>
        </trans-unit>
        <trans-unit id="370f3995d8ef065bc41012596e70d603a42a06c3" translate="yes" xml:space="preserve">
          <source>An index that selects the retained features from a feature vector. If &lt;code&gt;indices&lt;/code&gt; is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If &lt;code&gt;indices&lt;/code&gt; is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.</source>
          <target state="translated">保持された特徴を特徴ベクトルから選択するインデックス。 &lt;code&gt;indices&lt;/code&gt; がFalseの場合、これは形状のブール配列[＃入力フィーチャ]であり、対応するフィーチャが保持のために選択されている場合、要素はTrueです。 &lt;code&gt;indices&lt;/code&gt; がTrueの場合、これは形状の整数配列[＃出力フィーチャ]であり、その値は入力フィーチャベクトルへのインデックスです。</target>
        </trans-unit>
        <trans-unit id="537bad4eab470a047cae67c34b767bf9f3d01c01" translate="yes" xml:space="preserve">
          <source>An instance of the estimator.</source>
          <target state="translated">推定子のインスタンス。</target>
        </trans-unit>
        <trans-unit id="d0f36ad8d88245eefc6653aba92b1989e3a7de69" translate="yes" xml:space="preserve">
          <source>An int, giving the exact number of total jobs that are spawned</source>
          <target state="translated">int。生成されるジョブの総数を正確に表します。</target>
        </trans-unit>
        <trans-unit id="efb853d408e1363bd14470415d5367521c99f9fa" translate="yes" xml:space="preserve">
          <source>An interesting aspect of &lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt; is that connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; &lt;/a&gt;の興味深い側面は、接続の制約をこのアルゴリズムに追加できることです（隣接するクラスターのみをマージできます）。各サンプルに対して、データの特定の構造に従って隣接するサンプルを定義する接続マトリックスを使用します。たとえば、以下のスイスロールの例では、接続制約により、スイスロールで隣接していないポイントのマージが禁止され、ロールの重なり合った折り目にまたがるクラスターの形成が回避されます。</target>
        </trans-unit>
        <trans-unit id="edca22917cc04c2cee4dca2af159124718d78594" translate="yes" xml:space="preserve">
          <source>An interesting development of using a &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; is the ability to perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Out-of-core_algorithm&quot;&gt;out-of-core&lt;/a&gt; scaling. This means that we can learn from data that does not fit into the computer&amp;rsquo;s main memory.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt;を使用した興味深い開発は、&lt;a href=&quot;https://en.wikipedia.org/wiki/Out-of-core_algorithm&quot;&gt;コア外&lt;/a&gt;スケーリングを実行する機能です。これは、コンピュータのメインメモリに収まらないデータから学習できることを意味します。</target>
        </trans-unit>
        <trans-unit id="e061f170f65c98ddc125ba623de0d484bced6052" translate="yes" xml:space="preserve">
          <source>An introduction to machine learning with scikit-learn</source>
          <target state="translated">scikit-learnを使った機械学習の入門編</target>
        </trans-unit>
        <trans-unit id="f8000912a05784b4ed09f166d2281bb382154409" translate="yes" xml:space="preserve">
          <source>An iterable which yields either str, unicode or file objects.</source>
          <target state="translated">str、unicode、またはファイルオブジェクトのいずれかを返す反復可能なもの。</target>
        </trans-unit>
        <trans-unit id="ed21dc2777fffee9a18d3be232cdf0f7a675b104" translate="yes" xml:space="preserve">
          <source>An iterable yielding (train, test) splits as arrays of indices.</source>
          <target state="translated">反復可能な降伏(train,test)はインデックスの配列として分割されます。</target>
        </trans-unit>
        <trans-unit id="26021975f66731cfc83771c3f9a3b8617a224127" translate="yes" xml:space="preserve">
          <source>An iterable yielding train, test splits.</source>
          <target state="translated">イテレート可能な降伏列車、テストスプリット。</target>
        </trans-unit>
        <trans-unit id="793ecf7b6fedca1d245cd9de3dfad1789ad435c2" translate="yes" xml:space="preserve">
          <source>An iterable yielding train/test splits.</source>
          <target state="translated">列車とテストの分割を可能にする反復可能な降伏。</target>
        </trans-unit>
        <trans-unit id="2c74b450a38327bc579731e9a6b693f5a72972a9" translate="yes" xml:space="preserve">
          <source>An object for detecting outliers in a Gaussian distributed dataset.</source>
          <target state="translated">ガウス分布データセットの外れ値を検出するためのオブジェクト。</target>
        </trans-unit>
        <trans-unit id="bb3e30a61bd34f61e007c1401c3e81f0d43c8edb" translate="yes" xml:space="preserve">
          <source>An object of that type which is cloned for each validation.</source>
          <target state="translated">検証のたびにクローン化されるその型のオブジェクト。</target>
        </trans-unit>
        <trans-unit id="9720a88e8aa2bbe2107788283b8978f9a7cb4b22" translate="yes" xml:space="preserve">
          <source>An object to be used as a cross-validation generator,</source>
          <target state="translated">クロスバリデーション生成器として使用されるオブジェクト。</target>
        </trans-unit>
        <trans-unit id="bbffbe09f32930b2800698ca47bd093017747bdf" translate="yes" xml:space="preserve">
          <source>An object to be used as a cross-validation generator.</source>
          <target state="translated">クロスバリデーション生成器として使用されるオブジェクト。</target>
        </trans-unit>
        <trans-unit id="a7002171a59c54b21633748422d618355d53d5f0" translate="yes" xml:space="preserve">
          <source>An optional mask of the image, to consider only part of the pixels.</source>
          <target state="translated">ピクセルの一部のみを考慮するための画像のオプションのマスク.</target>
        </trans-unit>
        <trans-unit id="fe8123eda49913b2b31a4b7b815dc029043ed233" translate="yes" xml:space="preserve">
          <source>An optional progress meter.</source>
          <target state="translated">オプションのプログレスメーター。</target>
        </trans-unit>
        <trans-unit id="229c2b7073ef4d939c3f4e9dc0933cc0c25d3c6e" translate="yes" xml:space="preserve">
          <source>An optional second feature array. Only allowed if metric != &amp;ldquo;precomputed&amp;rdquo;.</source>
          <target state="translated">オプションの2番目の機能配列。メトリック！=「事前計算済み」の場合にのみ許可されます。</target>
        </trans-unit>
        <trans-unit id="0072ba612e2ac9888a715c6d07a5f35671c0b0f4" translate="yes" xml:space="preserve">
          <source>An ordered array of unique labels.</source>
          <target state="translated">ユニークなラベルの順序付き配列。</target>
        </trans-unit>
        <trans-unit id="a760a22c84feac040244658e5cd6e733d7fd7a9c" translate="yes" xml:space="preserve">
          <source>An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a binary coding with as many ones as there are trees in the forest.</source>
          <target state="translated">データセットを高次元の疎な表現に変換する教師なしの変換.データポイントは,各木のどの葉に分類されるかによって符号化される.葉のワンショット符号化を用いることで,森の中にある木の数だけ葉があるバイナリ符号化を行う.</target>
        </trans-unit>
        <trans-unit id="8bba08ddfbe8b42a7ddb2ba7f5d85088746ededd" translate="yes" xml:space="preserve">
          <source>An upper bound on the fraction of margin errors (see &lt;a href=&quot;../svm#nu-svc&quot;&gt;User Guide&lt;/a&gt;) and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].</source>
          <target state="translated">マージンエラーの割合の上限（&lt;a href=&quot;../svm#nu-svc&quot;&gt;ユーザーガイドを参照&lt;/a&gt;）およびサポートベクターの割合の下限。間隔（0、1]内にある必要があります。</target>
        </trans-unit>
        <trans-unit id="0d82cfe79ef09c873c85764ea87217dd841df582" translate="yes" xml:space="preserve">
          <source>An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].</source>
          <target state="translated">学習誤差の割合の上限と支持ベクトルの割合の下限.区間 (0,1].</target>
        </trans-unit>
        <trans-unit id="5f9f028f3aaaed07cbca0fd41b78759c2fe2428d" translate="yes" xml:space="preserve">
          <source>An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.</source>
          <target state="translated">学習誤差の割合の上限と支持ベクトルの割合の下限.区間 (0,1].デフォルトでは0.5が取られます.</target>
        </trans-unit>
        <trans-unit id="c638b2709b286483a90c4415602f89072f7cd76d" translate="yes" xml:space="preserve">
          <source>Analysis of the plots</source>
          <target state="translated">プロットの分析</target>
        </trans-unit>
        <trans-unit id="130d90555949c1ba7863adb5fc8aa4d5f4dd611e" translate="yes" xml:space="preserve">
          <source>Analyzing a portion of the ROC curve. McClish, 1989</source>
          <target state="translated">ROC曲線の一部を分析する。マックリッシュ,1989</target>
        </trans-unit>
        <trans-unit id="b98195adea1583e9b237e74652523f846b2b1879" translate="yes" xml:space="preserve">
          <source>And for multiple metric evaluation, the return value is a dict with the following keys - &lt;code&gt;['test_&amp;lt;scorer1_name&amp;gt;', 'test_&amp;lt;scorer2_name&amp;gt;', 'test_&amp;lt;scorer...&amp;gt;', 'fit_time', 'score_time']&lt;/code&gt;</source>
          <target state="translated">また、複数のメトリック評価の場合、戻り値は次のキーを持つdictです- &lt;code&gt;['test_&amp;lt;scorer1_name&amp;gt;', 'test_&amp;lt;scorer2_name&amp;gt;', 'test_&amp;lt;scorer...&amp;gt;', 'fit_time', 'score_time']&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c48696369bc4cd123e13a474e64394fa9533d40a" translate="yes" xml:space="preserve">
          <source>And some work with binary and multilabel (but not multiclass) problems:</source>
          <target state="translated">また、バイナリやマルチラベル(ただしマルチクラスではない)の問題を扱うものもあります。</target>
        </trans-unit>
        <trans-unit id="551fc600bc5b2c147bfd7d608b1227c6f0c95818" translate="yes" xml:space="preserve">
          <source>And the L2-normalized tf-idf changes to</source>
          <target state="translated">そして、L2正規化されたtf-idfは</target>
        </trans-unit>
        <trans-unit id="fd06a66dae1b69b6eb93cccfb52c33888c5a371c" translate="yes" xml:space="preserve">
          <source>And the classifier &amp;ldquo;predictions&amp;rdquo; are perfect:</source>
          <target state="translated">そして分類子の「予測」は完璧です：</target>
        </trans-unit>
        <trans-unit id="b646eac54bde60aa4d7c89014b7fd028412efa2f" translate="yes" xml:space="preserve">
          <source>Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster evaluation measure</source>
          <target state="translated">アンドリュー・ローゼンバーグとジュリア・ヒルシュバーグ、2007年。V-Measure.条件付きエントロピーに基づく外部クラスタ評価尺度</target>
        </trans-unit>
        <trans-unit id="5ca338dedf0d8331238e60ad1d0242b94a749529" translate="yes" xml:space="preserve">
          <source>Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and J&amp;ouml;rg Sander. &amp;ldquo;OPTICS: ordering points to identify the clustering structure.&amp;rdquo; ACM SIGMOD Record 28, no. 2 (1999): 49-60.</source>
          <target state="translated">Ankerst、Mihael、Markus M. Breunig、Hans-Peter Kriegel、およびJ&amp;ouml;rgSander。「OPTICS：クラスタリング構造を識別するための順序付けポイント。」ACM SIGMODレコード28、いいえ。2（1999）：49-60。</target>
        </trans-unit>
        <trans-unit id="6e2b1b23fa02a3f4b2a576f98b5ef19fd6944167" translate="yes" xml:space="preserve">
          <source>Another alternative is to take a symmetric version of the k nearest neighbors connectivity matrix of the points.</source>
          <target state="translated">別の選択肢として、点のk個の最近傍接続行列の対称版を取ることもできる。</target>
        </trans-unit>
        <trans-unit id="c8a5ccc165016bcc8540311d11c1eec6480387f0" translate="yes" xml:space="preserve">
          <source>Another approach is to monitor convergence on a validation score. In this case, the input data is split into a training set and a validation set. The model is then fitted on the training set and the stopping criterion is based on the prediction score computed on the validation set. This enables us to find the least number of iterations which is sufficient to build a model that generalizes well to unseen data and reduces the chance of over-fitting the training data.</source>
          <target state="translated">もう1つのアプローチは、検証スコアの収束を監視することです。この場合、入力データは訓練セットと検証セットに分割される。モデルが訓練セットに適合し、停止基準は検証セットで計算された予測スコアに基づいています。これにより、見えないデータによく一般化するモデルを構築するのに十分な最小限の反復回数を見つけ、訓練データにオーバーフィットする可能性を減らすことができます。</target>
        </trans-unit>
        <trans-unit id="be7184b0e2bd98f85cd552cd8ec12d77ca9e3964" translate="yes" xml:space="preserve">
          <source>Another aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the &lt;code&gt;Perceptron&lt;/code&gt; is still sensitive to badly labeled examples even after many examples whereas the &lt;code&gt;SGD*&lt;/code&gt; and &lt;code&gt;PassiveAggressive*&lt;/code&gt; families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time.</source>
          <target state="translated">適切なアルゴリズムを選択する際に考慮すべきもう1つの側面は、それらすべてが時間の経過とともに各例に同じ重要性を置くわけではないということです。つまり、 &lt;code&gt;Perceptron&lt;/code&gt; は多くの例の後でさえも悪いラベルの例に依然として敏感ですが、 &lt;code&gt;SGD*&lt;/code&gt; および &lt;code&gt;PassiveAggressive*&lt;/code&gt; ファミリーはこの種のアーティファクトに対してより堅牢です。逆に、後者はまた、学習率が時間の経過とともに低下するため、ストリームが遅れて来るときに著しく異なるが適切にラベル付けされた例をあまり重要視しない傾向があります。</target>
        </trans-unit>
        <trans-unit id="32a1cd2cb87665a11effa4dc4df8f03517ac0638" translate="yes" xml:space="preserve">
          <source>Another common application is to use time information: for instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.</source>
          <target state="translated">もう一つの一般的なアプリケーションは、時間情報を使用することです:例えば、グループはサンプルの収集年とすることができ、その結果、時間ベースの分裂に対してクロスバリデーションを行うことができます。</target>
        </trans-unit>
        <trans-unit id="8230731c6a46697977b2ab40afba474c6e0f5aba" translate="yes" xml:space="preserve">
          <source>Another efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm.</source>
          <target state="translated">中程度に高い次元のデータセットで外れ値検出を実行するもう一つの効率的な方法は、局所外れ値因子(LOF)アルゴリズムを使用することです。</target>
        </trans-unit>
        <trans-unit id="421e4566a6c577d45c51d939beed6fdb20866913" translate="yes" xml:space="preserve">
          <source>Another evaluation measure for multi-class classification is macro-averaging, which gives equal weight to the classification of each label.</source>
          <target state="translated">マルチクラス分類のもう一つの評価指標は、各ラベルの分類に等しく重みを与えるマクロ平均化である。</target>
        </trans-unit>
        <trans-unit id="9aeb8b5cf9580937985ce741399f76caf7f788f7" translate="yes" xml:space="preserve">
          <source>Another evaluation measure for multi-label classification is macro-averaging, which gives equal weight to the classification of each label.</source>
          <target state="translated">マルチラベル分類のもう一つの評価指標として、各ラベルの分類に等しく重みを与えるマクロ平均化があります。</target>
        </trans-unit>
        <trans-unit id="103287c3ab74c62412ea0fba62e90af2b9fbac2a" translate="yes" xml:space="preserve">
          <source>Another important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;Prediction Latency&lt;/a&gt; example that measures this quantity for a number of estimators on synthetic data:</source>
          <target state="translated">本番システムのサイジングの際に注意すべきもう1つの重要な指標は、スループット、つまり一定時間内に行うことができる予測の数です。以下は、合成データのいくつかの推定量に対してこの量を測定する&lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;予測レイテンシーの&lt;/a&gt;例のベンチマークです。</target>
        </trans-unit>
        <trans-unit id="57f765049776b7061ec3ebaa517aed91ea1819db" translate="yes" xml:space="preserve">
          <source>Another option is the &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt;. This uses round-robin linear regression, modeling each feature with missing values as a function of other features, in turn. The version implemented assumes Gaussian (output) variables. If your features are obviously non-normal, consider transforming them to look more normal to potentially improve performance.</source>
          <target state="translated">もう1つのオプションは、&lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt; &lt;/a&gt;です。これはラウンドロビン線形回帰を使用し、欠測値を持つ各特徴を他の特徴の関数としてモデル化します。実装されたバージョンは、ガウス（出力）変数を想定しています。機能が明らかに非正規である場合は、パフォーマンスを向上させる可能性があるため、より正規に見えるように変換することを検討してください。</target>
        </trans-unit>
        <trans-unit id="c8eb27e015d30244a13ff2ea0a8b9e91973d57ab" translate="yes" xml:space="preserve">
          <source>Another option is to use an iterable yielding (train, test) splits as arrays of indices, for example:</source>
          <target state="translated">別のオプションとして、例えば、反復可能な降伏(訓練、テスト)分割をインデックスの配列として使用することができます。</target>
        </trans-unit>
        <trans-unit id="ebe975675e662edf348908a46736c6c5923bfe28" translate="yes" xml:space="preserve">
          <source>Another possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;OneHotEncoder&lt;/code&gt;&lt;/a&gt;, which transforms each categorical feature with &lt;code&gt;n_categories&lt;/code&gt; possible values into &lt;code&gt;n_categories&lt;/code&gt; binary features, with one of them 1, and all others 0.</source>
          <target state="translated">カテゴリ特徴をscikit-learn推定器で使用できる特徴に変換する別の可能性は、one-of-Kを使用することです。これは、ワンホットまたはダミーエンコーディングとしても知られています。このタイプのエンコーディングは、&lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;OneHotEncoder&lt;/code&gt; &lt;/a&gt;で取得できます。OneHotEncoderは、 &lt;code&gt;n_categories&lt;/code&gt; の可能な値を持つ各カテゴリ特徴を、1つが1で他のすべてが0の &lt;code&gt;n_categories&lt;/code&gt; バイナリ特徴に変換します。</target>
        </trans-unit>
        <trans-unit id="be92131a59ab295e72fcba022db0017e8b66e901" translate="yes" xml:space="preserve">
          <source>Another possibility to take into account correlated variables in the dataset, is to estimate sparse coefficients. In some way we already did it manually when we dropped the AGE column in a previous Ridge estimation.</source>
          <target state="translated">データセット内の相関のある変数を考慮に入れるもう一つの可能性は、疎な係数を推定することです。以前のRidge推定でAGE列を削除したときに、いくつかの方法ですでに手動で行っています。</target>
        </trans-unit>
        <trans-unit id="bd4ca3c02f5c305e57327d7ebc584dba1a900469" translate="yes" xml:space="preserve">
          <source>Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.</source>
          <target state="translated">tfの上にあるもう一つの改良点は、コーパス内の多くの文書で発生し、コーパスの一部でしか発生しない単語よりも情報量が少ない単語の重みをダウンスケールすることです。</target>
        </trans-unit>
        <trans-unit id="b069cfdd4eba1168499f693bf3d06a42576bfe6a" translate="yes" xml:space="preserve">
          <source>Another set of biclusters like &lt;code&gt;a&lt;/code&gt;.</source>
          <target state="translated">biclustersの別のセットは好きです。 &lt;code&gt;a&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="dbed96b9cee766d1a5bef03a5ad136b7f76de1ff" translate="yes" xml:space="preserve">
          <source>Another significant feature involves whether the sender is affiliated with a university, as indicated either by their headers or their signature.</source>
          <target state="translated">もう一つの重要な特徴は、送信者が大学に所属しているかどうかであり、それはヘッダーまたは署名で示されます。</target>
        </trans-unit>
        <trans-unit id="943b31c92155c4448885494636d7164b8dc15821" translate="yes" xml:space="preserve">
          <source>Another strategy to reduce the variance is by subsampling the features analogous to the random splits in &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; . The number of subsampled features can be controlled via the &lt;code&gt;max_features&lt;/code&gt; parameter.</source>
          <target state="translated">分散を低減するための別の戦略は、ランダム分割に類似した特徴をサブサンプリングすることです&lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; を&lt;/a&gt;。サブサンプリングされた機能の数は、 &lt;code&gt;max_features&lt;/code&gt; パラメーターを介して制御できます。</target>
        </trans-unit>
        <trans-unit id="9bb442d3ca7e5d02384e286ef663eab8b5ac3081" translate="yes" xml:space="preserve">
          <source>Another way to compare the curves is to plot them on top of each other. Here, we create a figure with one row and two columns. The axes are passed into the &lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay.plot&quot;&gt;&lt;code&gt;plot&lt;/code&gt;&lt;/a&gt; function as a list, which will plot the partial dependence curves of each model on the same axes. The length of the axes list must be equal to the number of plots drawn.</source>
          <target state="translated">曲線を比較する別の方法は、それらを互いに重ねてプロットすることです。ここでは、1行2列の図を作成します。軸はリストとして&lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay.plot&quot;&gt; &lt;code&gt;plot&lt;/code&gt; &lt;/a&gt;関数に渡され、同じ軸上に各モデルの部分的な依存曲線がプロットされます。軸リストの長さは、描画されるプロットの数と同じである必要があります。</target>
        </trans-unit>
        <trans-unit id="f7f320421764ca2c9e221a3ad89c0802b9dda00c" translate="yes" xml:space="preserve">
          <source>Another way to reduce memory and computation time is to remove (near-)duplicate points and use &lt;code&gt;sample_weight&lt;/code&gt; instead.</source>
          <target state="translated">メモリと計算時間を削減する別の方法は、（ほぼ）重複するポイントを削除し、代わりに &lt;code&gt;sample_weight&lt;/code&gt; を使用することです。</target>
        </trans-unit>
        <trans-unit id="c89aea8f1a22bc16a7ca2249f6b1aa681a716bbf" translate="yes" xml:space="preserve">
          <source>Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least &lt;code&gt;eps&lt;/code&gt; in distance from any core sample, is considered an outlier by the algorithm.</source>
          <target state="translated">定義上、コアサンプルはすべてクラスタの一部です。コアサンプルではなく、コアサンプルから少なくとも &lt;code&gt;eps&lt;/code&gt; の距離にあるサンプルは、アルゴリズムによって異常値と見なされます。</target>
        </trans-unit>
        <trans-unit id="3a31e829c68185fcae1e17d8f929a1d6c89d47e3" translate="yes" xml:space="preserve">
          <source>Any estimator using the Huber loss would also be robust to outliers, e.g. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;loss='huber'&lt;/code&gt;.</source>
          <target state="translated">Huber損失を使用する推定量は、外れ値に対してもロバストです。たとえば、 &lt;code&gt;loss='huber'&lt;/code&gt; &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 'のSGDRegressorです。</target>
        </trans-unit>
        <trans-unit id="68c9d8de07d4bc7df0a34b129a047212dbcfefbc" translate="yes" xml:space="preserve">
          <source>Any further parameters are passed directly to the distance function. If using a &lt;code&gt;scipy.spatial.distance&lt;/code&gt; metric, the parameters are still metric dependent. See the scipy docs for usage examples.</source>
          <target state="translated">その他のパラメーターは、距離関数に直接渡されます。 &lt;code&gt;scipy.spatial.distance&lt;/code&gt; メトリックを使用している場合でも、パラメーターはメトリックに依存します。使用例については、scipyのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="38f47dd09163cb40bcb092eb016347a6afcf892a" translate="yes" xml:space="preserve">
          <source>Any further parameters are passed directly to the distance function. If using a scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy docs for usage examples.</source>
          <target state="translated">それ以上のパラメータは直接距離関数に渡されます。scipy.spatial.distanceメトリックを使用している場合、パラメータはメトリックに依存します。使用例はscipyのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="3ac4cf049edcb6bb9798310f2f664afd413547f9" translate="yes" xml:space="preserve">
          <source>Any further parameters are passed directly to the kernel function.</source>
          <target state="translated">それ以上のパラメータはカーネル関数に直接渡されます。</target>
        </trans-unit>
        <trans-unit id="02eaff0fa77b6c45a1e6d5d1d919c0643a037a82" translate="yes" xml:space="preserve">
          <source>Any pairwise distance</source>
          <target state="translated">任意のペアワイズ距離</target>
        </trans-unit>
        <trans-unit id="b551d3f373e53d945ef845b2e543fa8abdb837a0" translate="yes" xml:space="preserve">
          <source>Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use:</source>
          <target state="translated">推定量を構築する際に提供されるすべてのパラメータは、この方法で最適化することができます。具体的には、指定された推定量のすべてのパラメータの名前と現在の値を調べるには、以下を使用します。</target>
        </trans-unit>
        <trans-unit id="767d84a7d28245adee41a36e340241d2beefe72a" translate="yes" xml:space="preserve">
          <source>Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, a boolean mask, or with a &lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt;&lt;code&gt;make_column_selector&lt;/code&gt;&lt;/a&gt;. The &lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt;&lt;code&gt;make_column_selector&lt;/code&gt;&lt;/a&gt; is used to select columns based on data type or column name:</source>
          <target state="translated">スカラーまたは単一のアイテムリストとは別に、列の選択は、複数のアイテムのリスト、整数配列、スライス、ブールマスクとして、または&lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt; &lt;code&gt;make_column_selector&lt;/code&gt; を&lt;/a&gt;使用して指定できます。&lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt; &lt;code&gt;make_column_selector&lt;/code&gt; は、&lt;/a&gt;データ型やカラム名に基づいて列を選択するために使用されます。</target>
        </trans-unit>
        <trans-unit id="7c69ec44124fc60e723479a565a556b4cda43066" translate="yes" xml:space="preserve">
          <source>Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, or a boolean mask. Strings can reference columns if the input is a DataFrame, integers are always interpreted as the positional columns.</source>
          <target state="translated">カラムの選択は、スカラーや単一項目のリストの他に、複数項目のリスト、整数配列、スライス、ブーリアンマスクを指定することができます。入力がDataFrameの場合、文字列は列を参照することができますが、整数は常に位置的な列として解釈されます。</target>
        </trans-unit>
        <trans-unit id="4989a2f6df8d0581f9c1d9a7bb746db90f17c597" translate="yes" xml:space="preserve">
          <source>Apple Accelerate and vecLib frameworks (OSX only)</source>
          <target state="translated">Apple AccelerateとvecLibフレームワーク(OSXのみ</target>
        </trans-unit>
        <trans-unit id="a10c2e4fa481f8ae4b6f36b04c8c93e1d4615114" translate="yes" xml:space="preserve">
          <source>Applications to real world problems with some medium sized datasets or interactive user interface.</source>
          <target state="translated">中規模のデータセットやインタラクティブなユーザーインターフェースを用いた実世界の問題への応用。</target>
        </trans-unit>
        <trans-unit id="781951ec1e6932c1ea70532655c1b03656c5caff" translate="yes" xml:space="preserve">
          <source>Applies fit_predict of last step in pipeline after transforms.</source>
          <target state="translated">変換後のパイプラインの最後のステップの fit_predict を適用します。</target>
        </trans-unit>
        <trans-unit id="b55937b5acc0d0ace1d8fc39be387e51d59bfd33" translate="yes" xml:space="preserve">
          <source>Applies fit_transforms of a pipeline to the data, followed by the fit_predict method of the final estimator in the pipeline. Valid only if the final estimator implements fit_predict.</source>
          <target state="translated">パイプラインの fit_transform をデータに適用し、その後にパイプライン内の最終 estimator の fit_predict メソッドを適用します。最終推定子が fit_predict を実装している場合にのみ有効。</target>
        </trans-unit>
        <trans-unit id="e8dcdf95b2627510067599d6b4df4640274e7f9c" translate="yes" xml:space="preserve">
          <source>Applies the learned transformation to the given data.</source>
          <target state="translated">学習した変換を与えられたデータに適用します。</target>
        </trans-unit>
        <trans-unit id="e21a5ab651c86df31d86409fda574ccc4b1b0811" translate="yes" xml:space="preserve">
          <source>Applies transformers to columns of an array or pandas DataFrame.</source>
          <target state="translated">配列やパンダのDataFrameの列に変換器を適用します。</target>
        </trans-unit>
        <trans-unit id="bc1fc2d494d882727c783481b882146100e6cdea" translate="yes" xml:space="preserve">
          <source>Apply Term Frequency Inverse Document Frequency normalization to a sparse matrix of occurrence counts.</source>
          <target state="translated">発生回数の疎な行列に項頻度逆文書頻度正規化を適用します。</target>
        </trans-unit>
        <trans-unit id="48ea2e033ed029c8a6ff54b6ec3dbb5f6c117a68" translate="yes" xml:space="preserve">
          <source>Apply a correction to raw Minimum Covariance Determinant estimates.</source>
          <target state="translated">生の最小共分散決定子推定値に補正を適用します。</target>
        </trans-unit>
        <trans-unit id="3438e96e607439758fb53dafc23a0586474578cd" translate="yes" xml:space="preserve">
          <source>Apply a power transform featurewise to make data more Gaussian-like.</source>
          <target state="translated">データをよりガウシアン的にするために、特徴的なパワー変換を適用します。</target>
        </trans-unit>
        <trans-unit id="2abb9abf1584e2411d2ab4707e053695cc0afad4" translate="yes" xml:space="preserve">
          <source>Apply approximate feature map to X.</source>
          <target state="translated">Xに近似フィーチャーマップを適用します。</target>
        </trans-unit>
        <trans-unit id="72616a249f3c4471d707b9e1210b174fad823e8d" translate="yes" xml:space="preserve">
          <source>Apply clustering to a projection of the normalized Laplacian.</source>
          <target state="translated">正規化されたラプラシアンの投影にクラスタリングを適用します。</target>
        </trans-unit>
        <trans-unit id="b5fc36d5b38e5cc48bf65799eb3ba1f5f5dd4c40" translate="yes" xml:space="preserve">
          <source>Apply clustering to a projection to the normalized laplacian.</source>
          <target state="translated">正規化されたラプラシアンへの投影にクラスタリングを適用します.</target>
        </trans-unit>
        <trans-unit id="8426b1a96ac4159519d28aa77b01e6e7ff1b8a94" translate="yes" xml:space="preserve">
          <source>Apply decision function to an array of samples.</source>
          <target state="translated">サンプルの配列に決定関数を適用します。</target>
        </trans-unit>
        <trans-unit id="ae4427937ffbc660a10ac85e3e1e095b79e74a1a" translate="yes" xml:space="preserve">
          <source>Apply dimensionality reduction to X using the model.</source>
          <target state="translated">モデルを使用してXに次元削減を適用します。</target>
        </trans-unit>
        <trans-unit id="94efd724519284be8c4f7f9c1263433e19686d0b" translate="yes" xml:space="preserve">
          <source>Apply dimensionality reduction to X.</source>
          <target state="translated">Xに次元削減を適用します。</target>
        </trans-unit>
        <trans-unit id="067a30cda0bacc8769ba06b4343f4bb4f1512cf6" translate="yes" xml:space="preserve">
          <source>Apply feature map to X.</source>
          <target state="translated">フィーチャーマップをXに適用します。</target>
        </trans-unit>
        <trans-unit id="019b3cb4563fab68f9b7e6cb2810fdfe13086cd9" translate="yes" xml:space="preserve">
          <source>Apply inverse transformations in reverse order</source>
          <target state="translated">逆変換を逆順に適用する</target>
        </trans-unit>
        <trans-unit id="8ba6ffbae8fd06f4432eaf3f6080e23e84a08a61" translate="yes" xml:space="preserve">
          <source>Apply parallel or deflational algorithm for FastICA.</source>
          <target state="translated">FastICAのための並列またはデフレーションアルゴリズムを適用します。</target>
        </trans-unit>
        <trans-unit id="052f087320408b50bb181caf401c6c09884401a8" translate="yes" xml:space="preserve">
          <source>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</source>
          <target state="translated">サブリニア tf スケーリングを適用します。</target>
        </trans-unit>
        <trans-unit id="e79510482ec62ac16315b7e449f2ba4c8500bc2e" translate="yes" xml:space="preserve">
          <source>Apply the approximate feature map to X.</source>
          <target state="translated">近似特徴マップをXに適用します。</target>
        </trans-unit>
        <trans-unit id="125fbeef808b6029be65b8119e0b6dcb36461fee" translate="yes" xml:space="preserve">
          <source>Apply the dimension reduction learned on the train data.</source>
          <target state="translated">学習した次元削減を訓練データに適用します。</target>
        </trans-unit>
        <trans-unit id="ccff7a762ba43bf734237ef3a3f86ce4e5f76a2a" translate="yes" xml:space="preserve">
          <source>Apply the inverse power transformation using the fitted lambdas.</source>
          <target state="translated">フィットしたラムダを使用して、逆電力変換を適用します。</target>
        </trans-unit>
        <trans-unit id="0977868498dbf8902c5244b7ef40fc62ab65dab8" translate="yes" xml:space="preserve">
          <source>Apply the power transform to each feature using the fitted lambdas.</source>
          <target state="translated">フィットしたラムダを使用して、各特徴量に電力変換を適用します。</target>
        </trans-unit>
        <trans-unit id="82b34785a3e300efc01281606b13c116a0d93f34" translate="yes" xml:space="preserve">
          <source>Apply transforms to the data, and predict with the final estimator</source>
          <target state="translated">データに変換を適用し、最終推定器で予測する</target>
        </trans-unit>
        <trans-unit id="ca1ed4363821df5fce03d98ccc01802e4bf47ad5" translate="yes" xml:space="preserve">
          <source>Apply transforms, and decision_function of the final estimator</source>
          <target state="translated">トランスフォームを適用し、最終推定器の決定関数を決定する。</target>
        </trans-unit>
        <trans-unit id="2f4f9e2645ee8d058965cbdacfaf2a7eb9788d03" translate="yes" xml:space="preserve">
          <source>Apply transforms, and predict_log_proba of the final estimator</source>
          <target state="translated">トランスフォームを適用し、最終推定器のpredict_log_probaを適用する</target>
        </trans-unit>
        <trans-unit id="476ce75d7df4464fc2850d9c9869c6985e6ab19f" translate="yes" xml:space="preserve">
          <source>Apply transforms, and predict_proba of the final estimator</source>
          <target state="translated">トランスフォームを適用し、最終的な推定器のpredict_proba</target>
        </trans-unit>
        <trans-unit id="4521c8878b7f577e2b39e24b7f82d01d4c53bdb4" translate="yes" xml:space="preserve">
          <source>Apply transforms, and score with the final estimator</source>
          <target state="translated">変換を適用し、最終推定器でスコアをつける</target>
        </trans-unit>
        <trans-unit id="d02b9d52a2cce495b36832a6628a7547a7253b34" translate="yes" xml:space="preserve">
          <source>Apply transforms, and score_samples of the final estimator.</source>
          <target state="translated">トランスフォームを適用し、最終推定器のscore_samplesを適用します。</target>
        </trans-unit>
        <trans-unit id="ef451a675f7839fb36e9b71a5c79c50af524605d" translate="yes" xml:space="preserve">
          <source>Apply transforms, and transform with the final estimator</source>
          <target state="translated">変換を適用し、最終推定器で変換する</target>
        </trans-unit>
        <trans-unit id="de81ef7ceb9f90b36294e9caf98f17d7f5fb716c" translate="yes" xml:space="preserve">
          <source>Apply trees in the ensemble to X, return leaf indices.</source>
          <target state="translated">アンサンブル内の木をXに適用し、葉のインデックスを返します。</target>
        </trans-unit>
        <trans-unit id="46e9e1565b766b82cfa48c8669ac20be4ae1efca" translate="yes" xml:space="preserve">
          <source>Apply trees in the forest to X, return leaf indices.</source>
          <target state="translated">森の木をXに適用し、葉のインデックスを返します。</target>
        </trans-unit>
        <trans-unit id="c1f7ec5473437cc6f706dcde780cf75354c5c9db" translate="yes" xml:space="preserve">
          <source>Approximate a kernel map using a subset of the training data.</source>
          <target state="translated">学習データのサブセットを用いてカーネルマップを近似します。</target>
        </trans-unit>
        <trans-unit id="99c9230e6f99f216d46b329fe823b6165b309c69" translate="yes" xml:space="preserve">
          <source>Approximate feature map for additive chi2 kernel.</source>
          <target state="translated">加法カイ2カーネルの近似特徴量マップ。</target>
        </trans-unit>
        <trans-unit id="08c300a2e35faf622b6d14b8c7a89fd4c1146cc4" translate="yes" xml:space="preserve">
          <source>Approximate nearest neighbors in TSNE</source>
          <target state="translated">TSNEの近似最寄の隣人</target>
        </trans-unit>
        <trans-unit id="79a233ba78739efc19c54579d317a0c2897cb08a" translate="yes" xml:space="preserve">
          <source>Approximated breakdown point.</source>
          <target state="translated">おおよそのブレークダウンポイント。</target>
        </trans-unit>
        <trans-unit id="80c9ab0b0026778753b68f0b79aed7c300d9ec18" translate="yes" xml:space="preserve">
          <source>Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.</source>
          <target state="translated">RBFカーネルのフーリエ変換をモンテカルロ近似することで、RBFカーネルの特徴量マップを近似する。</target>
        </trans-unit>
        <trans-unit id="c5cd123a52fffa6abe1c128079596542a14c93e1" translate="yes" xml:space="preserve">
          <source>Approximates feature map of the &amp;ldquo;skewed chi-squared&amp;rdquo; kernel by Monte Carlo approximation of its Fourier transform.</source>
          <target state="translated">フーリエ変換のモンテカルロ近似により、「歪んだカイ2乗」カーネルの特徴マップを近似します。</target>
        </trans-unit>
        <trans-unit id="0747da454303400f203ff73991292f2e6b1d8099" translate="yes" xml:space="preserve">
          <source>Approximations to the Likelihood Gradient. International Conference on Machine Learning (ICML) 2008</source>
          <target state="translated">尤度勾配の近似.機械学習国際会議 (ICML)2008</target>
        </trans-unit>
        <trans-unit id="3121863aa17e74b4053a8b1b5b5e368203cc9eae" translate="yes" xml:space="preserve">
          <source>Are computed such that:</source>
          <target state="translated">のように計算されます。</target>
        </trans-unit>
        <trans-unit id="2745debaa64a20eedb49d9f14a0b807c87aa2d2a" translate="yes" xml:space="preserve">
          <source>Area</source>
          <target state="translated">Area</target>
        </trans-unit>
        <trans-unit id="40f1ed31aa37ba1df6f1e12267ca0106d885adfa" translate="yes" xml:space="preserve">
          <source>Area under ROC curve. If None, the roc_auc score is not shown.</source>
          <target state="translated">ROC曲線の下の面積。Noneの場合、roc_aucスコアは表示されません。</target>
        </trans-unit>
        <trans-unit id="ef8721938ea54cef3b6761a4de1893a7d8b789aa" translate="yes" xml:space="preserve">
          <source>Area under ROC for the multiclass problem</source>
          <target state="translated">マルチクラス問題のROC下面積</target>
        </trans-unit>
        <trans-unit id="9981f730845b36824e5458276d9c866bb62f0b81" translate="yes" xml:space="preserve">
          <source>Area under the precision-recall curve</source>
          <target state="translated">精度-リコール曲線の下の面積</target>
        </trans-unit>
        <trans-unit id="0d216eb7e93b45a2be7855da027249526cd9509f" translate="yes" xml:space="preserve">
          <source>Argument to the kernel.</source>
          <target state="translated">カーネルへの引数。</target>
        </trans-unit>
        <trans-unit id="5e08f171a6f1a9519e5708f61bf31cc48538834b" translate="yes" xml:space="preserve">
          <source>Arguments to send to the functional form. If empty and if fun=&amp;rsquo;logcosh&amp;rsquo;, fun_args will take value {&amp;lsquo;alpha&amp;rsquo; : 1.0}.</source>
          <target state="translated">関数形式に送信する引数。空で、fun = 'logcosh'の場合、fun_argsの値は{'alpha'：1.0}になります。</target>
        </trans-unit>
        <trans-unit id="c45228c98edaaa951ea722f923e4cffb69731e7a" translate="yes" xml:space="preserve">
          <source>Ariel Sharon</source>
          <target state="translated">アリエル・シャロン</target>
        </trans-unit>
        <trans-unit id="11f09cc06ef4f6b2c8293917d6e2b9c293adc607" translate="yes" xml:space="preserve">
          <source>Array 1 for distance computation.</source>
          <target state="translated">距離計算用の配列1。</target>
        </trans-unit>
        <trans-unit id="2b2aa765e7ec61bcc3223e5c31148344ebd52c37" translate="yes" xml:space="preserve">
          <source>Array 2 for distance computation.</source>
          <target state="translated">距離計算用の配列2。</target>
        </trans-unit>
        <trans-unit id="8a65824231356d07c95aeae6f87001027ea0bef6" translate="yes" xml:space="preserve">
          <source>Array containing labels.</source>
          <target state="translated">ラベルを含む配列。</target>
        </trans-unit>
        <trans-unit id="e6449394cdaf60dea5ac71fdd09a0ed9e9fcf7ee" translate="yes" xml:space="preserve">
          <source>Array containing numbers whose mean is desired. If &lt;code&gt;a&lt;/code&gt; is not an array, a conversion is attempted.</source>
          <target state="translated">平均が必要な数値を含む配列。 &lt;code&gt;a&lt;/code&gt; が配列でない場合、変換が試行されます。</target>
        </trans-unit>
        <trans-unit id="c018e704aa26bba356828629d984b5289beaf058" translate="yes" xml:space="preserve">
          <source>Array containing pairwise preference constraints (qid in svmlight format).</source>
          <target state="translated">ペアワイズプリファレンス制約を含む配列 (svmlight 形式の qid)。</target>
        </trans-unit>
        <trans-unit id="91335d05435a126405d2a8a45dca985aad48295a" translate="yes" xml:space="preserve">
          <source>Array containing points.</source>
          <target state="translated">点を含む配列。</target>
        </trans-unit>
        <trans-unit id="6f0a003e95ad4e5813fec46743d334c40fd183c4" translate="yes" xml:space="preserve">
          <source>Array dimensions of training vector &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">トレーニングベクトル &lt;code&gt;X&lt;/code&gt; の配列次元。</target>
        </trans-unit>
        <trans-unit id="e25bba3f8d7bf2293b0ad16bc13aa9e306ca9914" translate="yes" xml:space="preserve">
          <source>Array mapping from feature integer indices to feature name</source>
          <target state="translated">特徴の整数インデックスから特徴名への配列マッピング</target>
        </trans-unit>
        <trans-unit id="ea79422025037006dbb9fa56f6952fdafa7b797f" translate="yes" xml:space="preserve">
          <source>Array mapping from feature integer indices to feature name.</source>
          <target state="translated">特徴の整数インデックスから特徴名へのマッピングの配列。</target>
        </trans-unit>
        <trans-unit id="70e68dbf5038082528f259bb78c1b52974cdd1b7" translate="yes" xml:space="preserve">
          <source>Array of C i.e. inverse of regularization parameter values used for cross-validation.</source>
          <target state="translated">クロスバリデーションに使用される正則化パラメータ値の逆数であるCの配列。</target>
        </trans-unit>
        <trans-unit id="f23db680f590249d8c9bf389de4a77cfbca412ab" translate="yes" xml:space="preserve">
          <source>Array of C that maps to the best scores across every class. If refit is set to False, then for each class, the best C is the average of the C&amp;rsquo;s that correspond to the best scores for each fold. &lt;code&gt;C_&lt;/code&gt; is of shape(n_classes,) when the problem is binary.</source>
          <target state="translated">すべてのクラスで最高のスコアにマップするCの配列。refitがFalseに設定されている場合、各クラスのベストCは、各フォールドのベストスコアに対応するCの平均です。問題がバイナリの場合、 &lt;code&gt;C_&lt;/code&gt; はshape（n_classes、）です。</target>
        </trans-unit>
        <trans-unit id="a55c3729a06f5322394066f1c0c8cd702750ec36" translate="yes" xml:space="preserve">
          <source>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to &lt;code&gt;1 / (2C)&lt;/code&gt; in other linear models such as &lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">試すアルファ値の配列。正則化の強さ;正のフロートである必要があります。正則化は、問題の条件付けを改善し、推定値の分散を減らします。値が大きいほど、正則化が強くなります。アルファは、&lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;や&lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt;などの他の線形モデルの &lt;code&gt;1 / (2C)&lt;/code&gt; に対応します。</target>
        </trans-unit>
        <trans-unit id="ee7705f66f6136357dc8015abf939ac9ba5f0a41" translate="yes" xml:space="preserve">
          <source>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to &lt;code&gt;1 / (2C)&lt;/code&gt; in other linear models such as &lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt;. If using generalized cross-validation, alphas must be positive.</source>
          <target state="translated">試すアルファ値の配列。正則化の強さ; 正のフロートである必要があります。正則化は、問題の条件付けを改善し、推定値の分散を減らします。値が大きいほど、正則化が強くなります。アルファは、&lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;や&lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt;などの他の線形モデルの &lt;code&gt;1 / (2C)&lt;/code&gt; に対応します。一般化された交差検定を使用する場合、アルファは正でなければなりません。</target>
        </trans-unit>
        <trans-unit id="1bcf7c58e5cd52bc0017cb778f5c70ae9fb7f4c3" translate="yes" xml:space="preserve">
          <source>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to &lt;code&gt;C^-1&lt;/code&gt; in other linear models such as LogisticRegression or LinearSVC.</source>
          <target state="translated">試すアルファ値の配列。正則化強度; 正のフロートでなければなりません。正則化は問題の条件付けを改善し、推定値の分散を減らします。値が大きいほど、正則化が強くなります。Alphaは、LogisticRegressionやLinearSVCなどの他の線形モデルの &lt;code&gt;C^-1&lt;/code&gt; に対応します。</target>
        </trans-unit>
        <trans-unit id="56fbe430ec1266429078a0ab631106976ff42e91" translate="yes" xml:space="preserve">
          <source>Array of feature names.</source>
          <target state="translated">フィーチャー名の配列。</target>
        </trans-unit>
        <trans-unit id="f66e9a562c0b4e0d27e475880657f80a6deb9514" translate="yes" xml:space="preserve">
          <source>Array of feature-wise means to update with the new data X.</source>
          <target state="translated">新しいデータXで更新する機能別手段の配列。</target>
        </trans-unit>
        <trans-unit id="f6c98b4462671d9e778e0d714eb0f164a1614fcd" translate="yes" xml:space="preserve">
          <source>Array of feature-wise var to update with the new data X.</source>
          <target state="translated">新しいデータ X で更新する機能別 var の配列。</target>
        </trans-unit>
        <trans-unit id="5fcd28abe38a5e4606c96fde73b0772080b7a651" translate="yes" xml:space="preserve">
          <source>Array of images from which to extract patches. For color images, the last dimension specifies the channel: a RGB image would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="translated">パッチを抽出する画像の配列。カラー画像の場合、最後の次元でチャネルを指定します。RGB画像の &lt;code&gt;n_channels=3&lt;/code&gt; になります。</target>
        </trans-unit>
        <trans-unit id="6f81a8f931ec6cc551fbd84a217b7d3ae7ae6320" translate="yes" xml:space="preserve">
          <source>Array of indices to be used in a subsample. Can be of length less than n_samples in the case of a subsample, or equal to n_samples in the case of a bootstrap subsample with repeated indices. If None, the sample weight will be calculated over the full sample. Only &amp;ldquo;balanced&amp;rdquo; is supported for class_weight if this is provided.</source>
          <target state="translated">サブサンプルで使用されるインデックスの配列。サブサンプルの場合はn_samples未満の長さ、インデックスが繰り返されるブートストラップサブサンプルの場合はn_samplesと同じ長さになります。Noneの場合、サンプルの重量はサンプル全体に対して計算されます。class_weightが指定されている場合、「バランス」のみがサポートされます。</target>
        </trans-unit>
        <trans-unit id="68493ef46dcb2bca2edbdc205fc9d64a4faedf4e" translate="yes" xml:space="preserve">
          <source>Array of l1_ratio that maps to the best scores across every class. If refit is set to False, then for each class, the best l1_ratio is the average of the l1_ratio&amp;rsquo;s that correspond to the best scores for each fold. &lt;code&gt;l1_ratio_&lt;/code&gt; is of shape(n_classes,) when the problem is binary.</source>
          <target state="translated">すべてのクラスで最高のスコアにマップされるl1_ratioの配列。refitがFalseに設定されている場合、各クラスの最高のl1_ratioは、各フォールドの最高のスコアに対応するl1_ratioの平均です。 &lt;code&gt;l1_ratio_&lt;/code&gt; は問題がバイナリである場合（n_classes）形状です。</target>
        </trans-unit>
        <trans-unit id="fac2540408435b364a56995dfc908b1ce3b3f1b5" translate="yes" xml:space="preserve">
          <source>Array of l1_ratios used for cross-validation. If no l1_ratio is used (i.e. penalty is not &amp;lsquo;elasticnet&amp;rsquo;), this is set to &lt;code&gt;[None]&lt;/code&gt;</source>
          <target state="translated">相互検証に使用されるl1_ratiosの配列。l1_ratioが使用されていない場合（つまり、ペナルティが「elasticnet」ではない場合）、これは &lt;code&gt;[None]&lt;/code&gt; 設定されます。</target>
        </trans-unit>
        <trans-unit id="eeab30110685755d1d6fcfd1bdec963e11fd2b40" translate="yes" xml:space="preserve">
          <source>Array of labels assigned to the input data. if partial_fit is used instead of fit, they are assigned to the last batch of data.</source>
          <target state="translated">入力デー タ に割 り 当て ら れた ラ ベルの配列。</target>
        </trans-unit>
        <trans-unit id="7713344316e6072a30843b91363d87cccda00c35" translate="yes" xml:space="preserve">
          <source>Array of matplotlib axes. &lt;code&gt;None&lt;/code&gt; if &lt;code&gt;include_values&lt;/code&gt; is false.</source>
          <target state="translated">matplotlib軸の配列。 &lt;code&gt;include_values&lt;/code&gt; がfalseの場合は &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d4370a66f6bded85adaf7548951f4179c0716e58" translate="yes" xml:space="preserve">
          <source>Array of modal values.</source>
          <target state="translated">モーダル値の配列。</target>
        </trans-unit>
        <trans-unit id="1fd74761ce4c1dfdbf88f10700afe7876dc7373c" translate="yes" xml:space="preserve">
          <source>Array of ordered feature names used in the dataset.</source>
          <target state="translated">データセットで使用される順序付き特徴名の配列.</target>
        </trans-unit>
        <trans-unit id="0f885b177d721fef81c6f0296028811159f9ce61" translate="yes" xml:space="preserve">
          <source>Array of original class labels per sample.</source>
          <target state="translated">サンプルごとの元のクラスラベルの配列。</target>
        </trans-unit>
        <trans-unit id="7800a77bf571d12be284509e1c1715411139d56e" translate="yes" xml:space="preserve">
          <source>Array of original class labels per sample;</source>
          <target state="translated">サンプルごとの元のクラスラベルの配列。</target>
        </trans-unit>
        <trans-unit id="96778e504b9d48e677b9fd575a4cafc7576e9a4e" translate="yes" xml:space="preserve">
          <source>Array of pairwise distances between samples, or a feature array.</source>
          <target state="translated">サンプル間のペアワイズ距離の配列,または特徴量の配列.</target>
        </trans-unit>
        <trans-unit id="e091a26de25f0600efda87ce52273e245aa786c4" translate="yes" xml:space="preserve">
          <source>Array of pairwise kernels between samples, or a feature array.</source>
          <target state="translated">サンプル間のペアワイズカーネルの配列,または特徴量の配列.</target>
        </trans-unit>
        <trans-unit id="7ad46daa8cb15b6370dc0bc3d5a37d6d0d152fc8" translate="yes" xml:space="preserve">
          <source>Array of positive distances. If vertex i is connected to vertex j, then dist_matrix[i,j] gives the distance between the vertices. If vertex i is not connected to vertex j, then dist_matrix[i,j] = 0</source>
          <target state="translated">正の距離の配列。頂点iが頂点jに接続されている場合、dist_matrix[i,j]は頂点間の距離を与える。頂点iが頂点jに接続されていない場合、dist_matrix[i,j]=0.</target>
        </trans-unit>
        <trans-unit id="4e5f1b979067f81092a1e7cd9ffe302fc627e739" translate="yes" xml:space="preserve">
          <source>Array of precomputed feature-wise values to use for scaling.</source>
          <target state="translated">スケーリングに使用するために事前に計算された特徴量の配列。</target>
        </trans-unit>
        <trans-unit id="6812a29eee4afd325221734d77b9a4dae2cd6936" translate="yes" xml:space="preserve">
          <source>Array of precomputed sample-wise values to use for scaling.</source>
          <target state="translated">スケーリングに使用するために事前に計算されたサンプル単位の値の配列。</target>
        </trans-unit>
        <trans-unit id="fdeed78489293ac715c2ab2eb4b0404b2b3869aa" translate="yes" xml:space="preserve">
          <source>Array of samples (test vectors).</source>
          <target state="translated">サンプルの配列(テストベクター)。</target>
        </trans-unit>
        <trans-unit id="24f920c86b265a90a5e0d4a36297a04667e3e9df" translate="yes" xml:space="preserve">
          <source>Array of samples/test vectors.</source>
          <target state="translated">サンプル/テストベクターの配列。</target>
        </trans-unit>
        <trans-unit id="3fbc39a14b9b4da5186fad432e1588423be425d4" translate="yes" xml:space="preserve">
          <source>Array of scores of the estimator for each run of the cross validation.</source>
          <target state="translated">クロスバリデーションの各実行のための推定子のスコアの配列。</target>
        </trans-unit>
        <trans-unit id="45ad4c3a294e4989b4c11ac5e758294b0026c988" translate="yes" xml:space="preserve">
          <source>Array of shape (Nx, D), representing Nx points in D dimensions.</source>
          <target state="translated">形状 (Nx,D)の配列で、D 次元の Nx 点を表します。</target>
        </trans-unit>
        <trans-unit id="fc8b99cd3247136d0c5bc8738933867c6b7310fe" translate="yes" xml:space="preserve">
          <source>Array of shape (Ny, D), representing Ny points in D dimensions. If not specified, then Y=X.</source>
          <target state="translated">形状(Ny,D)の配列で、D次元のNy点を表す。指定されていない場合は Y=X となります。</target>
        </trans-unit>
        <trans-unit id="250d74905474bfb993e7135f2cc0cab46c1b40f6" translate="yes" xml:space="preserve">
          <source>Array of the classes occurring in the data, as given by &lt;code&gt;np.unique(y_org)&lt;/code&gt; with &lt;code&gt;y_org&lt;/code&gt; the original class labels.</source>
          <target state="translated">元のクラスラベルが &lt;code&gt;y_org&lt;/code&gt; の &lt;code&gt;np.unique(y_org)&lt;/code&gt; で指定された、データ内に存在するクラスの配列。</target>
        </trans-unit>
        <trans-unit id="b78e425c6e49838f94af51fc89b925b7dde837d3" translate="yes" xml:space="preserve">
          <source>Array of weighted counts for each mode.</source>
          <target state="translated">各モードの重み付けされたカウントの配列。</target>
        </trans-unit>
        <trans-unit id="94dd9c0571a23f9d9a0e09a5e6b6a6f23711b3ea" translate="yes" xml:space="preserve">
          <source>Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.</source>
          <target state="translated">個々のサンプルに割り当てられる重みの配列。指定されていない場合は、各サンプルに単位加重が与えられます。</target>
        </trans-unit>
        <trans-unit id="2d23bb743e4774802fe069cba46f38744d42d230" translate="yes" xml:space="preserve">
          <source>Array representing the cosine distances to each point, only present if return_distance=True.</source>
          <target state="translated">各点までの余弦距離を表す配列で、return_distance=Trueの場合のみ存在します。</target>
        </trans-unit>
        <trans-unit id="a5f32f9aefd1234d15dbae57ece52e2602c42baf" translate="yes" xml:space="preserve">
          <source>Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the &lt;code&gt;metric&lt;/code&gt; constructor parameter.</source>
          <target state="translated">各ポイントまでの距離を表す配列。return_distance= Trueの場合にのみ存在します。距離値は、 &lt;code&gt;metric&lt;/code&gt; コンストラクターパラメーターに従って計算されます。</target>
        </trans-unit>
        <trans-unit id="dadd9836b98c2a8bfba00660688e29f8956840ae" translate="yes" xml:space="preserve">
          <source>Array representing the lengths to points, only present if return_distance=True</source>
          <target state="translated">点までの長さを表す配列で、return_distance=Trueの場合のみ存在します。</target>
        </trans-unit>
        <trans-unit id="128b0965caa89fb141c0f27357f3b344c8ae14e0" translate="yes" xml:space="preserve">
          <source>Array with class_weight_vect[i] the weight for i-th class</source>
          <target state="translated">class_weight_vect[i]の配列で、i番目のクラスの重みを表す。</target>
        </trans-unit>
        <trans-unit id="8071ea6e98367794e2540e75731701074b3502c7" translate="yes" xml:space="preserve">
          <source>Array with sample weights as applied to the original y</source>
          <target state="translated">元の y に適用されたサンプル重みの配列</target>
        </trans-unit>
        <trans-unit id="454c01b5a8f0e04bcddbd63e1a11312ba378a0a5" translate="yes" xml:space="preserve">
          <source>Arrays containing points.</source>
          <target state="translated">点を含む配列。</target>
        </trans-unit>
        <trans-unit id="55e1c09be8a72c22c42432b49cce52b8593ad9f3" translate="yes" xml:space="preserve">
          <source>Arrays containing points. Respective shapes (n_samples1, n_features) and (n_samples2, n_features)</source>
          <target state="translated">点を含む配列.それぞれの形状(n_samples1,n_features)と(n_samples2,n_features)</target>
        </trans-unit>
        <trans-unit id="617153f408d9e36a02ce1e1b9ef6e01cb4b4ace4" translate="yes" xml:space="preserve">
          <source>Arrays for storing tree data, index, node data and node bounds.</source>
          <target state="translated">ツリーデータ、インデックス、ノードデータ、ノード境界を格納するための配列。</target>
        </trans-unit>
        <trans-unit id="3c2af9f8f5cb14e6db46f51362b9496e9213d67c" translate="yes" xml:space="preserve">
          <source>Art B. Owen (2006), A robust hybrid of lasso and ridge regression. &lt;a href=&quot;http://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;http://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</source>
          <target state="translated">Art B. Owen（2006）、なげなわと尾根回帰の堅牢なハイブリッド。&lt;a href=&quot;http://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;http://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c8d50e29fabbeb579c5217b8bfc0480f3d25842" translate="yes" xml:space="preserve">
          <source>Art B. Owen (2006), A robust hybrid of lasso and ridge regression. &lt;a href=&quot;https://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;https://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</source>
          <target state="translated">Art B. Owen（2006）、ラッソ回帰とリッジ回帰の堅牢なハイブリッド。&lt;a href=&quot;https://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;https://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="280c547a1f162abeda960694ff094a09bc3e0f0d" translate="yes" xml:space="preserve">
          <source>As &lt;code&gt;RobustScaler&lt;/code&gt;, &lt;code&gt;QuantileTransformer&lt;/code&gt; is robust to outliers in the sense that adding or removing outliers in the training set will yield approximately the same transformation on held out data. But contrary to &lt;code&gt;RobustScaler&lt;/code&gt;, &lt;code&gt;QuantileTransformer&lt;/code&gt; will also automatically collapse any outlier by setting them to the a priori defined range boundaries (0 and 1).</source>
          <target state="translated">&lt;code&gt;RobustScaler&lt;/code&gt; 、 &lt;code&gt;QuantileTransformer&lt;/code&gt; は、トレーニング・セット内の外れ値を追加または削除するには、約差し出したデータに同じ変換をもたらすという意味で、外れ値に対してロバストです。しかしに反し &lt;code&gt;RobustScaler&lt;/code&gt; 、 &lt;code&gt;QuantileTransformer&lt;/code&gt; はまた、自動的に先験的に定義された範囲の境界（0と1）にそれらを設定することで、任意の外れ値を崩壊します。</target>
        </trans-unit>
        <trans-unit id="4f9342a94a131883760ca784477b6aa2e1e17246" translate="yes" xml:space="preserve">
          <source>As &lt;code&gt;StandardScaler&lt;/code&gt;, &lt;code&gt;MinMaxScaler&lt;/code&gt; is very sensitive to the presence of outliers.</source>
          <target state="translated">&lt;code&gt;StandardScaler&lt;/code&gt; として、 &lt;code&gt;MinMaxScaler&lt;/code&gt; は外れ値の存在に非常に敏感です。</target>
        </trans-unit>
        <trans-unit id="0473784855cee607ac3c3e8942e446aa86c45018" translate="yes" xml:space="preserve">
          <source>As &lt;code&gt;leaf_size&lt;/code&gt; increases, the memory required to store a tree structure decreases. This is especially important in the case of ball tree, which stores a \(D\)-dimensional centroid for each node. The required storage space for &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; is approximately &lt;code&gt;1 / leaf_size&lt;/code&gt; times the size of the training set.</source>
          <target state="translated">&lt;code&gt;leaf_size&lt;/code&gt; 増加、メモリは、ツリー構造の減少を格納するために必要。これは、各ノードの\（D \）次元の重心を格納するボールツリーの場合に特に重要です。&lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; に&lt;/a&gt;必要なストレージ容量は、トレーニングセットのサイズの約 &lt;code&gt;1 / leaf_size&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="5ba99528590cd9030f3ddfaa7772e7d00f041704" translate="yes" xml:space="preserve">
          <source>As F-test captures only linear dependency, it rates x_1 as the most discriminative feature. On the other hand, mutual information can capture any kind of dependency between variables and it rates x_2 as the most discriminative feature, which probably agrees better with our intuitive perception for this example. Both methods correctly marks x_3 as irrelevant.</source>
          <target state="translated">F-testが線形依存関係のみを捉えるので、x_1を最も識別可能な特徴として評価します。一方、相互情報は、変数間のあらゆる種類の依存関係をキャプチャすることができ、x_2を最も識別的な特徴として評価しますが、これはおそらく、この例についての我々の直感的な認識とよりよく一致します。どちらの方法もx_3を無関係であると正しくマークします。</target>
        </trans-unit>
        <trans-unit id="d26c8ba867b91d66c4620ebdebaef0da4c712fd2" translate="yes" xml:space="preserve">
          <source>As \(\nu\rightarrow\infty\), the Mat&amp;eacute;rn kernel converges to the RBF kernel. When \(\nu = 1/2\), the Mat&amp;eacute;rn kernel becomes identical to the absolute exponential kernel, i.e.,</source>
          <target state="translated">\（\ nu \ rightarrow \ infty \）として、Mat&amp;eacute;rnカーネルはRBFカーネルに収束します。\（\ nu = 1/2 \）の場合、Mat&amp;eacute;rnカーネルは絶対指数カーネルと同一になります。つまり、</target>
        </trans-unit>
        <trans-unit id="fd1774dd2550566b72e9349a744134b65f1d5b37" translate="yes" xml:space="preserve">
          <source>As \(k\) becomes large compared to \(N\), the ability to prune branches in a tree-based query is reduced. In this situation, Brute force queries can be more efficient.</source>
          <target state="translated">As As \(k\)becomes large compared to \(N\),as the ability to prune branches in a tree-based query is reduced.このような状況では、ブルートフォースクエリの方が効率的です。</target>
        </trans-unit>
        <trans-unit id="faf1e694d41950e191ce698208593222026d0616" translate="yes" xml:space="preserve">
          <source>As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO.</source>
          <target state="translated">一般的なルールとして、ほとんどの著者と経験的証拠は、5倍または10倍のクロスバリデーションはLOOよりも優先されるべきであることを示唆しています。</target>
        </trans-unit>
        <trans-unit id="7511794679052faea86272ecd7141a948a5ee019" translate="yes" xml:space="preserve">
          <source>As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipy&amp;rsquo;s sparse matrix formats &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;documentation&lt;/a&gt; for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the &lt;code&gt;CSR&lt;/code&gt; and &lt;code&gt;CSC&lt;/code&gt; formats work best.</source>
          <target state="translated">経験則として、スパース比が90％を超えている場合は、おそらくスパースフォーマットの恩恵を受けることができると考えることができます。スパースマトリックス形式を構築（またはデータに変換）する方法の詳細については、Scipyのスパースマトリックス形式の&lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;ドキュメント&lt;/a&gt;を確認してください。ほとんどの場合、 &lt;code&gt;CSR&lt;/code&gt; および &lt;code&gt;CSC&lt;/code&gt; 形式が最適です。</target>
        </trans-unit>
        <trans-unit id="e6d544b03b9c4badced861e6939e6f09869b8c86" translate="yes" xml:space="preserve">
          <source>As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipy&amp;rsquo;s sparse matrix formats &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;documentation&lt;/a&gt; for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the &lt;code&gt;CSR&lt;/code&gt; and &lt;code&gt;CSC&lt;/code&gt; formats work best.</source>
          <target state="translated">経験則として、スパース率が90％を超える場合は、スパース形式の恩恵を受ける可能性があると考えることができます。スパース行列形式を構築（またはデータを変換）する方法の詳細については、Scipyのスパース行列形式の&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;ドキュメント&lt;/a&gt;を確認してください。ほとんどの場合、 &lt;code&gt;CSR&lt;/code&gt; および &lt;code&gt;CSC&lt;/code&gt; 形式が最適に機能します。</target>
        </trans-unit>
        <trans-unit id="353bdb2ad3c8962f2cd7a3693251833639b2a66a" translate="yes" xml:space="preserve">
          <source>As a stochastic method, the loss function is not necessarily decreasing at each iteration, and convergence is only guaranteed in expectation. For this reason, monitoring the convergence on the loss function can be difficult.</source>
          <target state="translated">確率論的手法であるため、損失関数は反復ごとに必ずしも減少するわけではなく、収束は期待値でしか保証されていない。このため、損失関数の収束を監視することは困難である。</target>
        </trans-unit>
        <trans-unit id="42def8933b75f8dc81489a17ca6bf2a4d3b1aab6" translate="yes" xml:space="preserve">
          <source>As a user, you may control the backend that joblib will use (regardless of what scikit-learn recommends) by using a context manager:</source>
          <target state="translated">ユーザとしては、コンテキストマネージャを使用して(scikit-learnが推奨するものとは関係なく)joblibが使用するバックエンドを制御することができます。</target>
        </trans-unit>
        <trans-unit id="c5c1051c685f7a17c464794391d76e887c975059" translate="yes" xml:space="preserve">
          <source>As alpha tends toward zero the coefficients found by Ridge regression stabilize towards the randomly sampled vector w. For big alpha (strong regularisation) the coefficients are smaller (eventually converging at 0) leading to a simpler and biased solution. These dependencies can be observed on the left plot.</source>
          <target state="translated">アルファがゼロに近づくにつれて、リッジ回帰によって発見された係数は、ランダムにサンプリングされたベクトルwに向かって安定します。大きなアルファ(強い正則化)では、係数は小さくなり(最終的には0に収束)、より単純で偏った解になります。これらの依存性は、左のプロットで観察できます。</target>
        </trans-unit>
        <trans-unit id="24f37f11c718f612ad3749aae9a0a774bc0cfd6c" translate="yes" xml:space="preserve">
          <source>As an alternative, the permutation importances of &lt;code&gt;rf&lt;/code&gt; are computed on a held out test set. This shows that the low cardinality categorical feature, &lt;code&gt;sex&lt;/code&gt; is the most important feature.</source>
          <target state="translated">別の方法として、 &lt;code&gt;rf&lt;/code&gt; の順列の重要度は、差し出されたテストセットで計算されます。これは、カーディナリティの低いカテゴリ機能である &lt;code&gt;sex&lt;/code&gt; が最も重要な機能であることを示しています。</target>
        </trans-unit>
        <trans-unit id="3c04824c2f2c674faf4d7d29dd0f1d9a774a897e" translate="yes" xml:space="preserve">
          <source>As an example, consider a word-level natural language processing task that needs features extracted from &lt;code&gt;(token, part_of_speech)&lt;/code&gt; pairs. One could use a Python generator function to extract features:</source>
          <target state="translated">例として、 &lt;code&gt;(token, part_of_speech)&lt;/code&gt; ペアから抽出された特徴を必要とする単語レベルの自然言語処理タスクを考えます。Pythonジェネレーター関数を使用して特徴を抽出できます。</target>
        </trans-unit>
        <trans-unit id="440e47d9bdf54f0d76b208827e3f8a7246ce9187" translate="yes" xml:space="preserve">
          <source>As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by</source>
          <target state="translated">例として、ブーリアン特徴量を持つデータセットがあり、サンプルの80%以上で1またはゼロ(オンまたはオフ)の特徴量をすべて削除したいとします。ブール型特徴量はベルヌーイのランダム変数であり,そのような変数の分散は</target>
        </trans-unit>
        <trans-unit id="0fc331775bcbccb582b3c6648f3387e8f638bc0a" translate="yes" xml:space="preserve">
          <source>As an iterable of string metrics::</source>
          <target state="translated">文字列メトリクスのイテレータブルとして:。</target>
        </trans-unit>
        <trans-unit id="670a68b39de7d2dc4b153165f015b7c7235ba10c" translate="yes" xml:space="preserve">
          <source>As an optimization problem, binary class L2 penalized logistic regression minimizes the following cost function:</source>
          <target state="translated">最適化問題として、2値クラスL2のペナルティ付きロジスティック回帰は、以下のコスト関数を最小化する。</target>
        </trans-unit>
        <trans-unit id="f278c77eb641c2fca3187e677f3037bfd6ce972e" translate="yes" xml:space="preserve">
          <source>As an optimization problem, binary class \(\ell_2\) penalized logistic regression minimizes the following cost function:</source>
          <target state="translated">最適化問題として、2値クラス(binary class ✿)の罰則付きロジスティック回帰は、以下のコスト関数を最小化する。</target>
        </trans-unit>
        <trans-unit id="a0923a21facc4895041c36b66fbffdd3ccd57707" translate="yes" xml:space="preserve">
          <source>As currently implemented, Dijkstra&amp;rsquo;s algorithm does not work for graphs with direction-dependent distances when directed == False. i.e., if dist_matrix[i,j] and dist_matrix[j,i] are not equal and both are nonzero, method=&amp;rsquo;D&amp;rsquo; will not necessarily yield the correct result.</source>
          <target state="translated">現在実装されているように、ダイクストラのアルゴリズムは、方向付けされた== Falseの場合、方向依存の距離を持つグラフでは機能しません。つまり、dist_matrix [i、j]とdist_matrix [j、i]が等しくなく、両方がゼロでない場合、method = 'D'は必ずしも正しい結果をもたらすとは限りません。</target>
        </trans-unit>
        <trans-unit id="18ae6c61fa89653926b85f1ce590dca96bbcbbc7" translate="yes" xml:space="preserve">
          <source>As described on the original website:</source>
          <target state="translated">元のサイトに記載されている通りです。</target>
        </trans-unit>
        <trans-unit id="5aa4aad7dfb43ef2769419ae7f460971c9284dbe" translate="yes" xml:space="preserve">
          <source>As described previously, the most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices:</source>
          <target state="translated">前述したように、最も広く使われている距離関数は2乗フロベニウスノルムであり、ユークリッドノルムを行列に拡張したものです。</target>
        </trans-unit>
        <trans-unit id="e81ebe44fbcb1c67bb2bf8d6d2417ec1c47e735b" translate="yes" xml:space="preserve">
          <source>As expected the confusion matrix shows that posts from the newsgroups on atheism and Christianity are more often confused for one another than with computer graphics.</source>
          <target state="translated">予想通り、混乱マトリックスを見ると、無神論とキリスト教に関するニュースグループの投稿は、コンピュータグラフィックスよりも互いに混乱していることが多いことがわかります。</target>
        </trans-unit>
        <trans-unit id="a21f305b83f6ecc940d012c4c2d2b2809b12f48f" translate="yes" xml:space="preserve">
          <source>As expected, &lt;code&gt;VarianceThreshold&lt;/code&gt; has removed the first column, which has a probability \(p = 5/6 &amp;gt; .8\) of containing a zero.</source>
          <target state="translated">予想通り、 &lt;code&gt;VarianceThreshold&lt;/code&gt; は、ゼロを含む確率\（p = 5/6&amp;gt; .8 \）を持つ最初の列を削除しました。</target>
        </trans-unit>
        <trans-unit id="0a862c171ad9c6c90137a8961c4a49a8109f123e" translate="yes" xml:space="preserve">
          <source>As expected, the dummy regressor is unable to correctly rank the samples and therefore performs the worst on this plot.</source>
          <target state="translated">予想通り、ダミーのリグレグレッサーはサンプルを正しくランク付けすることができないため、このプロットでは最悪の結果となっています。</target>
        </trans-unit>
        <trans-unit id="1461782e678132cdda4c11cb8a9f0f36fdc4dc12" translate="yes" xml:space="preserve">
          <source>As expected, the plot suggests that 3 features are informative, while the remaining are not.</source>
          <target state="translated">予想通り、プロットは3つの特徴が有益であることを示唆していますが、残りの特徴はそうではありません。</target>
        </trans-unit>
        <trans-unit id="21dea3d956b97f59f6554850f2e02a18d3e3f037" translate="yes" xml:space="preserve">
          <source>As for the &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt;, the utility class &lt;a href=&quot;generated/sklearn.preprocessing.binarizer#sklearn.preprocessing.Binarizer&quot;&gt;&lt;code&gt;Binarizer&lt;/code&gt;&lt;/a&gt; is meant to be used in the early stages of &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;. The &lt;code&gt;fit&lt;/code&gt; method does nothing as each sample is treated independently of others:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt;、ユーティリティクラス&lt;a href=&quot;generated/sklearn.preprocessing.binarizer#sklearn.preprocessing.Binarizer&quot;&gt; &lt;code&gt;Binarizer&lt;/code&gt; &lt;/a&gt;初期の段階で使用されることを意味している&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt;。 &lt;code&gt;fit&lt;/code&gt; 各サンプルは、他とは独立に処理されるような方法は、何もしません。</target>
        </trans-unit>
        <trans-unit id="448fad632016e4e86e34b67a2dd205dc93981193" translate="yes" xml:space="preserve">
          <source>As for the &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; classes, the preprocessing module provides a companion function &lt;a href=&quot;generated/sklearn.preprocessing.binarize#sklearn.preprocessing.binarize&quot;&gt;&lt;code&gt;binarize&lt;/code&gt;&lt;/a&gt; to be used when the transformer API is not necessary.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt;クラス、前処理モジュールは、コンパニオン機能を提供する&lt;a href=&quot;generated/sklearn.preprocessing.binarize#sklearn.preprocessing.binarize&quot;&gt; &lt;code&gt;binarize&lt;/code&gt; &lt;/a&gt;トランスAPIが必要でないときに使用されます。</target>
        </trans-unit>
        <trans-unit id="7e4621ab54b3c13fd3d03c5dfae345e68a0fd95a" translate="yes" xml:space="preserve">
          <source>As in &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; centers but does not scale the input data for each feature before applying the SVD.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;と同様に、&lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; は&lt;/a&gt;中心になりますが、SVDを適用する前に各機能の入力データをスケーリングしません。</target>
        </trans-unit>
        <trans-unit id="79a9833b8f532d00a6b35f7523ca2a620de3d115" translate="yes" xml:space="preserve">
          <source>As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values:</source>
          <target state="translated">分類設定の場合と同様に、はめ込み法は引数に配列 X と y をとりますが、この場合 y は整数値ではなく浮動小数点値を持つことが予想されます。</target>
        </trans-unit>
        <trans-unit id="357549c786d11a83e9ad1e4e4484c10d048f813a" translate="yes" xml:space="preserve">
          <source>As is shown in the result before discretization, linear model is fast to build and relatively straightforward to interpret, but can only model linear relationships, while decision tree can build a much more complex model of the data. One way to make linear model more powerful on continuous data is to use discretization (also known as binning). In the example, we discretize the feature and one-hot encode the transformed data. Note that if the bins are not reasonably wide, there would appear to be a substantially increased risk of overfitting, so the discretizer parameters should usually be tuned under cross validation.</source>
          <target state="translated">離散化前の結果に示すように、線形モデルは構築が速く、解釈も比較的簡単ですが、線形関係をモデル化することしかできませんが、決定木はデータのはるかに複雑なモデルを構築することができます。連続データ上で線形モデルをより強力にする方法の1つは、離散化(ビニングとも呼ばれる)を使用することです。この例では、特徴を離散化し、変換されたデータをワンホットエンコードしています。ビンの幅が妥当に広くない場合、オーバーフィットのリスクが大幅に増加するように見えるので、離散化パラメータは通常、クロスバリデーションの下で調整する必要があることに注意してください。</target>
        </trans-unit>
        <trans-unit id="67535506d22839df286d2c74a169c907b47bc78e" translate="yes" xml:space="preserve">
          <source>As mentioned above, we can interpret LDA as assigning \(x\) to the class whose mean \(\mu_k\) is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. Alternatively, LDA is equivalent to first &lt;em&gt;sphering&lt;/em&gt; the data so that the covariance matrix is the identity, and then assigning \(x\) to the closest mean in terms of Euclidean distance (still accounting for the class priors).</source>
          <target state="translated">上記のように、LDAは、平均\（\ mu_k \）がマハラノビス距離の観点から最も近いクラスに\（x \）を割り当てると同時に、クラスの事前確率も考慮に入れると解釈できます。あるいは、LDAは、最初に共分散行列が同一になるようにデータを&lt;em&gt;球形化し&lt;/em&gt;、次にユークリッド距離の観点から最も近い平均に\（x \）を割り当てることと同等です（クラスの事前分布を引き続き考慮します）。</target>
        </trans-unit>
        <trans-unit id="4d80681701a6bd5948db2415d41ef49851684994" translate="yes" xml:space="preserve">
          <source>As mentioned in the introduction, the total claim amount per unit of exposure can be modeled as the product of the prediction of the frequency model by the prediction of the severity model.</source>
          <target state="translated">冒頭で述べたように、被曝単位当たりの総請求額は、頻度モデルの予測値と重症度モデルの予測値との積としてモデル化することができる。</target>
        </trans-unit>
        <trans-unit id="9026eb9b1171dae37e620c4611084cbf5214d244" translate="yes" xml:space="preserve">
          <source>As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).</source>
          <target state="translated">ほとんどの文書では、通常、コーパスで使用される単語の非常に小さなサブセットを使用するため、結果として得られる行列は、多くの特徴値がゼロである(通常、そのうちの99%以上)ことになります。</target>
        </trans-unit>
        <trans-unit id="80903649b427f38994df0cc2bd896c257816e4cc" translate="yes" xml:space="preserve">
          <source>As neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit, non-parametric density estimation.</source>
          <target state="translated">隣接するデータ点が木の同じ葉の中にある可能性が高いので、変換は暗黙のノンパラメトリック密度推定を行います。</target>
        </trans-unit>
        <trans-unit id="604835d70709f3a91a0d5148ef02789e712875c2" translate="yes" xml:space="preserve">
          <source>As neither of these datasets have missing values, we will remove some values to create new versions with artificially missing data. The performance of &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt; on the full original dataset is then compared the performance on the altered datasets with the artificially missing values imputed using different techniques.</source>
          <target state="translated">これらのデータセットには欠測値がないため、一部の値を削除して、人為的に欠測データを含む新しいバージョンを作成します。次に、完全な元のデータセットに対する&lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt; &lt;code&gt;RandomForestRegressor&lt;/code&gt; &lt;/a&gt;のパフォーマンスが、変更されたデータセットに対するパフォーマンスと、さまざまな手法を使用して代入された人為的に欠落した値と比較されます。</target>
        </trans-unit>
        <trans-unit id="4b3b8dbd06a75124d0b25055322ec22921d45503" translate="yes" xml:space="preserve">
          <source>As noted above, for small sample sizes a brute force search can be more efficient than a tree-based query. This fact is accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level of this switch can be specified with the parameter &lt;code&gt;leaf_size&lt;/code&gt;. This parameter choice has many effects:</source>
          <target state="translated">上記のように、サンプルサイズが小さい場合、ブルートフォース検索はツリーベースのクエリよりも効率的です。この事実は、リーフノード内でブルートフォース検索に内部的に切り替えることにより、ボールツリーとKDツリーで説明されています。このスイッチのレベルは、パラメータ &lt;code&gt;leaf_size&lt;/code&gt; で指定できます。このパラメーターの選択には多くの効果があります。</target>
        </trans-unit>
        <trans-unit id="02cd1e4aacf73905da270451ea593d54a1eead69" translate="yes" xml:space="preserve">
          <source>As other classifiers, &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; take as input two arrays: an array &lt;code&gt;X&lt;/code&gt; of shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; holding the training samples, and an array &lt;code&gt;y&lt;/code&gt; of class labels (strings or integers), of shape &lt;code&gt;(n_samples)&lt;/code&gt;:</source>
          <target state="translated">他の分類として、&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;配列：入力二つの配列としてテイク &lt;code&gt;X&lt;/code&gt; 形状の &lt;code&gt;(n_samples, n_features)&lt;/code&gt; 訓練サンプルを保持し、そして配列 &lt;code&gt;y&lt;/code&gt; 形状のクラスラベル（文字列または整数）の、 &lt;code&gt;(n_samples)&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="480621b1ef230a82c36b7031ee3539b5a66ef9c0" translate="yes" xml:space="preserve">
          <source>As other classifiers, &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; take as input two arrays: an array X of size &lt;code&gt;[n_samples,
n_features]&lt;/code&gt; holding the training samples, and an array y of class labels (strings or integers), size &lt;code&gt;[n_samples]&lt;/code&gt;:</source>
          <target state="translated">他の分類子として、&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;は、2つの配列を入力として受け取ります。サイズが &lt;code&gt;[n_samples, n_features]&lt;/code&gt; 配列Xとトレーニングサンプルを保持し、クラスラベル（文字列または整数）の配列yがサイズ &lt;code&gt;[n_samples]&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="7f73865c72a9e8a8b8d3c39d4c4f128f641ec5e7" translate="yes" xml:space="preserve">
          <source>As other classifiers, SGD has to be fitted with two arrays: an array &lt;code&gt;X&lt;/code&gt; of shape (n_samples, n_features) holding the training samples, and an array y of shape (n_samples,) holding the target values (class labels) for the training samples:</source>
          <target state="translated">他の分類器と同様に、SGDには2つの配列を適合させる必要があります。トレーニングサンプルを保持する形状の配列 &lt;code&gt;X&lt;/code&gt; （n_samples、n_features）と、トレーニングサンプルのターゲット値（クラスラベル）を保持する形状の配列y（n_samples）です。 ：</target>
        </trans-unit>
        <trans-unit id="86b96557b2fb9eb69b558f788f743c67cbdf526d" translate="yes" xml:space="preserve">
          <source>As other classifiers, SGD has to be fitted with two arrays: an array X of size [n_samples, n_features] holding the training samples, and an array Y of size [n_samples] holding the target values (class labels) for the training samples:</source>
          <target state="translated">他の分類器と同様に、SGD は 2 つの配列を用いてフィットさせる必要があります。</target>
        </trans-unit>
        <trans-unit id="ba8715d103d1eabb437e2b429e94c0804a44a2eb" translate="yes" xml:space="preserve">
          <source>As other classifiers, forest classifiers have to be fitted with two arrays: a sparse or dense array X of size &lt;code&gt;[n_samples, n_features]&lt;/code&gt; holding the training samples, and an array Y of size &lt;code&gt;[n_samples]&lt;/code&gt; holding the target values (class labels) for the training samples:</source>
          <target state="translated">他の分類子と同様に、フォレスト分類子は2つの配列に適合させる必要があります。サイズが &lt;code&gt;[n_samples, n_features]&lt;/code&gt; スパースまたは密配列Xでトレーニングサンプルを保持し、サイズ &lt;code&gt;[n_samples]&lt;/code&gt; の配列Yがターゲット値（クラスラベル）を保持します。トレーニングサンプル：</target>
        </trans-unit>
        <trans-unit id="9d7e042981f1b86e720810a559a0b5f85e715465" translate="yes" xml:space="preserve">
          <source>As said above (see &amp;ldquo;&lt;a href=&quot;#the-pipeline&quot;&gt;The machine-learning pipeline&lt;/a&gt;&amp;rdquo;), we could also choose to scale numerical values before training the model. This can be useful to apply a similar amount regularization to all of them in the Ridge. The preprocessor is redefined in order to subtract the mean and scale variables to unit variance.</source>
          <target state="translated">上で述べたように（「&lt;a href=&quot;#the-pipeline&quot;&gt;機械学習パイプライン&lt;/a&gt;」を参照）、モデルをトレーニングする前に数値をスケーリングすることも選択できます。これは、リッジ内のすべてに同様の量の正則化を適用するのに役立ちます。平均変数と尺度変数を単位分散に減算するために、プリプロセッサが再定義されます。</target>
        </trans-unit>
        <trans-unit id="d9c002345c84e7b31630185ef2325d2598748527" translate="yes" xml:space="preserve">
          <source>As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized &lt;a href=&quot;https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms&quot;&gt;BLAS&lt;/a&gt; / &lt;a href=&quot;https://en.wikipedia.org/wiki/LAPACK&quot;&gt;LAPACK&lt;/a&gt; library.</source>
          <target state="translated">scikit-learnは一般にNumpy / Scipyと線形代数に大きく依存しているため、これらのライブラリのバージョンを明示的に管理することは理にかなっています。基本的に、Numpyが最適化された&lt;a href=&quot;https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms&quot;&gt;BLAS&lt;/a&gt; / &lt;a href=&quot;https://en.wikipedia.org/wiki/LAPACK&quot;&gt;LAPACK&lt;/a&gt;ライブラリを使用して構築されていることを確認する必要があります。</target>
        </trans-unit>
        <trans-unit id="52ae38c76153b785a0e48dca405bc79c3254bacc" translate="yes" xml:space="preserve">
          <source>As seen previously, the dataset contains columns with different data types and we need to apply a specific preprocessing for each data types. In particular categorical variables cannot be included in linear model if not coded as integers first. In addition, to avoid categorical features to be treated as ordered values, we need to one-hot-encode them. Our pre-processor will</source>
          <target state="translated">前に見たように、データセットには異なるデータ・タイプの列が含まれており、各データ・タイプに対して特定の前処理を適用する必要があります。特に、カテゴリカル変数は、最初に整数としてコード化されていない場合、線形モデルに含めることができません。さらに、カテゴリ変数の特徴が順序付きの値として扱われるのを避けるために、ワン・ホット・エンコードする必要があります。プリプロセッサは以下のようになります</target>
        </trans-unit>
        <trans-unit id="de312b28ec8e48929f9dedcb0b6804aa5a471fe2" translate="yes" xml:space="preserve">
          <source>As shown below, t-SNE for higher perplexities finds meaningful topology of two concentric circles, however the size and the distance of the circles varies slightly from the original. Contrary to the two circles dataset, the shapes visually diverge from S-curve topology on the S-curve dataset even for larger perplexity values.</source>
          <target state="translated">下図に示すように、より高いパープレキシシティのt-SNEでは、2つの同心円のトポロジーを見つけることができますが、円の大きさや距離は元の円からわずかに変化しています。2つの円のデータセットとは逆に、より大きなペルプレキシシティ値でもS字トポロジーから視覚的に乖離した形状になっています。</target>
        </trans-unit>
        <trans-unit id="d368728d274b369ac777e6745357cba862711ccd" translate="yes" xml:space="preserve">
          <source>As such variance is dataset dependent, R&amp;sup2; may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R&amp;sup2; score of 0.0.</source>
          <target state="translated">このような分散はデータセットに依存するため、R&amp;sup2;は異なるデータセット間で有意義に比較できない場合があります。最高のスコアは1.0であり、負の値になる可能性があります（モデルが任意に悪化する可能性があるため）。入力特徴を無視して、yの期待値を常に予測する定数モデルは、0.0のR&amp;sup2;スコアを取得します。</target>
        </trans-unit>
        <trans-unit id="8731c43edea0fd3a0536e8161228e2825929062b" translate="yes" xml:space="preserve">
          <source>As tf&amp;ndash;idf is very often used for text features, there is also another class called &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; that combines all the options of &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; in a single model:</source>
          <target state="translated">tf-idfはテキスト機能でよく使用されるため、&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; の&lt;/a&gt;すべてのオプションを1つのモデルに組み合わせた&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; &lt;/a&gt;という別のクラスもあります。</target>
        </trans-unit>
        <trans-unit id="9ec60e63b98738f893cbc703b7a17df4dbce6a3b" translate="yes" xml:space="preserve">
          <source>As the Earth is nearly spherical, the haversine formula provides a good approximation of the distance between two points of the Earth surface, with a less than 1% error on average.</source>
          <target state="translated">地球はほぼ球形であるため、ハバーシンの式は、地球表面の2点間の距離を平均して1%未満の誤差で近似することができます。</target>
        </trans-unit>
        <trans-unit id="ff6adce24795bb660ac39f8e682709377eb07fe5" translate="yes" xml:space="preserve">
          <source>As the Lasso regression yields sparse models, it can thus be used to perform feature selection, as detailed in &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1-based feature selection&lt;/a&gt;.</source>
          <target state="translated">Lasso回帰はスパースモデルを生成するため、&lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1ベースの特徴選択で&lt;/a&gt;詳述されているように、特徴選択の実行に使用できます。</target>
        </trans-unit>
        <trans-unit id="dc66d7007c269247f1b0a3bd5e17ce48735d32b4" translate="yes" xml:space="preserve">
          <source>As the algorithm tries to balance the volume (ie balance the region sizes), if we take circles with different sizes, the segmentation fails.</source>
          <target state="translated">アルゴリズムはボリュームのバランスを取ろうとするので(つまり領域サイズのバランスを取ろうとする)、異なるサイズの円を取るとセグメンテーションに失敗します。</target>
        </trans-unit>
        <trans-unit id="ca54f3bcc5c655eebc87fa446a643204f651e5fe" translate="yes" xml:space="preserve">
          <source>As the ground truth is known here, we also apply different cluster quality metrics to judge the goodness of fit of the cluster labels to the ground truth.</source>
          <target state="translated">ここでは、基底真理がわかっているので、基底真理に対するクラスタラベルの適合性を判定するために、異なるクラスタ品質メトリクスを適用します。</target>
        </trans-unit>
        <trans-unit id="26f19c73cbdda7d655c1c419bc1b28d6aee89730" translate="yes" xml:space="preserve">
          <source>As the negative of a distance, this kernel is only conditionally positive definite.</source>
          <target state="translated">距離の負として、このカーネルは条件付き正定値でしかありません。</target>
        </trans-unit>
        <trans-unit id="74315005f7ce6ac1c1dafe71dd19e66293fa501a" translate="yes" xml:space="preserve">
          <source>As the prior on the weights is a Gaussian prior, the histogram of the estimated weights is Gaussian.</source>
          <target state="translated">重みの優先順位はガウス先行であるため,推定された重みのヒストグラムはガウス先行である.</target>
        </trans-unit>
        <trans-unit id="f78b7e4ef3de6f9908e43081399b599b6d846fba" translate="yes" xml:space="preserve">
          <source>As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply.</source>
          <target state="translated">このアルゴリズムは尤度のみを最大化するので、手段がゼロに偏ったり、適用されるかもしれない、または適用されないかもしれない特定の構造を持つようにクラスタサイズが偏ったりすることはありません。</target>
        </trans-unit>
        <trans-unit id="492cebfc31b9e0ae2de0bb7669534987ce58057c" translate="yes" xml:space="preserve">
          <source>As usual the best way to adjust the feature extraction parameters is to use a cross-validated grid search, for instance by pipelining the feature extractor with a classifier:</source>
          <target state="translated">通常通り、特徴抽出パラメータを調整する最良の方法は、例えば、特徴抽出器を分類器とパイプライン化するなどして、クロスバリデートされたグリッド検索を使用することです。</target>
        </trans-unit>
        <trans-unit id="ea78a4c7988b659783feac98fcd695de89f8d61f" translate="yes" xml:space="preserve">
          <source>As we have seen, every estimator exposes a &lt;code&gt;score&lt;/code&gt; method that can judge the quality of the fit (or the prediction) on new data. &lt;strong&gt;Bigger is better&lt;/strong&gt;.</source>
          <target state="translated">これまで見てきたように、すべての推定量は、新しいデータに対する適合（または予測）の品質を判断できる &lt;code&gt;score&lt;/code&gt; メソッドを公開します。&lt;strong&gt;大きいほど良い&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="fff4f5f3ab6438d863abba19bbb7c007a060f360" translate="yes" xml:space="preserve">
          <source>As we&amp;rsquo;ll see, some cross-validation objects do specific things with labeled data, others behave differently with grouped data, and others do not use this information.</source>
          <target state="translated">後で見るように、いくつかの相互検証オブジェクトは、ラベル付きデータを使用して特定の処理を実行します。他のものは、グループ化されたデータとは異なる動作をし、他のものはこの情報を使用しません。</target>
        </trans-unit>
        <trans-unit id="c2a5e94d01ebd06105b2b62c25b4b493ea944024" translate="yes" xml:space="preserve">
          <source>As with &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt;, the module further provides convenience functions &lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt;&lt;code&gt;minmax_scale&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt;&lt;code&gt;maxabs_scale&lt;/code&gt;&lt;/a&gt; if you don&amp;rsquo;t want to create an object.</source>
          <target state="translated">同じように&lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt;、モジュールは、さらに便利な機能を提供し&lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt; &lt;code&gt;minmax_scale&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt; &lt;code&gt;maxabs_scale&lt;/code&gt; &lt;/a&gt;あなたがオブジェクトを作成しない場合。</target>
        </trans-unit>
        <trans-unit id="c97bbdc8d450f400aad05e1727632d201bf17f90" translate="yes" xml:space="preserve">
          <source>As with classification classes, the fit method will take as argument vectors X, y, only that in this case y is expected to have floating point values instead of integer values:</source>
          <target state="translated">分類クラスの場合と同様に、 fit メソッドは引数にベクトル X,y をとりますが、この場合 y は整数値ではなく浮動小数点値を持つことが予想されます。</target>
        </trans-unit>
        <trans-unit id="320d01bbc29ff09708f415fa50fd9da6e6972d95" translate="yes" xml:space="preserve">
          <source>As with other classifiers, &lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; takes as input two arrays: an array X, sparse or dense, of size &lt;code&gt;[n_samples, n_features]&lt;/code&gt; holding the training samples, and an array Y of integer values, size &lt;code&gt;[n_samples]&lt;/code&gt;, holding the class labels for the training samples:</source>
          <target state="translated">他の分類器と同様に、&lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt;は入力として2つの配列を取ります。サイズが &lt;code&gt;[n_samples, n_features]&lt;/code&gt; スパースまたはデンスで、トレーニングサンプルを保持する配列Xと、サイズ &lt;code&gt;[n_samples]&lt;/code&gt; の整数値の配列Yは、クラスラベルを保持しますトレーニングサンプル：</target>
        </trans-unit>
        <trans-unit id="211544b1f9de9dd9d971b4d12a5c9dd1ee84dd6a" translate="yes" xml:space="preserve">
          <source>As with other linear models, &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; will take in its &lt;code&gt;fit&lt;/code&gt; method arrays X, y and will store the coefficients \(w\) of the linear model in its &lt;code&gt;coef_&lt;/code&gt; member:</source>
          <target state="translated">他の線形モデルと同様に、&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;はその &lt;code&gt;fit&lt;/code&gt; メソッド配列X、yを &lt;code&gt;coef_&lt;/code&gt; 、線形モデルの係数\（w \）をcoef_メンバーに格納します。</target>
        </trans-unit>
        <trans-unit id="0550c97cec757ba988ca448a8879bb1fa8d6048a" translate="yes" xml:space="preserve">
          <source>As you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the &lt;code&gt;DictVectorizer&lt;/code&gt; class uses a &lt;code&gt;scipy.sparse&lt;/code&gt; matrix by default instead of a &lt;code&gt;numpy.ndarray&lt;/code&gt;.</source>
          <target state="translated">想像できるように、ドキュメントのコーパスの個々の単語の周りでこのようなコンテキストを抽出すると、結果の行列は非常に広くなり（多くのワンホット機能）、それらのほとんどはほとんどの場合ゼロに評価されます。結果のデータ構造をメモリに収まるようにするために、 &lt;code&gt;DictVectorizer&lt;/code&gt; クラスは、デフォルトで &lt;code&gt;numpy.ndarray&lt;/code&gt; の代わりに &lt;code&gt;scipy.sparse&lt;/code&gt; 行列を使用します。</target>
        </trans-unit>
        <trans-unit id="bf1b1999c59fa9b3c0bf1aeb80e6b35e3cde33f4" translate="yes" xml:space="preserve">
          <source>As you can see, by default the KFold cross-validation iterator does not take either datapoint class or group into consideration. We can change this by using the &lt;code&gt;StratifiedKFold&lt;/code&gt; like so.</source>
          <target state="translated">ご覧のとおり、デフォルトでは、KFold交差検証反復子はデータポイントクラスまたはグループを考慮していません。これを &lt;code&gt;StratifiedKFold&lt;/code&gt; を使用して変更できます。</target>
        </trans-unit>
        <trans-unit id="4243f9f7d96f83f5f8fe40416e149534c5c3a7c1" translate="yes" xml:space="preserve">
          <source>As you can see, it is a challenging task: after all, the images are of poor resolution. Do you agree with the classifier?</source>
          <target state="translated">ご覧のように、それは挑戦的なタスクです:結局のところ、画像の解像度が低いのです。あなたはこの分類器に同意しますか?</target>
        </trans-unit>
        <trans-unit id="91d172a6c45b01ca0f06c000f8b26183873978d6" translate="yes" xml:space="preserve">
          <source>As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:</source>
          <target state="translated">ご覧のように、[[0.5]]と[[2]]を返しますが、これは要素が距離0.5にあり、サンプルの3番目の要素であることを意味します(インデックスは0から始まります)。また、複数の点について問い合わせることもできます。</target>
        </trans-unit>
        <trans-unit id="285f08a9d27e1e1ebbb2de47dbd27bd9e8a8b2c7" translate="yes" xml:space="preserve">
          <source>As you can see, the &lt;code&gt;[1, 0]&lt;/code&gt; is comfortably classified as &lt;code&gt;1&lt;/code&gt; since the first two samples are ignored due to their sample weights.</source>
          <target state="translated">ご覧のとおり、最初の2つのサンプルはサンプルの重みのために無視されるため、 &lt;code&gt;[1, 0]&lt;/code&gt; は &lt;code&gt;1&lt;/code&gt; として快適に分類されます。</target>
        </trans-unit>
        <trans-unit id="1273827574a8e654ba4e8fa3b455e8b899058288" translate="yes" xml:space="preserve">
          <source>Ash</source>
          <target state="translated">Ash</target>
        </trans-unit>
        <trans-unit id="7e00bdd748db07e532eed9a3a30221202b903645" translate="yes" xml:space="preserve">
          <source>Ash:</source>
          <target state="translated">Ash:</target>
        </trans-unit>
        <trans-unit id="fa6467e4d61dfb46a998615f44d406b657f3e65c" translate="yes" xml:space="preserve">
          <source>Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).</source>
          <target state="translated">訓練セットの任意の文書に含まれる各単語に固定整数のIDを割り当てる(例えば、単語から整数のインデックスに辞書を構築することによって)。</target>
        </trans-unit>
        <trans-unit id="07fb5452be9437f82662fee13a0f5824d7cc38d4" translate="yes" xml:space="preserve">
          <source>Assign biclusters from one set to another in a one-to-one fashion to maximize the sum of their similarities. This step is performed using the Hungarian algorithm.</source>
          <target state="translated">1つの集合から別の集合へ、それらの類似度の合計を最大にするために1対1の方法でバイクラスターを割り当てます。このステップは,ハンガリー語アルゴリズムを用いて実行される.</target>
        </trans-unit>
        <trans-unit id="341c86e4b7fe6ff7ce7735c5402c926cbe3cd37f" translate="yes" xml:space="preserve">
          <source>Assume that there are no ties in y_score (which is likely to be the case if y_score is continuous) for efficiency gains.</source>
          <target state="translated">y_score に同点がない(y_score が連続している場合はそうなる可能性が高い)と仮定すると、効率向上のためには、y_score に同点がないと仮定する。</target>
        </trans-unit>
        <trans-unit id="eeab86fb8cce55c225e83b0d2dc18f408531cb78" translate="yes" xml:space="preserve">
          <source>Assume two label assignments (of the same N objects), \(U\) and \(V\). Their entropy is the amount of uncertainty for a partition set, defined by:</source>
          <target state="translated">2つのラベル割り当て(同じN個のオブジェクト)、\(U\)とV\(V\)を仮定します。それらのエントロピーは、分割集合の不確実性の量で、次式で定義されます。</target>
        </trans-unit>
        <trans-unit id="0e5d6eebf8d779ccbfdee109f1d6bfd1baee97c2" translate="yes" xml:space="preserve">
          <source>Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.</source>
          <target state="translated">あるデータが独立かつ同一に分散している(i.i.d.)と仮定することは、すべてのサンプルが同じ生成プロセスに由来し、生成プロセスが過去に生成されたサンプルの記憶を持たないと仮定することです。</target>
        </trans-unit>
        <trans-unit id="fc888006cf2fe36cad38d794da87b3aa0beb9fc6" translate="yes" xml:space="preserve">
          <source>At each stage the decision tree \(h_m(x)\) is chosen to minimize the loss function \(L\) given the current model \(F_{m-1}\) and its fit \(F_{m-1}(x_i)\)</source>
          <target state="translated">各段階では、損失関数が最小となるように決定木を選択する。</target>
        </trans-unit>
        <trans-unit id="bfe84c1effa1f2d8d0a150671e591a10f6f5331c" translate="yes" xml:space="preserve">
          <source>At first, a linear model will be applied on the original targets. Due to the non-linearity, the model trained will not be precise during the prediction. Subsequently, a logarithmic function is used to linearize the targets, allowing better prediction even with a similar linear model as reported by the median absolute error (MAE).</source>
          <target state="translated">最初に、元のターゲットに線形モデルが適用されます。非線形性のため、訓練されたモデルは予測中に正確ではありません。その後、対数関数を使用してターゲットを線形化し、中央値絶対誤差(MAE)によって報告されるように、類似の線形モデルでもより良い予測を可能にします。</target>
        </trans-unit>
        <trans-unit id="3103892102cc58ce3e902fe26301aa542c051586" translate="yes" xml:space="preserve">
          <source>At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen.</source>
          <target state="translated">フィット時には,コードブックのビットごとに1つの2値分類器がフィットします.予測時には,クラス空間に新しい点を投影するために分類器が使用され,その点に最も近いクラスが選択されます.</target>
        </trans-unit>
        <trans-unit id="a0707d969d20d6e3136e641e1fe81f003f9dea63" translate="yes" xml:space="preserve">
          <source>At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer makes this process easy with the transform method.</source>
          <target state="translated">学習時には、これは単純にクラスごとに1つの回帰器またはバイナリ分類器を学習することで構成されています。その際、複数クラスのラベルをバイナリ・ラベルに変換する必要があります(クラスに属しているか、属していないか)。LabelBinarizerは、変換メソッドを使ってこのプロセスを簡単にしてくれます。</target>
        </trans-unit>
        <trans-unit id="934e61570171dfc9a4fde1dc5a30d65977fe4f97" translate="yes" xml:space="preserve">
          <source>At prediction time, one assigns the class for which the corresponding model gave the greatest confidence. LabelBinarizer makes this easy with the inverse_transform method.</source>
          <target state="translated">予測時には、対応するモデルが最も信頼性の高いクラスを割り当てます。LabelBinarizerはこれをinverse_transformメソッドで簡単にしてくれます。</target>
        </trans-unit>
        <trans-unit id="01685dbfb47dea411fa9fd7d65acd3b07b6bad19" translate="yes" xml:space="preserve">
          <source>At present, no metric in &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; supports the multioutput-multiclass classification task.</source>
          <target state="translated">現在、&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt;のメトリックは、マルチ出力マルチクラス分類タスクをサポートしていません。</target>
        </trans-unit>
        <trans-unit id="e6b9e03e5fa2d5e24dd0fa35a80f06e2085ef3ae" translate="yes" xml:space="preserve">
          <source>At the end, the top 10 most uncertain predictions will be shown.</source>
          <target state="translated">最後には、不確実性の高い予測トップ10が表示されます。</target>
        </trans-unit>
        <trans-unit id="7dcf986df80f51359fc05a76ec4e2e0108cdc275" translate="yes" xml:space="preserve">
          <source>At the moment, we also don&amp;rsquo;t allow &amp;ldquo;multiclass-multioutput&amp;rdquo; input type.</source>
          <target state="translated">現時点では、「マルチクラス-マルチ出力」入力タイプも許可されていません。</target>
        </trans-unit>
        <trans-unit id="c524e1372c32ce7785a6874af380c32795f14a92" translate="yes" xml:space="preserve">
          <source>At the time of writing (2019), NumPy and SciPy packages distributed on pypi.org (used by &lt;code&gt;pip&lt;/code&gt;) and on the conda-forge channel are linked with OpenBLAS, while conda packages shipped on the &amp;ldquo;defaults&amp;rdquo; channel from anaconda.org are linked by default with MKL.</source>
          <target state="translated">執筆時点（2019）では、pypi.org（ &lt;code&gt;pip&lt;/code&gt; で使用）およびconda-forgeチャネルで配布されるNumPyおよびSciPyパッケージはOpenBLASにリンクされていますが、anaconda.orgから「defaults」チャネルで出荷されるcondaパッケージはデフォルトでMKLにリンクされています。</target>
        </trans-unit>
        <trans-unit id="9df8fa75ddc8543ff453e545d20ad90388cfe52d" translate="yes" xml:space="preserve">
          <source>Atlas (need hardware specific tuning by rebuilding on the target machine)</source>
          <target state="translated">アトラス(ターゲットマシン上での再構築によるハードウェア固有のチューニングが必要</target>
        </trans-unit>
        <trans-unit id="ea953d1f635660a779a7b571a3caa0a7e2c19942" translate="yes" xml:space="preserve">
          <source>Attribute Information</source>
          <target state="translated">属性情報</target>
        </trans-unit>
        <trans-unit id="9f84e7881c1e8ab9dc1a7da3d9d41fc35c6c92bb" translate="yes" xml:space="preserve">
          <source>Attribute Information (in order)</source>
          <target state="translated">属性情報(順</target>
        </trans-unit>
        <trans-unit id="023272de4f331b86e2df4744d3b1e32a0350f127" translate="yes" xml:space="preserve">
          <source>Attribute Information (in order):</source>
          <target state="translated">属性情報(順不同)。</target>
        </trans-unit>
        <trans-unit id="f66b29f74a8ebab57ad67bac7a0ee9e151e247d3" translate="yes" xml:space="preserve">
          <source>Attribute Information:</source>
          <target state="translated">属性情報です。</target>
        </trans-unit>
        <trans-unit id="08359a131c436e0cfbcef05e6e7b301827c6d117" translate="yes" xml:space="preserve">
          <source>Attribute name(s) given as string or a list/tuple of strings Eg.: &lt;code&gt;[&quot;coef_&quot;, &quot;estimator_&quot;, ...], &quot;coef_&quot;&lt;/code&gt;</source>
          <target state="translated">文字列または文字列のリスト/タプルとして指定された属性名例： &lt;code&gt;[&quot;coef_&quot;, &quot;estimator_&quot;, ...], &quot;coef_&quot;&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d3d9c6c9c4fb8ae7805f1c9bc9a51e56b622aec7" translate="yes" xml:space="preserve">
          <source>Attribute to access any fitted sub-estimators by name.</source>
          <target state="translated">フィットしたサブ推定値に名前でアクセスするための属性です。</target>
        </trans-unit>
        <trans-unit id="e30390c6b25519953f15954ce4132cba67fdd587" translate="yes" xml:space="preserve">
          <source>AttributeError</source>
          <target state="translated">AttributeError</target>
        </trans-unit>
        <trans-unit id="a6652617f2c799eb11ee727b16c5646c48af6905" translate="yes" xml:space="preserve">
          <source>Attributes</source>
          <target state="translated">Attributes</target>
        </trans-unit>
        <trans-unit id="12b65a8e129440be6a666c5f76241c3731cf3a1b" translate="yes" xml:space="preserve">
          <source>Attributes of named_steps map to keys, enabling tab completion in interactive environments:</source>
          <target state="translated">named_stepsの属性をキーにマッピングし、インタラクティブな環境でのタブ補完を可能にします。</target>
        </trans-unit>
        <trans-unit id="b8087185e5ee37cef4c337de5697d35d75d909fd" translate="yes" xml:space="preserve">
          <source>Attributes:</source>
          <target state="translated">Attributes:</target>
        </trans-unit>
        <trans-unit id="662dd9f8d8390601f16824595a7c397648198b36" translate="yes" xml:space="preserve">
          <source>Augment dataset with an additional dummy feature.</source>
          <target state="translated">データセットにダミー特徴を追加してデータセットを拡張する。</target>
        </trans-unit>
        <trans-unit id="6060232846b2935a9975d9bd33986976db84e5d5" translate="yes" xml:space="preserve">
          <source>Authors : Kemal Eren License: BSD 3 clause</source>
          <target state="translated">作者.Kemal Eren ライセンス:BSD 3節</target>
        </trans-unit>
        <trans-unit id="c8696543eb048b80bb9394a738477bc586b5e9f7" translate="yes" xml:space="preserve">
          <source>Authors: &lt;a href=&quot;mailto:mks542%40nyu.edu&quot;&gt;Manoj Kumar&lt;/a&gt;, &lt;a href=&quot;https://github.com/maikia&quot;&gt;Maria Telenczuk&lt;/a&gt;</source>
          <target state="translated">著者：&lt;a href=&quot;mailto:mks542%40nyu.edu&quot;&gt;Manoj Kumar&lt;/a&gt;、&lt;a href=&quot;https://github.com/maikia&quot;&gt;Maria Telenczuk&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b98ce7a4dae4be4f8e868a70ee741cf11d8125c8" translate="yes" xml:space="preserve">
          <source>Automatic Relevance Determination Regression (ARD)</source>
          <target state="translated">自動関連性決定回帰(ARD)</target>
        </trans-unit>
        <trans-unit id="79d2e9f45916ec9baf098fb8e9c666bf3b326d9b" translate="yes" xml:space="preserve">
          <source>Automatic grouping of similar objects into sets.</source>
          <target state="translated">似たようなオブジェクトをセットに自動でグループ化します。</target>
        </trans-unit>
        <trans-unit id="ef442c781e8f4741229b1581adda336c7e2079fb" translate="yes" xml:space="preserve">
          <source>Automatic selection</source>
          <target state="translated">自動選択</target>
        </trans-unit>
        <trans-unit id="e1af4aed4c1976557d8776224b00250d32e50c1e" translate="yes" xml:space="preserve">
          <source>Automatic selection:</source>
          <target state="translated">自動選択です。</target>
        </trans-unit>
        <trans-unit id="023744d610124c3af17954739cd092606ebe3ff4" translate="yes" xml:space="preserve">
          <source>Automatically extract clusters according to the Xi-steep method.</source>
          <target state="translated">Xi-steep法に従って自動的にクラスタを抽出します。</target>
        </trans-unit>
        <trans-unit id="6cfaabc56d1e51ca7fb8958edc60fddd81d43492" translate="yes" xml:space="preserve">
          <source>Available Metrics</source>
          <target state="translated">利用可能なメトリクス</target>
        </trans-unit>
        <trans-unit id="66f9440a5b62bebabad643e5080d2b83ccab9f91" translate="yes" xml:space="preserve">
          <source>Available Metrics The following lists the string metric identifiers and the associated distance metric classes:</source>
          <target state="translated">利用可能なメトリクス 以下に、文字列メトリック識別子と関連する距離メトリック・クラスの一覧を示します。</target>
        </trans-unit>
        <trans-unit id="7d386374f19b16d2a68aae1af1097f01c0217752" translate="yes" xml:space="preserve">
          <source>Available losses for regression are &amp;lsquo;least_squares&amp;rsquo;, &amp;lsquo;least_absolute_deviation&amp;rsquo;, which is less sensitive to outliers, and &amp;lsquo;poisson&amp;rsquo;, which is well suited to model counts and frequencies. For classification, &amp;lsquo;binary_crossentropy&amp;rsquo; is used for binary classification and &amp;lsquo;categorical_crossentropy&amp;rsquo; is used for multiclass classification. By default the loss is &amp;lsquo;auto&amp;rsquo; and will select the appropriate loss depending on &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-177&quot;&gt;y&lt;/a&gt; passed to &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt;.</source>
          <target state="translated">回帰に使用できる損失は、「least_squares」、「least_absolute_deviation」（外れ値の影響を受けにくい）、および「poisson」（モデルの数と頻度に適しています）です。分類の場合、「binary_crossentropy」はバイナリ分類に使用され、「categorical_crossentropy」はマルチクラス分類に使用されます。デフォルトでは、損失は「自動」であり、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;フィット&lt;/a&gt;するために渡された&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-177&quot;&gt;yに&lt;/a&gt;応じて適切な損失を選択します。</target>
        </trans-unit>
        <trans-unit id="eca931fa62c0bd7287f5c68d38715d784e965be8" translate="yes" xml:space="preserve">
          <source>AveBedrms average number of bedrooms</source>
          <target state="translated">AveBedrmsの平均ベッドルーム数</target>
        </trans-unit>
        <trans-unit id="a283071b7681c2d8946f1976d0c70df9b6239788" translate="yes" xml:space="preserve">
          <source>AveOccup average house occupancy</source>
          <target state="translated">AveOccup 平均住宅稼働率</target>
        </trans-unit>
        <trans-unit id="17df5670db0dd155c5ad44fbbe5780684721e711" translate="yes" xml:space="preserve">
          <source>AveRooms average number of rooms</source>
          <target state="translated">AveRoomsの平均部屋数</target>
        </trans-unit>
        <trans-unit id="09b414b4c6acadd2002166fddf247cff6dfe513e" translate="yes" xml:space="preserve">
          <source>Average anomaly score of X of the base classifiers.</source>
          <target state="translated">基本分類器のXの平均異常スコア。</target>
        </trans-unit>
        <trans-unit id="aaed8a8037e41bff9561818c652346d8c7001f53" translate="yes" xml:space="preserve">
          <source>Average blood pressure</source>
          <target state="translated">平均血圧</target>
        </trans-unit>
        <trans-unit id="6a54e71704b0c1b179b46e11f33d130b77d3a0d5" translate="yes" xml:space="preserve">
          <source>Average hinge loss (non-regularized)</source>
          <target state="translated">平均ヒンジ損失(非正規化</target>
        </trans-unit>
        <trans-unit id="6b044db8b528a2f509603d248114c200773b5ebd" translate="yes" xml:space="preserve">
          <source>Average log-likelihood of the samples under the current model</source>
          <target state="translated">現在のモデルでのサンプルの平均対数尤度</target>
        </trans-unit>
        <trans-unit id="eb68b4c700400b34d7c3583315c40c631209f07e" translate="yes" xml:space="preserve">
          <source>Average log-likelihood of the samples under the current model.</source>
          <target state="translated">現在のモデルでのサンプルの平均対数尤度。</target>
        </trans-unit>
        <trans-unit id="c993da9cb729cacf8ff6fdcfed939fa26cd8fbe9" translate="yes" xml:space="preserve">
          <source>Average of each column of kernel matrix</source>
          <target state="translated">カーネル行列の各列の平均</target>
        </trans-unit>
        <trans-unit id="85f369d4432a9c6380c3e6b6c74d7a111da4715c" translate="yes" xml:space="preserve">
          <source>Average of kernel matrix</source>
          <target state="translated">カーネル行列の平均</target>
        </trans-unit>
        <trans-unit id="d0fd4fa14e78cac718fcf8ea4ba2f14bf0250758" translate="yes" xml:space="preserve">
          <source>Average of the decision functions of the base classifiers.</source>
          <target state="translated">基本分類器の決定関数の平均.</target>
        </trans-unit>
        <trans-unit id="6599ad46541efb6c59215eff61a003e0c6f84646" translate="yes" xml:space="preserve">
          <source>Average precision. If None, the average precision is not shown.</source>
          <target state="translated">平均精度。Noneの場合、平均精度は表示されません。</target>
        </trans-unit>
        <trans-unit id="af6f79d617c2284de9fdf1ff445ccb0bef8560e1" translate="yes" xml:space="preserve">
          <source>Averaged weights assigned to the features.</source>
          <target state="translated">特徴に割り当てられた重みの平均。</target>
        </trans-unit>
        <trans-unit id="f2d4013e90ff32393ce2936382f74ff4d17a82cb" translate="yes" xml:space="preserve">
          <source>Averaged weights assigned to the features. Only available if &lt;code&gt;average=True&lt;/code&gt;.</source>
          <target state="translated">機能に割り当てられた平均の重み。 &lt;code&gt;average=True&lt;/code&gt; 場合にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="964142eff5ddb8396f41dd41083f429f4f575768" translate="yes" xml:space="preserve">
          <source>Avoid computation of the row norms of X.</source>
          <target state="translated">Xの行の規範の計算を避ける。</target>
        </trans-unit>
        <trans-unit id="9941876a6243b1a2b044ada9bc274f0eac824c27" translate="yes" xml:space="preserve">
          <source>Axes object to plot on. If &lt;code&gt;None&lt;/code&gt;, a new figure and axes is created.</source>
          <target state="translated">プロットするオブジェクトを軸します。場合 &lt;code&gt;None&lt;/code&gt; 、新しい数字と軸が作成されます。</target>
        </trans-unit>
        <trans-unit id="cdbc35f14da17d95fedf51be8b6d0e5ffbe47113" translate="yes" xml:space="preserve">
          <source>Axes to plot to. If None, use current axis. Any previous content is cleared.</source>
          <target state="translated">プロットする軸。なしの場合は、現在の軸を使用します。以前の内容はすべてクリアされます。</target>
        </trans-unit>
        <trans-unit id="4301ccb7939c0c6e359abdb2441c99fb2531f767" translate="yes" xml:space="preserve">
          <source>Axes with ROC Curve.</source>
          <target state="translated">ROC曲線を持つ軸。</target>
        </trans-unit>
        <trans-unit id="ff3b86b5f5ed2e8ea1a56f1b370527db0b52d66c" translate="yes" xml:space="preserve">
          <source>Axes with confusion matrix.</source>
          <target state="translated">混同行列を持つ軸。</target>
        </trans-unit>
        <trans-unit id="d788fc3169dcbb97f9fc05c8f3617be236c43224" translate="yes" xml:space="preserve">
          <source>Axes with precision recall curve.</source>
          <target state="translated">精密なリコール曲線を持つ軸。</target>
        </trans-unit>
        <trans-unit id="94256a7c3ba8c276f8434ee8dd2981a7e1ebede5" translate="yes" xml:space="preserve">
          <source>Axis along which the argmin and distances are to be computed.</source>
          <target state="translated">argminと距離が計算される軸。</target>
        </trans-unit>
        <trans-unit id="9912ed29fe6a32ef154522ba9276f54ba95862ff" translate="yes" xml:space="preserve">
          <source>Axis along which the axis should be computed.</source>
          <target state="translated">計算されるべき軸に沿った軸。</target>
        </trans-unit>
        <trans-unit id="e908ee13d1946f0bd5dd05b5fc4432878bcf585b" translate="yes" xml:space="preserve">
          <source>Axis along which to operate. Default is 0, i.e. the first axis.</source>
          <target state="translated">操作する軸。デフォルトは0、すなわち最初の軸です。</target>
        </trans-unit>
        <trans-unit id="5bcbcdbcc90358e775edd4243e64cbf53abf5bcb" translate="yes" xml:space="preserve">
          <source>Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.</source>
          <target state="translated">平均が計算される軸または軸。デフォルトでは、平坦化された配列の平均を計算します。</target>
        </trans-unit>
        <trans-unit id="ebbac4e2a88e22f8f6d03a59528261348ca05f77" translate="yes" xml:space="preserve">
          <source>Axis used to compute the means and standard deviations along. If 0, transform each feature, otherwise (if 1) transform each sample.</source>
          <target state="translated">沿って平均と標準偏差を計算するために使用される軸。0 の場合は各特徴量を変換し,そうでない場合は(1 の場合)各標本を変換します.</target>
        </trans-unit>
        <trans-unit id="1f1b4d8a7c2bf89e7c753b7dde1a7de7538ad410" translate="yes" xml:space="preserve">
          <source>Axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample.</source>
          <target state="translated">沿ってスケールするために使用される軸。0の場合は各特徴量を独立してスケールし、そうでない場合は(1の場合)各サンプルをスケールします。</target>
        </trans-unit>
        <trans-unit id="ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec" translate="yes" xml:space="preserve">
          <source>B</source>
          <target state="translated">B</target>
        </trans-unit>
        <trans-unit id="f9392f6fda3d4e0cffc8528669a8abc510a0238a" translate="yes" xml:space="preserve">
          <source>B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</source>
          <target state="translated">B 1000(Bk-0.63)^2 ここで、Bkは町ごとの黒人の割合である。</target>
        </trans-unit>
        <trans-unit id="0286abf8af6c149d27c85dea93b91d7cc057559a" translate="yes" xml:space="preserve">
          <source>B. C. Ross &amp;ldquo;Mutual Information between Discrete and Continuous Data Sets&amp;rdquo;. PLoS ONE 9(2), 2014.</source>
          <target state="translated">BCロス「離散データセットと連続データセット間の相互情報量」。PLoS ONE 9（2）、2014年。</target>
        </trans-unit>
        <trans-unit id="e776d2e85d14b59e337010c96ae3a1db78bda84a" translate="yes" xml:space="preserve">
          <source>B12</source>
          <target state="translated">B12</target>
        </trans-unit>
        <trans-unit id="598b91099876ac145b645491cf669799147b8703" translate="yes" xml:space="preserve">
          <source>Back-projection to the original space.</source>
          <target state="translated">元の空間へのバックプロジェクション。</target>
        </trans-unit>
        <trans-unit id="e935196220898869d9edfac83ba997f81f0fb5f8" translate="yes" xml:space="preserve">
          <source>Bad (e.g. independent labelings) have negative or close to 0.0 scores:</source>
          <target state="translated">悪い(独立したラベリングなど)は、マイナスまたは0.0に近いスコアを持つ。</target>
        </trans-unit>
        <trans-unit id="739e4c11d3b87131b9d408fbaefcc186a18d8f06" translate="yes" xml:space="preserve">
          <source>Bad (e.g. independent labelings) have non-positive scores:</source>
          <target state="translated">悪い(独立したラベリングなど)は非ポジティブなスコアを持っています。</target>
        </trans-unit>
        <trans-unit id="3594d99d3d09849089991e6e49125443f8f3dc1b" translate="yes" xml:space="preserve">
          <source>Bad (e.g. independent labelings) have zero scores:</source>
          <target state="translated">悪い(独立したラベリングなど)は0点です。</target>
        </trans-unit>
        <trans-unit id="ab5267c44135f5b9260b00b1b8283ecbffae624a" translate="yes" xml:space="preserve">
          <source>Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:</source>
          <target state="translated">バギング手法には様々な種類がありますが、ほとんどの場合、学習セットのランダムな部分集合を描画する方法によって互いに異なります。</target>
        </trans-unit>
        <trans-unit id="c18dc564424c60fd05a12c3420895fdf2cfdfe52" translate="yes" xml:space="preserve">
          <source>Bags of words</source>
          <target state="translated">言葉のバッグ</target>
        </trans-unit>
        <trans-unit id="c108b519256622b208a24e6481a9d957224afa52" translate="yes" xml:space="preserve">
          <source>Balance model complexity and cross-validated score</source>
          <target state="translated">バランスモデルの複雑さと交差検証されたスコア</target>
        </trans-unit>
        <trans-unit id="95bff14a4bbf238a2164b4e0cae458c0fe2685ef" translate="yes" xml:space="preserve">
          <source>Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (&lt;code&gt;sample_weight&lt;/code&gt;) for each class to the same value. Also note that weight-based pre-pruning criteria, such as &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt;, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="translated">トレーニングの前にデータセットのバランスを取って、ツリーが優勢なクラスに偏らないようにします。クラスの平衡化は、各クラスから同数のサンプルをサンプリングするか、各クラスのサンプルの重みの合計（ &lt;code&gt;sample_weight&lt;/code&gt; ）を同じ値に正規化することで実行できます。また、 &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt; などの重みベースの事前剪定基準は、min_samples_leafなどのサンプルの重みを認識しない基準よりも、支配的なクラスに偏らないことに注意して &lt;code&gt;min_samples_leaf&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="05f675285d8ed3983e70827edce654b1139be242" translate="yes" xml:space="preserve">
          <source>Balanced Accuracy as described in &lt;a href=&quot;#urbanowicz2015&quot; id=&quot;id10&quot;&gt;[Urbanowicz2015]&lt;/a&gt;: the average of sensitivity and specificity is computed for each class and then averaged over total number of classes.</source>
          <target state="translated">&lt;a href=&quot;#urbanowicz2015&quot; id=&quot;id10&quot;&gt;[Urbanowicz2015]で&lt;/a&gt;説明されているバランスの取れた精度：感度と特異度の平均がクラスごとに計算され、クラスの総数で平均化されます。</target>
        </trans-unit>
        <trans-unit id="784fa76a8b1c1ec60f28239d94fc65e8d7be9167" translate="yes" xml:space="preserve">
          <source>Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the accuracy of prediction algorithms for classification: an overview</source>
          <target state="translated">Baldi,Brunak,Chauvin,Andersen and Nielsen,(2000).分類のための予測アルゴリズムの精度評価:概要</target>
        </trans-unit>
        <trans-unit id="06f9184e762e9e45e71254eb60f8399d4c411472" translate="yes" xml:space="preserve">
          <source>Ball tree for fast generalized N-point problems.</source>
          <target state="translated">高速な一般化されたN点問題のためのボールツリー。</target>
        </trans-unit>
        <trans-unit id="7faaf3d73987f1f3ca80fb71528f4fcc07c23b8e" translate="yes" xml:space="preserve">
          <source>BallTree for fast generalized N-point problems</source>
          <target state="translated">高速で一般化されたN点問題のためのBallTree</target>
        </trans-unit>
        <trans-unit id="fef5acd09f6d09b9952e2688ee6340fe9a396e5b" translate="yes" xml:space="preserve">
          <source>BallTree(X, leaf_size=40, metric=&amp;rsquo;minkowski&amp;rsquo;, **kwargs)</source>
          <target state="translated">BallTree（X、leaf_size = 40、metric = 'minkowski'、** kwargs）</target>
        </trans-unit>
        <trans-unit id="f72ce994093395e37676061f1dd51a4ea80c1082" translate="yes" xml:space="preserve">
          <source>Bandwidth used in the RBF kernel.</source>
          <target state="translated">RBF カーネルで使用される帯域幅。</target>
        </trans-unit>
        <trans-unit id="0c486c3167e8e5060c79997e836642fbe914971f" translate="yes" xml:space="preserve">
          <source>Barnes-Hut is an approximation of the exact method. The approximation is parameterized with the angle parameter, therefore the angle parameter is unused when method=&amp;rdquo;exact&amp;rdquo;</source>
          <target state="translated">Barnes-Hutは、正確な方法の近似です。近似は角度パラメーターでパラメーター化されるため、method =&amp;rdquo; exact&amp;rdquo;の場合、角度パラメーターは使用されません。</target>
        </trans-unit>
        <trans-unit id="3b0ab9922dcf0fcbb725191c9b8bc0a7c9ecc19c" translate="yes" xml:space="preserve">
          <source>Barnes-Hut is significantly more scalable. Barnes-Hut can be used to embed hundred of thousands of data points while the exact method can handle thousands of samples before becoming computationally intractable</source>
          <target state="translated">Barnes-Hutは大幅にスケーラブルです。Barnes-Hutは何十万ものデータポイントを埋め込むのに使用できますが、正確な方法は計算が困難になる前に何千ものサンプルを扱うことができます。</target>
        </trans-unit>
        <trans-unit id="b10a9a2aa738bc222bc8e12bf3c9da484bcf459f" translate="yes" xml:space="preserve">
          <source>Barnes-Hut only works with dense input data. Sparse data matrices can only be embedded with the exact method or can be approximated by a dense low rank projection for instance using &lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt;&lt;code&gt;sklearn.decomposition.TruncatedSVD&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Barnes-Hutは、密な入力データでのみ機能します。スパースデータマトリックスは、exactメソッドでのみ埋め込むことができます。または、たとえば&lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt; &lt;code&gt;sklearn.decomposition.TruncatedSVD&lt;/code&gt; &lt;/a&gt;を使用して、密な低ランクの投影で近似できます。</target>
        </trans-unit>
        <trans-unit id="7063b67add41dd6938d0fe03decc31a786e18ce9" translate="yes" xml:space="preserve">
          <source>Base class for all estimators in scikit-learn</source>
          <target state="translated">scikit-learn のすべての estimator の基底クラス</target>
        </trans-unit>
        <trans-unit id="b3ed1a2c57def49034ad9434aa34e34b64c59294" translate="yes" xml:space="preserve">
          <source>Base class for all kernels.</source>
          <target state="translated">すべてのカーネルの基底クラス。</target>
        </trans-unit>
        <trans-unit id="597d1d5f179914ea7470a3760bd8ee3a2400c1bc" translate="yes" xml:space="preserve">
          <source>Base classes</source>
          <target state="translated">基底クラス</target>
        </trans-unit>
        <trans-unit id="425994da22262a2dfe067d38bec718a9f15fcaea" translate="yes" xml:space="preserve">
          <source>Base classes for all estimators.</source>
          <target state="translated">すべての推定子の基底クラス。</target>
        </trans-unit>
        <trans-unit id="b0cdf2e205135846db5a02006385fb2f510f153e" translate="yes" xml:space="preserve">
          <source>Base classifier for this ensemble.</source>
          <target state="translated">このアンサンブルの基本分類器.</target>
        </trans-unit>
        <trans-unit id="61ae0e2f9a4051180ae0aa7361de2f44f1e6a032" translate="yes" xml:space="preserve">
          <source>Base estimator for this ensemble.</source>
          <target state="translated">このアンサンブルの基底推定器。</target>
        </trans-unit>
        <trans-unit id="96ce9dc70ae7445a77ac8ebb388087887574787e" translate="yes" xml:space="preserve">
          <source>Base estimator object which implements the following methods:</source>
          <target state="translated">以下のメソッドを実装した基底推定器オブジェクト。</target>
        </trans-unit>
        <trans-unit id="a4f9906b1c0bdc268249c9569eb1485c9b45d816" translate="yes" xml:space="preserve">
          <source>Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to &amp;lsquo;drop&amp;rsquo; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">一緒に積み重ねられる基本推定量。リストの各要素は、文字列（つまり名前）のタプルと推定インスタンスとして定義されます。 &lt;code&gt;set_params&lt;/code&gt; を使用して、推定量を「drop」に設定できます。</target>
        </trans-unit>
        <trans-unit id="d3f0bf51e2f9cb27fcde269872aa74bb4b0677d2" translate="yes" xml:space="preserve">
          <source>Base of the logarithm used for the discount. A low value means a sharper discount (top results are more important).</source>
          <target state="translated">割引に使用される対数のベース。値が低いと割引がよりシャープになる(上位の結果の方が重要)。</target>
        </trans-unit>
        <trans-unit id="f6856d61f849440214e260ddb8260a2004382c80" translate="yes" xml:space="preserve">
          <source>Based on these bin intervals, &lt;code&gt;X&lt;/code&gt; is transformed as follows:</source>
          <target state="translated">これらのビンの間隔に基づいて、 &lt;code&gt;X&lt;/code&gt; は次のように変換されます。</target>
        </trans-unit>
        <trans-unit id="623eb094427cb27d5c4cd4b74097a0449f38ab15" translate="yes" xml:space="preserve">
          <source>Basically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation.</source>
          <target state="translated">基本的には、1.はハードドライブ上のファイル、データベース、ネットワークストリームなどからインスタンスを生成するリーダーかもしれません。しかし、これを実現する方法の詳細については、このドキュメントの範囲を超えています。</target>
        </trans-unit>
        <trans-unit id="93b2071c2229aa4389154bc62f3ee3617cb13ae0" translate="yes" xml:space="preserve">
          <source>Bayesian ARD regression.</source>
          <target state="translated">ベイズARD回帰。</target>
        </trans-unit>
        <trans-unit id="53c3e11f0d41baa7b5753d8723cd4012a9d964c3" translate="yes" xml:space="preserve">
          <source>Bayesian Ridge Regression</source>
          <target state="translated">ベイズリッジ回帰</target>
        </trans-unit>
        <trans-unit id="5f8e5b5e24015d9adc82586dba204556446370ab" translate="yes" xml:space="preserve">
          <source>Bayesian Ridge Regression is used for regression:</source>
          <target state="translated">回帰にはベイジアンリッジ回帰を使用しています。</target>
        </trans-unit>
        <trans-unit id="c13c97854320151b1c6271dc7d1a9fd73998cc11" translate="yes" xml:space="preserve">
          <source>Bayesian information criterion for the current model on the input X.</source>
          <target state="translated">入力X上の現在のモデルのベイズ情報基準。</target>
        </trans-unit>
        <trans-unit id="9caaf8acefcf32287fe9e5bedbc0bc32de3a2bfd" translate="yes" xml:space="preserve">
          <source>Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.</source>
          <target state="translated">ベイズ回帰技術は、推定手順に正則化パラメータを含めるために使用することができます:正則化パラメータは、ハードな意味で設定されるのではなく、手元のデータに合わせて調整されます。</target>
        </trans-unit>
        <trans-unit id="f02284fac00f823c24638d260dbf19c33e10b7f5" translate="yes" xml:space="preserve">
          <source>Bayesian regressors</source>
          <target state="translated">ベイズ回帰子</target>
        </trans-unit>
        <trans-unit id="2b720ec1817dce1bf9d7dcbcab6e206b85efdaa6" translate="yes" xml:space="preserve">
          <source>Bayesian ridge regression</source>
          <target state="translated">ベイズ式尾根回帰</target>
        </trans-unit>
        <trans-unit id="66a81420e9a002d3d7820b1130f20af6976fb2b7" translate="yes" xml:space="preserve">
          <source>Bayesian ridge regression.</source>
          <target state="translated">ベイズ式リッジ回帰。</target>
        </trans-unit>
        <trans-unit id="13244381ac86550f492dc53b5005b711b59fb204" translate="yes" xml:space="preserve">
          <source>Be aware that the number of features in the output array scales polynomially in the number of features of the input array, and exponentially in the degree. High degrees can cause overfitting.</source>
          <target state="translated">出力配列の特徴量の数は,入力配列の特徴量の数では多項式に,次数では指数関数的に変化することに注意してください.次数が大きいとオーバーフィットの原因となります.</target>
        </trans-unit>
        <trans-unit id="3faa94ddadd318647d89d2a02f2d45606c753b0d" translate="yes" xml:space="preserve">
          <source>Be invariant to class label: relabelling &lt;code&gt;y = [&quot;Happy&quot;, &quot;Sad&quot;]&lt;/code&gt; to &lt;code&gt;y = [1, 0]&lt;/code&gt; should not change the indices generated.</source>
          <target state="translated">クラスラベルに対して不変である： &lt;code&gt;y = [&quot;Happy&quot;, &quot;Sad&quot;]&lt;/code&gt; を &lt;code&gt;y = [1, 0]&lt;/code&gt; ラベル付けしても、生成されるインデックスは変更されません。</target>
        </trans-unit>
        <trans-unit id="3a63e14a349843f9c954f28468802eec39d0181f" translate="yes" xml:space="preserve">
          <source>Be mindful that this function is an order of magnitude slower than other metrics, such as the Adjusted Rand Index.</source>
          <target state="translated">この関数は、調整済みランド指数などの他の指標に比べて桁違いに遅いことに注意してください。</target>
        </trans-unit>
        <trans-unit id="83921ebb17d38b8a4b1a1e3f6653eb2aa6327ec9" translate="yes" xml:space="preserve">
          <source>Because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article.</source>
          <target state="translated">LARSは残差の反復的なリフィットに基づいているので、ノイズの影響に特に敏感であるように思われる。この問題は、Efron et al.(2004)Annals of Statisticsの論文の議論のセクションでWeisbergによって詳細に議論されています。</target>
        </trans-unit>
        <trans-unit id="5dbb3cd2ef2ab4d2456b8c42be279603ef8045ba" translate="yes" xml:space="preserve">
          <source>Because of scaling performed by this method, it is discouraged to use it together with methods that are not scale-invariant (like SVMs)</source>
          <target state="translated">この手法ではスケーリングが行われるため、スケール不変でない手法(SVMなど)との併用は推奨されません。</target>
        </trans-unit>
        <trans-unit id="63be2fc59867472cdd54d70a5100dc5be4d8ddfa" translate="yes" xml:space="preserve">
          <source>Because of the Python object overhead involved in calling the python function, this will be fairly slow, but it will have the same scaling as other distances.</source>
          <target state="translated">python関数を呼び出す際にPythonオブジェクトのオーバーヘッドが発生するため、かなり遅くなりますが、他の距離と同じようなスケーリングになります。</target>
        </trans-unit>
        <trans-unit id="8e7a8c8acef8e25d8225bca274332d6c56e271be" translate="yes" xml:space="preserve">
          <source>Because the models in each chain are arranged randomly there is significant variation in performance among the chains. Presumably there is an optimal ordering of the classes in a chain that will yield the best performance. However we do not know that ordering a priori. Instead we can construct an voting ensemble of classifier chains by averaging the binary predictions of the chains and apply a threshold of 0.5. The Jaccard similarity score of the ensemble is greater than that of the independent models and tends to exceed the score of each chain in the ensemble (although this is not guaranteed with randomly ordered chains).</source>
          <target state="translated">各チェーンのモデルはランダムに配置されているため、チェーン間で性能に大きなばらつきがあります。おそらく、チェーン内のクラスには、最高のパフォーマンスをもたらす最適な順序があると思われます。しかし、先験的にその順序を知ることはできません。その代わりに、分類器チェーンの二値予測を平均化し、0.5の閾値を適用することで、分類器チェーンの投票アンサンブルを構築することができます。アンサンブルのジャカード類似度スコアは独立モデルのスコアよりも大きく、アンサンブル内の各チェーンのスコアを上回る傾向があります(ランダムに順序付けされたチェーンではこれは保証されませんが)。</target>
        </trans-unit>
        <trans-unit id="7223d10fe616d4b0ef82e34ecf5c077bba0a2d4c" translate="yes" xml:space="preserve">
          <source>Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, &lt;code&gt;radius_neighbors&lt;/code&gt; returns arrays of objects, where each object is a 1D array of indices or distances.</source>
          <target state="translated">各ポイントの近傍の数は必ずしも同じではないため、複数のクエリポイントの結果を標準のデータ配列に収めることはできません。効率を上げるために、 &lt;code&gt;radius_neighbors&lt;/code&gt; はオブジェクトの配列を返します。各オブジェクトは、インデックスまたは距離の1D配列です。</target>
        </trans-unit>
        <trans-unit id="5b27b93fd142831db995faecc3827454df4cf37d" translate="yes" xml:space="preserve">
          <source>Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of zero.</source>
          <target state="translated">クエリセットがトレーニングセットと一致するので、各点の最も近い隣人は点そのものであり、距離は0です。</target>
        </trans-unit>
        <trans-unit id="40a1ac9f9c5627c60c3da1e0a0050f7b91b8daf1" translate="yes" xml:space="preserve">
          <source>Because this implementation uses a flat kernel and a Ball Tree to look up members of each kernel, the complexity will tend towards O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points. In higher dimensions the complexity will tend towards O(T*n^2).</source>
          <target state="translated">この実装ではフラットカーネルとボールツリーを使用して各カーネルのメンバーを検索するので、サンプル数をn、点の数をTとした低次元では、複雑さはO(T*n*log(n))に向かっていく傾向があります。高次元では,複雑さはO(T*n^2)の傾向にあります.</target>
        </trans-unit>
        <trans-unit id="5430204387d0c34e554350a9ea0af84d5f7d324c" translate="yes" xml:space="preserve">
          <source>Before we can use Ames dataset we still need to do some preprocessing. First, the dataset has many missing values. To impute them, we will exchange categorical missing values with the new category &amp;lsquo;missing&amp;rsquo; while the numerical missing values with the &amp;lsquo;mean&amp;rsquo; of the column. We will also encode the categories with either &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt;&lt;/a&gt; depending for which type of model we will use them (linear or non-linear model). To falicitate this preprocessing we will make two pipelines. You can skip this section if your data is ready to use and does not need preprocessing</source>
          <target state="translated">Amesデータセットを使用する前に、前処理を行う必要があります。まず、データセットには多くの欠落値があります。それらを代入するために、カテゴリの欠落値を新しいカテゴリ「missing」と交換し、数値の欠落値を列の「mean」と交換します。また、使用するモデルのタイプ（線形モデルまたは非線形モデル）に応じて、&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt; &lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt; の&lt;/a&gt;いずれかを使用してカテゴリをエンコードします。この前処理を容易にするために、2つのパイプラインを作成します。データを使用する準備ができており、前処理が必要ない場合は、このセクションをスキップできます</target>
        </trans-unit>
        <trans-unit id="f3c17036de4b5ca609e4df67622292c35365a396" translate="yes" xml:space="preserve">
          <source>Behaviour of the &lt;code&gt;decision_function&lt;/code&gt; which can be either &amp;lsquo;old&amp;rsquo; or &amp;lsquo;new&amp;rsquo;. Passing &lt;code&gt;behaviour='new'&lt;/code&gt; makes the &lt;code&gt;decision_function&lt;/code&gt; change to match other anomaly detection algorithm API which will be the default behaviour in the future. As explained in details in the &lt;code&gt;offset_&lt;/code&gt; attribute documentation, the &lt;code&gt;decision_function&lt;/code&gt; becomes dependent on the contamination parameter, in such a way that 0 becomes its natural threshold to detect outliers.</source>
          <target state="translated">「古い」または「新しい」のいずれかである &lt;code&gt;decision_function&lt;/code&gt; の動作。 &lt;code&gt;behaviour='new'&lt;/code&gt; を渡すと、 &lt;code&gt;decision_function&lt;/code&gt; が他の異常検出アルゴリズムAPIに一致するように変更されます。これは将来のデフォルトの動作になります。 &lt;code&gt;offset_&lt;/code&gt; 属性のドキュメントで詳細に説明されているように、 &lt;code&gt;decision_function&lt;/code&gt; は汚染パラメータに依存するようになり、0が外れ値を検出するための自然なしきい値になります。</target>
        </trans-unit>
        <trans-unit id="20bc631a66b3cc1b1a3392b3fbbb196ee59f89ba" translate="yes" xml:space="preserve">
          <source>Being a forward feature selection method like &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt;, orthogonal matching pursuit can approximate the optimum solution vector with a fixed number of non-zero elements:</source>
          <target state="translated">&lt;a href=&quot;#least-angle-regression&quot;&gt;最小角度回帰の&lt;/a&gt;ような前方特徴選択法であるため、直交マッチング追跡では、固定数の非ゼロ要素で最適解ベクトルを近似できます。</target>
        </trans-unit>
        <trans-unit id="305fd1e902f73a5b60bc86a4bdd6b4a6ea0e533f" translate="yes" xml:space="preserve">
          <source>Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation.</source>
          <target state="translated">以下は,Box-Cox と Yeo-Johnson をさまざまな確率分布に適用した例です.特定の分布に適用すると、乗変換は非常にガウス的な結果を得ますが、他の分布では効果がないことに注意してください。これは、変換前と変換後のデータを可視化することの重要性を強調しています。</target>
        </trans-unit>
        <trans-unit id="15f2be220fb72bee910f5e62c59989aad06195d5" translate="yes" xml:space="preserve">
          <source>Below is a summary of the classifiers supported by scikit-learn grouped by strategy; you don&amp;rsquo;t need the meta-estimators in this class if you&amp;rsquo;re using one of these, unless you want custom multiclass behavior:</source>
          <target state="translated">以下は、scikit-learnでサポートされている分類子を戦略別にまとめたものです。これらのいずれかを使用している場合は、カスタムマルチクラスの動作が必要でない限り、このクラスのメタ推定子は必要ありません。</target>
        </trans-unit>
        <trans-unit id="108ff87be25a1c1dd2316c9f1fb3ba28787755c8" translate="yes" xml:space="preserve">
          <source>Below is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file &lt;code&gt;iris.pdf&lt;/code&gt;:</source>
          <target state="translated">以下は、アイリスデータセット全体でトレーニングされた上記のツリーのgraphvizエクスポートの例です。結果は出力ファイル &lt;code&gt;iris.pdf&lt;/code&gt; に保存されます。</target>
        </trans-unit>
        <trans-unit id="f4264cf8db047b13d2c0262177f6f646451f4fbb" translate="yes" xml:space="preserve">
          <source>Below is an example of multiclass learning using Output-Codes:</source>
          <target state="translated">以下は、Output-Codeを用いたマルチクラス学習の例です。</target>
        </trans-unit>
        <trans-unit id="b49bf188ea86adad6efb34f880d43936d0fc0ac2" translate="yes" xml:space="preserve">
          <source>Below is an example of multiclass learning using OvO:</source>
          <target state="translated">以下は、OvOを使ったマルチクラス学習の例です。</target>
        </trans-unit>
        <trans-unit id="687c4cfff2712863f68152405d4b2638d130c286" translate="yes" xml:space="preserve">
          <source>Below is an example of multiclass learning using OvR:</source>
          <target state="translated">以下は、OvRを用いたマルチクラス学習の例です。</target>
        </trans-unit>
        <trans-unit id="c80585dafc7b9164da5261a6f83e5756b02054c8" translate="yes" xml:space="preserve">
          <source>Below is an example of multioutput classification:</source>
          <target state="translated">以下、マルチ出力の分類例です。</target>
        </trans-unit>
        <trans-unit id="5323f7dfcb00efe99d53b8ad7c93f56a3771bb8d" translate="yes" xml:space="preserve">
          <source>Below is an example of multioutput regression:</source>
          <target state="translated">以下、多出力回帰の例です。</target>
        </trans-unit>
        <trans-unit id="c1920b9ad7a1724faf2c50a13254e745783c4989" translate="yes" xml:space="preserve">
          <source>Below is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain most variance:</source>
          <target state="translated">以下は、4つの特徴からなる虹彩データセットの例で、最も分散を説明する2次元に投影されています。</target>
        </trans-unit>
        <trans-unit id="fe179d558d43e31394bc90d2c2bd912f3d7ad712" translate="yes" xml:space="preserve">
          <source>Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&amp;rsquo;, Wiley, 1980. 244-261.</source>
          <target state="translated">Belsley、Kuh＆Welsch、「回帰診断：影響力のあるデータと共線性の原因の特定」、Wiley、1980。244-261。</target>
        </trans-unit>
        <trans-unit id="9f292c5936105126748d157311146c4c77a1cc65" translate="yes" xml:space="preserve">
          <source>Benchmark classifiers</source>
          <target state="translated">ベンチマーク分類器</target>
        </trans-unit>
        <trans-unit id="fe07a59cb91f8e159239a44b010528ab1c647c88" translate="yes" xml:space="preserve">
          <source>Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012)</source>
          <target state="translated">Bergstra,J.,and Bengio,Y.,超パラメータ最適化のためのランダム探索,機械学習研究ジャーナル (2012)</target>
        </trans-unit>
        <trans-unit id="acb0e0e454302529b80959f07b2776ace520ba67" translate="yes" xml:space="preserve">
          <source>Bernhard Schoelkopf, Alexander J. Smola, and Klaus-Robert Mueller. 1999. Kernel principal component analysis. In Advances in kernel methods, MIT Press, Cambridge, MA, USA 327-352.</source>
          <target state="translated">Bernhard Schoelkopf、Alexander J.Smola、Klaus-Robert Mueller。1999.カーネル主成分分析。カーネル法の進歩、MITプレス、ケンブリッジ、マサチューセッツ州、米国 327-352。</target>
        </trans-unit>
        <trans-unit id="d8b44488165a02f44f4a67e6ba1ce39e5b9c9bb9" translate="yes" xml:space="preserve">
          <source>Bernoulli Restricted Boltzmann Machine (RBM).</source>
          <target state="translated">ベルヌーイ制限ボルツマンマシン(RBM)。</target>
        </trans-unit>
        <trans-unit id="5923a4b6b18db19ea579977a7a07a6737359d838" translate="yes" xml:space="preserve">
          <source>Besides scikit-learn, NumPy and SciPy also use BLAS internally, as explained earlier.</source>
          <target state="translated">scikit-learn以外にも、NumPyやSciPyでは、先ほど説明したようにBLASを内部で使用しています。</target>
        </trans-unit>
        <trans-unit id="7d9a50881bd5d34faa0a8f3ad342e9a7c84f5b08" translate="yes" xml:space="preserve">
          <source>Best fitted model (copy of the &lt;code&gt;base_estimator&lt;/code&gt; object).</source>
          <target state="translated">最適なモデル（ &lt;code&gt;base_estimator&lt;/code&gt; オブジェクトのコピー）。</target>
        </trans-unit>
        <trans-unit id="12a49239d2ca4095c10a87ae8e694332610beadd" translate="yes" xml:space="preserve">
          <source>Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">最良のスコアは1.0で,負のスコアになることがある(モデルが任意に悪くなることがあるので).入力特徴量を無視して常にyの期待値を予測する定数モデルは、R^2スコア0.0を取得します。</target>
        </trans-unit>
        <trans-unit id="792cb529b422a1c11c9b2f8bb241ed31b32984d9" translate="yes" xml:space="preserve">
          <source>Best possible score is 1.0, lower values are worse.</source>
          <target state="translated">ベストスコアは1.0、数値が低いほど悪い。</target>
        </trans-unit>
        <trans-unit id="28c6b37b189c99aed8957aff1f021f2f9f59464b" translate="yes" xml:space="preserve">
          <source>Beta-divergence loss functions</source>
          <target state="translated">ベータ発散損失関数</target>
        </trans-unit>
        <trans-unit id="d2c95a0bd9e48b10416c4f3dc812e08319356fa2" translate="yes" xml:space="preserve">
          <source>Beware not to use a regression scoring function with a classification problem, you will get useless results.</source>
          <target state="translated">分類問題で回帰スコアリング関数を使用しないように注意してください。</target>
        </trans-unit>
        <trans-unit id="a9f3a13f44f7a1a81c6757a16dde70a0eb1d4b70" translate="yes" xml:space="preserve">
          <source>Bias</source>
          <target state="translated">Bias</target>
        </trans-unit>
        <trans-unit id="ba237b5bed1b1a1e028b039eda8969e7ed30ca0a" translate="yes" xml:space="preserve">
          <source>Bias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias-variance_dilemma&quot;&gt;Bias-variance dilemma&lt;/a&gt;). Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.</source>
          <target state="translated">バイアスと分散は推定器の固有の特性であり、通常、バイアスと分散の両方ができるだけ低くなるように学習アルゴリズムとハイパーパラメーターを選択する必要があります（&lt;a href=&quot;https://en.wikipedia.org/wiki/Bias-variance_dilemma&quot;&gt;バイアス分散のジレンマを&lt;/a&gt;参照）。モデルの分散を減らす別の方法は、より多くのトレーニングデータを使用することです。ただし、真の関数が複雑すぎて分散が小さい推定器で近似できない場合にのみ、より多くのトレーニングデータを収集する必要があります。</target>
        </trans-unit>
        <trans-unit id="06f19409a98653c678fe6ff3c120895b283ca5ff" translate="yes" xml:space="preserve">
          <source>Bias-variance trade-off when setting the shrinkage: comparing the choices of Ledoit-Wolf and OAS estimators</source>
          <target state="translated">縮尺設定時のバイアス-分散トレードオフ:Ledoit-WolfとOAS推定量の選択の比較</target>
        </trans-unit>
        <trans-unit id="7c5f1fb15e060b340fb270b747d162cde9961929" translate="yes" xml:space="preserve">
          <source>Bias.</source>
          <target state="translated">Bias.</target>
        </trans-unit>
        <trans-unit id="e26ae344044922af518669ed7912f3779c4b00f9" translate="yes" xml:space="preserve">
          <source>Bias:</source>
          <target state="translated">Bias:</target>
        </trans-unit>
        <trans-unit id="6b339e821e48cfc38068b641fe05c82a58e3e342" translate="yes" xml:space="preserve">
          <source>Biases of the hidden units.</source>
          <target state="translated">隠れたユニットのバイアス。</target>
        </trans-unit>
        <trans-unit id="e50dfa9d24499c67dd80dc7ae0f62d209963644d" translate="yes" xml:space="preserve">
          <source>Biases of the visible units.</source>
          <target state="translated">目に見える単位のバイアス。</target>
        </trans-unit>
        <trans-unit id="d4404195d10795d2fac0ea59f4bd8efbaf381e98" translate="yes" xml:space="preserve">
          <source>Biclustering</source>
          <target state="translated">Biclustering</target>
        </trans-unit>
        <trans-unit id="a252b47dd9e6fba33ce507e3a8e6ec1bb1e9c7d0" translate="yes" xml:space="preserve">
          <source>Biclustering can be performed with the module &lt;a href=&quot;classes#module-sklearn.cluster.bicluster&quot;&gt;&lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt;&lt;/a&gt;. Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties.</source>
          <target state="translated">バイクラスタリングは、&lt;a href=&quot;classes#module-sklearn.cluster.bicluster&quot;&gt; &lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt; &lt;/a&gt;モジュールで実行できます。バイクラスタリングアルゴリズムは、データマトリックスの行と列を同時にクラスター化します。これらの行と列のクラスターは、バイクラスターと呼ばれます。それぞれが、いくつかの望ましいプロパティを持つ元のデータマトリックスのサブマトリックスを決定します。</target>
        </trans-unit>
        <trans-unit id="edf2774b82282aef622b14eeaa9c74d15c4f211e" translate="yes" xml:space="preserve">
          <source>Biclustering can be performed with the module &lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt;. Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties.</source>
          <target state="translated">バイクラスタリングは、モジュール &lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt; を使用して実行できます。バイクラスタリングアルゴリズムは、データマトリックスの行と列を同時にクラスター化します。これらの行と列のクラスターは、バイクラスターと呼ばれます。それぞれが、いくつかの望ましいプロパティを持つ元のデータ行列の部分行列を決定します。</target>
        </trans-unit>
        <trans-unit id="ec46a574c9badafd039dd72d03169fe1273b5603" translate="yes" xml:space="preserve">
          <source>Biclustering documents with the Spectral Co-clustering algorithm</source>
          <target state="translated">スペクトルコクラスタリングアルゴリズムを用いた文書のバイクラスタリング</target>
        </trans-unit>
        <trans-unit id="ee86c7bb4d236ce105b92fe63ada05dd4ec5870a" translate="yes" xml:space="preserve">
          <source>Biclustering has many other names in different fields including co-clustering, two-mode clustering, two-way clustering, block clustering, coupled two-way clustering, etc. The names of some algorithms, such as the Spectral Co-Clustering algorithm, reflect these alternate names.</source>
          <target state="translated">バイクラスタリングには、コクラスタリング、2モードクラスタリング、双方向クラスタリング、ブロッククラスタリング、結合された双方向クラスタリングなど、様々な分野で他の多くの名称があります。Spectral Co-Clustering アルゴリズムのようないくつかのアルゴリズムの名前は、これらの代替の名前を反映しています。</target>
        </trans-unit>
        <trans-unit id="9dbdcbb1f58605adc858ff7ea9bd66e335bd0acc" translate="yes" xml:space="preserve">
          <source>Biclustering metrics</source>
          <target state="translated">バイクラスタリングのメトリクス</target>
        </trans-unit>
        <trans-unit id="d9cf1c1fb4521641a79265f68d061558d2a8f977" translate="yes" xml:space="preserve">
          <source>Bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">大きい方が良い、すなわち、大きな値はインライアに対応しています。</target>
        </trans-unit>
        <trans-unit id="8a391f29f990251d22520f34bdb4ac600bbfd771" translate="yes" xml:space="preserve">
          <source>Bin continuous data into intervals.</source>
          <target state="translated">連続したデータをインターバルにビン詰めします。</target>
        </trans-unit>
        <trans-unit id="271e5dda8b524cc22b71bebf5401052359debca7" translate="yes" xml:space="preserve">
          <source>Binarization is a common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance.</source>
          <target state="translated">二値化は、テキストカウントデータの一般的な操作で、分析者は、例えば、発生回数を定量化するのではなく、特徴の有無のみを考慮することができます。</target>
        </trans-unit>
        <trans-unit id="98a952cd8923e662b6dc4cb343f8e07b8c748673" translate="yes" xml:space="preserve">
          <source>Binarize data (set feature values to 0 or 1) according to a threshold</source>
          <target state="translated">閾値に応じてデータを二値化(特徴量を0または1に設定)します。</target>
        </trans-unit>
        <trans-unit id="e6093920f16b7a0a90021c0a6f1b3ba3a339b537" translate="yes" xml:space="preserve">
          <source>Binarize each element of X</source>
          <target state="translated">X の各要素を 2 値化</target>
        </trans-unit>
        <trans-unit id="0b5b7e155ae5a5495c55eaab6001751a5f50a1e4" translate="yes" xml:space="preserve">
          <source>Binarize labels in a one-vs-all fashion</source>
          <target state="translated">ワンvsオールでラベルを二値化する</target>
        </trans-unit>
        <trans-unit id="44b97fdbb0f9ad559fc3a3794b99c9bc30004c1c" translate="yes" xml:space="preserve">
          <source>Binarizes labels in a one-vs-all fashion.</source>
          <target state="translated">ラベルを一対一で二値化する。</target>
        </trans-unit>
        <trans-unit id="2c6db33026290c5eceb25ad459dd62fbc5de5524" translate="yes" xml:space="preserve">
          <source>Binary and multiclass labels are supported. Only in the binary case does this relate to information about true and false positives and negatives. See references below.</source>
          <target state="translated">バイナリラベルとマルチクラスラベルがサポートされています。バイナリの場合のみ、これは真・偽陽性・陰性に関する情報に関係します。以下の参考文献を参照してください。</target>
        </trans-unit>
        <trans-unit id="76f6ce139477b2ff40abfa10ccd09f75138f8a5a" translate="yes" xml:space="preserve">
          <source>Binary array containing the code of each class.</source>
          <target state="translated">各クラスのコードを含むバイナリ配列。</target>
        </trans-unit>
        <trans-unit id="d5752168a3a19813275e09ded6a3644b6b365056" translate="yes" xml:space="preserve">
          <source>Binary indicators for missing values.</source>
          <target state="translated">欠損値のバイナリー指標。</target>
        </trans-unit>
        <trans-unit id="d20fae796db569b25a51c85d3dbd034384870434" translate="yes" xml:space="preserve">
          <source>Binary probability estimates for loss=&amp;rdquo;modified_huber&amp;rdquo; are given by (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions it is necessary to perform proper probability calibration by wrapping the classifier with &lt;a href=&quot;sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;sklearn.calibration.CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">loss =&amp;rdquo; modified_huber&amp;rdquo;のバイナリ確率推定は、（clip（decision_function（X）、-1、1）+ 1）/ 2で与えられます。他の損失関数の場合、分類器を&lt;a href=&quot;sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;sklearn.calibration.CalibratedClassifierCV&lt;/code&gt; &lt;/a&gt;ラップして、適切な確率キャリブレーションを実行する必要があります。代わりに、calibration.CalibratedClassifierCV。</target>
        </trans-unit>
        <trans-unit id="6ea0caebd3956174ca222001571ec7f4a771808d" translate="yes" xml:space="preserve">
          <source>Binary target values.</source>
          <target state="translated">バイナリのターゲット値。</target>
        </trans-unit>
        <trans-unit id="e17841c821731bb47f4b19f4849e509ece4c5657" translate="yes" xml:space="preserve">
          <source>Binary targets transform to a column vector</source>
          <target state="translated">バイナリターゲットは列ベクトルに変換されます</target>
        </trans-unit>
        <trans-unit id="2a2767f039bc988b0f4c4e7163ccdacd052867c1" translate="yes" xml:space="preserve">
          <source>Binding of the cross-validation routine (low-level routine)</source>
          <target state="translated">クロスバリデーションルーチン(低レベルルーチン)のバインディング</target>
        </trans-unit>
        <trans-unit id="64fb71b36aba4bcefad2e5d527659e300a2b2c01" translate="yes" xml:space="preserve">
          <source>Binomial deviance (&lt;code&gt;'deviance'&lt;/code&gt;): The negative binomial log-likelihood loss function for binary classification (provides probability estimates). The initial model is given by the log odds-ratio.</source>
          <target state="translated">二項逸脱度（ &lt;code&gt;'deviance'&lt;/code&gt; ）：バイナリ分類の負の二項対数尤度損失関数（確率推定値を提供）。初期モデルは、対数オッズ比で与えられます。</target>
        </trans-unit>
        <trans-unit id="7eef6382001e9a152cc75ac79b202341870bb6db" translate="yes" xml:space="preserve">
          <source>Birch</source>
          <target state="translated">Birch</target>
        </trans-unit>
        <trans-unit id="c8804a672e163b7d85424df4080099668af078fd" translate="yes" xml:space="preserve">
          <source>Birch does not scale very well to high dimensional data. As a rule of thumb if &lt;code&gt;n_features&lt;/code&gt; is greater than twenty, it is generally better to use MiniBatchKMeans.</source>
          <target state="translated">バーチは、高次元のデータにうまく対応できません。経験則として、 &lt;code&gt;n_features&lt;/code&gt; が20より大きい場合は、通常、MiniBatchKMeansを使用することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="055a71a716f70327eef9729a95cd9e98a091a68c" translate="yes" xml:space="preserve">
          <source>Bishop, &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern recognition and machine learning&lt;/a&gt;, chapter 7 Sparse Kernel Machines</source>
          <target state="translated">ビショップ、&lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;パターン認識と機械学習&lt;/a&gt;、第7章スパースカーネルマシン</target>
        </trans-unit>
        <trans-unit id="35b99b7929918827477b2be5f13db5f62d5bed94" translate="yes" xml:space="preserve">
          <source>Bishop, Christopher M. (2006). &amp;ldquo;Pattern recognition and machine learning&amp;rdquo;. Vol. 4 No. 4. New York: Springer.</source>
          <target state="translated">ビショップ、クリストファーM.（2006）。「パターン認識と機械学習」。巻。4第4。ニューヨーク：スプリンガー。</target>
        </trans-unit>
        <trans-unit id="3ee00b8c0e8ff24c1a59dc452e853cf83ab3d2bf" translate="yes" xml:space="preserve">
          <source>Blei, David M. and Michael I. Jordan. (2006). &amp;ldquo;Variational inference for Dirichlet process mixtures&amp;rdquo;. Bayesian analysis 1.1</source>
          <target state="translated">ブライ、デビッドM.、マイケルI.ジョーダン。（2006）。「ディリクレプロセス混合物の変分推論」。ベイジアン分析1.1</target>
        </trans-unit>
        <trans-unit id="66370792731dff7f31408706eed1fd3ef5297d11" translate="yes" xml:space="preserve">
          <source>Blind source separation using FastICA</source>
          <target state="translated">FastICAによるブラインドソース分離</target>
        </trans-unit>
        <trans-unit id="7d44bc449c2a26374800a503f10f3d8949505f40" translate="yes" xml:space="preserve">
          <source>Blue</source>
          <target state="translated">Blue</target>
        </trans-unit>
        <trans-unit id="d1411ae3cdd27fda5ea57577c408be223f586cf0" translate="yes" xml:space="preserve">
          <source>Body mass index</source>
          <target state="translated">体積指数</target>
        </trans-unit>
        <trans-unit id="eeb4978eef8f0138e90d13575223e62053c06fa3" translate="yes" xml:space="preserve">
          <source>Bonus point if the utility is able to give a confidence level for its predictions.</source>
          <target state="translated">ユーティリティーがその予測に信頼度を与えることができればボーナスポイントとなります。</target>
        </trans-unit>
        <trans-unit id="20ee87c5c904919ec390ea5b4c0a66ba0775ae58" translate="yes" xml:space="preserve">
          <source>BonusMalus</source>
          <target state="translated">BonusMalus</target>
        </trans-unit>
        <trans-unit id="3605ba73833c24e0fbda3c252c7bb0e601382c92" translate="yes" xml:space="preserve">
          <source>Boolean flag indicating wether the output of &lt;code&gt;transform&lt;/code&gt; is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the &lt;code&gt;sparse_threshold&lt;/code&gt; keyword.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; の出力がスパース行列または密なナンピー配列であるかどうかを示すブールフラグ。これは、個々のトランスフォーマーの出力と &lt;code&gt;sparse_threshold&lt;/code&gt; キーワードに依存します。</target>
        </trans-unit>
        <trans-unit id="c9d24ce1f33e1a2da8b7d2e8d02488db3a1a0ff6" translate="yes" xml:space="preserve">
          <source>Boolean flag indicating whether the output of &lt;code&gt;transform&lt;/code&gt; is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the &lt;code&gt;sparse_threshold&lt;/code&gt; keyword.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; の出力がスパース行列であるか密なnumpy配列であるかを示すブールフラグ。これは、個々のトランスフォーマーの出力と &lt;code&gt;sparse_threshold&lt;/code&gt; キーワードに依存します。</target>
        </trans-unit>
        <trans-unit id="dd883a95149825523369daab7f0f8ca0aca6e0ac" translate="yes" xml:space="preserve">
          <source>Boolean mask of inliers classified as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; に分類されたインライアのブールマスク。</target>
        </trans-unit>
        <trans-unit id="d2ef2625261226f9417113c581f3bab51f99fc11" translate="yes" xml:space="preserve">
          <source>Boolean mask or indices for test set.</source>
          <target state="translated">テストセットのブール値マスクまたはインデックス。</target>
        </trans-unit>
        <trans-unit id="30de559b973b97451b964227c7184d452eca2270" translate="yes" xml:space="preserve">
          <source>Boolean mask or indices for training set.</source>
          <target state="translated">学習セットのブール値マスクまたはインデックス.</target>
        </trans-unit>
        <trans-unit id="a38162711f2499e7a5519304c06ab030205ccd3e" translate="yes" xml:space="preserve">
          <source>Boolean mask or list of indices (as returned by the get_support member of feature selectors).</source>
          <target state="translated">ブール値のマスクまたはインデックスのリスト(フィーチャセレクタのget_supportメンバによって返されます)。</target>
        </trans-unit>
        <trans-unit id="c3ff5ac4f1442b62c49b82219fbca08a9aaf6fc5" translate="yes" xml:space="preserve">
          <source>Boolean thresholding of array-like or scipy.sparse matrix</source>
          <target state="translated">配列のような行列や scipy.sparse 行列のブール値のしきい値化</target>
        </trans-unit>
        <trans-unit id="52747050c0ad2a1de070c473421346e630cdac3f" translate="yes" xml:space="preserve">
          <source>Both &amp;lsquo;ascii&amp;rsquo; and &amp;lsquo;unicode&amp;rsquo; use NFKD normalization from &lt;a href=&quot;https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize&quot;&gt;&lt;code&gt;unicodedata.normalize&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">「ascii」と「unicode」はどちらも、&lt;a href=&quot;https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize&quot;&gt; &lt;code&gt;unicodedata.normalize&lt;/code&gt; &lt;/a&gt;からのNFKD正規化を使用します。</target>
        </trans-unit>
        <trans-unit id="cc08d18c56138ef4e57769f94344c2fcddc0c71c" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; create multiclass datasets by allocating each class one or more normally-distributed clusters of points. &lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. &lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt; &lt;code&gt;make_blobs&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt; &lt;code&gt;make_classification&lt;/code&gt; は&lt;/a&gt;どちらも、各クラスに1つ以上の正規分布の点のクラスターを割り当てることにより、マルチクラスデータセットを作成します。&lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt; &lt;code&gt;make_blobs&lt;/code&gt; &lt;/a&gt;は、各クラスターの中心と標準偏差に関するより優れた制御を提供し、クラスター化を示すために使用されます。&lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt; &lt;code&gt;make_classification&lt;/code&gt; &lt;/a&gt;は、次の方法でノイズを導入することを専門としています。相関、冗長、情報量の少ない機能。クラスごとに複数のガウスクラスター。特徴空間の線形変換。</target>
        </trans-unit>
        <trans-unit id="4b01e39314cd0b6e82319a49bae49952bbdf5790" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; support &lt;code&gt;warm_start=True&lt;/code&gt; which allows you to add more estimators to an already fitted model.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; は&lt;/a&gt;どちらも &lt;code&gt;warm_start=True&lt;/code&gt; をサポートしています。これにより、既に適合したモデルにさらに推定器を追加できます。</target>
        </trans-unit>
        <trans-unit id="0fdd261ec2b6eab839fe6617683b34303696fb85" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; can be used in a Pipeline as a way to build a composite estimator that supports imputation. See &lt;a href=&quot;../auto_examples/impute/plot_missing_values#sphx-glr-auto-examples-impute-plot-missing-values-py&quot;&gt;Imputing missing values before building an estimator&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;IterativeImputer&lt;/code&gt; の&lt;/a&gt;両方を、代入をサポートする複合推定量を構築する方法としてパイプラインで使用できます。&lt;a href=&quot;../auto_examples/impute/plot_missing_values#sphx-glr-auto-examples-impute-plot-missing-values-py&quot;&gt;推定量を作成する前に欠測値&lt;/a&gt;を代入するを参照してください。</target>
        </trans-unit>
        <trans-unit id="cf6c6c4310ce5d3e1dff50dd933fcb1d3d517f90" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt;&lt;code&gt;MLPRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neural_network.mlpclassifier#sklearn.neural_network.MLPClassifier&quot;&gt;&lt;code&gt;MLPClassifier&lt;/code&gt;&lt;/a&gt; use parameter &lt;code&gt;alpha&lt;/code&gt; for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes. Following plot displays varying decision function with value of alpha.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt; &lt;code&gt;MLPRegressor&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.neural_network.mlpclassifier#sklearn.neural_network.MLPClassifier&quot;&gt; &lt;code&gt;MLPClassifier&lt;/code&gt; &lt;/a&gt;はどちらも、正則化（L2正則化）項にパラメーター &lt;code&gt;alpha&lt;/code&gt; を使用します。これは、大きなマグニチュードの重みにペナルティを課すことで過剰適合を回避するのに役立ちます。次のプロットは、アルファの値とともに変化する決定関数を表示します。</target>
        </trans-unit>
        <trans-unit id="b53b3117e47a61020f59f56f0ab84443cf5a1cc4" translate="yes" xml:space="preserve">
          <source>Both &lt;strong&gt;tf&lt;/strong&gt; and &lt;strong&gt;tf&amp;ndash;idf&lt;/strong&gt; can be computed as follows using &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;strong&gt;tf&lt;/strong&gt;と&lt;strong&gt;tf&amp;ndash;idfの&lt;/strong&gt;両方は、&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt;を使用して次のように計算できます。</target>
        </trans-unit>
        <trans-unit id="4bf93c5f6114d3860df00831377e70ae0b54b0c9" translate="yes" xml:space="preserve">
          <source>Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.</source>
          <target state="translated">顔認証と顔認識は,通常,顔検出を実行するために訓練されたモデルの出力に対して実行されるタスクです.顔検出のための最も一般的なモデルはViola-Jonesと呼ばれ、OpenCVライブラリで実装されています。LFWの顔は、この顔検出器によって様々なオンラインサイトから抽出されたものです。</target>
        </trans-unit>
        <trans-unit id="ca7325084a8f64bcbbcfe176b54a08f8e59fb609" translate="yes" xml:space="preserve">
          <source>Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data \(P(X|y=k)\) for each class \(k\). Predictions can then be obtained by using Bayes&amp;rsquo; rule, for each training sample \(x \in \mathcal{R}^d\):</source>
          <target state="translated">LDAとQDAはどちらも、各クラス\（k \）のデータ\（P（X | y = k）\）のクラス条件付き分布をモデル化する単純な確率モデルから導出できます。次に、ベイズの定理を使用して、トレーニングサンプルごとに予測を取得できます\（x \ in \ mathcal {R} ^ d \）：</target>
        </trans-unit>
        <trans-unit id="91e41139595583cca1f08b85e6f25218cb1d806a" translate="yes" xml:space="preserve">
          <source>Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data \(P(X|y=k)\) for each class \(k\). Predictions can then be obtained by using Bayes&amp;rsquo; rule:</source>
          <target state="translated">LDAとQDAの両方は、各クラス\（k \）のデータ\（P（X | y = k）\）のクラス条件付き分布をモデル化する単純な確率モデルから導出できます。次に、ベイズの規則を使用して予測を取得できます。</target>
        </trans-unit>
        <trans-unit id="88ef4403ed65b61f6ce7d70445c003a80e194980" translate="yes" xml:space="preserve">
          <source>Both a large or small &lt;code&gt;leaf_size&lt;/code&gt; can lead to suboptimal query cost. For &lt;code&gt;leaf_size&lt;/code&gt; approaching 1, the overhead involved in traversing nodes can significantly slow query times. For &lt;code&gt;leaf_size&lt;/code&gt; approaching the size of the training set, queries become essentially brute force. A good compromise between these is &lt;code&gt;leaf_size = 30&lt;/code&gt;, the default value of the parameter.</source>
          <target state="translated">&lt;code&gt;leaf_size&lt;/code&gt; が大きくても小さくても、クエリコストが最適化されない可能性があります。 &lt;code&gt;leaf_size&lt;/code&gt; 1に近づいて、オーバーヘッドは、ノードを大幅にスロークエリ時間ができ横断に関与。 &lt;code&gt;leaf_size&lt;/code&gt; トレーニングセットのサイズに近づいて、クエリは、本質的に強引になります。これらの間の適切な妥協点は、パラメータのデフォルト値である &lt;code&gt;leaf_size = 30&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="fa5698e1f194999c5b948025698f212260de6074" translate="yes" xml:space="preserve">
          <source>Both for the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_people#sklearn.datasets.fetch_lfw_people&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_people&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt;&lt;/a&gt; function it is possible to get an additional dimension with the RGB color channels by passing &lt;code&gt;color=True&lt;/code&gt;, in that case the shape will be &lt;code&gt;(2200, 2, 62, 47, 3)&lt;/code&gt;.</source>
          <target state="translated">両方&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_people#sklearn.datasets.fetch_lfw_people&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_people&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt; は、&lt;/a&gt;通過することにより、RGBカラーチャンネルを追加のディメンションを取得することができる機能 &lt;code&gt;color=True&lt;/code&gt; その場合形状になり、 &lt;code&gt;(2200, 2, 62, 47, 3)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f849c057a89abeca98a91c38c141c4422d4b9044" translate="yes" xml:space="preserve">
          <source>Both kernel ridge regression (KRR) and GPR learn a target function by employing internally the &amp;ldquo;kernel trick&amp;rdquo;. KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction.</source>
          <target state="translated">カーネルリッジ回帰（KRR）とGPRはどちらも、「カーネルトリック」を内部的に使用することでターゲット関数を学習します。 KRRは、元の空間の非線形関数に対応するそれぞれのカーネルによって引き起こされる空間の線形関数を学習します。カーネル空間の線形関数は、リッジ正則化による平均二乗誤差損失に基づいて選択されます。 GPRは、カーネルを使用してターゲット関数の事前分布の共分散を定義し、観測されたトレーニングデータを使用して尤度関数を定義します。ベイズの定理に基づいて、ターゲット関数の（ガウス）事後分布が定義され、その平均が予測に使用されます。</target>
        </trans-unit>
        <trans-unit id="23db30f840724e288d4f8f1371ac596206d41d32" translate="yes" xml:space="preserve">
          <source>Both kernel ridge regression (KRR) and Gaussian process regression (GPR) learn a target function by employing internally the &amp;ldquo;kernel trick&amp;rdquo;. KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction.</source>
          <target state="translated">カーネルリッジ回帰（KRR）とガウスプロセス回帰（GPR）はどちらも、内部的に「カーネルトリック」を使用することでターゲット関数を学習します。 KRRは、元の空間の非線形関数に対応するそれぞれのカーネルによって引き起こされる空間の線形関数を学習します。カーネル空間の線形関数は、リッジ正則化による平均二乗誤差損失に基づいて選択されます。 GPRは、カーネルを使用してターゲット関数の事前分布の共分散を定義し、観測されたトレーニングデータを使用して尤度関数を定義します。ベイズの定理に基づいて、ターゲット関数の（ガウス）事後分布が定義され、その平均が予測に使用されます。</target>
        </trans-unit>
        <trans-unit id="a2ae11bd288fcc4a889903f5cd55e2c7dc5062c9" translate="yes" xml:space="preserve">
          <source>Both kernel ridge regression (KRR) and SVR learn a non-linear function by employing the kernel trick, i.e., they learn a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. They differ in the loss functions (ridge versus epsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR at prediction-time.</source>
          <target state="translated">カーネルリッジ回帰(KRR)もSVRもカーネルトリックを用いて非線形関数を学習します。これらは損失関数(リッジとイプシロンに依存しない損失)が異なります。SVRとは対照的に、KRRのフィッティングは閉形式で行うことができ、中規模のデータセットでは一般的に高速である。一方、学習されたモデルは非分散であるため、予測時にはSVRよりも遅くなります。</target>
        </trans-unit>
        <trans-unit id="9f6802fa6da263e373ed1a0b728e154415b2fd58" translate="yes" xml:space="preserve">
          <source>Both kinds of calibration can fix this issue and yield nearly identical results. This shows that sigmoid calibration can deal with situations where the calibration curve of the base classifier is sigmoid (e.g., for LinearSVC) but not where it is transposed-sigmoid (e.g., Gaussian naive Bayes).</source>
          <target state="translated">どちらのキャリブレーションもこの問題を修正でき、ほぼ同じ結果が得られます。このことは、シグモイド校正が、基本分類器の校正曲線がシグモイド(例えば、LinearSVCの場合)ではシグモイドであっても、トランスポーズされたシグモイド(例えば、ガウスナイーブベイズ)ではシグモイドではない状況に対応できることを示しています。</target>
        </trans-unit>
        <trans-unit id="708135c11a370a819f586687e493e8e334b863bb" translate="yes" xml:space="preserve">
          <source>Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters.</source>
          <target state="translated">線形モデルはどちらも線形決定境界(交差する双平面)を持つが、非線形カーネルモデル(多項式またはガウスRBF)は、カーネルの種類とそのパラメータに依存する形状を持つ、より柔軟な非線形決定境界を持つ。</target>
        </trans-unit>
        <trans-unit id="26e0a0e806824f91854b61cb523786b2b1be2f09" translate="yes" xml:space="preserve">
          <source>Both loaders and fetchers functions return a &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt;&lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;&lt;/a&gt; object holding at least two items: an array of shape &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; with key &lt;code&gt;data&lt;/code&gt; (except for 20newsgroups) and a numpy array of length &lt;code&gt;n_samples&lt;/code&gt;, containing the target values, with key &lt;code&gt;target&lt;/code&gt;.</source>
          <target state="translated">ローダー関数とフェッチャー関数の両方が、少なくとも2つのアイテムを保持する&lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt; &lt;code&gt;sklearn.utils.Bunch&lt;/code&gt; &lt;/a&gt;オブジェクトを返します。キー &lt;code&gt;data&lt;/code&gt; 形状 &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; の配列（20newsgroupsを除く）と、ターゲット値を含む長さ &lt;code&gt;n_samples&lt;/code&gt; のnumpy配列とキー &lt;code&gt;target&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fc56722e0a950bb51331dfd1a0000b2f92f9cd59" translate="yes" xml:space="preserve">
          <source>Both loaders and fetchers functions return a dictionary-like object holding at least two items: an array of shape &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; with key &lt;code&gt;data&lt;/code&gt; (except for 20newsgroups) and a numpy array of length &lt;code&gt;n_samples&lt;/code&gt;, containing the target values, with key &lt;code&gt;target&lt;/code&gt;.</source>
          <target state="translated">ローダーとフェッチャーの両方の関数は、少なくとも2つの項目を保持する辞書のようなオブジェクトを返します。キー &lt;code&gt;data&lt;/code&gt; （ &lt;code&gt;n_features&lt;/code&gt; を除く）を含む形状 &lt;code&gt;n_samples&lt;/code&gt; * n_featuresの配列と、ターゲット値を含み、キー &lt;code&gt;target&lt;/code&gt; を含む長さ &lt;code&gt;n_samples&lt;/code&gt; の numpy配列。</target>
        </trans-unit>
        <trans-unit id="c429ee44d91d629f81b84bf7db1e8151de09efd7" translate="yes" xml:space="preserve">
          <source>Both methods are compared in a regression problem using a BayesianRidge as supervised estimator.</source>
          <target state="translated">両手法は、教師付き推定器としてBayesianRidgeを用いた回帰問題で比較されています。</target>
        </trans-unit>
        <trans-unit id="1f719d18cf26f246fcd8f594b634dc25f50e93c4" translate="yes" xml:space="preserve">
          <source>Both models are able to rank policyholders by risky-ness significantly better than chance although they are also both far from perfect due to the natural difficulty of the prediction problem from few features.</source>
          <target state="translated">どちらのモデルも、少ない特徴量からの予測問題の自然な難しさのために、完全とは程遠いものの、偶然よりも有意に優れたリスク度による保険契約者のランク付けを行うことができます。</target>
        </trans-unit>
        <trans-unit id="9c567880fbb130b2f00326a7eed156cd91692677" translate="yes" xml:space="preserve">
          <source>Both models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic they can be integrated in more complex models, e.g. Mixture of Factor Analysers. One gets very different models (e.g. &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt;&lt;code&gt;FastICA&lt;/code&gt;&lt;/a&gt;) if non-Gaussian priors on the latent variables are assumed.</source>
          <target state="translated">どちらのモデルも基本的に、低ランクの共分散行列でガウスを推定します。両方のモデルは確率論的であるため、より複雑なモデル、たとえば因子分析の混合に統合できます。潜在変数の非ガウスの&lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt; &lt;code&gt;FastICA&lt;/code&gt; &lt;/a&gt;分布が想定されている場合、非常に異なるモデル（FastICAなど）が得られます。</target>
        </trans-unit>
        <trans-unit id="a62e9a7de64eaa33971b5c3675cc0b062a65784c" translate="yes" xml:space="preserve">
          <source>Both models have access to five components with which to fit the data. Note that the Expectation Maximisation model will necessarily use all five components while the Variational Inference model will effectively only use as many as are needed for a good fit. Here we can see that the Expectation Maximisation model splits some components arbitrarily, because it is trying to fit too many components, while the Dirichlet Process model adapts it number of state automatically.</source>
          <target state="translated">どちらのモデルも、データを適合させるための5つの成分を利用することができます。期待最大化モデルは必然的に5つの成分すべてを使用するのに対し、変分推論モデルは、効果的に、良好な適合に必要な数だけを使用することに注意してください。ここでは、ディリクレ過程モデルが状態の数を自動的に適応させているのに対し、期待最大化モデルは、あまりにも多くの成分を適合させようとしているため、いくつかの成分を任意に分割していることがわかります。</target>
        </trans-unit>
        <trans-unit id="5b17a14614025b8fc55b61f491f7a477ebf75742" translate="yes" xml:space="preserve">
          <source>Both scores have positive values between 0.0 and 1.0, larger values being desirable.</source>
          <target state="translated">どちらのスコアも0.0から1.0の間の正の値を持ち、より大きな値が望ましい。</target>
        </trans-unit>
        <trans-unit id="be67d7e7cbf7e3b7b92ab971146cc493ee899201" translate="yes" xml:space="preserve">
          <source>Box-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by \(\lambda\), which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a lognormal distribution to a normal distribution:</source>
          <target state="translated">Box-Coxは厳密に正のデータにのみ適用できる。どちらの方法でも、変換は最尤推定によって決定される\(\lambda\)によってパラメータ化される。ここでは、対数正規分布から正規分布へのサンプルの写像にBox-Coxを使用した例を示します。</target>
        </trans-unit>
        <trans-unit id="3b31bca12137b60e3b02c5fa6fcbe997bf063055" translate="yes" xml:space="preserve">
          <source>Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.</source>
          <target state="translated">Box-Coxは入力データが厳密に正であることを要求しますが、Yeo-Johnsonは正または負の両方のデータをサポートしています。</target>
        </trans-unit>
        <trans-unit id="3a33122410e3d55ce36b5d7a297f38716ddc9fb2" translate="yes" xml:space="preserve">
          <source>BrayCurtisDistance</source>
          <target state="translated">BrayCurtisDistance</target>
        </trans-unit>
        <trans-unit id="9567cb56bbf5a464a471c7e7c0603402f701c1b5" translate="yes" xml:space="preserve">
          <source>Breiman, &amp;ldquo;Arcing Classifiers&amp;rdquo;, Annals of Statistics 1998.</source>
          <target state="translated">Breiman、「Arcing Classifiers」、Annals of Statistics 1998。</target>
        </trans-unit>
        <trans-unit id="b92ea166bdb68582b38ba5d28898e88166331fb5" translate="yes" xml:space="preserve">
          <source>Breiman, &amp;ldquo;Random Forests&amp;rdquo;, Machine Learning, 45(1), 5-32, 2001.</source>
          <target state="translated">ブライマン、「ランダムフォレスト」、機械学習、45（1）、5-32、2001。</target>
        </trans-unit>
        <trans-unit id="e706a24019de3d6255ea9f7b340895835c77d481" translate="yes" xml:space="preserve">
          <source>Brendan J. Frey and Delbert Dueck, &amp;ldquo;Clustering by Passing Messages Between Data Points&amp;rdquo;, Science Feb. 2007</source>
          <target state="translated">Brendan J. FreyおよびDelbert Dueck、「データポイント間でメッセージを渡すことによるクラスタリング」、Science 2007年2月</target>
        </trans-unit>
        <trans-unit id="91c2780c7ce208d81dbc70c79b98b19c560e01f7" translate="yes" xml:space="preserve">
          <source>Breunig, Kriegel, Ng, and Sander (2000) &lt;a href=&quot;http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf&quot;&gt;LOF: identifying density-based local outliers.&lt;/a&gt; Proc. ACM SIGMOD</source>
          <target state="translated">Breunig、Kriegel、Ng、およびSander（2000）&lt;a href=&quot;http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf&quot;&gt;LOF：密度ベースの局所&lt;/a&gt;的外れ値の特定。手続き ACM SIGMOD</target>
        </trans-unit>
        <trans-unit id="b62edbfb07bbbc25ba8cf8bbc25ed044eba7bbe5" translate="yes" xml:space="preserve">
          <source>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp;amp; Sander, J. (2000, May). LOF: identifying density-based local outliers. In ACM sigmod record.</source>
          <target state="translated">Breunig、MM、Kriegel、HP、Ng、RT、およびSander、J。（2000、5月）。LOF：密度ベースの局所的外れ値を識別します。ACM sigmodレコード。</target>
        </trans-unit>
        <trans-unit id="f5333384a370e2b255c6a3cf2b193f53e9cafb20" translate="yes" xml:space="preserve">
          <source>Briefly, a first-order Taylor approximation says that \(l(z) \approx l(a) + (z - a) \frac{\partial l(a)}{\partial a}\). Here, \(z\) corresponds to \(F_{m - 1}(x_i) + h_m(x_i)\), and \(a\) corresponds to \(F_{m-1}(x_i)\)</source>
          <target state="translated">簡潔に言うと、1次のテイラー近似では ここでは、\(z\)が、F\(F_{m-1}(x_i)+h_m(x_i)A\)が、F\(a\)が、F\(F_{m-1}(x_i)A\)に対応しています。</target>
        </trans-unit>
        <trans-unit id="fb0165005c94e23be947347cfac674726ddcc8f3" translate="yes" xml:space="preserve">
          <source>Brier score</source>
          <target state="translated">ブリアスコア</target>
        </trans-unit>
        <trans-unit id="ce9563b2203c4e120af29d083264cdec661286f1" translate="yes" xml:space="preserve">
          <source>Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010). The balanced accuracy and its posterior distribution. Proceedings of the 20th International Conference on Pattern Recognition, 3121-24.</source>
          <target state="translated">Brodersen,K.H.;Ong,C.S.;Stephan,K.E.;Buhmann,J.M.(2010).バランスのとれた精度とその事後分布.パターン認識に関する第20回国際会議論文集,3121-24.</target>
        </trans-unit>
        <trans-unit id="3a47350b7d7cb691a01ccff5ad5d3aca87290c28" translate="yes" xml:space="preserve">
          <source>Brown</source>
          <target state="translated">Brown</target>
        </trans-unit>
        <trans-unit id="5b66173631e06f3f28d6125b3cb26a28cb7fca86" translate="yes" xml:space="preserve">
          <source>Build a Bagging ensemble of estimators from the training</source>
          <target state="translated">学習から推定値のバギングアンサンブルを構築する</target>
        </trans-unit>
        <trans-unit id="794678e50f47205b9017d4e509f3518d6f5fadf5" translate="yes" xml:space="preserve">
          <source>Build a Bagging ensemble of estimators from the training set (X, y).</source>
          <target state="translated">学習集合(X,y)から推定値のバギングアンサンブルを構築する。</target>
        </trans-unit>
        <trans-unit id="187f33f95926319c2456d01efd577acf94dfa6ae" translate="yes" xml:space="preserve">
          <source>Build a CF Tree for the input data.</source>
          <target state="translated">入力データのCFツリーを構築します。</target>
        </trans-unit>
        <trans-unit id="1367324c6e1505253779ffa63a9ab3c72dfbf04f" translate="yes" xml:space="preserve">
          <source>Build a HTML representation of an estimator.</source>
          <target state="translated">エスティメーターのHTML表現を構築します。</target>
        </trans-unit>
        <trans-unit id="1e09fb3267f6b0d5980c9be2a9dffb892209f693" translate="yes" xml:space="preserve">
          <source>Build a boosted classifier from the training set (X, y).</source>
          <target state="translated">学習集合 (X,y)から昇圧された分類器を構築します.</target>
        </trans-unit>
        <trans-unit id="bb1580cf6761f3cb6ad38eda4b239ef5f0001320" translate="yes" xml:space="preserve">
          <source>Build a boosted regressor from the training set (X, y).</source>
          <target state="translated">学習集合 (X,y)から昇圧回帰器を構築する.</target>
        </trans-unit>
        <trans-unit id="250228e6d05087ab14d0d947456ba0aa59ea7050" translate="yes" xml:space="preserve">
          <source>Build a contingency matrix describing the relationship between labels.</source>
          <target state="translated">ラベル間の関係を記述した分割行列を作成します。</target>
        </trans-unit>
        <trans-unit id="d74890ceefdff850d4364b5427ecc2c4535dee78" translate="yes" xml:space="preserve">
          <source>Build a decision tree classifier from the training set (X, y).</source>
          <target state="translated">学習集合(X,y)から決定木分類器を構築する.</target>
        </trans-unit>
        <trans-unit id="2b1b91e53bab7f86a619b51147d8ac61611972f5" translate="yes" xml:space="preserve">
          <source>Build a decision tree regressor from the training set (X, y).</source>
          <target state="translated">学習集合(X,y)から決定木回帰器を構築します。</target>
        </trans-unit>
        <trans-unit id="255fad2483c531d4d8beb88f94f03694ee2b25aa" translate="yes" xml:space="preserve">
          <source>Build a forest of trees from the training set (X, y).</source>
          <target state="translated">訓練集合(X,y)から木の森を作る。</target>
        </trans-unit>
        <trans-unit id="54ca7aba833ca6776a4b555adf51f52c37364032" translate="yes" xml:space="preserve">
          <source>Build a text report showing the main classification metrics</source>
          <target state="translated">主な分類指標を示すテキストレポートを作成する</target>
        </trans-unit>
        <trans-unit id="0acb360cebaf298906fb4db3eabfa00d9453c9d3" translate="yes" xml:space="preserve">
          <source>Build a text report showing the main classification metrics.</source>
          <target state="translated">主な分類基準を示すテキストレポートを作成します。</target>
        </trans-unit>
        <trans-unit id="02e35795301c2dfa78447e621bba28f5e7d78fd9" translate="yes" xml:space="preserve">
          <source>Build a text report showing the rules of a decision tree.</source>
          <target state="translated">決定木のルールを示すテキストレポートを作成します。</target>
        </trans-unit>
        <trans-unit id="657256d2b304ff4131eae140a5931256c38a3879" translate="yes" xml:space="preserve">
          <source>Build or fetch the effective stop words list</source>
          <target state="translated">効果的な停止語リストの構築または取得</target>
        </trans-unit>
        <trans-unit id="c274aea56fd427bf062945c347f3184c61ce0dda" translate="yes" xml:space="preserve">
          <source>Build or fetch the effective stop words list.</source>
          <target state="translated">効果的なストップワードリストを構築または取得します。</target>
        </trans-unit>
        <trans-unit id="1696170a9053ac1d87e48bfb4ce1d22a7e994777" translate="yes" xml:space="preserve">
          <source>Building a pipeline</source>
          <target state="translated">パイプラインの構築</target>
        </trans-unit>
        <trans-unit id="d58a993cf5326c10970b9b4db170c35508c44151" translate="yes" xml:space="preserve">
          <source>Bunch objects are sometimes used as an output for functions and methods. They extend dictionaries by enabling values to be accessed by key, &lt;code&gt;bunch[&quot;value_key&quot;]&lt;/code&gt;, or by an attribute, &lt;code&gt;bunch.value_key&lt;/code&gt;.</source>
          <target state="translated">バンチオブジェクトは、関数やメソッドの出力として使用されることがあります。彼らは、キーによってアクセスされる値を有効にすることにより、辞書を拡張 &lt;code&gt;bunch[&quot;value_key&quot;]&lt;/code&gt; 、または属性によって、 &lt;code&gt;bunch.value_key&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6b9d2f152fc732c61d6802508ad1d27938fd60fa" translate="yes" xml:space="preserve">
          <source>By &lt;strong&gt;averaging&lt;/strong&gt; the estimates of predictive ability over several randomized trees one can &lt;strong&gt;reduce the variance&lt;/strong&gt; of such an estimate and use it for feature selection. This is known as the mean decrease in impurity, or MDI. Refer to &lt;a href=&quot;#l2014&quot; id=&quot;id7&quot;&gt;[L2014]&lt;/a&gt; for more information on MDI and feature importance evaluation with Random Forests.</source>
          <target state="translated">いくつかのランダム化されたツリーの予測能力の推定値を&lt;strong&gt;平均化&lt;/strong&gt;することにより、そのような推定値の&lt;strong&gt;分散&lt;/strong&gt;を&lt;strong&gt;減らし、&lt;/strong&gt;特徴選択に使用できます。これは、不純物の平均減少、またはMDIとして知られています。ランダムフォレストを使用したMDIおよび機能の重要性評価の詳細については、&lt;a href=&quot;#l2014&quot; id=&quot;id7&quot;&gt;[L2014]&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="4271aec9443f091b8f5d73284775ad1de7a16657" translate="yes" xml:space="preserve">
          <source>By contrast, in &lt;strong&gt;boosting methods&lt;/strong&gt;, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.</source>
          <target state="translated">対照的に、&lt;strong&gt;ブースティング法では&lt;/strong&gt;、基本推定量が順次作成され、結合された推定量のバイアスを低減しようとします。動機は、いくつかの弱いモデルを組み合わせて強力なアンサンブルを作成することです。</target>
        </trans-unit>
        <trans-unit id="6c32bdf454c04c997b80ade2a7001005f06bf56a" translate="yes" xml:space="preserve">
          <source>By default \(\alpha_1 = \alpha_2 = \lambda_1 = \lambda_2 = 10^{-6}\).</source>
          <target state="translated">By default ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞﾌｫﾙﾄ ﾃﾞ ﾌｫﾙﾄ</target>
        </trans-unit>
        <trans-unit id="471aed0559b4b66da1109f2dd8bd4a3412768eaf" translate="yes" xml:space="preserve">
          <source>By default all available workers will be used (&lt;code&gt;n_jobs=-1&lt;/code&gt;) unless the caller passes an explicit value for the &lt;code&gt;n_jobs&lt;/code&gt; parameter.</source>
          <target state="translated">デフォルトでは、呼び出し側が &lt;code&gt;n_jobs&lt;/code&gt; パラメータに明示的な値を渡さない限り、利用可能なすべてのワーカーが使用されます（ &lt;code&gt;n_jobs=-1&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="5f6c87aa078eb9e4a070352402cf9ac4b38861b8" translate="yes" xml:space="preserve">
          <source>By default no shuffling occurs, including for the (stratified) K fold cross- validation performed by specifying &lt;code&gt;cv=some_integer&lt;/code&gt; to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, grid search, etc. Keep in mind that &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; still returns a random split.</source>
          <target state="translated">デフォルトでは、 &lt;code&gt;cv=some_integer&lt;/code&gt; を&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; に&lt;/a&gt;指定して実行される（層別）Kフォールド交差検証、グリッド検索などを含め、シャッフルは発生しません&lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt;依然としてランダムな分割を返すことに注意してください。</target>
        </trans-unit>
        <trans-unit id="e7a2fba0300cf0e1fb0a656c1e5e19ff70aee869" translate="yes" xml:space="preserve">
          <source>By default the data dir is set to a folder named &amp;lsquo;scikit_learn_data&amp;rsquo; in the user home folder.</source>
          <target state="translated">デフォルトでは、データディレクトリは、ユーザーのホームフォルダーにある「scikit_learn_data」という名前のフォルダーに設定されています。</target>
        </trans-unit>
        <trans-unit id="78999f1442e19c854d03f2d5d120c491c2044f43" translate="yes" xml:space="preserve">
          <source>By default the estimator&amp;rsquo;s &lt;code&gt;score&lt;/code&gt; method is used to compute the individual scores.</source>
          <target state="translated">デフォルトでは、個々のスコアを計算するために推定器の &lt;code&gt;score&lt;/code&gt; 方法が使用されます。</target>
        </trans-unit>
        <trans-unit id="4b5dc2d778b41c185a986f8e9ad3bff91cf8ed7b" translate="yes" xml:space="preserve">
          <source>By default the following backends are available:</source>
          <target state="translated">デフォルトでは以下のバックエンドが利用可能です。</target>
        </trans-unit>
        <trans-unit id="c9a567aacd54237b7723e5584e83694cffe0556c" translate="yes" xml:space="preserve">
          <source>By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=&amp;rsquo;exact&amp;rsquo; will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.</source>
          <target state="translated">デフォルトでは、勾配計算アルゴリズムはO（NlogN）時間で実行されるBarnes-Hut近似を使用します。method = 'exact'は、O（N ^ 2）時間の低速で正確なアルゴリズムで実行されます。正確なアルゴリズムは、最近傍エラーを3％よりも大きくする必要がある場合に使用する必要があります。ただし、正確な方法では数百万の例に対応できません。</target>
        </trans-unit>
        <trans-unit id="572245e5c8e088605558017246250c8893d52627" translate="yes" xml:space="preserve">
          <source>By default the order will be determined by the order of columns in the label matrix Y.:</source>
          <target state="translated">デフォルトでは,ラベル行列 Y...の列の順序によって順序が決定されます.</target>
        </trans-unit>
        <trans-unit id="cab46542ee630d0794253cdc9adb3976a1835662" translate="yes" xml:space="preserve">
          <source>By default the output is one-hot encoded into a sparse matrix (See &lt;a href=&quot;#preprocessing-categorical-features&quot;&gt;Encoding categorical features&lt;/a&gt;) and this can be configured with the &lt;code&gt;encode&lt;/code&gt; parameter. For each feature, the bin edges are computed during &lt;code&gt;fit&lt;/code&gt; and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as:</source>
          <target state="translated">デフォルトでは、出力はスパース行列にワンホットエンコードされ（「&lt;a href=&quot;#preprocessing-categorical-features&quot;&gt;カテゴリ機能のエンコード&lt;/a&gt;」を参照）、これは &lt;code&gt;encode&lt;/code&gt; パラメーターで構成できます。各特徴について、ビンのエッジは &lt;code&gt;fit&lt;/code&gt; 中に計算され、ビンの数とともに、それらは間隔を定義します。したがって、現在の例では、これらの間隔は次のように定義されています。</target>
        </trans-unit>
        <trans-unit id="7638d24983f6f0c8d02846709e02c43bfb3124b9" translate="yes" xml:space="preserve">
          <source>By default, &lt;a href=&quot;generated/sklearn.decomposition.minibatchdictionarylearning#sklearn.decomposition.MiniBatchDictionaryLearning&quot;&gt;&lt;code&gt;MiniBatchDictionaryLearning&lt;/code&gt;&lt;/a&gt; divides the data into mini-batches and optimizes in an online manner by cycling over the mini-batches for the specified number of iterations. However, at the moment it does not implement a stopping condition.</source>
          <target state="translated">デフォルトでは、&lt;a href=&quot;generated/sklearn.decomposition.minibatchdictionarylearning#sklearn.decomposition.MiniBatchDictionaryLearning&quot;&gt; &lt;code&gt;MiniBatchDictionaryLearning&lt;/code&gt; &lt;/a&gt;はデータをミニバッチに分割し、指定された反復回数だけミニバッチを循環させることにより、オンラインで最適化します。ただし、現時点では停止条件は実装されていません。</target>
        </trans-unit>
        <trans-unit id="2eb2435c3e98faf455297cd059d456431f056aed" translate="yes" xml:space="preserve">
          <source>By default, &lt;code&gt;float16&lt;/code&gt; results are computed using &lt;code&gt;float32&lt;/code&gt; intermediates for extra precision.</source>
          <target state="translated">デフォルトでは、 &lt;code&gt;float16&lt;/code&gt; の結果は、精度を &lt;code&gt;float32&lt;/code&gt; ためにfloat32中間体を使用して計算されます。</target>
        </trans-unit>
        <trans-unit id="06b991078a7a86c59d05afe16f66234f800e038c" translate="yes" xml:space="preserve">
          <source>By default, LocalOutlierFactor is only meant to be used for outlier detection (novelty=False). Set novelty to True if you want to use LocalOutlierFactor for novelty detection. In this case be aware that that you should only use predict, decision_function and score_samples on new unseen data and not on the training set.</source>
          <target state="translated">既定では、LocalOutlierFactor は外れ値検出にのみ使用されます (novelty=False)。新規性の検出にLocalOutlierFactorを使用したい場合は、noveltyをTrueに設定してください。この場合、predict,decision_function および score_samples は、新しい未見のデータでのみ使用し、訓練セットでは使用しないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="2fb7ae3e457e09c80061e4212540f2949867d13f" translate="yes" xml:space="preserve">
          <source>By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.</source>
          <target state="translated">デフォルトでは、一般化クロスバリデーションを実行し、これは効率的なリーブワンアウトクロスバリデーションの一形態である。</target>
        </trans-unit>
        <trans-unit id="76317967f826fa82a2b4a34c8b9f11ee88f99afc" translate="yes" xml:space="preserve">
          <source>By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation. Currently, only the n_features &amp;gt; n_samples case is handled efficiently.</source>
          <target state="translated">デフォルトでは、効率的なLeave-One-Out相互検証の形式である、一般化された相互検証を実行します。現在、n_features&amp;gt; n_samplesケースのみが効率的に処理されます。</target>
        </trans-unit>
        <trans-unit id="c04cdaf41bfb70ed4b9c123ddaef4117557b0e3d" translate="yes" xml:space="preserve">
          <source>By default, only the specified columns in &lt;code&gt;transformers&lt;/code&gt; are transformed and combined in the output, and the non-specified columns are dropped. (default of &lt;code&gt;'drop'&lt;/code&gt;). By specifying &lt;code&gt;remainder='passthrough'&lt;/code&gt;, all remaining columns that were not specified in &lt;code&gt;transformers&lt;/code&gt; will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting &lt;code&gt;remainder&lt;/code&gt; to be an estimator, the remaining non-specified columns will use the &lt;code&gt;remainder&lt;/code&gt; estimator. The estimator must support &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt;.</source>
          <target state="translated">デフォルトでは、 &lt;code&gt;transformers&lt;/code&gt; 指定された列のみが変換されて出力で結合され、指定されていない列は削除されます。 （デフォルトは &lt;code&gt;'drop'&lt;/code&gt; ）。指定することにより、 &lt;code&gt;remainder='passthrough'&lt;/code&gt; に指定されていない残りのすべてのカラム &lt;code&gt;transformers&lt;/code&gt; 自動的に渡されます。この列のサブセットは、トランスフォーマーの出力と連結されます。 &lt;code&gt;remainder&lt;/code&gt; を推定量に設定することにより、残りの指定されていない列は &lt;code&gt;remainder&lt;/code&gt; 推定量を使用します。推定量は、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;適合&lt;/a&gt;と&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;変換を&lt;/a&gt;サポートする必要があります。</target>
        </trans-unit>
        <trans-unit id="15494fd5cbda86aa1f330192fb9882a6711f9c4e" translate="yes" xml:space="preserve">
          <source>By default, only the specified columns in &lt;code&gt;transformers&lt;/code&gt; are transformed and combined in the output, and the non-specified columns are dropped. (default of &lt;code&gt;'drop'&lt;/code&gt;). By specifying &lt;code&gt;remainder='passthrough'&lt;/code&gt;, all remaining columns that were not specified in &lt;code&gt;transformers&lt;/code&gt; will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting &lt;code&gt;remainder&lt;/code&gt; to be an estimator, the remaining non-specified columns will use the &lt;code&gt;remainder&lt;/code&gt; estimator. The estimator must support &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt;. Note that using this feature requires that the DataFrame columns input at &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt; have identical order.</source>
          <target state="translated">デフォルトでは、 &lt;code&gt;transformers&lt;/code&gt; 指定された列のみが変換されて出力で結合され、指定されていない列は削除されます。 （デフォルトは &lt;code&gt;'drop'&lt;/code&gt; ）。指定することにより、 &lt;code&gt;remainder='passthrough'&lt;/code&gt; に指定されていない残りのすべてのカラム &lt;code&gt;transformers&lt;/code&gt; 自動的に渡されます。この列のサブセットは、トランスフォーマーの出力と連結されます。 &lt;code&gt;remainder&lt;/code&gt; を推定量に設定することにより、残りの指定されていない列は &lt;code&gt;remainder&lt;/code&gt; 推定量を使用します。推定量は、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;適合&lt;/a&gt;と&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;変換を&lt;/a&gt;サポートする必要があります。この機能を使用するには、DataFrame列を&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;適切に&lt;/a&gt;入力する必要があることに注意してくださいと&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;変換の&lt;/a&gt;順序は同じです。</target>
        </trans-unit>
        <trans-unit id="7750d963aa2e941a76e470078ac6c086adc6dcda" translate="yes" xml:space="preserve">
          <source>By default, only the specified columns in &lt;code&gt;transformers&lt;/code&gt; are transformed and combined in the output, and the non-specified columns are dropped. (default of &lt;code&gt;'drop'&lt;/code&gt;). By specifying &lt;code&gt;remainder='passthrough'&lt;/code&gt;, all remaining columns that were not specified in &lt;code&gt;transformers&lt;/code&gt; will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting &lt;code&gt;remainder&lt;/code&gt; to be an estimator, the remaining non-specified columns will use the &lt;code&gt;remainder&lt;/code&gt; estimator. The estimator must support &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt;.</source>
          <target state="translated">デフォルトでは、 &lt;code&gt;transformers&lt;/code&gt; 指定された列のみが変換されて出力に結合され、指定されていない列は削除されます。 （ &lt;code&gt;'drop'&lt;/code&gt; のデフォルト）。指定することにより、 &lt;code&gt;remainder='passthrough'&lt;/code&gt; に指定されていない残りのすべてのカラム &lt;code&gt;transformers&lt;/code&gt; 自動的に渡されます。この列のサブセットは、トランスフォーマーの出力と連結されます。 &lt;code&gt;remainder&lt;/code&gt; を推定器として設定することにより、残りの指定されていない列は &lt;code&gt;remainder&lt;/code&gt; 推定器を使用します。推定器は、 &lt;code&gt;fit&lt;/code&gt; と &lt;code&gt;transform&lt;/code&gt; サポートしている必要があります。</target>
        </trans-unit>
        <trans-unit id="4a782d670d6fc36b236c4a8940c016b292a90c5d" translate="yes" xml:space="preserve">
          <source>By default, parameter search uses the &lt;code&gt;score&lt;/code&gt; function of the estimator to evaluate a parameter setting. These are the &lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;sklearn.metrics.accuracy_score&lt;/code&gt;&lt;/a&gt; for classification and &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;sklearn.metrics.r2_score&lt;/code&gt;&lt;/a&gt; for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the &lt;code&gt;scoring&lt;/code&gt; parameter to &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt; and many of the specialized cross-validation tools described below. See &lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt; for more details.</source>
          <target state="translated">デフォルトでは、パラメーター検索は推定器の &lt;code&gt;score&lt;/code&gt; 関数を使用してパラメーター設定を評価します。これらは、分類の場合は&lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;sklearn.metrics.accuracy_score&lt;/code&gt; &lt;/a&gt;であり、回帰の場合は&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;sklearn.metrics.r2_score&lt;/code&gt; &lt;/a&gt;です。一部のアプリケーションでは、他のスコアリング関数の方が適しています（たとえば、不均衡な分類では、精度スコアは多くの場合有益ではありません）。代替のスコアリング関数は、&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt; &lt;code&gt;RandomizedSearchCV&lt;/code&gt; &lt;/a&gt;、および以下で説明する専用のクロス検証ツールの多くに &lt;code&gt;scoring&lt;/code&gt; パラメーターを使用して指定できます。詳細については&lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;、スコアリングパラメーター：モデル評価ルールの定義を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="14b262313cef67a36d56fb94a78ec0ffe131d0f9" translate="yes" xml:space="preserve">
          <source>By default, the &amp;lsquo;recursion&amp;rsquo; method is used on tree-based estimators that support it, and &amp;lsquo;brute&amp;rsquo; is used for the rest.</source>
          <target state="translated">デフォルトでは、「再帰」メソッドはそれをサポートするツリーベースの推定量で使用され、残りは「ブルート」が使用されます。</target>
        </trans-unit>
        <trans-unit id="cd89643870e89d7d33668ed08a7b04e11606d0f7" translate="yes" xml:space="preserve">
          <source>By default, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; uses a 3-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 3-fold. The default will change to a 5-fold cross-validation in version 0.22.</source>
          <target state="translated">デフォルトでは、&lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;は3分割交差検証を使用します。ただし、リグレッサではなく分類子が渡されたことを検出した場合、階層化された3重を使用します。バージョン0.22では、デフォルトが5分割交差検証に変更されます。</target>
        </trans-unit>
        <trans-unit id="d01628e3c1ff7c3c146d98c4400e308b8d21e62e" translate="yes" xml:space="preserve">
          <source>By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the &lt;code&gt;categories&lt;/code&gt; manually.</source>
          <target state="translated">デフォルトでは、エンコーダーは各機能の一意の値に基づいてカテゴリを取得します。または、 &lt;code&gt;categories&lt;/code&gt; 手動で指定することもできます。</target>
        </trans-unit>
        <trans-unit id="aabd7b3f7cca56e260a395d6492aceaf31d56d74" translate="yes" xml:space="preserve">
          <source>By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the &lt;code&gt;categories&lt;/code&gt; manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated.</source>
          <target state="translated">デフォルトでは、エンコーダーは各機能の一意の値に基づいてカテゴリを導出します。または、手動で &lt;code&gt;categories&lt;/code&gt; を指定することもできます。OneHotEncoderは以前、入力フィーチャが[0、max（values））の範囲の値を取ると想定していました。この動作は非推奨です。</target>
        </trans-unit>
        <trans-unit id="e22c55145c4d94c4a5b62e5b6f40a60aa6e4907e" translate="yes" xml:space="preserve">
          <source>By default, the initial model \(F_{0}\) is chosen as the constant that minimizes the loss: for a least-squares loss, this is the empirical mean of the target values. The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument.</source>
          <target state="translated">デフォルトでは、初期モデル\（F_ {0} \）が損失を最小化する定数として選択されます。最小二乗損失の場合、これはターゲット値の経験的平均です。初期モデルは、 &lt;code&gt;init&lt;/code&gt; 引数を介して指定することもできます。</target>
        </trans-unit>
        <trans-unit id="e16205e1ed0a918834ac9076544d9991fc9c4ca6" translate="yes" xml:space="preserve">
          <source>By default, the input is checked to be a non-empty 2D array containing only finite values. If the dtype of the array is object, attempt converting to float, raising on failure.</source>
          <target state="translated">デフォルトでは,入力は有限の値のみを含む空ではない2次元配列であることがチェックされます.配列の dtype が object の場合、float への変換を試みます。</target>
        </trans-unit>
        <trans-unit id="0397cff741cdca99d6793cf68ca8a230e768dce2" translate="yes" xml:space="preserve">
          <source>By default, the input is converted to an at least 2D numpy array. If the dtype of the array is object, attempt converting to float, raising on failure.</source>
          <target state="translated">デフォルトでは,入力は少なくとも2次元のnumpy配列に変換されます.配列の dtype が object の場合は float への変換を試み、失敗した場合は raise します。</target>
        </trans-unit>
        <trans-unit id="9dfabb9221fc44d77597df9d22d00887ce88c6cb" translate="yes" xml:space="preserve">
          <source>By default, the provided functions are checked at each fit to be the inverse of each other. However, it is possible to bypass this checking by setting &lt;code&gt;check_inverse&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;:</source>
          <target state="translated">デフォルトでは、提供された関数は、それぞれのはめあいで互いに逆になるようにチェックされます。ただし、 &lt;code&gt;check_inverse&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定することで、このチェックをバイパスすることができます。</target>
        </trans-unit>
        <trans-unit id="761cb5c85c42ad01cf3595b928efe71009e68e6e" translate="yes" xml:space="preserve">
          <source>By default, the score computed at each CV iteration is the &lt;code&gt;score&lt;/code&gt; method of the estimator. It is possible to change this by using the scoring parameter:</source>
          <target state="translated">デフォルトでは、各CV反復で計算される &lt;code&gt;score&lt;/code&gt; は、推定器のスコアメソッドです。これはスコアリングパラメーターを使用して変更できます。</target>
        </trans-unit>
        <trans-unit id="61fe756b7e3e334c670a2007b6939f399986a294" translate="yes" xml:space="preserve">
          <source>By default, the values each feature can take is inferred automatically from the dataset and can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute:</source>
          <target state="translated">デフォルトでは、各フィーチャが取り得る値はデータセットから自動的に推定され、 &lt;code&gt;categories_&lt;/code&gt; 属性で見つけることができます。</target>
        </trans-unit>
        <trans-unit id="588af344f8a3913fdf9179a25370573310fcee34" translate="yes" xml:space="preserve">
          <source>By default, zero-mean, unit-variance normalization is applied to the transformed data.</source>
          <target state="translated">デフォルトでは、ゼロ平均、単位分散正規化が変換データに適用されます。</target>
        </trans-unit>
        <trans-unit id="11243048cfb311c05ea90a5cca047fa81467c316" translate="yes" xml:space="preserve">
          <source>By definition a confusion matrix \(C\) is such that \(C_{i, j}\) is equal to the number of observations known to be in group \(i\) and predicted to be in group \(j\).</source>
          <target state="translated">By definition a confusion matrix ¶(C\)is such that ¶(C\(i,j)is such to be known to be in group ¶(i\)and predicted to be in group ¶(j\).</target>
        </trans-unit>
        <trans-unit id="c874644c0e92eeddd8de26271a422d136d288139" translate="yes" xml:space="preserve">
          <source>By definition a confusion matrix \(C\) is such that \(C_{i, j}\) is equal to the number of observations known to be in group \(i\) but predicted to be in group \(j\).</source>
          <target state="translated">By definition a confusion matrix ¶(C\)is such that ¶(C\(i,j)is such to be known to be in group ¶(i\)but predicted to be in group ¶(j\)に等しい。)</target>
        </trans-unit>
        <trans-unit id="7edbd0cd8c2bac6d61783ee719a48e462c1cfcf6" translate="yes" xml:space="preserve">
          <source>By definition, entry \(i, j\) in a confusion matrix is the number of observations actually in group \(i\), but predicted to be in group \(j\). Here is an example:</source>
          <target state="translated">定義では,Entry \(i,j\)in a confusion matrix は,実際にはGroup \(i\)にあるが,Group \(j\)にあると予測されたオブザベーションの数である.ここに例を示す。</target>
        </trans-unit>
        <trans-unit id="70e3cb436db4ad1c973fa29cd922d3f24b1b4676" translate="yes" xml:space="preserve">
          <source>By imposing a positive (increasing) or negative (decreasing) constraint on the features during the learning process, the estimator is able to properly follow the general trend instead of being subject to the variations.</source>
          <target state="translated">学習プロセス中に特徴量に正(増加)または負(減少)の制約を課すことで、推定器は変動の影響を受けることなく、一般的な傾向に適切に従うことができる。</target>
        </trans-unit>
        <trans-unit id="1dfab6b69e192f9e6ae185588957d2e4ef21b660" translate="yes" xml:space="preserve">
          <source>C parameter in C-Support Vector Classification</source>
          <target state="translated">Cサポートベクトル分類のCパラメータ</target>
        </trans-unit>
        <trans-unit id="cdda2bb3aa8ccb7b0f69433506f364d34d2e9052" translate="yes" xml:space="preserve">
          <source>C parameter in C-Support Vector Classification. 1 by default.</source>
          <target state="translated">C-Support Vector ClassificationのCパラメータ。デフォルトでは1。</target>
        </trans-unit>
        <trans-unit id="613d7a720c0f1c9138a31585cc88659b028aac73" translate="yes" xml:space="preserve">
          <source>C-Support Vector Classification.</source>
          <target state="translated">Cサポートベクトル分類。</target>
        </trans-unit>
        <trans-unit id="bd2a390d1ac4f130ef2be0118bce6d064e5c8f80" translate="yes" xml:space="preserve">
          <source>C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University.</source>
          <target state="translated">C.Kaynak (1995)Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition,MSc Thesis,Institute of Graduate Studies in Science and Engineering,Bogazici University.</target>
        </trans-unit>
        <trans-unit id="233528e4f33123dff21e034b86ff445b99733c7a" translate="yes" xml:space="preserve">
          <source>C. Molnar, &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Interpretable Machine Learning&lt;/a&gt;, Section 5.1, 2019.</source>
          <target state="translated">C.モルナル、&lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;解釈可能な機械学習&lt;/a&gt;、セクション5.1、2019。</target>
        </trans-unit>
        <trans-unit id="da3d99d58e8919ce7a794351fb460091679af9c1" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 118-120.</source>
          <target state="translated">CD Manning、P。RaghavanおよびH.Sch&amp;uuml;tze（2008）。情報検索入門。ケンブリッジ大学出版局、118-120ページ。</target>
        </trans-unit>
        <trans-unit id="da9635593b5caf5d4585f5ff3fe3cc183bfd45ee" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265.</source>
          <target state="translated">CD Manning、P。RaghavanおよびH.Sch&amp;uuml;tze（2008）。情報検索の紹介。Cambridge University Press、pp。234-265。</target>
        </trans-unit>
        <trans-unit id="14a7f63b48e4cd57491d39ae8fbc9659df0b4285" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning、P。RaghavanおよびH.Sch&amp;uuml;tze（2008）。情報検索の紹介。ケンブリッジ大学出版局。&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d8731241db66672efece43e5d06ce590569a5d55" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning、P。RaghavanおよびH.Sch&amp;uuml;tze（2008）。情報検索入門。ケンブリッジ大学出版局。&lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7f2e75005cfe431b03b43e4219e3b46d7acd0ff4" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning、P。RaghavanおよびH. Schuetze（2008）。情報検索の紹介。Cambridge University Press、pp。234-265。&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="75ec24d6932a118f83e8d64145c0a2ded87f2b67" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning、P。RaghavanおよびH. Schuetze（2008）。情報検索の紹介。Cambridge University Press、pp。234-265。&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="01f3fb182680db69cebca5748ed6286feb50b1a9" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning、P。RaghavanおよびH. Schuetze（2008）。情報検索入門。ケンブリッジ大学出版局、pp.234-265。&lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9600fefe6b9e4b354eca34f86431cfc60b66dab3" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning、P。RaghavanおよびH. Schuetze（2008）。情報検索入門。ケンブリッジ大学出版局、pp.234-265。&lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d19ae811e7ffeb32597a550f21eb906e9869f42b" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan, H. Sch&amp;uuml;tze, &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;Introduction to Information Retrieval&lt;/a&gt;, 2008.</source>
          <target state="translated">CD Manning、P。Raghavan、H。Sch&amp;uuml;tze、&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;Introduction to Information Retrieval&lt;/a&gt;、2008年。</target>
        </trans-unit>
        <trans-unit id="015610fbd8f75c742ed46a08abb26c26092afe90" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan, H. Sch&amp;uuml;tze, &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;Introduction to Information Retrieval&lt;/a&gt;, 2008.</source>
          <target state="translated">CDマニング、P。ラガヴァン、H。シュッツェ、&lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;情報検索の基礎&lt;/a&gt;、2008年。</target>
        </trans-unit>
        <trans-unit id="ac35b04dfbcb3d2d718c41403e2829f9afacc158" translate="yes" xml:space="preserve">
          <source>C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209.</source>
          <target state="translated">C.M.ビショップ(2006).パターン認識と機械学習.シュプリンガー、209頁。</target>
        </trans-unit>
        <trans-unit id="4e28cad37c371942b28c3b9844d1c63eab0cd98c" translate="yes" xml:space="preserve">
          <source>C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule&amp;rsquo;s precondition if the accuracy of the rule improves without it.</source>
          <target state="translated">C4.5はID3の後継であり、連続した属性値を個別の間隔のセットに分割する個別の属性（数値変数に基づく）を動的に定義することにより、機能がカテゴリにならなければならないという制限を取り除きました。 C4.5は、トレーニング済みツリー（つまり、ID3アルゴリズムの出力）をif-thenルールのセットに変換します。次に、各ルールのこれらの精度が評価され、適用される順序が決定されます。剪定は、それなしでルールの精度が向上する場合、ルールの前提条件を削除することによって行われます。</target>
        </trans-unit>
        <trans-unit id="f38b90367c7c83e3cb66fb496411f772ac84712e" translate="yes" xml:space="preserve">
          <source>C5.0 is Quinlan&amp;rsquo;s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.</source>
          <target state="translated">C5.0は、独自のライセンスに基づくクインランの最新バージョンのリリースです。より正確でありながら、使用するメモリが少なく、C4.5よりも小さいルールセットを構築します。</target>
        </trans-unit>
        <trans-unit id="d173d3b539a344f61d41ecea5a62c323f7d19142" translate="yes" xml:space="preserve">
          <source>CCA Canonical Correlation Analysis.</source>
          <target state="translated">CCA正準相関分析。</target>
        </trans-unit>
        <trans-unit id="b29de26d602ff110fad786ea8b03d97b6805af56" translate="yes" xml:space="preserve">
          <source>CCA inherits from PLS with mode=&amp;rdquo;B&amp;rdquo; and deflation_mode=&amp;rdquo;canonical&amp;rdquo;.</source>
          <target state="translated">CCAは、mode =&amp;rdquo; B&amp;rdquo;およびdeflation_mode =&amp;rdquo; canonical&amp;rdquo;のPLSを継承します。</target>
        </trans-unit>
        <trans-unit id="3b8d88f34c322c835ae4b658c65d92bfd4b52c68" translate="yes" xml:space="preserve">
          <source>CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</source>
          <target state="translated">CHAS チャールズ川のダミー変数(トラクトが川に接している場合は1、そうでない場合は0)。</target>
        </trans-unit>
        <trans-unit id="65e7931e7d9586486554e7903a3ba08da00b37e4" translate="yes" xml:space="preserve">
          <source>CRIM per capita crime rate by town</source>
          <target state="translated">町別一人当たりの犯罪率</target>
        </trans-unit>
        <trans-unit id="c1c39cb0e3578f55a7a5c64e876c1d8c4ac8dfba" translate="yes" xml:space="preserve">
          <source>CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are not supported.</source>
          <target state="translated">CSR、CSC、およびLILスパース行列がサポートされています。COO 疎行列はサポートされていません。</target>
        </trans-unit>
        <trans-unit id="73f41c9b08913eecfdd5c30b35b021c1e2d148fd" translate="yes" xml:space="preserve">
          <source>Cache size for gram matrix columns (in megabytes). 100 by default.</source>
          <target state="translated">グラム行列の列のキャッシュサイズ (メガバイト単位)。デフォルトでは100。</target>
        </trans-unit>
        <trans-unit id="596160dd9a9ac13e48baafc6fcbec2fd25ca71f9" translate="yes" xml:space="preserve">
          <source>Caching nearest neighbors</source>
          <target state="translated">近所のキャッシング</target>
        </trans-unit>
        <trans-unit id="9ff21fb5eb8a4fa17d7e155397ec6ccfabcba0a4" translate="yes" xml:space="preserve">
          <source>Caching transformers within a &lt;code&gt;Pipeline&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 内のトランスフォーマーのキャッシュ</target>
        </trans-unit>
        <trans-unit id="bc2913cfd35cdeb7164299f7eca5904d9beb65fc" translate="yes" xml:space="preserve">
          <source>Calculate approximate log-likelihood as score.</source>
          <target state="translated">近似対数尤度をスコアとして計算します。</target>
        </trans-unit>
        <trans-unit id="c1ebfc4b9bd037366318ba68c2c2cca59e1e8374" translate="yes" xml:space="preserve">
          <source>Calculate approximate perplexity for data X.</source>
          <target state="translated">データXの近似的な錯乱度を計算します。</target>
        </trans-unit>
        <trans-unit id="d43ab93e0150f4ef29d06ad998812dd6ba91c9c1" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from &lt;a href=&quot;sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;accuracy_score&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">各インスタンスのメトリックを計算し、それらの平均を見つけます（これが&lt;a href=&quot;sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;accuracy_score&lt;/code&gt; &lt;/a&gt;スコアと異なるマルチラベル分類でのみ意味があります）。</target>
        </trans-unit>
        <trans-unit id="e282cf212dffcdd940b3d9463a8434aa0c623305" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).</source>
          <target state="translated">各インスタンスのメトリクスを計算し、その平均値を求めます(マルチラベル分類の場合のみ意味があります)。</target>
        </trans-unit>
        <trans-unit id="59762e6da26cf0edea79b5cf712b11b4295f7c3a" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each instance, and find their average.</source>
          <target state="translated">各インスタンスのメトリクスを計算し、その平均値を求めます。</target>
        </trans-unit>
        <trans-unit id="b0449c6e69f12738c5fc9e626586315cc55b3e5c" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters &amp;lsquo;macro&amp;rsquo; to account for label imbalance; it can result in an F-score that is not between precision and recall.</source>
          <target state="translated">各ラベルのメトリックを計算し、サポートによって重み付けされた平均（各ラベルの実際のインスタンスの数）を見つけます。これは、ラベルの不均衡を説明するために「マクロ」を変更します。精度と再現率の間にないFスコアになる可能性があります。</target>
        </trans-unit>
        <trans-unit id="bc72c4ff58beeced2951accfefb204124157b6da" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label).</source>
          <target state="translated">各ラベルのメトリクスを計算し、サポート(各ラベルの真のインスタンス数)で加重した平均値を求めます。</target>
        </trans-unit>
        <trans-unit id="a0980fad9587146eedd0aa21fe16b429b81ffb7a" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters &amp;lsquo;macro&amp;rsquo; to account for label imbalance.</source>
          <target state="translated">各ラベルのメトリックを計算し、サポート（各ラベルの真のインスタンスの数）で重み付けされた平均を見つけます。これにより、ラベルの不均衡を考慮して「マクロ」が変更されます。</target>
        </trans-unit>
        <trans-unit id="c9463a17edbcd91794095643439dd340e0b094af" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</source>
          <target state="translated">各ラベルについてメトリクスを計算し、それらの非加重平均を求めます。これは、ラベルの不均衡を考慮に入れていません。</target>
        </trans-unit>
        <trans-unit id="a029600dc316664ad0a95bc83365c7c5938aa93b" translate="yes" xml:space="preserve">
          <source>Calculate metrics globally by considering each element of the label indicator matrix as a label.</source>
          <target state="translated">ラベル指標行列の各要素をラベルとみなして、グローバルにメトリクスを計算する。</target>
        </trans-unit>
        <trans-unit id="734c6eed79b78874fafa14f9d8cbb9961b3a854c" translate="yes" xml:space="preserve">
          <source>Calculate metrics globally by counting the total true positives, false negatives and false positives.</source>
          <target state="translated">真陽性、偽陰性、偽陽性の合計をカウントすることで、メトリクスをグローバルに計算します。</target>
        </trans-unit>
        <trans-unit id="23f0cbdf1f8f2eb83817496535820388353f4a46" translate="yes" xml:space="preserve">
          <source>Calculate the euclidean distances in the presence of missing values.</source>
          <target state="translated">欠損値がある場合のユークリッド距離を計算します。</target>
        </trans-unit>
        <trans-unit id="49fff0d8477b2f29bd766898572fb8e020082b5d" translate="yes" xml:space="preserve">
          <source>Calculates a covariance matrix shrunk on the diagonal</source>
          <target state="translated">対角線上で縮小された共分散行列を計算します.</target>
        </trans-unit>
        <trans-unit id="f0383dd4fd81be714fa7344f5483fa52315c3c0b" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/False_positive_rate&quot;&gt;fall out&lt;/a&gt; (also called the false positive rate) for each class:</source>
          <target state="translated">各クラスの&lt;a href=&quot;https://en.wikipedia.org/wiki/False_positive_rate&quot;&gt;フォールアウト&lt;/a&gt;（偽陽性率とも呼ばれます）の計算：</target>
        </trans-unit>
        <trans-unit id="89465d26eac0f77a714d2eb41333a3104974a090" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/False_positives_and_false_negatives&quot;&gt;miss rate&lt;/a&gt; (also called the false negative rate) for each class:</source>
          <target state="translated">各クラスの&lt;a href=&quot;https://en.wikipedia.org/wiki/False_positives_and_false_negatives&quot;&gt;ミス率&lt;/a&gt;（偽陰性率とも呼ばれます）の計算：</target>
        </trans-unit>
        <trans-unit id="485cdc580455f224386f0fdb1e7fd3b7d1083ba4" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;recall&lt;/a&gt; (also called the true positive rate or the sensitivity) for each class:</source>
          <target state="translated">各クラスの&lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;リコール&lt;/a&gt;（真陽性率または感度とも呼ばれます）の計算：</target>
        </trans-unit>
        <trans-unit id="a9101242444ef9428e2eaa8ec947f98e495b1f80" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;specificity&lt;/a&gt; (also called the true negative rate) for each class:</source>
          <target state="translated">各クラスの&lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;特異度&lt;/a&gt;（真の陰性率とも呼ばれます）の計算：</target>
        </trans-unit>
        <trans-unit id="c97307180f2f1c1353791c3b1ff02010b40d4cbc" translate="yes" xml:space="preserve">
          <source>Calculation over a dense representation, however, may leverage highly optimised vector operations and multithreading in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input representation on a machine with many CPUs and an optimized BLAS implementation.</source>
          <target state="translated">しかし、密な表現での計算は、BLAS の最適化されたベクトル演算とマルチスレッドを活用することができ、CPU キャッシュのミスが少なくなる傾向があります。そのため、多くのCPUを搭載し、BLASの実装が最適化されたマシンにおいて、疎な入力表現が密な入力表現よりも高速になるためには、通常、疎な入力表現はかなり高い値(最大10%の非ゼロ値、ハードウェアに依存してチェックされます)でなければなりません。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
