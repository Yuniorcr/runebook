<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="deb9c47cdc32b652eb6b8e87508a50e73ddd6520" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;Transformer&lt;/code&gt; API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;code&gt;preprocessing&lt;/code&gt; モジュールはさらに、ユーティリティクラス提供&lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; を&lt;/a&gt;実装する &lt;code&gt;Transformer&lt;/code&gt; テストセットに後で再適用同じ変換することができるように、APIは、トレーニングセット上の平均値と標準偏差を計算します。したがって、このクラスは&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; の&lt;/a&gt;初期段階での使用に適しています。</target>
        </trans-unit>
        <trans-unit id="1c988d2d2cf78f045059c60f8f22ddfe99f6b9b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;random_state&lt;/code&gt; parameter defaults to &lt;code&gt;None&lt;/code&gt;, meaning that the shuffling will be different every time &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; is iterated. However, &lt;code&gt;GridSearchCV&lt;/code&gt; will use the same shuffling for each set of parameters validated by a single call to its &lt;code&gt;fit&lt;/code&gt; method.</source>
          <target state="translated">&lt;code&gt;random_state&lt;/code&gt; へのパラメータのデフォルト値 &lt;code&gt;None&lt;/code&gt; シャッフルは毎回異なるものになることを意味し、 &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; 繰り返されます。ただし、 &lt;code&gt;GridSearchCV&lt;/code&gt; は、 &lt;code&gt;fit&lt;/code&gt; メソッドの1回の呼び出しで検証されたパラメーターの各セットに対して同じシャッフルを使用します。</target>
        </trans-unit>
        <trans-unit id="269b993dc29ab1401256004d5e02aa3f5322b3b6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remainder&lt;/code&gt; parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:</source>
          <target state="translated">&lt;code&gt;remainder&lt;/code&gt; パラメータは、残りの格付け列を変換するために推定に設定することができます。変換された値は、変換の最後に追加されます。</target>
        </trans-unit>
        <trans-unit id="ac9a3f7a78a3eea81f462ceabf4e7d7b7695c3e1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;roc_auc_score&lt;/code&gt; function can also be used in multi-class classification. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the multiclass ROC AUC scores are computed from the probability estimates that a sample belongs to a particular class according to the model. The OvO and OvR algorithms support weighting uniformly (&lt;code&gt;average='macro'&lt;/code&gt;) and weighting by the prevalence (&lt;code&gt;average='weighted'&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;roc_auc_score&lt;/code&gt; の機能は、マルチクラス分類に使用することができます。現在、2つの平均化戦略がサポートされています。1対1アルゴリズムは、ペアワイズROC AUCスコアの平均を計算し、1対レストアルゴリズムは、他のすべてのクラスに対する各クラスのROCAUCスコアの平均を計算します。どちらの場合も、マルチクラスROC AUCスコアは、サンプルがモデルに従って特定のクラスに属する確率推定から計算されます。 OvOおよびOvRアルゴリズムは、均一な重み付け（ &lt;code&gt;average='macro'&lt;/code&gt; ）および有病率による重み付け（ &lt;code&gt;average='weighted'&lt;/code&gt; ）をサポートします。</target>
        </trans-unit>
        <trans-unit id="5cf025ac399ddc1c71c004c7cb8bd73171357561" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;shrinkage&lt;/code&gt; parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</source>
          <target state="translated">&lt;code&gt;shrinkage&lt;/code&gt; 完全収縮パラメータは、手動で（経験的な共分散行列が使用されることを意味する）、具体的には0と1の間に収縮0相当の値を設定することができ、1相当の値（平均対角線その分散行列は、共分散行列の推定値として使用されます）。このパラメーターをこれら2つの極値の間の値に設定すると、共分散行列の縮小バージョンが推定されます。</target>
        </trans-unit>
        <trans-unit id="ac317fd98a77c706ac5d8a7e74a96d7bdbebfe03" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;3&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.covariance&lt;/code&gt; のパッケージには、共分散の堅牢な推定、最小共分散行列式を実装&lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;3&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b439b9c10b67475737ae3e151e90a6ed815edbd1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.covariance&lt;/code&gt; のパッケージは、共分散のロバストな推定器を実装最小共分散行列式&lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3] &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6908af1748b7fe666814b53ca6e0a8cab66eb012" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package embeds some small toy datasets as introduced in the &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Getting Started&lt;/a&gt; section.</source>
          <target state="translated">&lt;code&gt;sklearn.datasets&lt;/code&gt; のパッケージはで導入されたとして、いくつかの小さなおもちゃのデータセットを埋め込む&lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;はじめ&lt;/a&gt;セクション。</target>
        </trans-unit>
        <trans-unit id="82c9f1d0e48a88b08f4d85a858e0d70b67f55eec" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package is able to download datasets from the repository using the function &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.datasets&lt;/code&gt; のパッケージには、機能の使用してリポジトリからデータセットをダウンロードすることができます&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt; を&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8a80b4da04f92037fdd1ffd5771651becd3a649e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.preprocessing&lt;/code&gt; package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing&lt;/code&gt; パッケージには、下流の推定のために、より適している表現に生の特徴ベクトルを変更するには、いくつかの一般的なユーティリティ機能とトランスクラスを提供します。</target>
        </trans-unit>
        <trans-unit id="9a666cbe94ae4d4ef285ecec57ff29087d1b1c92" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;stop_words_&lt;/code&gt; attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.</source>
          <target state="translated">&lt;code&gt;stop_words_&lt;/code&gt; の属性は、大規模な取得し、酸洗時にモデルのサイズを大きくすることができます。この属性はイントロスペクションのためにのみ提供されており、酸洗いの前にdelattrを使用して安全に削除するか、Noneに設定できます。</target>
        </trans-unit>
        <trans-unit id="a2df492be59e0b5e94ab9ece47f7ab58734f8d4a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;svm.OneClassSVM&lt;/code&gt; is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.</source>
          <target state="translated">&lt;code&gt;svm.OneClassSVM&lt;/code&gt; は外れ値に敏感であることが知られているので、外れ値検出のために非常によく実行されません。この推定器は、トレーニングセットが外れ値で汚染されていない場合の新規性検出に最適です。とは言っても、高次元での、または基礎となるデータの分布を想定せずに外れ値を検出することは非常に難しく、One-class SVMは、ハイパーパラメーターの値によっては、このような状況で有用な結果をもたらす可能性があります。</target>
        </trans-unit>
        <trans-unit id="ae04908924f55e8ffa997283ff07a48e73585d0d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;tree_disp&lt;/code&gt; and &lt;code&gt;mlp_disp&lt;/code&gt;&lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt; objects contain all the computed information needed to recreate the partial dependence curves. This means we can easily create additional plots without needing to recompute the curves.</source>
          <target state="translated">&lt;code&gt;tree_disp&lt;/code&gt; と &lt;code&gt;mlp_disp&lt;/code&gt; &lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay&quot;&gt; &lt;code&gt;PartialDependenceDisplay&lt;/code&gt; &lt;/a&gt;オブジェクトが部分的に依存曲線を再現するために必要なすべての計算された情報が含まれています。これは、曲線を再計算することなく、追加のプロットを簡単に作成できることを意味します。</target>
        </trans-unit>
        <trans-unit id="a0172c625369e43fa830669101114953189c751e" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;kernel function&lt;/em&gt; can be any of the following:</source>
          <target state="translated">&lt;em&gt;カーネル関数は&lt;/em&gt;、次のいずれかになります。</target>
        </trans-unit>
        <trans-unit id="9153a9bf6c588ab4d00c19f8aab755b4d4790f32" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;reachability&lt;/em&gt; distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining &lt;em&gt;reachability&lt;/em&gt; distances and data set &lt;code&gt;ordering_&lt;/code&gt; produces a &lt;em&gt;reachability plot&lt;/em&gt;, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. &amp;lsquo;Cutting&amp;rsquo; the reachability plot at a single value produces DBSCAN like results; all points above the &amp;lsquo;cut&amp;rsquo; are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter &lt;code&gt;xi&lt;/code&gt;. There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the &lt;code&gt;cluster_hierarchy_&lt;/code&gt; parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster.</source>
          <target state="translated">&lt;em&gt;到達可能性の&lt;/em&gt;光学系によって生成された距離は、単一のデータセット内のクラスタの可変密度の抽出を可能にします。上記のプロットに示されているように、&lt;em&gt;到達可能性&lt;/em&gt;距離とデータセット &lt;code&gt;ordering_&lt;/code&gt; を組み合わせると、&lt;em&gt;到達可能性プロットが&lt;/em&gt;生成され&lt;em&gt;ます&lt;/em&gt;、ここで、ポイント密度はY軸に表され、ポイントは近くのポイントが隣接するように順序付けられます。到達可能性プロットを単一の値で「カット」すると、DBSCANのような結果が生成されます。 「カット」より上のすべてのポイントはノイズとして分類され、左から右に読み取るときに中断があるたびに、新しいクラスターを示します。 OPTICSを使用したデフォルトのクラスター抽出では、グラフ内の急勾配を調べてクラスターを見つけます。ユーザーは、パラメーター &lt;code&gt;xi&lt;/code&gt; を使用して、急勾配としてカウントされるものを定義できます。到達可能性プロット樹状図を介してデータの階層表現を生成するなど、グラフ自体の分析には他の可能性もあり、アルゴリズムによって検出されたクラスターの階層には、 &lt;code&gt;cluster_hierarchy_&lt;/code&gt; を介してアクセスできます。パラメータ。上記のプロットは、平面空間のクラスターの色が到達可能性プロットの線形セグメントクラスターと一致するように色分けされています。青と赤のクラスターは到達可能性プロットで隣接しており、より大きな親クラスターの子として階層的に表すことができることに注意してください。</target>
        </trans-unit>
        <trans-unit id="6eff32d476fb25ebcdfdcb46623a8711aa972809" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;conditional entropy of clusters given class&lt;/strong&gt;\(H(K|C)\) and the &lt;strong&gt;entropy of clusters&lt;/strong&gt;\(H(K)\) are defined in a symmetric manner.</source>
          <target state="translated">&lt;strong&gt;クラス&lt;/strong&gt; \（H（K | C ）\）が指定された&lt;strong&gt;クラスター&lt;/strong&gt;の&lt;strong&gt;条件付きエントロピーとクラスター&lt;/strong&gt; \（H（K）\）の&lt;strong&gt;エントロピーは&lt;/strong&gt;、対称的に定義されます。</target>
        </trans-unit>
        <trans-unit id="5adbfb7d3b47e961b1e75927796552dd26ad41e4" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;exposure&lt;/strong&gt; is the duration of the insurance coverage of a given policy, in years.</source>
          <target state="translated">&lt;strong&gt;露出は&lt;/strong&gt;年間で、与えられた政策の保険の期間です。</target>
        </trans-unit>
        <trans-unit id="a256791561f56278bffe5a6063bb1e7227590f9a" translate="yes" xml:space="preserve">
          <source>The AGE and EXPERIENCE coefficients are affected by strong variability which might be due to the collinearity between the 2 features: as AGE and EXPERIENCE vary together in the data, their effect is difficult to tease apart.</source>
          <target state="translated">AGEとEXPERIENCEの係数は、2つの特徴の間のコリネラリティに起因するかもしれない強い変動性の影響を受けます:AGEとEXPERIENCEがデータ中で一緒に変動するため、それらの効果を分離することは困難です。</target>
        </trans-unit>
        <trans-unit id="e076b7d1d24895a169d091c6b0d7ae51431e1b5b" translate="yes" xml:space="preserve">
          <source>The AGE coefficient is expressed in &amp;ldquo;dollars/hour per living years&amp;rdquo; while the EDUCATION one is expressed in &amp;ldquo;dollars/hour per years of education&amp;rdquo;. This representation of the coefficients has the benefit of making clear the practical predictions of the model: an increase of \(1\) year in AGE means a decrease of \(0.030867\) dollars/hour, while an increase of \(1\) year in EDUCATION means an increase of \(0.054699\) dollars/hour. On the other hand, categorical variables (as UNION or SEX) are adimensional numbers taking either the value 0 or 1. Their coefficients are expressed in dollars/hour. Then, we cannot compare the magnitude of different coefficients since the features have different natural scales, and hence value ranges, because of their different unit of measure. This is more visible if we plot the coefficients.</source>
          <target state="translated">AGE係数は「1生活年あたりのドル/時間」で表され、EDUCATION係数は「教育の1年あたりのドル/時間」で表されます。係数のこの表現には、モデルの実際の予測を明確にするという利点があります。AGEの\（1 \）年の増加は、\（0.030867 \）ドル/時間の減少を意味し、\（1 \）の増加を意味します。 ）教育における年は、\（0.054699 \）ドル/時間の増加を意味します。一方、カテゴリ変数（UNIONまたはSEXなど）は、値0または1のいずれかをとる無次元数です。それらの係数はドル/時間で表されます。次に、フィーチャの自然なスケールが異なり、測定単位が異なるために値の範囲が異なるため、異なる係数の大きさを比較することはできません。これは、係数をプロットするとよりわかりやすくなります。</target>
        </trans-unit>
        <trans-unit id="2143ba51f2aad99e99e8cd60a62ccfdecbf0f265" translate="yes" xml:space="preserve">
          <source>The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative.</source>
          <target state="translated">AMIは、2つのパーティションが同じ(完全に一致している)場合、1の値を返します。ランダムなパーティション(独立したラベリング)は、平均して0前後のAMIが期待されるため、負の値になることがあります。</target>
        </trans-unit>
        <trans-unit id="ad838931339007a1bda099c18480308bbb327339" translate="yes" xml:space="preserve">
          <source>The API is experimental (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">APIは実験的なものであり(特に戻り値構造)、将来のリリースでは下位互換性のない小さな変更があるかもしれません。</target>
        </trans-unit>
        <trans-unit id="00e3722885bdadae5430c6fecaacfc1cced893f6" translate="yes" xml:space="preserve">
          <source>The API is experimental in version 0.20 (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">このAPIはバージョン0.20では実験的なものであり(特に戻り値構造)、将来のリリースでは下位互換性のない小さな変更があるかもしれません。</target>
        </trans-unit>
        <trans-unit id="e07138bd94d8d9dd92474342c51f134fa3423d8d" translate="yes" xml:space="preserve">
          <source>The Ames housing dataset is not shipped with scikit-learn and therefore we will fetch it from &lt;a href=&quot;https://www.openml.org/d/42165&quot;&gt;OpenML&lt;/a&gt;.</source>
          <target state="translated">Amesハウジングデータセットはscikit-learnに付属していないため、&lt;a href=&quot;https://www.openml.org/d/42165&quot;&gt;OpenML&lt;/a&gt;から取得します。</target>
        </trans-unit>
        <trans-unit id="22556f00c2cdfc8c60673e1c663299619a64901c" translate="yes" xml:space="preserve">
          <source>The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a &lt;a href=&quot;#bgmm&quot;&gt;Variational Bayesian Gaussian mixture&lt;/a&gt; avoids the specification of the number of components for a Gaussian mixture model.</source>
          <target state="translated">BIC基準を使用すると、混合ガウスの成分数を効率的に選択できます。理論的には、それは漸近体制でのみコンポーネントの真の数を回復します（つまり、多くのデータが利用可能であり、データがガウス分布の混合から実際に生成されたと仮定した場合）。&lt;a href=&quot;#bgmm&quot;&gt;変分ベイズガウス混合&lt;/a&gt;を使用すると、混合ガウスモデルのコンポーネント数の指定が回避されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="c912c462511f5921d016fed26135f626922ccc57" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.</source>
          <target state="translated">Barnes-Hutの実装は、ターゲットの次元数が3以下の場合にのみ動作します。2Dの場合は、ビジュアライゼーションを構築する場合に典型的です。</target>
        </trans-unit>
        <trans-unit id="49027a83447d6307ec530f0dd81bd5ad76360faa" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.</source>
          <target state="translated">バーンズハットのt-SNE法は、2次元または3次元の埋め込みに限定されている。</target>
        </trans-unit>
        <trans-unit id="f05f901deabcb1ee376f8697fc79932cbf746ce6" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is \(O[d N log(N)]\), where \(d\) is the number of output dimensions and \(N\) is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is \(O[d N^2]\), but has several other notable differences:</source>
          <target state="translated">ここで実装されているバーンズハットのt-SNEは、他の多様性学習アルゴリズムに比べて、通常は非常に遅い。最適化は非常に難しく、勾配の計算は、\(O[d N log(N)]\(O[d N log(N)]\(D\(D\))は出力次元の数、\(N)(N)はサンプルの数です。Barnes-Hut法は、t-SNEの複雑さが \(O[d N^2]W\)である厳密法を改良したものですが、他にもいくつかの顕著な違いがあります。</target>
        </trans-unit>
        <trans-unit id="e724094f6d5c410991717a9e2fc045d6798def45" translate="yes" xml:space="preserve">
          <source>The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.</source>
          <target state="translated">Birchアルゴリズムには,閾値と分岐係数の2つのパラメータがあります.分岐係数はノード内のサブクラスターの数を制限し,閾値は入力サンプルと既存のサブクラスターの間の距離を制限します.</target>
        </trans-unit>
        <trans-unit id="8fa70be719c3475e8e012f73acc572a21c03cdd8" translate="yes" xml:space="preserve">
          <source>The Boston house-price data has been used in many machine learning papers that address regression problems.</source>
          <target state="translated">ボストンの住宅価格データは、回帰問題を扱う多くの機械学習の論文で使用されています。</target>
        </trans-unit>
        <trans-unit id="455243540177422da87f9c3aa93de30456a2bae3" translate="yes" xml:space="preserve">
          <source>The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &amp;lsquo;Hedonic prices and the demand for clean air&amp;rsquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics &amp;hellip;&amp;rsquo;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.</source>
          <target state="translated">ハリソンD.とルビンフェルドDLのボストンの住宅価格データ「快楽価格ときれいな空気の需要」、J。エンビロン。Economics＆Management、vol.5、81-102、1978. Belsley、Kuh＆Welsch、 'Regression diagnostics&amp;hellip;'、Wiley、1980で使用されています。NB後者の244-261ページの表では、さまざまな変換が使用されています。</target>
        </trans-unit>
        <trans-unit id="7bc398b9797cca816dee96d9ed40f902ff743772" translate="yes" xml:space="preserve">
          <source>The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt;&lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">Bunchオブジェクトは、そのキーが属性であることを公開する辞書です。Bunchオブジェクトの詳細については、&lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt; &lt;code&gt;sklearn.utils.Bunch&lt;/code&gt; を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="0e6c056d908bee991f648998f41ff72e8a7301c2" translate="yes" xml:space="preserve">
          <source>The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:</source>
          <target state="translated">CF サブクラスターは,クラスタリングに必要な情報を保持しているので,入力データ全体をメモリに保持する必要がありません.この情報には以下のものが含まれます。</target>
        </trans-unit>
        <trans-unit id="b64782d12e8b4e97de28df15435a193ab8a372e9" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Calinski-Harabasz指数は、DBSCANで得られたような密度ベースのクラスターのような他のクラスターの概念よりも、一般的に凸型クラスターの方が高い。</target>
        </trans-unit>
        <trans-unit id="b96c855c122e21633007587f3600e092339a5383" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Calinski-Harabaz指数は、DBSCANで得られたような密度ベースのクラスターのような他のクラスターの概念よりも、一般的に凸型クラスターの方が高い。</target>
        </trans-unit>
        <trans-unit id="6cdbc3a888667e8344981c2a8742122994627087" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al.</source>
          <target state="translated">Rennieらに記載されているComplement Naive Bayes分類器。</target>
        </trans-unit>
        <trans-unit id="2b0b8b0a4dd5523203a6ee4b5d195c2e6a35c0fe" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al. (2003).</source>
          <target state="translated">Rennieら(2003)に記載されている補完ナイーブベイズ分類器。</target>
        </trans-unit>
        <trans-unit id="3398d87a3aceb1ea8375c52fcc6a67cec5c63df9" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier was designed to correct the &amp;ldquo;severe assumptions&amp;rdquo; made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.</source>
          <target state="translated">Complement Naive Bayes分類器は、標準のMultinomial Naive Bayes分類器によって作成された「厳しい仮定」を修正するように設計されました。不均衡なデータセットに特に適しています。</target>
        </trans-unit>
        <trans-unit id="7c176cfd9628fae51161daa03c193aeae01d49ea" translate="yes" xml:space="preserve">
          <source>The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \(k\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.</source>
          <target state="translated">Contrastive Divergence法は、小さな反復回数、\(k\)の後に連鎖を止めることを提案します。 この方法は高速で分散が低いのですが、サンプルはモデル分布から離れています。</target>
        </trans-unit>
        <trans-unit id="1d09474bd461f8756796df6e9a77749a8489b6d3" translate="yes" xml:space="preserve">
          <source>The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than &lt;code&gt;eps&lt;/code&gt; to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than &lt;code&gt;eps&lt;/code&gt; from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering.</source>
          <target state="translated">DBSCANアルゴリズムは確定的であり、同じデータを同じ順序で指定すると、常に同じクラスターを生成します。ただし、データが異なる順序で提供されると、結果が異なる場合があります。まず、コアサンプルは常に同じクラスターに割り当てられますが、それらのクラスターのラベルは、データ内でこれらのサンプルが出現する順序によって異なります。 2番目に重要なこととして、非コアサンプルが割り当てられるクラスターは、データの順序によって異なる場合があります。これは、非コアサンプルの距離が、異なるクラスター内の2つのコアサンプルまで &lt;code&gt;eps&lt;/code&gt; 未満の場合に発生します。三角形の不等式により、これら2つのコアサンプルは &lt;code&gt;eps&lt;/code&gt; よりも離れている必要があります。お互いから、またはそれらは同じクラスターにあります。非コアサンプルは、データのパスで最初に生成されたクラスターに割り当てられるため、結果はデー​​タの順序によって異なります。</target>
        </trans-unit>
        <trans-unit id="b5783662e991f62ab3cc63e2843b11c10be0af6e" translate="yes" xml:space="preserve">
          <source>The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN.</source>
          <target state="translated">Davies-Boulding指数は、DBSCANから得られたような密度ベースのクラスターのような他のクラスターの概念よりも、凸型クラスターの方が一般的に高い。</target>
        </trans-unit>
        <trans-unit id="9dae6187a2e4264ce3764c47d6ced56e22112bc3" translate="yes" xml:space="preserve">
          <source>The Digit Dataset</source>
          <target state="translated">デジットデータセット</target>
        </trans-unit>
        <trans-unit id="15949e73904f01f7a29e0206a3232b388b3af43d" translate="yes" xml:space="preserve">
          <source>The Dirichlet process prior allows to define an infinite number of components and automatically selects the correct number of components: it activates a component only if it is necessary.</source>
          <target state="translated">ディリクレ処理の前処理では、無限の数のコンポーネントを定義することができ、正しい数のコンポーネントを自動的に選択することができます:必要な場合にのみコンポーネントを起動します。</target>
        </trans-unit>
        <trans-unit id="a257bf9309fabcccadc69c1bb14244b979e6f754" translate="yes" xml:space="preserve">
          <source>The Discounted Cumulative Gain divided by the Ideal Discounted Cumulative Gain (the DCG obtained for a perfect ranking), in order to have a score between 0 and 1.</source>
          <target state="translated">Discounted Cumulative GainをIdeal Discounted Cumulative Gain(完全なランキングのために得られるDCG)で割ったもので、0と1の間のスコアを持つようにします。</target>
        </trans-unit>
        <trans-unit id="d2e49448333a2cd92baf05825e927765f1835316" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is commonly combined with exponentiation.</source>
          <target state="translated">DotProductカーネルは、一般的に指数と組み合わせて使用されています。</target>
        </trans-unit>
        <trans-unit id="842c0c72bca462c8ce3e9ff0cead821c33430a8a" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0^2. For sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">DotProductカーネルは非定常であり、線形回帰からx_d (d=1,...,D)の係数にN(0,1)のプライアを置き、バイアスにN(0,sigma_0^2)のプライアを置くことで得ることができます。DotProduct カーネルは、原点に関する座標の回転には不変ですが、平行移動には不変です。これはパラメータ sigma_0^2 によってパラメータ化されます。sigma_0^2 =0 の場合、カーネルは均質な線形カーネルと呼ばれ、そうでない場合は不均質です。カーネルは次式で与えられます</target>
        </trans-unit>
        <trans-unit id="078b4d2b566f931479e95c97a38f05103a2ecb7d" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0 \(\sigma\) which controls the inhomogenity of the kernel. For \(\sigma_0^2 =0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">DotProductカーネルは非定常であり、線形回帰から、\(x_d (d=1,...,D)の係数に\(N(0,1)\)の先行値を、バイアスに\(N(0,sigma_0^2)の先行値を置くことで得られる。DotProductカーネルは原点座標の回転には不変ですが、移動には不変です。カーネルの不均一性を制御するパラメータsigma_0でパラメータ化されています。For \(\sigma_0^2 =0)では,カーネルは同質線形カーネルと呼ばれ,そうでなければ不均質である.カーネルは次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="c219b28ff4dd0afb8b865b96896a279315780153" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.</source>
          <target state="translated">Elastic Netミキシングパラメーター。0&amp;lt;= l1_ratio &amp;lt;= 1です。l1_ratio= 0はL2ペナルティに対応し、l1_ratio = 1はL1に対応します。デフォルトは0.15です。</target>
        </trans-unit>
        <trans-unit id="bc71ca7c36fa98be5da488c2c0295a9dca2a49c6" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if &lt;code&gt;penalty&lt;/code&gt; is &amp;lsquo;elasticnet&amp;rsquo;.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1のElasticNetミキシングパラメータ。l1_ratio= 0はL2ペナルティに対応し、l1_ratio = 1はL1に対応します。 &lt;code&gt;penalty&lt;/code&gt; が「elasticnet」の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="557cde32667038df92bd17ed0a660f8397aa9f15" translate="yes" xml:space="preserve">
          <source>The Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. Setting &lt;code&gt;l1_ratio=0&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while setting &lt;code&gt;l1_ratio=1&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">Elastic-Netミキシングパラメータ、 &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; 。 &lt;code&gt;penalty='elasticnet'&lt;/code&gt; 場合にのみ使用されます。 &lt;code&gt;l1_ratio=0&lt;/code&gt; を設定することは、 &lt;code&gt;penalty='l2'&lt;/code&gt; を使用することと同等ですが、 &lt;code&gt;l1_ratio=1&lt;/code&gt; を設定することは、 &lt;code&gt;penalty='l1'&lt;/code&gt; を使用することと同等です。以下のために &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt; 、ペナルティは、L1とL2の組み合わせです。</target>
        </trans-unit>
        <trans-unit id="71317f65b5997839fb7f2d86b8c73dc4399ad254" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2.</source>
          <target state="translated">ElasticNet混合パラメーター、0 &amp;lt;l1_ratio &amp;lt;=1。l1_ratio= 1の場合、ペナルティはL1 / L2ペナルティです。l1_ratio = 0の場合、L2ペナルティです。以下のために &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 、ペナルティは、L1 / L2とL2の組み合わせです。</target>
        </trans-unit>
        <trans-unit id="f913d5ab29d41b1d0b8510f8960f88320058e481" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in &lt;code&gt;[.1, .5, .7,
.9, .95, .99, 1]&lt;/code&gt;</source>
          <target state="translated">ElasticNet混合パラメーター、0 &amp;lt;l1_ratio &amp;lt;=1。l1_ratio= 1の場合、ペナルティはL1 / L2ペナルティです。 l1_ratio = 0の場合、L2ペナルティです。以下のために &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 、ペナルティは、L1 / L2とL2の組み合わせです。このパラメーターはリストにすることができます。その場合、さまざまな値が交差検証によってテストされ、最高の予測スコアを与えるものが使用されます。 l1_ratioの値のリストの適切な選択は、 &lt;code&gt;[.1, .5, .7, .9, .95, .99, 1]&lt;/code&gt; 、。のように、多くの値を1（つまりLasso）に近づけ、0（つまりRidge）に近づけないことです。 95、.99、1]</target>
        </trans-unit>
        <trans-unit id="62ded71b403044434993092527c854b8f53e5c17" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. For &lt;code&gt;l1_ratio = 0&lt;/code&gt; the penalty is an L2 penalty. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; it is an L1 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">ElasticNetミキシングパラメーター &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; です。ため &lt;code&gt;l1_ratio = 0&lt;/code&gt; ペナルティL2ペナルティです。 &lt;code&gt;For l1_ratio = 1&lt;/code&gt; 場合、L1ペナルティです。以下のために &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 、ペナルティは、L1とL2の組み合わせです。</target>
        </trans-unit>
        <trans-unit id="706ede4c7ae02c53bd29139976a5c9ab95013fca" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a periodicity parameter periodicity&amp;gt;0. Only the isotropic variant where l is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">ExpSineSquaredカーネルでは、周期的な関数をモデリングできます。これは、長さスケールパラメーターlength_scale&amp;gt; 0および周期性パラメーター周期性&amp;gt; 0によってパラメーター化されます。現時点では、lがスカラーである等方性バリアントのみがサポートされています。以下によって与えられるカーネル：</target>
        </trans-unit>
        <trans-unit id="648ff6d48b0697b9c2efa5d7b7f9a5acf1d3efbe" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows one to model functions which repeat themselves exactly. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a periodicity parameter \(p&amp;gt;0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">ExpSineSquaredカーネルを使用すると、正確に繰り返される関数をモデル化できます。これは、長さスケールパラメーター\（l&amp;gt; 0 \）と周期性パラメーター\（p&amp;gt; 0 \）によってパラメーター化されます。現時点では、\（l \）がスカラーである等方性バリアントのみがサポートされています。カーネルは次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="31f2158684fd941b75a9b2f295ee3be83d4fb497" translate="yes" xml:space="preserve">
          <source>The Exponentiation kernel takes one base kernel and a scalar parameter \(p\) and combines them via</source>
          <target state="translated">Exponentiation カーネルは、ベースカーネルとスカラーパラメータを取り、\(p)を介してそれらを組み合わせます。</target>
        </trans-unit>
        <trans-unit id="bd1aecbb19e2286cfafdebb8292f1071a3eb508c" translate="yes" xml:space="preserve">
          <source>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</source>
          <target state="translated">F-betaスコアは、精度とリコールの加重調和平均として解釈することができ、F-betaスコアは1で最高値に達し、0で最悪値に達する。</target>
        </trans-unit>
        <trans-unit id="fcc8a075bfea5f42d0e38256200386182d33cc42" translate="yes" xml:space="preserve">
          <source>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</source>
          <target state="translated">F-βスコアは、精度とリコールの加重調和平均であり、最適値は1、最悪値は0に達します。</target>
        </trans-unit>
        <trans-unit id="553773b1d0f7903e424f857dbba13f4b2343a5a1" translate="yes" xml:space="preserve">
          <source>The F-beta score weights recall more than precision by a factor of &lt;code&gt;beta&lt;/code&gt;. &lt;code&gt;beta == 1.0&lt;/code&gt; means recall and precision are equally important.</source>
          <target state="translated">Fベータスコアの重みは、 &lt;code&gt;beta&lt;/code&gt; 係数で精度よりも再現されます。 &lt;code&gt;beta == 1.0&lt;/code&gt; は、再現率と精度が等しく重要であることを意味します。</target>
        </trans-unit>
        <trans-unit id="3f53f44feaa854c0fdf5365ef5e43bc7fa440b01" translate="yes" xml:space="preserve">
          <source>The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:</source>
          <target state="translated">F1スコアは、精度とリコールの加重平均として解釈することができ、F1スコアの最高値は1で、最悪値は0である。 F1スコアに対する精度とリコールの相対的な寄与は等しい。F1スコアの式は次のようになります。</target>
        </trans-unit>
        <trans-unit id="518509d08bd98ece386f11a6583f35b4f559fdac" translate="yes" xml:space="preserve">
          <source>The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:</source>
          <target state="translated">下の図は、カリフォルニア州の住宅データセットについて、4つの一方向と1つの二方向の部分依存性プロットを示しています。</target>
        </trans-unit>
        <trans-unit id="8d40f6c8f7d760d5a9c3769f72b85293905bcf21" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in a 2-dimensional parameter space (\(m=2\)) when \(R(w) = 1\).</source>
          <target state="translated">下図は、2次元パラメータ空間の正則化項の輪郭を示しています(\(m=2\)when η(R(w)=1\))。</target>
        </trans-unit>
        <trans-unit id="2280ec0b9d5ba5eef024fd7f6041defdab8c9d48" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in the parameter space when \(R(w) = 1\).</source>
          <target state="translated">下の図は、\(R(w)=1\)の時のパラメータ空間の異なる正則化項の輪郭を示しています。</target>
        </trans-unit>
        <trans-unit id="af21e4c771514d1aca6cd1c14dcff1ce67a0e477" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt;&lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt;&lt;/a&gt;) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:</source>
          <target state="translated">Fowlkes-Mallowsインデックス（&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt; &lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt; &lt;/a&gt;）は、サンプルのグラウンドトゥルースクラスの割り当てがわかっている場合に使用できます。 Fowlkes-MallowsスコアFMIは、ペアワイズ精度と再現率の幾何平均として定義されます。</target>
        </trans-unit>
        <trans-unit id="bfdc8ed29be1f90cbfb4dc9576ce17a736f93294" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall:</source>
          <target state="translated">Fowlkes-Mallows指数(FMI)は、精度とリコールの幾何学的平均値として定義されています。</target>
        </trans-unit>
        <trans-unit id="fb7ab21b6034e9b9d6397353d40d09f0a02e126e" translate="yes" xml:space="preserve">
          <source>The French Motor Third-Party Liability Claims dataset</source>
          <target state="translated">フランス自動車第三者賠償請求データセット</target>
        </trans-unit>
        <trans-unit id="90c804ef552ac504772d87a0c90e2454ea50931f" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">GP事前平均はゼロと見なされます。事前分布の共分散は、&lt;a href=&quot;#gp-kernels&quot;&gt;カーネル&lt;/a&gt;オブジェクトを渡すことによって指定されます。カーネルのハイパーパラメーターは、GaussianProcessRegressorのフィッティング中に、渡さ &lt;code&gt;optimizer&lt;/code&gt; 基づいてlog-marginal-likelihood（LML）を最大化することにより最適化されます。 LMLには複数のローカルオプティマがある場合があるため、 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; を指定することにより、オプティマイザーを繰り返し起動できます。最初の実行は常に、カーネルの初期ハイパーパラメーター値から開始されます。後続の実行は、許容値の範囲からランダムに選択されたハイパーパラメーター値から行われます。初期ハイパーパラメータを固定したままにする必要がある場合は、オプティマイザとして &lt;code&gt;None&lt;/code&gt; を渡すことができます。</target>
        </trans-unit>
        <trans-unit id="060746131ee7579fe50d723eccf0216e72cc0775" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">GPの事前平均はゼロと見なされます。事前の共分散は、&lt;a href=&quot;#gp-kernels&quot;&gt;カーネル&lt;/a&gt;オブジェクトを渡すことによって指定されます。カーネルのハイパーパラメータは、渡さ &lt;code&gt;optimizer&lt;/code&gt; 基づいて対数周辺尤度（LML）を最大化することにより、GaussianProcessRegressorのフィッティング中に最適化されます。 LMLには複数のローカルオプティマがある場合があるため、 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; を指定することでオプティマイザを繰り返し開始できます。最初の実行は、常にカーネルの初期ハイパーパラメータ値から開始して実行されます。後続の実行は、許可された値の範囲からランダムに選択されたハイパーパラメータ値から実行されます。初期ハイパーパラメータを固定しておく必要がある場合は、オプティマイザとして &lt;code&gt;None&lt;/code&gt; を渡すことができます。</target>
        </trans-unit>
        <trans-unit id="d9ff401cc389e116b429c312d580f50e1ed79479" translate="yes" xml:space="preserve">
          <source>The Gini coefficient (based on the area under the curve) can be used as a model selection metric to quantify the ability of the model to rank policyholders. Note that this metric does not reflect the ability of the models to make accurate predictions in terms of absolute value of total claim amounts but only in terms of relative amounts as a ranking metric.</source>
          <target state="translated">ジニ係数(曲線下面積に基づく)は、保険契約者をランク付けするモデルの能力を定量化するためのモデル選択の指標として使用することができる。この指標は、総保険金請求額の絶対値の観点から正確な予測を行うモデルの能力を反映しているのではなく、順位付けの指標としての相対的な金額の観点からのみ反映されていることに注意してください。</target>
        </trans-unit>
        <trans-unit id="18dcaaaea80b191d9100a9a4ae22f95aa636616a" translate="yes" xml:space="preserve">
          <source>The Gini index reflects the ability of a model to rank predictions irrespective of their absolute values, and therefore only assess their ranking power.</source>
          <target state="translated">ジニ指数は、絶対値に関係なく予測値をランク付けするモデルの能力を反映しており、したがって、そのランク付け力のみを評価します。</target>
        </trans-unit>
        <trans-unit id="01a974069deaad9f4a696951139fa6f180c4610d" translate="yes" xml:space="preserve">
          <source>The HLLE algorithm comprises three stages:</source>
          <target state="translated">HLLEアルゴリズムは3つの段階から構成されています。</target>
        </trans-unit>
        <trans-unit id="eaec5120030b979edc1dcb8e844874e0db8c76f1" translate="yes" xml:space="preserve">
          <source>The Hamming loss is the fraction of labels that are incorrectly predicted.</source>
          <target state="translated">ハミング損失とは、誤って予測されたラベルの割合のことです。</target>
        </trans-unit>
        <trans-unit id="304ac7ce83417558cbe44e7723fbf3a93a036677" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss, when &lt;code&gt;normalize&lt;/code&gt; parameter is set to True. It is always between 0 and 1, lower being better.</source>
          <target state="translated">&lt;code&gt;normalize&lt;/code&gt; パラメーターがTrueに設定されている場合、ハミング損失はサブセットのゼロ1損失によって上限が設定されます。常に0から1の間で、低いほど良いです。</target>
        </trans-unit>
        <trans-unit id="ee89b1587a7c7b8d189cccf628a41e55c5d7ebe4" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1.</source>
          <target state="translated">ハミング損失は、0-1損失のサブセットによって上限が設定されています。サンプルで正規化すると、ハミング損失は常に0と1の間になります。</target>
        </trans-unit>
        <trans-unit id="e3001f46869621e71f2b9c72a2ca6b3e38966a32" translate="yes" xml:space="preserve">
          <source>The Haversine (or great circle) distance is the angular distance between two points on the surface of a sphere. The first distance of each point is assumed to be the latitude, the second is the longitude, given in radians. The dimension of the data must be 2.</source>
          <target state="translated">ハヴァージン距離(または大円)は球体表面上の2点間の角距離である.各点の最初の距離を緯度とし、2番目の距離を経度とし、ラジアン単位で与えます。データの次元は2でなければならない.</target>
        </trans-unit>
        <trans-unit id="2f575d5541e123fdf35ce0dd5c6fb4373f14dde3" translate="yes" xml:space="preserve">
          <source>The Huber Regressor optimizes the squared loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; and the absolute loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt;, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.</source>
          <target state="translated">Huber Regressorは、 &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; サンプルの損失の2乗を最適化します。&amp;lt;イプシロンとサンプルの絶対損失 &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt; 、ここでwとsigmaは最適化されるパラメーターです。パラメータsigmaは、yが特定の係数で拡大または縮小された場合、同じロバスト性を実現するためにイプシロンを再スケーリングする必要がないことを確認します。これは、Xのさまざまな機能のスケールが異なる可能性があることを考慮していないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="a428acf86562c566ebfc0a4a1d3e42c700a1a75f" translate="yes" xml:space="preserve">
          <source>The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter &lt;code&gt;epsilon&lt;/code&gt;. This parameter depends on the scale of the target variables.</source>
          <target state="translated">Huberおよびイプシロンに依存しない損失関数は、ロバスト回帰に使用できます。不感領域の幅は、パラメータ &lt;code&gt;epsilon&lt;/code&gt; で指定する必要があります。このパラメーターは、ターゲット変数のスケールに依存します。</target>
        </trans-unit>
        <trans-unit id="0a2150cee6da11610a14f68e745e5d0639a39459" translate="yes" xml:space="preserve">
          <source>The Iris Dataset</source>
          <target state="translated">アイリスデータセット</target>
        </trans-unit>
        <trans-unit id="7c0ef25371cb6aecb434ff0290ad88905a1c2367" translate="yes" xml:space="preserve">
          <source>The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.</source>
          <target state="translated">アイリスのデータセットは、3種類のアイリス(セトサ、バーシカラー、ヴァージニカ)の花を、萼の長さ、萼の幅、花びらの長さ、花びらの幅の4つの属性で表現しています。</target>
        </trans-unit>
        <trans-unit id="3877561d1fbee6aff8708ef4ec5fcabe6115833e" translate="yes" xml:space="preserve">
          <source>The IsolationForest &amp;lsquo;isolates&amp;rsquo; observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</source>
          <target state="translated">IsolationForestは、ランダムに機能を選択し、選択した機能の最大値と最小値の間の分割値をランダムに選択することにより、観測を「分離」します。</target>
        </trans-unit>
        <trans-unit id="9e47d533732129c7b308a9673031913c04e17afd" translate="yes" xml:space="preserve">
          <source>The Isomap algorithm comprises three stages:</source>
          <target state="translated">Isomapアルゴリズムは3つの段階から構成されています。</target>
        </trans-unit>
        <trans-unit id="43f0627e56441244b0d44b83d4abbf8510d9eac0" translate="yes" xml:space="preserve">
          <source>The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">Jaccardインデックス[1]、またはJaccard類似度係数は、交差のサイズを2つのラベルセットの和集合のサイズで割ったものとして定義され、サンプルの予測ラベルのセットを &lt;code&gt;y_true&lt;/code&gt; の対応するラベルのセットと比較するために使用されます。</target>
        </trans-unit>
        <trans-unit id="54b0fb2f21ad6f516f16dd3562af3d3b867660fe" translate="yes" xml:space="preserve">
          <source>The Jaccard similarity coefficient of the \(i\)-th samples, with a ground truth label set \(y_i\) and predicted label set \(\hat{y}_i\), is defined as</source>
          <target state="translated">第1のサンプルのJaccard類似度係数は、真実のラベルと予測されたラベルのセットを用いて、次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="1b6ab133b67b3354e1eaee98fb0609deafb4a4e0" translate="yes" xml:space="preserve">
          <source>The Johnson-Lindenstrauss bound for embedding with random projections</source>
          <target state="translated">ランダム射影を用いた埋め込みのジョンソン-リンデンストラウス境界</target>
        </trans-unit>
        <trans-unit id="e9607194f44934f99bc18e8a1d649dcabdd2e40a" translate="yes" xml:space="preserve">
          <source>The K-means algorithm aims to choose centroids that minimise the &lt;strong&gt;inertia&lt;/strong&gt;, or &lt;strong&gt;within-cluster sum-of-squares criterion&lt;/strong&gt;:</source>
          <target state="translated">K-meansアルゴリズムは、&lt;strong&gt;慣性&lt;/strong&gt;を最小化する重心、または&lt;strong&gt;クラスター内の二乗和基準&lt;/strong&gt;を選択することを目的としています。</target>
        </trans-unit>
        <trans-unit id="2e420c9d276fc6531e2d5a1fd3367ac4465f247c" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">KDD Cup '99データセットは、MITリンカーンラボ[1]によって作成された1998 DARPA侵入検知システム（IDS）評価データセットのtcpdump部分を処理することによって作成されました。人工的なデータ（&lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;データセットのホームページで&lt;/a&gt;説明）は、クローズドネットワークと手動注入攻撃を使用して生成され、バックグラウンドで通常のアクティビティを伴うさまざまな種類の攻撃を多数生成しました。最初の目標は教師あり学習アルゴリズム用の大規模なトレーニングセットを作成することであったため、現実世界では非現実的であり、「異常な」データの検出を目的とする教師なしの異常検出には不適切な異常データの割合が大きく（80.1％）、すなわち</target>
        </trans-unit>
        <trans-unit id="c580c78b1750c851ebf65b6b32d643554e25c44f" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">KDDカップ'99データセットは、MITリンカーン研究所によって作成された1998年のDARPA侵入検知システム（IDS）評価データセットのtcpdump部分を処理することによって作成されました[1]。人工データ（&lt;a href=&quot;https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;データセットのホームページに記載&lt;/a&gt;）は、クローズドネットワークと手作業による攻撃を使用して生成され、バックグラウンドで通常のアクティビティを伴う多数の異なるタイプの攻撃を生成しました。当初の目標は教師あり学習アルゴリズム用の大規模なトレーニングセットを作成することであったため、現実世界では非現実的であり、「異常な」データの検出を目的とした教師なし異常検出には不適切な異常データの大部分（80.1％）があります。すなわち、</target>
        </trans-unit>
        <trans-unit id="42488af2a88df53e7b3301c7d5a125afdc9cd2ce" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.</source>
          <target state="translated">元の空間と埋め込まれた空間の共同確率の Kullback-Leibler (KL)発散は勾配降下法によって最小化される.KL発散は凸ではないことに注意してください。つまり、異なる初期化で複数回の再起動を行うと、KL発散の局所的な最小値になってしまいます。そのため,異なるシードを試してみて,KL発散が最も小さい埋め込みを選択することが有用な場合もあります.</target>
        </trans-unit>
        <trans-unit id="c1f022c5d8701b9b00fce305ae262139841dede3" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use 0 for no regularization.</source>
          <target state="translated">L2正則化パラメータ。正則化を行わない場合は0を使用します。</target>
        </trans-unit>
        <trans-unit id="5b439efc6a95c460b62f48e9b2ed58ff5ae6d780" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use &lt;code&gt;0&lt;/code&gt; for no regularization (default).</source>
          <target state="translated">L2正則化パラメーター。正則化を行わない場合は &lt;code&gt;0&lt;/code&gt; を使用します（デフォルト）。</target>
        </trans-unit>
        <trans-unit id="9a35ab88c062292779ce626a30b50f45159613b2" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">LARSモデルは、推定器&lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt;、またはその低レベルの実装&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt; &lt;code&gt;lars_path_gram&lt;/code&gt; を&lt;/a&gt;使用して使用できます。</target>
        </trans-unit>
        <trans-unit id="5c5e824f9a2e5664a81c2abf8d8de696419cc018" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">LARSモデルは、推定器&lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt;またはその低レベルの実装&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; を&lt;/a&gt;使用して使用できます。</target>
        </trans-unit>
        <trans-unit id="5604e6549ba6e538c77a361942c228433b92a65e" translate="yes" xml:space="preserve">
          <source>The LTSA algorithm comprises three stages:</source>
          <target state="translated">LTSAアルゴリズムは3つのステージから構成されています。</target>
        </trans-unit>
        <trans-unit id="5921a49032d4468242b223b9e118c35472eff0fa" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation consist of retrieving the path with function &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Larsアルゴリズムは、ほぼ無料で正則化パラメーターに沿った係数のフルパスを提供します。したがって、一般的な操作は、関数&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; を使用し&lt;/a&gt;てパスを取得することで構成されます</target>
        </trans-unit>
        <trans-unit id="066ac8e9a006935363e41b727fa078b6cbe4a689" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation is to retrieve the path with one of the functions &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Larsアルゴリズムは、正則化パラメーターに沿った係数のフルパスをほぼ無料で提供します。したがって、一般的な操作は、関数&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt; &lt;code&gt;lars_path_gram&lt;/code&gt; の&lt;/a&gt;いずれかを使用してパスを取得することです。</target>
        </trans-unit>
        <trans-unit id="9503ea7db44738c355b5bdf4d0264864cc9e2161" translate="yes" xml:space="preserve">
          <source>The Lasso is a linear model that estimates sparse coefficients with l1 regularization.</source>
          <target state="translated">Lassoは、l1正則化で疎な係数を推定する線形モデルです。</target>
        </trans-unit>
        <trans-unit id="0f5aa687d48724082c940128def6bdb3f6066405" translate="yes" xml:space="preserve">
          <source>The Lasso optimization function varies for mono and multi-outputs.</source>
          <target state="translated">ラッソ最適化機能は、モノ出力とマルチ出力で異なります。</target>
        </trans-unit>
        <trans-unit id="3c1b2c3b1551762e6f1da2d8a29e9acf6404c123" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">使用するLassoソルバー:座標降下法またはLARS。特徴量の数がサンプル数よりも多い、非常に疎なグラフにはLARSを使用します。その他の場合は、数値的に安定しているcdを使用します。</target>
        </trans-unit>
        <trans-unit id="2de8b3228aef7c3b031e79cb56d858268d66f64e" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p &amp;gt; n. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">使用する投げ縄ソルバー：座標降下またはLARS。LARSを使用して、p&amp;gt; nの非常にスパースなグラフを作成します。他の場所では、数値的に安定したcdを好みます。</target>
        </trans-unit>
        <trans-unit id="59ce670bca31c79340517b84d85b37c8ca4886a2" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">共分散行列のLedoit-Wolf推定量は、&lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; &lt;/a&gt;パッケージの&lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt;関数を使用してサンプルで計算できます。または、&lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt;オブジェクトを同じサンプルに適合させることで取得できます。</target>
        </trans-unit>
        <trans-unit id="6c4c48b93f6a66f7991382ba54fcfe5fb18a6ebb" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">共分散行列のLedoit-Wolf推定量は、 &lt;code&gt;sklearn.covariance&lt;/code&gt; パッケージの&lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt;関数を使用してサンプルで計算できます。または、同じサンプルに&lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt;オブジェクトを当てはめることで取得できます。</target>
        </trans-unit>
        <trans-unit id="aec2f6a4d0fb4e3ecc2ba56ea33e122b851a535b" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset constains two small dataset:</source>
          <target state="translated">Linnerudデータセットは、2つの小さなデータセットを保持しています。</target>
        </trans-unit>
        <trans-unit id="6be9aee52b8392e35ed4aeda9bdbad80150f0295" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club:</source>
          <target state="translated">Linnerudデータセットは多出力回帰データセットである。これは、フィットネスクラブに所属する20人の中年男性から収集した3つの運動変数(データ)と3つの生理学的変数(ターゲット)から構成されています。</target>
        </trans-unit>
        <trans-unit id="0460202c91e1eee28482597354a8366af4e5eb02" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for novelty detection. Note that when LOF is used for novelty detection you MUST not use predict, decision_function and score_samples on the training set as this would lead to wrong results. You must only use these methods on new unseen data (which are not in the training set). See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for outlier detection.</source>
          <target state="translated">Local Outlier Factor（LOF）アルゴリズムは、監視されていない異常検出方法で、特定のデータポイントの近傍に対する局所密度偏差を計算します。隣接するサンプルよりも密度が大幅に低いサンプルを異常値と見なします。この例は、LOFをノベルティ検出に使用する方法を示しています。 LOFが目新しさの検出に使用されている場合、誤った結果につながる可能性があるため、トレーニングセットでpredict、decision_function、score_samplesを使用しないでください。これらのメソッドは、新しく表示されないデータ（トレーニングセットに含まれていない）に対してのみ使用する必要があります。参照してください。&lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;ユーザーガイド&lt;/a&gt;：外れ値検出とノベルティの検出方法と、外れ値の検出のためのLOFを使用する方法の違いの詳細については。</target>
        </trans-unit>
        <trans-unit id="d0e3831e71a419f2c6e3df80ad462189c4208703" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection.</source>
          <target state="translated">Local Outlier Factor（LOF）アルゴリズムは、監視されていない異常検出方法であり、特定のデータポイントの近傍に対する局所密度偏差を計算します。隣接するサンプルよりも密度が大幅に低いサンプルを異常値と見なします。この例は、外れ値の検出にLOFを使用する方法を示しています。これは、scikit-learnでのこの推定器のデフォルトの使用例です。LOFが外れ値の検出に使用される場合、LODにはpredict、decision_function、score_samplesメソッドがないことに注意してください。参照してください。&lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;ユーザーガイド&lt;/a&gt;：外れ値検出とノベルティの検出方法と、新規性検出のためのLOFを使用する方法の違いの詳細については。</target>
        </trans-unit>
        <trans-unit id="e334ea68d0fb9bc258e189ffac7524671f8600d5" translate="yes" xml:space="preserve">
          <source>The MLLE algorithm comprises three stages:</source>
          <target state="translated">MLLEのアルゴリズムは3つの段階から構成されています。</target>
        </trans-unit>
        <trans-unit id="514baa8efb372e193dd4209109d33c4d6c755f88" translate="yes" xml:space="preserve">
          <source>The Matplotlib Figure object.</source>
          <target state="translated">Matplotlib 図形オブジェクト。</target>
        </trans-unit>
        <trans-unit id="5adf2471ff49dbf27acc1a8b4862b530e5b52366" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).</source>
          <target state="translated">マシューズ相関係数(+1は完全予測、0は平均ランダム予測、-1と逆予測を表す)。</target>
        </trans-unit>
        <trans-unit id="21efcbf7188af02e8a17818fd892c631c3cc436c" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. [source: Wikipedia]</source>
          <target state="translated">マシューズ相関係数は,バイナリおよびマルチクラス分類の品質の尺度として機械学習で使用される.これは,真正と偽正と偽負を考慮に入れており,一般に,クラスのサイズが非常に異なる場合でも使用できるバランスのとれた尺度とみなされている.MCCは,本質的には-1と+1の間の相関係数の値である.係数が+1の場合は完全な予測、0の場合は平均的なランダム予測、-1の場合は逆予測を表します。この統計量はファイ係数としても知られている。出典:ウィキペディア</target>
        </trans-unit>
        <trans-unit id="6faebd1cede47746805364be40ce28b059dea40d" translate="yes" xml:space="preserve">
          <source>The Mean Squared Error (in the sense of the Frobenius norm) between &lt;code&gt;self&lt;/code&gt; and &lt;code&gt;comp_cov&lt;/code&gt; covariance estimators.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; と &lt;code&gt;comp_cov&lt;/code&gt; の共分散推定量間の平均二乗誤差（フロベニウスノルムの意味で）。</target>
        </trans-unit>
        <trans-unit id="d3bca24cc7fb644916020d52b90a1ddb7136c5be" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.</source>
          <target state="translated">最小共分散決定子共分散推定量は、ガウス分布のデータに適用されますが、単モーダルで対称な分布から引き出されたデータにも適用できます。これは、マルチモーダルデータでの使用を意図したものではありません(MinCovDetオブジェクトを適合させるために使用されるアルゴリズムは、このような場合に失敗する可能性が高いです)。マルチモーダルデータを扱うためには,射影追跡法を検討する必要があります.</target>
        </trans-unit>
        <trans-unit id="451133ac20cdcd95892f17968e1c348a4b0f4b8f" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">最小共分散行列式推定量（MCD）は、PJRousseuwによって&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;で導入されました。</target>
        </trans-unit>
        <trans-unit id="e452597962db33c03eccea44c483adb03cd8dfa7" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;3&lt;/a&gt;.</source>
          <target state="translated">最小共分散行列式推定量（MCD）は、PJRousseuwによって&lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;3で&lt;/a&gt;導入されました。</target>
        </trans-unit>
        <trans-unit id="d811e159b509e69af06fa8b1a6b421d8a9ea40ca" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in [1].</source>
          <target state="translated">最小共分散決定量推定器(MCD)は、P.J.Rousseuwによって[1]で紹介されています。</target>
        </trans-unit>
        <trans-unit id="ecfb18631be5aab9ee26a60642b369e5e4e48a3b" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;3&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">最小共分散行列式推定量は、PJRousseeuwによって&lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;3で&lt;/a&gt;導入されたデータセットの共分散のロバスト推定量です。アイデアは、外れ値ではない「良好な」観測値の特定の比率（h）を見つけて、それらの経験的共分散行列を計算することです。次に、この経験的共分散行列は、実行された観測値の選択を補正するために再スケーリングされます（「整合性ステップ」）。最小共分散行列式推定量を計算すると、マハラノビス距離に従って観測値に重みを付けることができ、データセットの共分散行列の再重み付けされた推定につながります（「再重み付けステップ」）。</target>
        </trans-unit>
        <trans-unit id="92f2af418b12a7a58727045874e736340d33efbd" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">最小共分散行列式推定量は、PJ Rousseeuw &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;によって導入されたデータセットの共分散のロバスト推定量です。アイデアは、外れ値ではない「良い」観測値の所定の割合（h）を見つけ、その経験的共分散行列を計算することです。次に、この経験的共分散行列は、実行された観測の選択を補正するために再スケーリングされます（「整合性ステップ」）。最小共分散行列式推定量を計算したら、マハラノビス距離に従って観測に重みを付けることができ、データセットの共分散行列の再重み付けされた推定につながります（「再重み付けステップ」）。</target>
        </trans-unit>
        <trans-unit id="26a5ec87ffc45e2d236a64ca891cc8c307910b23" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples} - n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples} + n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance. After a correction step aiming at compensating the fact that the estimates were learned from only a portion of the initial data, we end up with robust estimates of the data set location and covariance.</source>
          <target state="translated">最小共分散行列式推定量は、堅牢で高分解のポイントです（つまり、高度に汚染されたデータセットの共分散行列を\（\ frac {n_ \ text {samples}-n_ \ text {features}-まで推定するために使用できます） 1} {2} \）外れ値）共分散の推定量。アイデアは、経験的共分散が最小の行列式を持つ\（\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \）観測を見つけ、観測の「純粋な」サブセットを生成することです。場所と共分散の標準推定値を計算します。推定値が初期データの一部からのみ学習されたという事実を補償することを目的とした修正ステップの後、データセットの場所と共分散のロバストな推定値が得られます。</target>
        </trans-unit>
        <trans-unit id="868c6b821132444289ad5893f70525225594997c" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples}-n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples}+n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance.</source>
          <target state="translated">最小共分散行列式推定量は、堅牢で高分解のポイントです（つまり、非常に汚染されたデータセットの共分散行列を\（\ frac {n_ \ text {samples} -n_ \ text {features}-まで推定するために使用できます） 1} {2} \）外れ値）共分散の推定量。アイデアは、経験的共分散が最小の行列式を持つ\（\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \）観測を見つけて、観測の「純粋な」サブセットを生成することです。場所と共分散の標準推定値を計算します。</target>
        </trans-unit>
        <trans-unit id="b2ce758bd900de6631654a32b9b8962161ffb45f" translate="yes" xml:space="preserve">
          <source>The Mutual Information is a measure of the similarity between two labels of the same data. Where \(|U_i|\) is the number of the samples in cluster \(U_i\) and \(|V_j|\) is the number of the samples in cluster \(V_j\), the Mutual Information between clusterings \(U\) and \(V\) is given as:</source>
          <target state="translated">相互情報とは、同じデータのラベル間の類似度を測定したものです。ここで、\(|U_i|\)はクラスター内のサンプル数、\(|V_j|\)はクラスター内のサンプル数であり、クラスター間の相互情報は次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="4607a569600ef039f2071e80730b55efccd8dd8d" translate="yes" xml:space="preserve">
          <source>The Nystroem method, as implemented in &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; uses the &lt;code&gt;rbf&lt;/code&gt; kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter &lt;code&gt;n_components&lt;/code&gt;.</source>
          <target state="translated">Nystroemで実装されている&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;メソッドは、カーネルの低ランク近似の一般的な方法です。これは、カーネルが評価されるデータを本質的にサブサンプリングすることによってこれを実現します。デフォルトでは、&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;は &lt;code&gt;rbf&lt;/code&gt; カーネルを使用しますが、任意のカーネル関数または事前計算されたカーネルマトリックスを使用できます。使用されるサンプルの数-これは計算された特徴の次元でもあります-は、パラメーター &lt;code&gt;n_components&lt;/code&gt; によって指定されます。</target>
        </trans-unit>
        <trans-unit id="acd97cd02b3cf6e9137b2a9dda0c8e62c31c52e4" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">共分散行列のOAS推定量は、&lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; &lt;/a&gt;パッケージの&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt;関数を使用してサンプルで計算できます。または、同じサンプルに&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt;オブジェクトを適合させることで取得できます。</target>
        </trans-unit>
        <trans-unit id="ad47de32445041d6ef99f77621d867a03cd9751f" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">共分散行列のOAS推定量は、 &lt;code&gt;sklearn.covariance&lt;/code&gt; パッケージの&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt;関数を使用してサンプルで計算できます。または、&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt;オブジェクトを同じサンプルに適合させることで取得できます。</target>
        </trans-unit>
        <trans-unit id="6f45a4e82bc4c1b489f40318ecca4028acd0f46e" translate="yes" xml:space="preserve">
          <source>The One-Class SVM has been introduced by Sch&amp;ouml;lkopf et al. for that purpose and implemented in the &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; module in the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The \(\nu\) parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.</source>
          <target state="translated">One-Class SVMはSch&amp;ouml;lkopfらによって導入されました。その目的のために、&lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt;オブジェクトの&lt;a href=&quot;svm#svm&quot;&gt;サポートベクターマシン&lt;/a&gt;モジュールに実装されています。フロンティアを定義するには、カーネルとスカラーパラメーターの選択が必要です。通常、RBFカーネルが選択されますが、その帯域幅パラメーターを設定するための正確な式やアルゴリズムはありません。これは、scikit-learn実装のデフォルトです。 \（\ nu \）パラメータは、One-Class SVMのマージンとも呼ばれ、フロンティアの外にある新しい、ただし規則的な観測値を見つける確率に対応しています。</target>
        </trans-unit>
        <trans-unit id="ce00ae4f245475e7026810e89464783863843edd" translate="yes" xml:space="preserve">
          <source>The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.</source>
          <target state="translated">PCAは教師なしの次元削減を行い、ロジスティック回帰は予測を行う。</target>
        </trans-unit>
        <trans-unit id="04f0f7d7b53f41cd954be6291735d195a21f52b7" translate="yes" xml:space="preserve">
          <source>The Poisson deviance cannot be computed on non-positive values predicted by the model. For models that do return a few non-positive predictions (e.g. &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;) we ignore the corresponding samples, meaning that the obtained Poisson deviance is approximate. An alternative approach could be to use &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt;&lt;code&gt;TransformedTargetRegressor&lt;/code&gt;&lt;/a&gt; meta-estimator to map &lt;code&gt;y_pred&lt;/code&gt; to a strictly positive domain.</source>
          <target state="translated">ポアソン逸脱度は、モデルによって予測された正でない値では計算できません。いくつかの非正の予測を返すモデル（&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; など&lt;/a&gt;）の場合、対応するサンプルを無視します。つまり、取得されたポアソン逸脱度は近似値です。別のアプローチは、&lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt; &lt;code&gt;TransformedTargetRegressor&lt;/code&gt; &lt;/a&gt;メタ推定器を使用して &lt;code&gt;y_pred&lt;/code&gt; を厳密に正のドメインにマップすることです。</target>
        </trans-unit>
        <trans-unit id="bc5a34aa26f7428f3e35a346a70f3689822fe771" translate="yes" xml:space="preserve">
          <source>The Poisson deviance computed as an evaluation metric reflects both the calibration and the ranking power of the model. It also makes a linear assumption on the ideal relationship between the expected value and the variance of the response variable. For the sake of conciseness we did not check whether this assumption holds.</source>
          <target state="translated">評価指標として計算されたポアソン・ディビアンスは,モデルの較正と順位付け力の両方を反映する.また、期待値と応答変数の分散の間の理想的な関係について線形の仮定を行う。簡潔さのために、この仮定が保持されているかどうかをチェックしませんでした。</target>
        </trans-unit>
        <trans-unit id="450889f232106f285e0f6ca8296ef879dd035563" translate="yes" xml:space="preserve">
          <source>The Probability Density Functions (PDF) of these distributions are illustrated in the following figure,</source>
          <target state="translated">これらの分布の確率密度関数(PDF)を下図に示します。</target>
        </trans-unit>
        <trans-unit id="935532caca213849eac9af7588fc52963397d517" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">&lt;code&gt;multioutput='uniform_average'&lt;/code&gt; &lt;code&gt;score&lt;/code&gt; を呼び出すときに使用されるR2スコアは、バージョン0.23のmultioutput = 'uniform_average'を使用して、デフォルト値の&lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;との整合性を維持します。これは、すべてのマルチ出力リグレッサ（&lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor&quot;&gt; &lt;code&gt;MultiOutputRegressor&lt;/code&gt; &lt;/a&gt;を除く）の &lt;code&gt;score&lt;/code&gt; メソッドに影響します。</target>
        </trans-unit>
        <trans-unit id="11e4fa9675506dfc6e48ea958dcc499e9784ad90" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;sklearn.multioutput.multioutputregressor#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">&lt;code&gt;multioutput='uniform_average'&lt;/code&gt; &lt;code&gt;score&lt;/code&gt; を呼び出すときに使用されるR2スコアは、バージョン0.23のmultioutput = 'uniform_average'を使用して、デフォルト値の&lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;との整合性を維持します。これは、すべてのマルチ出力リグレッサ（&lt;a href=&quot;sklearn.multioutput.multioutputregressor#sklearn.multioutput.MultiOutputRegressor&quot;&gt; &lt;code&gt;MultiOutputRegressor&lt;/code&gt; &lt;/a&gt;を除く）の &lt;code&gt;score&lt;/code&gt; メソッドに影響します。</target>
        </trans-unit>
        <trans-unit id="a83459e6ef7baec271df4e0325c8da4e8711242a" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">RBFカーネルは定常カーネルです。これは、「二乗指数」カーネルとしても知られています。これは、長さスケールパラメーター\（l&amp;gt; 0 \）によってパラメーター化されます。これは、スカラー（カーネルの等方性バリアント）または入力Xと同じ次元数のベクトル（カーネルの異方性バリアント）のいずれかです。カーネルは次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="33c279bdcb922f55f225b113a682e1a5cf28cc6d" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter length_scale&amp;gt;0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">RBFカーネルは固定カーネルです。「二乗指数」カーネルとしても知られています。これは、長さスケールパラメーターlength_scale&amp;gt; 0によってパラメーター化されます。これは、スカラー（カーネルの等方性バリアント）または入力Xと同じ次元数のベクトル（カーネルの異方性バリアント）のいずれかです。カーネルは以下によって与えられます：</target>
        </trans-unit>
        <trans-unit id="f47997c4010f2e20accd04690970d6547a71bfc2" translate="yes" xml:space="preserve">
          <source>The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.</source>
          <target state="translated">RBF カーネルは,メモリ上で密な行列で表現される完全に接続されたグラフを生成します.この行列は非常に大きくなる可能性があり,アルゴリズムの各繰り返しに対して完全な行列の乗算計算を実行するコストと相まって,法外に長い実行時間をもたらす可能性があります.一方,KNN カーネルは,よりメモリに優しい疎な行列を生成し,実行時間を大幅に短縮することができます.</target>
        </trans-unit>
        <trans-unit id="512de356729b7d551b43d5d76bd8681dea941e19" translate="yes" xml:space="preserve">
          <source>The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.</source>
          <target state="translated">RBMは、特定のグラフィカルモデルを使用してデータの可能性を最大化しようとします。使用されるパラメーター学習アルゴリズム（&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;）は、表現が入力データから遠く離れることを防ぎます。これにより、興味深い規則性を捉えることができますが、小さなデータセットではモデルの有用性は低くなり、通常は密度推定には役立ちません。</target>
        </trans-unit>
        <trans-unit id="95d48012cfd5cdfda48a80ad45de8292db32665c" translate="yes" xml:space="preserve">
          <source>The R^2 score or ndarray of scores if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">'multioutput'が 'raw_values'の場合​​、R ^ 2スコアまたはスコアのndarray。</target>
        </trans-unit>
        <trans-unit id="57cf85e67b74d4b36ce4e00a60cc2afb37409d42" translate="yes" xml:space="preserve">
          <source>The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.</source>
          <target state="translated">Rand Indexは、すべてのサンプルのペアを考慮し、予測されたクラスタリングと真のクラスタリングで同じクラスタまたは異なるクラスタに割り当てられたペアをカウントすることで、2つのクラスタリング間の類似性の尺度を計算します。</target>
        </trans-unit>
        <trans-unit id="21d3f8892ca41bf337636e90759152be22d9788c" translate="yes" xml:space="preserve">
          <source>The RandomTreesEmbedding, from the &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module, is not technically a manifold embedding method, as it learn a high-dimensional representation on which we apply a dimensionality reduction method. However, it is often useful to cast a dataset into a representation in which the classes are linearly-separable.</source>
          <target state="translated">&lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;モジュールのRandomTreesEmbeddingは、次元削減法を適用する高次元表現を学習するため、技術的には多様体埋め込み法ではありません。ただし、データセットをクラスが線形分離可能な表現にキャストすると便利な場合があります。</target>
        </trans-unit>
        <trans-unit id="9863f2af9163cbec042d3a8db1d9d0da7a79bddf" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length scales. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a scale mixture parameter \(\alpha&amp;gt;0\). Only the isotropic variant where length_scale \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">RationalQuadraticカーネルは、異なる特徴的な長さスケールを持つRBFカーネルのスケール混合（無限の合計）と見なすことができます。これは、長さスケールパラメーター\（l&amp;gt; 0 \）とスケール混合パラメーター\（\ alpha&amp;gt; 0 \）によってパラメーター化されます。現時点では、length_scale \（l \）がスカラーである等方性バリアントのみがサポートされています。カーネルは次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="ddaea903844387569c2d558e504bb67163af0e53" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a scale mixture parameter alpha&amp;gt;0. Only the isotropic variant where length_scale is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">RationalQuadraticカーネルは、異なる特性長スケールを持つRBFカーネルのスケール混合（無限合計）と見なすことができます。これは、長さスケールパラメーターlength_scale&amp;gt; 0およびスケール混合パラメーターalpha&amp;gt; 0によってパラメーター化されます。現在、length_scaleがスカラーである等方性バリアントのみがサポートされています。以下によって与えられるカーネル：</target>
        </trans-unit>
        <trans-unit id="4290d451a4bc3d8974b88b0877f706f5c5e0d4c1" translate="yes" xml:space="preserve">
          <source>The SAGA solver supports both float64 and float32 bit arrays.</source>
          <target state="translated">SAGAソルバーは、float64ビットとfloat32ビットの両方の配列をサポートしています。</target>
        </trans-unit>
        <trans-unit id="66cebb865fc92b18216814d7cca47981b6cb6b41" translate="yes" xml:space="preserve">
          <source>The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the &lt;em&gt;stress&lt;/em&gt;) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.</source>
          <target state="translated">SMACOF（COmplicated Function をメジャー化することによるスケーリング）アルゴリズムは、多次元スケーリングアルゴリズムで、メジャー化手法を使用して目的関数（&lt;em&gt;ストレス&lt;/em&gt;）を最小化します。ガットマン変換とも呼ばれるストレスメジャー化は、ストレスの単調な収束を保証し、勾配降下法などの従来の手法よりも強力です。</target>
        </trans-unit>
        <trans-unit id="e2c4971923aff3f7fe2f8678fb678a8a1fe12716" translate="yes" xml:space="preserve">
          <source>The SMACOF algorithm for metric MDS can summarized by the following steps:</source>
          <target state="translated">メトリックMDSのためのSMACOFアルゴリズムは、以下のステップで要約することができます。</target>
        </trans-unit>
        <trans-unit id="9991accadfafce42fd0097ac5bf1452a92166503" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient &lt;em&gt;s&lt;/em&gt; for a single sample is then given as:</source>
          <target state="translated">単一のサンプルのシルエット係数&lt;em&gt;s&lt;/em&gt;は、次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="977018d27e576be441891586d29895a67820e9bb" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.</source>
          <target state="translated">サンプルの集合に対するシルエット係数は、各サンプルのシルエット係数の平均値として与えられる。</target>
        </trans-unit>
        <trans-unit id="a8aa62253fcb64807b1a0f2a014811a76f4c09cc" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.</source>
          <target state="translated">シルエット係数は、サンプルがそれ自身に似たサンプルでどれだけよくクラスタリングされているかを示す指標である。シルエット係数が高いクラスタリングモデルは、同じクラスタ内のサンプルが互いに類似している場合は密であり、異なるクラスタ内のサンプルが互いにあまり類似していない場合はよく分離されていると言われています。</target>
        </trans-unit>
        <trans-unit id="5da8604230381cdf66d3aec27ea08ace466e606c" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">シルエット係数は、各サンプルの平均クラスター内距離（ &lt;code&gt;a&lt;/code&gt; ）と平均最近傍クラスター距離（ &lt;code&gt;b&lt;/code&gt; ）を使用して計算されます。サンプルのシルエット係数は &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; です。シルエット係数は、ラベルの数が2 &amp;lt;= n_labels &amp;lt;= n_samples-1の場合にのみ定義されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="4273a368cac49099b8caa7c819b214a9d85c0ff4" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. To clarify, &lt;code&gt;b&lt;/code&gt; is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">シルエット係数は、各サンプルの平均クラスター内距離（ &lt;code&gt;a&lt;/code&gt; ）と平均最近傍クラスター距離（ &lt;code&gt;b&lt;/code&gt; ）を使用して計算されます。サンプルのシルエット係数は &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; です。明確にするために、 &lt;code&gt;b&lt;/code&gt; は、サンプルと、そのサンプルが含まれていない最も近いクラスターとの間の距離です。シルエット係数は、ラベルの数が2 &amp;lt;= n_labels &amp;lt;= n_samples-1の場合にのみ定義されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="74a577cf3f9facf0caa36a3cd46ce91a25197ef1" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">シルエット係数は、DBSCANで得られるような密度ベースのクラスターのような他のクラスターの概念に比べて、一般的に凸型クラスターの方が高い。</target>
        </trans-unit>
        <trans-unit id="b08373a116f8a28b8f59b4a939f8e86bc18a285c" translate="yes" xml:space="preserve">
          <source>The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result.</source>
          <target state="translated">データからスピアマン相関係数を推定し、その結果の符号を結果とします。</target>
        </trans-unit>
        <trans-unit id="1aca461bec942a3ea49c28ca350e80c03610ed92" translate="yes" xml:space="preserve">
          <source>The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:</source>
          <target state="translated">スペクトル埋め込み(ラプラシアン固有マップ)アルゴリズムは、3つの段階から構成されています。</target>
        </trans-unit>
        <trans-unit id="5c2cf1989e094ea2eafd60dd4bfcc701bf5b4819" translate="yes" xml:space="preserve">
          <source>The Stack Exchange family of sites hosts &lt;a href=&quot;https://meta.stackexchange.com/q/130524&quot;&gt;multiple subdomains for Machine Learning questions&lt;/a&gt;.</source>
          <target state="translated">Stack Exchangeファミリーのサイトは&lt;a href=&quot;https://meta.stackexchange.com/q/130524&quot;&gt;、機械学習の質問用に複数のサブドメインを&lt;/a&gt;ホストしています。</target>
        </trans-unit>
        <trans-unit id="530048073ab2f0fd4beae1a41150629fce3bbc04" translate="yes" xml:space="preserve">
          <source>The TF-IDF vectorized posts form a word frequency matrix, which is then biclustered using Dhillon&amp;rsquo;s Spectral Co-Clustering algorithm. The resulting document-word biclusters indicate subsets words used more often in those subsets documents.</source>
          <target state="translated">TF-IDFのベクトル化されたポストは単語頻度マトリックスを形成し、その後DhillonのSpectral Co-Clusteringアルゴリズムを使用してバイクラスター化されます。結果として得られるドキュメント単語バイクラスタは、これらのサブセットドキュメントでより頻繁に使用されるサブセット単語を示します。</target>
        </trans-unit>
        <trans-unit id="a51b4bd7337a8a43bf76bb00c70355d636e98cf2" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">Vメジャーは実際には上記の相互情報量（NMI）に相当し、集計関数は算術平均です&lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4af22841a0f64abdacec5694995ece03a0f97488" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id16&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">Vメジャーは、実際には上記の相互情報量（NMI）と同等であり、集計関数は算術平均です&lt;a href=&quot;#b2011&quot; id=&quot;id16&quot;&gt;[B2011]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="28e22a992327910c0281c7997fd0381a055b2da5" translate="yes" xml:space="preserve">
          <source>The V-measure is the harmonic mean between homogeneity and completeness:</source>
          <target state="translated">Vメジャーは、均質性と完全性の間の調和平均です。</target>
        </trans-unit>
        <trans-unit id="8e9e469c4bec48fbecca2280bcc57b2302181e26" translate="yes" xml:space="preserve">
          <source>The WAGE is increasing when EDUCATION is increasing. Note that the dependence between WAGE and EDUCATION represented here is a marginal dependence, i.e., it describes the behavior of a specific variable without keeping the others fixed.</source>
          <target state="translated">EDUCATIONが増加しているとき,WAGEは増加している.ここで表されるWAGEとEDUCATIONの間の依存性は、限界依存性であることに注意してください、すなわち、他の変数を固定することなく、特定の変数の動作を記述します。</target>
        </trans-unit>
        <trans-unit id="9d831d6fbe54cab7c78889ea027eda02af846327" translate="yes" xml:space="preserve">
          <source>The Yeo-Johnson transform is given by:</source>
          <target state="translated">Yeo-Johnson変換は次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="dd310b3ef8cade9b7b44463cf24d9960d1a07902" translate="yes" xml:space="preserve">
          <source>The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \(Z\):</source>
          <target state="translated">2 番目から始まる単数ベクトルは、所望のパーティショニング情報を提供する。それらは、行列を形成するために使用されます。</target>
        </trans-unit>
        <trans-unit id="0151312e57232e026343f2a42d71b34eecec6ec6" translate="yes" xml:space="preserve">
          <source>The \(\nu\)-SVC formulation &lt;a href=&quot;#id17&quot; id=&quot;id8&quot;&gt;15&lt;/a&gt; is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="translated">\（\ nu \）-SVC定式化&lt;a href=&quot;#id17&quot; id=&quot;id8&quot;&gt;15&lt;/a&gt;は、\（C \）-SVCの再パラメータ化であるため、数学的に同等です。</target>
        </trans-unit>
        <trans-unit id="6bae7eee8e57958cb7a015c09e85982ab959fe35" translate="yes" xml:space="preserve">
          <source>The \(k\)-neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; is the most commonly used technique. The optimal choice of the value \(k\) is highly data-dependent: in general a larger \(k\) suppresses the effects of noise, but makes the classification boundaries less distinct.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt;の\（k \）-neighbors分類は、最も一般的に使用される手法です。値\（k \）の最適な選択は、データに大きく依存します。一般に、\（k \）を大きくすると、ノイズの影響が抑制されますが、分類境界が明確になりません。</target>
        </trans-unit>
        <trans-unit id="e8862ed89c87547157e194d96a8f490026b2e0fb" translate="yes" xml:space="preserve">
          <source>The ability to reproduce the data of the regularized model is similar to the one of the non-regularized model.</source>
          <target state="translated">正則化モデルのデータを再現する能力は、非正則化モデルのデータを再現する能力に似ています。</target>
        </trans-unit>
        <trans-unit id="7ea48a47287b56e3f66c9ec81d51c291e8fedf59" translate="yes" xml:space="preserve">
          <source>The above vectorization scheme is simple but the fact that it holds an &lt;strong&gt;in- memory mapping from the string tokens to the integer feature indices&lt;/strong&gt; (the &lt;code&gt;vocabulary_&lt;/code&gt; attribute) causes several &lt;strong&gt;problems when dealing with large datasets&lt;/strong&gt;:</source>
          <target state="translated">上記のベクトル化スキームは単純ですが、&lt;strong&gt;文字列トークンから整数機能インデックス&lt;/strong&gt;（ &lt;code&gt;vocabulary_&lt;/code&gt; 属性）&lt;strong&gt;へのメモリ内マッピングを&lt;/strong&gt;保持しているため&lt;strong&gt;、大きなデータセットを処理するときに&lt;/strong&gt;いくつかの&lt;strong&gt;問題が発生し&lt;/strong&gt;ます。</target>
        </trans-unit>
        <trans-unit id="f78fe42a387595f69f94294d9712aa8fff9cf7ee" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores during early stopping. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="translated">早期停止時にスコアを比較する際に使用する絶対的な許容度。耐性が高いほど、早期停止の可能性が高くなります:耐性が高いということは、後続の反復が基準スコアの改善とみなされにくくなることを意味します。</target>
        </trans-unit>
        <trans-unit id="a971e24181d9be44e4bb51f3963013ec9c9d01d5" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="translated">スコアを比較するときに使用する絶対的な許容範囲です。許容度が高いほど、早期停止する可能性が高くなります:許容度が高いということは、それ以降の反復が基準スコアの改善とみなされにくくなることを意味します。</target>
        </trans-unit>
        <trans-unit id="58279b870becff96bab0fc34e17d7f045af03b34" translate="yes" xml:space="preserve">
          <source>The abstract base class for all kernels is &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt;. Kernel implements a similar interface as &lt;code&gt;Estimator&lt;/code&gt;, providing the methods &lt;code&gt;get_params()&lt;/code&gt;, &lt;code&gt;set_params()&lt;/code&gt;, and &lt;code&gt;clone()&lt;/code&gt;. This allows setting kernel values also via meta-estimators such as &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;GridSearch&lt;/code&gt;. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with &lt;code&gt;k1__&lt;/code&gt; and parameters of the right operand with &lt;code&gt;k2__&lt;/code&gt;. An additional convenience method is &lt;code&gt;clone_with_theta(theta)&lt;/code&gt;, which returns a cloned version of the kernel but with the hyperparameters set to &lt;code&gt;theta&lt;/code&gt;. An illustrative example:</source>
          <target state="translated">すべてのカーネルの抽象基本クラスは&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;です。カーネルは &lt;code&gt;Estimator&lt;/code&gt; と同様のインターフェースを実装し、メソッド &lt;code&gt;get_params()&lt;/code&gt; 、 &lt;code&gt;set_params()&lt;/code&gt; 、および &lt;code&gt;clone()&lt;/code&gt; を提供します。これにより、 &lt;code&gt;Pipeline&lt;/code&gt; や &lt;code&gt;GridSearch&lt;/code&gt; などのメタ推定器を介してカーネル値を設定することもできます。カーネルのネスト構造（カーネルオペレーターを適用することにより、以下を参照）により、カーネルパラメーターの名前が比較的複雑になる可能性があることに注意してください。一般的に、バイナリカーネルオペレータのため、左のオペランドのパラメータが付いさ &lt;code&gt;k1__&lt;/code&gt; 及び右側オペランドのパラメータ &lt;code&gt;k2__&lt;/code&gt; 。追加の便利な方法は &lt;code&gt;clone_with_theta(theta)&lt;/code&gt; は、カーネルのクローンバージョンを返しますが、ハイパーパラメータは &lt;code&gt;theta&lt;/code&gt; に設定されています。説明的な例：</target>
        </trans-unit>
        <trans-unit id="d6dfb2a865fc7561c9990f60831354c8c2b2acf1" translate="yes" xml:space="preserve">
          <source>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: &amp;ldquo;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&amp;rdquo;, Optimization Methods and Software 1, 1992, 23-34].</source>
          <target state="translated">3次元空間で分離平面を取得するために使用される実際の線形プログラムは、[KPベネットとOLマンガリアン：「2つの線形分離不能セットのロバスト線形プログラミング識別」、最適化方法とソフトウェア1、1992、23- 34]。</target>
        </trans-unit>
        <trans-unit id="5c16c787f9e7f87c21ce4f86533b13082a6022ce" translate="yes" xml:space="preserve">
          <source>The actual number of iteration performed by the solver. Only returned if &lt;code&gt;return_n_iter&lt;/code&gt; is True.</source>
          <target state="translated">ソルバーによって実行された実際の反復回数。 &lt;code&gt;return_n_iter&lt;/code&gt; がTrueの場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="9cc109786e99b4c544e4621619329291c5062f5c" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion.</source>
          <target state="translated">停止基準に到達するまでの実際の反復回数。</target>
        </trans-unit>
        <trans-unit id="68fff8e45d31e6c0edd22bde7c766ffe083896ee" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">停止基準に達するまでの実際の反復回数。マルチクラスフィットの場合は、すべてのバイナリフィットの最大値です。</target>
        </trans-unit>
        <trans-unit id="c6a597b70b1c3dba7fc91a33b2b458a1718ae376" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion.</source>
          <target state="translated">停止基準に到達するまでの実際の反復回数。</target>
        </trans-unit>
        <trans-unit id="be3e6ed927427ff958a3dc691ec6f2ca38b153e8" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">停止基準に到達するための実際の反復回数。マルチクラスフィットの場合は、すべてのバイナリフィットの最大値です。</target>
        </trans-unit>
        <trans-unit id="905122fa4ae495ec172d69201d288d9cbd8eb107" translate="yes" xml:space="preserve">
          <source>The actual number of neighbors used for &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; queries.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt;クエリに使用されるネイバーの実際の数。</target>
        </trans-unit>
        <trans-unit id="d1e2217f69f6ffa7102e10d2dcd6eed0b5a03524" translate="yes" xml:space="preserve">
          <source>The actual number of quantiles used to discretize the cumulative distribution function.</source>
          <target state="translated">累積分布関数を離散化するために使用される実際の分数。</target>
        </trans-unit>
        <trans-unit id="99f948dd8ff8ebdf8ccc1268f27f992c4d54a1e0" translate="yes" xml:space="preserve">
          <source>The actual number of samples</source>
          <target state="translated">実際のサンプル数</target>
        </trans-unit>
        <trans-unit id="83f4574f37ccf239bd98ad8929c323f413ece25a" translate="yes" xml:space="preserve">
          <source>The actual number of samples.</source>
          <target state="translated">実際のサンプル数。</target>
        </trans-unit>
        <trans-unit id="257683b273bf01e4ad40af7f18f19c426c837478" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel as used here is given by</source>
          <target state="translated">ここで使用される加法カイ二乗カーネルは次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="721e722946fa16cdc1d4e80a122a59df383d3b35" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel is a kernel on histograms, often used in computer vision.</source>
          <target state="translated">加法カイ2乗カーネルは、コンピュータビジョンでよく使われるヒストグラム上のカーネルです。</target>
        </trans-unit>
        <trans-unit id="509930cd9616089e31b59f87837fde9a2850682a" translate="yes" xml:space="preserve">
          <source>The additive version of this kernel</source>
          <target state="translated">このカーネルの加算版</target>
        </trans-unit>
        <trans-unit id="479259eedb54024948ef963b0d114b00fbb05f1b" translate="yes" xml:space="preserve">
          <source>The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split the graph into comparably sized components.</source>
          <target state="translated">隣接行列は,正規化されたグラフのラプラシアンを計算するために使用され,そのスペクトル(特に最小の固有値に関連する固有ベクトル)は,グラフを比較可能な大きさの成分に分割するのに必要な最小のカット数という観点から解釈されます.</target>
        </trans-unit>
        <trans-unit id="b4f6e52ff52334d95b6f16934aecd26800758ca5" translate="yes" xml:space="preserve">
          <source>The adjacency matrix of the graph to embed.</source>
          <target state="translated">埋め込むグラフの隣接行列。</target>
        </trans-unit>
        <trans-unit id="f8c06d6c9bebb104301c962acd25687eac4074b4" translate="yes" xml:space="preserve">
          <source>The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).</source>
          <target state="translated">このようにして、調整されたRand指数は、クラスタ数とサンプル数に依存しないランダムなラベリングの場合は0.0に近い値を持ち、クラスタリングが同一の場合(順列まで)は正確に1.0に近い値を持つことが保証されています。</target>
        </trans-unit>
        <trans-unit id="83e2303d70450b875b04241e83f4a4153932e0a9" translate="yes" xml:space="preserve">
          <source>The advantage of using approximate explicit feature maps compared to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;kernel trick&lt;/a&gt;, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; can make non-linear learning on large datasets possible.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;カーネルトリック&lt;/a&gt;と比較して近似の明示的な機能マップを使用する利点は、機能マップを暗黙的に利用するため、明示的なマッピングがオンライン学習により適し、非常に大きなデータセットでの学習コストを大幅に削減できることです。標準のカーネル化されたSVMは大規模なデータセットにうまくスケーリングできませんが、おおよそのカーネルマップを使用すると、はるかに効率的な線形SVMを使用できます。特に、カーネルマップ近似と&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;の組み合わせにより、大規模なデータセットでの非線形学習が可能になります。</target>
        </trans-unit>
        <trans-unit id="e8cf8d3d0c8dfd7b6a3c8351c56803f2fdf0d2d8" translate="yes" xml:space="preserve">
          <source>The advantages of Bayesian Regression are:</source>
          <target state="translated">ベイズ回帰のメリットは</target>
        </trans-unit>
        <trans-unit id="d7269fefe21b18a1d9b1e8a11da2f35edf3e36ec" translate="yes" xml:space="preserve">
          <source>The advantages of GBRT are:</source>
          <target state="translated">GBRTのメリットは</target>
        </trans-unit>
        <trans-unit id="6bdcff89c23af609c3f964058bbddc38e45558f8" translate="yes" xml:space="preserve">
          <source>The advantages of Gaussian processes are:</source>
          <target state="translated">ガウス過程の利点は</target>
        </trans-unit>
        <trans-unit id="c632bf8abb99b63a04554183dcc32e66bb0cf004" translate="yes" xml:space="preserve">
          <source>The advantages of LARS are:</source>
          <target state="translated">LARSのメリットは</target>
        </trans-unit>
        <trans-unit id="54923c815d40d3138e9fbe2ad92c97dd60ef1b2e" translate="yes" xml:space="preserve">
          <source>The advantages of Multi-layer Perceptron are:</source>
          <target state="translated">多層パーセプトロンの利点は、以下の通りです。</target>
        </trans-unit>
        <trans-unit id="1dabd14761114fadc83f1114e989744a341117db" translate="yes" xml:space="preserve">
          <source>The advantages of Stochastic Gradient Descent are:</source>
          <target state="translated">確率勾配降下法の利点は、以下の通りです。</target>
        </trans-unit>
        <trans-unit id="a12cea148a9927c20c87185a7ad428a4bd953760" translate="yes" xml:space="preserve">
          <source>The advantages of support vector machines are:</source>
          <target state="translated">サポートベクターマシンの利点は</target>
        </trans-unit>
        <trans-unit id="b3ce019dea8f1fa7178760b3ed7e1bed715f9894" translate="yes" xml:space="preserve">
          <source>The affinity matrix describing the relationship of the samples to embed. &lt;strong&gt;Must be symmetric&lt;/strong&gt;.</source>
          <target state="translated">埋め込むサンプルの関係を表す親和性マトリックス。&lt;strong&gt;対称でなければなりません&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="f8f51936a34abb4d8304c65310991fbeb781a2b1" translate="yes" xml:space="preserve">
          <source>The algorithm automatically sets the number of clusters, instead of relying on a parameter &lt;code&gt;bandwidth&lt;/code&gt;, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided &lt;code&gt;estimate_bandwidth&lt;/code&gt; function, which is called if the bandwidth is not set.</source>
          <target state="translated">アルゴリズムは、検索する領域のサイズを指定するパラメーターの &lt;code&gt;bandwidth&lt;/code&gt; に依存するのではなく、クラスターの数を自動的に設定します。このパラメーターは手動で設定できますが、帯域幅が設定されていない場合に呼び出される、提供 &lt;code&gt;estimate_bandwidth&lt;/code&gt; れている推定の帯域幅関数を使用して推定できます。</target>
        </trans-unit>
        <trans-unit id="999ae100a463cc0ff60234ca1a1687522770fa40" translate="yes" xml:space="preserve">
          <source>The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is &amp;ldquo;n_samples choose n_subsamples&amp;rdquo;, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.</source>
          <target state="translated">アルゴリズムは、Xのサンプルのサイズがn_subsamplesであるサブセットの最小二乗解を計算します。特徴とサンプルの数の間のn_subsamplesの値は、ロバスト性と効率の間で妥協した推定器になります。最小二乗解の数は「n_samplesはn_subsamplesを選択」であるため、非常に大きくなる可能性があり、したがってmax_subpopulationで制限できます。この制限に達すると、サブセットはランダムに選択されます。最後のステップでは、すべての最小二乗解の空間中央値（またはL1中央値）が計算されます。</target>
        </trans-unit>
        <trans-unit id="19207c781cd295df6565aab588711bf1a9323bef" translate="yes" xml:space="preserve">
          <source>The algorithm can also be understood through the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi diagrams&lt;/a&gt;. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.</source>
          <target state="translated">アルゴリズムは、&lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;ボロノイ図の&lt;/a&gt;概念を通じて理解することもできます。最初に、ポイントのボロノイ図が現在の重心を使用して計算されます。ボロノイ図の各セグメントは、個別のクラスターになります。次に、重心は各セグメントの平均に更新されます。アルゴリズムは、停止基準が満たされるまでこれを繰り返します。通常、アルゴリズムは、反復間の目的関数の相対的な減少が指定された許容値よりも小さくなると停止します。これは、この実装では当てはまりません。重心が許容値よりも小さくなると、反復が停止します。</target>
        </trans-unit>
        <trans-unit id="ac387733191797f4111e67a021bb9641f7899ac2" translate="yes" xml:space="preserve">
          <source>The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">この問題を解決するために採用されたアルゴリズムは、Friedman 2008 BiostatisticsペーパーのGLassoアルゴリズムです。これは、R &lt;code&gt;glasso&lt;/code&gt; パッケージと同じアルゴリズムです。</target>
        </trans-unit>
        <trans-unit id="2d7b95845283c4d98cc4167d4e0748ee6c69e84f" translate="yes" xml:space="preserve">
          <source>The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. &amp;ldquo;Algorithms for computing the sample variance: Analysis and recommendations.&amp;rdquo; The American Statistician 37.3 (1983): 242-247:</source>
          <target state="translated">増分平均とstdのアルゴリズムは、Chan、Tony F.、Gene H. Golub、およびRandall J. LeVequeの式1.5a、bに示されています。「サンプル分散を計算するためのアルゴリズム：分析と推奨事項。」アメリカ統計学者37.3（1983）：242-247：</target>
        </trans-unit>
        <trans-unit id="c4e655f6aa7cf22a58a123dc60b12c815d9f7df3" translate="yes" xml:space="preserve">
          <source>The algorithm is adapted from Guyon [1] and was designed to generate the &amp;ldquo;Madelon&amp;rdquo; dataset.</source>
          <target state="translated">このアルゴリズムはGuyon [1]から採用され、「Madelon」データセットを生成するように設計されています。</target>
        </trans-unit>
        <trans-unit id="da28c971693f926f3030184039a925bcc2b1ca76" translate="yes" xml:space="preserve">
          <source>The algorithm is from Marsland [1].</source>
          <target state="translated">アルゴリズムはMarsland [1]からのものです。</target>
        </trans-unit>
        <trans-unit id="94c105f8aebba6a360eae1c93878348f10f58fca" translate="yes" xml:space="preserve">
          <source>The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.</source>
          <target state="translated">このアルゴリズムは、アルゴリズムの実行中に複数の最近傍探索を必要とするため、非常にスケーラブルではありません。このアルゴリズムは収束することが保証されていますが、セントロイドの変化が小さい場合、アルゴリズムは反復処理を停止します。</target>
        </trans-unit>
        <trans-unit id="da3ddd72ab3f78c12c541d3f34c2389bc8e810e8" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including features at each step, the estimated coefficients are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">アルゴリズムは順方向のステップワイズ回帰に​​似ていますが、各ステップに特徴を含める代わりに、推定された係数は、残差との各相関に等角の方向に増加します。</target>
        </trans-unit>
        <trans-unit id="3dc6012ab672dce7fe920dc60add16368612e345" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">アルゴリズムは順方向ステップワイズ回帰に​​似ていますが、各ステップで変数を含めるのではなく、推定パラメーターは、残差との各相関と等角の方向に増加します。</target>
        </trans-unit>
        <trans-unit id="0335c4d6784d824f2c45b2464a3517f723de00c5" translate="yes" xml:space="preserve">
          <source>The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.</source>
          <target state="translated">このアルゴリズムは確率的なものであり、異なるシードを用いて複数回の再起動を行うと、異なる埋め込みが得られる可能性があります。しかし、誤差が最も少ない埋め込みを選ぶことは完全に正当なことです。</target>
        </trans-unit>
        <trans-unit id="7c7de93467b285ca3f0b63a2764ebf7e5ab511ed" translate="yes" xml:space="preserve">
          <source>The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \(b\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.</source>
          <target state="translated">このアルゴリズムは、バニラのk-meansと同様に、2つの主要なステップの間で反復します。最初のステップでは、ミニバッチを形成するために、データセットからランダムに \(b\)サンプルが引き出されます。これらは、次に、最も近いセントロイドに割り当てられます。第2のステップでは,セントロイドが更新される.k平均とは対照的に,これはサンプルごとに行われる.ミニバッチ内の各サンプルについて、割り当てられたセントロイドは、サンプルのストリーミング平均と、そのセントロイドに割り当てられた以前のすべてのサンプルを取ることによって更新されます。これは、時間の経過とともにセントロイドの変化率を減少させる効果があります。これらのステップは、収束または所定の反復回数に達するまで実行されます。</target>
        </trans-unit>
        <trans-unit id="eac8adf7ae77ba9768138371c6c71edd5a3a76a7" translate="yes" xml:space="preserve">
          <source>The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.</source>
          <target state="translated">このアルゴリズムは,対応するブロック間定数チェッカーボード行列が元の行列に対して良好な近似を提供するように,行列の行と列を分割します.</target>
        </trans-unit>
        <trans-unit id="36533938d0e94afa89a69e7b9609c61646ab71f5" translate="yes" xml:space="preserve">
          <source>The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.</source>
          <target state="translated">アルゴリズムは,完全な入力サンプル・データを,ノイズの影響を受ける可能性のあるインライアと,データについての誤った測定や無効な仮説などによって引き起こされるアウトライアのセットに分割する.結果として得られるモデルは,決定されたインライアからのみ推定される.</target>
        </trans-unit>
        <trans-unit id="76e87cc3ec2713663d8449fef17b4e0c4101c305" translate="yes" xml:space="preserve">
          <source>The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.</source>
          <target state="translated">アルゴリズムは、あらかじめ設定された最大反復回数に達したとき、または損失の改善が特定の小さな数以下になったときに停止します。</target>
        </trans-unit>
        <trans-unit id="841b170fb7ba2429816cb7c51af4dbae57b471af" translate="yes" xml:space="preserve">
          <source>The algorithm supports sample weights, which can be given by a parameter &lt;code&gt;sample_weight&lt;/code&gt;. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \(X\).</source>
          <target state="translated">このアルゴリズムは、サンプルの重みをサポートしています。これは、パラメーター &lt;code&gt;sample_weight&lt;/code&gt; で指定できます。これにより、クラスターの中心と慣性の値を計算するときに、いくつかのサンプルにより多くの重みを割り当てることができます。たとえば、サンプルに重み2を割り当てることは、そのサンプルの複製をデータセット\（X \）に追加することと同じです。</target>
        </trans-unit>
        <trans-unit id="a8238e48cf484817aaf23aa2adc5e4a70681d78f" translate="yes" xml:space="preserve">
          <source>The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.</source>
          <target state="translated">NearestNeighbors モジュールが使用するアルゴリズム。詳細は NearestNeighbors モジュールのドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="3602b334476c395c578aab7a7d09d75f92100f28" translate="yes" xml:space="preserve">
          <source>The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.</source>
          <target state="translated">このアルゴリズムは、入力データ行列を二部グラフとして扱います:行列の行と列は、2つの頂点のセットに対応し、各エントリは、行と列の間の辺に対応します。このアルゴリズムは,重い部分グラフを見つけるために,このグラフの正規化カットを近似します.</target>
        </trans-unit>
        <trans-unit id="e4086364ea17b4442e0265a08361358aa304a247" translate="yes" xml:space="preserve">
          <source>The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop.</source>
          <target state="translated">重みの推定に用いられるアルゴリズム.このアルゴリズムは n_components を何度も呼び出します。</target>
        </trans-unit>
        <trans-unit id="e4e3e486ab0114bca02d02635f9410b77ca99a25" translate="yes" xml:space="preserve">
          <source>The algorithm used to fit the model is coordinate descent.</source>
          <target state="translated">モデルのフィットに使用されるアルゴリズムは、座標降下です。</target>
        </trans-unit>
        <trans-unit id="17e35eb9c6004e2d60b73104c93585aacf4a2e98" translate="yes" xml:space="preserve">
          <source>The algorithmic complexity of affinity propagation is quadratic in the number of points.</source>
          <target state="translated">親和性伝播のアルゴリズム的複雑さは、点の数で二次的である。</target>
        </trans-unit>
        <trans-unit id="4c657d81ec38ba678d89f7b7de187e376e50c0f3" translate="yes" xml:space="preserve">
          <source>The algorithms for regression and classification only differ in the concrete loss function used.</source>
          <target state="translated">回帰と分類のアルゴリズムは、使用する具体的な損失関数が異なるだけです。</target>
        </trans-unit>
        <trans-unit id="50542b2bc530bb15fc1bcd67d7b073d61bea03ab" translate="yes" xml:space="preserve">
          <source>The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.</source>
          <target state="translated">特徴をランダムにスケールするために使用される,安定性選択項目のアルファ・パラメータ.0から1の間でなければなりません。</target>
        </trans-unit>
        <trans-unit id="35d70c04237d10e5da496ff67ac57fce44665490" translate="yes" xml:space="preserve">
          <source>The alpha parameter of the GraphicalLasso setting the sparsity of the model is set by internal cross-validation in the GraphicalLassoCV. As can be seen on figure 2, the grid to compute the cross-validation score is iteratively refined in the neighborhood of the maximum.</source>
          <target state="translated">モデルのスパース度を設定するGraphicalLassoのαパラメータは、GraphicalLassoCVの内部クロスバリデーションによって設定される。図2に示すように、交差検証スコアを計算するグリッドは、最大値付近で反復的に精緻化されている。</target>
        </trans-unit>
        <trans-unit id="2075fb3135d145905cc5a41db16871a4bf5168da" translate="yes" xml:space="preserve">
          <source>The alpha-quantile of the huber loss function and the quantile loss function. Only if &lt;code&gt;loss='huber'&lt;/code&gt; or &lt;code&gt;loss='quantile'&lt;/code&gt;.</source>
          <target state="translated">フーバー損失関数と分位点損失関数のアルファ分位数。 &lt;code&gt;loss='huber'&lt;/code&gt; または &lt;code&gt;loss='quantile'&lt;/code&gt; の場合のみ。</target>
        </trans-unit>
        <trans-unit id="28a2d4f18d224d5f96bb30d3faa50cdca5ce2935" translate="yes" xml:space="preserve">
          <source>The alphas along the path where models are computed.</source>
          <target state="translated">モデルが計算されるパスに沿ったアルファ。</target>
        </trans-unit>
        <trans-unit id="4390acc7061cc013783802ff89b6303810b0044d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set.</source>
          <target state="translated">データセットの汚染量、すなわちデータセット内の外れ値の割合。</target>
        </trans-unit>
        <trans-unit id="a3cf6bdbcc68281bb427886cdd573766d75b2a0b" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Range is (0, 0.5).</source>
          <target state="translated">データセットの汚染量、すなわちデータセット内の外れ値の割合。範囲は(0,0.5)です。</target>
        </trans-unit>
        <trans-unit id="db48eba23a1b19c738dbbe1eb3626efb11bd507f" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. If &amp;lsquo;auto&amp;rsquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">データセットの汚染の量、つまりデータセット内の外れ値の比率。決定関数のしきい値を定義するためにフィッティングするときに使用されます。'auto'の場合、決定関数のしきい値は元の論文と同様に決定されます。</target>
        </trans-unit>
        <trans-unit id="f37071a773077642311acdcd66bbd39bda02514d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.</source>
          <target state="translated">データセットの汚染量、すなわちデータセット内の外れ値の割合。サンプルのスコアのしきい値を定義するためにフィッティングの際に使用されます。</target>
        </trans-unit>
        <trans-unit id="0b69accc98489279e23dd931886f63d4aafd913d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. If &amp;ldquo;auto&amp;rdquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">データセットの汚染の量、つまりデータセット内の外れ値の比率。フィッティングするとき、これを使用して決定関数のしきい値を定義します。「自動」の場合、決定関数のしきい値は元の論文と同様に決定されます。</target>
        </trans-unit>
        <trans-unit id="a618727acf5955128fa616ec196fb65bef331d06" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the scores of the samples.</source>
          <target state="translated">データセットの汚染量、すなわちデータセット内の外れ値の割合。フィッティングの際に,サンプルのスコアのしきい値を定義するために使用されます.</target>
        </trans-unit>
        <trans-unit id="057670e523b801c82d76306d2019fbc0b1e55fa7" translate="yes" xml:space="preserve">
          <source>The amount of penalization chosen by cross validation</source>
          <target state="translated">クロスバリデーションによって選択されたペナルティの量</target>
        </trans-unit>
        <trans-unit id="0a8889ae5955aac97641248ca40c9031408985ae" translate="yes" xml:space="preserve">
          <source>The amount of variance explained by each of the selected components.</source>
          <target state="translated">選択された各成分によって説明される分散の量。</target>
        </trans-unit>
        <trans-unit id="0c35119665a5a383bf2a2ea2491d6548297b77b9" translate="yes" xml:space="preserve">
          <source>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</source>
          <target state="translated">入力サンプルの異常スコアは、森林内の樹木の平均異常スコアとして計算されます。</target>
        </trans-unit>
        <trans-unit id="f5bff141bb39bd3c0bc3c7fd23ee875e735e27a8" translate="yes" xml:space="preserve">
          <source>The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.</source>
          <target state="translated">各標本の異常値スコアは,局所的外れ値因子と呼ばれる.これは、隣人に対する与えられたサンプルの密度の局所的な偏差を測定します。これは、異常スコアが、オブジェクトが周囲の近隣に対してどれだけ孤立しているかに依存するという点で局所的です。より正確には、局所性はk-最も近い隣人によって与えられ、その距離は局所密度を推定するために使用されます。サンプルの局所密度を隣人の局所密度と比較することで、隣人よりもかなり低い密度を持つサンプルを識別することができます。これらは外れ値と考えられます。</target>
        </trans-unit>
        <trans-unit id="9d2b05e02bf58b122225781451ec470ef3bbad43" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal.</source>
          <target state="translated">入力サンプルの異常スコア。低いほど異常です。</target>
        </trans-unit>
        <trans-unit id="7ea9f6456fff678e2acc40131f68202326a3c878" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">入力サンプルの異常スコア。低いほど異常である.負のスコアは外れ値を表し,正のスコアは外れ値を表す.</target>
        </trans-unit>
        <trans-unit id="f8f5cba472ebac3a5205adfc98746f94cfb55c44" translate="yes" xml:space="preserve">
          <source>The approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; can be combined with the approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; to yield an approximate feature map for the exponentiated chi squared kernel. See the &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; for details and &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; for combination with the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt;によって提供される近似特徴マップを&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;によって提供される近似特徴マップと組み合わせて、指数カイ2乗カーネルの近似特徴マップを生成できます。参照&lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt;詳細とについて&lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt;との組み合わせのため&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="98f8783bf76339c13180f5a0c21989e5d9843d3d" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the data by linear combinations.</source>
          <target state="translated">ほとんどのデータを線形の組み合わせで説明するのに必要な特異ベクトルのおおよその数。</target>
        </trans-unit>
        <trans-unit id="01540c9a0c0d75da953bc96aafe62b761a8754d9" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.</source>
          <target state="translated">入力データの大部分を線形の組み合わせで説明するのに必要な特異ベクトルのおおよその数。この種の特異スペクトルを入力に使用することで、生成器は実際によく観察される相関を再現することができる。</target>
        </trans-unit>
        <trans-unit id="9b1212b40ba5f9b8205d7b64e2e9c20d35b415b3" translate="yes" xml:space="preserve">
          <source>The arithmetic mean is the sum of the elements along the axis divided by the number of elements.</source>
          <target state="translated">算術平均は、軸に沿った要素の総和を要素数で割ったものです。</target>
        </trans-unit>
        <trans-unit id="35a07a5d9cec4bce6db2b0fe073857a7979c5b16" translate="yes" xml:space="preserve">
          <source>The array has 0.16% of non zero values.</source>
          <target state="translated">配列には0.16%の非ゼロ値があります。</target>
        </trans-unit>
        <trans-unit id="a77140549775c60e078b0c26c1b965d02c08ebb3" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations</source>
          <target state="translated">(対数)密度評価の配列</target>
        </trans-unit>
        <trans-unit id="449fd4c0217076d90d22633d6cb32b7300c30336" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations, shape = X.shape[:-1]</source>
          <target state="translated">(log)密度評価の配列,shape=X.shape[:-1].</target>
        </trans-unit>
        <trans-unit id="d00a652e1f004fd3ddfd653b6794ad2aba8108c7" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations.</source>
          <target state="translated">log(密度)評価の配列。</target>
        </trans-unit>
        <trans-unit id="0e2b34732ef7f3aed76135ded438ba1b0e2806d3" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations. These are normalized to be probability densities, so values will be low for high-dimensional data.</source>
          <target state="translated">log(密度)評価の配列。これらは確率密度に正規化されているので、高次元データでは値が低くなります。</target>
        </trans-unit>
        <trans-unit id="531360783ebbdd4b65c83458a2d186f114e4086b" translate="yes" xml:space="preserve">
          <source>The automatic estimation from Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604 by Thomas P. Minka is also compared.</source>
          <target state="translated">PCAのための次元性の自動選択からの自動推定.Thomas P.MinkaによるNIPS 2000:598-604も比較しています。</target>
        </trans-unit>
        <trans-unit id="37bf2d4736a8a0a6781e876ebdb656796aca8913" translate="yes" xml:space="preserve">
          <source>The available cross validation iterators are introduced in the following section.</source>
          <target state="translated">利用可能なクロスバリデーションイテレータについては、次のセクションで紹介します。</target>
        </trans-unit>
        <trans-unit id="52d681893d146fe7dad6c4424a94a534bab69431" translate="yes" xml:space="preserve">
          <source>The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.</source>
          <target state="translated">平均的な複雑さはO(k n T)で与えられ、nはサンプル数、Tは反復回数である。</target>
        </trans-unit>
        <trans-unit id="d9092bb0da47977d9b43f41f12fd7dcc75d26801" translate="yes" xml:space="preserve">
          <source>The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with &lt;code&gt;n_labels&lt;/code&gt; as its expected value, but samples are bounded (using rejection sampling) by &lt;code&gt;n_classes&lt;/code&gt;, and must be nonzero if &lt;code&gt;allow_unlabeled&lt;/code&gt; is False.</source>
          <target state="translated">インスタンスあたりのラベルの平均数。より正確には、サンプルあたりのラベル数は、 &lt;code&gt;n_labels&lt;/code&gt; を期待値として持つポアソン分布から導き出されますが、サンプルは（拒否サンプリングを使用して） &lt;code&gt;n_classes&lt;/code&gt; によって制限され、 &lt;code&gt;allow_unlabeled&lt;/code&gt; がFalseの場合はゼロ以外でなければなりません。</target>
        </trans-unit>
        <trans-unit id="50521765649b7ccece3cb452f18896c14c55c5d8" translate="yes" xml:space="preserve">
          <source>The average precision score in multi-label settings</source>
          <target state="translated">マルチラベル設定での平均精度スコア</target>
        </trans-unit>
        <trans-unit id="66fdff3c7719e9049c28a125538dc0d9761fb599" translate="yes" xml:space="preserve">
          <source>The averaged NDCG scores for all samples.</source>
          <target state="translated">全サンプルのNDCGスコアの平均値。</target>
        </trans-unit>
        <trans-unit id="7f192c1d1db49dfae3574eb70a5374ae8608eb71" translate="yes" xml:space="preserve">
          <source>The averaged intercept term.</source>
          <target state="translated">平均化された切片項。</target>
        </trans-unit>
        <trans-unit id="9595497d17dae79a02bc4117e4b89bb9f68b4736" translate="yes" xml:space="preserve">
          <source>The averaged intercept term. Only available if &lt;code&gt;average=True&lt;/code&gt;.</source>
          <target state="translated">平均切片項。 &lt;code&gt;average=True&lt;/code&gt; 場合にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="c831f95b759b1e5121cd7c90ec4906a408d65670" translate="yes" xml:space="preserve">
          <source>The averaged sample DCG scores.</source>
          <target state="translated">サンプルのDCGスコアの平均値。</target>
        </trans-unit>
        <trans-unit id="5f1e3570b35bda8ded1b9651eb310bcefe0589f6" translate="yes" xml:space="preserve">
          <source>The axes with which the grid has been created or None if the grid has been given.</source>
          <target state="translated">グリッドが作成された軸、またはグリッドが指定されている場合はなし。</target>
        </trans-unit>
        <trans-unit id="da553a7d6d695e12dca5e56a77d2e07dce1c7a46" translate="yes" xml:space="preserve">
          <source>The axis along which &lt;code&gt;X&lt;/code&gt; will be subsampled. &lt;code&gt;axis=0&lt;/code&gt; will select rows while &lt;code&gt;axis=1&lt;/code&gt; will select columns.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; がサブサンプリングされる軸。 &lt;code&gt;axis=0&lt;/code&gt; は行を選択し、 &lt;code&gt;axis=1&lt;/code&gt; は列を選択します。</target>
        </trans-unit>
        <trans-unit id="2af1a0461163ef9a917dd17f9a6c7032c07c79bb" translate="yes" xml:space="preserve">
          <source>The axis along which to impute.</source>
          <target state="translated">帰属させる軸。</target>
        </trans-unit>
        <trans-unit id="efae1008b127cbb418b23469c7f222e2a6660b67" translate="yes" xml:space="preserve">
          <source>The bag of words representation is quite simplistic but surprisingly useful in practice.</source>
          <target state="translated">単語の袋の表現はかなり単純ですが、実際には驚くほど便利です。</target>
        </trans-unit>
        <trans-unit id="a41734ff5b2ce49bfc6f83ebb1bb324e76d37d70" translate="yes" xml:space="preserve">
          <source>The bags of words representation implies that &lt;code&gt;n_features&lt;/code&gt; is the number of distinct words in the corpus: this number is typically larger than 100,000.</source>
          <target state="translated">単語のバッグ表現は、 &lt;code&gt;n_features&lt;/code&gt; がコーパス内の個別の単語の数であることを意味します。この数は通常100,000を超えます。</target>
        </trans-unit>
        <trans-unit id="eac13f6898f72e0eaad5c85bd1b456a5882d1a72" translate="yes" xml:space="preserve">
          <source>The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.</source>
          <target state="translated">不均衡なデータセットに対処するための二値分類問題や多クラス分類問題における均衡精度。各クラスで得られたリコールの平均値として定義される。</target>
        </trans-unit>
        <trans-unit id="fd188197709d9753bf6d2d7d8ebbb1b211377cd3" translate="yes" xml:space="preserve">
          <source>The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.</source>
          <target state="translated">ここでの帯域幅は平滑化パラメータとして機能し、結果のバイアスと分散の間のトレードオフを制御します。帯域幅が大きいと、非常に滑らかな(つまり高バイアスの)密度分布になります。帯域幅が小さいと、平滑でない(すなわち、高分散)密度分布になります。</target>
        </trans-unit>
        <trans-unit id="220a3cae014ca1f6f44493020405f4e460f4038d" translate="yes" xml:space="preserve">
          <source>The bandwidth of the kernel.</source>
          <target state="translated">カーネルの帯域幅。</target>
        </trans-unit>
        <trans-unit id="eaa1089b1fdc51cb43f528072a19cd3f3ec50c61" translate="yes" xml:space="preserve">
          <source>The bandwidth parameter.</source>
          <target state="translated">帯域幅パラメータ。</target>
        </trans-unit>
        <trans-unit id="ca9efc037882c6303cbf300aed185a1d9a7dd7ae" translate="yes" xml:space="preserve">
          <source>The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classifier.</source>
          <target state="translated">棒グラフは、各分類器の精度、訓練時間(正規化)、試験時間(正規化)を示す。</target>
        </trans-unit>
        <trans-unit id="6ef0fbe42e5306306c086ebee574632386d4e293" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center. This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">基本分類器は、25の基本推定量（ツリー）を持つランダムフォレスト分類器です。この分類子が800のトレーニングデータポイントすべてでトレーニングされると、予測に過度の自信があり、大きなログ損失が発生します。残りの200データポイントでmethod = 'sigmoid'を使用して600データポイントでトレーニングされた同一の分類子をキャリブレーションすると、予測の信頼性が低下します。つまり、確率ベクトルがシンプレックスのエッジから中心に移動します。この調整により、対数損失が減少します。代替案は、対数損失の同様の減少をもたらすであろうベース推定量の数を増やすことであったことに注意してください。</target>
        </trans-unit>
        <trans-unit id="3f89ef1e6cb2a54f74eb0a982beea8ffd3333592" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center:</source>
          <target state="translated">基本分類器は、25の基本推定量（ツリー）を持つランダムフォレスト分類器です。この分類子が800のトレーニングデータポイントすべてでトレーニングされると、予測に過度の自信があり、大きなログ損失が発生します。残りの200データポイントでmethod = 'sigmoid'を使用して600データポイントでトレーニングされた同一の分類子をキャリブレーションすると、予測の信頼性が低下します。つまり、確率ベクトルがシンプレックスのエッジから中心に向かって移動します。</target>
        </trans-unit>
        <trans-unit id="2000944fc04d6f6e7393d659c7173e725c27d458" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;.</source>
          <target state="translated">ブーストされたアンサンブルが構築される基本推定量。 &lt;code&gt;None&lt;/code&gt; の場合、基本推定量は &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="cdab13e5017fd96224833fdcfcd75615e152e1af" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</source>
          <target state="translated">ブーストされたアンサンブルが構築されるベース推定量。サンプルの重み付けのサポートと、適切な &lt;code&gt;classes_&lt;/code&gt; および &lt;code&gt;n_classes_&lt;/code&gt; 属性が必要です。 &lt;code&gt;None&lt;/code&gt; の場合、基本推定量は &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="7ad7363185e9af79d948e6b752e7cc25ad567f4b" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;.</source>
          <target state="translated">ブーストされたアンサンブルが構築される基本推定量。サンプルの重み付けのサポート、および適切な &lt;code&gt;classes_&lt;/code&gt; 属性と &lt;code&gt;n_classes_&lt;/code&gt; 属性が必要です。 &lt;code&gt;None&lt;/code&gt; の場合、基本推定量は &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="39ba9d176a83a208ed96d60f33bec321ac051ac5" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</source>
          <target state="translated">ブーストされたアンサンブルが構築されるベース推定量。サンプルの重み付けのサポートが必要です。 &lt;code&gt;None&lt;/code&gt; の場合、基本推定量は &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="09e53f3d87743d060ad05fb04ae38587d00307b0" translate="yes" xml:space="preserve">
          <source>The base estimator from which the classifier chain is built.</source>
          <target state="translated">分類器チェーンが構築されるベース推定器。</target>
        </trans-unit>
        <trans-unit id="c229b061f41bb16a20d56916c466508eaa778c17" translate="yes" xml:space="preserve">
          <source>The base estimator from which the ensemble is grown.</source>
          <target state="translated">アンサンブルを成長させるベースとなる推定器。</target>
        </trans-unit>
        <trans-unit id="5066de8ea8cb5c40493cb3ffbb0e912b0ff83ff9" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This can be both a fitted (if &lt;code&gt;prefit&lt;/code&gt; is set to True) or a non-fitted estimator. The estimator must have either a &lt;code&gt;feature_importances_&lt;/code&gt; or &lt;code&gt;coef_&lt;/code&gt; attribute after fitting.</source>
          <target state="translated">トランスが作成されるベース推定量。これは、適合（ &lt;code&gt;prefit&lt;/code&gt; がTrueに設定されている場合）または適合されていない推定量の両方になります。推定器は、フィッティング後に &lt;code&gt;feature_importances_&lt;/code&gt; または &lt;code&gt;coef_&lt;/code&gt; 属性のいずれかを持つ必要があります。</target>
        </trans-unit>
        <trans-unit id="8935e197daa0e6c85ff9f9b0e235723b5ff13893" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the &lt;code&gt;SelectFromModel&lt;/code&gt;, i.e when prefit is False.</source>
          <target state="translated">トランスが作成されるベース推定量。これは、フィットされていない推定量が &lt;code&gt;SelectFromModel&lt;/code&gt; に渡された場合、つまりprefitがFalseの場合にのみ保存されます。</target>
        </trans-unit>
        <trans-unit id="8062156aa44a8a655e18edffbba0a99e0e098c8a" translate="yes" xml:space="preserve">
          <source>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.</source>
          <target state="translated">データセットのランダムな部分集合に適合する基底推定量。Noneの場合、基底推定器は決定木である。</target>
        </trans-unit>
        <trans-unit id="d80c39c018abc387662fda22711c8b7bb4707717" translate="yes" xml:space="preserve">
          <source>The base kernel</source>
          <target state="translated">ベースカーネル</target>
        </trans-unit>
        <trans-unit id="1b7bc12e1d1eb0bb8e39e15748d7f9b27d93ef1a" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns uniform weights to each neighbor. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.</source>
          <target state="translated">基本的な最近隣分類は均一の重みを使用します。つまり、クエリポイントに割り当てられた値は、最近隣の単純多数決から計算されます。状況によっては、近傍により近いものが適合により寄与するように近傍に重みを付ける方が適切です。これは、 &lt;code&gt;weights&lt;/code&gt; キーワードを使用して実行できます。デフォルト値の &lt;code&gt;weights = 'uniform'&lt;/code&gt; 、各近傍に均一の重みを割り当てます。 &lt;code&gt;weights = 'distance'&lt;/code&gt; は、クエリポイントからの距離の逆数に比例する重みを割り当てます。または、距離のユーザー定義関数を指定して、重みを計算することもできます。</target>
        </trans-unit>
        <trans-unit id="650ceb2a1485e5f890a3dcff86c2748037640c3b" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns equal weights to all points. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.</source>
          <target state="translated">基本的な最近傍回帰では、均一の重みが使用されます。つまり、ローカル近傍内の各ポイントがクエリポイントの分類に均一に寄与します。状況によっては、近くのポイントが遠くのポイントよりも回帰に寄与するようにポイントに重みを付けることが有利な場合があります。これは、 &lt;code&gt;weights&lt;/code&gt; キーワードを使用して実行できます。デフォルト値の &lt;code&gt;weights = 'uniform'&lt;/code&gt; 、すべてのポイントに等しい重みを割り当てます。 &lt;code&gt;weights = 'distance'&lt;/code&gt; は、クエリポイントからの距離の逆数に比例する重みを割り当てます。または、距離のユーザー定義関数を指定して、重みを計算することもできます。</target>
        </trans-unit>
        <trans-unit id="1cb5ab68855135ab83ff44b066a7bba092358a07" translate="yes" xml:space="preserve">
          <source>The behavior of &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; is summarized in the following table.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt;の動作を次の表にまとめます。</target>
        </trans-unit>
        <trans-unit id="9c1d95adae9acea07102a8bfe1db513459ebeaee" translate="yes" xml:space="preserve">
          <source>The behavior of the model is very sensitive to the &lt;code&gt;gamma&lt;/code&gt; parameter. If &lt;code&gt;gamma&lt;/code&gt; is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with &lt;code&gt;C&lt;/code&gt; will be able to prevent overfitting.</source>
          <target state="translated">モデルの動作は、 &lt;code&gt;gamma&lt;/code&gt; パラメーターに非常に敏感です。 &lt;code&gt;gamma&lt;/code&gt; が大きすぎる場合、サポートベクトルの影響範囲の半径にはサポートベクトル自体のみが含まれ、 &lt;code&gt;C&lt;/code&gt; による正則化ではオーバーフィットを防ぐことができません。</target>
        </trans-unit>
        <trans-unit id="2ab99fa4c73a04aa03f4ac31b15851c1ddbe51a1" translate="yes" xml:space="preserve">
          <source>The below plot uses the first two features. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;here&lt;/a&gt; for more information on this dataset.</source>
          <target state="translated">以下のプロットは、最初の2つの機能を使用しています。このデータセットの詳細については、&lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;こちら&lt;/a&gt;をご覧ください。</target>
        </trans-unit>
        <trans-unit id="4992d67c6e1aff2322ba4c5bb3c9e0c82a7a36cf" translate="yes" xml:space="preserve">
          <source>The best model is selected by cross-validation.</source>
          <target state="translated">クロスバリデーションにより最適なモデルが選択されます。</target>
        </trans-unit>
        <trans-unit id="1a3f42d6835c4f6686d54e99d9f9cd0e3a597fec" translate="yes" xml:space="preserve">
          <source>The best performance is 1 with &lt;code&gt;normalize == True&lt;/code&gt; and the number of samples with &lt;code&gt;normalize == False&lt;/code&gt;.</source>
          <target state="translated">最高のパフォーマンスは、 &lt;code&gt;normalize == True&lt;/code&gt; は1 、 &lt;code&gt;normalize == False&lt;/code&gt; はサンプル数です。</target>
        </trans-unit>
        <trans-unit id="4b2ab3d16cd4ed6ed82ae137ea7b0ef6d79ff832" translate="yes" xml:space="preserve">
          <source>The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.</source>
          <target state="translated">最高のp値は1/(n_permutations+1)で、最悪は1.0です。</target>
        </trans-unit>
        <trans-unit id="ed9ac623a873633695e926c14de64904b54167f4" translate="yes" xml:space="preserve">
          <source>The best possible score is 1.0, lower values are worse.</source>
          <target state="translated">最高のスコアは1.0で、低い値の方が悪いです。</target>
        </trans-unit>
        <trans-unit id="9230aaef9f0a1fa5e57a82e7f44b36cb5d04d36e" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.</source>
          <target state="translated">最良値は1、最悪値は-1です。0に近い値は、クラスタが重複していることを示します。</target>
        </trans-unit>
        <trans-unit id="6e2eac60da3011cdc30f86be85be8ffa7e5f1da5" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.</source>
          <target state="translated">最良値は1、最悪値は-1です。0に近い値は、クラスタが重複していることを示します。負の値は一般的に、サンプルが間違ったクラスタに割り当てられていることを示します。</target>
        </trans-unit>
        <trans-unit id="b4a689b86b5cda51ef6628ef622a0daf756424bd" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0 when &lt;code&gt;adjusted=False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;adjusted=False&lt;/code&gt; 場合、最適な値は1で、最悪の値は0 です。</target>
        </trans-unit>
        <trans-unit id="488558d8da77f986fd351608404cb6a07bd4fe58" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0.</source>
          <target state="translated">最良値は1、最悪値は0です。</target>
        </trans-unit>
        <trans-unit id="b82813aa45a878f8df07ac95972e2e5116e63bed" translate="yes" xml:space="preserve">
          <source>The bias term in the underlying linear model.</source>
          <target state="translated">基礎となる線形モデルのバイアス項。</target>
        </trans-unit>
        <trans-unit id="d5675ace96b0ccc75bff7a0b67255343283bf5b9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each column.</source>
          <target state="translated">各列の二重クラスタラベル。</target>
        </trans-unit>
        <trans-unit id="d135838b65ddeda2dac8521941211167551d33d9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each row.</source>
          <target state="translated">各行の双曲線ラベル。</target>
        </trans-unit>
        <trans-unit id="b216c2ddafb03b97c09e36e1d0b96ac53ff1f7ba" translate="yes" xml:space="preserve">
          <source>The bins have identical widths.</source>
          <target state="translated">ビンの幅は同じです。</target>
        </trans-unit>
        <trans-unit id="c0eb1458ca2a62b6254994f184c01ee65afc52b4" translate="yes" xml:space="preserve">
          <source>The bins have the same number of samples and depend on &lt;code&gt;y_prob&lt;/code&gt;.</source>
          <target state="translated">ビンには同じ数のサンプルがあり、 &lt;code&gt;y_prob&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="4f153639172a63c8dba2ed27e16715aaa100936d" translate="yes" xml:space="preserve">
          <source>The bipartite structure allows for the use of efficient block Gibbs sampling for inference.</source>
          <target state="translated">二分割構造により、効率的なブロックギブスサンプリングを推論に利用することができます。</target>
        </trans-unit>
        <trans-unit id="a902e8cbc26950a56283f916fe814a1e5f8ed790" translate="yes" xml:space="preserve">
          <source>The bottleneck of a gradient boosting procedure is building the decision trees. Building a traditional decision tree (as in the other GBDTs &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;) requires sorting the samples at each node (for each feature). Sorting is needed so that the potential gain of a split point can be computed efficiently. Splitting a single node has thus a complexity of \(\mathcal{O}(n_\text{features} \times n \log(n))\) where \(n\) is the number of samples at the node.</source>
          <target state="translated">勾配ブースティング手順のボトルネックは、決定木を構築することです。（他のGBDTの&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; のように&lt;/a&gt;）従来の決定木を構築するには、各ノードで（機能ごとに）サンプルを並べ替える必要があります。分割点の潜在的なゲインを効率的に計算できるように、並べ替えが必要です。したがって、単一のノードを分割すると、複雑さが\（\ mathcal {O}（n_ \ text {features} \ times n \ log（n））\）になります。ここで、\（n \）はノードのサンプル数です。</target>
        </trans-unit>
        <trans-unit id="b104bf424ac624d7987a6846ffafb73fcb6a3323" translate="yes" xml:space="preserve">
          <source>The bounding box for each cluster center when centers are generated at random.</source>
          <target state="translated">中心がランダムに生成されたときの各クラスタ中心の外接箱。</target>
        </trans-unit>
        <trans-unit id="e73634a1f979122b85e5e2e94c3d89b631e61d63" translate="yes" xml:space="preserve">
          <source>The boxes represent repeated sampling.</source>
          <target state="translated">箱は繰り返しのサンプリングを表しています。</target>
        </trans-unit>
        <trans-unit id="de87510aaa8a4634fe7b670444f1141d0b934e04" translate="yes" xml:space="preserve">
          <source>The breast cancer dataset is a classic and very easy binary classification dataset.</source>
          <target state="translated">乳がんデータセットは、古典的で非常に簡単な二値分類データセットです。</target>
        </trans-unit>
        <trans-unit id="2045b20286b76d60e5fa1f74a1df8fc02a45c66b" translate="yes" xml:space="preserve">
          <source>The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the &amp;ldquo;calibration&amp;rdquo; of a set of probabilistic predictions.</source>
          <target state="translated">ブライアスコアの損失も0〜1であり、スコアが低いほど（平均二乗差が小さいほど）、予測はより正確になります。これは、一連の確率的予測の「キャリブレーション」の尺度と考えることができます。</target>
        </trans-unit>
        <trans-unit id="b76bd58217534fc8775d360bccb78e720219d44b" translate="yes" xml:space="preserve">
          <source>The calibration is based on the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; method of the &lt;code&gt;base_estimator&lt;/code&gt; if it exists, else on &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="translated">キャリブレーションは、 &lt;code&gt;base_estimator&lt;/code&gt; が存在する場合は&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;メソッドに基づいており、存在しない場合は&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_probaに&lt;/a&gt;基づいています。</target>
        </trans-unit>
        <trans-unit id="6452eadf0058adcc76ac85d9ade38659e522fb0c" translate="yes" xml:space="preserve">
          <source>The calibration of the model can be assessed by plotting the mean observed value vs the mean predicted value on groups of test samples binned by predicted risk.</source>
          <target state="translated">モデルの較正は,予測リスクで区分した試験サンプルのグループで,観測値の平均値と予測値の平均値をプロットすることで評価できる.</target>
        </trans-unit>
        <trans-unit id="74481417cca6e6adb9f25dfbb8e31b8fc201eb01" translate="yes" xml:space="preserve">
          <source>The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function.</source>
          <target state="translated">逆変換に使用するコールアブル。これは、inverse transform と同じ引数が渡され、args と kwargs が転送されます。inverse_func が None の場合は、inverse_func が ID 関数となります。</target>
        </trans-unit>
        <trans-unit id="81ea3433c521d90153f147dfae64d0df290c9757" translate="yes" xml:space="preserve">
          <source>The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function.</source>
          <target state="translated">変換に使用する callable。これはtransformと同じ引数が渡され、argsとkwargsが転送されます。func が None の場合、func は identity 関数になります。</target>
        </trans-unit>
        <trans-unit id="a1419fd6aa69dd9ba89b3d48a7257eafd52e186a" translate="yes" xml:space="preserve">
          <source>The categorical Naive Bayes classifier is suitable for classification with discrete features that are categorically distributed. The categories of each feature are drawn from a categorical distribution.</source>
          <target state="translated">カテゴリカル・ナイーブ・ベイズ分類器は、カテゴリカルに分布する離散的な特徴を用いた分類に適しています。各特徴のカテゴリは、カテゴリ分布から引き出されます。</target>
        </trans-unit>
        <trans-unit id="1f38539ea641dd8b50f2f7d85a7f466ba670ba50" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;).</source>
          <target state="translated">フィッティング中に決定された各特徴のカテゴリ（Xの特徴の順に、 &lt;code&gt;transform&lt;/code&gt; の出力に対応）。</target>
        </trans-unit>
        <trans-unit id="12b8e6304a220ce13ab40041e41390ae67d86b4e" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;). This includes the category specified in &lt;code&gt;drop&lt;/code&gt; (if any).</source>
          <target state="translated">フィッティング中に決定された各機能のカテゴリ（Xの機能の順序で、 &lt;code&gt;transform&lt;/code&gt; の出力に対応）。これには、 &lt;code&gt;drop&lt;/code&gt; 指定されたカテゴリが含まれます（存在する場合）。</target>
        </trans-unit>
        <trans-unit id="11dc40b09acf079281dfd9a81c3951e435409b7d" translate="yes" xml:space="preserve">
          <source>The centers of each cluster. Only returned if &lt;code&gt;return_centers=True&lt;/code&gt;.</source>
          <target state="translated">各クラスターの中心。 &lt;code&gt;return_centers=True&lt;/code&gt; の場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="9d4177cfd25fe2d42f6a09b10ba8f700ec03cc45" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is given by</source>
          <target state="translated">カイ二乗カーネルは次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="e654b311ef425d553f0f6705f2dd43fc95968d41" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is most commonly used on histograms (bags) of visual words.</source>
          <target state="translated">カイ二乗カーネルは、視覚的な単語のヒストグラム(袋)で最もよく使われます。</target>
        </trans-unit>
        <trans-unit id="085e153138bf3768d8fc53856c1f1238c6d3caff" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt;&lt;code&gt;chi2_kernel&lt;/code&gt;&lt;/a&gt; and then passed to an &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt;:</source>
          <target state="translated">カイ2乗カーネルは、コンピュータービジョンアプリケーションで非線形SVMをトレーニングするための非常に人気のある選択肢です。これは、使用して計算することができ&lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt; &lt;code&gt;chi2_kernel&lt;/code&gt; を&lt;/a&gt;し、その後に渡さ&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; &lt;/a&gt;で &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="9233e345f2318b5fc92b12dd745f5adf2df3c33c" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative. This kernel is most commonly applied to histograms.</source>
          <target state="translated">カイ2乗カーネルは,X と Y の各行のペア間で計算されます.このカーネルは,ヒストグラムに最も一般的に適用される.</target>
        </trans-unit>
        <trans-unit id="a8ba729b106653acd0bc97ad998e8dac7b2b635e" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is given by:</source>
          <target state="translated">カイ二乗カーネルは次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="959a186ffdea8e09c8857bbd475b2cf453d363af" translate="yes" xml:space="preserve">
          <source>The child estimator template used to create the collection of fitted sub-estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ac1548c51d0c4529f9b6250f5d16db6b392dc40" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_features&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_features&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_features]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_features + i&lt;/code&gt;</source>
          <target state="translated">各非リーフノードの子。 &lt;code&gt;n_features&lt;/code&gt; 未満の値は、元のサンプルであるツリーの葉に対応します。 &lt;code&gt;n_features&lt;/code&gt; 以上のノード &lt;code&gt;i&lt;/code&gt; は非リーフノードであり、子 &lt;code&gt;children_[i - n_features]&lt;/code&gt; ます。あるいは、i番目の反復で、children [i] [0]とchildren [i] [1]がマージされ、ノード &lt;code&gt;n_features + i&lt;/code&gt; 形成されます。</target>
        </trans-unit>
        <trans-unit id="c8989abb78a441b49758b99ec48d8b090a3ce248" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_samples&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_samples&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_samples]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_samples + i&lt;/code&gt;</source>
          <target state="translated">各非リーフノードの子。 &lt;code&gt;n_samples&lt;/code&gt; より小さい値は、元のサンプルであるツリーの葉に対応します。 &lt;code&gt;n_samples&lt;/code&gt; 以上のノード &lt;code&gt;i&lt;/code&gt; は非リーフノードであり、子 &lt;code&gt;children_[i - n_samples]&lt;/code&gt; ます。あるいは、i番目の反復で、children [i] [0]とchildren [i] [1]がマージされ、ノード &lt;code&gt;n_samples + i&lt;/code&gt; 形成されます。</target>
        </trans-unit>
        <trans-unit id="f3473fe6f2b500963d93290672a7a3ef3f212da2" translate="yes" xml:space="preserve">
          <source>The choice of features is not particularly helpful, but serves to illustrate the technique.</source>
          <target state="translated">特徴の選択は特に参考になりませんが、テクニックを説明するのに役立ちます。</target>
        </trans-unit>
        <trans-unit id="15475f4de31a0dbbaa5b0396fc18010629c57dd3" translate="yes" xml:space="preserve">
          <source>The choice of the distribution depends on the problem at hand:</source>
          <target state="translated">分布の選択は、手元の問題によって異なります。</target>
        </trans-unit>
        <trans-unit id="1f460fe2e0e34556a3c6a8a6d017a5f63eeb9ea9" translate="yes" xml:space="preserve">
          <source>The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">各混合成分の精度行列のコレスキー分解。精度行列は、共分散行列の逆です。共分散行列は対称正定行列であるため、ガウス混合は精度行列によって同等にパラメーター化できます。共分散行列の代わりに精度行列を格納すると、テスト時に新しいサンプルの対数尤度を計算するのがより効率的になります。形状は &lt;code&gt;covariance_type&lt;/code&gt; によって異なります。</target>
        </trans-unit>
        <trans-unit id="8e7b3855f3de3292a39f43c9800aebcf16bc77cb" translate="yes" xml:space="preserve">
          <source>The claim &lt;strong&gt;frequency&lt;/strong&gt; is the number of claims divided by the exposure, typically measured in number of claims per year.</source>
          <target state="translated">クレームの&lt;strong&gt;頻度&lt;/strong&gt;は、クレームの数をエクスポージャーで割ったものであり、通常は1年あたりのクレームの数で測定されます。</target>
        </trans-unit>
        <trans-unit id="54b709e28c0bc302f278f75cf2f9281781fc40ea" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; used with the optional parameter &lt;code&gt;svd_solver='randomized'&lt;/code&gt; is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.</source>
          <target state="translated">オプションのパラメーター &lt;code&gt;svd_solver='randomized'&lt;/code&gt; で使用されるクラス&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;は、その場合に非常に役立ちます。ほとんどの特異ベクトルを削除するため、計算を特異ベクトルの近似推定に制限する方がはるかに効率的です。実際に変換を実行し続けます。</target>
        </trans-unit>
        <trans-unit id="9afc238719eb2575519db8e476eb36c39ec071a2" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt;&lt;code&gt;DictVectorizer&lt;/code&gt;&lt;/a&gt; can be used to convert feature arrays represented as lists of standard Python &lt;code&gt;dict&lt;/code&gt; objects to the NumPy/SciPy representation used by scikit-learn estimators.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt; &lt;code&gt;DictVectorizer&lt;/code&gt; &lt;/a&gt;クラスを使用して、標準のPython &lt;code&gt;dict&lt;/code&gt; オブジェクトのリストとして表される機能配列を、scikit-learn推定器で使用されるNumPy / SciPy表現に変換できます。</target>
        </trans-unit>
        <trans-unit id="0b3d7d58279a2952a288a94db8a135a534d78ec7" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; is a high-speed, low-memory vectorizer that uses a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;feature hashing&lt;/a&gt;, or the &amp;ldquo;hashing trick&amp;rdquo;. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no &lt;code&gt;inverse_transform&lt;/code&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt;クラスは、高速機能、低メモリのベクトライザーであり、&lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;特徴ハッシュ&lt;/a&gt;、または「ハッシュトリック」と呼ばれる手法を使用します。&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt;行うように、トレーニングで遭遇する特徴のハッシュテーブルを構築する代わりに、FeatureHasherのインスタンスはハッシュ関数を特徴に適用して、サンプルマトリックスの列インデックスを直接決定します。その結果、検査可能性を犠牲にして、速度が向上し、メモリ使用量が減少します。調理人は、入力機能は同じように見えたし、何も持っていないものを覚えていない &lt;code&gt;inverse_transform&lt;/code&gt; の方法を。</target>
        </trans-unit>
        <trans-unit id="46148e4d160f9510def06597f1921af425f74fcb" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing function to data. It solves the following problem:</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; は&lt;/a&gt;、減少しない関数をデータに適合させます。次の問題を解決します。</target>
        </trans-unit>
        <trans-unit id="0773ec8e22bf216d4df607f3329b18b1168f7c09" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing real function to 1-dimensional data. It solves the following problem:</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; は&lt;/a&gt;、減少しない実関数を1次元データに適合させます。次の問題を解決します。</target>
        </trans-unit>
        <trans-unit id="04846022f8485d75461ddcb301a4717897728672" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; implements this component wise deterministic sampling. Each component is sampled \(n\) times, yielding \(2n+1\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \(n\) is usually chosen to be 1 or 2, transforming the dataset to size &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (in the case of \(n=2\)).</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt;は、このコンポーネントの決定論的サンプリングを実装します。各コンポーネントは\（n \）回サンプリングされ、入力次元ごとに\（2n + 1 \）次元になります（フーリエ変換の実部と複素部からの2つの倍数）。文献では、\（n \）は通常1または2に選択され、データセットをサイズ &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; （\（n = 2 \）の場合）。</target>
        </trans-unit>
        <trans-unit id="9fdc0ed638cb0889fa5320ee5ada72b80a16402f" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt;&lt;code&gt;ElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt; &lt;code&gt;ElasticNetCV&lt;/code&gt; &lt;/a&gt;クラスを使用して、交差検定によってパラメーター &lt;code&gt;alpha&lt;/code&gt; （\（\ alpha \））および &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ rho \））を設定できます。</target>
        </trans-unit>
        <trans-unit id="c89182d0633b09742e745213024a53ccec4fee0a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt;&lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt; &lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt; &lt;/a&gt;を使用して、交差検証によってパラメーター &lt;code&gt;alpha&lt;/code&gt; （\（\ alpha \））および &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ rho \））を設定できます。</target>
        </trans-unit>
        <trans-unit id="6c66ee2c7a53fbda2b2b2da602d1e782a1fcc1b3" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;は、1次SGD学習ルーチンを実装します。アルゴリズムはトレーニングの例を反復し、各例について、次によって与えられる更新規則に従ってモデルパラメーターを更新します。</target>
        </trans-unit>
        <trans-unit id="f128e57d09cf8b511f002f70c40429b250044323" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;は、分類のためのさまざまな損失関数とペナルティをサポートする単純な確率的勾配降下学習ルーチンを実装します。</target>
        </trans-unit>
        <trans-unit id="3fc72b8fffc5a512701e2a944bd472c169bc1771" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; trained with the hinge loss, equivalent to a linear SVM.</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;は、分類に対するさまざまな損失関数とペナルティをサポートする、単純な確率的勾配降下学習ルーチンを実装します。以下は、線形SVMと同等のヒンジ損失でトレーニングされた&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; の&lt;/a&gt;決定境界です。</target>
        </trans-unit>
        <trans-unit id="9be5b4c1c13de0290ee1bddf8fed519138b844e9" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; is well suited for regression problems with a large number of training samples (&amp;gt; 10.000), for other problems we recommend &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;クラスは、線形回帰モデルに適合するためのさまざまな損失関数とペナルティをサポートする単純な確率的勾配降下学習ルーチンを実装しています。&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;は、多数のトレーニングサンプル（&amp;gt; 10.000）の回帰問題に適しています。他の問題については、&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;、または&lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt; &lt;code&gt;ElasticNet&lt;/code&gt; &lt;/a&gt;をお勧めします。</target>
        </trans-unit>
        <trans-unit id="5ca16e4c08c0574f508b6c526beeb2111eade54a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;OneClassSVM&lt;/code&gt;&lt;/a&gt; implements a One-Class SVM which is used in outlier detection.</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;OneClassSVM&lt;/code&gt; &lt;/a&gt;は、外れ値の検出に使用されるOne-Class SVMを実装します。</target>
        </trans-unit>
        <trans-unit id="5eaffd69df806c5b8353c88358cfd0a49b275168" translate="yes" xml:space="preserve">
          <source>The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in &lt;code&gt;gbrt.classes_&lt;/code&gt;.</source>
          <target state="translated">PDPを計算する必要があるクラスラベル。gbrtがマルチクラスモデルの場合のみ。 &lt;code&gt;gbrt.classes_&lt;/code&gt; にある必要があります。</target>
        </trans-unit>
        <trans-unit id="2a31ab08188f24bd31e99f218cab5467eddf2ef0" translate="yes" xml:space="preserve">
          <source>The class labels.</source>
          <target state="translated">クラスのラベルです。</target>
        </trans-unit>
        <trans-unit id="7c57202048886b9c3874e17b392c104a7a21da98" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="translated">入力サンプルのクラスログ確率。クラスの順序は、属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;の順序に対応しています。</target>
        </trans-unit>
        <trans-unit id="075fd1f474c95d9f3895b75ead4b1f9c99be54a8" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">入力サンプルのクラス対数確率。クラスの順序は、属性 &lt;code&gt;classes_&lt;/code&gt; の順序に対応しています。</target>
        </trans-unit>
        <trans-unit id="b5b69c523a5547b26d366d35f11f7dab77d71024" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. The smaller \(\nu\), the less smooth the approximated function is. As \(\nu\rightarrow\infty\), the kernel becomes equivalent to the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel. When \(\nu = 1/2\), the Mat&amp;eacute;rn kernel becomes identical to the absolute exponential kernel. Important intermediate values are \(\nu=1.5\) (once differentiable functions) and \(\nu=2.5\) (twice differentiable functions).</source>
          <target state="translated">Maternカーネルのクラスは、&lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; の&lt;/a&gt;一般化です。結果の関数の滑らかさを制御する追加のパラメーター\（\ nu \）があります。\（\ nu \）が小さいほど、近似関数は滑らかではありません。\（\ nu \ rightarrow \ infty \）として、カーネルは&lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt;カーネルと同等になります。\（\ nu = 1/2 \）の場合、Mat&amp;eacute;rnカーネルは絶対指数カーネルと同一になります。重要な中間値は、\（\ nu = 1.5 \）（1回の微分可能関数）と\（\ nu = 2.5 \）（2回の微分可能関数）です。</target>
        </trans-unit>
        <trans-unit id="f9e49144e04860d9a1f4a8512901a503e5d41c6f" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the RBF and the absolute exponential kernel parameterized by an additional parameter nu. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).</source>
          <target state="translated">Matern カーネルのクラスは,RBF と絶対指数カーネルの一般化であり,追加のパラメータ nu でパラメータ化されています.nu が小さいほど,近似関数は滑らかではありません.nu=inf の場合,カーネルは RBF カーネルと等価になり,nu=0.5 の場合は絶対指数カーネルと等価になります.重要な中間値は、nu=1.5 (1回微分可能な関数)とnu=2.5 (2回微分可能な関数)です。</target>
        </trans-unit>
        <trans-unit id="e48205f573bda857c27ec3937eb80960daed3556" translate="yes" xml:space="preserve">
          <source>The class ordering is preserved:</source>
          <target state="translated">クラスの並び順は保存されます。</target>
        </trans-unit>
        <trans-unit id="0401a90c4f50bc0cacd4ce11e3ddd76a2d543405" translate="yes" xml:space="preserve">
          <source>The class prior probabilities. By default, the class proportions are inferred from the training data.</source>
          <target state="translated">クラスの事前確率。デフォルトでは、クラスの割合は学習データから推測されます。</target>
        </trans-unit>
        <trans-unit id="e9e493bffc66549584cbf656024fe268e745fcd4" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples.</source>
          <target state="translated">入力サンプルのクラス確率.</target>
        </trans-unit>
        <trans-unit id="6a7a001ab832443f9ea75be4af1e68e4b9eb028c" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute.</source>
          <target state="translated">入力サンプルのクラス確率。出力の順序は、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;属性の順序と同じです。</target>
        </trans-unit>
        <trans-unit id="81478ef84d2d645ae9c4e632247f7f3bc9052a92" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute.</source>
          <target state="translated">入力サンプルのクラス確率。出力の順序は、 &lt;code&gt;classes_&lt;/code&gt; 属性の順序と同じです。</target>
        </trans-unit>
        <trans-unit id="eb02560f00e2596a11150c32ba96675012f41151" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="translated">入力サンプルのクラス確率。クラスの順序は、属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;の順序に対応しています。</target>
        </trans-unit>
        <trans-unit id="8ea2403e3ddde25cdaa7213df112f25928f5232d" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">入力サンプルのクラス確率。クラスの順序は、属性 &lt;code&gt;classes_&lt;/code&gt; の順序に対応しています。</target>
        </trans-unit>
        <trans-unit id="64b22dfb54911895a611d3b9cfbaf3a3f14d97f5" translate="yes" xml:space="preserve">
          <source>The class to report if &lt;code&gt;average='binary'&lt;/code&gt; and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting &lt;code&gt;labels=[pos_label]&lt;/code&gt; and &lt;code&gt;average != 'binary'&lt;/code&gt; will report scores for that label only.</source>
          <target state="translated">&lt;code&gt;average='binary'&lt;/code&gt; でデータがバイナリの場合に報告するクラス。データがマルチクラスまたはマルチラベルの場合、これは無視されます。設定 &lt;code&gt;labels=[pos_label]&lt;/code&gt; と &lt;code&gt;average != 'binary'&lt;/code&gt; だけでラベルのスコアを報告します。</target>
        </trans-unit>
        <trans-unit id="b02b6e7e5cca0ff24c2691f6e7d2bc9c247d17d1" translate="yes" xml:space="preserve">
          <source>The class to use to build the returned adjacency matrix.</source>
          <target state="translated">返された隣接関係行列を構築するために利用されるクラス.</target>
        </trans-unit>
        <trans-unit id="08bf02b9f7a917eed20c9c94c384791c0ed4578f" translate="yes" xml:space="preserve">
          <source>The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.</source>
          <target state="translated">一対一の適合を実行するクラス。Noneの場合は、与えられた問題が2値であると仮定します。</target>
        </trans-unit>
        <trans-unit id="26dc8660d1112d77620f09027ce7b9fbee6027a3" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; fits a logistic regression model, while with &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; it fits a linear support vector machine (SVM).</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; は&lt;/a&gt;、さまざまな（凸）損失関数とさまざまなペナルティを使用して、分類と回帰の線形モデルに適合する機能を提供します。たとえば、 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; の場合、&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;はロジスティック回帰モデルに適合し、 &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; の場合、線形サポートベクターマシン（SVM）に適合します。</target>
        </trans-unit>
        <trans-unit id="d29e2c71b01d97c9c93dc5aad937ec38e91b7ca9" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide two criteria to stop the algorithm when a given level of convergence is reached:</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;は、指定された収束レベルに達したときにアルゴリズムを停止するための2つの基準を提供します。</target>
        </trans-unit>
        <trans-unit id="d7ef8ada32aed500913b0cf053368e18ea306f33" translate="yes" xml:space="preserve">
          <source>The classes in &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can handle either NumPy arrays or &lt;code&gt;scipy.sparse&lt;/code&gt; matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt;のクラスは、NumPy配列または &lt;code&gt;scipy.sparse&lt;/code&gt; 行列を入力として処理できます。密な行列の場合、多数の可能な距離メトリックがサポートされます。スパース行列の場合、任意のミンコフスキーメトリックが検索でサポートされます。</target>
        </trans-unit>
        <trans-unit id="f39d71815c55889dd46cb64855cc1d40f837d2d9" translate="yes" xml:space="preserve">
          <source>The classes in the &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators&amp;rsquo; accuracy scores or to boost their performance on very high-dimensional datasets.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; &lt;/a&gt;モジュールのクラスは、推定器の精度スコアを向上させるため、または非常に高次元のデータセットでのパフォーマンスを向上させるために、サンプルセットの特徴選択/次元削減に使用できます。</target>
        </trans-unit>
        <trans-unit id="df672af473b055efb339763c1ea56e27edeec8c9" translate="yes" xml:space="preserve">
          <source>The classes in this submodule allow to approximate the embedding \(\phi\), thereby working explicitly with the representations \(\phi(x_i)\), which obviates the need to apply the kernel or store training examples.</source>
          <target state="translated">このサブモジュールのクラスでは、埋め込みを近似することで、表現を明示的に扱うことができ、カーネルの適用や学習例の保存が不要になります。</target>
        </trans-unit>
        <trans-unit id="41647bba96e968e0bd58965ae65f2bc0365d42c6" translate="yes" xml:space="preserve">
          <source>The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</source>
          <target state="translated">クラスのラベル(単一出力問題)、またはクラスのラベルの配列のリスト(複数出力問題)。</target>
        </trans-unit>
        <trans-unit id="772daeb9b0eaa408b264b0fa548b1cbd24779805" translate="yes" xml:space="preserve">
          <source>The classes labels.</source>
          <target state="translated">クラスのラベル。</target>
        </trans-unit>
        <trans-unit id="13f18ce429d97546a8b16cf132e8e6c03147cb50" translate="yes" xml:space="preserve">
          <source>The classic implementation of the clustering method based on the Lloyd&amp;rsquo;s algorithm. It consumes the whole set of input data at each iteration.</source>
          <target state="translated">ロイドのアルゴリズムに基づくクラスタリング手法の古典的な実装。各反復で入力データのセット全体を消費します。</target>
        </trans-unit>
        <trans-unit id="b3e4f700832d6cdb3a215961499bbf68e1aa40b8" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">分類は、視覚化の目的でPCAとCCAによって検出された最初の2つの主成分に射影し、続いて線形カーネルを持つ2つのSVCを使用して&lt;a href=&quot;../../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; メタ分類子&lt;/a&gt;を使用し、各クラスの識別モデルを学習することによって実行されます。PCAは教師なし次元削減を実行するために使用され、CCAは教師なし次元削減を実行するために使用されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="4b0c76ea0ae326d19f535ab9520b9dd024cf9124" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">分類は、PCAとCCAが見つけた最初の2つの主成分に投影して視覚化し、その後、&lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; メタ分類子&lt;/a&gt;を使用して、線形カーネルを持つ2つのSVCを使用し、各クラスの判別モデルを学習します。PCAは教師なし次元削減を実行するために使用され、CCAは教師付き次元削減を実行するために使用されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="3f7b4b923ee0ab9dbf257d91692081f3d6b6945f" translate="yes" xml:space="preserve">
          <source>The classification target. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas Series.</source>
          <target state="translated">分類対象。 &lt;code&gt;as_frame=True&lt;/code&gt; の場合、 &lt;code&gt;target&lt;/code&gt; はパンダシリーズになります。</target>
        </trans-unit>
        <trans-unit id="7c83c4f2209684d200eb3e2732a4357005bab63e" translate="yes" xml:space="preserve">
          <source>The classifier which predicts given the output of &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;estimators_&lt;/code&gt; の出力を指定して予測する分類器。</target>
        </trans-unit>
        <trans-unit id="d9bcc8debd316bfd1c4b9e73630f6ba7215b9e3d" translate="yes" xml:space="preserve">
          <source>The classifier whose output decision function needs to be calibrated to offer more accurate predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.</source>
          <target state="translated">より正確な predict_proba 出力を提供するために,出力決定関数を校正する必要がある分類器.cv=prefit の場合,その分類器はすでにデータに適合していなければいけません.</target>
        </trans-unit>
        <trans-unit id="4188695350bfd2d35a500d1eee9968d78806da89" translate="yes" xml:space="preserve">
          <source>The classifier whose output need to be calibrated to provide more accurate &lt;code&gt;predict_proba&lt;/code&gt; outputs.</source>
          <target state="translated">より正確な &lt;code&gt;predict_proba&lt;/code&gt; 出力を提供するために、出力を調整する必要がある分類子。</target>
        </trans-unit>
        <trans-unit id="917e6ef2892859cfeed06565c1a2e19769195da1" translate="yes" xml:space="preserve">
          <source>The cluster ordered list of sample indices.</source>
          <target state="translated">サンプルインデックスのクラスター順リスト。</target>
        </trans-unit>
        <trans-unit id="88e39a3dfd7087282361f3b39109354f7ff32ede" translate="yes" xml:space="preserve">
          <source>The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs.</source>
          <target state="translated">以下のコードは、複数のジョブ内で予測の構築と計算を並列化する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="6a1cc4b5491e9b42fd445850086762fcb248e836" translate="yes" xml:space="preserve">
          <source>The code below plots the dependency of y against individual x_i and normalized values of univariate F-tests statistics and mutual information.</source>
          <target state="translated">以下のコードは、個々のx_iに対するyの依存性と、一変量F検定統計量と相互情報の正規化された値をプロットしています。</target>
        </trans-unit>
        <trans-unit id="4b0a65eb61a5277b5e28aef5c3b0e7ea577bdc07" translate="yes" xml:space="preserve">
          <source>The code-examples in the above tutorials are written in a &lt;em&gt;python-console&lt;/em&gt; format. If you wish to easily execute these examples in &lt;strong&gt;IPython&lt;/strong&gt;, use:</source>
          <target state="translated">上記のチュートリアルのコード例は、&lt;em&gt;python-console&lt;/em&gt;形式で記述されています。これらの例を&lt;strong&gt;IPython&lt;/strong&gt;で簡単に実行したい場合は、次を使用します。</target>
        </trans-unit>
        <trans-unit id="ce298b790cf6df954f5ce5058b04debe04deb707" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">係数 R^2 は,(1-u/v)として定義され,ここで u は残差平方和((y_true-y_pred)**2).sum()であり,v は回帰平方和((y_true-y_true.mean())である.**2).sum()。可能な最高スコアは1.0で,負の値にすることができる(モデルが任意に悪くなる可能性があるため).入力特徴量を無視して常にyの期待値を予測する定数モデルは,R^2スコア0.0を得ます.</target>
        </trans-unit>
        <trans-unit id="48121e6567bc65fc6ea38acd7d461ac73e2ea472" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">係数R^2は,(1-u/v)として定義され,ここで u は2乗の残差総和((y_true-y_pred)**2).sum()であり,v は2乗の総和((y_true-y_true.mean())である.**2).sum()となります。可能な最高のスコアは1.0で,負の値になることがある(モデルが任意に悪くなることがあるため).入力特徴量を無視して常にyの期待値を予測する定数モデルは,R^2スコア0.0を得ます.</target>
        </trans-unit>
        <trans-unit id="a7b8f1c4eb5f60ca5d7c57d0dcd2ad470def6e62" translate="yes" xml:space="preserve">
          <source>The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="translated">通常最小二乗法の係数推定値は、特徴の独立性に依存しています。特徴が相関し、計画行列\（X \）の列が近似線形依存性を持つ場合、計画行列は特異に近くなり、その結果、最小二乗推定は観測されたターゲットのランダムエラーに非常に敏感になります。大きな変動を生み出します。&lt;em&gt;多重共線性の&lt;/em&gt;この状況は、たとえば、実験計画なしでデータが収集された場合に発生する可能&lt;em&gt;性&lt;/em&gt;があります。</target>
        </trans-unit>
        <trans-unit id="f0dabbd241b37afcd99edc97579580c22f2703f9" translate="yes" xml:space="preserve">
          <source>The coefficient of the underlying linear model. It is returned only if coef is True.</source>
          <target state="translated">基礎となる線形モデルの係数。coefがTrueの場合のみ返されます。</target>
        </trans-unit>
        <trans-unit id="8b0531322a0447d32da022aa29f51bdb5853bf2b" translate="yes" xml:space="preserve">
          <source>The coefficients \(w\) of the model can be accessed:</source>
          <target state="translated">モデルの係数にアクセスできます。</target>
        </trans-unit>
        <trans-unit id="8bf2184b15d694a8d6ef65832d1a84477f9388bd" translate="yes" xml:space="preserve">
          <source>The coefficients are significantly different. AGE and EXPERIENCE coefficients are both positive but they now have less influence on the prediction.</source>
          <target state="translated">係数は有意に異なっている。AGEとEXPERIENCEの係数はどちらも正の値ですが、現在は予測への影響が少なくなっています。</target>
        </trans-unit>
        <trans-unit id="b2933da174c8fa332cc9ad841c0c25a1ab1996d2" translate="yes" xml:space="preserve">
          <source>The coefficients can be forced to be positive.</source>
          <target state="translated">係数は強制的に正の値にすることができる。</target>
        </trans-unit>
        <trans-unit id="3c89d0ba57b5481659a50cc73b873694c355fd9b" translate="yes" xml:space="preserve">
          <source>The coefficients of the linear model: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</source>
          <target state="translated">線形モデルの係数： &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="5395d49e7d69c07e1b2416a99bbd87627621ae2f" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the coefficient of determination are also calculated.</source>
          <target state="translated">また、係数、二乗の残差和、決定係数も計算されます。</target>
        </trans-unit>
        <trans-unit id="a7757730afe97cb8593dea4757d82727872c7529" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the variance score are also calculated.</source>
          <target state="translated">係数、二乗の残差和、分散スコアも計算されます。</target>
        </trans-unit>
        <trans-unit id="f15e1f2c7fb96cd43c32056a6c407c9c1c570883" translate="yes" xml:space="preserve">
          <source>The collection of fitted base estimators.</source>
          <target state="translated">フィットした基底推定器のコレクション。</target>
        </trans-unit>
        <trans-unit id="4d0721afe1a467c5d2eb2443395ac7ab7a4e3f8b" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &amp;lsquo;drop&amp;rsquo;.</source>
          <target state="translated">'drop'ではない &lt;code&gt;estimators&lt;/code&gt; で定義された適合サブ推定量のコレクション。</target>
        </trans-unit>
        <trans-unit id="e97d4f2642ef3372648a38fef3350e771647c3d6" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; ではない、 &lt;code&gt;estimators&lt;/code&gt; で定義されている、当てはめられたサブ推定器のコレクション。</target>
        </trans-unit>
        <trans-unit id="0b6e390487ac628e8895818624f2b32fd0c95d36" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators.</source>
          <target state="translated">フィットしたサブエスティメーターのコレクション。</target>
        </trans-unit>
        <trans-unit id="929cef17b33a8aadbb4cff8f87d4813d9e794dca" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators. &lt;code&gt;loss_.K&lt;/code&gt; is 1 for binary classification, otherwise n_classes.</source>
          <target state="translated">適合したサブ推定量のコレクション。 &lt;code&gt;loss_.K&lt;/code&gt; は、バイナリ分類の場合は1、それ以外の場合はn_classesです。</target>
        </trans-unit>
        <trans-unit id="34213cac4f6d6096a4a72655d4c46d8da28f907c" translate="yes" xml:space="preserve">
          <source>The collection of fitted transformers as tuples of (name, fitted_transformer, column). &lt;code&gt;fitted_transformer&lt;/code&gt; can be an estimator, &amp;lsquo;drop&amp;rsquo;, or &amp;lsquo;passthrough&amp;rsquo;. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (&amp;lsquo;remainder&amp;rsquo;, transformer, remaining_columns) corresponding to the &lt;code&gt;remainder&lt;/code&gt; parameter. If there are remaining columns, then &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt;, otherwise &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt;.</source>
          <target state="translated">（name、fitted_transformer、column）のタプルとしてのフィット済みトランスフォーマーのコレクション。 &lt;code&gt;fitted_transformer&lt;/code&gt; は、推定器、 'drop'、または 'passthrough'にすることができます。列が選択されていない場合、これは未適合のトランスになります。残りの列がある場合、最後の要素は、 &lt;code&gt;remainder&lt;/code&gt; パラメーターに対応する次の形式のタプルです：（ 'remainder'、トランスフォーマー、maining_columns）。列が残っている場合は &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt; 、それ以外の場合は &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="3626dc1999e74681fe5d5d9d41be14f043b3b76d" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the image, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="translated">イメージから抽出されたパッチのコレクション &lt;code&gt;n_patches&lt;/code&gt; は &lt;code&gt;max_patches&lt;/code&gt; または抽出可能なパッチの総数です。</target>
        </trans-unit>
        <trans-unit id="cd0348cc380532b01c9102e25a743ff6f1c3fc49" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the images, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;n_samples * max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="translated">イメージから抽出されたパッチのコレクション &lt;code&gt;n_patches&lt;/code&gt; は &lt;code&gt;n_samples * max_patches&lt;/code&gt; または抽出できるパッチの総数のいずれかです。</target>
        </trans-unit>
        <trans-unit id="d6d471d12278a874c4225d83bbd25d7f04a0a976" translate="yes" xml:space="preserve">
          <source>The color map illustrates the decision function learned by the SVC.</source>
          <target state="translated">カラーマップは、SVCが学習した決定関数を示しています。</target>
        </trans-unit>
        <trans-unit id="093f5986f8d055d5b47f11ad021ce6ecfcef91e9" translate="yes" xml:space="preserve">
          <source>The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.</source>
          <target state="translated">indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]の列は、i番目の推定量の指標値を与える。</target>
        </trans-unit>
        <trans-unit id="f0cca3e664aa52c0e892615c12365c0a6af40452" translate="yes" xml:space="preserve">
          <source>The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.</source>
          <target state="translated">この例で使用されている組み合わせは、このデータセットでは特に有用ではなく、FeatureUnionの使用法を説明するためだけに使用されています。</target>
        </trans-unit>
        <trans-unit id="238b85659a6f96b691291ac49250c2a48af41530" translate="yes" xml:space="preserve">
          <source>The complete set of patches. If the patches contain colour information, channels are indexed along the last dimension: RGB patches would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="translated">パッチの完全なセット。パッチに色情報が含まれている場合、チャネルは最後の次元に沿ってインデックスが付けられます。RGBパッチの &lt;code&gt;n_channels=3&lt;/code&gt; になります。</target>
        </trans-unit>
        <trans-unit id="b4849a6d39c196dd952c4e98da8a227fce685751" translate="yes" xml:space="preserve">
          <source>The complexity parameter \(\alpha \geq 0\) controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="translated">複雑度パラメータは 縮みの量を制御します。</target>
        </trans-unit>
        <trans-unit id="4524a6f67fb21e2d5fb712c219044b7460dbad0e" translate="yes" xml:space="preserve">
          <source>The components of the random matrix are drawn from N(0, 1 / n_components).</source>
          <target state="translated">ランダム行列の成分は N(0,1/n_components)から引き出されます。</target>
        </trans-unit>
        <trans-unit id="4d179dd17ec2e9e8ac79ee8fb26a8b8727033655" translate="yes" xml:space="preserve">
          <source>The compromise between l1 and l2 penalization chosen by cross validation</source>
          <target state="translated">クロスバリデーションによって選択されたl1とl2のペナルティの間の妥協点</target>
        </trans-unit>
        <trans-unit id="37c8e8aa42af430070b2b87be4c5db5db294cc7b" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;fit&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 中の計算は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="cc728704a5b1deaa221b61ada3dd42ad8e661fbd" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;predict&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;predict&lt;/code&gt; 中の計算は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="cc5659751ed96e6a9a511b247760eba7bd9fab63" translate="yes" xml:space="preserve">
          <source>The computation of Davies-Bouldin is simpler than that of Silhouette scores.</source>
          <target state="translated">ダヴィース=ボルダンの計算は、シルエットのスコアよりも簡単です。</target>
        </trans-unit>
        <trans-unit id="d526bd934b93818cdf8a3fccada8e6b2b34d84f2" translate="yes" xml:space="preserve">
          <source>The computational overhead of each SVD is &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt;, but only 2 * batch_size samples remain in memory at a time. There will be &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD computations to get the principal components, versus 1 large SVD of complexity &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; for PCA.</source>
          <target state="translated">各SVDの計算オーバーヘッドは &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt; ですが、一度にメモリに残るのは2 * batch_sizeサンプルのみです。PCAの場合、複雑さ &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; 1つの大きなSVDに対して、主成分を取得するための &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD計算があります。</target>
        </trans-unit>
        <trans-unit id="2fa163515de812b4d78a2bbcef87e239a36ae4d1" translate="yes" xml:space="preserve">
          <source>The concept of early stopping is simple. We specify a &lt;code&gt;validation_fraction&lt;/code&gt; which denotes the fraction of the whole dataset that will be kept aside from training to assess the validation loss of the model. The gradient boosting model is trained using the training set and evaluated using the validation set. When each additional stage of regression tree is added, the validation set is used to score the model. This is continued until the scores of the model in the last &lt;code&gt;n_iter_no_change&lt;/code&gt; stages do not improve by atleast &lt;code&gt;tol&lt;/code&gt;. After that the model is considered to have converged and further addition of stages is &amp;ldquo;stopped early&amp;rdquo;.</source>
          <target state="translated">早期停止のコンセプトは簡単です。モデルの検証損失を評価するためのトレーニングは別として、データセット全体の割合を示す &lt;code&gt;validation_fraction&lt;/code&gt; を指定します。勾配ブースティングモデルは、トレーニングセットを使用してトレーニングされ、検証セットを使用して評価されます。回帰ツリーの各ステージが追加されると、検証セットを使用してモデルにスコアが付けられます。これは、最後の &lt;code&gt;n_iter_no_change&lt;/code&gt; ステージでのモデルのスコアが少なくとも &lt;code&gt;tol&lt;/code&gt; によって改善されなくなるまで継続されます。その後、モデルは収束したと見なされ、ステージの追加は「早期に停止」されます。</target>
        </trans-unit>
        <trans-unit id="feb60f640c62fd0856cb50e8401267d4f55d4774" translate="yes" xml:space="preserve">
          <source>The concrete &lt;code&gt;LossFunction&lt;/code&gt; object.</source>
          <target state="translated">具体的な &lt;code&gt;LossFunction&lt;/code&gt; オブジェクト。</target>
        </trans-unit>
        <trans-unit id="4f21e1340272b60cf06a69153cc19d0140d625cf" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">具体的な損失関数は、 &lt;code&gt;loss&lt;/code&gt; パラメーターを介して設定できます。&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;は、次の損失関数をサポートしています。</target>
        </trans-unit>
        <trans-unit id="c24a50928ec0729629e45e8b53b6d8b9f0112f79" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">具体的な損失関数は、 &lt;code&gt;loss&lt;/code&gt; パラメーターを介して設定できます。&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;は、次の損失関数をサポートしています。</target>
        </trans-unit>
        <trans-unit id="a5b98bb304bc3ec6a422167859a2f5da5580fa47" translate="yes" xml:space="preserve">
          <source>The concrete penalty can be set via the &lt;code&gt;penalty&lt;/code&gt; parameter. SGD supports the following penalties:</source>
          <target state="translated">具体的なペナルティは、 &lt;code&gt;penalty&lt;/code&gt; パラメータを介して設定できます。SGDは次のペナルティをサポートしています。</target>
        </trans-unit>
        <trans-unit id="c0c66ccf788e88932593e02308e1dcae0bb5c1f0" translate="yes" xml:space="preserve">
          <source>The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:</source>
          <target state="translated">各ユニットの条件付き確率分布は、それが受け取る入力のロジスティック・シグモイド活性化関数で与えられる。</target>
        </trans-unit>
        <trans-unit id="6e94f7f28c2cc9bab8c93a85a4438b802d82d10d" translate="yes" xml:space="preserve">
          <source>The confidence score for a sample is the signed distance of that sample to the hyperplane.</source>
          <target state="translated">標本の信頼度スコアは、その標本の超平面までの符号付き距離です。</target>
        </trans-unit>
        <trans-unit id="b129f98741f30e6ae05e0872d86b02b5a921e905" translate="yes" xml:space="preserve">
          <source>The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;&lt;/a&gt; to restrict merging to nearest neighbors as in &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;this example&lt;/a&gt;, or using &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt;&lt;/a&gt; to enable only merging of neighboring pixels on an image, as in the &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;coin&lt;/a&gt; example.</source>
          <target state="translated">接続性制約は、接続性マトリックスを介して課せられます。接続するデータセットのインデックスを持つ行と列の交点にのみ要素を持つscipyスパースマトリックス。このマトリックスは、事前情報から構築できます。たとえば、あるページから別のページにリンクしているページをマージするだけで、Webページをクラスター化したい場合があります。また、データから学習することもできます。&lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;たとえば&lt;/a&gt;、この例のように&lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; &lt;/a&gt;を使用してマージを最近傍に制限するか、&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt; &lt;/a&gt;ようにsklearn.feature_extraction.image.grid_to_graphを使用して画像上の隣接ピクセルのみのマージを有効にします。&lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;コイン&lt;/a&gt;例。</target>
        </trans-unit>
        <trans-unit id="cba08514635a6da99fcbf046720afd85a65680ea" translate="yes" xml:space="preserve">
          <source>The constant value which defines the covariance: k(x_1, x_2) = constant_value</source>
          <target state="translated">共分散を定義する定数値:k(x_1,x_2)=constant_value</target>
        </trans-unit>
        <trans-unit id="83836055dd86bf18d0180179c00f49511034950c" translate="yes" xml:space="preserve">
          <source>The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings.</source>
          <target state="translated">計算された分割表は、典型的には、2つのクラスタリング間の類似度統計量(この文書に記載されている他のものと同様)の計算に利用される。</target>
        </trans-unit>
        <trans-unit id="ca0505a0687635a00a190438e7d26956e48c468c" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</source>
          <target state="translated">収束のしきい値。下界平均利得がこの閾値を下回るとEMの反復は停止します。</target>
        </trans-unit>
        <trans-unit id="0e26ef32f94bda64e8f6e396c3c303a3311ac4e1" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold.</source>
          <target state="translated">収束閾値。(モデルに対する訓練データの)尤度の下界平均利得がこの閾値を下回るとEMの反復は停止する.</target>
        </trans-unit>
        <trans-unit id="80f05388942910150044c6d47584d8bddc73ef4c" translate="yes" xml:space="preserve">
          <source>The converse mapping from feature name to column index is stored in the &lt;code&gt;vocabulary_&lt;/code&gt; attribute of the vectorizer:</source>
          <target state="translated">機能名から列インデックスへの逆マッピングは、ベクトライザーの &lt;code&gt;vocabulary_&lt;/code&gt; 属性に格納されます。</target>
        </trans-unit>
        <trans-unit id="42e30f7491cd0bbd1be9eaa1008d97761b319a5f" translate="yes" xml:space="preserve">
          <source>The converted and validated X.</source>
          <target state="translated">変換されて検証されたX。</target>
        </trans-unit>
        <trans-unit id="f4a00aeed47e5a0cd669edcdec893bcad4bc87b6" translate="yes" xml:space="preserve">
          <source>The converted and validated array.</source>
          <target state="translated">変換されて検証された配列。</target>
        </trans-unit>
        <trans-unit id="043f2ec629cc510b9c1127fe6b10b6fe4448d241" translate="yes" xml:space="preserve">
          <source>The converted and validated y.</source>
          <target state="translated">変換されて検証されたy。</target>
        </trans-unit>
        <trans-unit id="81a7bc326e697c66d8802c043c85fabeabbf241b" translate="yes" xml:space="preserve">
          <source>The converted dataname.</source>
          <target state="translated">変換されたデータ名。</target>
        </trans-unit>
        <trans-unit id="223ada656bcdd3fee604b90acc8ae4baf52723e0" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is</source>
          <target state="translated">UCI ML Breast Cancer Wisconsin (Diagnostic)データセットのコピーは以下の通りです。</target>
        </trans-unit>
        <trans-unit id="44e5d228f37bd8405cfefcfa348ce3d27d939cb4" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit</source>
          <target state="translated">UCI MLワインデータセットデータセットのコピーをダウンロードし、適合するように修正しました。</target>
        </trans-unit>
        <trans-unit id="e11d4d7608fe1fa28c8a673b1389ac6181cb7877" translate="yes" xml:space="preserve">
          <source>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights \(w_1\), \(w_2\), &amp;hellip;, \(w_N\) to each of the training samples. Initially, those weights are all set to \(w_i = 1/N\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence &lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;.</source>
          <target state="translated">AdaBoostの中核となる原則は、一連の弱学習器（つまり、小さな決定木などのランダムな推測よりもほんの少しだけ優れているモデル）を繰り返し修正されたデータのバージョンに当てはめることです。それらすべての予測は、加重多数決（または合計）を介して結合され、最終的な予測が生成されます。いわゆるブースティング反復ごとのデータ変更は、各トレーニングサンプルに重み\（w_1 \）、\（w_2 \）、&amp;hellip;、\（w_N \）を適用することで構成されます。最初は、これらの重みはすべて\（w_i = 1 / N \）に設定されているため、最初のステップでは単純に元のデータで弱学習器をトレーニングします。連続する反復ごとに、サンプルの重みが個別に変更され、再重み付けされたデータに学習アルゴリズムが再適用されます。特定のステップで、前のステップで生成されたブーストされたモデルによって誤って予測されたトレーニング例は、重みが増加しますが、正しく予測されたものは減少します。反復が進むにつれて、予測が困難な例は、ますます大きな影響を受けます。これにより、後続の各弱学習器は、シーケンス内の前の学習器で見逃された例に集中することを強いられます&lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2a94b871b5efce504e5077120d25fb83e224b7ee" translate="yes" xml:space="preserve">
          <source>The corpus is a collection of \(D\) documents.</source>
          <target state="translated">コーパスは、ドキュメントの集合体である。</target>
        </trans-unit>
        <trans-unit id="81757ae28a30627b83ded3cb76d108f655541caf" translate="yes" xml:space="preserve">
          <source>The correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y)).</source>
          <target state="translated">各回帰子と目標の間の相関が計算され,すなわち,((X[:,i]-平均(X[:,i]))*(y-mean_y)/(std(X[:,i])*std(y))。</target>
        </trans-unit>
        <trans-unit id="302d596f93db30183d7b9abc1999d9cfb95f2d1f" translate="yes" xml:space="preserve">
          <source>The corresponding image is:</source>
          <target state="translated">対応する画像は</target>
        </trans-unit>
        <trans-unit id="9a9313289c48f29b6d60eb0302d43d099de5f93d" translate="yes" xml:space="preserve">
          <source>The corresponding training labels.</source>
          <target state="translated">対応する訓練ラベル。</target>
        </trans-unit>
        <trans-unit id="8f6395cc147644bb3051e08a46acbc9cd47145af" translate="yes" xml:space="preserve">
          <source>The cosine distance is defined as &lt;code&gt;1 - cosine_similarity&lt;/code&gt;: the lowest value is 0 (identical point) but it is bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only on their relative angles.</source>
          <target state="translated">コサイン距離は &lt;code&gt;1 - cosine_similarity&lt;/code&gt; として定義されます。最も低い値は0（同一のポイント）ですが、最も遠いポイントの場合、上限は2になります。その値は、ベクトルポイントのノルムには依存せず、相対角度にのみ依存します。</target>
        </trans-unit>
        <trans-unit id="c4b95b481d689de7dde58522ad60a16deab9f841" translate="yes" xml:space="preserve">
          <source>The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm</source>
          <target state="translated">余弦距離は、各サンプルを単位ノルムに正規化した場合の2乗ユークリッド距離の半分に相当します。</target>
        </trans-unit>
        <trans-unit id="59fadedc84e4bf70f9182651c0ff609a02cebdec" translate="yes" xml:space="preserve">
          <source>The cost complexity measure of a single node is \(R_\alpha(t)=R(t)+\alpha\). The branch, \(T_t\), is defined to be a tree where node \(t\) is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, \(R(T_t)&amp;lt;R(t)\). However, the cost complexity measure of a node, \(t\), and its branch, \(T_t\), can be equal depending on \(\alpha\). We define the effective \(\alpha\) of a node to be the value where they are equal, \(R_\alpha(T_t)=R_\alpha(t)\) or \(\alpha_{eff}(t)=\frac{R(t)-R(T_t)}{|T|-1}\). A non-terminal node with the smallest value of \(\alpha_{eff}\) is the weakest link and will be pruned. This process stops when the pruned tree&amp;rsquo;s minimal \(\alpha_{eff}\) is greater than the &lt;code&gt;ccp_alpha&lt;/code&gt; parameter.</source>
          <target state="translated">単一ノードのコストの複雑さの尺度は\（R_ \ alpha（t）= R（t）+ \ alpha \）です。ブランチ\（T_t \）は、ノード\（t \）がそのルートであるツリーとして定義されます。一般に、ノードの不純物は、その末端ノードの不純物の合計\（R（T_t）&amp;lt;R（t）\）よりも大きくなります。ただし、ノード\（t \）とそのブランチ\（T_t \）のコストの複雑さの尺度は、\（\ alpha \）に応じて等しくなる可能性があります。ノードの有効な\（\ alpha \）は、それらが等しい値、\（R_ \ alpha（T_t）= R_ \ alpha（t）\）または\（\ alpha_ {eff}（t））と定義します。 = \ frac {R（t）-R（T_t）} {| T | -1} \）。 \（\ alpha_ {eff} \）の値が最小の非終端ノードは最も弱いリンクであり、プルーニングされます。このプロセスは、プルーニングされたツリーの最小\（\ alpha_ {eff} \）が &lt;code&gt;ccp_alpha&lt;/code&gt; パラメーターよりも大きい場合に停止します。</target>
        </trans-unit>
        <trans-unit id="de99d6d126cf03f253c85bcda8fea4fdd47045eb" translate="yes" xml:space="preserve">
          <source>The cost function of an isomap embedding is</source>
          <target state="translated">アイソマップ埋め込みのコスト関数は</target>
        </trans-unit>
        <trans-unit id="40200d49debb9a752670d9f4e91dc77cd9d66f0a" translate="yes" xml:space="preserve">
          <source>The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.</source>
          <target state="translated">木を使用するコスト(すなわち、データを予測するコスト)は、木を訓練するために使用されるデータポイントの数で対数的になります。</target>
        </trans-unit>
        <trans-unit id="c9b6813659bffb0c08aeb974fcecdaaf97e786b0" translate="yes" xml:space="preserve">
          <source>The covariance matrix of a data set is known to be well approximated by the classical &lt;em&gt;maximum likelihood estimator&lt;/em&gt; (or &amp;ldquo;empirical covariance&amp;rdquo;), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population&amp;rsquo;s covariance matrix.</source>
          <target state="translated">データセットの共分散行列は、観測の数が特徴（観測を記述する変数）の数と比較して十分に大きい&lt;em&gt;場合&lt;/em&gt;、古典的な&lt;em&gt;最尤推定量&lt;/em&gt;（または「経験的共分散」）によってよく近似されることが知られています。より正確には、サンプルの最尤推定量は、対応する母集団の共分散行列の不偏推定量です。</target>
        </trans-unit>
        <trans-unit id="ff9747571dfcab155502f0d51c20ced0a4eda87c" translate="yes" xml:space="preserve">
          <source>The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions.</source>
          <target state="translated">共分散行列は,この値に単位行列をかけたものである.このデータセットは,対称正規分布のみを生成する.</target>
        </trans-unit>
        <trans-unit id="d4f25f1f739fb6455b136c8d6d48a91032e89970" translate="yes" xml:space="preserve">
          <source>The covariance of each mixture component. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">各混合コンポーネントの共分散。形状は &lt;code&gt;covariance_type&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="37e0aa90ea88b46d75d1f0f3b55b29aae27815f8" translate="yes" xml:space="preserve">
          <source>The covariance to compare with.</source>
          <target state="translated">比較する共分散。</target>
        </trans-unit>
        <trans-unit id="a8664fdb20de58ef24fb2b254f60a073e2dad9f4" translate="yes" xml:space="preserve">
          <source>The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the canonical correlation analysis (CCA).</source>
          <target state="translated">クロス分解モジュールは、部分最小二乗法(PLS)と正準相関分析(CCA)の2つの主要なアルゴリズムを含んでいます。</target>
        </trans-unit>
        <trans-unit id="5725093c72b0120c682574584b056d0b295efdd9" translate="yes" xml:space="preserve">
          <source>The cross validation score obtained on the training data</source>
          <target state="translated">学習データに対して得られたクロスバリデーションスコア</target>
        </trans-unit>
        <trans-unit id="7d9ee6d7f0b44964f013a45e604e6c29cc8a3f8f" translate="yes" xml:space="preserve">
          <source>The cross-validation can then be performed easily:</source>
          <target state="translated">その後、クロスバリデーションを簡単に行うことができます。</target>
        </trans-unit>
        <trans-unit id="58fa7e854fcd94e774f9d9c000c917c76d680086" translate="yes" xml:space="preserve">
          <source>The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores:</source>
          <target state="translated">Plattスケーリングに伴うクロスバリデーションは、大規模なデータセットでは高価な作業である。さらに、確率推定値はスコアと矛盾することがある。</target>
        </trans-unit>
        <trans-unit id="b91dc81955b1dbc1a750c7c0ae1e516f1cb5789e" translate="yes" xml:space="preserve">
          <source>The cross-validation score can be directly calculated using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper. Given an estimator, the cross-validation object and the input dataset, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</source>
          <target state="translated">交差検証スコアは、&lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;ヘルパーを使用して直接計算できます。推定器、交差検証オブジェクト、および入力データセットを指定すると、&lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;はデータを繰り返しトレーニングとテストセットに分割し、トレーニングセットを使用して推定器をトレーニングし、クロスの各反復のテストセットに基づいてスコアを計算します検証。</target>
        </trans-unit>
        <trans-unit id="0c9e67494c034c3aceceb77302443d5c2bb54301" translate="yes" xml:space="preserve">
          <source>The cross-validation scores such that &lt;code&gt;grid_scores_[i]&lt;/code&gt; corresponds to the CV score of the i-th subset of features.</source>
          <target state="translated">&lt;code&gt;grid_scores_[i]&lt;/code&gt; が特徴のi番目のサブセットのCVスコアに対応するような交差検証スコア。</target>
        </trans-unit>
        <trans-unit id="7186db76e260a93ceff6e7a012a18028bce16f5b" translate="yes" xml:space="preserve">
          <source>The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see &lt;code&gt;NearestNeighbors&lt;/code&gt;.</source>
          <target state="translated">現在の実装では、ボールツリーとkdツリーを使用してポイントの近傍を決定しているため、（0.14より前のscikit-learnバージョンで行われていた）完全な距離行列の計算が回避されます。カスタム指標を使用する可能性は維持されています。詳細については、 &lt;code&gt;NearestNeighbors&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="e3ec89890503565d6b8dae28b11789379133ea78" translate="yes" xml:space="preserve">
          <source>The current loss computed with the loss function.</source>
          <target state="translated">損失関数を用いて計算された現在の損失。</target>
        </trans-unit>
        <trans-unit id="a415ef6f77731ce51df5a94c5015135ebd3f462c" translate="yes" xml:space="preserve">
          <source>The curse of dimensionality</source>
          <target state="translated">次元の呪い</target>
        </trans-unit>
        <trans-unit id="3eaad00bc0bba0577db9b826b646317a24a90409" translate="yes" xml:space="preserve">
          <source>The data</source>
          <target state="translated">データ</target>
        </trans-unit>
        <trans-unit id="973622d96c176a0a77b0dde8f6175466c5afec2c" translate="yes" xml:space="preserve">
          <source>The data is always a 2D array, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt;, although the original data may have had a different shape. In the case of the digits, each original sample is an image of shape &lt;code&gt;(8, 8)&lt;/code&gt; and can be accessed using:</source>
          <target state="translated">データは常に2D配列の形状 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; ですが、元のデータは異なる形状であった可能性があります。数字の場合、各元のサンプルは形状 &lt;code&gt;(8, 8)&lt;/code&gt; 8、8）のイメージであり、次を使用してアクセスできます。</target>
        </trans-unit>
        <trans-unit id="0e6af14908ee7619e613da95e5ef5e8434ef93d9" translate="yes" xml:space="preserve">
          <source>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.</source>
          <target state="translated">データは非負であると仮定され、しばしばL1ノルムが1になるように正規化される。正規化は,離散確率分布間の距離であるカイ2乗距離との関係で合理化される.</target>
        </trans-unit>
        <trans-unit id="aa2161d468d436ce31e1b1a4a535d19d4acf7995" translate="yes" xml:space="preserve">
          <source>The data is generated with the &lt;code&gt;make_checkerboard&lt;/code&gt; function, then shuffled and passed to the Spectral Biclustering algorithm. The rows and columns of the shuffled matrix are rearranged to show the biclusters found by the algorithm.</source>
          <target state="translated">データは &lt;code&gt;make_checkerboard&lt;/code&gt; 関数で生成され、シャッフルされてスペクトルバイクラスタリングアルゴリズムに渡されます。シャッフルされた行列の行と列が再配置され、アルゴリズムによって検出された双クラスターが表示されます。</target>
        </trans-unit>
        <trans-unit id="cbdb41e13849fa39e43a0c0831b005495a0d0342" translate="yes" xml:space="preserve">
          <source>The data is split according to the cv parameter. Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.</source>
          <target state="translated">データは,cvパラメータに従って分割されます.各サンプルは,ちょうど1つの試験集合に属し,その予測は,対応する訓練集合に適合した推定器を用いて計算されます.</target>
        </trans-unit>
        <trans-unit id="549dd87c3787b6b83f2456e4cff9d8aa392d12c1" translate="yes" xml:space="preserve">
          <source>The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.</source>
          <target state="translated">このデータは、イタリアの同じ地域で3つの異なる栽培者によって栽培されたワインの化学分析の結果です。3種類のワインに含まれる異なる成分について13種類の測定が行われています。</target>
        </trans-unit>
        <trans-unit id="92b1cf272aa9964f4bcd0a98099303474f1eb6db" translate="yes" xml:space="preserve">
          <source>The data list to learn.</source>
          <target state="translated">学ぶべきデータリスト。</target>
        </trans-unit>
        <trans-unit id="279fbf7584e00e713412e8ff4546300536ffeb5a" translate="yes" xml:space="preserve">
          <source>The data matrix</source>
          <target state="translated">データマトリクス</target>
        </trans-unit>
        <trans-unit id="8a93d3171d08360f1c2b565ec2f68ceaa0cb40fb" translate="yes" xml:space="preserve">
          <source>The data matrix to learn.</source>
          <target state="translated">学ぶためのデータマトリックス。</target>
        </trans-unit>
        <trans-unit id="a36ea8c8057a9ae699a69f099ee316c3295afaf3" translate="yes" xml:space="preserve">
          <source>The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.</source>
          <target state="translated">p個の特徴量とn個のサンプルを持つデータ行列.データセットは,生の推定値を計算するのに使用されたものでなければなりません.</target>
        </trans-unit>
        <trans-unit id="f0ed1480aeed96966c83d245c0e82839723677ea" translate="yes" xml:space="preserve">
          <source>The data matrix.</source>
          <target state="translated">データマトリクスです。</target>
        </trans-unit>
        <trans-unit id="3e01104c886db328397aa7550432fcf04c711803" translate="yes" xml:space="preserve">
          <source>The data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="translated">データマトリックス。 &lt;code&gt;as_frame=True&lt;/code&gt; の場合、 &lt;code&gt;data&lt;/code&gt; はpandasDataFrameになります。</target>
        </trans-unit>
        <trans-unit id="70cee63c42777050ee29bb5c41f5ed813382b6c9" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is int</source>
          <target state="translated">返される疎な行列のデータ.デフォルトでは int</target>
        </trans-unit>
        <trans-unit id="13b8870b16632bb144f99a987da0a17b912fd261" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is the dtype of img</source>
          <target state="translated">返される疎な行列のデータ。デフォルトでは img</target>
        </trans-unit>
        <trans-unit id="a9c1f6d9c961581c21421066f6ce8f7c709084a9" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained.</source>
          <target state="translated">&lt;code&gt;gbrt&lt;/code&gt; がトレーニングされたデータ。</target>
        </trans-unit>
        <trans-unit id="c66d20ce5405cff4e02a00c321afd4016f52379a" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained. It is used to generate a &lt;code&gt;grid&lt;/code&gt; for the &lt;code&gt;target_variables&lt;/code&gt;. The &lt;code&gt;grid&lt;/code&gt; comprises &lt;code&gt;grid_resolution&lt;/code&gt; equally spaced points between the two &lt;code&gt;percentiles&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;gbrt&lt;/code&gt; がトレーニングされたデータ。 &lt;code&gt;target_variables&lt;/code&gt; の &lt;code&gt;grid&lt;/code&gt; を生成するために使用されます。 &lt;code&gt;grid&lt;/code&gt; 含むが &lt;code&gt;grid_resolution&lt;/code&gt; 両者の等間隔の点 &lt;code&gt;percentiles&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2ffaf51af185adce5e53447e39433481435abf03" translate="yes" xml:space="preserve">
          <source>The data samples transformed.</source>
          <target state="translated">変換されたデータサンプル。</target>
        </trans-unit>
        <trans-unit id="65c0e8f3b46fedbf2f41b7e1589997d8de30db77" translate="yes" xml:space="preserve">
          <source>The data set contains images of hand-written digits: 10 classes where each class refers to a digit.</source>
          <target state="translated">データセットには、手書きの数字の画像が含まれています。各クラスは1桁の数字を参照しています。</target>
        </trans-unit>
        <trans-unit id="a49ccf8db4843f7af3fe75d2e385d4c5ae4a247b" translate="yes" xml:space="preserve">
          <source>The data that should be scaled.</source>
          <target state="translated">秤にかけるべきデータ。</target>
        </trans-unit>
        <trans-unit id="4362ce6dec3d09ef8de03aa7af16c30845dd1b85" translate="yes" xml:space="preserve">
          <source>The data that should be transformed back.</source>
          <target state="translated">変換して戻すべきデータ。</target>
        </trans-unit>
        <trans-unit id="47ba75ee664e21a51401b4c1203a08e7a876f44e" translate="yes" xml:space="preserve">
          <source>The data to be transformed by subset.</source>
          <target state="translated">サブセットで変換するデータ。</target>
        </trans-unit>
        <trans-unit id="03bb048cfbbc3212fd60edf266a3822d3176ef87" translate="yes" xml:space="preserve">
          <source>The data to be transformed using a power transformation.</source>
          <target state="translated">電力変換を用いて変換するデータ。</target>
        </trans-unit>
        <trans-unit id="73ce115221fb870e05c90b8f043ca278fdee74a1" translate="yes" xml:space="preserve">
          <source>The data to be transformed.</source>
          <target state="translated">変換するデータです。</target>
        </trans-unit>
        <trans-unit id="df7f442215a9627450351a5239ccb7b837681c69" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse matricesは,不要なコピーを避けるためにCSR形式でなければなりません.</target>
        </trans-unit>
        <trans-unit id="5401bf0fdb954957c9a3f345344e508dbd6bac54" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse matricesは,不要なコピーを避けるためにCSRまたはCSC形式でなければなりません.</target>
        </trans-unit>
        <trans-unit id="515108ae5aad51ada5b234ccff65d93cdd42711e" translate="yes" xml:space="preserve">
          <source>The data to center and scale.</source>
          <target state="translated">センタリングしてスケールするデータ。</target>
        </trans-unit>
        <trans-unit id="975edd1be086184330a8a3ebbf935588408d4a98" translate="yes" xml:space="preserve">
          <source>The data to determine the categories of each feature.</source>
          <target state="translated">各特徴のカテゴリを決定するためのデータです。</target>
        </trans-unit>
        <trans-unit id="a77a7e1c81043cc97f700ad0d213253f486b563a" translate="yes" xml:space="preserve">
          <source>The data to encode.</source>
          <target state="translated">エンコードするデータです。</target>
        </trans-unit>
        <trans-unit id="d843c1d7e5e8986d5ad23251cff2a94ef8ee7955" translate="yes" xml:space="preserve">
          <source>The data to fit.</source>
          <target state="translated">フィットするデータ。</target>
        </trans-unit>
        <trans-unit id="e7fbe72ba2fcf8fff679b5e28b0fef9589e7589d" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be for example a list, or an array.</source>
          <target state="translated">合わせたいデータ。リストや配列などにすることができます。</target>
        </trans-unit>
        <trans-unit id="00725d5de44209593a99040fefb16433276154c2" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be, for example a list, or an array at least 2d.</source>
          <target state="translated">フィットするデータ。例えば、リストや少なくとも2次元の配列などが考えられます。</target>
        </trans-unit>
        <trans-unit id="53846ac28d489b02b9949ef562a942f522e52d48" translate="yes" xml:space="preserve">
          <source>The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse行列は,不要なコピーを避けるためにCSR形式でなければなりません.</target>
        </trans-unit>
        <trans-unit id="c13e1ef3ee3dcba79bb567a64bd572c32814ce01" translate="yes" xml:space="preserve">
          <source>The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse matricesは,不要なコピーを避けるためにCSR形式でなければなりません.</target>
        </trans-unit>
        <trans-unit id="89435a85b63a4550536a3843494521cdfd812011" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row.</source>
          <target state="translated">変換するデータを一行ずつ</target>
        </trans-unit>
        <trans-unit id="a092ffb0fb8aec81b90c0802c73b2ae6cfa80dcc" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row. Sparse input should preferably be in CSC format.</source>
          <target state="translated">変換するデータを行単位で指定します。疎な入力はCSCフォーマットであることが望ましい。</target>
        </trans-unit>
        <trans-unit id="3f0555e5cf2c3f2eb80a653fe1f67911e223ec90" translate="yes" xml:space="preserve">
          <source>The data to transform.</source>
          <target state="translated">変換するデータです。</target>
        </trans-unit>
        <trans-unit id="f0a016c9443bbddef85796fd3691360a3c817ea0" translate="yes" xml:space="preserve">
          <source>The data used to compute the mean and standard deviation used for later scaling along the features axis.</source>
          <target state="translated">特徴軸に沿った後のスケーリングに使用される平均と標準偏差を計算するために使用されるデータ。</target>
        </trans-unit>
        <trans-unit id="55438370e51e74e9e8aa5e6ed6a37b2c111bd898" translate="yes" xml:space="preserve">
          <source>The data used to compute the median and quantiles used for later scaling along the features axis.</source>
          <target state="translated">特徴軸に沿った後のスケーリングに使用される中央値と定量値の計算に使用されたデータ。</target>
        </trans-unit>
        <trans-unit id="0ae3bec0fd19bdece5355d54cf80d1ae7bcc7c26" translate="yes" xml:space="preserve">
          <source>The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</source>
          <target state="translated">フィーチャー軸に沿った後のスケーリングに使用されるフィーチャーごとの最小値と最大値を計算するために使用されるデータ。</target>
        </trans-unit>
        <trans-unit id="e26762b4f27727d3210d3fe2a64ad24b366c1062" translate="yes" xml:space="preserve">
          <source>The data used to estimate the optimal transformation parameters.</source>
          <target state="translated">最適な変換パラメータを推定するために使用されるデータ。</target>
        </trans-unit>
        <trans-unit id="56c320de81ba4f246c360453cbd2da4e63d63c7f" translate="yes" xml:space="preserve">
          <source>The data used to fit the model. If &lt;code&gt;copy_X=False&lt;/code&gt;, then &lt;code&gt;X_fit_&lt;/code&gt; is a reference. This attribute is used for the calls to transform.</source>
          <target state="translated">モデルの近似に使用されるデータ。場合 &lt;code&gt;copy_X=False&lt;/code&gt; 、その後 &lt;code&gt;X_fit_&lt;/code&gt; は参照です。この属性は、変換の呼び出しに使用されます。</target>
        </trans-unit>
        <trans-unit id="03376550c822ad450e2d14840686210c4804ebc6" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis.</source>
          <target state="translated">フィーチャー軸に沿ってスケールするために使用されるデータです。</target>
        </trans-unit>
        <trans-unit id="c4374e3fd9ee0863ad791a18672a80272e651c9f" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;. Additionally, the sparse matrix needs to be nonnegative if &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; is False.</source>
          <target state="translated">フィーチャ軸に沿ってスケーリングするために使用されるデータ。スパース行列が指定されている場合は、スパース &lt;code&gt;csc_matrix&lt;/code&gt; に変換されます。さらに、 &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; がFalseの場合、スパース行列は負でない必要があります。</target>
        </trans-unit>
        <trans-unit id="848647eb355f23af31ef1bf06cbb1e0d945eea47" translate="yes" xml:space="preserve">
          <source>The data used to scale along the specified axis.</source>
          <target state="translated">指定された軸に沿ってスケールするために使用されるデータです。</target>
        </trans-unit>
        <trans-unit id="db728424571c6d0aa73c940cbe613870747b4067" translate="yes" xml:space="preserve">
          <source>The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)</source>
          <target state="translated">様々な分類器を比較するために、他の多くのデータと一緒に使用した。RDAだけが100%正しい分類を達成していますが、クラスは分離可能です。(RDA:100%、QDA 99.4%、LDA 98.9%、1NN 96.1% (z変換データ))(すべての結果は、リーブ・オン・アウト技法を使用しています)</target>
        </trans-unit>
        <trans-unit id="660fe59aa4f1107f7d968ac4c0471ddbff4317b5" translate="yes" xml:space="preserve">
          <source>The data.</source>
          <target state="translated">データです。</target>
        </trans-unit>
        <trans-unit id="02bda070eb404181c9796aadeceacf78aac05356" translate="yes" xml:space="preserve">
          <source>The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a &lt;code&gt;sample_weight&lt;/code&gt; when fitting DBSCAN.</source>
          <target state="translated">データセットは、データで重複が発生した場合に完全に重複するものを削除するか、BIRCHを使用して圧縮できます。そうすると、多数のポイントに対して比較的少数の代理人しか存在しなくなります。その後、DBSCANをフィッティングするときに &lt;code&gt;sample_weight&lt;/code&gt; を指定できます。</target>
        </trans-unit>
        <trans-unit id="6bbac4d66b74c67e38ab2b8e6e075ffda05e6522" translate="yes" xml:space="preserve">
          <source>The dataset is called &amp;ldquo;Twenty Newsgroups&amp;rdquo;. Here is the official description, quoted from the &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;website&lt;/a&gt;:</source>
          <target state="translated">データセットは「20ニュースグループ」と呼ばれます。ここに公式の説明があり、&lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;ウェブサイト&lt;/a&gt;から引用されています：</target>
        </trans-unit>
        <trans-unit id="c4573e45f21f00151e7512eaf1b6ae1cd41e22bd" translate="yes" xml:space="preserve">
          <source>The dataset is from Zhu et al [1].</source>
          <target state="translated">データセットはZhu et al [1]からのものです。</target>
        </trans-unit>
        <trans-unit id="1a602d6b3832c4acaa760453be6cb4d26914f03f" translate="yes" xml:space="preserve">
          <source>The dataset is generated using the &lt;code&gt;make_biclusters&lt;/code&gt; function, which creates a matrix of small values and implants bicluster with large values. The rows and columns are then shuffled and passed to the Spectral Co-Clustering algorithm. Rearranging the shuffled matrix to make biclusters contiguous shows how accurately the algorithm found the biclusters.</source>
          <target state="translated">データセットは、 &lt;code&gt;make_biclusters&lt;/code&gt; 関数を使用して生成されます。関数make_biclustersは、小さな値の行列を作成し、大きな値でbiclusterを埋め込みます。次に、行と列がシャッフルされ、スペクトル共クラスタリングアルゴリズムに渡されます。シャッフルされた行列を再配置してバイクラスターを隣接させると、アルゴリズムがどのように正確にバイクラスターを見つけたかがわかります。</target>
        </trans-unit>
        <trans-unit id="4131a4e690502c19383d02f0465052ba8df886e1" translate="yes" xml:space="preserve">
          <source>The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">データセットは、インデックス順で近くのポイントがパラメーター空間で近くなるように構造化され、K最近傍のほぼブロック対角行列になります。このような疎なグラフは、教師なし学習のためにポイント間の空間関係を利用するさまざまな状況で&lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt; &lt;/a&gt;ます。特に、&lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt;、およびsklearn.cluster.SpectralClusteringを参照してください。</target>
        </trans-unit>
        <trans-unit id="774649dea26c780c5e43d2464445b85ef137fcca" translate="yes" xml:space="preserve">
          <source>The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classification).</source>
          <target state="translated">データセットは、回帰(resp.分類)のためのBoston Housingデータセット(resp.20 Newsgroups)です。</target>
        </trans-unit>
        <trans-unit id="c0f7ef482ab49522d3cd87d64587ff804f6727c1" translate="yes" xml:space="preserve">
          <source>The dataset used for evaluation is a 2D grid of isotropic Gaussian clusters widely spaced.</source>
          <target state="translated">評価に使用したデータセットは、等方性ガウスクラスターを広く間隔をおいて配置した2次元グリッドです。</target>
        </trans-unit>
        <trans-unit id="c07599d3a55d8254ba468cc109830533370f5dcf" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is Reuters-21578 as provided by the UCI ML repository. It will be automatically downloaded and uncompressed on first run.</source>
          <target state="translated">この例で使用したデータセットは、UCI ML リポジトリから提供されている Reuters-21578 です。データセットは自動的にダウンロードされ、最初の実行時に圧縮されません。</target>
        </trans-unit>
        <trans-unit id="f610543312a82d7ead69951d61b82950d647ad3c" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, aka &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">この例で使用されているデータセットは、「野生のラベル付きの顔」、別名&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFWの&lt;/a&gt;前処理された抜粋です。</target>
        </trans-unit>
        <trans-unit id="257296081e1c4b1a90a4ff7a918d81d5746f6705" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, also known as &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">この例で使用されているデータセットは、&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;としても知られる「野生のラベル付き顔」の前処理された抜粋です。</target>
        </trans-unit>
        <trans-unit id="0e491b7a83df0aa734f40ae016e056d9d5ef3d92" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.</source>
          <target state="translated">この例で使用するデータセットは、自動的にダウンロードされ、キャッシュされて文書分類の例で再利用されるニュースグループのデータセットです。</target>
        </trans-unit>
        <trans-unit id="b391209f39032c4c37a7d267548501e64198b514" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.</source>
          <target state="translated">この例で使用するデータセットは、20 のニュースグループデータセットです。これは自動的にダウンロードされ、キャッシュされます。</target>
        </trans-unit>
        <trans-unit id="b6261e60f08853c79c52801fadd1554784335b15" translate="yes" xml:space="preserve">
          <source>The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).</source>
          <target state="translated">使用したデータセットは、UCIで利用可能なWine Datasetです。このデータセットは、測定する特性(アルコール含有量、リンゴ酸など)が異なるため、スケールが不均一な連続的な特徴を持っています。</target>
        </trans-unit>
        <trans-unit id="2320c3b81471635dac46a3209d55417f5e2427c2" translate="yes" xml:space="preserve">
          <source>The dataset will be downloaded from the &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 homepage&lt;/a&gt; if necessary. The compressed size is about 656 MB.</source>
          <target state="translated">データセットは、必要に応じて&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1ホームページ&lt;/a&gt;からダウンロードされます。圧縮サイズは約656 MBです。</target>
        </trans-unit>
        <trans-unit id="aadaaf4f9f002ac5fc13c03e41ad65b9ea025276" translate="yes" xml:space="preserve">
          <source>The dataset: wages</source>
          <target state="translated">データセット:賃金</target>
        </trans-unit>
        <trans-unit id="a104c196a5a6b4961911da3fe75edb0c176425f0" translate="yes" xml:space="preserve">
          <source>The datasets also contain a full description in their &lt;code&gt;DESCR&lt;/code&gt; attribute and some contain &lt;code&gt;feature_names&lt;/code&gt; and &lt;code&gt;target_names&lt;/code&gt;. See the dataset descriptions below for details.</source>
          <target state="translated">また、データセットの &lt;code&gt;DESCR&lt;/code&gt; 属性には完全な説明が含まれ、一部には &lt;code&gt;feature_names&lt;/code&gt; と &lt;code&gt;target_names&lt;/code&gt; が含まれています。詳細については、以下のデータセットの説明を参照してください。</target>
        </trans-unit>
        <trans-unit id="266ce3ccfa4a57091d07a7fe8bfc007064ff062e" translate="yes" xml:space="preserve">
          <source>The decision function computed the final estimator.</source>
          <target state="translated">決定関数は最終推定値を計算した。</target>
        </trans-unit>
        <trans-unit id="26c7542245412a79b0296d695d58227ed1491d25" translate="yes" xml:space="preserve">
          <source>The decision function is equal (up to a constant factor) to the log-posterior of the model, i.e. &lt;code&gt;log p(y = k | x)&lt;/code&gt;. In a binary classification setting this instead corresponds to the difference &lt;code&gt;log p(y = 1 | x) - log p(y = 0 | x)&lt;/code&gt;. See &lt;a href=&quot;../lda_qda#lda-qda-math&quot;&gt;Mathematical formulation of the LDA and QDA classifiers&lt;/a&gt;.</source>
          <target state="translated">決定関数は、モデルの対数後方、つまり &lt;code&gt;log p(y = k | x)&lt;/code&gt; 等しくなります（一定の係数まで）。二項分類設定では、これは代わりに差 &lt;code&gt;log p(y = 1 | x) - log p(y = 0 | x)&lt;/code&gt; ます。&lt;a href=&quot;../lda_qda#lda-qda-math&quot;&gt;LDAおよびQDA分類器の数学的定式化を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="f5da29d39d3005818dbe52be4adbd180f7105540" translate="yes" xml:space="preserve">
          <source>The decision function is:</source>
          <target state="translated">決定機能は</target>
        </trans-unit>
        <trans-unit id="fdcfe75a710515596cfa4af6113e103288190672" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">アンサンブルのツリーから予測された生の値に対応する、入力サンプルの決定関数。クラスは、属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;のクラスに対応します。回帰と二項分類は、 &lt;code&gt;k == 1&lt;/code&gt; 特殊なケースであり、それ以外の場合は &lt;code&gt;k==n_classes&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="13c22c73db1fc45ace2498b233349bae422df959" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">アンサンブルのツリーから予測された生の値に対応する、入力サンプルの決定関数。クラスの順序は、属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;の順序に対応しています。回帰と二項分類により、形状の配列[n_samples]が生成されます。</target>
        </trans-unit>
        <trans-unit id="c0e6f8aea5cc6c4291ed60eb8cccabc092233917" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The columns correspond to the classes in sorted order, as they appear in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">入力サンプルの決定関数。列は、属性 &lt;code&gt;classes_&lt;/code&gt; に表示されるとおり、ソートされた順序でクラスに対応します。回帰とバイナリ分類は &lt;code&gt;k == 1&lt;/code&gt; 特殊なケースであり、それ以外の場合は &lt;code&gt;k==n_classes&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="ae769935874da2534c2ccb6ca3e5794a84e4d439" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">入力サンプルの決定関数。出力の順序は、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;属性の順序と同じです。二項分類は、 &lt;code&gt;k == 1&lt;/code&gt; 特殊なケースであり、それ以外の場合は &lt;code&gt;k==n_classes&lt;/code&gt; です。二項分類の場合、-1または1に近い値は、それぞれ &lt;code&gt;classes_&lt;/code&gt; の1番目または2番目のクラスに似ていることを意味します。</target>
        </trans-unit>
        <trans-unit id="06ac27333a17bb0e5a70b7bdd7a4a5eafb52284d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">入力サンプルの決定関数。出力の順序は、 &lt;code&gt;classes_&lt;/code&gt; 属性の順序と同じです。バイナリ分類は &lt;code&gt;k == 1&lt;/code&gt; 特殊なケースであり、それ以外の場合は &lt;code&gt;k==n_classes&lt;/code&gt; です。バイナリ分類の場合、-1または1に近い値は、それぞれ &lt;code&gt;classes_&lt;/code&gt; の最初のクラスまたは2番目のクラスに似ています。</target>
        </trans-unit>
        <trans-unit id="7a79704b70e2487a7278def4173b216c7abdf67d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">入力サンプルの決定関数。クラスの順序は、属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;の順序に対応しています。回帰と二項分類により、形状の配列[n_samples]が生成されます。</target>
        </trans-unit>
        <trans-unit id="271144c81e33420b0281bbd1b4c29fbb1374c6ac" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">入力サンプルの決定関数。クラスの順序は、属性 &lt;code&gt;classes_&lt;/code&gt; の順序に対応しています。回帰とバイナリ分類は &lt;code&gt;k == 1&lt;/code&gt; 特殊なケースであり、それ以外の場合は &lt;code&gt;k==n_classes&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="c04fb9b1c64a374fbc2424dc1dc2e69edec92fae" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">入力サンプルの決定関数。クラスの順序は、属性 &lt;code&gt;classes_&lt;/code&gt; の順序に対応しています。回帰とバイナリ分類により、形状の配列[n_samples]が生成されます。</target>
        </trans-unit>
        <trans-unit id="7a4d1c2d06404b468f740213ed27426daa73066a" translate="yes" xml:space="preserve">
          <source>The decision rule for Bernoulli naive Bayes is based on</source>
          <target state="translated">ベルヌーイナイーブベイズの決定規則は</target>
        </trans-unit>
        <trans-unit id="5c243c13540ddb2ab9ba5e07515f5ccc5bddcdc4" translate="yes" xml:space="preserve">
          <source>The decision tree estimator to be exported. It can be an instance of DecisionTreeClassifier or DecisionTreeRegressor.</source>
          <target state="translated">エクスポートされる決定木推定子。これは、DecisionTreeClassifier または DecisionTreeRegressor のインスタンスになります。</target>
        </trans-unit>
        <trans-unit id="0e0b3ca71cdfb4dae3b0e05518ff98bb98da9e44" translate="yes" xml:space="preserve">
          <source>The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve:</source>
          <target state="translated">決定木構造を分析することで、特徴と予測対象との関係についての更なる洞察を得ることができる。この例では、取得する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="7930dea6ad7960124a06f25452c69dc408a6af03" translate="yes" xml:space="preserve">
          <source>The decision tree to be exported to GraphViz.</source>
          <target state="translated">GraphVizにエクスポートする決定木。</target>
        </trans-unit>
        <trans-unit id="096b758652a55c20db43d0b3794cde42f9ae935e" translate="yes" xml:space="preserve">
          <source>The decision tree to be plotted.</source>
          <target state="translated">プロットされる決定木。</target>
        </trans-unit>
        <trans-unit id="41d54cbac8ff4bb674faa62fe3a273da70a2c8ec" translate="yes" xml:space="preserve">
          <source>The decision values for the samples are computed by adding the normalized sum of pair-wise classification confidence levels to the votes in order to disambiguate between the decision values when the votes for all the classes are equal leading to a tie.</source>
          <target state="translated">サンプルの決定値は、すべてのクラスの投票が同点になるように等しい場合に、決定値の間の曖昧さをなくすために、投票にペアごとの分類信頼度の正規化された合計を加えることによって計算されます。</target>
        </trans-unit>
        <trans-unit id="9db4a0cde49703606f721a9d732403d53161167d" translate="yes" xml:space="preserve">
          <source>The decoding strategy depends on the vectorizer parameters.</source>
          <target state="translated">デコード戦略はベクタライザのパラメータに依存します。</target>
        </trans-unit>
        <trans-unit id="8ab5c7033c87e7cea9882c536950d8e60cf30550" translate="yes" xml:space="preserve">
          <source>The default coding of images is based on the &lt;code&gt;uint8&lt;/code&gt; dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; don&amp;rsquo;t forget to scale to the range 0 - 1 as done in the following example.</source>
          <target state="translated">画像のデフォルトのコーディングは、メモリを節約するための &lt;code&gt;uint8&lt;/code&gt; dtypeに基づいています。多くの場合、入力を最初に浮動小数点表現に変換すると、機械学習アルゴリズムが最適に機能します。また、 &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; を使用する場合は、次の例のように0〜1の範囲にスケーリングすることを忘れないでください。</target>
        </trans-unit>
        <trans-unit id="ec537a685ef4f8a1d5936e658cd9e5b93a584d77" translate="yes" xml:space="preserve">
          <source>The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:</source>
          <target state="translated">デフォルトの設定では、少なくとも2文字以上の単語を抽出して文字列をトークン化します。このステップを行う特定の関数を明示的に要求することができます。</target>
        </trans-unit>
        <trans-unit id="3d025c574ea3cc62ecf87d5b0a9f83d7b3c75d6a" translate="yes" xml:space="preserve">
          <source>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module for the list of possible cross-validation objects.</source>
          <target state="translated">使用されるデフォルトの交差検定ジェネレーターは、層別K折りです。整数が指定されている場合、それは使用される折りたたみの数です。可能な相互検証オブジェクトのリストについては、モジュール&lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt;モジュールを参照してください。</target>
        </trans-unit>
        <trans-unit id="24e256639376dfcfda93b74d1117680d7d248e59" translate="yes" xml:space="preserve">
          <source>The default dataset is the 20 newsgroups dataset. To run the example on the digits dataset, pass the &lt;code&gt;--use-digits-dataset&lt;/code&gt; command line argument to this script.</source>
          <target state="translated">デフォルトのデータセットは、20のニュースグループデータセットです。 &lt;code&gt;--use-digits-dataset&lt;/code&gt; データセットで例を実行するには、-use-digits-datasetコマンドライン引数をこのスクリプトに渡します。</target>
        </trans-unit>
        <trans-unit id="53050da8c5d49a141b5e2d80a70c0fa67fd7eb00" translate="yes" xml:space="preserve">
          <source>The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the &amp;ndash;twenty-newsgroups command line argument to this script.</source>
          <target state="translated">デフォルトのデータセットは数字データセットです。20のニュースグループデータセットで例を実行するには、このスクリプトに-twenty-newsgroupsコマンドライン引数を渡します。</target>
        </trans-unit>
        <trans-unit id="8018aa0e057c3b81a78860eae3f5892dde5e1c45" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this estimator.&amp;rdquo;</source>
          <target state="translated">デフォルトのエラーメッセージは、「この％（name）sインスタンスはまだ適合していません。この推定量を使用する前に、適切な引数を指定して「fit」を呼び出してください。」</target>
        </trans-unit>
        <trans-unit id="713044bd0fefe6c8ff38e7a00814412176e5caf0" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this method.&amp;rdquo;</source>
          <target state="translated">デフォルトのエラーメッセージは、「この％（name）sインスタンスはまだ適合していません。このメソッドを使用する前に、適切な引数で 'fit'を呼び出してください。」</target>
        </trans-unit>
        <trans-unit id="bd1a08c69ea4f3f536ea9f567759a2eca8958b5b" translate="yes" xml:space="preserve">
          <source>The default parameters (n_samples / n_features / n_components) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).</source>
          <target state="translated">デフォルトのパラメータ(n_samples/n_features/n_components)を使用すると、数十秒で実行可能な例ができます。問題の次元を大きくすることもできますが、時間の複雑さはNMFでは多項式であることに注意してください。LDAでは、時間の複雑さは(n_samples*iterations)に比例します。</target>
        </trans-unit>
        <trans-unit id="6bb3234d6819185e21b8b8e0778371ddb1ae1e23" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net &lt;a href=&quot;#id15&quot; id=&quot;id1&quot;&gt;11&lt;/a&gt; solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">デフォルト設定は &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; です。L1ペナルティはスパース解につながり、ほとんどの係数をゼロにします。Elastic Net &lt;a href=&quot;#id15&quot; id=&quot;id1&quot;&gt;11&lt;/a&gt;は、相関性の高い属性が存在する場合のL1ペナルティのいくつかの欠陥を解決します。パラメータ &lt;code&gt;l1_ratio&lt;/code&gt; は、L1とL2のペナルティの凸結合を制御します。</target>
        </trans-unit>
        <trans-unit id="e964bf4dcf7d784d0dee842886fe27c5060b1af1" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">デフォルト設定は &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; です。L1ペナルティはスパースソリューションにつながり、ほとんどの係数がゼロになります。Elastic Netは、相関性の高い属性が存在する場合のL1ペナルティのいくつかの欠点を解決します。パラメータ &lt;code&gt;l1_ratio&lt;/code&gt; は、L1およびL2ペナルティの凸の組み合わせを制御します。</target>
        </trans-unit>
        <trans-unit id="d600d29df52c52da3fc6f38fdcf73036ecf8cbdf" translate="yes" xml:space="preserve">
          <source>The default slice is a rectangular shape around the face, removing most of the background:</source>
          <target state="translated">デフォルトのスライスは顔の周りを囲む長方形で、 背景の大部分が除去されています。</target>
        </trans-unit>
        <trans-unit id="f2f8fa0d6181b12438bd7660924ca4c2f89302ad" translate="yes" xml:space="preserve">
          <source>The default solver is &amp;lsquo;svd&amp;rsquo;. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the &amp;lsquo;svd&amp;rsquo; solver cannot be used with shrinkage.</source>
          <target state="translated">デフォルトのソルバーは「svd」です。分類と変換の両方を実行でき、共分散行列の計算に依存しません。これは、フィーチャの数が多い場合に有利です。ただし、「svd」ソルバーは収縮では使用できません。</target>
        </trans-unit>
        <trans-unit id="2307e8c1575f885543e9c12c3e37adbd9d66c3dd" translate="yes" xml:space="preserve">
          <source>The default strategy implements one step of the bootstrapping procedure.</source>
          <target state="translated">デフォルトのストラテジーは、ブートストラップ手順の1つのステップを実装しています。</target>
        </trans-unit>
        <trans-unit id="f17725906dcf13fd5a8abccea776a77c1270d7a9" translate="yes" xml:space="preserve">
          <source>The default value &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; uses &lt;code&gt;n_features&lt;/code&gt; rather than &lt;code&gt;n_features / 3&lt;/code&gt;. The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].</source>
          <target state="translated">デフォルト値の &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; の用途の &lt;code&gt;n_features&lt;/code&gt; ではなく &lt;code&gt;n_features / 3&lt;/code&gt; 。後者はもともと[1]で提案されていましたが、前者は[2]で経験的に正当化されました。</target>
        </trans-unit>
        <trans-unit id="71b5a78b2592e847cac7e9a1c29294e2be25d4eb" translate="yes" xml:space="preserve">
          <source>The default value of &lt;code&gt;copy&lt;/code&gt; changed from False to True in 0.23.</source>
          <target state="translated">0.23では、 &lt;code&gt;copy&lt;/code&gt; のデフォルト値がFalseからTrueに変更されました。</target>
        </trans-unit>
        <trans-unit id="8ad5c264779356671425d0f732ca8de7347758f3" translate="yes" xml:space="preserve">
          <source>The default values for the parameters controlling the size of the trees (e.g. &lt;code&gt;max_depth&lt;/code&gt;, &lt;code&gt;min_samples_leaf&lt;/code&gt;, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.</source>
          <target state="translated">ツリーのサイズを制御するパラメーターのデフォルト値（例： &lt;code&gt;max_depth&lt;/code&gt; 、 &lt;code&gt;min_samples_leaf&lt;/code&gt; など）は、一部のデータセットでは非常に大きくなる可能性のある、完全に成長した枝刈りされていないツリーを導きます。メモリの消費を減らすには、ツリーの複雑さとサイズを、これらのパラメータ値を設定して制御する必要があります。</target>
        </trans-unit>
        <trans-unit id="4209f696edb2b5615bc39c07cc72cc1b347ebd54" translate="yes" xml:space="preserve">
          <source>The definitive description of key concepts and API elements for using scikit-learn and developing compatible tools.</source>
          <target state="translated">scikit-learnを使用して互換性のあるツールを開発するための重要なコンセプトとAPI要素の決定版です。</target>
        </trans-unit>
        <trans-unit id="767ee5df767908c4f8729c932c4a3d808fe558cd" translate="yes" xml:space="preserve">
          <source>The degree of the polynomial features. Default = 2.</source>
          <target state="translated">多項式特徴量の次数。デフォルトは2です。</target>
        </trans-unit>
        <trans-unit id="c0ae4038f8b612638e22a63002b9a0592ebf4ed6" translate="yes" xml:space="preserve">
          <source>The density of w, between 0 and 1</source>
          <target state="translated">0から1の間のwの密度</target>
        </trans-unit>
        <trans-unit id="1385416aaed19b5930c017ff407294aed6e786b1" translate="yes" xml:space="preserve">
          <source>The depth of a tree is the maximum distance between the root and any leaf.</source>
          <target state="translated">木の深さは、根とどの葉の間の最大の距離です。</target>
        </trans-unit>
        <trans-unit id="64211b28a272c8cc95fc17412f0b29158a9e57aa" translate="yes" xml:space="preserve">
          <source>The desired absolute tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 0.</source>
          <target state="translated">結果の希望する絶対的な許容範囲。許容範囲を大きくすると、一般的に実行が速くなります。デフォルトは0です。</target>
        </trans-unit>
        <trans-unit id="d976f3ec62dae27876361a8dc74b8b1f3089859a" translate="yes" xml:space="preserve">
          <source>The desired relative tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 1E-8.</source>
          <target state="translated">結果の望ましい相対的な許容範囲。許容範囲を大きくすると、一般的に実行が速くなります。デフォルトは1E-8です。</target>
        </trans-unit>
        <trans-unit id="e06750706d15ac4ccfc7d440948abe624b2a5a8e" translate="yes" xml:space="preserve">
          <source>The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:</source>
          <target state="translated">糖尿病データセットは、442人の患者の10の生理学的変数(年齢、性別、体重、血圧)を測定し、1年後の病気の進行度を示しています。</target>
        </trans-unit>
        <trans-unit id="9fc4977037d4a4b0eddabbe726bd2d0de7d06481" translate="yes" xml:space="preserve">
          <source>The dict at &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; gives the parameter setting for the best model, that gives the highest mean score (&lt;code&gt;search.best_score_&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; のdict は、最高の平均スコア（ &lt;code&gt;search.best_score_&lt;/code&gt; ）を与える最良のモデルのパラメーター設定を提供します。</target>
        </trans-unit>
        <trans-unit id="bfaa3d320392a9f6c8addb9130a129b654790105" translate="yes" xml:space="preserve">
          <source>The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.</source>
          <target state="translated">疎符号化に用いられる辞書アトム。行は単位ノルムに正規化されているものとする。</target>
        </trans-unit>
        <trans-unit id="7373631cc038017694fe3a2fdfb8d4bfc8e42605" translate="yes" xml:space="preserve">
          <source>The dictionary factor in the matrix factorization.</source>
          <target state="translated">行列因数分解の辞書因子。</target>
        </trans-unit>
        <trans-unit id="09a0fc68f904b631d6aa1871e7b81e42ac764443" translate="yes" xml:space="preserve">
          <source>The dictionary is fitted on the distorted left half of the image, and subsequently used to reconstruct the right half. Note that even better performance could be achieved by fitting to an undistorted (i.e. noiseless) image, but here we start from the assumption that it is not available.</source>
          <target state="translated">辞書は、歪んだ画像の左半分にフィットされ、その後、右半分を再構築するために使用されます。歪んでいない(つまりノイズのない)画像にフィットすることで、さらに優れた性能が得られることに注意してくださいが、ここでは、それが利用できないことを前提にしています。</target>
        </trans-unit>
        <trans-unit id="03531d72d72ed916646e794ac15c0622fe9c9b59" translate="yes" xml:space="preserve">
          <source>The dictionary learning objects offer, via the &lt;code&gt;split_code&lt;/code&gt; parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.</source>
          <target state="translated">辞書学習オブジェクトは、 &lt;code&gt;split_code&lt;/code&gt; パラメータを介して、スパースコーディングの結果の正と負の値を分離する可能性を提供します。これは、学習アルゴリズムが特定の原子の負の負荷から対応する正の負荷に異なる重みを割り当てることができるため、教師あり学習に使用される特徴の抽出に辞書学習が使用される場合に役立ちます。</target>
        </trans-unit>
        <trans-unit id="c5012873b1ea68e5deed2b04cdb4e90687b2c340" translate="yes" xml:space="preserve">
          <source>The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.</source>
          <target state="translated">データの疎符号化を解くための辞書行列.アルゴリズムの中には,意味のある出力のために正規化された行を想定しているものもあります.</target>
        </trans-unit>
        <trans-unit id="f137f3a53fdf8d92f7463e89f6383b5a88f3c320" translate="yes" xml:space="preserve">
          <source>The dictionary with normalized components (D).</source>
          <target state="translated">正規化された成分を持つ辞書(D)。</target>
        </trans-unit>
        <trans-unit id="d49d4bb0bf7407d291ade3beb50a3cbdf55548f8" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size &lt;code&gt;p&lt;/code&gt; unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups.</source>
          <target state="translated">LeavePGroupsOutとGroupShuffleSplitの違いは、前者はサイズ &lt;code&gt;p&lt;/code&gt; の一意のグループのすべてのサブセットを使用して分割を生成するのに対し、GroupShuffleSplitはユーザーが決定したランダムテスト分割の数を生成し、それぞれが一意のグループのユーザーが決定した割合を生成することです。</target>
        </trans-unit>
        <trans-unit id="f8e596d2cfcc0c0ae56904e18bdffdb0d463a655" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with all the samples assigned to &lt;code&gt;p&lt;/code&gt; different values of the groups while the latter uses samples all assigned the same groups.</source>
          <target state="translated">LeavePGroupsOutとLeaveOneGroupOutの違いは、前者はすべてのサンプルがグループの &lt;code&gt;p&lt;/code&gt; 個の異なる値に割り当てられたテストセットを構築するのに対し、後者はすべて同じグループに割り当てられたサンプルを使用することです。</target>
        </trans-unit>
        <trans-unit id="123fc66dcb39304255b8e25c6c10c654dbb2b0f3" translate="yes" xml:space="preserve">
          <source>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \(P(x_i \mid y)\).</source>
          <target state="translated">異なるナイーブベイズ分類器は、主に、\(P(x_i \mid y)y)の分布に関する仮定によって異なります。</target>
        </trans-unit>
        <trans-unit id="fc85907a08e94b0f17af8c0c01aad9ef06729185" translate="yes" xml:space="preserve">
          <source>The digits dataset is made of 1797 8x8 images of hand-written digits</source>
          <target state="translated">数字データセットは、手書きの数字の8x8画像1797枚で構成されています。</target>
        </trans-unit>
        <trans-unit id="46ad3d3589f97f144056aa3785c730497272c03e" translate="yes" xml:space="preserve">
          <source>The dimension of the projected subspace.</source>
          <target state="translated">投影された部分空間の次元。</target>
        </trans-unit>
        <trans-unit id="5572a493038acd1179abd12e1cd0c48e6709dea1" translate="yes" xml:space="preserve">
          <source>The dimension of the projection subspace.</source>
          <target state="translated">投影部分空間の次元。</target>
        </trans-unit>
        <trans-unit id="984d56bddca949d9f1aa278aca87eb46a4072ffd" translate="yes" xml:space="preserve">
          <source>The dimensionality of the resulting representation is &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt;. If &lt;code&gt;max_leaf_nodes == None&lt;/code&gt;, the number of leaf nodes is at most &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt;.</source>
          <target state="translated">結果の表現の次元は &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt; です。 &lt;code&gt;max_leaf_nodes == None&lt;/code&gt; 場合、リーフノードの数は最大で &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="068779d9c792d7cec6633fa1a442091cb7f6f6f3" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.</source>
          <target state="translated">ランダム投影行列の次元と分布は,データセットの任意の2つのサンプル間のペアワイズ距離を維持するように制御される.</target>
        </trans-unit>
        <trans-unit id="1fc3554ec8e7cb3510bb9bcb462301a368807c4a" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.</source>
          <target state="translated">ランダム射影行列の次元と分布は、データセットの任意の2つのサンプル間のペアワイズ距離を維持するように制御されている。このようにランダム射影は距離に基づく手法に適した近似手法である。</target>
        </trans-unit>
        <trans-unit id="42010c6a5240a459a14ffe4007a4a9d645a82c6f" translate="yes" xml:space="preserve">
          <source>The dimensions of one patch.</source>
          <target state="translated">1つのパッチの寸法。</target>
        </trans-unit>
        <trans-unit id="16b41d4b4452bd551af457fee2f8b60407215654" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet).</source>
          <target state="translated">重量分布上の各成分のディリクレット濃度(ディリクレット)。</target>
        </trans-unit>
        <trans-unit id="6b13240ddd6531c8fdb797758cabfbb4633f5e80" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;:</source>
          <target state="translated">重量分布における各成分のディリクレ濃度（ディリクレ）。タイプは &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="b470809fd29296951902d887ca553f032b8a40c4" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to &lt;code&gt;1. / n_components&lt;/code&gt;.</source>
          <target state="translated">重量分布における各成分のディリクレ濃度（ディリクレ）。これは、文献では一般にガンマと呼ばれています。濃度が高くなると、より多くの質量が中央に配置され、より多くのコンポーネントがアクティブになります。一方、濃度が低いと、混合ウェイトシンプレックスのエッジでより多くの質量が得られます。パラメータの値は0より大きくなければなりません &lt;code&gt;1. / n_components&lt;/code&gt; の場合、1に設定されます。/ n_components。</target>
        </trans-unit>
        <trans-unit id="cbf40d766d38685ebd59a9222ccafd32de07a34f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Bayesian regression include:</source>
          <target state="translated">ベイズ回帰のデメリットとしては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="ec84046fbad625e7f2f2f6744eab5866f4d72369" translate="yes" xml:space="preserve">
          <source>The disadvantages of GBRT are:</source>
          <target state="translated">GBRTのデメリットは</target>
        </trans-unit>
        <trans-unit id="cfaf1a3af8c2483843ec10c36faa80968b886bf2" translate="yes" xml:space="preserve">
          <source>The disadvantages of Gaussian processes include:</source>
          <target state="translated">ガウス過程の欠点としては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="c8e0c1974308fc5ae09e195effcd7396a8db8601" translate="yes" xml:space="preserve">
          <source>The disadvantages of Multi-layer Perceptron (MLP) include:</source>
          <target state="translated">多層パーセプトロン(MLP)の欠点としては、以下のようなものがある。</target>
        </trans-unit>
        <trans-unit id="c71179ab39a085455ac6f6888a2f6fd3afe84d6f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Stochastic Gradient Descent include:</source>
          <target state="translated">確率勾配降下のデメリットとしては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="84c28c658fb7eb00eb9347365efe97c2f04c30f3" translate="yes" xml:space="preserve">
          <source>The disadvantages of decision trees include:</source>
          <target state="translated">デシジョンツリーのデメリットとしては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="47feb0b4f4476b90d13a7091a56a1b0f68ad3a62" translate="yes" xml:space="preserve">
          <source>The disadvantages of support vector machines include:</source>
          <target state="translated">サポートベクターマシンの欠点としては、以下のようなものがあります。</target>
        </trans-unit>
        <trans-unit id="266f0e7f0f21de8e86a9f2ce2320de66f17358df" translate="yes" xml:space="preserve">
          <source>The disadvantages of the LARS method include:</source>
          <target state="translated">LARS法の欠点としては、以下のようなものがある。</target>
        </trans-unit>
        <trans-unit id="fe9e13a1ad2c431f3f290607f881625ead9408c0" translate="yes" xml:space="preserve">
          <source>The disadvantages to using t-SNE are roughly:</source>
          <target state="translated">t-SNEを使うことのデメリットは大まかに言えば。</target>
        </trans-unit>
        <trans-unit id="cd2f4d25623f857cb298f31868b60b26f2d776d4" translate="yes" xml:space="preserve">
          <source>The display objects store the computed values that were passed as arguments. This allows for the visualizations to be easliy combined using matplotlib&amp;rsquo;s API. In the following example, we place the displays next to each other in a row.</source>
          <target state="translated">表示オブジェクトには、引数として渡された計算値が格納されます。これにより、matplotlibのAPIを使用して視覚化を簡単に組み合わせることができます。次の例では、ディスプレイを並べて配置します。</target>
        </trans-unit>
        <trans-unit id="a7ea673185d2a40501e91d63f573c417ac055cd1" translate="yes" xml:space="preserve">
          <source>The distance metric to use</source>
          <target state="translated">使用する距離メトリック</target>
        </trans-unit>
        <trans-unit id="fade8506c426ca456f7b5f3ebd671dcd06e51421" translate="yes" xml:space="preserve">
          <source>The distance metric to use. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="translated">使用する距離メトリック。これは、 &lt;code&gt;metric&lt;/code&gt; パラメータまたはその同義語と同じになります。たとえば、 &lt;code&gt;metric&lt;/code&gt; パラメータが「ミンコフスキー」に設定され、 &lt;code&gt;p&lt;/code&gt; パラメータが2に設定されている場合は「ユークリッド」です。</target>
        </trans-unit>
        <trans-unit id="0063e1a06973d2332dc280d77426f57f299b9005" translate="yes" xml:space="preserve">
          <source>The distance metric to use. Note that not all metrics are valid with all algorithms. Refer to the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; for a description of available algorithms. Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is &amp;lsquo;euclidean&amp;rsquo;.</source>
          <target state="translated">使用する距離メトリック。すべてのメトリックがすべてのアルゴリズムで有効であるとは限らないことに注意してください。使用可能なアルゴリズムの説明については、&lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; の&lt;/a&gt;ドキュメントを参照してください。密度出力の正規化は、ユークリッド距離メトリックに対してのみ正しいことに注意してください。デフォルトは「ユークリッド」です。</target>
        </trans-unit>
        <trans-unit id="3c4790f8b52fd2ea34a9c9dfcea72e27e7f13da0" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the p param equal to 2.)</source>
          <target state="translated">各サンプルポイントのk近傍を計算するために使用される距離メトリック。DistanceMetricクラスは、使用可能なメトリックのリストを提供します。デフォルトの距離は「ユークリッド」です（p paramが2の「minkowski」メトリック）。</target>
        </trans-unit>
        <trans-unit id="f687007319ee4f163c7b6668932fdc048712a932" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the param equal to 2.)</source>
          <target state="translated">各サンプルポイントの特定の半径内の近傍を計算するために使用される距離メトリック。DistanceMetricクラスは、使用可能なメトリックのリストを提供します。デフォルトの距離は「ユークリッド」です（パラメーターが2の「ミンコフスキー」メトリック）。</target>
        </trans-unit>
        <trans-unit id="8c96fb568d2fd154ab3cf2ef1e009cb92f123321" translate="yes" xml:space="preserve">
          <source>The distance metric used. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="translated">使用される距離メトリック。これは、 &lt;code&gt;metric&lt;/code&gt; パラメータまたはその同義語と同じになります。たとえば、 &lt;code&gt;metric&lt;/code&gt; パラメータが「ミンコフスキー」に設定され、 &lt;code&gt;p&lt;/code&gt; パラメータが2に設定されている場合は「ユークリッド」です。</target>
        </trans-unit>
        <trans-unit id="a36fd43a3aae80628a6e6b30a7a5575b2bb66390" translate="yes" xml:space="preserve">
          <source>The distances between the row vectors of &lt;code&gt;X&lt;/code&gt; and the row vectors of &lt;code&gt;Y&lt;/code&gt; can be evaluated using &lt;a href=&quot;generated/sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;pairwise_distances&lt;/code&gt;&lt;/a&gt;. If &lt;code&gt;Y&lt;/code&gt; is omitted the pairwise distances of the row vectors of &lt;code&gt;X&lt;/code&gt; are calculated. Similarly, &lt;a href=&quot;generated/sklearn.metrics.pairwise.pairwise_kernels#sklearn.metrics.pairwise.pairwise_kernels&quot;&gt;&lt;code&gt;pairwise.pairwise_kernels&lt;/code&gt;&lt;/a&gt; can be used to calculate the kernel between &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; using different kernel functions. See the API reference for more details.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; の行ベクトルと &lt;code&gt;Y&lt;/code&gt; の行ベクトルの間の距離は、&lt;a href=&quot;generated/sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;pairwise_distances&lt;/code&gt; &lt;/a&gt;を使用して評価できます。場合 &lt;code&gt;Y&lt;/code&gt; が省略されているの行ベクトルのペアワイズ距離 &lt;code&gt;X&lt;/code&gt; を算出します。同様に、&lt;a href=&quot;generated/sklearn.metrics.pairwise.pairwise_kernels#sklearn.metrics.pairwise.pairwise_kernels&quot;&gt; &lt;code&gt;pairwise.pairwise_kernels&lt;/code&gt; &lt;/a&gt;を使用して、異なるカーネル関数を使用して &lt;code&gt;X&lt;/code&gt; と &lt;code&gt;Y&lt;/code&gt; の間のカーネルを計算できます。詳細については、APIリファレンスを参照してください。</target>
        </trans-unit>
        <trans-unit id="7e12a027c556a152948ba7c771e2cb65d4ac3e99" translate="yes" xml:space="preserve">
          <source>The distinct labels used in classifying instances.</source>
          <target state="translated">インスタンスの分類に使用される明確なラベル。</target>
        </trans-unit>
        <trans-unit id="2333c2cc37e50157e20c383f74b00f781cb37a1b" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; is asserted by the fact that &lt;code&gt;p&lt;/code&gt; is defining an eps-embedding with good probability as defined by:</source>
          <target state="translated">ランダムな射影 &lt;code&gt;p&lt;/code&gt; によって導入される歪みは、 &lt;code&gt;p&lt;/code&gt; が次のように定義される確率の高いeps埋め込みを定義しているという事実によって主張されます。</target>
        </trans-unit>
        <trans-unit id="befef2d542b10663820383e2f1e59843332d14a0" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; only changes the distance between two points by a factor (1 +- eps) in an euclidean space with good probability. The projection &lt;code&gt;p&lt;/code&gt; is an eps-embedding as defined by:</source>
          <target state="translated">ランダムな射影 &lt;code&gt;p&lt;/code&gt; によって導入された歪みは、ユークリッド空間で2点間の距離を係数（1 +-eps）だけ変化させます。射影 &lt;code&gt;p&lt;/code&gt; は、次のように定義されるeps埋め込みです。</target>
        </trans-unit>
        <trans-unit id="b6d88e183a439e17415e4b06c82155c9e34fa344" translate="yes" xml:space="preserve">
          <source>The distributions in &lt;code&gt;scipy.stats&lt;/code&gt; prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via &lt;code&gt;np.random.seed&lt;/code&gt; or set using &lt;code&gt;np.random.set_state&lt;/code&gt;. However, beginning scikit-learn 0.18, the &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module sets the random state provided by the user if scipy &amp;gt;= 0.16 is also available.</source>
          <target state="translated">バージョンscipy 0.16より前の &lt;code&gt;scipy.stats&lt;/code&gt; の配布では、ランダムな状態を指定できません。代わりに、グローバルnumpyランダム状態を使用し &lt;code&gt;np.random.set_state&lt;/code&gt; 。これはnp.random.seedを介してシードするか、 &lt;code&gt;np.random.seed&lt;/code&gt; を使用して設定できます。ただし、scikit-learn 0.18以降、scipy&amp;gt; = 0.16も使用できる場合、&lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt;モジュールはユーザーが提供するランダムな状態を設定します。</target>
        </trans-unit>
        <trans-unit id="e4d3a4f449c4df99f288fa9346370fef4e50edc3" translate="yes" xml:space="preserve">
          <source>The dual gap at the end of the optimization for the optimal alpha (&lt;code&gt;alpha_&lt;/code&gt;).</source>
          <target state="translated">最適化アルファ（ &lt;code&gt;alpha_&lt;/code&gt; ）の最適化の最後にあるデュアルギャップ。</target>
        </trans-unit>
        <trans-unit id="1c5b7990e4f078095970dbd44954a4ff0f6bdb22" translate="yes" xml:space="preserve">
          <source>The dual gaps at the end of the optimization for each alpha.</source>
          <target state="translated">各αの最適化の最後にあるデュアルギャップ。</target>
        </trans-unit>
        <trans-unit id="eceb770d3ab0f05ce2328559f3653f372ef1e2c9" translate="yes" xml:space="preserve">
          <source>The dual problem is</source>
          <target state="translated">二重問題は</target>
        </trans-unit>
        <trans-unit id="aa2b6f5781584ece88d8eb541efa4219ed937f23" translate="yes" xml:space="preserve">
          <source>The dual problem to the primal is</source>
          <target state="translated">原始人との二重問題は</target>
        </trans-unit>
        <trans-unit id="c16a68f2d6169f4f9aa27b4b94f9b3db38d73c1f" translate="yes" xml:space="preserve">
          <source>The dummy regression model predicts a constant frequency. This model does not attribute the same tied rank to all samples but is none-the-less globally well calibrated (to estimate the mean frequency of the entire population).</source>
          <target state="translated">ダミー回帰モデルは,一定の頻度を予測する.このモデルは,すべての標本に同じ拘束順位を属性しないが,グローバルによく較正されている(母集団全体の平均頻度を推定する).</target>
        </trans-unit>
        <trans-unit id="b5eac57a33eeb0d09a6d5ebfcbe611bef2da07d8" translate="yes" xml:space="preserve">
          <source>The edges of each bin. Contain arrays of varying shapes &lt;code&gt;(n_bins_, )&lt;/code&gt; Ignored features will have empty arrays.</source>
          <target state="translated">各ビンの端。さまざまな形状の配列を含む &lt;code&gt;(n_bins_, )&lt;/code&gt; 無視された機能には空の配列があります。</target>
        </trans-unit>
        <trans-unit id="50c07dc6eab301b9d9953e2bd5849c6aa3abe8d7" translate="yes" xml:space="preserve">
          <source>The effect of the transformer is weaker than on the synthetic data. However, the transform induces a decrease of the MAE.</source>
          <target state="translated">変換の効果は合成データよりも弱い。しかし、変換はMAEを減少させる。</target>
        </trans-unit>
        <trans-unit id="8b7256d0b08ed885751aee08f85ac36bb70ae336" translate="yes" xml:space="preserve">
          <source>The effective size of the batch is computed here. If there are no more jobs to dispatch, return False, else return True.</source>
          <target state="translated">バッチの有効なサイズはここで計算されます。もうディスパッチするジョブがない場合はFalseを返し、そうでない場合はTrueを返します。</target>
        </trans-unit>
        <trans-unit id="a044cf1265a1684d79e7b48898279d20ad6f0dac" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities</source>
          <target state="translated">使用する固有値分解ストラテジー。AMGはpyamgをインストールする必要があります。非常に大規模で疎な問題では高速になりますが、不安定性を引き起こす可能性があります。</target>
        </trans-unit>
        <trans-unit id="1dc685b8ff4dd3ccaa26f5f653c44ef0324bd82f" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities.</source>
          <target state="translated">使用する固有値分解ストラテジー。AMGはpyamgをインストールする必要があります。非常に大規模で疎な問題では高速化できますが、不安定性を引き起こす可能性もあります。</target>
        </trans-unit>
        <trans-unit id="e9d800f8ff5b9787b828397b459d27c3b21cb754" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems.</source>
          <target state="translated">使用する固有値分解ストラテジー。AMGはpyamgをインストールする必要があります。非常に大規模で疎な問題ではより高速になります。</target>
        </trans-unit>
        <trans-unit id="f14ed9796539634c3492800089020cfe2ebbd7af" translate="yes" xml:space="preserve">
          <source>The elastic net optimization function varies for mono and multi-outputs.</source>
          <target state="translated">弾性ネット最適化関数は、単出力と多出力で変化します。</target>
        </trans-unit>
        <trans-unit id="4d9a50594cec642f130c977c2c5c095fc619b302" translate="yes" xml:space="preserve">
          <source>The elements of the estimators parameter, having been fitted on the training data. If an estimator has been set to &lt;code&gt;'drop'&lt;/code&gt;, it will not appear in &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="translated">トレーニングデータに適合した推定量パラメーターの要素。推定に設定されている場合は &lt;code&gt;'drop'&lt;/code&gt; 、それはでは表示されません &lt;code&gt;estimators_&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e0191353bc3bca50b627784afbd555eb11c1f3dd" translate="yes" xml:space="preserve">
          <source>The empirical covariance matrix of a sample can be computed using the &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt;&lt;code&gt;empirical_covariance&lt;/code&gt;&lt;/a&gt; function of the package, or by fitting an &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; object to the data sample with the &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt;&lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Be careful that results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately. More precisely, if &lt;code&gt;assume_centered=False&lt;/code&gt;, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and &lt;code&gt;assume_centered=True&lt;/code&gt; should be used.</source>
          <target state="translated">サンプルの経験的共分散行列は、パッケージの&lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt; &lt;code&gt;empirical_covariance&lt;/code&gt; &lt;/a&gt;関数を使用するか、&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt; &lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt; &lt;/a&gt;メソッドを使用してデータサンプルに&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; &lt;/a&gt;オブジェクトを近似させることで計算できます。結果がデータが中央に配置されているかどうかに依存することに注意してください。 &lt;code&gt;assume_centered&lt;/code&gt; 、assume_centeredパラメータを正確に使用したい場合があります。より正確には、 &lt;code&gt;assume_centered=False&lt;/code&gt; の場合、テストセットはトレーニングセットと同じ平均ベクトルを持っていると想定されます。そうでない場合は、両方をユーザーが中央に配置し、 &lt;code&gt;assume_centered=True&lt;/code&gt; を使用する必要があります。</target>
        </trans-unit>
        <trans-unit id="d4b0da98d5cb047aa932ef62858edca40e188517" translate="yes" xml:space="preserve">
          <source>The encoded signal (Y).</source>
          <target state="translated">符号化された信号(Y)である。</target>
        </trans-unit>
        <trans-unit id="cc2fdf3023f61dc3df7bba02538cce691e7cf2cd" translate="yes" xml:space="preserve">
          <source>The energy function measures the quality of a joint assignment:</source>
          <target state="translated">エネルギー関数は、ジョイントアサインの質を測定します。</target>
        </trans-unit>
        <trans-unit id="1bfb5cae50ef0b1032c729ea4ab68ff502e7f906" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;test_fold[i]&lt;/code&gt; represents the index of the test set that sample &lt;code&gt;i&lt;/code&gt; belongs to. It is possible to exclude sample &lt;code&gt;i&lt;/code&gt; from any test set (i.e. include sample &lt;code&gt;i&lt;/code&gt; in every training set) by setting &lt;code&gt;test_fold[i]&lt;/code&gt; equal to -1.</source>
          <target state="translated">エントリ &lt;code&gt;test_fold[i]&lt;/code&gt; は、サンプル &lt;code&gt;i&lt;/code&gt; が属するテストセットのインデックスを表します。 &lt;code&gt;test_fold[i]&lt;/code&gt; を-1に設定することにより、任意のテストセットからサンプル &lt;code&gt;i&lt;/code&gt; を除外する（つまり、すべてのトレーニングセットにサンプル &lt;code&gt;i&lt;/code&gt; を含める）ことができます。</target>
        </trans-unit>
        <trans-unit id="503425d1f7e07b98f32316b1d85343640a4e1294" translate="yes" xml:space="preserve">
          <source>The equivalence between &lt;code&gt;alpha&lt;/code&gt; and the regularization parameter of SVM, &lt;code&gt;C&lt;/code&gt; is given by &lt;code&gt;alpha = 1 / C&lt;/code&gt; or &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt;, depending on the estimator and the exact objective function optimized by the model.</source>
          <target state="translated">&lt;code&gt;alpha&lt;/code&gt; とSVMの正則化パラメーター &lt;code&gt;C&lt;/code&gt; の間の同等性は、推定量とモデルによって最適化された正確な目的関数に応じて、 &lt;code&gt;alpha = 1 / C&lt;/code&gt; または &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt; で与えられます。</target>
        </trans-unit>
        <trans-unit id="2fec157ea85a1f513b11852f437f8dc13c9361bb" translate="yes" xml:space="preserve">
          <source>The error message or a substring of the error message.</source>
          <target state="translated">エラーメッセージまたはエラーメッセージの部分文字列。</target>
        </trans-unit>
        <trans-unit id="67a9569df158acc253e75d9fa812424d1f2b58c6" translate="yes" xml:space="preserve">
          <source>The estimated (sparse) precision matrix.</source>
          <target state="translated">推定された(疎な)精度行列。</target>
        </trans-unit>
        <trans-unit id="cdf0d65945f589fd5da3781e66a0a81a30e934b1" translate="yes" xml:space="preserve">
          <source>The estimated covariance matrix.</source>
          <target state="translated">推定共分散行列。</target>
        </trans-unit>
        <trans-unit id="4b02a2f0d176ac6fb9b484de9fcc2a6b0be67a5a" translate="yes" xml:space="preserve">
          <source>The estimated labels.</source>
          <target state="translated">推定ラベル。</target>
        </trans-unit>
        <trans-unit id="77a18d9bfc3d1fc8448c4fb559b59c860611bbc7" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;.</source>
          <target state="translated">Tipping and Bishop 1999からの確率的PCAモデルに従った推定ノイズ共分散。C。Bishopによる「パターン認識と機械学習」、12.2.1 pを参照してください。574または&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="735cdf6c1c61f38b1092aa1bd4262b34c0d5f0f3" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;. It is required to compute the estimated data covariance and score samples.</source>
          <target state="translated">Tipping and Bishop 1999からの確率的PCAモデルに従った推定ノイズ共分散。C。Bishopによる「パターン認識と機械学習」、12.2.1 pを参照してください。574または&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;。推定データの共分散とスコアサンプルを計算する必要があります。</target>
        </trans-unit>
        <trans-unit id="cc49065d7efcb582d87899f7997c53b1f8d60c78" translate="yes" xml:space="preserve">
          <source>The estimated noise variance for each feature.</source>
          <target state="translated">各特徴の推定ノイズ分散。</target>
        </trans-unit>
        <trans-unit id="960e70b3d078f74487289ecbb537290d1f7473d4" translate="yes" xml:space="preserve">
          <source>The estimated number of components. Relevant when &lt;code&gt;n_components=None&lt;/code&gt;.</source>
          <target state="translated">コンポーネントの推定数。 &lt;code&gt;n_components=None&lt;/code&gt; の場合に関連します。</target>
        </trans-unit>
        <trans-unit id="df011fa4ccaad62cb4a83a0955409a4c4b3035c8" translate="yes" xml:space="preserve">
          <source>The estimated number of components. When n_components is set to &amp;lsquo;mle&amp;rsquo; or a number between 0 and 1 (with svd_solver == &amp;lsquo;full&amp;rsquo;) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.</source>
          <target state="translated">コンポーネントの推定数。n_componentsが 'mle'または0〜1の数値（svd_solver == 'full'）に設定されている場合、この数値は入力データから推定されます。それ以外の場合は、パラメーターn_componentsに等しいか、n_componentsがNoneの場合はn_featuresおよびn_samplesの小さい方の値になります。</target>
        </trans-unit>
        <trans-unit id="cd106f5afe35a52902113758dbc47d95941530e6" translate="yes" xml:space="preserve">
          <source>The estimated number of connected components in the graph.</source>
          <target state="translated">グラフ内の連結成分の推定数。</target>
        </trans-unit>
        <trans-unit id="54994eb61f1dce6c8894f0827b84346580c11cb2" translate="yes" xml:space="preserve">
          <source>The estimation of the EXPERIENCE coefficient is now less variable and remain important for all models trained during cross-validation.</source>
          <target state="translated">EXPERIENCE係数の推定は、現在では変数が少なくなり、クロスバリデーション中に訓練されたすべてのモデルに対して重要であることに変わりはありません。</target>
        </trans-unit>
        <trans-unit id="a98c4d1bc7caeb1163bf14c014655348909c79f3" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts.</source>
          <target state="translated">モデルの推定は,p個のサブサンプル点のすべての可能な組み合わせのサブ集団の傾きと切片を計算することによって行われる.切片が適合する場合,p は n_features+1 以上でなければならない.最終的な傾きと切片は,これらの傾きと切片の空間的中央値として定義される.</target>
        </trans-unit>
        <trans-unit id="aed02faa2e2402cc32a5968b7e86a54cff535c4e" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.</source>
          <target state="translated">モデルの推定は、オブザベーションの限界対数尤度を反復的に最大化することによって行われる。</target>
        </trans-unit>
        <trans-unit id="44fe06813d3627aaa56531c52961387365e10d66" translate="yes" xml:space="preserve">
          <source>The estimation of the number of degrees of freedom is given by:</source>
          <target state="translated">自由度数の推定は次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="2bed56c931836016f42a33afdedfac5cb8f555f4" translate="yes" xml:space="preserve">
          <source>The estimator also implements &lt;code&gt;partial_fit&lt;/code&gt;, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.</source>
          <target state="translated">推定器は、ミニバッチに対して1回だけ繰り返すことによって辞書を更新する &lt;code&gt;partial_fit&lt;/code&gt; も実装します。これは、データが最初からすぐに利用できない場合、またはデータがメモリに収まらない場合に、オンライン学習に使用できます。</target>
        </trans-unit>
        <trans-unit id="efc1e3841cf31ff119d8e77fb55d7093c1df4dd4" translate="yes" xml:space="preserve">
          <source>The estimator objects for each cv split. This is available only if &lt;code&gt;return_estimator&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">各CV分割の推定子オブジェクト。これは、 &lt;code&gt;return_estimator&lt;/code&gt; パラメーターが &lt;code&gt;True&lt;/code&gt; に設定されている場合にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="f33cc8b973e26f8a535386dbd610021f87dfdf8f" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned</source>
          <target state="translated">クローン化される推定子または推定子のグループ</target>
        </trans-unit>
        <trans-unit id="c1ff61d32d7a7a2deb658878854ba14950f2c891" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned.</source>
          <target state="translated">クローン化される推定子または推定子のグループ。</target>
        </trans-unit>
        <trans-unit id="65944411996b76786bcb63db12ac31b08b55648a" translate="yes" xml:space="preserve">
          <source>The estimator that provides the initial predictions. Set via the &lt;code&gt;init&lt;/code&gt; argument or &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">初期予測を提供する推定量。 &lt;code&gt;init&lt;/code&gt; 引数または &lt;code&gt;loss.init_estimator&lt;/code&gt; を介して設定します。</target>
        </trans-unit>
        <trans-unit id="f7ae9218cda2abb5c086ad75a4ab1e94b87ebec1" translate="yes" xml:space="preserve">
          <source>The estimator to use at each step of the round-robin imputation. If &lt;code&gt;sample_posterior&lt;/code&gt; is True, the estimator must support &lt;code&gt;return_std&lt;/code&gt; in its &lt;code&gt;predict&lt;/code&gt; method.</source>
          <target state="translated">ラウンドロビン代入の各ステップで使用する推定量。場合 &lt;code&gt;sample_posterior&lt;/code&gt; がTrueで、推定器はサポートされている必要があり &lt;code&gt;return_std&lt;/code&gt; その中で &lt;code&gt;predict&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="e68c5f93929a570a29c52208fc20a3ad933e6e26" translate="yes" xml:space="preserve">
          <source>The estimator to visualize.</source>
          <target state="translated">見える化するための推定器。</target>
        </trans-unit>
        <trans-unit id="5314881b9357d8103e571e2b27bb56496dd3b052" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute, but can be accessed by index or name by indexing (with &lt;code&gt;[idx]&lt;/code&gt;) the Pipeline:</source>
          <target state="translated">パイプラインの推定量は、 &lt;code&gt;steps&lt;/code&gt; 属性にリストとして格納されますが、インデックスまたは名前でアクセスできます（ &lt;code&gt;[idx]&lt;/code&gt; を使用）。パイプライン：</target>
        </trans-unit>
        <trans-unit id="ade400270e658832b6e128c7ef187979eae33356" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute:</source>
          <target state="translated">パイプラインの推定量は、 &lt;code&gt;steps&lt;/code&gt; 属性にリストとして格納されます。</target>
        </trans-unit>
        <trans-unit id="bc948c82039e32a77f975544660f3043a6111d3d" translate="yes" xml:space="preserve">
          <source>The estimators of the pipeline can be retrieved by index:</source>
          <target state="translated">パイプラインの推定値をインデックスで検索することができます。</target>
        </trans-unit>
        <trans-unit id="a46349c122935a13db8091d877d4179b37995289" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. For example, it is possible to use these estimators to turn a binary classifier or a regressor into a multiclass classifier. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime performance improves.</source>
          <target state="translated">このモジュールで提供される推定量はメタ推定量であり、コンストラクタでベース推定量を提供する必要があります。例えば、これらの推定量を使用して、バイナリ分類器またはレグレッサーをマルチクラス分類器に変換することができます。また、精度や実行時の性能が向上することを期待して、これらの推定子をマルチクラス推定子と一緒に使用することも可能である。</target>
        </trans-unit>
        <trans-unit id="1868049da2c813b87c2e3d48a5c71f72cc892852" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. The meta-estimator extends single output estimators to multioutput estimators.</source>
          <target state="translated">このモジュールで提供される推定量はメタ推定量であり、コンストラクタでベース推定量を提供する必要があります。メタ推定量は、単一出力の推定量を複数出力の推定量に拡張します。</target>
        </trans-unit>
        <trans-unit id="fe77b9f934c3d4135d1310eb25c7f2c922f35971" translate="yes" xml:space="preserve">
          <source>The exact API of all functions and classes, as given by the docstrings. The API documents expected types and allowed features for all functions, and all parameters available for the algorithms.</source>
          <target state="translated">docstringsによって与えられた、すべての関数とクラスの正確なAPI。このAPIは、すべての関数に期待される型と許容される機能、アルゴリズムで利用可能なすべてのパラメータを文書化しています。</target>
        </trans-unit>
        <trans-unit id="213a31afd656f2a0c8b37dd55b40fc5308db3ca1" translate="yes" xml:space="preserve">
          <source>The exact additive chi squared kernel.</source>
          <target state="translated">厳密な加法カイ二乗カーネル。</target>
        </trans-unit>
        <trans-unit id="14b1a49c77fdcb8a0d3244090e7c6df0b2a6cd07" translate="yes" xml:space="preserve">
          <source>The exact chi squared kernel.</source>
          <target state="translated">厳密なカイ二乗カーネル。</target>
        </trans-unit>
        <trans-unit id="21c56a998f59a2c6be6550db485efcc395359e33" translate="yes" xml:space="preserve">
          <source>The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of &lt;code&gt;n_estimators&lt;/code&gt; at which the error stabilizes.</source>
          <target state="translated">以下の例は、トレーニング中に新しいツリーを追加するたびにOOBエラーを測定する方法を示しています。結果のプロットにより、施術者はエラーが安定する &lt;code&gt;n_estimators&lt;/code&gt; の適切な値を概算できます。</target>
        </trans-unit>
        <trans-unit id="556b3073adbd8a84d1d4b2fe21db4807c2daf4c8" translate="yes" xml:space="preserve">
          <source>The example below uses a support vector classifier with a non-linear kernel to build a model with optimized hyperparameters by grid search. We compare the performance of non-nested and nested CV strategies by taking the difference between their scores.</source>
          <target state="translated">以下の例では、非線形カーネルを持つサポートベクター分類器を用いて、グリッド探索によって最適化されたハイパーパラメタを持つモデルを構築しています。非入れ子にしたCV戦略と入れ子にしたCV戦略の性能を,それぞれのスコアの差を取って比較しています.</target>
        </trans-unit>
        <trans-unit id="4cd198cf307950e0cb10d5d2e6b86c91624772aa" translate="yes" xml:space="preserve">
          <source>The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features.</source>
          <target state="translated">本例では、線形回帰(線形モデル)と決定木(木ベースモデル)の予測結果を、実値特徴量を離散化した場合としない場合で比較している。</target>
        </trans-unit>
        <trans-unit id="3101cd4fceb55d1be63d1bff5cb3f45996fd9aa0" translate="yes" xml:space="preserve">
          <source>The example demonstrates syntax and speed only; it doesn&amp;rsquo;t actually do anything useful with the extracted vectors. See the example scripts {document_classification_20newsgroups,clustering}.py for actual learning on text documents.</source>
          <target state="translated">この例は構文と速度のみを示しています。実際には、抽出されたベクトルに対して有用なことは何もしません。テキストドキュメントの実際の学習については、サンプルスクリプト{document_classification_20newsgroups、clustering} .pyをご覧ください。</target>
        </trans-unit>
        <trans-unit id="32ed3af5edbdaa27f9eba4e5a30255c6afab4414" translate="yes" xml:space="preserve">
          <source>The example is engineered to show the effect of the choice of different metrics. It is applied to waveforms, which can be seen as high-dimensional vector. Indeed, the difference between metrics is usually more pronounced in high dimension (in particular for euclidean and cityblock).</source>
          <target state="translated">この例は、異なるメトリクスの選択の効果を示すために設計されています。これは、高次元ベクトルとして見ることができる波形に適用されています。実際、メトリクス間の違いは通常、高次元ではより顕著になります(特にユークリッドとシティブロックの場合)。</target>
        </trans-unit>
        <trans-unit id="36ed82faabe70da57df289b8a54bb16e8830773a" translate="yes" xml:space="preserve">
          <source>The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge.</source>
          <target state="translated">この例は、リッジの予測が、データセットに存在する外れ値に強く影響されていることを示しています。Huber回帰器は、モデルがこれらの外れ値に線形損失を使用するので、外れ値の影響をあまり受けません。Huber回帰器のパラメータεを増加させると、決定関数はリッジのそれに近づきます。</target>
        </trans-unit>
        <trans-unit id="889bc091af3b0106e91a81884d921d1b2df9bdd6" translate="yes" xml:space="preserve">
          <source>The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected &lt;code&gt;n_components=5&lt;/code&gt; which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.</source>
          <target state="translated">以下の例では、コンポーネントの数が固定されている混合ガウスモデルを、事前にディリクレプロセスを使用した変分混合ガウスモデルと比較しています。ここでは、古典的なガウス混合が2つのクラスターで構成されるデータセットの5つのコンポーネントに適合しています。事前にディリクレプロセスを使用した変分ガウス混合は、2つのコンポーネントのみに制限できるのに対し、ガウス混合は、ユーザーがアプリオリに事前に設定する必要のある固定数のコンポーネントを含むデータに適合します。この場合、ユーザーは &lt;code&gt;n_components=5&lt;/code&gt; を選択しましたが、これはこのおもちゃのデータセットの真の生成分布と一致しません。観測が非常に少ない場合、事前にディリクレプロセスを使用した変分ガウス混合モデルは保守的な立場をとることができ、1つのコンポーネントのみに適合します。</target>
        </trans-unit>
        <trans-unit id="2cf55b61801dba638e3df4951d7127011c13c263" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation &lt;a href=&quot;#veb2009&quot; id=&quot;id13&quot;&gt;[VEB2009]&lt;/a&gt;. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">相互情報量の期待値は、次の式&lt;a href=&quot;#veb2009&quot; id=&quot;id13&quot;&gt;[VEB2009]&lt;/a&gt;を使用して計算できます。この式では、\（a_i = | U_i | \）（\（U_i \）の要素数）および\（b_j = | V_j | \）（\（V_j \）の要素数）です。</target>
        </trans-unit>
        <trans-unit id="e5c4a6233958dced6338c85ecba7ef5860ffcbbc" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">相互情報の期待値は、次式[VEB2009]を用いて計算することができる。この式では、\(a_i=|U_i|\)(U_i|\)の要素数)と、\(b_j=|V_j|\)(V_j)の要素数)を表しています。</target>
        </trans-unit>
        <trans-unit id="5075a63ec7be1cbe42a641e5ae277f9e4b63f160" translate="yes" xml:space="preserve">
          <source>The experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The first figure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the assumption of feature-independence and result in an overly confident classifier, which is indicated by the typical transposed-sigmoid curve.</source>
          <target state="translated">実験は、20個の特徴量を持つ10万サンプル(うち1000サンプルをモデルフィッティングに使用)の2値分類用人工データセットを用いて行います。20個の特徴のうち、情報量が多いのは2個だけで、冗長性があるのは10個です。最初の図は、ロジスティック回帰、ガウスナイーブベイズ、ガウスナイーブベイズを用いて、アイソトニック校正とシグモイド校正の両方で得られた推定確率を示しています。校正性能は、凡例で報告されているBrierスコアで評価されます(小さいほど良い)。ここでは、ロジスティック回帰がよくキャリブレーションされているのに対し、生のガウスナイーブベイズは非常に悪い結果になることが観察できます。これは、特徴の非依存性の仮定に違反する冗長な特徴のためで、典型的な転置シグモイド曲線で示されるように、自信過剰な分類器になります。</target>
        </trans-unit>
        <trans-unit id="b61a812b707dfd6196631d345ad23c1f697f044e" translate="yes" xml:space="preserve">
          <source>The experimental data presents a long tail distribution for &lt;code&gt;y&lt;/code&gt;. In all models, we predict the expected frequency of a random variable, so we will have necessarily fewer extreme values than for the observed realizations of that random variable. This explains that the mode of the histograms of model predictions doesn&amp;rsquo;t necessarily correspond to the smallest value. Additionally, the normal distribution used in &lt;code&gt;Ridge&lt;/code&gt; has a constant variance, while for the Poisson distribution used in &lt;code&gt;PoissonRegressor&lt;/code&gt; and &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;, the variance is proportional to the predicted expected value.</source>
          <target state="translated">実験データは、 &lt;code&gt;y&lt;/code&gt; のロングテール分布を示しています。すべてのモデルで、確率変数の予想頻度を予測するため、その確率変数の観測された実現よりも必然的に極値が少なくなります。これは、モデル予測のヒストグラムの最頻値が必ずしも最小値に対応するとは限らないことを説明しています。さらに、 &lt;code&gt;Ridge&lt;/code&gt; で使用される正規分布には一定の分散がありますが、 &lt;code&gt;PoissonRegressor&lt;/code&gt; および &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; で使用されるポアソン分布の場合、分散は予測された期待値に比例します。</target>
        </trans-unit>
        <trans-unit id="e1db213d528f30cf745c1554ad2a43991932646f" translate="yes" xml:space="preserve">
          <source>The explained variance or ndarray if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">'multioutput'が 'raw_values'の場合​​、説明された分散またはndarray。</target>
        </trans-unit>
        <trans-unit id="dff7adf6114ae66681ccc96cdb5c8abce36c0555" translate="yes" xml:space="preserve">
          <source>The explicit constant as predicted by the &amp;ldquo;constant&amp;rdquo; strategy. This parameter is useful only for the &amp;ldquo;constant&amp;rdquo; strategy.</source>
          <target state="translated">「定数」戦略によって予測される明示的な定数。このパラメーターは、「一定」戦略のみに役立ちます。</target>
        </trans-unit>
        <trans-unit id="96483cbe0c7b434b8387960f89d9c2700f0af59b" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate [default 0.5].</source>
          <target state="translated">逆スケーリング学習率の指数 [デフォルト0.5]。</target>
        </trans-unit>
        <trans-unit id="fe088102cec241da3f7e07b043fd901378b61425" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate.</source>
          <target state="translated">逆スケーリング学習率の指数。</target>
        </trans-unit>
        <trans-unit id="1fa85799d065de026e0253585d451384c9ee6013" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to &amp;lsquo;invscaling&amp;rsquo;. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">逆スケーリング学習率の指数。learning_rateが 'invscaling'に設定されている場合、効果的な学習率の更新に使用されます。solver = 'sgd'の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="c7acf68e47ad4fe8cc8cf54d82e56ffc97c60a11" translate="yes" xml:space="preserve">
          <source>The exponent for the base kernel</source>
          <target state="translated">ベースカーネルの指数</target>
        </trans-unit>
        <trans-unit id="fe68de8b995171a61c550ec2f7815b01a2d28ec7" translate="yes" xml:space="preserve">
          <source>The exponentiated version of the kernel, which is usually preferable.</source>
          <target state="translated">カーネルの指数化されたバージョンで、通常はこれが好ましい。</target>
        </trans-unit>
        <trans-unit id="d1490a4bcb15c22959ee30800b231e4413480004" translate="yes" xml:space="preserve">
          <source>The external estimator fit on the reduced dataset.</source>
          <target state="translated">縮小されたデータセットに外部推定器がフィットします。</target>
        </trans-unit>
        <trans-unit id="18d0cd5609d8f78695dc43242ffb9cf995a65ee9" translate="yes" xml:space="preserve">
          <source>The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):</source>
          <target state="translated">抽出されたTF-IDFベクトルは非常に疎であり、30000次元以上の空間に平均159個の非ゼロ成分が存在する(0.5%以下の非ゼロ特徴量)。</target>
        </trans-unit>
        <trans-unit id="e5bc1ae1e0e90623cdbf461e900aded771c1b79d" translate="yes" xml:space="preserve">
          <source>The extracted dataset will only retain pictures of people that have at least &lt;code&gt;min_faces_per_person&lt;/code&gt; different pictures.</source>
          <target state="translated">抽出されたデータセットは、少なくとも &lt;code&gt;min_faces_per_person&lt;/code&gt; の異なる写真を持つ人々の写真のみを保持します。</target>
        </trans-unit>
        <trans-unit id="413a1ee9e7f0b0741202aa45f471425edb5c2085" translate="yes" xml:space="preserve">
          <source>The extraction method used to extract clusters using the calculated reachability and ordering. Possible values are &amp;ldquo;xi&amp;rdquo; and &amp;ldquo;dbscan&amp;rdquo;.</source>
          <target state="translated">計算された到達可能性と順序を使用してクラスターを抽出するために使用される抽出方法。可能な値は「xi」と「dbscan」です。</target>
        </trans-unit>
        <trans-unit id="7cb7e9ef70987101280d538c40ac3f0463557635" translate="yes" xml:space="preserve">
          <source>The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.</source>
          <target state="translated">ハイパーキューブのサイズを乗算した値。値が大きいほどクラスタ/クラスが分散し、分類作業が容易になります。</target>
        </trans-unit>
        <trans-unit id="65d18b0a37e6414a591311e75d9ded04a4293e5e" translate="yes" xml:space="preserve">
          <source>The factory can be any callable that takes no argument and return an instance of &lt;code&gt;ParallelBackendBase&lt;/code&gt;.</source>
          <target state="translated">ファクトリは、引数をとらず、 &lt;code&gt;ParallelBackendBase&lt;/code&gt; のインスタンスを返す任意の呼び出し可能オブジェクトにすることができます。</target>
        </trans-unit>
        <trans-unit id="0cd5de0c793252ae54dc423f82b86e165f05af5f" translate="yes" xml:space="preserve">
          <source>The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&amp;rsquo;s paper. Note that it&amp;rsquo;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.</source>
          <target state="translated">有名なアイリスデータベース。サーRAフィッシャーによって最初に使用されました。データセットはフィッシャーの論文から取られています。これはRと同じですが、UCI Machine Learning Repositoryとは異なり、2つの誤ったデータポイントがあります。</target>
        </trans-unit>
        <trans-unit id="64135ddd2f4a94ed0611a8692e8099bf4451f815" translate="yes" xml:space="preserve">
          <source>The feature (e.g. &lt;code&gt;[0]&lt;/code&gt;) or pair of interacting features (e.g. &lt;code&gt;[(0, 1)]&lt;/code&gt;) for which the partial dependency should be computed.</source>
          <target state="translated">部分的な依存関係を計算する必要のある特徴（例 &lt;code&gt;[0]&lt;/code&gt; ）または相互作用する特徴のペア（例 &lt;code&gt;[(0, 1)]&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="e9c1f9439f6a49e182ea739a4c5151e4783dfb88" translate="yes" xml:space="preserve">
          <source>The feature importance scores of a fit gradient boosting model can be accessed via the &lt;code&gt;feature_importances_&lt;/code&gt; property:</source>
          <target state="translated">適合勾配ブースティングモデルの機能重要度スコアには、 &lt;code&gt;feature_importances_&lt;/code&gt; プロパティを介してアクセスできます。</target>
        </trans-unit>
        <trans-unit id="29293679b6f2571a09f6b43f7ca55454fa94d5ff" translate="yes" xml:space="preserve">
          <source>The feature importances.</source>
          <target state="translated">特徴的な輸入品。</target>
        </trans-unit>
        <trans-unit id="596d7f9b612d1877ad65dac2d6189030efea06e6" translate="yes" xml:space="preserve">
          <source>The feature matrix &lt;code&gt;X&lt;/code&gt; should be standardized before fitting. This ensures that the penalty treats features equally.</source>
          <target state="translated">特徴行列 &lt;code&gt;X&lt;/code&gt; は、フィッティングする前に標準化する必要があります。これにより、ペナルティが機能を同等に扱うことが保証されます。</target>
        </trans-unit>
        <trans-unit id="40d3ffb0824ad292679a9b4129b9e33a1cef7858" translate="yes" xml:space="preserve">
          <source>The feature matrix. Categorical features are encoded as ordinals.</source>
          <target state="translated">特徴行列.カテゴリ特徴量は順序として符号化されます.</target>
        </trans-unit>
        <trans-unit id="0a487efba080872bb151e33f60795da2b55ed658" translate="yes" xml:space="preserve">
          <source>The feature ranking, such that &lt;code&gt;ranking_[i]&lt;/code&gt; corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.</source>
          <target state="translated">機能のランキング &lt;code&gt;ranking_[i]&lt;/code&gt; はi番目の機能のランキング位置に対応します。選択された（つまり、最良の推定）機能にはランク1が割り当てられます。</target>
        </trans-unit>
        <trans-unit id="c61b5bb2da747f9d4f147f5fd2e2f8308d20750d" translate="yes" xml:space="preserve">
          <source>The features and estimators that are experimental aren&amp;rsquo;t subject to deprecation cycles. Use them at your own risks!</source>
          <target state="translated">実験的な機能と推定量は、非推奨のサイクルの影響を受けません。ご自身の責任で使用してください！</target>
        </trans-unit>
        <trans-unit id="acbf27a6428b599cd8900c8ca4f4ecae3f0cc8b0" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and &lt;code&gt;max_features=n_features&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">機能は常に、分割ごとにランダムに並べ替えられます。したがって、最適なスプリットの検索中に列挙されたいくつかのスプリットで基準の改善が同一である場合、同じトレーニングデータと &lt;code&gt;max_features=n_features&lt;/code&gt; を使用しても、最適なスプリットは異なる場合があります。フィッティング中に確定的な動作を得るには、 &lt;code&gt;random_state&lt;/code&gt; を修正する必要があります。</target>
        </trans-unit>
        <trans-unit id="c25846da01d7420f77d01daefe6ea3229aead8d7" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, &lt;code&gt;max_features=n_features&lt;/code&gt; and &lt;code&gt;bootstrap=False&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">機能は常に、分割ごとにランダムに並べ替えられます。したがって、最適なスプリットの検索中に列挙されたいくつかのスプリットで基準の改善が同じである場合、同じトレーニングデータ &lt;code&gt;max_features=n_features&lt;/code&gt; および &lt;code&gt;bootstrap=False&lt;/code&gt; でも、最適なスプリットは異なる場合があります。フィッティング中に確定的な動作を得るには、 &lt;code&gt;random_state&lt;/code&gt; を修正する必要があります。</target>
        </trans-unit>
        <trans-unit id="79b5615baa935539329fdffd05466c4966130183" translate="yes" xml:space="preserve">
          <source>The features indices which will be returned when calling &lt;code&gt;transform&lt;/code&gt;. They are computed during &lt;code&gt;fit&lt;/code&gt;. For &lt;code&gt;features='all'&lt;/code&gt;, it is to &lt;code&gt;range(n_features)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; を呼び出したときに返される機能のインデックス。それらは、 &lt;code&gt;fit&lt;/code&gt; 中に計算されます。以下のため &lt;code&gt;features='all'&lt;/code&gt; 、それがにある &lt;code&gt;range(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7bb18d685bec7903d9b61cd6026a227300d22db3" translate="yes" xml:space="preserve">
          <source>The features of &lt;code&gt;X&lt;/code&gt; have been transformed from \([x_1, x_2]\) to \([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within any linear model.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; の機能が\（[x_1、x_2] \）から\（[1、x_1、x_2、x_1 ^ 2、x_1 x_2、x_2 ^ 2] \）に変換され、任意の線形モデル内で使用できるようになりました。</target>
        </trans-unit>
        <trans-unit id="b791adba0e25e8ba284f5da492871659f500ccc6" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2)\) to \((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).</source>
          <target state="translated">Xの特徴は、\((X_1,X_2)から\((1,X_1,X_2,X_1^2,X_1X_2,X_2^2)に変換されています。)</target>
        </trans-unit>
        <trans-unit id="0ec9e3dee3b599b742aa9c67dee3f4cab82e4d00" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2, X_3)\) to \((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).</source>
          <target state="translated">Xの特徴を、\((X_1,X_2,X_3)から\((1,X_1,X_2,X_3,X_1X_2,X_1X_3,X_1X_3,X_2X_3,X_1X_2X_3)に変換しました。)</target>
        </trans-unit>
        <trans-unit id="eaccd9379a9dbc6cce41fc318001c8bf42339abe" translate="yes" xml:space="preserve">
          <source>The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.</source>
          <target state="translated">下の図は、モデルの適合度における収縮とサブサンプリングの効果を示しています。縮みが縮みなしよりも優れていることがよくわかります。縮小を伴うサブサンプリングは、モデルの精度をさらに高めることができる。一方,収縮なしのサブサンプリングは,あまりよくない.</target>
        </trans-unit>
        <trans-unit id="409a640cc1c72ef47cfd3f6ea68f986d77585eba" translate="yes" xml:space="preserve">
          <source>The figure below shows four one-way and one two-way partial dependence plots for the California housing dataset, with a &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">次の図は、&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; を使用&lt;/a&gt;した、カリフォルニアの住宅データセットの4つの一方向および1つの双方向の部分依存プロットを示しています。</target>
        </trans-unit>
        <trans-unit id="50a711fcdd2b061e3d348535a1d7a4d9bddb7ed5" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">次の図は、最小二乗損失と500の基本学習器を持つ&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;をボストンの住宅価格データセット（&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt;）に適用した結果を示しています。左側のプロットは、各反復での学習誤差とテスト誤差を示しています。各反復での &lt;code&gt;train_score_&lt;/code&gt; は、勾配ブースティングモデルのtrain_score_属性に格納されます。各反復でのテストエラーは、各ステージで予測を生成するジェネレーターを返す&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt;メソッドを介して取得できます。このようなプロットを使用して、早期停止することにより、ツリー（つまり &lt;code&gt;n_estimators&lt;/code&gt; ）の最適な数を決定できます。右側のプロットは、機能の重要性を示しています。 &lt;code&gt;feature_importances_&lt;/code&gt; プロパティ。</target>
        </trans-unit>
        <trans-unit id="76597dd5c3aa4f5be5177f9ebf1af69e02fc15a8" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the impurity-based feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">次の図は、最小二乗損失と500人の基本学習者を持つ&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;をボストンの住宅価格データセット（&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt;）に適用した結果を示しています。左側のプロットは、各反復でのトレインとテストのエラーを示しています。各反復でのトレインエラーは、勾配ブースティングモデルの &lt;code&gt;train_score_&lt;/code&gt; 属性に格納されます。各反復でのテストエラーは、各ステージで予測を生成するジェネレーターを返す&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt;メソッドを介して取得できます。このようなプロットは、早期停止によって最適なツリー数（つまり、 &lt;code&gt;n_estimators&lt;/code&gt; ）を決定するために使用できます。右側のプロットは、不純物ベースの特徴の重要性を示しています。 &lt;code&gt;feature_importances_&lt;/code&gt; プロパティ。</target>
        </trans-unit>
        <trans-unit id="cc8de6d2f1f6034186ab0af10d91754eadd34954" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">この図は、両方の方法がターゲット関数の合理的なモデルを学習することを示しています。 GPRは関数の周期性をおよそ2 * pi（6.28）であると正しく識別しますが、KRRは2倍の周期性4 * piを選択します。その上、GPRはKRRでは利用できない予測に妥当な信頼限界を提供します。 2つの方法の主な違いは、フィッティングと予測に必要な時間です。KRRのフィッティングは原則的に高速ですが、ハイパーパラメーター最適化のグリッド検索は、ハイパーパラメーターの数（「次元の呪い」）に応じて指数関数的にスケーリングします。 GPRのパラメーターの勾配ベースの最適化は、この指数スケーリングの影響を受けないため、3次元のハイパーパラメーター空間を使用したこの例ではかなり高速になります。予測の時間は似ています。しかしながら、GPRの予測分布の分散の生成には、平均の予測よりもかなり時間がかかります。</target>
        </trans-unit>
        <trans-unit id="7a70cd6dc569a4ad4c8d12310e45e3bf56f13e7d" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly \(2*\pi\) (6.28), while KRR chooses the doubled periodicity \(4*\pi\) . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">この図は、両方の方法がターゲット関数の合理的なモデルを学習することを示しています。 GPRは関数の周期性をおよそ\（2 * \ pi \）（6.28）であると正しく識別しますが、KRRは2倍の周期性\（4 * \ pi \）を選択します。その上、GPRはKRRでは利用できない予測に妥当な信頼限界を提供します。 2つの方法の主な違いは、フィッティングと予測に必要な時間です。KRRのフィッティングは原則的に高速ですが、ハイパーパラメーター最適化のグリッド検索は、ハイパーパラメーターの数（「次元の呪い」）に応じて指数関数的にスケーリングします。 GPRのパラメーターの勾配ベースの最適化は、この指数スケーリングの影響を受けないため、3次元のハイパーパラメーター空間を使用したこの例ではかなり高速になります。予測の時間は似ています。しかしながら、GPRの予測分布の分散の生成には、平均の予測よりもかなり時間がかかります。</target>
        </trans-unit>
        <trans-unit id="4c819c571e9cf27ccc7a880b7ae493959bac8667" translate="yes" xml:space="preserve">
          <source>The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding.</source>
          <target state="translated">図は、ブースティングの反復回数の関数としての負のOOB改善の累積和を示しています。ご覧のように、最初の100回の繰り返しでテスト損失を追跡しますが、その後は悲観的な方法で分岐します。この図はまた、通常はテスト損失のより良い推定値を提供しますが、計算量が多くなる3倍クロスバリデーションの性能を示しています。</target>
        </trans-unit>
        <trans-unit id="37bfa29a2e04a43879779df5389399bec3580356" translate="yes" xml:space="preserve">
          <source>The figure shows the trade-off between cross-validated score and the number of PCA components. The balanced case is when n_components=10 and accuracy=0.88, which falls into the range within 1 standard deviation of the best accuracy score.</source>
          <target state="translated">図は、交差検証されたスコアとPCA成分の数のトレードオフを示しています。バランスが取れているのは、n_components=10、精度=0.88の場合で、これは最高精度スコアの1標準偏差以内の範囲内に収まっています。</target>
        </trans-unit>
        <trans-unit id="fc5bec6cdf25a030ca7f5736bc605af5657e6e59" translate="yes" xml:space="preserve">
          <source>The figures below are used to illustrate the effect of scaling our &lt;code&gt;C&lt;/code&gt; to compensate for the change in the number of samples, in the case of using an &lt;code&gt;l1&lt;/code&gt; penalty, as well as the &lt;code&gt;l2&lt;/code&gt; penalty.</source>
          <target state="translated">以下の図は、 &lt;code&gt;l1&lt;/code&gt; ペナルティと &lt;code&gt;l2&lt;/code&gt; ペナルティを使用する場合に、サンプル数の変化を補償するために &lt;code&gt;C&lt;/code&gt; をスケーリングする効果を示すために使用されます。</target>
        </trans-unit>
        <trans-unit id="0b0d592cc5daeb578d9b82714e3800c3092f375f" translate="yes" xml:space="preserve">
          <source>The figures illustrate the interpolating property of the Gaussian Process model as well as its probabilistic nature in the form of a pointwise 95% confidence interval.</source>
          <target state="translated">図は、ガウスプロセスモデルの補間特性とその確率的性質を、点単位の95%信頼区間の形で示しています。</target>
        </trans-unit>
        <trans-unit id="e96dfdb18e509ef4ea2f5731757d5c9fff16f17b" translate="yes" xml:space="preserve">
          <source>The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.</source>
          <target state="translated">図は,クラスサポートサイズ(各クラスの要素数)による正規化の有無による混同行列を示しています.このような正規化は、クラスの不均衡の場合に、どのクラスが誤分類されているかをより視覚的に解釈できるようにするために興味深いものです。</target>
        </trans-unit>
        <trans-unit id="e0f5d06a7d2ed2f3c6fb0ae21cf62a8ad73fbd7f" translate="yes" xml:space="preserve">
          <source>The filenames for the images.</source>
          <target state="translated">画像のファイル名です。</target>
        </trans-unit>
        <trans-unit id="4c01e855d5c880b7b53c9ce8ac56ca5941d4bf79" translate="yes" xml:space="preserve">
          <source>The filenames holding the dataset.</source>
          <target state="translated">データセットを保持するファイル名。</target>
        </trans-unit>
        <trans-unit id="83d2d2e5c2f316a3531e949e8bac6210f7e21821" translate="yes" xml:space="preserve">
          <source>The files themselves are loaded in memory in the &lt;code&gt;data&lt;/code&gt; attribute. For reference the filenames are also available:</source>
          <target state="translated">ファイル自体は、 &lt;code&gt;data&lt;/code&gt; 属性でメモリに読み込まれます。参考までに、ファイル名も使用できます。</target>
        </trans-unit>
        <trans-unit id="4834eeb362cb8f80edd0fa52b738fe7db255a20e" translate="yes" xml:space="preserve">
          <source>The filesystem path to the root folder where MLComp datasets are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead.</source>
          <target state="translated">mlcomp_root が None の場合は、MLCOMP_DATASETS_HOME 環境変数が代わりに検索されます。</target>
        </trans-unit>
        <trans-unit id="5e83705f6f5cf8453734b26cfa75b02cefbf9a06" translate="yes" xml:space="preserve">
          <source>The final sum of similarities is divided by the size of the larger set.</source>
          <target state="translated">最終的な類似性の合計は、より大きな集合の大きさで割られています。</target>
        </trans-unit>
        <trans-unit id="6023cd4bbdadd0261fe4fb93bbac4f79dd623983" translate="yes" xml:space="preserve">
          <source>The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set).</source>
          <target state="translated">慣性基準の最終値(訓練セット内のすべてのオブザベーションの最も近いセントロイドまでの二乗距離の合計).</target>
        </trans-unit>
        <trans-unit id="65db46a7a2384cdd4de97d3c70536cd9da4c8616" translate="yes" xml:space="preserve">
          <source>The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points).</source>
          <target state="translated">応力の最終値(視差の二乗距離と拘束されたすべての点の距離の和)。</target>
        </trans-unit>
        <trans-unit id="379910ba877ee4fe0038b0877574596b2d4e7156" translate="yes" xml:space="preserve">
          <source>The first 4 plots use the &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; with different numbers of informative features, clusters per class and classes. The final 2 plots use &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">最初の4つのプロットは、&lt;a href=&quot;../../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt; &lt;code&gt;make_classification&lt;/code&gt; &lt;/a&gt;な数の有益な機能、クラスごとのクラスター、およびクラスでmake_classificationを使用します。最後の2つのプロットは、&lt;a href=&quot;../../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt; &lt;code&gt;make_blobs&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt; &lt;code&gt;make_gaussian_quantiles&lt;/code&gt; を&lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="7e7a148ceab046969e5524f650bd9948ad6ae0ac" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;[.9, .1]&lt;/code&gt; in &lt;code&gt;y_pred&lt;/code&gt; denotes 90% probability that the first sample has label 0. The log loss is non-negative.</source>
          <target state="translated">&lt;code&gt;y_pred&lt;/code&gt; の最初の &lt;code&gt;[.9, .1]&lt;/code&gt; は、最初のサンプルのラベルが0である確率が90％であることを示します。対数損失は負ではありません。</target>
        </trans-unit>
        <trans-unit id="c41f4a219241dbe01041bd4816c28f057f1f8fc9" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;n_samples % n_splits&lt;/code&gt; folds have size &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt;, other folds have size &lt;code&gt;n_samples // n_splits&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">最初の &lt;code&gt;n_samples % n_splits&lt;/code&gt; フォールドのサイズは &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt; 、他のフォールドのサイズは &lt;code&gt;n_samples // n_splits&lt;/code&gt; 。ここで、 &lt;code&gt;n_samples&lt;/code&gt; はサンプルの数です。</target>
        </trans-unit>
        <trans-unit id="318062ceb48883ba19b024b3ac85479b5d766c8b" translate="yes" xml:space="preserve">
          <source>The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices. In general, multiple points can be queried at the same time.</source>
          <target state="translated">返される最初の配列には、1.6よりも近いすべての点までの距離が含まれ、2番目の配列にはそのインデックスが含まれます。一般的に、複数の点を同時に問い合わせることができます。</target>
        </trans-unit>
        <trans-unit id="b072f11f22e8fd40f8f011ced740f859610c5839" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the product-kernel</source>
          <target state="translated">製品カーネルの最初のベースカーネル</target>
        </trans-unit>
        <trans-unit id="2843deb348cef2ebb64b5cb6c478da3efa931340" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the sum-kernel</source>
          <target state="translated">和カーネルの第一基底カーネル</target>
        </trans-unit>
        <trans-unit id="23baf5dbd5fca89380aba285e5a6dca77d9ff4b8" translate="yes" xml:space="preserve">
          <source>The first column of images shows true faces. The next columns illustrate how extremely randomized trees, k nearest neighbors, linear regression and ridge regression complete the lower half of those faces.</source>
          <target state="translated">画像の最初の列は真の顔を示しています。次の列は、極端にランダム化された木、k個の最も近い隣人、線形回帰、尾根回帰が、これらの顔の下半分をどのように完成させるかを示しています。</target>
        </trans-unit>
        <trans-unit id="758127cc4d0b762bc4a77876dd4551f141ccd7cd" translate="yes" xml:space="preserve">
          <source>The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.</source>
          <target state="translated">1つ目は、ノイズレベルが高く、長さスケールが大きいモデルに対応しており、ノイズによるデータの変動をすべて説明しています。</target>
        </trans-unit>
        <trans-unit id="2d71e99749591e16661c08d2c19738355ed2fad8" translate="yes" xml:space="preserve">
          <source>The first element of each line can be used to store a target variable to predict.</source>
          <target state="translated">各行の最初の要素は、予測する対象変数を格納するために使用することができます。</target>
        </trans-unit>
        <trans-unit id="d151d2a5867c5ba125b375d964535691cae0b0d6" translate="yes" xml:space="preserve">
          <source>The first example illustrates how robust covariance estimation can help concentrating on a relevant cluster when another one exists. Here, many observations are confounded into one and break down the empirical covariance estimation. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">最初の例は、堅牢な共分散推定が、別のクラスターが存在する場合に、関連するクラスターに集中するのにどのように役立つかを示しています。ここでは、多くの観察結果が1つに混同されており、経験的共分散推定を分析しています。もちろん、いくつかのスクリーニングツールは、2つのクラスター（サポートベクターマシン、ガウス混合モデル、一変量異常値検出など）の存在を指摘します。しかし、それが高次元の例であったとしたら、これらのどれもそれほど簡単には適用できませんでした。</target>
        </trans-unit>
        <trans-unit id="3ed755a713ce6230a16c443825e8cd0dee1503e8" translate="yes" xml:space="preserve">
          <source>The first example illustrates how the Minimum Covariance Determinant robust estimator can help concentrate on a relevant cluster when outlying points exist. Here the empirical covariance estimation is skewed by points outside of the main cluster. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">最初の例は、最小共分散行列式のロバスト推定量が、範囲外の点が存在する場合に、関連するクラスターに集中するのにどのように役立つかを示しています。ここで、経験的共分散推定は、メインクラスターの外側の点によって歪められています。もちろん、いくつかのスクリーニングツールは、2つのクラスターの存在を指摘しているでしょう（サポートベクターマシン、ガウス混合モデル、単変量外れ値検出など）。しかし、それが高次元の例であったとしたら、これらのどれもそれほど簡単に適用することはできませんでした。</target>
        </trans-unit>
        <trans-unit id="ecfbbb8300d863a918a602e1c4f90841d4b71a3f" translate="yes" xml:space="preserve">
          <source>The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):</source>
          <target state="translated">最初のローダは、顔識別タスクに使用されます:マルチクラス分類タスク(したがって教師付き学習)。</target>
        </trans-unit>
        <trans-unit id="1272790baab931c65e23f13d4b17128820578899" translate="yes" xml:space="preserve">
          <source>The first model is a classical Gaussian Mixture Model with 10 components fit with the Expectation-Maximization algorithm.</source>
          <target state="translated">最初のモデルは、10個の成分を持つ古典的なガウス混合モデルで、期待値最大化アルゴリズムを用いて適合させます。</target>
        </trans-unit>
        <trans-unit id="a28da4c4f339f2d2823befd3907c74a3bd7b1459" translate="yes" xml:space="preserve">
          <source>The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.</source>
          <target state="translated">最初のプロットは、2つの入力特徴と2つの可能なターゲット・クラス(2値分類)のみを含む単純化された分類問題における、さまざまなパラメータ値の決定関数の可視化です。この種のプロットは、より多くの特徴や対象クラスを持つ問題ではできないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="fdb1a20b80ba5f009466ef7acd3785afda979734" translate="yes" xml:space="preserve">
          <source>The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit &amp;ldquo;block&amp;rdquo; is stacked above each point on a regular grid. As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution. If we instead center each block on the point it represents, we get the estimate shown in the bottom left panel. This is a kernel density estimation with a &amp;ldquo;top hat&amp;rdquo; kernel. This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution.</source>
          <target state="translated">最初のプロットは、ヒストグラムを使用してポイントの密度を1Dで視覚化する場合の問題の1つを示しています。直感的に、ヒストグラムは、単位「ブロック」が規則的なグリッドの各ポイントの上に積み重ねられるスキームと考えることができます。ただし、上部の2つのパネルが示すように、これらのブロックのグリッドを選択すると、密度分布の基になる形状について非常に多様なアイデアが導かれる可能性があります。代わりに、各ブロックをそれが表す点の中央に配置すると、左下のパネルに推定値が表示されます。これは、「シルクハット」カーネルを使用したカーネル密度推定です。このアイデアは、他のカーネル形状に一般化できます。最初の図の右下のパネルは、同じ分布でのガウスカーネル密度推定を示しています。</target>
        </trans-unit>
        <trans-unit id="80a6f35d6de94c1655a0ab570a2d81b8e9f0c281" translate="yes" xml:space="preserve">
          <source>The first plot shows that with an increasing number of samples &lt;code&gt;n_samples&lt;/code&gt;, the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; increased logarithmically in order to guarantee an &lt;code&gt;eps&lt;/code&gt;-embedding.</source>
          <target state="translated">サンプルの数が増加していることを最初のプロットが示す &lt;code&gt;n_samples&lt;/code&gt; 、寸法の最小数 &lt;code&gt;n_components&lt;/code&gt; を保証するために対数的に増加 &lt;code&gt;eps&lt;/code&gt; -Embeddingを。</target>
        </trans-unit>
        <trans-unit id="0fa5069414b33f645c4a3a3261f97c50a380bf97" translate="yes" xml:space="preserve">
          <source>The first plot shows the best inertia reached for each combination of the model (&lt;code&gt;KMeans&lt;/code&gt; or &lt;code&gt;MiniBatchKMeans&lt;/code&gt;) and the init method (&lt;code&gt;init=&quot;random&quot;&lt;/code&gt; or &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt;) for increasing values of the &lt;code&gt;n_init&lt;/code&gt; parameter that controls the number of initializations.</source>
          <target state="translated">最初のプロットは、モデル（ &lt;code&gt;KMeans&lt;/code&gt; または &lt;code&gt;MiniBatchKMeans&lt;/code&gt; ）とinitメソッド（ &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; または &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt; ）の各組み合わせで到達した最適な慣性を示し、初期化の数を制御する &lt;code&gt;n_init&lt;/code&gt; パラメーターの値を増やしています。</target>
        </trans-unit>
        <trans-unit id="35a5988878bfd98d1cc6f738d4af80878102b501" translate="yes" xml:space="preserve">
          <source>The first row of output array indicates that there are three samples whose true cluster is &amp;ldquo;a&amp;rdquo;. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is &amp;ldquo;b&amp;rdquo;. Of them, none is in predicted cluster 0, one is in 1 and two are in 2.</source>
          <target state="translated">出力配列の最初の行は、真のクラスターが「a」である3つのサンプルがあることを示しています。そのうち2つは予測クラスター0にあり、1つは1にあり、1つは2にありません。2行目は、真のクラスターが「b」であるサンプルが3つあることを示しています。それらのうち、どれも予測クラスター0にありません。1つは1に、2つは2にあります。</target>
        </trans-unit>
        <trans-unit id="94d22c244cece5d36684b54eb3a3c36ef460564d" translate="yes" xml:space="preserve">
          <source>The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.</source>
          <target state="translated">最初の2つの損失関数は怠惰で、例がマージン制約に違反した場合にのみモデルパラメータを更新するため、訓練が非常に効率的になり、L2ペナルティが使用されている場合でも、よりスパースなモデルが得られる可能性があります。</target>
        </trans-unit>
        <trans-unit id="90159f341e51eef650aa99c9ddd91af9915c59a3" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the &lt;code&gt;transform&lt;/code&gt; method.</source>
          <target state="translated">近似モデルは、 &lt;code&gt;transform&lt;/code&gt; 方法を使用して、入力を最も識別力のある方向に投影することにより、入力の次元を削減するためにも使用できます。</target>
        </trans-unit>
        <trans-unit id="865ac3c349894331005e5c9a9918fbe1c4576705" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.</source>
          <target state="translated">また、フィットしたモデルは、最も識別性の高い方向に投影することで、入力の次元を下げるために使用することができます。</target>
        </trans-unit>
        <trans-unit id="47ade7eb6fd49b2a025d9a87896bfcf3ba24402d" translate="yes" xml:space="preserve">
          <source>The fitted model.</source>
          <target state="translated">フィットしたモデル。</target>
        </trans-unit>
        <trans-unit id="474bb96d2aba10d56f1c494aef7da54c3ce49fa5" translate="yes" xml:space="preserve">
          <source>The flattened data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="translated">平坦化されたデータマトリックス。 &lt;code&gt;as_frame=True&lt;/code&gt; の場合、 &lt;code&gt;data&lt;/code&gt; はpandasDataFrameになります。</target>
        </trans-unit>
        <trans-unit id="14120dbe50cfe7a93d61eb3782205eb1e9322e9d" translate="yes" xml:space="preserve">
          <source>The flexibility of controlling the smoothness of the learned function via \(\nu\) allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Mat&amp;eacute;rn kernel are shown in the following figure:</source>
          <target state="translated">\（\ nu \）を介して学習した関数の滑らかさを制御する柔軟性により、真の基本的な関数関係のプロパティに適応できます。次の図は、Mat&amp;eacute;rnカーネルの結果のGPの前後を示しています。</target>
        </trans-unit>
        <trans-unit id="130cb7c407aba5d31bddc566759f6c2692fa934a" translate="yes" xml:space="preserve">
          <source>The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.</source>
          <target state="translated">以下のフローチャートは、どの推定値をデータ上で試すべきかについて、どのように問題にアプローチするかについて、ユーザーに少しだけ大まかなガイドを与えるように設計されています。</target>
        </trans-unit>
        <trans-unit id="f162534c96a36fe6b7c86b6775d49d882723bf35" translate="yes" xml:space="preserve">
          <source>The folder names are used as supervised signal label names. The individual file names are not important.</source>
          <target state="translated">フォルダ名は、スーパーバイズドシグナルのラベル名として使用されます。個々のファイル名は重要ではありません。</target>
        </trans-unit>
        <trans-unit id="f2db8b6a5929e1458b6943c5d307e1ce8a02db89" translate="yes" xml:space="preserve">
          <source>The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.</source>
          <target state="translated">それぞれの折り目の中で、はっきりとしたグループの数がほぼ同じという意味では、ほぼバランスが取れています。</target>
        </trans-unit>
        <trans-unit id="7910d480ac1425d50ff9fab631fdf912810ec26e" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">以下は,目標値が特徴量の線形の組み合わせであると予想される回帰のための手法のセットである.数学的表記法では,もし \(\hat{y}\)が予測値であれば.</target>
        </trans-unit>
        <trans-unit id="b43c7b928f1a786f0281c067a04292ccfddc7963" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">以下は、目標値が入力変数の線形結合であると予想される回帰のための方法のセットである。数学的な概念では、もし \(\hat{y}\)が予測値であれば、次のようになります。</target>
        </trans-unit>
        <trans-unit id="81301a81f070c1a1a05c02b69a367906c9b88d21" translate="yes" xml:space="preserve">
          <source>The following clustering assignment is slightly better, since it is homogeneous but not complete:</source>
          <target state="translated">以下のクラスタリング割り当ては、均質ではあるが完全ではないので、やや良い。</target>
        </trans-unit>
        <trans-unit id="563a9a6f3a15e2fd51ba9e9b647870430bf12e58" translate="yes" xml:space="preserve">
          <source>The following code defines a linear kernel and creates a classifier instance that will use that kernel:</source>
          <target state="translated">以下のコードは,線形カーネルを定義し,そのカーネルを使用する分類器インスタンスを作成します.</target>
        </trans-unit>
        <trans-unit id="b8e362395586349d1fcb8b526001a536e06f5d5d" translate="yes" xml:space="preserve">
          <source>The following code is a bit verbose, feel free to jump directly to the analysis of the &lt;a href=&quot;#results&quot;&gt;results&lt;/a&gt;.</source>
          <target state="translated">次のコードは少し冗長です。自由に&lt;a href=&quot;#results&quot;&gt;結果の&lt;/a&gt;分析に直接進んでください。</target>
        </trans-unit>
        <trans-unit id="b414b22c1d4fa210fe910463f00d0b843e42262f" translate="yes" xml:space="preserve">
          <source>The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the &lt;code&gt;groups&lt;/code&gt; parameter.</source>
          <target state="translated">これを行うには、次の相互検証スプリッターを使用できます。サンプルのグループ化識別子は &lt;code&gt;groups&lt;/code&gt; パラメータで指定されます。</target>
        </trans-unit>
        <trans-unit id="6757b5cfa24458748d25f81af60d324180a74dc4" translate="yes" xml:space="preserve">
          <source>The following cross-validators can be used in such cases.</source>
          <target state="translated">このような場合には、以下のようなクロスバリデーターを使用することができます。</target>
        </trans-unit>
        <trans-unit id="6da3fe3659ba9973062031cbded9ac1ee9e80559" translate="yes" xml:space="preserve">
          <source>The following dataset has integer features, two of which are the same in every sample. These are removed with the default setting for threshold:</source>
          <target state="translated">以下のデータセットは,整数の特徴量を持ち,そのうちの2つはすべてのサンプルで同じである.これらは,しきい値のデフォルト設定で削除されます.</target>
        </trans-unit>
        <trans-unit id="54024b35aa398c3f2441f7e2cd8d0e6044cd9d46" translate="yes" xml:space="preserve">
          <source>The following estimators have built-in variable selection fitting procedures, but any estimator using a L1 or elastic-net penalty also performs variable selection: typically &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with an appropriate penalty.</source>
          <target state="translated">次の推定量には変数選択フィッティング手順が組み込まれていますが、L1またはエラスティックネットペナルティを使用する推定量も変数選択を実行します。通常、適切なペナルティを持つ&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="f50629b49593a5773731e8a4d6d7fbab3d06afa6" translate="yes" xml:space="preserve">
          <source>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</source>
          <target state="translated">以下の例は、データを分割し、モデルをフィットさせ、スコアを5回連続して計算することで、虹彩データセット上の線形カーネルサポートベクターマシンの精度を推定する方法を示しています(毎回異なる分割を行う)。</target>
        </trans-unit>
        <trans-unit id="cd40f6b0be33059696fbdf36c2c2af62f27e4578" translate="yes" xml:space="preserve">
          <source>The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="translated">次の例は、順列ベースの特徴の重要性とは対照的に、不純物ベースの特徴の重要性の制限を強調しています：&lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;順列の重要性とランダムフォレストの特徴の重要性（MDI）&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="50761302c7bb4483750fc53752f14a1bf26cb023" translate="yes" xml:space="preserve">
          <source>The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.</source>
          <target state="translated">次の例は、Olivetti顔データセットからスパースPCAを使用して抽出された16個のコンポーネントを示しています。正則化項が多くのゼロをどのように誘導するかがわかります。さらに、データの自然な構造により、非ゼロ係数が垂直方向に隣接します。モデルはこれを数学的に強制しません。各コンポーネントはベクトル\（h \ in \ mathbf {R} ^ {4096} \）であり、64x64ピクセルイメージとして人間にわかりやすい視覚化中を除いて、垂直隣接の概念はありません。以下に示すコンポーネントが局所的に見えるという事実は、データの固有の構造の影響であり、このような局所的なパターンによって再構成エラーが最小限に抑えられます。隣接性とさまざまな種類の構造を考慮したスパース性を誘発する規範が存在します。&lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]を&lt;/a&gt;参照そのような方法のレビューのため。スパースPCAの使用方法の詳細については、以下の「例」セクションを参照してください。</target>
        </trans-unit>
        <trans-unit id="218baecbc1e50d08c530ba0bc6ca1335ad228789" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">次の例は、線形サポートベクターマシン、決定木、およびK最近傍分類器に基づいてソフト&lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; &lt;/a&gt;を使用した場合に、決定領域がどのように変化するかを示しています。</target>
        </trans-unit>
        <trans-unit id="0b4e5d16e2ae8075091f7ec9c797f8dd043b5cfe" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;code&gt;VotingClassifier&lt;/code&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">次の例は、線形サポートベクターマシン、ディシジョンツリー、およびK最近傍分類子に基づいてソフト &lt;code&gt;VotingClassifier&lt;/code&gt; を使用した場合に、決定領域がどのように変化するかを示しています。</target>
        </trans-unit>
        <trans-unit id="fcb0630f60eb0a1b1e5514ec57d9a7f6cbd21ce7" translate="yes" xml:space="preserve">
          <source>The following example illustrates the effect of scaling the regularization parameter when using &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; for &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;classification&lt;/a&gt;. For SVC classification, we are interested in a risk minimization for the equation:</source>
          <target state="translated">次の例は、&lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;分類に&lt;/a&gt;&lt;a href=&quot;../../modules/svm#svm&quot;&gt;サポートベクターマシン&lt;/a&gt;を使用する場合の正則化パラメーターのスケーリングの効果を示しています。SVC分類については、方程式のリスク最小化に関心があります。</target>
        </trans-unit>
        <trans-unit id="35227ebb51bdeaa9a401c5e44c0ca9ca35e1f0e6" translate="yes" xml:space="preserve">
          <source>The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">次の例は、&lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt;モデルを使用した顔認識タスクの個々のピクセルの相対的な重要度を色分けして示しています。</target>
        </trans-unit>
        <trans-unit id="e4e3a609b2ed09432a121fc3917b934875c3544c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit an AdaBoost classifier with 100 weak learners:</source>
          <target state="translated">次の例では、AdaBoost分類器を100人の弱い学習者にフィットさせる方法を示しています。</target>
        </trans-unit>
        <trans-unit id="08beea172467f4b9200a143a3d6a2c2ed164664c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the VotingRegressor:</source>
          <target state="translated">以下の例では、VotingRegressorにフィットする方法を示しています。</target>
        </trans-unit>
        <trans-unit id="486be98f63ccda08601d245e4a85066d3abba297" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the majority rule classifier:</source>
          <target state="translated">以下の例では、多数決分類器を適合させる方法を示しています。</target>
        </trans-unit>
        <trans-unit id="6a236adbadcca90d4c1ec977d69fa3c364957c12" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 most informative features in the Friedman #1 dataset.</source>
          <target state="translated">以下の例は、Friedman #1データセットの中で最も情報量の多い5つの特徴を取得する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="376fe34989034d77dfd504e8201c3d4b2dfa1573" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.</source>
          <target state="translated">以下の例では、Friedman #1データセットの中の5つの右の情報的特徴を取得する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="25b5efc453e06d92e8572618fefd1930c5c8aab3" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.</source>
          <target state="translated">以下の例では、Friedman #1データセットの中の事前に知られていない5つの情報特徴量を取得する方法を示します。</target>
        </trans-unit>
        <trans-unit id="e99af0f029f79715308d4d1c9ecbe2acb8983e4d" translate="yes" xml:space="preserve">
          <source>The following example will, for instance, transform some British spelling to American spelling:</source>
          <target state="translated">次の例では、例えば、いくつかのイギリスのスペルをアメリカのスペルに変換します。</target>
        </trans-unit>
        <trans-unit id="ad9262c5496a82045b0e3b64071f384beed06659" translate="yes" xml:space="preserve">
          <source>The following experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The figure shows the estimated probabilities obtained with logistic regression, a linear support-vector classifier (SVC), and linear SVC with both isotonic calibration and sigmoid calibration. The Brier score is a metric which is a combination of calibration loss and refinement loss, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt;, reported in the legend (the smaller the better). Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve.</source>
          <target state="translated">次の実験は、20の特徴を持つ100,000個のサンプル（そのうち1000個はモデルの当てはめに使用されます）のバイナリ分類の人工データセットで実行されます。20の機能のうち、2つだけが有益であり、10つは冗長です。この図は、ロジスティック回帰、線形サポートベクトル分類器（SVC）、および等張性キャリブレーションとシグモイドキャリブレーションの両方を使用した線形SVCで得られた推定確率を示しています。ブライアスコアは、キャリブレーション損失とリファインメント損失の組み合わせであるメトリックである&lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt;、凡例で報告されている（小さいほど良い）。校正損失は、ROCセグメントの傾きから導き出された経験的確率からの平均二乗偏差として定義されます。リファインメントロスは、最適なコストカーブの下の面積で測定される予想される最適なロスとして定義できます。</target>
        </trans-unit>
        <trans-unit id="7e7b7eaddffc735bc9fdc4de0de59e2218717940" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approximately seven times faster than fitting &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; since it has learned a sparse model using only approximately 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">次の図は、人工データセットの&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;を比較しています。これは、正弦波のターゲット関数と5つおきのデータポイントに追加される強いノイズで構成されています。&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;の学習モデルがプロットされ、RBFカーネルの複雑さ/正則化と帯域幅の両方がグリッド検索を使用して最適化されています。学習した機能は非常に似ています。ただし、&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; の&lt;/a&gt;フィッティングは&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; の&lt;/a&gt;フィッティングよりも約7倍高速です（どちらもグリッド検索を使用）。ただし、&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; を使用&lt;/a&gt;すると、100000の目標値の予測が3倍以上高速になります。 100個のトレーニングデータポイントの約1/3のみをサポートベクターとして使用してスパースモデルを学習したためです。</target>
        </trans-unit>
        <trans-unit id="2b3bbe94b0947452c8fc360afc3d0e6f8832ae22" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approx. seven times faster than fitting &lt;code&gt;SVR&lt;/code&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">次の図は、正弦波のターゲット関数と5番目ごとのデータ&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;追加された強いノイズで構成される人工データセットのKernelRidgeと &lt;code&gt;SVR&lt;/code&gt; を比較しています。&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;と &lt;code&gt;SVR&lt;/code&gt; の学習モデルがプロットされ、RBFカーネルの複雑さ/正規化と帯域幅の両方がグリッド検索を使用して最適化されています。学習した機能は非常に似ています。ただし、&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; の&lt;/a&gt;フィッティングは約です。 &lt;code&gt;SVR&lt;/code&gt; のフィッティングよりも7倍高速です（両方ともグリッド検索を使用）。ただし、SVRを使用すると、約100,000だけを使用してスパースモデルが学習されるため、100,000のターゲット値の予測は3倍以上速くなります。サポートベクターとしての100トレーニングデータポイントの1/3。</target>
        </trans-unit>
        <trans-unit id="7f576baedfe43e5a873ff30c0a921c0894cd8a99" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zero entries in the coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yield scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">次の図は、単純なLassoまたはMultiTaskLassoで得られた係数行列Wの非ゼロ項目の位置を比較したものです。Lassoの推定では、散在した非ゼロが得られますが、MultiTaskLassoの非ゼロは完全な列になっています。</target>
        </trans-unit>
        <trans-unit id="b11b5ffd2f0d8dbd878722867b9b243b2e73a5ea" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zeros in W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yields scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">次の図は、単純なLassoとMultiTaskLassoで得られたWの非ゼロの位置を比較したものです。Lassoの推定では、散在した非ゼロが得られますが、MultiTaskLassoの非ゼロは完全な列になります。</target>
        </trans-unit>
        <trans-unit id="c66df17d5ba5efe635efde7f93de1fc62e75c222" translate="yes" xml:space="preserve">
          <source>The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">次の図は、正弦波のターゲット関数と強いノイズで構成される人工データセットの両方の方法を示しています。この図は、周期関数の学習に適したExpSineSquaredカーネルに基づいて、KRRとGPRの学習モデルを比較します。カーネルのハイパーパラメーターは、カーネルの滑らかさ（length_scale）と周期性（周期性）を制御します。さらに、データのノイズレベルは、カーネル内の追加のWhiteKernelコンポーネントとKRRの正則化パラメーターalphaによってGPRによって明示的に学習されます。</target>
        </trans-unit>
        <trans-unit id="7af807be2b2e68501b3d7cc153c6244aca835299" translate="yes" xml:space="preserve">
          <source>The following guide focuses on &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, which might be preferred for small sample sizes since binning may lead to split points that are too approximate in this setting.</source>
          <target state="translated">次のガイドでは、&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; に&lt;/a&gt;焦点を当てています。これらは、ビニングによって分割点がこの設定で近似しすぎる可能性があるため、サンプルサイズが小さい場合に適しています。</target>
        </trans-unit>
        <trans-unit id="c2a01fd3fe0b5b9cd28910783764aacd71509b19" translate="yes" xml:space="preserve">
          <source>The following illustrate the probability density functions of the target before and after applying the logarithmic functions.</source>
          <target state="translated">以下に、対数関数を適用する前と適用後の対象の確率密度関数を説明する。</target>
        </trans-unit>
        <trans-unit id="b0293fdb9cc21af9db21d1f9f61ccde2a4565147" translate="yes" xml:space="preserve">
          <source>The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.</source>
          <target state="translated">下の画像は、タヌキの顔の画像の一部から抽出した4×4ピクセルの画像パッチから学習した辞書がどのように見えるかを示しています。</target>
        </trans-unit>
        <trans-unit id="a988a0a50289dff96522065b976c597b6e4e63e8" translate="yes" xml:space="preserve">
          <source>The following image shows on the data above the estimated probability using a Gaussian naive Bayes classifier without calibration, with a sigmoid calibration and with a non-parametric isotonic calibration. One can observe that the non-parametric model provides the most accurate probability estimates for samples in the middle, i.e., 0.5.</source>
          <target state="translated">次の画像は、校正なし、シグモイド校正あり、ノンパラメトリック等張校正ありのガウスナイーブベイズ分類器を用いた推定確率を上のデータ上に示しています。ノンパラメトリックモデルが、中間の標本、すなわち0.5の標本に対して最も正確な確率推定値を提供していることがわかります。</target>
        </trans-unit>
        <trans-unit id="b45c55863d114ed9459a8475837831820ff65766" translate="yes" xml:space="preserve">
          <source>The following images demonstrate the benefit of probability calibration. The first image present a dataset with 2 classes and 3 blobs of data. The blob in the middle contains random samples of each class. The probability for the samples in this blob should be 0.5.</source>
          <target state="translated">以下の画像は、確率校正の利点を示しています。最初の画像は、2つのクラスと3つのブロブからなるデータセットを示しています。真ん中のブロブには、各クラスのランダムなサンプルが含まれています。このブロブのサンプルの確率は0.5でなければなりません。</target>
        </trans-unit>
        <trans-unit id="ba49479bf1cc39dce48fe5a925ee39da6796af0a" translate="yes" xml:space="preserve">
          <source>The following lists the string metric identifiers and the associated distance metric classes:</source>
          <target state="translated">以下に、文字列メトリック識別子と関連する距離メトリッククラスの一覧を示します。</target>
        </trans-unit>
        <trans-unit id="807c0e6975f025a025e2951d9c9164c1f3454c71" translate="yes" xml:space="preserve">
          <source>The following loss functions are supported and can be specified using the parameter &lt;code&gt;loss&lt;/code&gt;:</source>
          <target state="translated">以下の損失関数がサポートされており、パラメーター &lt;code&gt;loss&lt;/code&gt; を使用して指定できます。</target>
        </trans-unit>
        <trans-unit id="f56ebb3690526678e7e0211eddd43dd69c89de3e" translate="yes" xml:space="preserve">
          <source>The following parts are parallelized:</source>
          <target state="translated">以下の部分を並列化しています。</target>
        </trans-unit>
        <trans-unit id="69eaa3387fa8263cde077e4bb7511faccc7173e0" translate="yes" xml:space="preserve">
          <source>The following plot compares how well the probabilistic predictions of different classifiers are calibrated, using &lt;a href=&quot;generated/sklearn.calibration.calibration_curve#sklearn.calibration.calibration_curve&quot;&gt;&lt;code&gt;calibration_curve&lt;/code&gt;&lt;/a&gt;. The x axis represents the average predicted probability in each bin. The y axis is the &lt;em&gt;fraction of positives&lt;/em&gt;, i.e. the proportion of samples whose class is the positive class (in each bin).</source>
          <target state="translated">次のプロットは、calibration_curveを使用して、さまざまな分類器の確率的予測がどの程度適切に調整されているかを比較してい&lt;a href=&quot;generated/sklearn.calibration.calibration_curve#sklearn.calibration.calibration_curve&quot;&gt; &lt;code&gt;calibration_curve&lt;/code&gt; &lt;/a&gt;。x軸は、各ビンの平均予測確率を表します。y軸は、&lt;em&gt;陽性&lt;/em&gt;の割合、つまり、クラスが陽性クラスであるサンプルの割合です（各ビン内）。</target>
        </trans-unit>
        <trans-unit id="70624e811991b8fac875c2f0dd0db6a8fb2e1e2b" translate="yes" xml:space="preserve">
          <source>The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.</source>
          <target state="translated">以下のプロットは、クラスタ数とサンプル数が様々なクラスタリング性能評価指標に与える影響を示しています。</target>
        </trans-unit>
        <trans-unit id="661cd7d1cca4025c0fd4f4a1e938e7140f6ef24b" translate="yes" xml:space="preserve">
          <source>The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; differ slightly from the standard textbook notation that defines the idf as</source>
          <target state="translated">以下のセクションには、tf-idfsが正確に計算される方法と、scikit-learnの&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; で&lt;/a&gt;計算されるtf-idfsが、idfを</target>
        </trans-unit>
        <trans-unit id="78ad5f101e233b65633dffc5c68bec29c1eae0b0" translate="yes" xml:space="preserve">
          <source>The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.</source>
          <target state="translated">以下のセクションでは、異なるクロスバリデーション戦略に応じてデータセットの分割を生成するために使用できるインデックスを生成するためのユーティリティをリストアップします。</target>
        </trans-unit>
        <trans-unit id="999c74375c41b701ce62b520b704cf4b739a66b1" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean feature value of the two nearest neighbors of samples with missing values:</source>
          <target state="translated">次のスニペットは、欠測値を持つサンプルの2つの最近傍の平均特徴値を使用して、 &lt;code&gt;np.nan&lt;/code&gt; としてエンコードされた欠測値を置き換える方法を示しています。</target>
        </trans-unit>
        <trans-unit id="5c75d78bf5e5b93a438a72e22f9eee2db09a82ed" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean value of the columns (axis 0) that contain the missing values:</source>
          <target state="translated">次のスニペットは、欠損値を含む列（軸0）の平均値を使用して、欠損値を &lt;code&gt;np.nan&lt;/code&gt; としてエンコードする方法を示しています。</target>
        </trans-unit>
        <trans-unit id="35e4b9d817818dd84b86f006c91682ff56db6021" translate="yes" xml:space="preserve">
          <source>The following subsections are only rough guidelines: the same estimator can fall into multiple categories, depending on its parameters.</source>
          <target state="translated">以下のサブセクションはあくまでも大まかなガイドラインであり、同じ推定値でもパラメータによっては複数のカテゴリに分類されることがあります。</target>
        </trans-unit>
        <trans-unit id="b6c9fc0377f0a1e8adb0ffabc9be6ea63e8b1dd6" translate="yes" xml:space="preserve">
          <source>The following table lists some specific EDMs and their unit deviance (all of these are instances of the Tweedie family):</source>
          <target state="translated">以下の表は、いくつかの特定のEDMとその単位逸脱を示しています(これらはすべてTweedieファミリーのインスタンスです)。</target>
        </trans-unit>
        <trans-unit id="8aa5b699e7b8122b521ff7ffa5779f4500a14f28" translate="yes" xml:space="preserve">
          <source>The following table summarizes the penalties supported by each solver:</source>
          <target state="translated">以下の表は、各ソルバーがサポートするペナルティをまとめたものです。</target>
        </trans-unit>
        <trans-unit id="9d55ab5466e0380dae4e49e3b2bfc5d99624d130" translate="yes" xml:space="preserve">
          <source>The following toy example demonstrates how the model ignores the samples with zero sample weights:</source>
          <target state="translated">次のおもちゃの例は、サンプル重みがゼロのサンプルをモデルが無視する方法を示しています。</target>
        </trans-unit>
        <trans-unit id="46781e9d9b616f25e94b9f1718ce2c06cffa693e" translate="yes" xml:space="preserve">
          <source>The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.</source>
          <target state="translated">以下の2つの参考文献は、scikit-learnの座標降下ソルバーで使用される反復と、収束制御に使用される二元性ギャップ計算について説明しています。</target>
        </trans-unit>
        <trans-unit id="5c7ef4cd14485e27e01a8eae72d8f974a84e41ab" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;によって学習されたモデルの形式は、サポートベクター回帰（&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;）と同じです。ただし、さまざまな損失関数が使用されます。KRRは二乗誤差損失を使用し、サポートベクター回帰は\（\ epsilon \）に依存しない損失を使用し、両方ともl2正則化と組み合わせます。&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;とは対照的に、&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; の&lt;/a&gt;フィッティングは閉じた形式で実行でき、通常、中規模のデータセットの方が高速です。一方、学習されたモデルはスパースではないため、予測時に\（\ epsilon&amp;gt; 0 \）のスパースモデルを学習する&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;よりも低速です。</target>
        </trans-unit>
        <trans-unit id="d492404a63c6d65da32b70e6dcf87e75a94b6a4d" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;code&gt;SVR&lt;/code&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;code&gt;SVR&lt;/code&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;によって学習されたモデルの形式は、サポートベクトル回帰（ &lt;code&gt;SVR&lt;/code&gt; ）と同じです。ただし、異なる損失関数が使用されます。KRRは二乗誤差損失を使用しますが、サポートベクトル回帰は\（\ epsilon \）に依存しない損失を使用します。どちらもl2正則化と組み合わせて使用​​されます。 &lt;code&gt;SVR&lt;/code&gt; とは対照的に、&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; の&lt;/a&gt;フィッティングは閉じた形式で行うことができ、通常、中規模のデータセットの方が高速です。一方、学習されたモデルは非スパースであるため、\（\ epsilon&amp;gt; 0 \）のスパースモデルを予測時に学習するSVRよりも低速です。</target>
        </trans-unit>
        <trans-unit id="cb6eb14fd1b6ffac38e7d6c15d36b7076beafa85" translate="yes" xml:space="preserve">
          <source>The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon &amp;gt; 0, at prediction-time.</source>
          <target state="translated">KRRによって学習されたモデルの形式は、サポートベクトル回帰（SVR）と同じです。ただし、異なる損失関数が使用されます。KRRでは二乗誤差損失が使用され、サポートベクトル回帰ではイプシロンに依存しない損失が使用されます。両方ともl2正則化と組み合わせて使用​​されます。 SVRとは対照的に、KRRモデルの近似は閉じた形式で行うことができ、通常、中規模のデータセットの方が高速です。一方、学習されたモデルはスパースではないため、&amp;epsilon;が0より大きいスパースモデルを予測時に学習するSVRよりも低速です。</target>
        </trans-unit>
        <trans-unit id="785bfd19f7d4d298d07ec2673131bad97bee7c7c" translate="yes" xml:space="preserve">
          <source>The form of these kernels is as follows:</source>
          <target state="translated">これらのカーネルの形態は以下の通りである。</target>
        </trans-unit>
        <trans-unit id="cecc632cfa798838e6560a2f37e713a3ae87c1a6" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as idf(t) = log [ n / df(t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]).</source>
          <target state="translated">ドキュメントセット内のドキュメントdの項tのtf-idfを計算するために使用される式は、tf-idf（t、d）= tf（t、d）* idf（t）であり、idfが計算されます。 as idf（t）= log [n / df（t）] + 1（ &lt;code&gt;smooth_idf=False&lt;/code&gt; の場合）。ここで、nはドキュメントセット内のドキュメントの総数、df（t）はtのドキュメント頻度です。ドキュメント頻度は、用語tを含むドキュメントセット内のドキュメントの数です。上記の式のidfに「1」を追加すると、idfがゼロの用語、つまりトレーニングセット内のすべてのドキュメントで発生する用語が完全に無視されるわけではありません。 （上記のidf式は、idfをidf（t）= log [n /（df（t）+ 1）]として定義する標準の教科書表記とは異なることに注意してください）。</target>
        </trans-unit>
        <trans-unit id="99fa4cbad7a403537a73165a2b1d717c58bb9b6b" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).</source>
          <target state="translated">項tのtf-idfを計算するために使用される式は、tf-idf（d、t）= tf（t）* idf（d、t）であり、idfはidf（d、t）= logとして計算されます[n / df（d、t）] + 1（ &lt;code&gt;smooth_idf=False&lt;/code&gt; の場合）、ここでnはドキュメントの総数、df（d、t）はドキュメントの頻度です。ドキュメントの頻度は、用語tを含むドキュメントdの数です。上記の式でidfに「1」を追加することの効果は、idfがゼロの項、つまりトレーニングセット内のすべてのドキュメントで発生する項が完全に無視されないことです。 （上記のidf式は、idfをidf（d、t）= log [n /（df（d、t）+ 1）]として定義する標準の教科書表記とは異なることに注意してください。</target>
        </trans-unit>
        <trans-unit id="2757038bccf5826fff274cfe8684f779a3893302" translate="yes" xml:space="preserve">
          <source>The formula used here does not correspond to the one given in the article. In the original article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn&amp;rsquo;t affect the value of the estimator.</source>
          <target state="translated">ここで使用されている式は、記事に記載されている式に対応していません。元の記事では、式（23）は、分子と分母の両方で2 / pにTrace（cov * cov）が乗算されることを示していますが、この操作は省略されています。大きなpの場合、2 / pの値が非常に小さいためです推定器の値に影響を与えないこと。</target>
        </trans-unit>
        <trans-unit id="25426ebad0f81610af376f208a1d7f7cdb7c96c5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. &lt;code&gt;subsample&lt;/code&gt; interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;. Choosing &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; leads to a reduction of variance and an increase in bias.</source>
          <target state="translated">個々の基本学習者をフィッティングするために使用されるサンプルの割合。1.0より小さい場合、これは確率的勾配ブーストになります。 &lt;code&gt;subsample&lt;/code&gt; はパラメーター &lt;code&gt;n_estimators&lt;/code&gt; と相互作用します。 &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; 選択すると、分散が減少し、バイアスが増加します。</target>
        </trans-unit>
        <trans-unit id="568fbbbfe83b2ef390104a99310641880cb62ce5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.</source>
          <target state="translated">各無作為化デザインで使用するサンプルの割合。0から1の間である必要があります。 1の場合、すべてのサンプルが使用されます。</target>
        </trans-unit>
        <trans-unit id="52b16a6d5efc38a8b1d594773d860718286a7706" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class are randomly exchanged. Larger values introduce noise in the labels and make the classification task harder.</source>
          <target state="translated">クラスがランダムに交換されるサンプルの割合。値が大きくなるとラベルにノイズが入り、分類作業が難しくなります。</target>
        </trans-unit>
        <trans-unit id="3e4c940191ac2ce629537cccbeb4e1290fc15133" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder. Note that the default setting flip_y &amp;gt; 0 might lead to less than n_classes in y in some cases.</source>
          <target state="translated">クラスがランダムに割り当てられているサンプルの割合。値を大きくすると、ラベルにノイズが発生し、分類タスクが難しくなります。デフォルト設定のflip_y&amp;gt; 0は、場合によってはyのn_classes未満になる可能性があることに注意してください。</target>
        </trans-unit>
        <trans-unit id="2b8586795acd1de1d253165ce5ff74dbee8020e0" translate="yes" xml:space="preserve">
          <source>The free parameters in the model are C and epsilon.</source>
          <target state="translated">モデルの自由パラメータはCとεです。</target>
        </trans-unit>
        <trans-unit id="852901dbf93c375963721fc84a4888e6b45c78dd" translate="yes" xml:space="preserve">
          <source>The full description of the dataset</source>
          <target state="translated">データセットの完全な説明</target>
        </trans-unit>
        <trans-unit id="d9127a02d65d598dd00fad372a9aa50b138962b6" translate="yes" xml:space="preserve">
          <source>The full description of the dataset.</source>
          <target state="translated">データセットの完全な説明。</target>
        </trans-unit>
        <trans-unit id="19877feb57bb09f6d25fbaaa466f42d45ca4d944" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt;&lt;code&gt;img_to_graph&lt;/code&gt;&lt;/a&gt; returns such a matrix from a 2D or 3D image. Similarly, &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;grid_to_graph&lt;/code&gt;&lt;/a&gt; build a connectivity matrix for images given the shape of these image.</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt; &lt;code&gt;img_to_graph&lt;/code&gt; &lt;/a&gt;は、2Dまたは3D画像からそのような行列を返します。同様に、&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;grid_to_graph&lt;/code&gt; &lt;/a&gt;は、画像の形状が指定された画像の接続性マトリックスを作成します。</target>
        </trans-unit>
        <trans-unit id="0a9dad7d233dabf5b0bcafb3e330d5014a951e15" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt;&lt;code&gt;lasso_path&lt;/code&gt;&lt;/a&gt; is useful for lower-level tasks, as it computes the coefficients along the full path of possible values.</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt; &lt;code&gt;lasso_path&lt;/code&gt; &lt;/a&gt;は、可能な値のフルパスに沿って係数を計算するため、低レベルのタスクに役立ちます。</target>
        </trans-unit>
        <trans-unit id="6bbdd98a7d5d87ddd3bbdfb8569481a5e5e04495" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt;&lt;code&gt;cohen_kappa_score&lt;/code&gt;&lt;/a&gt; computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen&amp;rsquo;s kappa&lt;/a&gt; statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt; &lt;code&gt;cohen_kappa_score&lt;/code&gt; は、&lt;/a&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;コーエンのカッパ&lt;/a&gt;統計を計算します。この測定は、分類子とグラウンドトゥルースではなく、異なる人間のアノテーターによるラベリングを比較することを目的としています。</target>
        </trans-unit>
        <trans-unit id="93f21bf6b976a0d8004246b9140ac3f9b0310fba" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt;&lt;code&gt;laplacian_kernel&lt;/code&gt;&lt;/a&gt; is a variant on the radial basis function kernel defined as:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt; &lt;code&gt;laplacian_kernel&lt;/code&gt; &lt;/a&gt;は、次のように定義された動径基底関数カーネルのバリアントです。</target>
        </trans-unit>
        <trans-unit id="3fe1ad7d2ae4a1d31d30a7b5c63e81d53b7568bb" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt; computes the linear kernel, that is, a special case of &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;degree=1&lt;/code&gt; and &lt;code&gt;coef0=0&lt;/code&gt; (homogeneous). If &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are column vectors, their linear kernel is:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt;は線形カーネルを計算します。つまり、 &lt;code&gt;degree=1&lt;/code&gt; 、 &lt;code&gt;coef0=0&lt;/code&gt; （同種）の&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; の&lt;/a&gt;特殊なケースです。場合 &lt;code&gt;x&lt;/code&gt; と &lt;code&gt;y&lt;/code&gt; は列ベクトルで、その線形カーネルは、次のとおりです。</target>
        </trans-unit>
        <trans-unit id="72295044331d14363e1e1fe37cea504256d7c67f" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt;は、2つのベクトル間の次数dの多項式カーネルを計算します。多項式カーネルは、2つのベクトル間の類似性を表します。概念的には、多項式カーネルは、同じ次元のベクトル間の類似性だけでなく、次元全体の類似性も考慮します。これを機械学習アルゴリズムで使用すると、機能の相互作用を考慮することができます。</target>
        </trans-unit>
        <trans-unit id="2f5b051b13aa3478b0b4a17587fa0bec1e6b87b4" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt;&lt;code&gt;rbf_kernel&lt;/code&gt;&lt;/a&gt; computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt; &lt;code&gt;rbf_kernel&lt;/code&gt; &lt;/a&gt;は、2つのベクトル間の動径基底関数（RBF）カーネルを計算します。このカーネルは次のように定義されています。</target>
        </trans-unit>
        <trans-unit id="a4ac87fe4bd82908551f017a1e984b845dfe00ca" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt;&lt;code&gt;sigmoid_kernel&lt;/code&gt;&lt;/a&gt; computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt; &lt;code&gt;sigmoid_kernel&lt;/code&gt; &lt;/a&gt;は、2つのベクトル間のシグモイドカーネルを計算します。シグモイドカーネルは、双曲線正接、または多層パーセプトロンとも呼ばれます（ニューラルネットワーク分野では、ニューロン活性化関数としてよく使用されるため）。次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="2e327cd188c9064345dfa4a6a6764985ae429bc7" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;receiver operating characteristic curve, or ROC curve&lt;/a&gt;. Quoting Wikipedia :</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt;は、&lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;受信者動作特性曲線、またはROC曲線を&lt;/a&gt;計算します。ウィキペディアの引用：</target>
        </trans-unit>
        <trans-unit id="a52cb65c85990e02ab77ee98f8132a1c3b7a7b6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; has a similar interface to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt;はcross_val_scoreと同様のインターフェースを&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;いますが、入力内の各要素について、テストセット内にあるときにその要素に対して取得された予測を返します。すべての要素を1回だけテストセットに割り当てる相互検証戦略のみを使用できます（そうでない場合、例外が発生します）。</target>
        </trans-unit>
        <trans-unit id="9da333e152484b3ac872458a2a577c489d5042de" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt;&lt;code&gt;validation_curve&lt;/code&gt;&lt;/a&gt; can help in this case:</source>
          <target state="translated">この場合、関数&lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt; &lt;code&gt;validation_curve&lt;/code&gt; &lt;/a&gt;が役立ちます。</target>
        </trans-unit>
        <trans-unit id="ef1bfedb8d96897b52fb746234adf67b6ec8f0b2" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;normalize&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset, either using the &lt;code&gt;l1&lt;/code&gt; or &lt;code&gt;l2&lt;/code&gt; norms:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt; &lt;code&gt;normalize&lt;/code&gt; &lt;/a&gt;は、 &lt;code&gt;l1&lt;/code&gt; または &lt;code&gt;l2&lt;/code&gt; ノルムを使用して、単一の配列のようなデータセットに対してこの操作を実行する迅速かつ簡単な方法を提供します。</target>
        </trans-unit>
        <trans-unit id="621182b188f45e3dd8884ee1d03015ee530041c8" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset:</source>
          <target state="translated">関数&lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt;は、単一の配列のようなデータセットに対してこの操作を実行するための迅速かつ簡単な方法を提供します。</target>
        </trans-unit>
        <trans-unit id="6ffd4cf30ad5a8e8b6ec54710ec8f9345471233e" translate="yes" xml:space="preserve">
          <source>The function &lt;code&gt;plot_regression_results&lt;/code&gt; is used to plot the predicted and true targets.</source>
          <target state="translated">関数 &lt;code&gt;plot_regression_results&lt;/code&gt; は、予測された真のターゲットをプロットするために使用されます。</target>
        </trans-unit>
        <trans-unit id="14656519a034ad24c06a19460128dde85e00e72d" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;および&lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]で&lt;/a&gt;説明されているように、この関数はk最近傍距離からのエントロピー推定に基づくノンパラメトリック手法に依存しています。どちらの方法も、元々&lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]で&lt;/a&gt;提案されたアイデアに基づいています。</target>
        </trans-unit>
        <trans-unit id="cf1d60e35345e80734f9e0bef9342c1dede910a1" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;および&lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]で&lt;/a&gt;説明されているように、この関数はk最近傍距離からのエントロピー推定に基づくノンパラメトリック手法に依存しています。どちらの方法も、元々&lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]で&lt;/a&gt;提案されたアイデアに基づいています。</target>
        </trans-unit>
        <trans-unit id="fb8314efd5d7c36c4d228413206cd5331b82e456" translate="yes" xml:space="preserve">
          <source>The function requires either the argument &lt;code&gt;grid&lt;/code&gt; which specifies the values of the target features on which the partial dependence function should be evaluated or the argument &lt;code&gt;X&lt;/code&gt; which is a convenience mode for automatically creating &lt;code&gt;grid&lt;/code&gt; from the training data. If &lt;code&gt;X&lt;/code&gt; is given, the &lt;code&gt;axes&lt;/code&gt; value returned by the function gives the axis for each target feature.</source>
          <target state="translated">関数には、部分依存関数を評価する必要があるターゲットフィーチャの値を指定する引数 &lt;code&gt;grid&lt;/code&gt; 、またはトレーニングデータから &lt;code&gt;grid&lt;/code&gt; を自動的に作成するための便利なモードである引数 &lt;code&gt;X&lt;/code&gt; が必要です。場合 &lt;code&gt;X&lt;/code&gt; が与えられ、 &lt;code&gt;axes&lt;/code&gt; 関数によって返される値は、各対象地物のための軸を与えます。</target>
        </trans-unit>
        <trans-unit id="d2fedea237bdc334f6322c4692b523df3b5f6fbf" translate="yes" xml:space="preserve">
          <source>The function to be decorated</source>
          <target state="translated">飾る機能</target>
        </trans-unit>
        <trans-unit id="f7fc9606332ff37416d86060e52aa56595a5e3ce" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;friedman_mse&amp;rdquo; for the mean squared error with improvement score by Friedman, &amp;ldquo;mse&amp;rdquo; for mean squared error, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error. The default value of &amp;ldquo;friedman_mse&amp;rdquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">分割の品質を測定する関数。サポートされる基準は、Friedmanによる改善スコア付きの平均二乗誤差の「friedman_mse」、平均二乗誤差の「mse」、および平均絶対誤差の「mae」です。「friedman_mse」のデフォルト値は、場合によってはより良い近似を提供できるため、一般的には最適です。</target>
        </trans-unit>
        <trans-unit id="8c0bede693f27f1257cedfa964bd89180e6bfada" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain.</source>
          <target state="translated">分割の品質を測定する関数。サポートされている基準は、ジニの不純さを表す「ジニ」と情報ゲインを表す「エントロピー」です。</target>
        </trans-unit>
        <trans-unit id="03e746b06910ee8c07bc239c0b780e5294140dfb" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain. Note: this parameter is tree-specific.</source>
          <target state="translated">分割の品質を測定する関数。サポートされている基準は、ジニの不純さを表す「ジニ」と情報ゲインを表す「エントロピー」です。注：このパラメーターはツリー固有です。</target>
        </trans-unit>
        <trans-unit id="c211c5fb9289c19b7ab7fb609c5f42d0fd7fb417" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, &amp;ldquo;friedman_mse&amp;rdquo;, which uses mean squared error with Friedman&amp;rsquo;s improvement score for potential splits, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.</source>
          <target state="translated">分割の品質を測定する関数。サポートされる基準は、平均二乗誤差の「mse」です。これは、特徴選択基準としての分散減少に等しく、各末端ノードの平均「friedman_mse」を使用してL2損失を最小化します。分割、および平均絶対誤差の「前」。これは、各端末ノードの中央値を使用してL1損失を最小化します。</target>
        </trans-unit>
        <trans-unit id="bf422f8c78875448c6e34d2c008baf4a5621c47d" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error.</source>
          <target state="translated">分割の品質を測定する関数。サポートされる基準は、特徴の選択基準としての分散減少に等しい平均二乗誤差の「mse」と平均絶対誤差の「mae」です。</target>
        </trans-unit>
        <trans-unit id="8599be908b4b8dfee70cafa139c14ed8be2f9fde" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;lsquo;friedman_mse&amp;rsquo; for the mean squared error with improvement score by Friedman, &amp;lsquo;mse&amp;rsquo; for mean squared error, and &amp;lsquo;mae&amp;rsquo; for the mean absolute error. The default value of &amp;lsquo;friedman_mse&amp;rsquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">分割の品質を測定する関数。サポートされている基準は、フリードマンによる改善スコアを伴う平均二乗誤差の「friedman_mse」、平均二乗誤差の「mse」、および平均絶対誤差の「mae」です。'friedman_mse'のデフォルト値は、場合によってはより適切な近似値を提供できるため、一般的に最適です。</target>
        </trans-unit>
        <trans-unit id="8b3c9548b1b42f5af71c7bfe5f885c712284a1e6" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;, or a tuple of such objects.</source>
          <target state="translated">距離行列の各チャンクに適用され、必要な値に削減される関数。 &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; は繰り返し呼び出されます。ここで、 &lt;code&gt;D_chunk&lt;/code&gt; は、ペアの距離行列の隣接する垂直スライスで、行 &lt;code&gt;start&lt;/code&gt; から始まります。長さ &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; 配列、リスト、スパース行列、またはそのようなオブジェクトのタプルを返す必要があります。</target>
        </trans-unit>
        <trans-unit id="0114f911a9d8b12f31f5da2c60626b44d9e0ba26" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return one of: None; an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;; or a tuple of such objects. Returning None is useful for in-place operations, rather than reductions.</source>
          <target state="translated">距離行列の各チャンクに適用され、必要な値に縮小する関数。 &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; は繰り返し呼び出されます。ここで、 &lt;code&gt;D_chunk&lt;/code&gt; は、行の &lt;code&gt;start&lt;/code&gt; から始まるペアワイズ距離行列の連続する垂直スライスです。次のいずれかを返す必要があります。なし。長さ &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; 配列、リスト、またはスパース行列。またはそのようなオブジェクトのタプル。Noneを返すことは、削減ではなく、インプレース操作に役立ちます。</target>
        </trans-unit>
        <trans-unit id="34f983a96e6e1ba7a36ae52fb70df739e3a5aca9" translate="yes" xml:space="preserve">
          <source>The functional form of the G function used in the approximation to neg-entropy. Could be either &amp;lsquo;logcosh&amp;rsquo;, &amp;lsquo;exp&amp;rsquo;, or &amp;lsquo;cube&amp;rsquo;. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:</source>
          <target state="translated">否定エントロピーの近似で使用されるG関数の関数形。「logcosh」、「exp」、または「cube」のいずれかです。独自の機能を提供することもできます。関数とその派生物の値を含むタプルをポイントで返す必要があります。例：</target>
        </trans-unit>
        <trans-unit id="f3580ae3ff67a44925f6075f1fc46034cd1f3d14" translate="yes" xml:space="preserve">
          <source>The generated array.</source>
          <target state="translated">生成された配列。</target>
        </trans-unit>
        <trans-unit id="22dd341f8c92381f47326cd6fbe4fff46dd59a6b" translate="yes" xml:space="preserve">
          <source>The generated matrix.</source>
          <target state="translated">生成された行列。</target>
        </trans-unit>
        <trans-unit id="b670a7959baa25f4a9c290d5df951114cede8bad" translate="yes" xml:space="preserve">
          <source>The generated samples.</source>
          <target state="translated">生成されたサンプル。</target>
        </trans-unit>
        <trans-unit id="61a0705074aa05bf429a2ff6dd9f09842d1c86f2" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">センターを初期化するために使用されるジェネレータ。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="17ea6c727e52953fb5e03ae24f1b3fcbc132ab70" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">センターを初期化するために使用されるジェネレータ。複数の関数呼び出しにわたって再現可能な出力のためにintを渡します。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="473c7ed23812d4bf8fcf6c97ab0551b7301ccf95" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">コードブックの初期化に使用されるジェネレーター。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="a0fb8deb3b7c972152c6c4cb8a6e10d0c386fcd5" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">コードブックを初期化するために使用されるジェネレーター。複数の関数呼び出しにわたって再現可能な出力のためにintを渡します。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="4e831547e83750929880db075c40fc4aa61eb4b8" translate="yes" xml:space="preserve">
          <source>The generator used to randomize the design. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">設計をランダム化するために使用されるジェネレーター。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="cd84f22d9e60e3f86bb2455d78aaf109647d76b1" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select a subset of samples. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;sample_size is not None&lt;/code&gt;.</source>
          <target state="translated">サンプルのサブセットをランダムに選択するために使用されるジェネレーター。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;sample_size is not None&lt;/code&gt; 場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="36aa9d7d033388d006150bbc613bf9f61afb2f31" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">帯域幅推定のために入力ポイントからサンプルをランダムに選択するために使用されるジェネレーター。intを使用して、ランダム性を決定論的にします。&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="eebbb245bcabbcd7cdd05b74aa699ac85737839b" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">帯域幅推定のために入力ポイントからサンプルをランダムに選択するために使用されるジェネレータ。intを使用して、ランダム性を決定論的にします。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="ff752b880533b887a66601fd8d804091da8b44b7" translate="yes" xml:space="preserve">
          <source>The goal is to compare different estimators to see which one is best for the &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt; when using a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt;&lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt;&lt;/a&gt; estimator on the California housing dataset with a single value randomly removed from each row.</source>
          <target state="translated">目標は、カリフォルニアの住宅データセットで&lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt; &lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt; &lt;/a&gt;推定量を使用し、各行から1つの値をランダムに削除した場合に、さまざまな推定量を比較して、&lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt; &lt;/a&gt;に最適な推定量を確認することです。</target>
        </trans-unit>
        <trans-unit id="6c9c479a6511fc655c70a7c51ab0a85c04034b21" translate="yes" xml:space="preserve">
          <source>The goal is to measure the latency one can expect when doing predictions either in bulk or atomic (i.e. one by one) mode.</source>
          <target state="translated">目標は、バルクモードまたはアトミック(1つずつ)モードで予測を行う際に期待できる待ち時間を測定することです。</target>
        </trans-unit>
        <trans-unit id="95b3715b7309073ce23a5e1ae6bd1c83fb5ab128" translate="yes" xml:space="preserve">
          <source>The goal of &lt;strong&gt;ensemble methods&lt;/strong&gt; is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</source>
          <target state="translated">&lt;strong&gt;アンサンブル法&lt;/strong&gt;の目的は、特定の学習アルゴリズムで構築されたいくつかの基本推定量の予測を組み合わせて、単一の推定量に対する一般化可能性/ロバスト性を改善することです。</target>
        </trans-unit>
        <trans-unit id="a736cc9a27a226bf9910cdd3892bdfd0fe07d378" translate="yes" xml:space="preserve">
          <source>The goal of NCA is to learn an optimal linear transformation matrix of size &lt;code&gt;(n_components, n_features)&lt;/code&gt;, which maximises the sum over all samples \(i\) of the probability \(p_i\) that \(i\) is correctly classified, i.e.:</source>
          <target state="translated">NCAの目標は、サイズ &lt;code&gt;(n_components, n_features)&lt;/code&gt; 最適な線形変換行列を学習することです。これにより、\（i \）が正しく分類される確率\（p_i \）のすべてのサンプル\（i \）の合計が最大化されます。すなわち：</target>
        </trans-unit>
        <trans-unit id="4be143708c968fbd1f8e4e8288622de6f1251f81" translate="yes" xml:space="preserve">
          <source>The goal of this example is to analyze the graph of links inside wikipedia articles to rank articles by relative importance according to this eigenvector centrality.</source>
          <target state="translated">この例の目的は、wikipediaの記事内のリンクのグラフを分析して、この固有ベクトルの中心性に応じて相対的な重要度で記事をランク付けすることです。</target>
        </trans-unit>
        <trans-unit id="5095c3c5239296131fc48d25860f3771ab9b91e3" translate="yes" xml:space="preserve">
          <source>The goal of this example is to show intuitively how the metrics behave, and not to find good clusters for the digits. This is why the example works on a 2D embedding.</source>
          <target state="translated">この例の目的は、メトリクスがどのように振る舞うかを直感的に示すことであり、桁の良いクラスタを見つけることではありません。このため、この例は 2D エンベッディングで動作します。</target>
        </trans-unit>
        <trans-unit id="20bd11aa678c9e04777a40ec0f194ab6b7581aa6" translate="yes" xml:space="preserve">
          <source>The goal of this guide is to explore some of the main &lt;code&gt;scikit-learn&lt;/code&gt; tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics.</source>
          <target state="translated">このガイドの目的は、20の異なるトピックに関するテキストドキュメントのコレクション（ニュースグループの投稿）の分析という1つの実用的なタスクに関する主要な &lt;code&gt;scikit-learn&lt;/code&gt; ツールのいくつかを探索することです。</target>
        </trans-unit>
        <trans-unit id="c224302bdf144aacfdfc260a6638fe1107c2e8d4" translate="yes" xml:space="preserve">
          <source>The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</source>
          <target state="translated">与えられた文書内のトークンの生の出現頻度の代わりに tf-idf を使用する目的は、与えられたコーパス内で非常に頻繁に出現し、それゆえに経験的には学習コーパスのごく一部に出現する特徴よりも情報量が少ないトークンの影響をスケールダウンさせることです。</target>
        </trans-unit>
        <trans-unit id="7c4e551c63ec44d66aa3a175c12b51d46fb0ca3e" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when &lt;code&gt;eval_gradient&lt;/code&gt; is True.</source>
          <target state="translated">カーネルのハイパーパラメータに関するカーネルk（X、X）の勾配。 &lt;code&gt;eval_gradient&lt;/code&gt; がTrueの場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="3325084d0d02139eb921a2545ba48cff39657720" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</source>
          <target state="translated">カーネルのハイパーパラメータに対するカーネル k(X,X)の勾配.eval_gradientがTrueの場合のみ返される。</target>
        </trans-unit>
        <trans-unit id="1d28aeaa2593827c70ee0cbf35f8d5891fd13e08" translate="yes" xml:space="preserve">
          <source>The graph data is fetched from the DBpedia dumps. DBpedia is an extraction of the latent structured data of the Wikipedia content.</source>
          <target state="translated">グラフデータはDBpediaのダンプから取得します。DBpediaはWikipediaのコンテンツの潜在構造化データを抽出します。</target>
        </trans-unit>
        <trans-unit id="94a8641249b9d38c741294dd700ba9c013e60d15" translate="yes" xml:space="preserve">
          <source>The graph should contain only one connect component, elsewhere the results make little sense.</source>
          <target state="translated">グラフは1つの接続コンポーネントだけを含むべきであり、それ以外の場所では結果はほとんど意味をなさない。</target>
        </trans-unit>
        <trans-unit id="0aca214d3abec7143ce0e3db3703c7759829a517" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level Bayesian model:</source>
          <target state="translated">LDAのグラフィカルモデルは3階層ベイズモデルです。</target>
        </trans-unit>
        <trans-unit id="53c200c93d4342eee587d3c6f82bd52b691b78c8" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level generative model:</source>
          <target state="translated">LDAのグラフィカルモデルは、3レベルの生成モデルです。</target>
        </trans-unit>
        <trans-unit id="b70b496cfa90301063390d7d501e40e03165e505" translate="yes" xml:space="preserve">
          <source>The graphical model of an RBM is a fully-connected bipartite graph.</source>
          <target state="translated">RBMのグラフィカルモデルは、完全に接続された二部グラフです。</target>
        </trans-unit>
        <trans-unit id="43e8165a75805bcff11ce07ae2103365830550d8" translate="yes" xml:space="preserve">
          <source>The grid of &lt;code&gt;target_variables&lt;/code&gt; values for which the partial dependecy should be evaluated (either &lt;code&gt;grid&lt;/code&gt; or &lt;code&gt;X&lt;/code&gt; must be specified).</source>
          <target state="translated">部分的な依存関係を評価する必要がある &lt;code&gt;target_variables&lt;/code&gt; 値のグリッド（ &lt;code&gt;grid&lt;/code&gt; または &lt;code&gt;X&lt;/code&gt; を指定する必要があります）。</target>
        </trans-unit>
        <trans-unit id="1fb55e8edd66947697c84e91d6ac3fa247bd29e9" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting</source>
          <target state="translated">フィッティングに使用されるアルファのグリッド</target>
        </trans-unit>
        <trans-unit id="c8ce42c94fb7c4644fc397d481823f721280ec22" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio</source>
          <target state="translated">各 l1_ratio に対して、フィッティングに使用されるアルファのグリッド</target>
        </trans-unit>
        <trans-unit id="92edf0193fba09badba27bf7739525bce6da7601" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio.</source>
          <target state="translated">各l1_ratioのフィッティングに使用されるアルファのグリッド。</target>
        </trans-unit>
        <trans-unit id="22daba72217df04ec1af1a8a340277c45da4b54e" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting.</source>
          <target state="translated">フィッティングに使用されるアルファのグリッド。</target>
        </trans-unit>
        <trans-unit id="37c7f40c413d2aeb11b8645a838ea3c671b821b6" translate="yes" xml:space="preserve">
          <source>The grid points between 0 and 1: alpha/alpha_max</source>
          <target state="translated">0 と 1 の間の格子点:alpha/alpha_max</target>
        </trans-unit>
        <trans-unit id="1e90c7186240eacb6693334f5a2cb603522a3c95" translate="yes" xml:space="preserve">
          <source>The grid search instance behaves like a normal &lt;code&gt;scikit-learn&lt;/code&gt; model. Let&amp;rsquo;s perform the search on a smaller subset of the training data to speed up the computation:</source>
          <target state="translated">グリッド検索インスタンスは、通常の &lt;code&gt;scikit-learn&lt;/code&gt; モデルのように動作します。計算を高速化するために、トレーニングデータのより小さなサブセットで検索を実行してみましょう。</target>
        </trans-unit>
        <trans-unit id="0fb72e7d32db09c24209afb30ebd0c3ef9469a12" translate="yes" xml:space="preserve">
          <source>The grid search provided by &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; exhaustively generates candidates from a grid of parameter values specified with the &lt;code&gt;param_grid&lt;/code&gt; parameter. For instance, the following &lt;code&gt;param_grid&lt;/code&gt;:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;によって提供されるグリッド検索は、 &lt;code&gt;param_grid&lt;/code&gt; パラメーターで指定されたパラメーター値のグリッドから候補を徹底的に生成します。たとえば、次の &lt;code&gt;param_grid&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="5d877967ec11cd12214237a4561547e48123553d" translate="yes" xml:space="preserve">
          <source>The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.</source>
          <target state="translated">メトリックを選択するためのガイドラインは、異なるクラスのサンプル間の距離を最大化し、各クラス内での距離を最小化するものを使用することです。</target>
        </trans-unit>
        <trans-unit id="42b20fdbf19bf6fb28b9336fb1c931e7ebd9ff54" translate="yes" xml:space="preserve">
          <source>The handwritten digit dataset has 1797 total points. The model will be trained using all points, but only 30 will be labeled. Results in the form of a confusion matrix and a series of metrics over each class will be very good.</source>
          <target state="translated">手書きの数字データセットには1797点の合計点があります。モデルはすべてのポイントを使用して訓練されますが、ラベル付けされるのは30点のみです。混乱行列と各クラスの一連のメトリクスの形での結果は非常に良いものになるでしょう。</target>
        </trans-unit>
        <trans-unit id="1dbebc05eff701a160c5a2149017afcbb50998d6" translate="yes" xml:space="preserve">
          <source>The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">採用されているハッシュ関数は、Murmurhash3の符号付き32ビット版です。</target>
        </trans-unit>
        <trans-unit id="86a9e346ee809bf0b27001ea98677db7fe898a9e" translate="yes" xml:space="preserve">
          <source>The higher &lt;code&gt;p&lt;/code&gt; the less weight is given to extreme deviations between true and predicted targets.</source>
          <target state="translated">&lt;code&gt;p&lt;/code&gt; が高いほど、真のターゲットと予測されたターゲットの間の極端な偏差に与えられる重みは小さくなります。</target>
        </trans-unit>
        <trans-unit id="6ddf3e17ce9800f43a67078b41067ddc0d8c6a4c" translate="yes" xml:space="preserve">
          <source>The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex.</source>
          <target state="translated">濃度が高いほど中央に多くの質量が置かれ、より多くの成分が活性化され、濃度が低いパラメータは単純体の端に多くの質量が置かれることになります。</target>
        </trans-unit>
        <trans-unit id="d55d0516aec7ebe6efc3bc1e9f0b83c115b36898" translate="yes" xml:space="preserve">
          <source>The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">高いほど,その特徴の重要度が高い.特徴の重要度は,その特徴によってもたらされる基準の(正規化された)総削減量として計算される.これは,ジニ重要度としても知られている.</target>
        </trans-unit>
        <trans-unit id="1006ebebebbef4245b7568132e1eabf566a79140" translate="yes" xml:space="preserve">
          <source>The highest p-value for features to be kept.</source>
          <target state="translated">残すべき特徴の最も高いp値。</target>
        </trans-unit>
        <trans-unit id="0c1d204d0071e8e2a8101579dd644bade4aef0d9" translate="yes" xml:space="preserve">
          <source>The highest uncorrected p-value for features to keep.</source>
          <target state="translated">残すべき特徴のための最も高い無補正のp値。</target>
        </trans-unit>
        <trans-unit id="3708900f6a079bc16cd76c37da5647812d1a0427" translate="yes" xml:space="preserve">
          <source>The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights.</source>
          <target state="translated">推定された重みのヒストグラムは,重みにスパースシティを誘発する事前分布が暗示されているため,非常にピークを迎えています.</target>
        </trans-unit>
        <trans-unit id="d24bdc7c6b1e4dbeb474c6e9f76042366464386e" translate="yes" xml:space="preserve">
          <source>The hyperparameters</source>
          <target state="translated">ハイパーパラメタ</target>
        </trans-unit>
        <trans-unit id="25f29cead3cfba36338bcf026da5a846edf25636" translate="yes" xml:space="preserve">
          <source>The i-th score &lt;code&gt;train_score_[i]&lt;/code&gt; is the deviance (= loss) of the model at iteration &lt;code&gt;i&lt;/code&gt; on the in-bag sample. If &lt;code&gt;subsample == 1&lt;/code&gt; this is the deviance on the training data.</source>
          <target state="translated">i番目のスコア &lt;code&gt;train_score_[i]&lt;/code&gt; は、バッグ内サンプルの反復 &lt;code&gt;i&lt;/code&gt; におけるモデルの逸脱（=損失）です。 &lt;code&gt;subsample == 1&lt;/code&gt; 場合、これはトレーニングデータの逸脱です。</target>
        </trans-unit>
        <trans-unit id="7768180d6d5c29f9be6eb9d80f99cbf93007e25a" translate="yes" xml:space="preserve">
          <source>The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.</source>
          <target state="translated">i.i.d.の仮定は、基礎となる生成過程で従属サンプルのグループが得られる場合には破られます。</target>
        </trans-unit>
        <trans-unit id="d5e94b9dd17268ac8457a9d26f68d07b8365584a" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; の&lt;/a&gt;背後にある考え方は、概念的に異なる機械学習分類子を組み合わせ、多数決または平均予測確率（ソフト投票）を使用してクラスラベルを予測することです。このような分類器は、個々の弱点のバランスをとるために、同等にパフォーマンスの高いモデルのセットに役立ちます。</target>
        </trans-unit>
        <trans-unit id="1f4d07aca22136c4b5a250fb6a846554681fb509" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt; &lt;code&gt;VotingRegressor&lt;/code&gt; の&lt;/a&gt;背後にある考え方は、概念的に異なる機械学習リグレッサを組み合わせて、平均予測値を返すことです。このようなリグレッサーは、個々の弱点のバランスをとるために、同等にパフォーマンスの高いモデルのセットに役立ちます。</target>
        </trans-unit>
        <trans-unit id="190c7529dfd22ae7ed70d4f84d1deb499abbe749" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;code&gt;VotingClassifier&lt;/code&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; の背後にある考え方は、概念的に異なる機械学習分類子を組み合わせ、多数決または平均予測確率（ソフト投票）を使用してクラスラベルを予測することです。そのような分類子は、個々の弱点のバランスをとるために、同等に機能する一連のモデルに役立ちます。</target>
        </trans-unit>
        <trans-unit id="f4f9efa7d25a183372e2e9ce75bfe20581578033" translate="yes" xml:space="preserve">
          <source>The image as a numpy array: height x width x color</source>
          <target state="translated">画像を numpy 配列で表現したもの:高さ x 幅 x 色</target>
        </trans-unit>
        <trans-unit id="77bae2f833c03e6f85b6a43b3aa3ead8f81ce2e5" translate="yes" xml:space="preserve">
          <source>The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.</source>
          <target state="translated">画像は256階調に量子化され、符号なし8ビット整数として格納されます。ローダはこれらを間隔[0,1]の浮動小数点値に変換します。</target>
        </trans-unit>
        <trans-unit id="6db946ce103c288b47eadb4f1488a8fdc91a7488" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients. See &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; for another implementation:</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;の実装では、係数を近似するアルゴリズムとして座標降下を使用します。別の実装については、&lt;a href=&quot;#least-angle-regression&quot;&gt;最小角度回帰&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="64dabeaa3f8e38c30333615b1b174cbd1348b11e" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; &lt;/a&gt;の実装では、係数を適合させるアルゴリズムとして座標降下を使用します。</target>
        </trans-unit>
        <trans-unit id="555e24352725a2133c35e7a52f56a702cd7f81c4" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; &lt;/a&gt;の実装では、係数を近似するアルゴリズムとして座標降下を使用します。</target>
        </trans-unit>
        <trans-unit id="f9cb17c43bed4736104925d68075ebd6b07ace68" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt;. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:</source>
          <target state="translated">この実装は、&lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]の&lt;/a&gt;アルゴリズム2.1に基づいています。標準のscikit-learn推定器のAPIに加えて、GaussianProcessRegressor：</target>
        </trans-unit>
        <trans-unit id="70780cc3dac260d0fb4e85faed94f4134c9dc0ae" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">実装は、Rasmussen and WilliamsによるGaussian Processes for Machine Learning (GPML)のアルゴリズム2.1に基づいています。</target>
        </trans-unit>
        <trans-unit id="1904d3648b9818da40270920e1855bd9dcd752d1" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">実装は、Rasmussen and WilliamsによるGaussian Processes for Machine Learning (GPML)のアルゴリズム3.1,3.2,5.1に基づいています。</target>
        </trans-unit>
        <trans-unit id="0eeedaeb1d413ea7240926a8ca5811e478d2d62b" translate="yes" xml:space="preserve">
          <source>The implementation is based on an ensemble of ExtraTreeRegressor. The maximum depth of each tree is set to &lt;code&gt;ceil(log_2(n))&lt;/code&gt; where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="translated">実装は、ExtraTreeRegressorのアンサンブルに基づいています。各ツリーの最大深度は &lt;code&gt;ceil(log_2(n))&lt;/code&gt; に設定されます。ここで、\（n \）は、ツリーの構築に使用されるサンプルの数です（詳細については、（Liu et al。、2008）を参照してください）。</target>
        </trans-unit>
        <trans-unit id="166f55526ba60cbc129406f3899569f7b0eb4efd" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm.</source>
          <target state="translated">実装はlibsvmをベースにしています。</target>
        </trans-unit>
        <trans-unit id="cb75b7ddb016087965f5fad7be27fc6c1cb16290" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.</source>
          <target state="translated">実装はlibsvmをベースにしています。フィット時間の複雑さはサンプル数の2次関数以上であり、数万サンプル以上のデータセットへのスケールアップは困難です。</target>
        </trans-unit>
        <trans-unit id="fa8767a0fc5ecb59976beb65f6a5637eaddaef83" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVR&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDRegressor&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="translated">実装はlibsvmに基づいています。適合時間の複雑さは、サンプル数の2次以上であるため、10000サンプルを超えるデータセットにスケーリングすることは困難です。大規模なデータセットの場合は、代わりに&lt;a href=&quot;sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVR&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDRegressor&lt;/code&gt; &lt;/a&gt;を使用することを検討してください。おそらく、&lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt; &lt;/a&gt;トランスフォーマーの後で使用してください。</target>
        </trans-unit>
        <trans-unit id="c37eaa47201071ab85ed7ee61d61deb2b696067c" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="translated">実装はlibsvmに基づいています。適合時間は、サンプル数に少なくとも2次関数的に比例し、数万サンプルを超えると実用的でない場合があります。大規模なデータセットの場合は、代わりに&lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt;を使用することを検討してください。おそらく、&lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt; &lt;/a&gt;トランスフォーマーの後で使用してください。</target>
        </trans-unit>
        <trans-unit id="a4dec30aaf65f05fda35ca6a3928a075e5f1587d" translate="yes" xml:space="preserve">
          <source>The implementation is designed to:</source>
          <target state="translated">実施形態は、以下のように設計されている。</target>
        </trans-unit>
        <trans-unit id="e44f638bc5a5bdc8405d919df1e1a354ca31f099" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; is based on an ensemble of &lt;a href=&quot;generated/sklearn.tree.extratreeregressor#sklearn.tree.ExtraTreeRegressor&quot;&gt;&lt;code&gt;tree.ExtraTreeRegressor&lt;/code&gt;&lt;/a&gt;. Following Isolation Forest original paper, the maximum depth of each tree is set to \(\lceil \log_2(n) \rceil\) where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt; &lt;code&gt;ensemble.IsolationForest&lt;/code&gt; &lt;/a&gt;の実装は、&lt;a href=&quot;generated/sklearn.tree.extratreeregressor#sklearn.tree.ExtraTreeRegressor&quot;&gt; &lt;code&gt;tree.ExtraTreeRegressor&lt;/code&gt; の&lt;/a&gt;アンサンブルに基づいています。Isolation Forestの元の論文に従って、各ツリーの最大深度は\（\ lceil \ log_2（n）\ rceil \）に設定されます。ここで、\（n \）はツリーの構築に使用されるサンプルの数です（（Liu et al 。、2008）詳細については）。</target>
        </trans-unit>
        <trans-unit id="08d84e0cf0c38a38100a9dad48e0efc744fd2db8" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;.</source>
          <target state="translated">実装&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt; scikit学習では、多変量線形回帰モデルの一般化は以下の&lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt;複数の寸法の中央値の一般化である空間メジアン使用&lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9] &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="fda50d88e9c81601cfc445f1c078677bdd2f3d40" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id36&quot;&gt;12&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id37&quot;&gt;13&lt;/a&gt;.</source>
          <target state="translated">scikit-learnでの&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt;の実装は、複数の次元への中央値の一般化である空間中央値を使用した多変量線形回帰モデル&lt;a href=&quot;#f1&quot; id=&quot;id36&quot;&gt;12&lt;/a&gt;への一般化に従います&lt;a href=&quot;#f2&quot; id=&quot;id37&quot;&gt;13&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="eb185b2e148e3613cdde933dd05a6b32e3b9acbe" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM&lt;/a&gt; of L&amp;eacute;on Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">SGDの実装は、&lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;確率的勾配SVMの&lt;/a&gt;影響を受けますレオン・ボトゥの。 SvmSGDと同様に、重みベクトルは、スカラーと、L2正則化の場合に効率的な重み更新を可能にするベクトルの積として表されます。スパース特徴ベクトルの場合、より頻繁に更新されるという事実を考慮して、切片はより小さな学習率（0.01倍）で更新されます。トレーニング例は順次取り上げられ、観察された例ごとに学習率が低下します。 Shalev-Shwartzらの学習率スケジュールを採用しました。 2007.複数クラスの分類では、「1対すべて」のアプローチが使用されます。鶴岡他により提案された打ち切り勾配アルゴリズムを使用する。 L1正則化のための2009（およびElastic Net）。コードはCythonで書かれています。</target>
        </trans-unit>
        <trans-unit id="3db47613e45248213b8540e2c5b681f6afec29a2" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;code&gt;Stochastic Gradient SVM&lt;/code&gt; of &lt;a href=&quot;#id10&quot; id=&quot;id7&quot;&gt;7&lt;/a&gt;. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse input &lt;code&gt;X&lt;/code&gt;, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from &lt;a href=&quot;#id12&quot; id=&quot;id8&quot;&gt;8&lt;/a&gt;. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed in &lt;a href=&quot;#id13&quot; id=&quot;id9&quot;&gt;9&lt;/a&gt; for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">SGDの実装は、&lt;a href=&quot;#id10&quot; id=&quot;id7&quot;&gt;7の&lt;/a&gt; &lt;code&gt;Stochastic Gradient SVM&lt;/code&gt; 影響を受けます。 SvmSGDと同様に、重みベクトルは、L2正則化の場合に効率的な重み更新を可能にするスカラーとベクトルの積として表されます。スパース入力 &lt;code&gt;X&lt;/code&gt; の場合、切片はより頻繁に更新されるという事実を考慮して、より小さな学習率（0.01を掛けたもの）で更新されます。トレーニング例は順番にピックアップされ、観察された例ごとに学習率が低下します。&lt;a href=&quot;#id12&quot; id=&quot;id8&quot;&gt;8&lt;/a&gt;からの学習率スケジュールを採用しました。マルチクラス分類の場合、「1対すべて」のアプローチが使用されます。&lt;a href=&quot;#id13&quot; id=&quot;id9&quot;&gt;9で&lt;/a&gt;提案された切り捨てられた勾配アルゴリズムを使用しますL1正則化（およびエラスティックネット）用。コードはCythonで書かれています。</target>
        </trans-unit>
        <trans-unit id="234c6abfe6fe107b5770fe372b7d9e14789fc069" translate="yes" xml:space="preserve">
          <source>The implementation of logistic regression in scikit-learn can be accessed from class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.</source>
          <target state="translated">scikit-learnのロジスティック回帰の実装には、&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;クラスからアクセスできます。この実装は、オプションのL2またはL1正則化を使用して、バイナリ、One対Rest、または多項ロジスティック回帰に適合できます。</target>
        </trans-unit>
        <trans-unit id="11a45f793137837fb140d6499ba2bfe32aacbae3" translate="yes" xml:space="preserve">
          <source>The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">特徴の重要度は,その特徴によってもたらされる基準の(正規化された)総削減量として計算される.これは,ジニ重要度としても知られている.</target>
        </trans-unit>
        <trans-unit id="285ba1aeef83b4f72aaed81188dabeb94680bb69" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator.</source>
          <target state="translated">前の反復と比較した、out-of-bagサンプルの損失（=逸脱）の改善。 &lt;code&gt;oob_improvement_[0]&lt;/code&gt; は、 &lt;code&gt;init&lt;/code&gt; 推定量よりも第1ステージの損失の改善です。</target>
        </trans-unit>
        <trans-unit id="820150544fe550e2e074294e51088eda9cdc8014" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator. Only available if &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;</source>
          <target state="translated">前の反復と比較した、アウトオブバッグサンプルの損失（=逸脱度）の改善。 &lt;code&gt;oob_improvement_[0]&lt;/code&gt; は、 &lt;code&gt;init&lt;/code&gt; 推定量に対する第1段階の損失の改善です。 &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; 場合にのみ使用可能</target>
        </trans-unit>
        <trans-unit id="d60773790f2e7bcb156876ca46ea3f1191f69076" translate="yes" xml:space="preserve">
          <source>The impurity at \(m\) is computed using an impurity function \(H()\), the choice of which depends on the task being solved (classification or regression)</source>
          <target state="translated">不純物は、不純物関数を使って計算されます。</target>
        </trans-unit>
        <trans-unit id="d750c1db4cd012b1969f8fbe25c31782ef1879ba" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importance ranks the numerical features to be the most important features. As a result, the non-predictive &lt;code&gt;random_num&lt;/code&gt; variable is ranked the most important!</source>
          <target state="translated">不純物ベースの特徴の重要性は、数値特徴を最も重要な特徴としてランク付けします。その結果、予測 &lt;code&gt;random_num&lt;/code&gt; 変数が最も重要にランク付けされます。</target>
        </trans-unit>
        <trans-unit id="eb74ef34fa0b599fafdb1111102755a10d538f2b" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore &lt;strong&gt;do not necessarily inform us on which features are most important to make good predictions on held-out dataset&lt;/strong&gt;. Secondly, &lt;strong&gt;they favor high cardinality features&lt;/strong&gt;, that is features with many unique values. &lt;a href=&quot;permutation_importance#permutation-importance&quot;&gt;Permutation feature importance&lt;/a&gt; is an alternative to impurity-based feature importance that does not suffer from these flaws. These two methods of obtaining feature importance are explored in: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="translated">ツリーベースのモデルで計算された不純物ベースの特徴の重要性には、誤解を招く結論につながる可能性のある2つの欠陥があります。まず、トレーニングデータセットから導出された統計に基づいて計算されるため&lt;strong&gt;、保持されたデータセットで適切な予測を行うためにどの機能が最も重要であるかを必ずしも通知するわけでは&lt;/strong&gt;あり&lt;strong&gt;ません&lt;/strong&gt;。第二に、&lt;strong&gt;それらは高いカーディナリティ機能&lt;/strong&gt;、つまり多くの固有の値を持つ機能を好みます。&lt;a href=&quot;permutation_importance#permutation-importance&quot;&gt;順列特徴の重要性&lt;/a&gt;は、これらの欠陥の影響を受けない不純物ベースの特徴の重要性に代わるものです。特徴の重要性を取得するこれらの2つの方法については、&lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;順列の重要性とランダムフォレストの特徴の重要性（MDI）で説明してい&lt;/a&gt;ます。</target>
        </trans-unit>
        <trans-unit id="a85ca2be296db64c4eea5e4c85e7d3c8770ab473" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances.</source>
          <target state="translated">不純物をベースにした特徴的な輸入品。</target>
        </trans-unit>
        <trans-unit id="f89a5a9d5e4b0a0db4c9a50975418d448408f475" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature if axis == 0.</source>
          <target state="translated">axis ==0 の場合の各特徴のインputation fill 値。</target>
        </trans-unit>
        <trans-unit id="52ad716a60ba342ef84206f96283ef6240a59825" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature.</source>
          <target state="translated">各特徴のインputation fill値。</target>
        </trans-unit>
        <trans-unit id="66edd575acaaa453b1fb3cfd5f9ed2a63e5987ae" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature. Computing statistics can result in &lt;code&gt;np.nan&lt;/code&gt; values. During &lt;a href=&quot;#sklearn.impute.SimpleImputer.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt;, features corresponding to &lt;code&gt;np.nan&lt;/code&gt; statistics will be discarded.</source>
          <target state="translated">各機能のインピュテーションフィル値。統計を計算すると、 &lt;code&gt;np.nan&lt;/code&gt; 値が得られる可能性があります。&lt;a href=&quot;#sklearn.impute.SimpleImputer.transform&quot;&gt; &lt;code&gt;transform&lt;/code&gt; &lt;/a&gt;中、 &lt;code&gt;np.nan&lt;/code&gt; 統計に対応する機能は破棄されます。</target>
        </trans-unit>
        <trans-unit id="7cd5e57b66f65afed5b5b066bdba074e0cee3d73" translate="yes" xml:space="preserve">
          <source>The imputation strategy.</source>
          <target state="translated">インプット戦略。</target>
        </trans-unit>
        <trans-unit id="d67cf9b9c87d5f16b287aedd5a057873a2e85c3c" translate="yes" xml:space="preserve">
          <source>The imputed dataset. &lt;code&gt;n_output_features&lt;/code&gt; is the number of features that is not always missing during &lt;code&gt;fit&lt;/code&gt;.</source>
          <target state="translated">帰属されたデータセット。 &lt;code&gt;n_output_features&lt;/code&gt; は、 &lt;code&gt;fit&lt;/code&gt; 中に常に欠落しているとは限らないフィーチャの数です。</target>
        </trans-unit>
        <trans-unit id="fc59b667379a9f19464aefa0413c2d5b611edad6" translate="yes" xml:space="preserve">
          <source>The imputed input data.</source>
          <target state="translated">インプットされた入力データ。</target>
        </trans-unit>
        <trans-unit id="aa1b27a8286b1b98805decd6657b8fa02347921e" translate="yes" xml:space="preserve">
          <source>The index (of the &lt;code&gt;cv_results_&lt;/code&gt; arrays) which corresponds to the best candidate parameter setting.</source>
          <target state="translated">最適な候補パラメータ設定に対応する（ &lt;code&gt;cv_results_&lt;/code&gt; 配列の）インデックス。</target>
        </trans-unit>
        <trans-unit id="d8f1885dbc976f2ceb3f7711b10b324e6546b97b" translate="yes" xml:space="preserve">
          <source>The index is computed only quantities and features inherent to the dataset.</source>
          <target state="translated">インデックスは,データセットに固有の量と特徴のみを計算します.</target>
        </trans-unit>
        <trans-unit id="eaf4dd06369196819331db8b96a42731aed3d8e0" translate="yes" xml:space="preserve">
          <source>The index is defined as the average similarity between each cluster \(C_i\) for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of this index, similarity is defined as a measure \(R_{ij}\) that trades off:</source>
          <target state="translated">この指標は、各クラスタ間の平均類似度として定義される。この指標では、類似度は、トレードオフする尺度として定義される。</target>
        </trans-unit>
        <trans-unit id="7ad5522250591909d8f74ff707e1fa5837de3ae6" translate="yes" xml:space="preserve">
          <source>The index is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared):</source>
          <target state="translated">この指数は、すべてのクラスタについてのクラスタ間分散とクラスタ間分散の和の比です(分散は距離の2乗の和として定義されます)。</target>
        </trans-unit>
        <trans-unit id="f4af6c67193225ac6c1c304797e9ea3402785083" translate="yes" xml:space="preserve">
          <source>The index of the cluster.</source>
          <target state="translated">クラスタのインデックス。</target>
        </trans-unit>
        <trans-unit id="bf1ee02857437dc2bc3e741c8e3e56ef69d1ae10" translate="yes" xml:space="preserve">
          <source>The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.</source>
          <target state="translated">単語のインデックス値は、学習コーパス全体の中での頻度とリンクしています。</target>
        </trans-unit>
        <trans-unit id="03c2241f87945c783a2d50da20a997d6e73ac147" translate="yes" xml:space="preserve">
          <source>The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don&amp;rsquo;t use this parameter unless you know what to do.</source>
          <target state="translated">ソートされたトレーニング入力サンプルのインデックス。多くのツリーが同じデータセットで成長している場合、これにより、ツリー間で順序をキャッシュできます。Noneの場合、データはここでソートされます。何をすべきか分からない場合は、このパラメーターを使用しないでください。</target>
        </trans-unit>
        <trans-unit id="0f615a1a241d635bbd7f8073a9962f88455d5840" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each column.</source>
          <target state="translated">各列のクラスターメンバーシップを示す指標。</target>
        </trans-unit>
        <trans-unit id="7c7c8e5dd3219610397a571246f91a523e9b0296" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each row.</source>
          <target state="translated">各行のクラスターメンバーシップを示す指標。</target>
        </trans-unit>
        <trans-unit id="1d09056572af0ab7d260b8a9a458cace43041c54" translate="yes" xml:space="preserve">
          <source>The inertia matrix uses a Heapq-based representation.</source>
          <target state="translated">慣性行列は、Heapq ベースの表現を使用しています。</target>
        </trans-unit>
        <trans-unit id="4f68a65921a2da6924b081b35cf28ef22fa8adaa" translate="yes" xml:space="preserve">
          <source>The inferred value of max_features.</source>
          <target state="translated">max_featuresの推定値。</target>
        </trans-unit>
        <trans-unit id="9af964a5bdc6c7f6e3e23dbd4ccea9f871ad0d0c" translate="yes" xml:space="preserve">
          <source>The initial coefficients to warm-start the optimization.</source>
          <target state="translated">最適化をウォームスタートするための初期係数。</target>
        </trans-unit>
        <trans-unit id="e326d26a40d08d9ebf56e15d175f98f61ace6983" translate="yes" xml:space="preserve">
          <source>The initial guess for the covariance.</source>
          <target state="translated">共分散の最初の推測。</target>
        </trans-unit>
        <trans-unit id="a31ad0b6b83b24ae472799017e15e5984848ac41" translate="yes" xml:space="preserve">
          <source>The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)</source>
          <target state="translated">各特徴量のノイズ分散の初期推定値.Noneの場合,デフォルトはnp.ones(n_features)です.</target>
        </trans-unit>
        <trans-unit id="5af1f38d3d578b440a3ef0c3e0d6f491e678143f" translate="yes" xml:space="preserve">
          <source>The initial intercept to warm-start the optimization.</source>
          <target state="translated">最適化をウォームスタートするための初期切片。</target>
        </trans-unit>
        <trans-unit id="ffb7aede167c9f36a232ef307d2df81471bb1b4f" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.0 as eta0 is not used by the default schedule &amp;lsquo;optimal&amp;rsquo;.</source>
          <target state="translated">「一定」、「invscaling」、または「適応」スケジュールの初期学習率。eta0はデフォルトのスケジュール「optimal」では使用されないため、デフォルト値は0.0です。</target>
        </trans-unit>
        <trans-unit id="3434635df97d9946f02b9082e79d95e6999ccc03" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.01.</source>
          <target state="translated">'constant'、 'invscaling'、または 'adaptive'スケジュールの初期学習率。デフォルト値は0.01です。</target>
        </trans-unit>
        <trans-unit id="f396af141edab02acfc3d8c05ea1f6e174dd0d5b" translate="yes" xml:space="preserve">
          <source>The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">使用された初期学習率。重みを更新する際のステップサイズを制御します。solver = 'sgd'または 'adam'の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="8aa315cba5e51992abb63a7c33e92703aa91a364" translate="yes" xml:space="preserve">
          <source>The initial model \(F_{0}\) is problem specific, for least-squares regression one usually chooses the mean of the target values.</source>
          <target state="translated">初期モデル \(F_{0}\)は問題に特有のもので、最小二乗回帰では、通常、目標値の平均値を選択します。</target>
        </trans-unit>
        <trans-unit id="fcbcdc2001232ebf2d1267dbd5c1db8c05bef33b" translate="yes" xml:space="preserve">
          <source>The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument. The passed object has to implement &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">初期モデルは、 &lt;code&gt;init&lt;/code&gt; 引数で指定することもできます。渡されたオブジェクトは、 &lt;code&gt;fit&lt;/code&gt; と &lt;code&gt;predict&lt;/code&gt; を実装する必要があります。</target>
        </trans-unit>
        <trans-unit id="2b0323f433eec5618cbc80c984794d0b5575bec5" translate="yes" xml:space="preserve">
          <source>The initial transformation will be a random array of shape &lt;code&gt;(n_components, n_features)&lt;/code&gt;. Each value is sampled from the standard normal distribution.</source>
          <target state="translated">最初の変換は、形状のランダム配列 &lt;code&gt;(n_components, n_features)&lt;/code&gt; ます。各値は、標準正規分布からサンプリングされます。</target>
        </trans-unit>
        <trans-unit id="17bd189eec62d21587058ccd88b444461d73119b" translate="yes" xml:space="preserve">
          <source>The initial values of the coefficients.</source>
          <target state="translated">係数の初期値です。</target>
        </trans-unit>
        <trans-unit id="32a92ee0ef2ffe7a34cba56e64f123bd9b2ca181" translate="yes" xml:space="preserve">
          <source>The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the first layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the weight matrix as a 28x28 pixel image.</source>
          <target state="translated">入力データは28x28ピクセルの手書き数字からなり,データセットの特徴量は784個である.したがって,第1層の重み行列は,(784,hidden_layer_size[0])の形をしています.したがって,重み行列の1列を28x28ピクセルの画像として可視化することができます.</target>
        </trans-unit>
        <trans-unit id="c8ff70c87eb4edb0d6df0b02362dd0f826eb86c0" translate="yes" xml:space="preserve">
          <source>The input data matrix</source>
          <target state="translated">入力データ行列</target>
        </trans-unit>
        <trans-unit id="28402823f8037a445e34f883189d18c9c8586801" translate="yes" xml:space="preserve">
          <source>The input data to complete.</source>
          <target state="translated">入力データを完成させる。</target>
        </trans-unit>
        <trans-unit id="10ea3f8f2392cebb3ea568adacc2ca87c3d24f21" translate="yes" xml:space="preserve">
          <source>The input data to project into a smaller dimensional space.</source>
          <target state="translated">より小さな次元の空間に投影するための入力データ。</target>
        </trans-unit>
        <trans-unit id="8b941f334dfce6c774093743936f754bf8137c6b" translate="yes" xml:space="preserve">
          <source>The input data.</source>
          <target state="translated">入力データです。</target>
        </trans-unit>
        <trans-unit id="a383a11075a72cc4c498a6da492b12429e0d8bd0" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods:</source>
          <target state="translated">入力行列を正規化して、市松模様をより明確にする。3つの方法が考えられます。</target>
        </trans-unit>
        <trans-unit id="5296b71b9c59f1ea88d1300f56d80b11acdd4263" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is preprocessed as follows:</source>
          <target state="translated">入力行列は、以下のように前処理されます。</target>
        </trans-unit>
        <trans-unit id="1dfee05c320291671e1507b730cbc56e48fcc05b" translate="yes" xml:space="preserve">
          <source>The input samples with only the selected features.</source>
          <target state="translated">選択された特徴量のみの入力サンプル。</target>
        </trans-unit>
        <trans-unit id="aa9c51d1052bb07178d99dc6b9998838f3fd04f4" translate="yes" xml:space="preserve">
          <source>The input samples.</source>
          <target state="translated">入力サンプルです。</target>
        </trans-unit>
        <trans-unit id="691f5d8dd518149d763a76be65224ad6a47a3de2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">入力サンプル。内部的には、それは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換され、疎行列が疎 &lt;code&gt;csr_matrix&lt;/code&gt; に提供される場合。</target>
        </trans-unit>
        <trans-unit id="80a0f34908e0a755318c3eb528444068386165b2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">入力サンプル。内部的には、そのdtypeは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換されます。スパース行列が指定されている場合は、スパース &lt;code&gt;csr_matrix&lt;/code&gt; に変換されます。</target>
        </trans-unit>
        <trans-unit id="ad241eeecc4b3d71885329d69a3ecbb931dd0903" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">入力サンプル。内部的には、そのdtypeは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換されます。スパース行列が指定されている場合、それはスパース &lt;code&gt;csr_matrix&lt;/code&gt; に変換されます。</target>
        </trans-unit>
        <trans-unit id="574b784828dbe4184d33a76077708bbde6ac5b78" translate="yes" xml:space="preserve">
          <source>The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.</source>
          <target state="translated">入力サンプル。スパース行列は、CSC、CSR、COO、DOK、LILのいずれかになります。COO,DOK,LIL は CSR に変換されます。</target>
        </trans-unit>
        <trans-unit id="b99e327cb34f5c84fcbd4e91cfb496e38c8c302f" translate="yes" xml:space="preserve">
          <source>The input samples. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csc_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">入力サンプル。効率を最大にするには、 &lt;code&gt;dtype=np.float32&lt;/code&gt; を使用します。スパース行列もサポートされています。効率を最大にするには、スパース &lt;code&gt;csc_matrix&lt;/code&gt; を使用してください。</target>
        </trans-unit>
        <trans-unit id="6e6a9dea87222450502841b10c214ed0343cd360" translate="yes" xml:space="preserve">
          <source>The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt;&lt;code&gt;make_low_rank_matrix&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">入力セットは、十分に条件付けされているか（デフォルト）、低ランク脂肪尾特異プロファイルを持つことができます。詳細については、&lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt; &lt;code&gt;make_low_rank_matrix&lt;/code&gt; &lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="289f5749e232fcbed1540450cf860647da777cb8" translate="yes" xml:space="preserve">
          <source>The input set is well conditioned, centered and gaussian with unit variance.</source>
          <target state="translated">入力集合は、良好な条件付きで、中央に位置し、単位分散を持つガウス分布です。</target>
        </trans-unit>
        <trans-unit id="9196dd161fc035e27746b1e485ce67d2bc4a2f97" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.</source>
          <target state="translated">この変換器への入力は,カテゴリ的(離散的)特徴量によって取られる値を表す整数または文字列の配列のようなものでなければなりません.特徴量は,順序の整数に変換されます.これにより,特徴量ごとに1列の整数(0からn_categories-1)が得られます.</target>
        </trans-unit>
        <trans-unit id="a912a205d28c73bb81095b58a51d5b9233f6732c" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the &lt;code&gt;sparse&lt;/code&gt; parameter)</source>
          <target state="translated">このトランスフォーマーへの入力は、整数または文字列の配列のようなものである必要があり、カテゴリ（離散）機能によって取得される値を示します。機能は、ワンホット（別名「one-of-K」または「ダミー」）エンコード方式を使用してエンコードされます。これにより、カテゴリごとにバイナリ列が作成され、スパース行列またはスパース配列が返されます（ &lt;code&gt;sparse&lt;/code&gt; パラメーターによって異なります）。</target>
        </trans-unit>
        <trans-unit id="6fca358f78b22e3193c17b331b39a22e0cf2c917" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array.</source>
          <target state="translated">このトランスフォーマーへの入力は、整数または文字列の配列のようなものである必要があり、カテゴリカル（離散）特徴がとる値を示します。機能は、ワンホット（別名「one-of-K」または「ダミー」）エンコーディングスキームを使用してエンコードされます。これにより、各カテゴリのバイナリ列が作成され、疎行列または密配列が返されます。</target>
        </trans-unit>
        <trans-unit id="c9721cff229ea27eaddea830cb215d9ecb24c54d" translate="yes" xml:space="preserve">
          <source>The instance.</source>
          <target state="translated">インスタンスです。</target>
        </trans-unit>
        <trans-unit id="d6846270bbc73ec820141552eb32b90be4589180" translate="yes" xml:space="preserve">
          <source>The integer id or the string name metadata of the MLComp dataset to load</source>
          <target state="translated">ロードするMLCompデータセットの整数idまたは文字列名のメタデータ</target>
        </trans-unit>
        <trans-unit id="0ff5072970d94a45dfd0b3dc59cc200193becdfd" translate="yes" xml:space="preserve">
          <source>The integer labels (0 or 1) for class membership of each sample.</source>
          <target state="translated">各サンプルのクラスメンバーシップを表す整数のラベル(0または1)。</target>
        </trans-unit>
        <trans-unit id="a6afcce2561a80c34c914f95ed66620399eb0d7c" translate="yes" xml:space="preserve">
          <source>The integer labels for class membership of each sample.</source>
          <target state="translated">各サンプルのクラスメンバーシップを表す整数のラベル。</target>
        </trans-unit>
        <trans-unit id="868af1be557e4830c6f25b4ff3211b7a9aad7eb0" translate="yes" xml:space="preserve">
          <source>The integer labels for cluster membership of each sample.</source>
          <target state="translated">各サンプルのクラスタメンバーシップを表す整数のラベル。</target>
        </trans-unit>
        <trans-unit id="05d4e37f01ed47119c132248581df1196214fb7d" translate="yes" xml:space="preserve">
          <source>The integer labels for quantile membership of each sample.</source>
          <target state="translated">各標本の分位メンバーシップの整数ラベル。</target>
        </trans-unit>
        <trans-unit id="7da460eaa0239b5647e6b97a087bcede08e5a0c5" translate="yes" xml:space="preserve">
          <source>The intercept of the model. Only returned if &lt;code&gt;return_intercept&lt;/code&gt; is True and if X is a scipy sparse array.</source>
          <target state="translated">モデルの切片。 &lt;code&gt;return_intercept&lt;/code&gt; がTrueで、Xがscipyスパース配列の場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="f0408bacce1626a183a8eadd09dc2d6f8b9e549c" translate="yes" xml:space="preserve">
          <source>The intercept term.</source>
          <target state="translated">インターセプト用語です。</target>
        </trans-unit>
        <trans-unit id="39d99a92784fd6547680c69e2a029d63ab730943" translate="yes" xml:space="preserve">
          <source>The inverse document frequency (IDF) vector; only defined if &lt;code&gt;use_idf&lt;/code&gt; is True.</source>
          <target state="translated">逆ドキュメント頻度（IDF）ベクトル。 &lt;code&gt;use_idf&lt;/code&gt; がTrueの場合にのみ定義されます。</target>
        </trans-unit>
        <trans-unit id="70e42545b4f301fb133ffa67ef01dbd1c81a14a6" translate="yes" xml:space="preserve">
          <source>The inverse of the Box-Cox transformation is given by:</source>
          <target state="translated">Box-Cox変換の逆数は次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="1296b0fc0246edf109c97645c02261b1d3d028bb" translate="yes" xml:space="preserve">
          <source>The inverse of the Yeo-Johnson transformation is given by:</source>
          <target state="translated">Yeo-Johnson変換の逆数は次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="8621ba40f721d68ec6f12f67b5e3cc856e6fc6d9" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">虹彩データセットは、古典的で非常に簡単な多クラス分類データセットです。</target>
        </trans-unit>
        <trans-unit id="82f3a3d98edcec3969a3e26fc0789c7c6c1f74ef" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:</source>
          <target state="translated">花菖蒲のデータセットは、花弁と萼の長さと幅から3種類の花菖蒲(Setosa,Versicolour,Virginica)を識別する分類タスクである。</target>
        </trans-unit>
        <trans-unit id="968fa226a66bed28f98c84df2ac306ac992e59c2" translate="yes" xml:space="preserve">
          <source>The isotonic regression optimization problem is defined by:</source>
          <target state="translated">等張回帰最適化問題は次のように定義される。</target>
        </trans-unit>
        <trans-unit id="710a9703f51d61fb5f41b8c77a926ca070febd6c" translate="yes" xml:space="preserve">
          <source>The iteration will stop when &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; where pg_i is the i-th component of the projected gradient.</source>
          <target state="translated">反復は、 &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; ときに停止します。i = 1、...、n} &amp;lt;= &lt;code&gt;tol&lt;/code&gt; ここで、pg_iは投影された勾配のi番目の成分です。</target>
        </trans-unit>
        <trans-unit id="d1f978f02193a9d7a16e2ac4f28051f1be20fa33" translate="yes" xml:space="preserve">
          <source>The iterator consumption and dispatching is protected by the same lock so calling this function should be thread safe.</source>
          <target state="translated">イテレータの消費とディスパッチは同じロックで保護されているので、この関数の呼び出しはスレッドセーフであるべきです。</target>
        </trans-unit>
        <trans-unit id="e6bcf49b626a9dedb0663face3bd765b3dedb378" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the bias vector corresponding to layer i + 1.</source>
          <target state="translated">リストのi番目の要素は、レイヤi+1に対応するバイアスベクトルを表す。</target>
        </trans-unit>
        <trans-unit id="4b396b269a89e1a7151872f147950b58bd31d2c2" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the weight matrix corresponding to layer i.</source>
          <target state="translated">リストの第i番目の要素は、レイヤiに対応する重み行列を表す。</target>
        </trans-unit>
        <trans-unit id="8ce9197f7d2eba2389cfe44c6d9806383e84232a" translate="yes" xml:space="preserve">
          <source>The ith element represents the number of neurons in the ith hidden layer.</source>
          <target state="translated">ith要素は、ith隠れ層のニューロンの数を表す。</target>
        </trans-unit>
        <trans-unit id="110dde91ce5735296cfcdbdf4849eb2634f008ea" translate="yes" xml:space="preserve">
          <source>The justification for the formula in the loss=&amp;rdquo;modified_huber&amp;rdquo; case is in the appendix B in: &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&lt;/a&gt;</source>
          <target state="translated">loss =&amp;rdquo; modified_huber&amp;rdquo;の場合の式の正当性は、&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http&lt;/a&gt;：//jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdfの付録Bにあります。</target>
        </trans-unit>
        <trans-unit id="08f434f0a998f2afa3af3de6e921bb2935ec3174" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space.</source>
          <target state="translated">k-meansアルゴリズムは、一連の\（N \）サンプル\（X \）を\（K \）互いに素なクラスター\（C \）に分割します。各クラスターは、サンプルの平均\（\ mu_j \）で表されます。集まる。この手段は一般にクラスター「セントロイド」と呼ばれます。それらは同じ空間に住んでいますが、一般に\（X \）からのポイントではないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="2ad3a7b0acdb773b3a5a78e7f86e7401f887f5a4" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space. The K-means algorithm aims to choose centroids that minimise the &lt;em&gt;inertia&lt;/em&gt;, or within-cluster sum of squared criterion:</source>
          <target state="translated">k-meansアルゴリズムは、\（N \）サンプルのセット\（X \）を\（K \）ばらばらのクラスター\（C \）に分割し、それぞれがサンプルの平均\（\ mu_j \）で記述されます集まる。この手段は一般にクラスター「重心」と呼ばれます。それらは同じ空間に住んでいますが、一般的には\（X \）からのポイントではないことに注意してください。K平均アルゴリズムは、&lt;em&gt;慣性&lt;/em&gt;を最小化する重心、またはクラスター内の二乗基準の合計を選択することを目的としています。</target>
        </trans-unit>
        <trans-unit id="3042a74be3a11ce6ced2d21d393c8c55a0b044ff" translate="yes" xml:space="preserve">
          <source>The k-means problem is solved using either Lloyd&amp;rsquo;s or Elkan&amp;rsquo;s algorithm.</source>
          <target state="translated">k-means問題は、ロイドまたはエルカンのアルゴリズムを使用して解決されます。</target>
        </trans-unit>
        <trans-unit id="698fe2144ac8be0b801483c0a1d13dc4d07c73ce" translate="yes" xml:space="preserve">
          <source>The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).</source>
          <target state="translated">カッパスコア(docstring参照)は、-1から1の間の数値です。 0.8以上のスコアは一般的に良好な一致とみなされ、0以下は一致しないことを意味します(実質的にランダムなラベル)。</target>
        </trans-unit>
        <trans-unit id="951d69c60ea24190bd0c5a4dcae9aeb94c1c913d" translate="yes" xml:space="preserve">
          <source>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</source>
          <target state="translated">カッパ統計量は、-1から1の間の数値で、最大値は完全一致を意味し、0以下は偶然の一致を意味する。</target>
        </trans-unit>
        <trans-unit id="e7d87035112e23c601a476b538c50df786049966" translate="yes" xml:space="preserve">
          <source>The kernel density estimator can be used with any of the valid distance metrics (see &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt;&lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt;&lt;/a&gt; for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine distance&lt;/a&gt; which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:</source>
          <target state="translated">カーネル密度推定器は、有効な距離メトリック（使用可能なメトリックのリストについては&lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt; &lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt; &lt;/a&gt;を参照）のいずれかで使用できますが、結果はユークリッドメトリックに対してのみ正しく正規化されます。特に有用なメトリックの1つは、球上のポイント間の角度距離を測定する&lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;ハバシン距離&lt;/a&gt;です。これは、地理空間データの視覚化にカーネル密度推定を使用する例です。この場合、南米大陸の2つの異なる種の観測の分布です。</target>
        </trans-unit>
        <trans-unit id="f5acb5ee70e93822d6214e7faa0671c1edbb5e04" translate="yes" xml:space="preserve">
          <source>The kernel is composed of several terms that are responsible for explaining different properties of the signal:</source>
          <target state="translated">カーネルは、信号のさまざまな特性を説明するためのいくつかの用語で構成されています。</target>
        </trans-unit>
        <trans-unit id="075d7f089a3306b950fac8e515a19a5954a7251d" translate="yes" xml:space="preserve">
          <source>The kernel is given by:</source>
          <target state="translated">カーネルは次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="b2a264496619ccec6a15428322dff95c6847afef" translate="yes" xml:space="preserve">
          <source>The kernel specifying the covariance function of the GP. If None is passed, the kernel &amp;ldquo;1.0 * RBF(1.0)&amp;rdquo; is used as default. Note that the kernel&amp;rsquo;s hyperparameters are optimized during fitting.</source>
          <target state="translated">GPの共分散関数を指定するカーネル。Noneが渡されると、カーネル「1.0 * RBF（1.0）」がデフォルトとして使用されます。カーネルのハイパーパラメータはフィッティング中に最適化されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="3f4b36434e95db38ce416f2948e370303e8d7ce4" translate="yes" xml:space="preserve">
          <source>The kernel to use. Valid kernels are [&amp;lsquo;gaussian&amp;rsquo;|&amp;rsquo;tophat&amp;rsquo;|&amp;rsquo;epanechnikov&amp;rsquo;|&amp;rsquo;exponential&amp;rsquo;|&amp;rsquo;linear&amp;rsquo;|&amp;rsquo;cosine&amp;rsquo;] Default is &amp;lsquo;gaussian&amp;rsquo;.</source>
          <target state="translated">使用するカーネル。有効なカーネルは['gaussian' | 'tophat' | 'epanechnikov' | 'exponential' | 'linear' | 'cosine']です。デフォルトは 'gaussian'です。</target>
        </trans-unit>
        <trans-unit id="7d7a3328bfc6be5d4c52eb80a02db6810f6af97a" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers.</source>
          <target state="translated">予測に利用されるカーネル.2値分類の場合,カーネルの構造はパラメータとして渡されたものと同じですが,ハイパーパラメータが最適化されています.複数クラス分類の場合は,1-versus-rest分類器で使用された異なるカーネルからなるCompoundKernelが返されます.</target>
        </trans-unit>
        <trans-unit id="5a1810e4f4370ea9d4b3eb4b69c5661264f9680f" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters</source>
          <target state="translated">予測に利用されるカーネル.カーネルの構造は,パラメータとして渡されたものと同じですが,ハイパーパラメータが最適化されています.</target>
        </trans-unit>
        <trans-unit id="317ceb705f764af792a5c1b02b0c75da4f7767d2" translate="yes" xml:space="preserve">
          <source>The key &lt;code&gt;'params'&lt;/code&gt; is used to store a list of parameter settings dicts for all the parameter candidates.</source>
          <target state="translated">キー &lt;code&gt;'params'&lt;/code&gt; は、すべてのパラメーター候補のパラメーター設定辞書のリストを格納するために使用されます。</target>
        </trans-unit>
        <trans-unit id="467dfd2cbee563f1dc16d74e0763e99176db4d68" translate="yes" xml:space="preserve">
          <source>The l1-penalized estimator can recover part of this off-diagonal structure. It learns a sparse precision. It is not able to recover the exact sparsity pattern: it detects too many non-zero coefficients. However, the highest non-zero coefficients of the l1 estimated correspond to the non-zero coefficients in the ground truth. Finally, the coefficients of the l1 precision estimate are biased toward zero: because of the penalty, they are all smaller than the corresponding ground truth value, as can be seen on the figure.</source>
          <target state="translated">l1-penalized estimatorは、この対角線から外れた構造の一部を回復することができる。これは疎な精度を学習する。それは正確な疎度パターンを回復することができません:それはあまりにも多くの非ゼロ係数を検出します。しかし、推定されたl1の最も高い非ゼロ係数は、基底真理の非ゼロ係数に対応している。最後に、l1精度推定の係数はゼロに偏っています:ペナルティのため、図で見ることができるように、それらはすべて対応する基底真理値よりも小さくなっています。</target>
        </trans-unit>
        <trans-unit id="3f115dd7f6ab226f3d1efb2261d982c44eb021f0" translate="yes" xml:space="preserve">
          <source>The label of the positive class</source>
          <target state="translated">正のクラスのラベル</target>
        </trans-unit>
        <trans-unit id="1daeeb6b0700e2caf583964dcec09c381eeb8ea9" translate="yes" xml:space="preserve">
          <source>The label of the positive class. Only applied to binary &lt;code&gt;y_true&lt;/code&gt;. For multilabel-indicator &lt;code&gt;y_true&lt;/code&gt;, &lt;code&gt;pos_label&lt;/code&gt; is fixed to 1.</source>
          <target state="translated">ポジティブクラスのラベル。バイナリ &lt;code&gt;y_true&lt;/code&gt; にのみ適用されます。マルチラベルインジケーター &lt;code&gt;y_true&lt;/code&gt; の場合、 &lt;code&gt;pos_label&lt;/code&gt; は1に固定されます。</target>
        </trans-unit>
        <trans-unit id="e8fa4295eafd0b055339770364309144cc85ebe4" translate="yes" xml:space="preserve">
          <source>The label of the positive class. When &lt;code&gt;pos_label=None&lt;/code&gt;, if y_true is in {-1, 1} or {0, 1}, &lt;code&gt;pos_label&lt;/code&gt; is set to 1, otherwise an error will be raised.</source>
          <target state="translated">ポジティブクラスのラベル。場合 &lt;code&gt;pos_label=None&lt;/code&gt; y_trueは{-1、1}または{0、1}である場合、 &lt;code&gt;pos_label&lt;/code&gt; が1に設定されていない、そうでない場合はエラーが発生します。</target>
        </trans-unit>
        <trans-unit id="844ad03ed5a5991044f5c2a9015a7d46ffc83176" translate="yes" xml:space="preserve">
          <source>The label sets.</source>
          <target state="translated">ラベルセットです。</target>
        </trans-unit>
        <trans-unit id="4e35273308f7d89cd0147256c1b5ea034960bf40" translate="yes" xml:space="preserve">
          <source>The labels assigned to samples. Points which are not included in any cluster are labeled as -1.</source>
          <target state="translated">サンプルに割り当てられたラベル。どのクラスターにも含まれていない点には-1のラベルが付けられます。</target>
        </trans-unit>
        <trans-unit id="34e6595f06fcaf1ac9d996e42fb91cf1dc35ee40" translate="yes" xml:space="preserve">
          <source>The labels of the clusters.</source>
          <target state="translated">クラスターのラベル。</target>
        </trans-unit>
        <trans-unit id="f920ebdbb736d4fac6e624f1f0a24fae7f686c27" translate="yes" xml:space="preserve">
          <source>The laplacian kernel is defined as:</source>
          <target state="translated">ラプラシアンカーネルは次のように定義されています。</target>
        </trans-unit>
        <trans-unit id="06271ccbc4593ebfa05907a5273f644dc124ef42" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the coefficient vector.</source>
          <target state="translated">lasso estimateは、このようにして、最小二乗ペナルティの最小化を、\(\alpha ||w||_1)addedで解くことができます。</target>
        </trans-unit>
        <trans-unit id="321fd6d2804d3c8ea8df389dda895240e812e24c" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the parameter vector.</source>
          <target state="translated">lasso estimateは、このようにして、最小二乗ペナルティの最小化を、\(\alpha ||w||_1)addedで解くことができます。</target>
        </trans-unit>
        <trans-unit id="374010cdf4f6f26c62c956d36c36442b1f12b646" translate="yes" xml:space="preserve">
          <source>The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.</source>
          <target state="translated">最後の特徴は、パーセプトロンの方がヒンジ損失を考慮した場合、SGDよりも学習速度がわずかに速く、結果として得られるモデルがスパースであることを示唆しています。</target>
        </trans-unit>
        <trans-unit id="6312a138a2adc5ee223c24bc72b6c9e5099fd7ea" translate="yes" xml:space="preserve">
          <source>The last dataset is an example of a &amp;lsquo;null&amp;rsquo; situation for clustering: the data is homogeneous, and there is no good clustering. For this example, the null dataset uses the same parameters as the dataset in the row above it, which represents a mismatch in the parameter values and the data structure.</source>
          <target state="translated">最後のデータセットは、クラスタリングの「null」状況の例です。データは均一であり、適切なクラスタリングはありません。この例では、nullデータセットは、その上の行のデータセットと同じパラメーターを使用しています。これは、パラメーター値とデータ構造の不一致を表しています。</target>
        </trans-unit>
        <trans-unit id="2cc9bc385685f028c8287b17d0d958b5050b56a0" translate="yes" xml:space="preserve">
          <source>The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.</source>
          <target state="translated">最後の精度とリコール値はそれぞれ1.と0.であり、対応するしきい値を持たない。これにより、グラフがy軸上で始まることが保証される。</target>
        </trans-unit>
        <trans-unit id="9ddff80583e9f2c8a48f3b64e4a9a06a8a8cefcd" translate="yes" xml:space="preserve">
          <source>The last two panels show how we can sample from the last two models. The resulting samples distributions do not look exactly like the original data distribution. The difference primarily stems from the approximation error we made by using a model that assumes that the data was generated by a finite number of Gaussian components instead of a continuous noisy sine curve.</source>
          <target state="translated">最後の2つのパネルは、最後の2つのモデルからどのようにサンプリングできるかを示しています。結果として得られた標本分布は、元のデータ分布と全く同じようには見えません。この違いは、主に、データが連続したノイズの多い正弦曲線ではなく、有限のガウス成分によって生成されたと仮定したモデルを使用したことによる近似誤差に起因しています。</target>
        </trans-unit>
        <trans-unit id="a4514c8e853c44e1e4a78bf33662f9b32277ba44" translate="yes" xml:space="preserve">
          <source>The latent variables of X.</source>
          <target state="translated">Xの潜在変数。</target>
        </trans-unit>
        <trans-unit id="03a214378b3ef2e13a3cc6617d05b3fe221146c8" translate="yes" xml:space="preserve">
          <source>The learning rate \(\eta\) can be either constant or gradually decaying. For classification, the default learning rate schedule (&lt;code&gt;learning_rate='optimal'&lt;/code&gt;) is given by</source>
          <target state="translated">学習率\（\ eta \）は、一定にすることも、徐々に減衰させることもできます。分類の場合、デフォルトの学習率スケジュール（ &lt;code&gt;learning_rate='optimal'&lt;/code&gt; ）は、</target>
        </trans-unit>
        <trans-unit id="36c5b5659a911c2db600cd9fbf9c0485b5053e95" translate="yes" xml:space="preserve">
          <source>The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a &amp;lsquo;ball&amp;rsquo; with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.</source>
          <target state="translated">t-SNEの学習率は通常、[10.0、1000.0]の範囲です。学習率が高すぎる場合、データは「ボール」のように見える可能性があり、任意の点が最も近い近傍から等距離になります。学習率が低すぎる場合、ほとんどのポイントは、外れ値がほとんどない密集した雲の中で圧縮されて見える場合があります。コスト関数がローカルの最小値に留まっている場合は、学習率を上げると役立つことがあります。</target>
        </trans-unit>
        <trans-unit id="55a4138aa9f2827acbf6f24a7d7d78c5c9231271" translate="yes" xml:space="preserve">
          <source>The learning rate for weight updates. It is &lt;em&gt;highly&lt;/em&gt; recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.</source>
          <target state="translated">体重更新の学習率。このハイパーパラメータを調整することを&lt;em&gt;強く&lt;/em&gt;お勧めします。妥当な値は10 ** [0。、-3。]の範囲です。</target>
        </trans-unit>
        <trans-unit id="aa4707838d034cc31029160760df726f17933ef5" translate="yes" xml:space="preserve">
          <source>The learning rate schedule:</source>
          <target state="translated">学習率のスケジュールです。</target>
        </trans-unit>
        <trans-unit id="3497cec934b609e53594ddceecdfac3a47086873" translate="yes" xml:space="preserve">
          <source>The learning rate, also known as &lt;em&gt;shrinkage&lt;/em&gt;. This is used as a multiplicative factor for the leaves values. Use &lt;code&gt;1&lt;/code&gt; for no shrinkage.</source>
          <target state="translated">&lt;em&gt;収縮&lt;/em&gt;とも呼ばれる学習率。これは、葉の値の乗法係数として使用されます。収縮しない場合は &lt;code&gt;1&lt;/code&gt; を使用します。</target>
        </trans-unit>
        <trans-unit id="0f65b2c5f287b4c02eec9ec5be4e57eb68fdc4c9" translate="yes" xml:space="preserve">
          <source>The least squares loss (along with the implicit use of the identity link function) of the Ridge regression model seems to cause this model to be badly calibrated. In particular, it tends to underestimate the risk and can even predict invalid negative frequencies.</source>
          <target state="translated">リッジ回帰モデルの最小二乗損失(同一性リンク関数の暗黙の使用とともに)は、このモデルがひどく較正されている原因となっているようです。特に、リスクを過小評価する傾向があり、無効な負の度数を予測することさえある。</target>
        </trans-unit>
        <trans-unit id="ed4ad540aa3d79251b7dbdba80cd56b31d18c164" translate="yes" xml:space="preserve">
          <source>The least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; this method has a cost of \(O(n_{\text{samples}} n_{\text{features}}^2)\), assuming that \(n_{\text{samples}} \geq n_{\text{features}}\).</source>
          <target state="translated">最小二乗解は、Xの特異値分解を使用して計算されます。Xが形状の行列 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; このメソッドのコストは\（O（n _ {\ text {samples}} n _ {\ text {features）です。 }} ^ 2）\）、\（n _ {\ text {samples}} \ geq n _ {\ text {features}} \）と仮定します。</target>
        </trans-unit>
        <trans-unit id="866660dd294846977c41d9eb05a5f823762d12dd" translate="yes" xml:space="preserve">
          <source>The left and right examples highlight the &lt;code&gt;n_labels&lt;/code&gt; parameter: more of the samples in the right plot have 2 or 3 labels.</source>
          <target state="translated">左と右の例は、 &lt;code&gt;n_labels&lt;/code&gt; パラメータを強調しています。右のプロットのサンプルには、2つまたは3つのラベルがあります。</target>
        </trans-unit>
        <trans-unit id="3656a1b3513126216409c003f2c19b4a1bde366e" translate="yes" xml:space="preserve">
          <source>The leftmost layer, known as the input layer, consists of a set of neurons \(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.</source>
          <target state="translated">一番左の層は、入力層と呼ばれ、入力の特徴を表すニューロンの集合である\(\{x_i | x_1,x_2,...,x_m\})からなる。隠れ層の各ニューロンは、前の層の値を加重線形和に変換し、その後、非線形活性化関数を用いて-双曲線タン関数のようなものです。出力層は最後の隠れた層から値を受け取り、出力値に変換します。</target>
        </trans-unit>
        <trans-unit id="7b2e6a226728025cdbf60737ae02746b4f5067e4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel.</source>
          <target state="translated">カーネルの長さのスケール。</target>
        </trans-unit>
        <trans-unit id="658641ffb44e40c4155905b8a662123f9fbe36a4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension.</source>
          <target state="translated">カーネルの長さのスケール。float の場合は,等方性カーネルが利用されます.配列の場合は,異方性カーネルが使用され,l の各次元がそれぞれの特徴次元の長さスケールを定義します.</target>
        </trans-unit>
        <trans-unit id="c8dac18109c7577d20154429eb0e5e79a703a997" translate="yes" xml:space="preserve">
          <source>The likelihood of the data set with &lt;code&gt;self.covariance_&lt;/code&gt; as an estimator of its covariance matrix.</source>
          <target state="translated">共分散行列の推定量として &lt;code&gt;self.covariance_&lt;/code&gt; を使用したデータセットの尤度。</target>
        </trans-unit>
        <trans-unit id="9d0f580fc795d85a96fff8300e9ac1a9bc3bbb0b" translate="yes" xml:space="preserve">
          <source>The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.</source>
          <target state="translated">多項式特徴量で訓練された線形モデルは、入力された多項式係数を正確に回復することができます。</target>
        </trans-unit>
        <trans-unit id="9cf1fb22576b6c52a81ed3fb3cd57a1b7c534cc0" translate="yes" xml:space="preserve">
          <source>The linear models &lt;code&gt;LinearSVC()&lt;/code&gt; and &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; yield slightly different decision boundaries. This can be a consequence of the following differences:</source>
          <target state="translated">線形モデル &lt;code&gt;LinearSVC()&lt;/code&gt; および &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; は、わずかに異なる決定境界を生成します。これは、次の違いの結果である可能性があります。</target>
        </trans-unit>
        <trans-unit id="e79cf5350a0d3a4c8abb9ba5736a33d5a0c53fd5" translate="yes" xml:space="preserve">
          <source>The linear models assume no interactions between the input variables which likely causes under-fitting. Inserting a polynomial feature extractor (&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt;) indeed increases their discrimative power by 2 points of Gini index. In particular it improves the ability of the models to identify the top 5% riskiest profiles.</source>
          <target state="translated">線形モデルは、入力変数間に相互作用がないことを前提としています。多項式特徴抽出器（&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; &lt;/a&gt;）を挿入すると、実際にジニ係数の2ポイントだけ識別力が向上します。特に、上位5％の最もリスクの高いプロファイルを特定するモデルの機能が向上します。</target>
        </trans-unit>
        <trans-unit id="086fe9ce38700dda3eefe00cc625a2fbfd905812" translate="yes" xml:space="preserve">
          <source>The linear operator to apply to the data to get the independent sources. This is equal to the unmixing matrix when &lt;code&gt;whiten&lt;/code&gt; is False, and equal to &lt;code&gt;np.dot(unmixing_matrix, self.whitening_)&lt;/code&gt; when &lt;code&gt;whiten&lt;/code&gt; is True.</source>
          <target state="translated">独立したソースを取得するためにデータに適用する線形演算子。とき、これはアンミキシング行列に等しい &lt;code&gt;whiten&lt;/code&gt; Falseで、かつ等しく &lt;code&gt;np.dot(unmixing_matrix, self.whitening_)&lt;/code&gt; する場合 &lt;code&gt;whiten&lt;/code&gt; Trueです。</target>
        </trans-unit>
        <trans-unit id="a3db33e367d642c4ed744d98677d2b41b0f8967e" translate="yes" xml:space="preserve">
          <source>The linear transformation learned during fitting.</source>
          <target state="translated">フィッティング中に学習した線形変換。</target>
        </trans-unit>
        <trans-unit id="c3af4059661dbcc46e497f8eb762a99fb7562b4b" translate="yes" xml:space="preserve">
          <source>The link function is determined by the &lt;code&gt;link&lt;/code&gt; parameter.</source>
          <target state="translated">リンク機能は、 &lt;code&gt;link&lt;/code&gt; パラメータによって決定されます。</target>
        </trans-unit>
        <trans-unit id="7828a03a3850299c9821c4410a12edc3f4715fc7" translate="yes" xml:space="preserve">
          <source>The link function of the GLM, i.e. mapping from linear predictor &lt;code&gt;X @ coeff + intercept&lt;/code&gt; to prediction &lt;code&gt;y_pred&lt;/code&gt;. Option &amp;lsquo;auto&amp;rsquo; sets the link depending on the chosen family as follows:</source>
          <target state="translated">GLMのリンク関数、つまり線形予測子 &lt;code&gt;X @ coeff + intercept&lt;/code&gt; から予測 &lt;code&gt;y_pred&lt;/code&gt; へのマッピング。オプション 'auto'は、選択したファミリに応じてリンクを次のように設定します。</target>
        </trans-unit>
        <trans-unit id="df16104005b0ad36b12ecec77e07a777907b9657" translate="yes" xml:space="preserve">
          <source>The linkage distance threshold above which, clusters will not be merged. If not &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;n_clusters&lt;/code&gt; must be &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;compute_full_tree&lt;/code&gt; must be &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">それを超えると、クラスターはマージされないリンケージ距離のしきい値。 &lt;code&gt;None&lt;/code&gt; でない場合、 &lt;code&gt;n_clusters&lt;/code&gt; は &lt;code&gt;None&lt;/code&gt; であり、 &lt;code&gt;compute_full_tree&lt;/code&gt; は &lt;code&gt;True&lt;/code&gt; である必要があります。</target>
        </trans-unit>
        <trans-unit id="e68f70b09864b10f1aea37e78800b7b9e97f9614" translate="yes" xml:space="preserve">
          <source>The list of Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. A value of 0 is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while 1 is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">&lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; のElastic-Netミキシングパラメータのリスト。 &lt;code&gt;penalty='elasticnet'&lt;/code&gt; 場合にのみ使用されます。値0は、 &lt;code&gt;penalty='l2'&lt;/code&gt; を使用することと同等ですが、1は、 &lt;code&gt;penalty='l1'&lt;/code&gt; を使用することと同等です。以下のために &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt; ペナルティは、L1とL2の組み合わせです。</target>
        </trans-unit>
        <trans-unit id="7b28aee2c83cc27005872232d54b931e2f0eba84" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each cross-validation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">バリデーションフォールド以外のすべてのフォールドに適合し、バリデーションフォールドで校正された、クロスバリデーションフォールドごとに1つずつ校正された分類器のリスト。</target>
        </trans-unit>
        <trans-unit id="02b9c4dcf92e3aac8d7274c2bb8e453c5501153c" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each crossvalidation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">較正された分類器のリストで、各クロスバリデーションフォールドに1つずつあり、バリデーションフォールド以外のすべてのフォールドに適合し、バリデーションフォールドで較正されています。</target>
        </trans-unit>
        <trans-unit id="89a2f4c0656f755e155226e23cfb6b592b80d152" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end,
-start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after such nested smaller clusters. Since &lt;code&gt;labels&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(clusters) &amp;gt;
np.unique(labels)&lt;/code&gt;.</source>
          <target state="translated">各行の &lt;code&gt;[start, end]&lt;/code&gt; 形式のクラスターのリスト（すべてのインデックスを含む）。クラスターは &lt;code&gt;(end, -start)&lt;/code&gt; （昇順）に従って順序付けられるため、小さなクラスターを含む大きなクラスターは、そのようなネストされた小さなクラスターの後に続きます。 &lt;code&gt;labels&lt;/code&gt; は階層を反映しないため、通常は &lt;code&gt;len(clusters) &amp;gt; np.unique(labels)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="4b68d75689eb0ac219c6fca1ed0b8741a6d57de9" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end, -start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after those smaller ones. Since &lt;code&gt;labels_&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(cluster_hierarchy_) &amp;gt; np.unique(optics.labels_)&lt;/code&gt;. Please also note that these indices are of the &lt;code&gt;ordering_&lt;/code&gt;, i.e. &lt;code&gt;X[ordering_][start:end + 1]&lt;/code&gt; form a cluster. Only available when &lt;code&gt;cluster_method='xi'&lt;/code&gt;.</source>
          <target state="translated">各行の &lt;code&gt;[start, end]&lt;/code&gt; 形式のクラスターのリスト（すべてのインデックスを含む）。クラスターは &lt;code&gt;(end, -start)&lt;/code&gt; （昇順）に従って順序付けられるため、小さなクラスターを含む大きなクラスターは、それらの小さなクラスターの後に続きます。 &lt;code&gt;labels_&lt;/code&gt; は階層を反映しないため、通常は &lt;code&gt;len(cluster_hierarchy_) &amp;gt; np.unique(optics.labels_)&lt;/code&gt; です。これらのインデックスは &lt;code&gt;ordering_&lt;/code&gt; のものであることに注意してください。つまり、 &lt;code&gt;X[ordering_][start:end + 1]&lt;/code&gt; はクラスターを形成します。 &lt;code&gt;cluster_method='xi'&lt;/code&gt; の場合にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="662dd2cdb21b969ce9a44b42150a945acbfed523" translate="yes" xml:space="preserve">
          <source>The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.</source>
          <target state="translated">各反復時の対物関数の値とデュアルギャップのリスト。return_costsがTrueの場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="177835998b74cd6b15a39a13b3bf0448af9a1ccf" translate="yes" xml:space="preserve">
          <source>The local outlier factor (LOF) of a sample captures its supposed &amp;lsquo;degree of abnormality&amp;rsquo;. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.</source>
          <target state="translated">サンプルの局所異常値（LOF）は、想定される「異常の程度」を示します。これは、サンプルのローカル到達可能性密度とそのk最近傍の到達可能性密度の比の平均です。</target>
        </trans-unit>
        <trans-unit id="f5a19f67242aeecdaccb15b2c01bb7b8a32da9cd" translate="yes" xml:space="preserve">
          <source>The log likelihood at each iteration.</source>
          <target state="translated">各反復における対数尤度。</target>
        </trans-unit>
        <trans-unit id="d6423c4071c8619e84a84f9bcba64b070be431d3" translate="yes" xml:space="preserve">
          <source>The log-marginal-likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;self.kernel_.theta&lt;/code&gt; の対数限界尤度</target>
        </trans-unit>
        <trans-unit id="28c02970f0172adf1161a050ed9ce282eb08888a" translate="yes" xml:space="preserve">
          <source>The log-posterior of LDA can also be written &lt;a href=&quot;#id7&quot; id=&quot;id1&quot;&gt;3&lt;/a&gt; as:</source>
          <target state="translated">LDAの対数事後確率は、次のように書くこともできます&lt;a href=&quot;#id7&quot; id=&quot;id1&quot;&gt;3&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b9f3852ad5acaf7d90c57ecdaa290bd2f37a1610" translate="yes" xml:space="preserve">
          <source>The log-transformed bounds on the kernel&amp;rsquo;s hyperparameters theta</source>
          <target state="translated">カーネルのハイパーパラメーターthetaの対数変換された境界</target>
        </trans-unit>
        <trans-unit id="997c53b0e19e31f9cbe762633463d7411a04ec12" translate="yes" xml:space="preserve">
          <source>The logarithm used is the natural logarithm (base-e).</source>
          <target state="translated">使用している対数は自然対数(Base-e)です。</target>
        </trans-unit>
        <trans-unit id="b4b4750e8021650b4da5de07fd1cb495068051c3" translate="yes" xml:space="preserve">
          <source>The logistic regression with One-Vs-Rest is not a multiclass classifier out of the box. As a result it has more trouble in separating class 2 and 3 than the other estimators.</source>
          <target state="translated">One-Vs-Restを用いたロジスティック回帰は、箱から出してマルチクラスの分類器ではありません。その結果、他の推定量よりもクラス2と3を分離するのに苦労する。</target>
        </trans-unit>
        <trans-unit id="658bb63624739c29e98c4d11ac78e5bf2f1fae2c" translate="yes" xml:space="preserve">
          <source>The loss function that &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; minimizes is given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; が&lt;/a&gt;最小化する損失関数は、</target>
        </trans-unit>
        <trans-unit id="47dc1dfa051244f9df37eb81c616d6773441e3ed" translate="yes" xml:space="preserve">
          <source>The loss function to be used. Defaults to &amp;lsquo;hinge&amp;rsquo;, which gives a linear SVM.</source>
          <target state="translated">使用する損失関数。デフォルトは 'hinge'で、線形SVMを提供します。</target>
        </trans-unit>
        <trans-unit id="ced89f4970549dd56e97e03e89e06fddefc0d81d" translate="yes" xml:space="preserve">
          <source>The loss function to be used. The possible values are &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;</source>
          <target state="translated">使用する損失関数。可能な値は「squared_loss」、「huber」、「epsilon_insensitive」、または「squared_epsilon_insensitive」です。</target>
        </trans-unit>
        <trans-unit id="dcd8a12c459702396e7eec22715d06aed2aee153" translate="yes" xml:space="preserve">
          <source>The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper.</source>
          <target state="translated">使用する損失関数:epsilon_insensitive:参考文献のPA-Iに相当する。</target>
        </trans-unit>
        <trans-unit id="4c1d6cfb55920bc5e055d28ac3a6b6a90335ca56" translate="yes" xml:space="preserve">
          <source>The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper.</source>
          <target state="translated">使用する損失関数:ヒンジ:参考文献のPA-Iに相当する.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
