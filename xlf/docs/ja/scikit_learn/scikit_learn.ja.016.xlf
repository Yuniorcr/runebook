<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="20a9e2ae79f377ec69f0ec221a6bcb99fe892698" translate="yes" xml:space="preserve">
          <source>Inputs: fitted predictive model \(m\), tabular dataset (training or validation) \(D\).</source>
          <target state="translated">Inputs:fitted predictive model ™,tabular dataset (training or validation)</target>
        </trans-unit>
        <trans-unit id="fffa8f8e3b740ecfc583b9bf477ffcbdb298b533" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest.</source>
          <target state="translated">すでに装着されているLSHの森に新しいデータを挿入します。</target>
        </trans-unit>
        <trans-unit id="e78cacac23222d74508b7d4b79fbb8a5cb79c6fc" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest. Cost is proportional to new total size, so additions should be batched.</source>
          <target state="translated">既に適合しているLSHフォレストに新しいデータを挿入します。コストは新しい合計サイズに比例するので、追加はバッチで行う必要があります。</target>
        </trans-unit>
        <trans-unit id="aa15440a6446ecaf7603f9c0287316507f4328a1" translate="yes" xml:space="preserve">
          <source>Inspecting coefficients across the folds of a cross-validation loop gives an idea of their stability.</source>
          <target state="translated">クロスバリデーションループのひだをまたいで係数を検査すると、その安定性についてのアイデアが得られます。</target>
        </trans-unit>
        <trans-unit id="d3ce4618efaae8bf391d0768eaf2c4b834adfb9b" translate="yes" xml:space="preserve">
          <source>Inspection</source>
          <target state="translated">Inspection</target>
        </trans-unit>
        <trans-unit id="4c0fbc7b0ca330086776985f409e7f037b2f9494" translate="yes" xml:space="preserve">
          <source>Instance of the estimator.</source>
          <target state="translated">推定子のインスタンス。</target>
        </trans-unit>
        <trans-unit id="58768f013d8600aed4da42a9f67c30c0b0e7f2be" translate="yes" xml:space="preserve">
          <source>Instead of computing with a set of cardinality &amp;lsquo;n choose k&amp;rsquo;, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if &amp;lsquo;n choose k&amp;rsquo; is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.</source>
          <target state="translated">カーディナリティーのセット「nchoosek」で計算する代わりに、nはサンプルの数、kはサブサンプルの数（少なくともフィーチャの数）です。「nchoose」の場合、特定の最大サイズの確率的部分母集団のみを考慮してください。 k 'がmax_subpopulationより大きい。問題のサイズが小さい場合以外は、n_subsamplesが変更されていない場合、このパラメーターはメモリ使用量とランタイムを決定します。</target>
        </trans-unit>
        <trans-unit id="ce6171dee8019fcd810326710a2a425d2ef2e21c" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the L1 norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="translated">ベクトル結果を与える代わりに、LARS解は、パラメーターベクトルのL1ノルムの各値の解を表す曲線で構成されます。完全な係数パスは、サイズが（n_features、max_features + 1）の配列 &lt;code&gt;coef_path_&lt;/code&gt; に格納されます。最初の列は常にゼロです。</target>
        </trans-unit>
        <trans-unit id="848ef7b85f04c1e0179836725b124a8c68948c34" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the \(\ell_1\) norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="translated">ベクトルの結果を与える代わりに、LARS解は、パラメーターベクトルの\（\ ell_1 \）ノルムの各値の解を示す曲線で構成されます。完全な係数パスは、サイズ（n_features、max_features + 1）の配列 &lt;code&gt;coef_path_&lt;/code&gt; に格納されます。最初の列は常にゼロです。</target>
        </trans-unit>
        <trans-unit id="1c47d2e573aac76a94273f4c46c066cf6f2a8ad1" translate="yes" xml:space="preserve">
          <source>Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. We try out all classifiers on either words or bigrams, with or without idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:</source>
          <target state="translated">チェーンの様々な構成要素のパラメータを微調整する代わりに、可能な値のグリッド上で最適なパラメータを徹底的に探索することができます。我々は、線形SVMに対して0.01または0.001のペナルティパラメータで、idfの有無にかかわらず、単語またはビッグラムのいずれかについて、すべての分類器を試してみました。</target>
        </trans-unit>
        <trans-unit id="db33f6d449a5c5c7a074dd03bb12ec7fc077641c" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_centering=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSR matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">代わりに、呼び出し元は &lt;code&gt;with_centering=False&lt;/code&gt; （この場合、CSRマトリックスの機能に対して分散スケーリングのみが実行されます）を明示的に設定するか、実体化された密配列が適合すると &lt;code&gt;X.toarray()&lt;/code&gt; を呼び出すことが期待されますメモリ内。</target>
        </trans-unit>
        <trans-unit id="f080b277d95a6b1142abd6eb9ea11a07abcb1917" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_mean=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSC matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">代わりに、呼び出し元は &lt;code&gt;with_mean=False&lt;/code&gt; を明示的に設定するか（その場合、CSCマトリックスの機能に対して分散スケーリングのみが実行されます）、実体化された密配列が適合すると &lt;code&gt;X.toarray()&lt;/code&gt; を呼び出すことが期待されますメモリ内。</target>
        </trans-unit>
        <trans-unit id="b11f1ba476938b01d18dd66d0c3826617a20151e" translate="yes" xml:space="preserve">
          <source>Instead, the distribution over \(w\) is assumed to be an axis-parallel, elliptical Gaussian distribution.</source>
          <target state="translated">その代わりに、\(w\)上の分布は、軸平行の楕円ガウス分布であると仮定します。</target>
        </trans-unit>
        <trans-unit id="33a2873657f7cc53fbafced5857dd217868f1368" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. By default, it is &amp;lsquo;strict&amp;rsquo;, meaning that a UnicodeDecodeError will be raised. Other values are &amp;lsquo;ignore&amp;rsquo; and &amp;lsquo;replace&amp;rsquo;.</source>
          <target state="translated">与えられた &lt;code&gt;encoding&lt;/code&gt; ではない文字を含むバイトシーケンスが分析のために与えられた場合にどうするかについての指示。デフォルトでは 'strict'であり、UnicodeDecodeErrorが発生することを意味します。他の値は「無視」と「置換」です。</target>
        </trans-unit>
        <trans-unit id="d22b7ba366228e805a5817961de5812cf7af3a5e" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. Passed as keyword argument &amp;lsquo;errors&amp;rsquo; to bytes.decode.</source>
          <target state="translated">与えられた &lt;code&gt;encoding&lt;/code&gt; ではない文字を含むバイトシーケンスが分析のために与えられた場合にどうするかについての指示。キーワード引数 'errors'としてbytes.decodeに渡されます。</target>
        </trans-unit>
        <trans-unit id="98ae123013fca86e4cc21f01a470888e055215cc" translate="yes" xml:space="preserve">
          <source>Integer array of labels. If not provided, labels will be inferred from y_true and y_pred.</source>
          <target state="translated">ラベルの整数配列。省略された場合、ラベルは y_true と y_pred から推測されます。</target>
        </trans-unit>
        <trans-unit id="e031a894709099be1ecbe448974105f94db94157" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to linear predictor.</source>
          <target state="translated">切片(別名バイアス)が線形予測器に追加される.</target>
        </trans-unit>
        <trans-unit id="fb86aae8ca1d5ea8c3a2f0216a09b115ca2c4371" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to the decision function.</source>
          <target state="translated">切片(別名バイアス)が決定関数に追加されます。</target>
        </trans-unit>
        <trans-unit id="02c60e7ce23b1ba7da9aadaca682e74dd23bd987" translate="yes" xml:space="preserve">
          <source>Intercept term.</source>
          <target state="translated">インターセプト用語。</target>
        </trans-unit>
        <trans-unit id="077392291decf12f1b024c471b5bea6bcd10e56c" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid loosing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">アルゴリズムによって保持される十分な内部統計。それらを保持することは、進化の履歴が失われるのを避けるために、オンライン設定で役立ちますが、エンドユーザーが使用することはできません。（n_components、n_components）は、辞書の共分散行列です。B（n_features、n_components）はデータ近似行列です</target>
        </trans-unit>
        <trans-unit id="e1895bccbde849f2ce31dc6715c34549e1152575" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid losing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">アルゴリズムによって保持される内部の十分統計。それらを保持することは、進化の履歴を失うことを避けるために、オンライン設定で役立ちますが、エンドユーザーには何の役にも立ちません。（n_components、n_components）は、辞書の共分散行列です。B（n_features、n_components）はデータ近似行列です</target>
        </trans-unit>
        <trans-unit id="14f5f43f255d2aa36ff5598f3fb3ace3d6d04389" translate="yes" xml:space="preserve">
          <source>Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.</source>
          <target state="translated">内部的には、非ガウスの事後処理をガウスで近似するためにラプラス近似が使用されます。</target>
        </trans-unit>
        <trans-unit id="0b925a293764508f95547bba83dbd960f81b58e6" translate="yes" xml:space="preserve">
          <source>Internally, the target &lt;code&gt;y&lt;/code&gt; is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">内部的には、ターゲット &lt;code&gt;y&lt;/code&gt; は常に2次元配列に変換され、scikit-learnトランスフォーマーで使用されます。予測時に、出力は &lt;code&gt;y&lt;/code&gt; と同じ数の次元を持つように再形成されます。</target>
        </trans-unit>
        <trans-unit id="465a9fa03a440d5f1b8441512ea129ccebe5933c" translate="yes" xml:space="preserve">
          <source>Internally, this method uses &lt;code&gt;max_iter = 1&lt;/code&gt;. Therefore, it is not guaranteed that a minimum of the cost function is reached after calling it once. Matters such as objective convergence and early stopping should be handled by the user.</source>
          <target state="translated">内部的には、このメソッドは &lt;code&gt;max_iter = 1&lt;/code&gt; 使用します。したがって、一度呼び出した後、コスト関数の最小値に達することは保証されません。客観的な収束や早期打ち切りなどの問題は、ユーザーが処理する必要があります。</target>
        </trans-unit>
        <trans-unit id="a6c7ce41d2f8fb06b74993c6b6972d365c014219" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython.</source>
          <target state="translated">内部的には、&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;と&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;を使用してすべての計算を処理します。これらのライブラリは、CおよびCythonを使用してラップされています。</target>
        </trans-unit>
        <trans-unit id="921b6b42e9e212246385b90b6e2081ffae4bdd4d" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;&lt;a href=&quot;#id14&quot; id=&quot;id9&quot;&gt;12&lt;/a&gt; and &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;&lt;a href=&quot;#id13&quot; id=&quot;id10&quot;&gt;11&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython. For a description of the implementation and details of the algorithms used, please refer to their respective papers.</source>
          <target state="translated">内部的には、我々が使用&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;LIBSVM &lt;/a&gt;&lt;a href=&quot;#id14&quot; id=&quot;id9&quot;&gt;12&lt;/a&gt;と&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear &lt;/a&gt;&lt;a href=&quot;#id13&quot; id=&quot;id10&quot;&gt;11を&lt;/a&gt;すべての計算を処理します。これらのライブラリは、CとCythonを使用してラップされています。実装の説明と使用されるアルゴリズムの詳細については、それぞれの論文を参照してください。</target>
        </trans-unit>
        <trans-unit id="a02157db035ff864370a2c436b6c81a38e8d8a3c" translate="yes" xml:space="preserve">
          <source>Interpreting coefficients: scale matters</source>
          <target state="translated">係数の解釈:スケールの重要性</target>
        </trans-unit>
        <trans-unit id="a2d983855292bfa7e006da9cc5e0020136bdcd0e" translate="yes" xml:space="preserve">
          <source>Interruption of multiprocesses jobs with &amp;lsquo;Ctrl-C&amp;rsquo;</source>
          <target state="translated">'Ctrl-C'によるマルチプロセスジョブの中断</target>
        </trans-unit>
        <trans-unit id="c8666d7061618ff72086e37218ea77619df4e168" translate="yes" xml:space="preserve">
          <source>Intuitive interpretation: clustering with bad V-measure can be &lt;strong&gt;qualitatively analyzed in terms of homogeneity and completeness&lt;/strong&gt; to better feel what &amp;lsquo;kind&amp;rsquo; of mistakes is done by the assignment.</source>
          <target state="translated">直感的な解釈：不適切なVメジャーのあるクラスタリングは&lt;strong&gt;、均質性と完全性の観点から定性的に分析し&lt;/strong&gt;て、割り当てによってどのような「種類の」間違いが行われたかをよりよく感じることができます。</target>
        </trans-unit>
        <trans-unit id="b3bf13a5a75c5bcae60f4d54f651f7f504b37960" translate="yes" xml:space="preserve">
          <source>Intuitively, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;precision&lt;/a&gt; is the ability of the classifier not to label as positive a sample that is negative, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;recall&lt;/a&gt; is the ability of the classifier to find all the positive samples.</source>
          <target state="translated">直感的には、&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;精度&lt;/a&gt;とは、分類子が負のサンプルを陽性としてラベル付けしない能力であり、&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;再現&lt;/a&gt;率は、分類子がすべての陽性サンプルを見つける能力です。</target>
        </trans-unit>
        <trans-unit id="d7d0867c1bea54b1fdaded0f6d4a137c7b95792e" translate="yes" xml:space="preserve">
          <source>Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data.</source>
          <target state="translated">直感的には、ヒストグラムはブロックの積み重ねであると考えることもできます。適切なグリッド空間にブロックを積み重ねることで、ヒストグラムが復元されます。しかし、通常のグリッド上にブロックを積み重ねるのではなく、各ブロックをそのブロックが表す点に中心を置き、各位置の高さの合計を合計するとどうなるでしょうか?このアイデアが左下の可視化につながります。ヒストグラムほどきれいではないかもしれませんが、データがブロックの位置を駆動するという事実は、基礎となるデータのはるかに優れた表現であることを意味します。</target>
        </trans-unit>
        <trans-unit id="a413ab311fb3ee6ba0089ad38e522b4769b873e8" translate="yes" xml:space="preserve">
          <source>Intuitively, the &lt;code&gt;gamma&lt;/code&gt; parameter defines how far the influence of a single training example reaches, with low values meaning &amp;lsquo;far&amp;rsquo; and high values meaning &amp;lsquo;close&amp;rsquo;. The &lt;code&gt;gamma&lt;/code&gt; parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.</source>
          <target state="translated">直感的に、 &lt;code&gt;gamma&lt;/code&gt; パラメータは、単一のトレーニング例の影響が及ぶ範囲を定義します。低い値は「遠い」を意味し、高い値は「近い」を意味します。 &lt;code&gt;gamma&lt;/code&gt; パラメータは、サポートベクトルとしてモデルによって選択されたサンプルの影響の半径の逆数として見ることができます。</target>
        </trans-unit>
        <trans-unit id="0af317bc827b64b57bcc63f42ad5928a61b8cb1f" translate="yes" xml:space="preserve">
          <source>Intuitively, this matrix can be interpreted as a matrix of pseudo features (the points raised to some power). The matrix is akin to (but different from) the matrix induced by a polynomial kernel.</source>
          <target state="translated">直感的には,この行列は疑似特徴量(ある乗に上げられた点)の行列として解釈することができます.この行列は,多項式カーネルによって誘導される行列に似ています(ただし,それとは異なります).</target>
        </trans-unit>
        <trans-unit id="d0136f60343b9ddfe4e95ff298a3f11e6a44a13d" translate="yes" xml:space="preserve">
          <source>Intuitively, we&amp;rsquo;re trying to maximize the margin (by minimizing \(||w||^2 = w^Tw\)), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value \(y_i (w^T \phi (x_i) + b)\) would be \(\geq 1\) for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance \(\zeta_i\) from their correct margin boundary. The penalty term &lt;code&gt;C&lt;/code&gt; controls the strengh of this penalty, and as a result, acts as an inverse regularization parameter (see note below).</source>
          <target state="translated">直感的には、マージンを最大化しようとしています（\（|| w || ^ 2 = w ^ Tw \）を最小化することにより）が、サンプルが誤って分類された場合、またはマージン境界内にある場合はペナルティが発生します。理想的には、値\（y_i（w ^ T \ phi（x_i）+ b）\）は、すべてのサンプルで\（\ geq 1 \）になり、完全な予測を示します。ただし、問題は通常、超平面で常に完全に分離できるとは限らないため、一部のサンプルを正しいマージン境界から\（\ zeta_i \）の距離に置くことができます。ペナルティ項 &lt;code&gt;C&lt;/code&gt; は、このペナルティの強さを制御し、その結果、逆正則化パラメーターとして機能します（以下の注を参照）。</target>
        </trans-unit>
        <trans-unit id="fcf37d79a0d7f3a40e6e7bdc86aa285b256f5c04" translate="yes" xml:space="preserve">
          <source>Inverse Gaussian</source>
          <target state="translated">逆ガウス</target>
        </trans-unit>
        <trans-unit id="20dc7b25181635b005eb94a34d79a1d1ef88f5eb" translate="yes" xml:space="preserve">
          <source>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</source>
          <target state="translated">正則化の強さの逆数;正の浮動小数点数でなければなりません。サポートベクターマシンのように,値が小さいほど強い正則化を指定します.</target>
        </trans-unit>
        <trans-unit id="33bf667eeef9f8f87ba0b221f0610de05f350c0d" translate="yes" xml:space="preserve">
          <source>Inverse the transformation.</source>
          <target state="translated">変換を逆にします。</target>
        </trans-unit>
        <trans-unit id="c6d1024dc4c416573a81f58d53b390ce79e27d74" translate="yes" xml:space="preserve">
          <source>Inverse the transformation. Return a vector of size nb_features with the values of Xred assigned to each group of features</source>
          <target state="translated">変換を逆にします.特徴の各グループに割り当てられた Xred の値を持つサイズ nb_features のベクトルを返します.</target>
        </trans-unit>
        <trans-unit id="68776e7556a932d7c1772f163bcd0ea5d3036f2f" translate="yes" xml:space="preserve">
          <source>Inverse transform matrix. Only available when &lt;code&gt;fit_inverse_transform&lt;/code&gt; is True.</source>
          <target state="translated">逆変換行列。 &lt;code&gt;fit_inverse_transform&lt;/code&gt; がTrueの場合にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="a53229d5506328691d3b32e8898ac28b845cf1d2" translate="yes" xml:space="preserve">
          <source>Inverse transformed array.</source>
          <target state="translated">逆変換された配列。</target>
        </trans-unit>
        <trans-unit id="6d0db9202e10d4b2a1eb16356a668a8027a929fc" translate="yes" xml:space="preserve">
          <source>Invokes the passed method name of the passed estimator. For method=&amp;rsquo;predict_proba&amp;rsquo;, the columns correspond to the classes in sorted order.</source>
          <target state="translated">渡された推定器の渡されたメソッド名を呼び出します。method = 'predict_proba'の場合、列はソートされた順序でクラスに対応します。</target>
        </trans-unit>
        <trans-unit id="93ce645e781a1eaea74d358c1f7aa54ffb25426b" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;'drop'&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; で &lt;code&gt;fit&lt;/code&gt; メソッドを呼び出すと、クラス属性 &lt;code&gt;self.estimators_&lt;/code&gt; に格納される元の推定量のクローンが適合します。 &lt;code&gt;set_params&lt;/code&gt; を使用して、推定量を &lt;code&gt;'drop'&lt;/code&gt; 設定できます。</target>
        </trans-unit>
        <trans-unit id="d37270b3f9f32ae673296712eb4a194d52812d8f" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;None&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; で &lt;code&gt;fit&lt;/code&gt; メソッドを呼び出すと、クラス属性 &lt;code&gt;self.estimators_&lt;/code&gt; に格納される元の推定量のクローンが適合します。推定器は、 &lt;code&gt;set_params&lt;/code&gt; を使用して &lt;code&gt;None&lt;/code&gt; に設定できます。</target>
        </trans-unit>
        <trans-unit id="0f0eee1c2a0f3878912e931a58a1d92e46d13c5a" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingRegressor&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;'drop'&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;VotingRegressor&lt;/code&gt; で &lt;code&gt;fit&lt;/code&gt; メソッドを呼び出すと、クラス属性 &lt;code&gt;self.estimators_&lt;/code&gt; に格納される元の推定量のクローンが適合します。 &lt;code&gt;set_params&lt;/code&gt; を使用して、推定量を &lt;code&gt;'drop'&lt;/code&gt; 設定できます。</target>
        </trans-unit>
        <trans-unit id="42b4a555867c758d3e1c4078b74a325ea5729a8f" translate="yes" xml:space="preserve">
          <source>Iris-Setosa</source>
          <target state="translated">Iris-Setosa</target>
        </trans-unit>
        <trans-unit id="0e4a66fb06fc31fa26bb267122a303163869bd83" translate="yes" xml:space="preserve">
          <source>Iris-Versicolour</source>
          <target state="translated">Iris-Versicolour</target>
        </trans-unit>
        <trans-unit id="c11352543468838c7f536aa067f758dd5cf065cc" translate="yes" xml:space="preserve">
          <source>Iris-Virginica</source>
          <target state="translated">Iris-Virginica</target>
        </trans-unit>
        <trans-unit id="bb0f5655f4fe0f8adc1a787c53ae1e836f4be186" translate="yes" xml:space="preserve">
          <source>Iso-probability lines for Gaussian Processes classification (GPC)</source>
          <target state="translated">ガウス過程分類(GPC)のための等確率線</target>
        </trans-unit>
        <trans-unit id="2b50512539d0e21a6687a0e4968f704ff8cc80fe" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm</source>
          <target state="translated">アイソレーションフォレストアルゴリズム</target>
        </trans-unit>
        <trans-unit id="90b7e1d9dae263e13bf54b6eb5bbce295b25c458" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm.</source>
          <target state="translated">アイソレーションフォレストアルゴリズム。</target>
        </trans-unit>
        <trans-unit id="00617c131e78d4c4ef41c400773154d235217731" translate="yes" xml:space="preserve">
          <source>IsolationForest example</source>
          <target state="translated">アイソレーションフォレストの例</target>
        </trans-unit>
        <trans-unit id="3a2755971bbebbe11d424139f5382799c401f262" translate="yes" xml:space="preserve">
          <source>Isomap Embedding</source>
          <target state="translated">アイソマップ埋め込み</target>
        </trans-unit>
        <trans-unit id="fe769adce6faebe1974c95ecc576637486cbe643" translate="yes" xml:space="preserve">
          <source>Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik, Mair Journal of Statistical Software 2009</source>
          <target state="translated">Rにおけるアイソトーン最適化:プール-隣接-ビオレータアルゴリズム(PAVA)とアクティブセット法 Leeuw,Hornik,Mair Journal of Statistical Software 2009</target>
        </trans-unit>
        <trans-unit id="906c68921cb26d68c13066c88efbe4d7d97d1205" translate="yes" xml:space="preserve">
          <source>Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol. 14, No. 2 (May, 1989), pp. 303-308</source>
          <target state="translated">アイソトニックメディアン回帰.線形計画法によるアプローチ Nilotpal Chakravarti オペレーションズリサーチの数学 第14巻第2号(1989年5月)、303-308頁</target>
        </trans-unit>
        <trans-unit id="73b36c35655a3846d59943ac16d2df052178f43b" translate="yes" xml:space="preserve">
          <source>Isotonic Regression</source>
          <target state="translated">等張回帰</target>
        </trans-unit>
        <trans-unit id="c214056f848cd4e39c52f175df94ac0d422815da" translate="yes" xml:space="preserve">
          <source>Isotonic fit of y.</source>
          <target state="translated">yのアイソトニックフィット。</target>
        </trans-unit>
        <trans-unit id="350a83a6eea9b1b3e9903b81e34485a4ebed4999" translate="yes" xml:space="preserve">
          <source>Isotonic regression model.</source>
          <target state="translated">等張回帰モデル。</target>
        </trans-unit>
        <trans-unit id="7c5ae8804283297e052b100d9986cbd5cd009701" translate="yes" xml:space="preserve">
          <source>Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.</source>
          <target state="translated">関数が呼び出された/クラスがインスタンス化されたときに警告を発行し、docstringに警告を追加します。</target>
        </trans-unit>
        <trans-unit id="4de98053a0f4264ca5362b17521388fcee7300ef" translate="yes" xml:space="preserve">
          <source>It adapts to the data at hand.</source>
          <target state="translated">手元のデータに適応します。</target>
        </trans-unit>
        <trans-unit id="d00cd2eb4ac76616c412b13d0e3140cdba7905a2" translate="yes" xml:space="preserve">
          <source>It allows specifying multiple metrics for evaluation.</source>
          <target state="translated">評価のための複数のメトリクスを指定することができます。</target>
        </trans-unit>
        <trans-unit id="f7ac040f9311efb440d25da16c027a81ab8e3ad5" translate="yes" xml:space="preserve">
          <source>It also can be expressed in set cardinality formulation:</source>
          <target state="translated">また、集合カーディナリティ定式化で表現することもできます。</target>
        </trans-unit>
        <trans-unit id="aedad5338d2a0edf1701c1d5c20ad5954bfd8c84" translate="yes" xml:space="preserve">
          <source>It can also be directly used as the &lt;code&gt;kernel&lt;/code&gt; argument:</source>
          <target state="translated">&lt;code&gt;kernel&lt;/code&gt; 引数として直接使用することもできます。</target>
        </trans-unit>
        <trans-unit id="9678c3fa14b59b03394b92e8e0080149cf3f64c8" translate="yes" xml:space="preserve">
          <source>It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).</source>
          <target state="translated">また、ブーリアンランダム変数を考慮する推定量の前処理ステップとしても使用できる(例:ベイズ設定でベルヌーイ分布を使用してモデル化されたもの)。</target>
        </trans-unit>
        <trans-unit id="94554b8e34efbb328f639daf4ccda2adc301f69d" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.</source>
          <target state="translated">また、数値以外のラベル(ハッシュ化可能で比較可能なものであれば)を数値ラベルに変換するためにも使用できます。</target>
        </trans-unit>
        <trans-unit id="f96e6d208d3d13906cbf9cd9c045b4122e99a4e4" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels:</source>
          <target state="translated">また、数値以外のラベル(ハッシュ化可能で比較可能なものであれば)を数値ラベルに変換するためにも使用できます。</target>
        </trans-unit>
        <trans-unit id="52f6dad43e1775ee0bbb04be9ef515bae958e0a2" translate="yes" xml:space="preserve">
          <source>It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting.</source>
          <target state="translated">また、オーバーフィットを防ぐためにモデルパラメータを縮小する損失関数に正則化項を追加することもできます。</target>
        </trans-unit>
        <trans-unit id="998bd5d13863b9f1e85f5a6708bf38f625d563b0" translate="yes" xml:space="preserve">
          <source>It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.</source>
          <target state="translated">また、scipy.sparse.linalgのARPACK実装を使用することもできます。</target>
        </trans-unit>
        <trans-unit id="a5f9c7ba1af0aaff84e6645b602de8095311d995" translate="yes" xml:space="preserve">
          <source>It can be called with parameters &lt;code&gt;(estimator, X, y)&lt;/code&gt;, where &lt;code&gt;estimator&lt;/code&gt; is the model that should be evaluated, &lt;code&gt;X&lt;/code&gt; is validation data, and &lt;code&gt;y&lt;/code&gt; is the ground truth target for &lt;code&gt;X&lt;/code&gt; (in the supervised case) or &lt;code&gt;None&lt;/code&gt; (in the unsupervised case).</source>
          <target state="translated">パラメータ &lt;code&gt;(estimator, X, y)&lt;/code&gt; を使用して呼び出すことができます。ここで &lt;code&gt;estimator&lt;/code&gt; は評価するモデル、 &lt;code&gt;X&lt;/code&gt; は検証データ、 &lt;code&gt;y&lt;/code&gt; は &lt;code&gt;X&lt;/code&gt; のグラウンドトゥルースターゲット（監視ありの場合）または &lt;code&gt;None&lt;/code&gt; （監視なしの場合）です。場合）。</target>
        </trans-unit>
        <trans-unit id="9a6afc7a825a539f282e6908ea3004d59da105e7" translate="yes" xml:space="preserve">
          <source>It can be downloaded/loaded using the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt; &lt;/a&gt;関数を使用してダウンロード/ロードできます。</target>
        </trans-unit>
        <trans-unit id="afe5a10e4cd1db3d3b82e37290c3a4b0be9670c8" translate="yes" xml:space="preserve">
          <source>It can be interpreted as a weighted difference per entry.</source>
          <target state="translated">エントリーごとの重み付けされた差と解釈することができます。</target>
        </trans-unit>
        <trans-unit id="d898e853ebb8a8ce7531765c1307531f5ab826e6" translate="yes" xml:space="preserve">
          <source>It can be noted that k-means (and minibatch k-means) are very sensitive to feature scaling and that in this case the IDF weighting helps improve the quality of the clustering by quite a lot as measured against the &amp;ldquo;ground truth&amp;rdquo; provided by the class label assignments of the 20 newsgroups dataset.</source>
          <target state="translated">k-means（およびミニバッチk-means）は特徴のスケーリングに非常に敏感であり、この場合、IDFの重み付けは、以下によって提供される「グラウンドトゥルース」に対して測定されるように、クラスター化の品質をかなり向上させるのに役立ちます。 20ニュースグループデータセットのクラスラベル割り当て。</target>
        </trans-unit>
        <trans-unit id="074f1a9d1908eeea94dd9624b8e4e74f70971f1f" translate="yes" xml:space="preserve">
          <source>It can be seen from the plots that the results of &lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;Orthogonal Matching Pursuit (OMP)&lt;/a&gt; with two non-zero coefficients is a bit less biased than when keeping only one (the edges look less prominent). It is in addition closer from the ground truth in Frobenius norm.</source>
          <target state="translated">プロットから、2つの非ゼロ係数を使用した&lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;直交マッチング追跡（OMP）&lt;/a&gt;の結果は、1つだけを保持する場合よりもバイアスが少し少ないことがわかります（エッジが目立たないように見えます）。さらに、フロベニウスノルムでは、グラウンドトゥルースに近い。</target>
        </trans-unit>
        <trans-unit id="0cf8fb702abea7c91fd29d6847c4f9bb34be57f9" translate="yes" xml:space="preserve">
          <source>It can be shown that the \(\nu\)-SVC formulation is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="translated">\(\nu\)SVCの定式化は、「\(C\)SVC」の再パラメータ化であり、数学的には等価であることが示される。</target>
        </trans-unit>
        <trans-unit id="157aa7190191e4be1be236c86eabe4e67d5e1efd" translate="yes" xml:space="preserve">
          <source>It can be used for univariate features selection, read more in the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">単変量特徴の選択に使用できます。詳細については、&lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;ユーザーガイドを&lt;/a&gt;ご覧ください。</target>
        </trans-unit>
        <trans-unit id="711c50760d4f6c264d6b8a92b5297202a600fa0b" translate="yes" xml:space="preserve">
          <source>It can be used to include regularization parameters in the estimation procedure.</source>
          <target state="translated">推定手順に正則化パラメータを含めるために使用することができます。</target>
        </trans-unit>
        <trans-unit id="e52b5bc871c7db656a1b43abcce93011714c74d2" translate="yes" xml:space="preserve">
          <source>It does not require a learning rate.</source>
          <target state="translated">学習率は必要ありません。</target>
        </trans-unit>
        <trans-unit id="4d19424efe5e9e20338f3273e68fb2ccbb132c12" translate="yes" xml:space="preserve">
          <source>It doesn&amp;rsquo;t give a single metric to use as an objective for clustering optimisation.</source>
          <target state="translated">クラスタリングの最適化の目的として使用する単一のメトリックを提供しません。</target>
        </trans-unit>
        <trans-unit id="53127b98db145a1107f55ea45dbcbf9eb40fb387" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;2&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="translated">[Hoyer、2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;2&lt;/a&gt;で、注意深く制約された場合、&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;はデータセットのパーツベースの表現を生成し、解釈可能なモデルを生成できることが観察されています。次の例は、PCA固有顔と比較して、Olivettifacesデータセットの画像から&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;によって検出された16個のスパースコンポーネントを表示します。</target>
        </trans-unit>
        <trans-unit id="0a46e66645323d1f8dad68441e68b478eacb85f4" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="translated">[Hoyer、2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt;で、注意深く制約されている場合、&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;はパーツベースのデータセットの表現を生成し、解釈可能なモデルを生成することが観察されています。次の例では、PCA固有面と比較して、オリベッティ面データセットの画像から&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;によって検出された16のスパースコンポーネントを表示します。</target>
        </trans-unit>
        <trans-unit id="8dcb00db48002a7fdf6f8f4ffd6c64f833e7dfb1" translate="yes" xml:space="preserve">
          <source>It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map.</source>
          <target state="translated">これは、コンピュータビジョンでよく使用される指数化カイ2乗カーネルに似た特性を持ちますが、特徴量マップの単純なモンテカルロ近似が可能です。</target>
        </trans-unit>
        <trans-unit id="37dc8b6f979214e286ace27e5272dd91d61126bc" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="translated">ノイズのないデータに適用されるMLで有用であることが証明されています。たとえば&lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;、量子力学の機械学習について簡単に説明し&lt;/a&gt;ます。</target>
        </trans-unit>
        <trans-unit id="bbe0780585e0153715a866cbcbfa3d1d5e8429c4" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="translated">ノイズのないデータに適用されるMLで役立つことが証明されています。&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;一言で言えば、量子力学の機械学習などを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="e3ea912466304dbf8e7c52052ad02a2c286c1ad1" translate="yes" xml:space="preserve">
          <source>It implements a variant of Random Kitchen Sinks.[1]</source>
          <target state="translated">ランダムキッチンシンクの変形を実装しています[1]。</target>
        </trans-unit>
        <trans-unit id="74d404b8e11acc4d9a6402146bf70c71d78d2e94" translate="yes" xml:space="preserve">
          <source>It is a Linear Model trained with an L1 prior as regularizer.</source>
          <target state="translated">これは、正則化器としてL1先行を用いて学習された線形モデルです。</target>
        </trans-unit>
        <trans-unit id="971eff281c404ac7ff23799c2f2e17c93f769de1" translate="yes" xml:space="preserve">
          <source>It is a memory-efficient, online-learning algorithm provided as an alternative to &lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt;. It constructs a tree data structure with the cluster centroids being read off the leaf. These can be either the final cluster centroids or can be provided as input to another clustering algorithm such as &lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">これは、&lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; の&lt;/a&gt;代替として提供される、メモリ効率の高いオンライン学習アルゴリズムです。クラスターの重心が葉から読み取られるツリーデータ構造を構築します。これらは、最終的なクラスターの重心にすることも、&lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; &lt;/a&gt;などの別のクラスター化アルゴリズムへの入力として提供することもできます。</target>
        </trans-unit>
        <trans-unit id="1a9933b24a1c1c056c0577574d6613078e271127" translate="yes" xml:space="preserve">
          <source>It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is &lt;code&gt;n_samples&lt;/code&gt;, the update method is same as batch learning. In the literature, this is called kappa.</source>
          <target state="translated">オンライン学習法における学習率を制御するパラメータです。漸近収束を保証するために、値は（ &lt;code&gt;n_samples&lt;/code&gt; ]の間に設定する必要があります。値が0.0で、batch_sizeがn_samplesの場合、更新方法はバッチ学習と同じです。文献では、これをカッパと呼びます。</target>
        </trans-unit>
        <trans-unit id="15f125826dc7be5a6512e2415a2ab7dc87afbdb7" translate="yes" xml:space="preserve">
          <source>It is advised to set the parameter &lt;code&gt;epsilon&lt;/code&gt; to 1.35 to achieve 95% statistical efficiency.</source>
          <target state="translated">95％の統計的効率を達成するには、パラメーター &lt;code&gt;epsilon&lt;/code&gt; を1.35 に設定することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="542f7392581e6c4610879b36a37978fc74650959" translate="yes" xml:space="preserve">
          <source>It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.</source>
          <target state="translated">また、テキスト処理のコミュニティでは、正規化されたカウント(別名項頻度)やTF-IDF値を持つ特徴量が実際にはわずかに優れた性能を発揮することが多いとしても、バイナリ特徴量を使用することが一般的です(おそらく確率論的な推論を単純化するためでしょう)。</target>
        </trans-unit>
        <trans-unit id="005dab4eb22b6ead110b29a8850c3898f552d977" translate="yes" xml:space="preserve">
          <source>It is also known as the Variance Ratio Criterion.</source>
          <target state="translated">分散比基準とも呼ばれています。</target>
        </trans-unit>
        <trans-unit id="657bf821e2dc05fc87b192deecf1d4c429b7d563" translate="yes" xml:space="preserve">
          <source>It is also possible to compute the permutation importances on the training set. This reveals that &lt;code&gt;random_num&lt;/code&gt; gets a significantly higher importance ranking than when computed on the test set. The difference between those two plots is a confirmation that the RF model has enough capacity to use that random numerical feature to overfit. You can further confirm this by re-running this example with constrained RF with min_samples_leaf=10.</source>
          <target state="translated">トレーニングセットの順列の重要度を計算することも可能です。これは、 &lt;code&gt;random_num&lt;/code&gt; がテストセットで計算された場合よりも大幅に高い重要度ランキングを取得することを示しています。これらの2つのプロットの違いは、RFモデルにそのランダムな数値特徴を使用して過剰適合させるのに十分な容量があることの確認です。 min_samples_leaf = 10の制約付きRFを使用してこの例を再実行すると、これをさらに確認できます。</target>
        </trans-unit>
        <trans-unit id="4221678f503b8b29e4ba191986027706279048ac" translate="yes" xml:space="preserve">
          <source>It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros.</source>
          <target state="translated">また、データ内に存在する可能性のある制約と一致するように、辞書および/またはコードを正の制約にすることも可能である。以下に、異なる正の制約が適用された面を示します。赤は負の値、青は正の値、白はゼロを表しています。</target>
        </trans-unit>
        <trans-unit id="cd4e94997f77b01819c6451ba1d9a9d98a78db7c" translate="yes" xml:space="preserve">
          <source>It is also possible to efficiently produce a sparse graph showing the connections between neighboring points:</source>
          <target state="translated">また、隣接する点間のつながりを示す疎なグラフを効率的に作成することも可能である。</target>
        </trans-unit>
        <trans-unit id="0396c69b921d6f190c09b79532ebbdc31b35e115" translate="yes" xml:space="preserve">
          <source>It is also possible to encode each column into &lt;code&gt;n_categories - 1&lt;/code&gt; columns instead of &lt;code&gt;n_categories&lt;/code&gt; columns by using the &lt;code&gt;drop&lt;/code&gt; parameter. This parameter allows the user to specify a category for each feature to be dropped. This is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression (&lt;a href=&quot;generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt;&lt;code&gt;LinearRegression&lt;/code&gt;&lt;/a&gt;), since co-linearity would cause the covariance matrix to be non-invertible. When this parameter is not None, &lt;code&gt;handle_unknown&lt;/code&gt; must be set to &lt;code&gt;error&lt;/code&gt;:</source>
          <target state="translated">それに各列を符号化することも可能である &lt;code&gt;n_categories - 1&lt;/code&gt; 代わりの列 &lt;code&gt;n_categories&lt;/code&gt; のカラムを使用して、 &lt;code&gt;drop&lt;/code&gt; パラメータ。このパラメーターを使用すると、ユーザーはドロップする各機能のカテゴリーを指定できます。これは、一部の分類器で入力行列の共直線性を回避するのに役立ちます。このような機能は、たとえば、非正則化回帰（&lt;a href=&quot;generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt; &lt;code&gt;LinearRegression&lt;/code&gt; &lt;/a&gt;）を使用する場合に役立ちます。これは、共線形性により共分散行列が非可逆になるためです。このパラメーターがNoneでない場合、 &lt;code&gt;handle_unknown&lt;/code&gt; を &lt;code&gt;error&lt;/code&gt; に設定する必要があります。</target>
        </trans-unit>
        <trans-unit id="e5cc911e1a3213d4a6ea82327423c6b7195a9251" translate="yes" xml:space="preserve">
          <source>It is also possible to map data to a normal distribution using &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; by setting &lt;code&gt;output_distribution='normal'&lt;/code&gt;. Using the earlier example with the iris dataset:</source>
          <target state="translated">使用正規分布にデータをマッピングすることも可能である&lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; を&lt;/a&gt;設定することにより &lt;code&gt;output_distribution='normal'&lt;/code&gt; 。以前の例をirisデータセットで使用します。</target>
        </trans-unit>
        <trans-unit id="f35eb3fdcbe153523a6b78440df1aad8edf3b026" translate="yes" xml:space="preserve">
          <source>It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:</source>
          <target state="translated">例えば、クロスバリデーションイテレータを渡すことで、他のクロスバリデーション戦略を使用することも可能です。</target>
        </trans-unit>
        <trans-unit id="c62a692f1bef88aa9ea1dc55b02f68e6ac2b429f" translate="yes" xml:space="preserve">
          <source>It is classically used to separate mixed signals (a problem known as &lt;em&gt;blind source separation&lt;/em&gt;), as in the example below:</source>
          <target state="translated">以下の例のように、混合信号を分離するために古典的に使用されます（&lt;em&gt;ブラインドソース分離&lt;/em&gt;として知られる問題）。</target>
        </trans-unit>
        <trans-unit id="579f13cf2a54010546e31ecfaa7ced83f4da4e12" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as an ordinary least squares.</source>
          <target state="translated">これは前方選択と同じくらいの計算速度で、通常の最小二乗法と同じくらいの複雑さを持っています。</target>
        </trans-unit>
        <trans-unit id="eaad2537722d5ddb17252eb65683de60a4e9ec00" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as ordinary least squares.</source>
          <target state="translated">これは前方選択と同じくらいの計算速度で、通常の最小二乗と同じ複雑さを持っています。</target>
        </trans-unit>
        <trans-unit id="2a0b5a3028e23e8f66ba4845cf98605a35e74ca3" translate="yes" xml:space="preserve">
          <source>It is converted to an F score then to a p-value.</source>
          <target state="translated">それはFスコアに変換され、次にp値に変換されます。</target>
        </trans-unit>
        <trans-unit id="9da4ca4cacbca0baec3287f1b2124c4dcd00df7a" translate="yes" xml:space="preserve">
          <source>It is easily modified to produce solutions for other estimators, like the Lasso.</source>
          <target state="translated">Lassoのような他の推定器のためのソリューションを生成するために簡単に変更することができます。</target>
        </trans-unit>
        <trans-unit id="a180f7cced602efdc3c3224733570427c990972f" translate="yes" xml:space="preserve">
          <source>It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren&amp;rsquo;t from this window of time.</source>
          <target state="translated">分類器が、ニュースグループヘッダーなどの20のニュースグループデータに表示される特定のものに適合しすぎるのは簡単です。多くの分類子は非常に高いFスコアを達成しますが、その結果は、この時間枠以外のドキュメントに一般化されません。</target>
        </trans-unit>
        <trans-unit id="f3493e2a2c4e4ad9e265942ad2fd137cc9804a32" translate="yes" xml:space="preserve">
          <source>It is generally recommended to avoid using significantly more processes or threads than the number of CPUs on a machine. Over-subscription happens when a program is running too many threads at the same time.</source>
          <target state="translated">一般的には、マシンのCPU数よりも大幅に多くのプロセスやスレッドを使用しないようにすることをお勧めします。オーバーサブスクリプションは、プログラムが同時に多くのスレッドを実行している場合に発生します。</target>
        </trans-unit>
        <trans-unit id="2e5aa329cff0eb3a121eaf66246e864cad7413ee" translate="yes" xml:space="preserve">
          <source>It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten&amp;rsquo;s FAQ [2].</source>
          <target state="translated">特徴の数が非常に多い場合は、別の次元削減方法（たとえば、密なデータの場合はPCA、疎なデータの場合はTruncatedSVD）を使用して、次元数を妥当な量（たとえば50）に減らすことを強くお勧めします。これにより、ノイズが抑制され、サンプル間のペアワイズ距離の計算が高速化されます。その他のヒントについては、Laurens van der MaatenのFAQ [2]を参照してください。</target>
        </trans-unit>
        <trans-unit id="4c694641d1b1cd9e68259bd2f7aabf747615adda" translate="yes" xml:space="preserve">
          <source>It is important to assign an identifier to unlabeled points along with the labeled data when training the model with the &lt;code&gt;fit&lt;/code&gt; method. The identifier that this implementation uses is the integer value \(-1\).</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 法でモデルをトレーニングするときは、ラベルの付いていないデータにラベルの付いていないポイントに識別子を割り当てることが重要です。この実装が使用する識別子は整数値\（-1 \）です。</target>
        </trans-unit>
        <trans-unit id="643f8f6ee250eb138ea3f0ff80df853cb01ed9c1" translate="yes" xml:space="preserve">
          <source>It is important to keep in mind that the coefficients that have been dropped may still be related to the outcome by themselves: the model chose to suppress them because they bring little or no additional information on top of the other features. Additionnaly, this selection is unstable for correlated features, and should be interpreted with caution.</source>
          <target state="translated">削除された係数は、それ自体がまだ結果に関連している可能性があることに留意することが重要です:モデルは、他の特徴の上に追加の情報をほとんどもたらさないため、それらを抑制することを選択しました。さらに、この選択は、相関のある特徴に対して不安定であり、注意して解釈する必要があります。</target>
        </trans-unit>
        <trans-unit id="a067b4f8fd8c4002a8fc9abd7aa015e146d303ad" translate="yes" xml:space="preserve">
          <source>It is important to note that when the number of samples is much larger than the number of features, one would expect that no shrinkage would be necessary. The intuition behind this is that if the population covariance is full rank, when the number of sample grows, the sample covariance will also become positive definite. As a result, no shrinkage would necessary and the method should automatically do this.</source>
          <target state="translated">重要なことは,標本の数が特徴量の数よりもはるかに大きい場合,縮小は必要ないと予想されることです.これは,母集団共分散がフルランクであれば,標本数が増えると標本共分散も正定値になるという直感的なものです.その結果、収縮は必要なく、この方法は自動的に行われるはずです。</target>
        </trans-unit>
        <trans-unit id="d9e45bb570908f10d72f7b51c91c236b78670a3c" translate="yes" xml:space="preserve">
          <source>It is made of 150 observations of irises, each described by 4 features: their sepal and petal length and width, as detailed in &lt;code&gt;iris.DESCR&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;iris.DESCR&lt;/code&gt; で詳述されているように、それはアイリスの150の観測で構成され、それぞれが4つの特徴（それらのがく片と花弁の長さと幅）によって記述されます。</target>
        </trans-unit>
        <trans-unit id="1856ef8269f110a1ccc7c37c9db810b8176fc5f8" translate="yes" xml:space="preserve">
          <source>It is more efficient than the LassoCV if only a small number of features are selected compared to the total number, for instance if there are very few samples compared to the number of features.</source>
          <target state="translated">例えば,特徴量の数に対してサンプル数が非常に少ない場合など,全体の数に対して少ない数の特徴量しか選択されない場合には,LassoCVよりも効率的である.</target>
        </trans-unit>
        <trans-unit id="1d58fe1839ae301f10f6b9aaac159ed67a9eabfe" translate="yes" xml:space="preserve">
          <source>It is not appropriate to pass these predictions into an evaluation metric. Use &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt;&lt;code&gt;cross_validate&lt;/code&gt;&lt;/a&gt; to measure generalization error.</source>
          <target state="translated">これらの予測を評価指標に渡すことは適切ではありません。汎化エラーを測定するには、&lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt; &lt;code&gt;cross_validate&lt;/code&gt; &lt;/a&gt;を使用します。</target>
        </trans-unit>
        <trans-unit id="9af2e9f8fa22ff915f29e1a168987ff536812b91" translate="yes" xml:space="preserve">
          <source>It is not recommended to hard-code the backend name in a call to Parallel in a library. Instead it is recommended to set soft hints (prefer) or hard constraints (require) so as to make it possible for library users to change the backend from the outside using the parallel_backend context manager.</source>
          <target state="translated">ライブラリ内のParallel呼び出しでバックエンド名をハードコーディングすることは推奨されません。代わりに、ライブラリユーザが parallel_backend コンテキストマネージャを使って外部からバックエンドを変更できるように、ソフトヒント(好ましい)やハード制約(必要)を設定することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="b26cf025ddad5c1f883715bf24d85887eccade22" translate="yes" xml:space="preserve">
          <source>It is not regularized (penalized).</source>
          <target state="translated">正規化されていない(ペナルティを課されている)。</target>
        </trans-unit>
        <trans-unit id="3023247377e7882a0cbda1c2d8280926be6aa8ba" translate="yes" xml:space="preserve">
          <source>It is now possible to prune most tree-based estimators once the trees are built. The pruning is based on minimal cost-complexity. Read more in the &lt;a href=&quot;../../modules/tree#minimal-cost-complexity-pruning&quot;&gt;User Guide&lt;/a&gt; for details.</source>
          <target state="translated">ツリーが構築されると、ほとんどのツリーベースの推定量を剪定することが可能になりました。剪定は、最小限のコストの複雑さに基づいています。詳細については、&lt;a href=&quot;../../modules/tree#minimal-cost-complexity-pruning&quot;&gt;ユーザーガイド&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="f18c5e38092754d2adb7bb6eb5c0799854e297b3" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where p &amp;gt;&amp;gt; n (i.e., when the number of dimensions is significantly greater than the number of points)</source>
          <target state="translated">p &amp;gt;&amp;gt; nの場合（つまり、次元数がポイント数よりも大幅に多い場合）には、数値的に効率的です。</target>
        </trans-unit>
        <trans-unit id="3387956abcbcbc8c7f2e1b04a07247bbce69a743" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where the number of features is significantly greater than the number of samples.</source>
          <target state="translated">これは、特徴量の数がサンプル数よりも有意に多い場合に数値的に効率的です。</target>
        </trans-unit>
        <trans-unit id="8c7122bd43c891f087ca247f2fcde5236f637b0c" translate="yes" xml:space="preserve">
          <source>It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values.</source>
          <target state="translated">低特異値に関連する成分の特異値ベクトルを落とすことで、分散の大部分を保持する低次元空間にデータを投影することは、しばしば興味深いことである。</target>
        </trans-unit>
        <trans-unit id="5ed0af274291a2311daa7ee05d7bd79f85fc7e49" translate="yes" xml:space="preserve">
          <source>It is possible and recommended to search the hyper-parameter space for the best &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt; score.</source>
          <target state="translated">ハイパーパラメータースペースを検索して、最高の&lt;a href=&quot;cross_validation#cross-validation&quot;&gt;交差検証&lt;/a&gt;スコアを取得することが可能であり、推奨されています。</target>
        </trans-unit>
        <trans-unit id="19b21329d1ba1e2fd90b4634d03905cf0f5e7826" translate="yes" xml:space="preserve">
          <source>It is possible to adjust the threshold of the binarizer:</source>
          <target state="translated">バイナライザーのしきい値を調整することができます。</target>
        </trans-unit>
        <trans-unit id="5fdc4b2c9af36a36fca156373f6cb7c574f550b3" translate="yes" xml:space="preserve">
          <source>It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:</source>
          <target state="translated">平均化の代わりに、ラベルごとの精度、リコール、F1スコア、サポートを計算することができます。</target>
        </trans-unit>
        <trans-unit id="c890fcaf4f6baafc6ccf39a67fce7daf92b8b950" translate="yes" xml:space="preserve">
          <source>It is possible to control the randomness for reproducibility of the results by explicitly seeding the &lt;code&gt;random_state&lt;/code&gt; pseudo random number generator.</source>
          <target state="translated">&lt;code&gt;random_state&lt;/code&gt; 疑似乱数ジェネレータを明示的にシードすることにより、結果の再現性のためにランダム性を制御することが可能です。</target>
        </trans-unit>
        <trans-unit id="8c436001d07579c89b669f127dbaf0c3bd65de34" translate="yes" xml:space="preserve">
          <source>It is possible to customize the behavior by passing a callable to the vectorizer constructor:</source>
          <target state="translated">vectorizer のコンストラクタに callable を渡すことで動作をカスタマイズすることができます。</target>
        </trans-unit>
        <trans-unit id="0839b4d3a34db46e778e581b781425b62631583b" translate="yes" xml:space="preserve">
          <source>It is possible to disable either centering or scaling by either passing &lt;code&gt;with_mean=False&lt;/code&gt; or &lt;code&gt;with_std=False&lt;/code&gt; to the constructor of &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;with_mean=False&lt;/code&gt; または &lt;code&gt;with_std=False&lt;/code&gt; を&lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt;のコンストラクターに渡すことにより、センタリングまたはスケーリングを無効にすることができます。</target>
        </trans-unit>
        <trans-unit id="85a1eed4b8a0f5199be27070848fbc013f8f8638" translate="yes" xml:space="preserve">
          <source>It is possible to get back the category names as follows:</source>
          <target state="translated">以下のようにカテゴリー名を取り戻すことが可能です。</target>
        </trans-unit>
        <trans-unit id="6b11842b410c7ed9014abd60118219965dd51782" translate="yes" xml:space="preserve">
          <source>It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:</source>
          <target state="translated">学習データ上で学習された変換の正確な性質を知るために,スケーラ属性を内観することが可能である.</target>
        </trans-unit>
        <trans-unit id="d994dbf018869cdf387e647211852d55a08f6930" translate="yes" xml:space="preserve">
          <source>It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">ロードするカテゴリーのリストを&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt;関数に渡すことにより、カテゴリーのサブセレクションのみをロードすることが可能です：</target>
        </trans-unit>
        <trans-unit id="03749a52f5e2e7e976928d01767369407c7307c4" translate="yes" xml:space="preserve">
          <source>It is possible to mix sparse and dense arrays in the same run:</source>
          <target state="translated">疎な配列と密な配列を同時に実行することが可能です.</target>
        </trans-unit>
        <trans-unit id="54adadb321f18cc462f6fa41bc0c4a25f1f6b29d" translate="yes" xml:space="preserve">
          <source>It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The &lt;code&gt;statsmodels
package &amp;lt;https://pypi.org/project/statsmodels/&amp;gt;&lt;/code&gt; natively supports this. Within sklearn, one could use bootstrapping instead as well.</source>
          <target state="translated">ペナルティなしで回帰の場合、係数のp値と信頼区間を取得することができます。 &lt;code&gt;statsmodels package &amp;lt;https://pypi.org/project/statsmodels/&amp;gt;&lt;/code&gt; ネイティブにはこれをサポートしています。sklearn内では、代わりにブートストラップを使用することもできます。</target>
        </trans-unit>
        <trans-unit id="be16ce674bae3bf54f6cdc3d4d41c5990ca746b6" translate="yes" xml:space="preserve">
          <source>It is possible to overcome those limitations by combining the &amp;ldquo;hashing trick&amp;rdquo; (&lt;a href=&quot;#feature-hashing&quot;&gt;Feature hashing&lt;/a&gt;) implemented by the &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt;&lt;/a&gt; class and the text preprocessing and tokenization features of the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt; &lt;/a&gt;クラスによって実装された「ハッシュトリック」（&lt;a href=&quot;#feature-hashing&quot;&gt;機能ハッシュ&lt;/a&gt;）とCountVectorizerのテキスト前処理およびトークン化機能を組み合わせることにより、これらの制限を克服することが可能&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="737f1fd475d1dc22b14b4896563d476e36ccb4e8" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Pythonの組み込み永続化モデルである&lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;を使用して、scikit-learnでモデルを保存できます。</target>
        </trans-unit>
        <trans-unit id="1d319918af937e7b588f1bdf4ba0c9bc1d2e6a8f" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Pythonの組み込み永続化モデル、つまり&lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;を使用して、scikit-learnでモデルを保存できます。</target>
        </trans-unit>
        <trans-unit id="c1dfe5304fca32594b4f7b15a0ed1671355448d1" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/3/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Pythonの組み込み永続性モデル、つまり&lt;a href=&quot;https://docs.python.org/3/library/pickle.html&quot;&gt;pickle&lt;/a&gt;を使用して、scikit-learnにモデルを保存することができます。</target>
        </trans-unit>
        <trans-unit id="33bdca276514e666ea92e40ef8d9c04e5206a96b" translate="yes" xml:space="preserve">
          <source>It is possible to specify this explicitly using the parameter &lt;code&gt;categories&lt;/code&gt;. There are two genders, four possible continents and four web browsers in our dataset:</source>
          <target state="translated">パラメータの &lt;code&gt;categories&lt;/code&gt; を使用して、これを明示的に指定することが可能です。データセットには、2つの性別、4つの可能な大陸、4つのWebブラウザーがあります。</target>
        </trans-unit>
        <trans-unit id="7291e604dadcc649d0d77ccb4ebf5e3c457ba713" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt;&lt;code&gt;plot_confusion_matrix&lt;/code&gt;&lt;/a&gt; to create a &lt;a href=&quot;#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt;&lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt;&lt;/a&gt;. All parameters are stored as attributes.</source>
          <target state="translated">&lt;a href=&quot;sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt; &lt;code&gt;plot_confusion_matrix&lt;/code&gt; &lt;/a&gt;を使用して&lt;a href=&quot;#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt; &lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt; &lt;/a&gt;を作成することをお勧めします。すべてのパラメータは属性として保存されます。</target>
        </trans-unit>
        <trans-unit id="ab8a32e7f4d09197b24de455e36e8e5fe3df1e84" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt;&lt;code&gt;plot_precision_recall_curve&lt;/code&gt;&lt;/a&gt; to create a visualizer. All parameters are stored as attributes.</source>
          <target state="translated">&lt;a href=&quot;sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt; &lt;code&gt;plot_precision_recall_curve&lt;/code&gt; &lt;/a&gt;を使用してビジュアライザーを作成することをお勧めします。すべてのパラメータは属性として保存されます。</target>
        </trans-unit>
        <trans-unit id="80b150a43cd5aba26116528fb4f30933db56b582" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt;&lt;code&gt;plot_roc_curve&lt;/code&gt;&lt;/a&gt; to create a visualizer. All parameters are stored as attributes.</source>
          <target state="translated">&lt;a href=&quot;sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt; &lt;code&gt;plot_roc_curve&lt;/code&gt; &lt;/a&gt;を使用してビジュアライザーを作成することをお勧めします。すべてのパラメータは属性として保存されます。</target>
        </trans-unit>
        <trans-unit id="f18c0fc60439524a8f745b0ef99ce87a71814897" translate="yes" xml:space="preserve">
          <source>It is recommended to use &lt;a href=&quot;sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create a &lt;a href=&quot;#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt;. All parameters are stored as attributes.</source>
          <target state="translated">使用することをお勧めし&lt;a href=&quot;sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; を&lt;/a&gt;作成する&lt;a href=&quot;#sklearn.inspection.PartialDependenceDisplay&quot;&gt; &lt;code&gt;PartialDependenceDisplay&lt;/code&gt; を&lt;/a&gt;。すべてのパラメータは属性として保存されます。</target>
        </trans-unit>
        <trans-unit id="cbfa3a3539d1ad40958cd50540686b2891b5e349" translate="yes" xml:space="preserve">
          <source>It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features.</source>
          <target state="translated">下流モデルでは、さらに特徴の線形独立性について何らかの仮定をすることができるので、特徴を独立にセンタリングしてスケールするだけでは十分ではないことがあります。</target>
        </trans-unit>
        <trans-unit id="b49f552a6383ab2c79a28eb8ae358eb905c8df59" translate="yes" xml:space="preserve">
          <source>It is sometimes tedious to find the model which will best perform on a given dataset. Stacking provide an alternative by combining the outputs of several learners, without the need to choose a model specifically. The performance of stacking is usually close to the best model and sometimes it can outperform the prediction performance of each individual model.</source>
          <target state="translated">与えられたデータセットで最もパフォーマンスを発揮するモデルを見つけるのは時に面倒です。スタッキングは,特にモデルを選択する必要がなく,複数の学習者の出力を組み合わせることで,代替手段を提供する.スタッキングの性能は,通常,最良のモデルに近く,時には個々のモデルの予測性能を上回ることもある.</target>
        </trans-unit>
        <trans-unit id="311a1593daf1f1187805481bab88c0d399c3cecf" translate="yes" xml:space="preserve">
          <source>It is sometimes worthwhile storing the state of a specific transformer since it could be used again. Using a pipeline in &lt;code&gt;GridSearchCV&lt;/code&gt; triggers such situations. Therefore, we use the argument &lt;code&gt;memory&lt;/code&gt; to enable caching.</source>
          <target state="translated">特定の変圧器は再び使用できるため、特定の変圧器の状態を保存しておくと便利な場合があります。 &lt;code&gt;GridSearchCV&lt;/code&gt; でパイプラインを使用すると、このような状況がトリガーされます。したがって、引数 &lt;code&gt;memory&lt;/code&gt; を使用してキャッシュを有効にします。</target>
        </trans-unit>
        <trans-unit id="9d1619fcc011ef5a461992b135770b43d5982f12" translate="yes" xml:space="preserve">
          <source>It is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values.</source>
          <target state="translated">ユーザーが欠測値による不確実性の測定に興味がない場合、予測や分類の文脈で単一入力と複数入力がどの程度有用であるかについては、まだ未解決の問題です。</target>
        </trans-unit>
        <trans-unit id="ba51434e495bfdf85ff2401c563345468fae8389" translate="yes" xml:space="preserve">
          <source>It is the fastest algorithm for learning mixture models</source>
          <target state="translated">混合モデルを学習するための最速のアルゴリズムである</target>
        </trans-unit>
        <trans-unit id="5c9cedaa4c291702a05bee05d8b7517536cf8c97" translate="yes" xml:space="preserve">
          <source>It is the opposite as as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">これは、大きい方が良いのとは逆で、つまり、大きな値はインライアに対応します。</target>
        </trans-unit>
        <trans-unit id="eec2b41e384c85c1e4587a1c1f6fa007f24ac337" translate="yes" xml:space="preserve">
          <source>It is the opposite as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">これは逆で、大きい方が良いということで、大きな値はインライアに対応します。</target>
        </trans-unit>
        <trans-unit id="b0c456c256349cc53ca134d105c8be601465dd39" translate="yes" xml:space="preserve">
          <source>It is worth noting that RandomForests and ExtraTrees can be fitted in parallel on many cores as each tree is built independently of the others. AdaBoost&amp;rsquo;s samples are built sequentially and so do not use multiple cores.</source>
          <target state="translated">RandomForestsとExtraTreesは、各ツリーが他のツリーから独立して構築されているため、多くのコアに並列でフィットできることに注意してください。AdaBoostのサンプルは順番に作成されるため、複数のコアを使用しないでください。</target>
        </trans-unit>
        <trans-unit id="9876d3b6328cf0e41b8f18a6a35d45d86ad7b5f1" translate="yes" xml:space="preserve">
          <source>It is worth noting that more than 93% of policyholders have zero claims. If we were to convert this problem into a binary classification task, it would be significantly imbalanced, and even a simplistic model that would only predict mean can achieve an accuracy of 93%.</source>
          <target state="translated">注目すべきは、保険契約者の93%以上がクレームゼロであることである。この問題を2値分類のタスクに変換すると、著しくアンバランスになり、平均値だけを予測する単純なモデルでも93%の精度を達成することができます。</target>
        </trans-unit>
        <trans-unit id="86441e9f0bfca4823b62f4a6d4cecce7c1b80a8e" translate="yes" xml:space="preserve">
          <source>It might be possible to trade some accuracy on the training set for a slightly better accuracy on the test set by limiting the capacity of the trees (for instance by setting &lt;code&gt;min_samples_leaf=5&lt;/code&gt; or &lt;code&gt;min_samples_leaf=10&lt;/code&gt;) so as to limit overfitting while not introducing too much underfitting.</source>
          <target state="translated">ツリーの容量を制限することにより（たとえば、 &lt;code&gt;min_samples_leaf=5&lt;/code&gt; または &lt;code&gt;min_samples_leaf=10&lt;/code&gt; を設定することにより）、トレーニングセットの精度をテストセットの精度と交換して、過剰適合を制限し、導入しないことも可能です。多くの不適合。</target>
        </trans-unit>
        <trans-unit id="c1e3cdc828409a9c2610db18343fc252165aed81" translate="yes" xml:space="preserve">
          <source>It might seem questionable to use a (penalized) Least Squares loss to fit a classification model instead of the more traditional logistic or hinge losses. However in practice all those models can lead to similar cross-validation scores in terms of accuracy or precision/recall, while the penalized least squares loss used by the &lt;a href=&quot;generated/sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier&quot;&gt;&lt;code&gt;RidgeClassifier&lt;/code&gt;&lt;/a&gt; allows for a very different choice of the numerical solvers with distinct computational performance profiles.</source>
          <target state="translated">従来のロジスティック損失やヒンジ損失の代わりに、（ペナルティ付きの）最小二乗損失を使用して分類モデルを適合させることは疑わしいように思われるかもしれません。ただし、実際には、これらすべてのモデルは、精度または適合率/再現率の点で同様の交差&lt;a href=&quot;generated/sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier&quot;&gt; &lt;code&gt;RidgeClassifier&lt;/code&gt; &lt;/a&gt;可能性がありますが、RidgeClassifierによって使用されるペナルティ付き最小二乗損失により、異なる計算パフォーマンスプロファイルを持つ数値ソルバーの非常に異なる選択が可能になります。</target>
        </trans-unit>
        <trans-unit id="a4029704b1c865bc18db0f7f71b472d5421882ac" translate="yes" xml:space="preserve">
          <source>It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model.</source>
          <target state="translated">これは、クロスバリデーションやモデルを調整する同様の試みで有用な、完全なピースワイズ線形解パスを生成します。</target>
        </trans-unit>
        <trans-unit id="dec67b5f65557893043d8c253cdd2dab65f3a96a" translate="yes" xml:space="preserve">
          <source>It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.</source>
          <target state="translated">これは,モデル中の独立変数によって説明された分散(y)の割合を表す.これは適合度の指標を提供し,したがって,説明された分散の割合を通じて,見えない標本がモデルによって予測される可能性がどれだけ高いかの指標となる.</target>
        </trans-unit>
        <trans-unit id="a4dee1947755b5fe4ca1a29e0b9b0f0b85817660" translate="yes" xml:space="preserve">
          <source>It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.</source>
          <target state="translated">こ れは、 テ ス ト ス コ アに加えて、 fit-time ・ score-time (お よ びオプシ ョ ンでは学習 ス コ ア と 、 嵌め込み推定子を含む)を含む dict を返し ます。</target>
        </trans-unit>
        <trans-unit id="be2089f68dcca4fd6c2250f878425dd499fc444a" translate="yes" xml:space="preserve">
          <source>It returns a dictionary-like object, with the following attributes:</source>
          <target state="translated">以下の属性を持つ辞書的なオブジェクトを返します。</target>
        </trans-unit>
        <trans-unit id="6f65f619abd94296c7075a4b5d91a76ac1e641bc" translate="yes" xml:space="preserve">
          <source>It returns a floating point number that quantifies the &lt;code&gt;estimator&lt;/code&gt; prediction quality on &lt;code&gt;X&lt;/code&gt;, with reference to &lt;code&gt;y&lt;/code&gt;. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.</source>
          <target state="translated">これは、 &lt;code&gt;y&lt;/code&gt; を参照して、 &lt;code&gt;X&lt;/code&gt; での &lt;code&gt;estimator&lt;/code&gt; 予測品質を定量化する浮動小数点数を返します。繰り返しになりますが、慣例により、数値が大きいほど良いので、スコアラーが損失を返す場合、その値は無効にする必要があります。</target>
        </trans-unit>
        <trans-unit id="58c0c1b9288f5ba70bfdf3e509c8376ea38265d4" translate="yes" xml:space="preserve">
          <source>It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.</source>
          <target state="translated">ジョンソン-リンデンストラウス・リーマは、データセットの構造を仮定しないので、必要な成分の数を非常に保守的に見積もることができることに注意する必要があります。</target>
        </trans-unit>
        <trans-unit id="af7347a7c717add0101a2649bad5550dc47a184a" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;を使用して、数字データセットのSVMで分類するためにRBFカーネルの特徴マップを概算する方法を示します。元の空間で線形SVMを使用した結果、近似マッピングを使用した線形SVM、およびカーネル化されたSVMを使用した結果が比較されます。さまざまな量のモンテカルロサンプリング（ランダムフーリエ機能を使用する&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;の場合）のタイミングと精度、および近似マッピング用のトレーニングセットのさまざまなサイズのサブセット（&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; の場合&lt;/a&gt;）が示されています。</target>
        </trans-unit>
        <trans-unit id="5bcecde02163a3a6b9fb69b7700a66c21be36347" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="translated">これは、&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;を使用して、数字データセットのSVMで分類するためのRBFカーネルの特徴マップを概算する方法を示しています。元の空間で線形SVMを使用した結果、近似マッピングを使用した線形SVM、およびカーネル化SVMを使用した結果が比較されます。さまざまな量のモンテカルロサンプリング（ランダムなフーリエ機能を使用する&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;の場合）のタイミングと精度、および近似マッピングのトレーニングセット（&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; の場合&lt;/a&gt;）の異なるサイズのサブセットが表示されます。</target>
        </trans-unit>
        <trans-unit id="45249c231a1e8d583e28deb277d22f5fe88e16b7" translate="yes" xml:space="preserve">
          <source>It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=&amp;rsquo;l1&amp;rsquo; or projected on the euclidean unit sphere if norm=&amp;rsquo;l2&amp;rsquo;.</source>
          <target state="translated">テキストドキュメントのコレクションをscipy.sparseマトリックスに変換し、トークンの出現回数（またはバイナリの出現情報）を保持します。norm= 'l1'の場合はトークンの頻度として正規化され、norm = 'l2'の場合はユークリッド単位の球に投影されます。</target>
        </trans-unit>
        <trans-unit id="9b65a724f589693294d8b39fded9beef68bf84ef" translate="yes" xml:space="preserve">
          <source>It updates its model only on mistakes.</source>
          <target state="translated">間違いがあった時だけモデルを更新してくれます。</target>
        </trans-unit>
        <trans-unit id="4ef1ebaa3d2757730ff62ab1b50211fc95aec89a" translate="yes" xml:space="preserve">
          <source>It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.</source>
          <target state="translated">これは、入力データの形状と抽出する成分の数に応じて、完全SVDまたはHalkoら2009の方法によるランダム化された切り捨てSVDのLAPACK実装を使用します。</target>
        </trans-unit>
        <trans-unit id="74ae47bdcf2723d7a82146ada9f167c02a150388" translate="yes" xml:space="preserve">
          <source>It will plot the class decision boundaries given by a Nearest Neighbors classifier when using the Euclidean distance on the original features, versus using the Euclidean distance after the transformation learned by Neighborhood Components Analysis. The latter aims to find a linear transformation that maximises the (stochastic) nearest neighbor classification accuracy on the training set.</source>
          <target state="translated">これは、元の特徴量にユークリッド距離を使用した場合と、近傍成分分析で学習した変換後のユークリッド距離を使用した場合の、最近傍分類器によって与えられるクラス決定境界をプロットします。後者の目的は、学習セット上の(確率的な)最近傍分類の精度を最大化する線形変換を見つけることです。</target>
        </trans-unit>
        <trans-unit id="80b76c72ce07f72ff1bcc8279eceffb68f3a73b2" translate="yes" xml:space="preserve">
          <source>It would be possible to get even higher predictive performance with a larger neural network but the training would also be significantly more expensive.</source>
          <target state="translated">より大きなニューラルネットワークを使用することで、より高い予測性能を得ることが可能になるが、訓練にはかなりの費用がかかる。</target>
        </trans-unit>
        <trans-unit id="eb35f7366145b28ea8e69aba84c19929cb4fd162" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the &lt;code&gt;return_X_y&lt;/code&gt; parameter to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">また、 &lt;code&gt;return_X_y&lt;/code&gt; パラメータを &lt;code&gt;True&lt;/code&gt; に設定することにより、これらの関数のほとんどすべてが、データとターゲットのみを含むタプルになるように出力を制限することもできます。</target>
        </trans-unit>
        <trans-unit id="522ad20a1aa12ab3e8e796322f388a9318be6368" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows:</source>
          <target state="translated">カーネルの形状が結果の分布の滑らかさにどのように影響するかは明らかです。scikit-learnカーネル密度推定器は、次のように使用できます。</target>
        </trans-unit>
        <trans-unit id="35600165765d17d14f9a53e3a40d6c087a8e15cc" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes.</source>
          <target state="translated">クラスターの階層的マージを表すツリーを樹状図として視覚化することができます。目視検査は、データの構造を理解するのに役立つことがよくありますが、サンプルサイズが小さい場合はさらに役立ちます。</target>
        </trans-unit>
        <trans-unit id="262f72bd253b7e8f886f3645ecd5afaaf624d7fc" translate="yes" xml:space="preserve">
          <source>Iterate 2 and 3 until convergence.</source>
          <target state="translated">収束するまで2と3を繰り返します。</target>
        </trans-unit>
        <trans-unit id="e39adff24d3659cea88912062bf2425d11890a07" translate="yes" xml:space="preserve">
          <source>Iterative imputation of the missing values</source>
          <target state="translated">欠損値の反復的インputation</target>
        </trans-unit>
        <trans-unit id="f2f172891cc8c1241e8513ed23c4f46ceb939f0f" translate="yes" xml:space="preserve">
          <source>Iterative procedure to maximize the evidence</source>
          <target state="translated">エビデンスを最大化するための反復手順</target>
        </trans-unit>
        <trans-unit id="1e87dcaf344d15783f1af4ad18b162b497d772d4" translate="yes" xml:space="preserve">
          <source>Its dual is</source>
          <target state="translated">そのデュアルは</target>
        </trans-unit>
        <trans-unit id="ce6398892ce7bfa57c8075a52d29f534a23469a6" translate="yes" xml:space="preserve">
          <source>Its validation performance, measured via the \(R^2\) score, is significantly larger than the chance level. This makes it possible to use the &lt;a href=&quot;generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;permutation_importance&lt;/code&gt;&lt;/a&gt; function to probe which features are most predictive:</source>
          <target state="translated">\（R ^ 2 \）スコアを介して測定されたその検証パフォーマンスは、チャンスレベルよりも大幅に大きくなっています。これにより、&lt;a href=&quot;generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt; &lt;code&gt;permutation_importance&lt;/code&gt; &lt;/a&gt;関数を使用して、どの機能が最も予測的であるかを調べることができます。</target>
        </trans-unit>
        <trans-unit id="07da5fcab12f57a49e864df0a2dcb43ec8cd118b" translate="yes" xml:space="preserve">
          <source>J&amp;oslash;rgensen, B. (1992). The theory of exponential dispersion models and analysis of deviance. Monografias de matem&amp;aacute;tica, no. 51. See also &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_dispersion_model&quot;&gt;Exponential dispersion model.&lt;/a&gt;</source>
          <target state="translated">J&amp;oslash;rgensen、B。（1992）。指数分散モデルの理論と逸脱度の分析。Monografiasdematem&amp;aacute;tica、いいえ。51.&lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_dispersion_model&quot;&gt;指数分散モデル&lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="d7e5d74ebe16b65b422b57dd9089de563aa7d4b5" translate="yes" xml:space="preserve">
          <source>J. Cohen (1960). &amp;ldquo;A coefficient of agreement for nominal scales&amp;rdquo;. Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104.</source>
          <target state="translated">J.コーエン（1960）。「名目スケールの一致係数」。教育的および心理的測定20（1）：37-46。doi：10.1177 / 001316446002000104。</target>
        </trans-unit>
        <trans-unit id="4ba292a3729a3ffa6797e98ae7a24bba4f0e087f" translate="yes" xml:space="preserve">
          <source>J. Davis, M. Goadrich, &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;The Relationship Between Precision-Recall and ROC Curves&lt;/a&gt;, ICML 2006.</source>
          <target state="translated">J.デイビス、M。ゴードリッヒ、&lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;プレシジョンリコールとROC曲線の関係&lt;/a&gt;、ICML 2006。</target>
        </trans-unit>
        <trans-unit id="9f9ca6a90c561398be254053aedc4a945c9160d7" translate="yes" xml:space="preserve">
          <source>J. Friedman, &amp;ldquo;Multivariate adaptive regression splines&amp;rdquo;, The Annals of Statistics 19 (1), pages 1-67, 1991.</source>
          <target state="translated">J.フリードマン、「多変量適応回帰スプライン」、Annals of Statistics 19（1）、ページ1-67、1991。</target>
        </trans-unit>
        <trans-unit id="f3a4e2abf1b3937c134504328e857f33b32a50ea" translate="yes" xml:space="preserve">
          <source>J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</source>
          <target state="translated">J.Friedman,Greedy Function Approximation.勾配ブースティングマシン,統計学環,Vol.29,No.5,2001.</target>
        </trans-unit>
        <trans-unit id="f06167e7b529cb39087bd6b97f521b456419a7cd" translate="yes" xml:space="preserve">
          <source>J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov. &amp;ldquo;Neighbourhood Components Analysis&amp;rdquo;. Advances in Neural Information Processing Systems. 17, 513-520, 2005. &lt;a href=&quot;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&quot;&gt;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&lt;/a&gt;</source>
          <target state="translated">J.ゴールドバーガー、G。ヒントン、S。ローワイス、R。サラクトディノフ。「近隣コンポーネント分析」。ニューラル情報処理システムの進歩。17、513-520、2005。http &lt;a href=&quot;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&quot;&gt;:&lt;/a&gt; //www.cs.nyu.edu/~roweis/papers/ncanips.pdf</target>
        </trans-unit>
        <trans-unit id="37cd6f13ac969b2cba8a5a7a242580515d06793b" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="translated">J.マイラル、F。バッハ、J。ポンセ、G。サピロ、2009：スパースコーディングのためのオンライン辞書学習（&lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="8417a497b98a8d480d1cb9818d1f322bb7565268" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="translated">J. Mairal、F。Bach、J。Ponce、G。Sapiro、2009年：スパースコーディングのためのオンライン辞書学習（&lt;a href=&quot;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="d67bb0042b556d1819c5bcf6e551c8393e0d921f" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="translated">J. Nothman、H。QinおよびR. Yurchak（2018）。&lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;「無料のオープンソースソフトウェアパッケージでワードリストを停止する」&lt;/a&gt;。では&lt;em&gt;PROC。NLPオープンソースソフトウェアワークショップ&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="40bd5fef8dd5af699d79639063d7832cf1e45e49" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;https://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="translated">J. Nothman、H。Qin、R。Yurchak（2018）&lt;a href=&quot;https://aclweb.org/anthology/W18-2502&quot;&gt;「無料のオープンソースソフトウェアパッケージのストップワードリスト」&lt;/a&gt;。では&lt;em&gt;PROC。NLPオープンソースソフトウェアのワークショップ&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="692e866d29ab5ac27b12eb939942a283756e56c6" translate="yes" xml:space="preserve">
          <source>J. Zhu, H. Zou, S. Rosset, T. Hastie. &amp;ldquo;Multi-class AdaBoost&amp;rdquo;, 2009.</source>
          <target state="translated">J. Zhu、H。Zou、S。Rosset、T。Hastie。「マルチクラスAdaBoost」、2009年。</target>
        </trans-unit>
        <trans-unit id="5d92020b429e9c336d3ae7d33c4ba163d63036bb" translate="yes" xml:space="preserve">
          <source>J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.</source>
          <target state="translated">J.R.クインラン C4.5:機械学習のためのプログラム。モルガン・カウフマン、1993年。</target>
        </trans-unit>
        <trans-unit id="b259e0488f66e10ad76098d33440aa4c5f23c876" translate="yes" xml:space="preserve">
          <source>JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case&lt;/a&gt;</source>
          <target state="translated">JAウェゲリン&lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;2ブロックの場合に重点を置いた部分最小二乗（PLS）法の調査&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89591613ce2ead27076b0dd7b68b18da1f4e31d9" translate="yes" xml:space="preserve">
          <source>Jaccard similarity coefficient score</source>
          <target state="translated">ジャカード類似度係数スコア</target>
        </trans-unit>
        <trans-unit id="3dd35b446a7d3de6ee5688cfabde9bb7cc55f61a" translate="yes" xml:space="preserve">
          <source>JaccardDistance</source>
          <target state="translated">JaccardDistance</target>
        </trans-unit>
        <trans-unit id="493395686693db33a59d5eea00e82ad6c02c5742" translate="yes" xml:space="preserve">
          <source>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.</source>
          <target state="translated">ジェイコブ A.ウェゲリン 偏最小二乗法(PLS)の調査、特に2ブロックの場合に重点を置いて。ワシントン大学統計学部技術報告書371,ワシントン大学シアトル校,2000年.</target>
        </trans-unit>
        <trans-unit id="b9a5b6145a558ec82725430f200fe46fa35f6aac" translate="yes" xml:space="preserve">
          <source>Jarvelin, K., &amp;amp; Kekalainen, J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS), 20(4), 422-446.</source>
          <target state="translated">Jarvelin、K。、＆Kekalainen、J。（2002）IR技術の累積ゲインベースの評価。情報システムに関するACMトランザクション（TOIS）、20（4）、422-446。</target>
        </trans-unit>
        <trans-unit id="80af07b09c2acb231d89e62f1382b88433574f92" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,</source>
          <target state="translated">ジェシー・リード ベルンハルト・プファリンガー ジェフ・ホームズ ユー・フランク</target>
        </trans-unit>
        <trans-unit id="6f9c9a3eee3a8f7459d68946df6ef289f22fee94" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, &amp;ldquo;Classifier Chains for Multi-label Classification&amp;rdquo;, 2009.</source>
          <target state="translated">Jesse Read、Bernhard Pfahringer、Geoff Holmes、Eibe Frank、「Classifier Chains for multi-label Classification」、2009年。</target>
        </trans-unit>
        <trans-unit id="43121fdb3391941437c9f79e4fe2a93d4f69bed7" translate="yes" xml:space="preserve">
          <source>Joblib also tries to limit the oversubscription by limiting the number of threads usable in some third-party library threadpools like OpenBLAS, MKL or OpenMP. The default limit in each worker is set to &lt;code&gt;max(cpu_count() // effective_n_jobs, 1)&lt;/code&gt; but this limit can be overwritten with the &lt;code&gt;inner_max_num_threads&lt;/code&gt; argument which will be used to set this limit in the child processes.</source>
          <target state="translated">Joblibはまた、OpenBLAS、MKL、OpenMPなどのサードパーティライブラリスレッドプールで使用できるスレッドの数を制限することにより、オーバーサブスクリプションを制限しようとします。各ワーカーのデフォルトの制限は &lt;code&gt;max(cpu_count() // effective_n_jobs, 1)&lt;/code&gt; に設定されていますが、この制限は、子プロセスでこの制限を設定するために使用される &lt;code&gt;inner_max_num_threads&lt;/code&gt; 引数で上書きできます。</target>
        </trans-unit>
        <trans-unit id="bbd429df4e4127b1956a8217cca140ac1f4576de" translate="yes" xml:space="preserve">
          <source>Joblib is able to support both multi-processing and multi-threading. Whether joblib chooses to spawn a thread or a process depends on the &lt;strong&gt;backend&lt;/strong&gt; that it&amp;rsquo;s using.</source>
          <target state="translated">Joblibは、マルチプロセッシングとマルチスレッドの両方をサポートできます。joblibがスレッドまたはプロセスの生成を選択するかどうかは、&lt;strong&gt;使用&lt;/strong&gt;している&lt;strong&gt;バックエンド&lt;/strong&gt;によって異なります。</target>
        </trans-unit>
        <trans-unit id="dbbc7351b3326fa09a2af62a2a8c482a7f498e4a" translate="yes" xml:space="preserve">
          <source>Joblib is currently unable to avoid oversubscription in a multi-threading context. It can only do so with the &lt;code&gt;loky&lt;/code&gt; backend (which spawns processes).</source>
          <target state="translated">Joblibは現在、マルチスレッドコンテキストでのオーバーサブスクリプションを回避できません。これは、（プロセスを生成する） &lt;code&gt;loky&lt;/code&gt; バックエンドでのみ実行できます。</target>
        </trans-unit>
        <trans-unit id="2da78ef6529cd970b51628e985f5f2ea249ac134" translate="yes" xml:space="preserve">
          <source>Johanna Hardin, David M Rocke. The distribution of robust distances. Journal of Computational and Graphical Statistics. December 1, 2005, 14(4): 928-946.</source>
          <target state="translated">ジョアンナ・ハーディン、デビッド・M・ロッケ ロバスト距離の分布。計算統計学とグラフ統計学のジャーナル.2005年12月1日,14(4):928-946.</target>
        </trans-unit>
        <trans-unit id="3cd3820aa7670cf9157f83a7b518b28ca957a3fc" translate="yes" xml:space="preserve">
          <source>John K. Dixon, &amp;ldquo;Pattern Recognition with Partly Missing Data&amp;rdquo;, IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue: 10, pp. 617 - 621, Oct. 1979. &lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/4310090/&quot;&gt;http://ieeexplore.ieee.org/abstract/document/4310090/&lt;/a&gt;</source>
          <target state="translated">John K. Dixon、「データが部分的に欠落しているパターン認識」、IEEE Transactions on Systems、Man、and Cyber​​netics、Volume：9、Issue：10、pp。617-621、1979年10月&lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/4310090/&quot;&gt;。http：//ieeexplore.ieee。 org / abstract / document / 4310090 /&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f669c43cc07002b0d2c74696c2e577f06e2f830d" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, (2015). &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;.</source>
          <target state="translated">ジョン。D. Kelleher、Brian Mac Namee、Aoife D'Arcy、（2015）。&lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;予測データ分析のための機械学習の基礎：アルゴリズム、実施例、およびケーススタディ&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7aafef76ed32e6bfff8b0b682dc86da3ac5a13fa" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;, 2015.</source>
          <target state="translated">ジョン。D. Kelleher、Brian Mac Namee、Aoife D'Arcy、&lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics：Algorithms、Worked Examples、and Case Studies&lt;/a&gt;、2015。</target>
        </trans-unit>
        <trans-unit id="19e6bf8efc7133dd97d0abbd89569a0495139bdc" translate="yes" xml:space="preserve">
          <source>Joint feature selection with multi-task Lasso</source>
          <target state="translated">マルチタスクLassoを用いた合同特徴選択</target>
        </trans-unit>
        <trans-unit id="6e210d8e33bded6f565ddf30568ce6ee46546dcb" translate="yes" xml:space="preserve">
          <source>Joint parameter selection</source>
          <target state="translated">ジョイントパラメータの選択</target>
        </trans-unit>
        <trans-unit id="5dcd2dd79faa568a08732dcdc7a1c5d001632db6" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="translated">機械学習研究15（10月）のジャーナル：3221から3245まで、2014年&lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7eaac587d1f40d409b66183976f554af82049338" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="translated">Journal of Machine Learning Research 15（10月）：3221-3245、2014年&lt;a href=&quot;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;。https&lt;/a&gt;：//lvdmaaten.github.io/publications/papers/JMLR_2014.pdf</target>
        </trans-unit>
        <trans-unit id="6689749f561220cbe925de6f0809b1dc75c6258d" translate="yes" xml:space="preserve">
          <source>July, 1988</source>
          <target state="translated">1988年7月</target>
        </trans-unit>
        <trans-unit id="854e66ede1ccc0e35f92ec3068666dcad934aaf9" translate="yes" xml:space="preserve">
          <source>July; 1998</source>
          <target state="translated">1998年7月</target>
        </trans-unit>
        <trans-unit id="a2a7da9b458fe4f43b31552673f0b66352445d61" translate="yes" xml:space="preserve">
          <source>Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN Error Measures in MultiClass Prediction</source>
          <target state="translated">Jurman,Riccadonna,Furlanello,(2012).マルチクラス予測におけるMCCとCENの誤差尺度の比較</target>
        </trans-unit>
        <trans-unit id="a8dcc7a6052d083397dd89c88efdae4d16610c1e" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="translated">トレーニングから差し引かれたデータで予測子をテストすることが重要であるのと同じように、前処理（標準化、特徴選択など）と同様の&lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;データ変換&lt;/a&gt;も同様にトレーニングセットから学習し、差し押さえられた予測データに適用する必要があります：</target>
        </trans-unit>
        <trans-unit id="bb93e8a53cb9de4ad35209178664abb04cf0e5f7" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;https://scikit-learn.org/0.23/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="translated">トレーニングから差し出されたデータで予測子をテストすることが重要であるのと同様に、前処理（標準化、特徴選択など）および同様の&lt;a href=&quot;https://scikit-learn.org/0.23/data_transforms.html#data-transforms&quot;&gt;データ変換&lt;/a&gt;も同様にトレーニングセットから学習し、予測のために差し出されたデータに適用する必要があります：</target>
        </trans-unit>
        <trans-unit id="19ba747b37c5ad6ac1cc4689022bdfff83622716" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a in b), but with a nicer default message.</source>
          <target state="translated">self.assertTrue(a in b)と同じですが、デフォルトのメッセージはより良いものになっています。</target>
        </trans-unit>
        <trans-unit id="a7abce2837c2684f6308e0a3abb93654038ccc5f" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a not in b), but with a nicer default message.</source>
          <target state="translated">self.assertTrue(a not in b)と同じですが、デフォルトのメッセージはより良いものになります。</target>
        </trans-unit>
        <trans-unit id="57113affe2edb0e21a97719d086746970040c08f" translate="yes" xml:space="preserve">
          <source>K&amp;auml;rkk&amp;auml;inen and S. &amp;Auml;yr&amp;auml;m&amp;ouml;: &lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;On Computation of Spatial Median for Robust Data Mining.&lt;/a&gt;</source>
          <target state="translated">K&amp;auml;rkk&amp;auml;inenとS.&amp;Auml;yr&amp;auml;m&amp;ouml;：&lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;ロバストデータマイニングのための空間中央値の計算について。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c859ded2bd8aa408f6c0369beaf2c2cf3f0ddde" translate="yes" xml:space="preserve">
          <source>K(X, Y) = &amp;lt;X, Y&amp;gt; / (||X||*||Y||)</source>
          <target state="translated">K（X、Y）= &amp;lt;X、Y&amp;gt; /（|| X || * || Y ||）</target>
        </trans-unit>
        <trans-unit id="86beb78a3bdf4132202cbc165378339bb7f278e3" translate="yes" xml:space="preserve">
          <source>K-Folds cross-validator</source>
          <target state="translated">K-Foldsクロスバリデーター</target>
        </trans-unit>
        <trans-unit id="66e29f0aeaaf6f3b77934175874c79014b658ea2" translate="yes" xml:space="preserve">
          <source>K-Means</source>
          <target state="translated">K-Means</target>
        </trans-unit>
        <trans-unit id="bc6e2dbca5eeaca5cfd908f6085c13e70dbbe207" translate="yes" xml:space="preserve">
          <source>K-Means clustering</source>
          <target state="translated">Kミーンズクラスタリング</target>
        </trans-unit>
        <trans-unit id="57bc3e1ca5db2c1c7f140e0c9654a7e1bd0c4cd4" translate="yes" xml:space="preserve">
          <source>K-Means clustering.</source>
          <target state="translated">K-Meansクラスタリング。</target>
        </trans-unit>
        <trans-unit id="4dcab3f446cd66684df43251a7a52921a5b665f1" translate="yes" xml:space="preserve">
          <source>K-dimensional tree for fast generalized N-point problems.</source>
          <target state="translated">高速な一般化されたN点問題のためのK次元木。</target>
        </trans-unit>
        <trans-unit id="c532c5671424d23a3a3bc85d7cee5f6f8a964404" translate="yes" xml:space="preserve">
          <source>K-fold iterator variant with non-overlapping groups.</source>
          <target state="translated">非重複群を持つK-foldイテレータの変形。</target>
        </trans-unit>
        <trans-unit id="8434c9f312099287fd33427192dcba6bdae1583b" translate="yes" xml:space="preserve">
          <source>K-means Clustering</source>
          <target state="translated">K平均クラスタリング</target>
        </trans-unit>
        <trans-unit id="16176fa529a1e6d30521129cdc8f04353aaff22e" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient by using the triangle inequality, but currently doesn&amp;rsquo;t support sparse data. &amp;ldquo;auto&amp;rdquo; chooses &amp;ldquo;elkan&amp;rdquo; for dense data and &amp;ldquo;full&amp;rdquo; for sparse data.</source>
          <target state="translated">使用するK平均アルゴリズム。従来のEMスタイルのアルゴリズムは「完全」です。「elkan」バリエーションは、三角形の不等式を使用するとより効率的ですが、現在スパースデータをサポートしていません。「auto」は、密なデータには「elkan」を、疎なデータには「full」を選択します。</target>
        </trans-unit>
        <trans-unit id="c00998c8eabed1e931fa81c6f69faf59aad07506" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient on data with well-defined clusters, by using the triangle inequality. However it&amp;rsquo;s more memory intensive due to the allocation of an extra array of shape (n_samples, n_clusters).</source>
          <target state="translated">使用するK-meansアルゴリズム。古典的なEMスタイルのアルゴリズムは「フル」です。「elkan」バリエーションは、三角不等式を使用することにより、明確に定義されたクラスターを持つデータでより効率的です。ただし、形状の追加の配列（n_samples、n_clusters）が割り当てられるため、メモリを大量に消費します。</target>
        </trans-unit>
        <trans-unit id="848dff73d6f69d92cd5b01b40f76a731abde9743" translate="yes" xml:space="preserve">
          <source>K-means can be used for vector quantization. This is achieved using the transform method of a trained model of &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">K-meansは、ベクトル量子化に使用できます。これは、&lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; の&lt;/a&gt;トレーニング済みモデルの変換メソッドを使用して実現されます。</target>
        </trans-unit>
        <trans-unit id="ba78203e9e9f38ce3f7e015938283eb704622fc1" translate="yes" xml:space="preserve">
          <source>K-means clustering</source>
          <target state="translated">K平均クラスタリング</target>
        </trans-unit>
        <trans-unit id="4c31918fe250fba32eafec9c8bd2408d0665baa0" translate="yes" xml:space="preserve">
          <source>K-means clustering algorithm.</source>
          <target state="translated">K-meansクラスタリングアルゴリズム。</target>
        </trans-unit>
        <trans-unit id="3f9399be9d9993e05f4712a210efb7bcf391430f" translate="yes" xml:space="preserve">
          <source>K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.</source>
          <target state="translated">K-meansは、小さな、すべて等しい、対角共分散行列を持つ、期待値最大化アルゴリズムと同等です。</target>
        </trans-unit>
        <trans-unit id="f931e58c5b02fb6c60e80955646f359bee6ac7ee" translate="yes" xml:space="preserve">
          <source>K-means is often referred to as Lloyd&amp;rsquo;s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose \(k\) samples from the dataset \(X\). After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.</source>
          <target state="translated">K平均法は、しばしばロイドのアルゴリズムと呼ばれます。基本的に、アルゴリズムには3つのステップがあります。最初のステップでは、初期の重心を選択します。最も基本的な方法は、データセット\（X \）から\（k \）サンプルを選択することです。初期化後、K-meansは他の2つのステップ間のループで構成されます。最初のステップでは、各サンプルを最も近い重心に割り当てます。2番目のステップでは、前の各重心に割り当てられたすべてのサンプルの平均値を取得して、新しい重心を作成します。古い重心と新しい重心の差が計算され、この値がしきい値より小さくなるまで、アルゴリズムはこれらの最後の2つのステップを繰り返します。つまり、重心が大きく動かなくなるまで繰り返されます。</target>
        </trans-unit>
        <trans-unit id="c91b0be65ee9c7db25b71aa279369cca08edc7ca" translate="yes" xml:space="preserve">
          <source>K-means quantization</source>
          <target state="translated">K平均量子化</target>
        </trans-unit>
        <trans-unit id="5cf295fcd230ab825b1fa5bcf82b9dac494d126c" translate="yes" xml:space="preserve">
          <source>KDTree for fast generalized N-point problems</source>
          <target state="translated">高速な一般化されたN点問題のためのKDTree</target>
        </trans-unit>
        <trans-unit id="34d74f913e8bd68fa4a9d1c4d3966f34ca72fc15" translate="yes" xml:space="preserve">
          <source>KDTree(X, leaf_size=40, metric=&amp;rsquo;minkowski&amp;rsquo;, **kwargs)</source>
          <target state="translated">KDTree（X、leaf_size = 40、metric = 'minkowski'、** kwargs）</target>
        </trans-unit>
        <trans-unit id="e6f48940c5e34202cb03c9567fde221973b85eb8" translate="yes" xml:space="preserve">
          <source>KNN Based Imputation</source>
          <target state="translated">KNNベースのインピュテーション</target>
        </trans-unit>
        <trans-unit id="8a130d990c735953536ce43a1c5cf50c4989bca1" translate="yes" xml:space="preserve">
          <source>Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.</source>
          <target state="translated">カッパスコアは、バイナリ問題やマルチクラス問題では計算できますが、マルチラベル問題では計算できません(ラベルごとのスコアを手動で計算する場合を除く)。</target>
        </trans-unit>
        <trans-unit id="7642aa421288e310d04c4d2f1c88ac24e935cd02" translate="yes" xml:space="preserve">
          <source>Ke et. al. &lt;a href=&quot;https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree&quot;&gt;&amp;ldquo;LightGBM: A Highly Efficient Gradient BoostingDecision Tree&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">Keet。al。&lt;a href=&quot;https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree&quot;&gt;「LightGBM：非常に効率的な勾配BoostingDecisionツリー」&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="992bd2f88020b43ae9d881f8a8ecb43504cd4a74" translate="yes" xml:space="preserve">
          <source>Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False.</source>
          <target state="translated">3つのRGBチャンネルを平均化して1つのグレーレベルチャンネルにするのではなく、3つのRGBチャンネルを保持します。colorがTrueの場合、データの形状は、color=Falseの場合の形状よりも1つ多くの次元を持っています。</target>
        </trans-unit>
        <trans-unit id="e85b73c38883acb129e419e1a2f1719d75a444c5" translate="yes" xml:space="preserve">
          <source>Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005.</source>
          <target state="translated">このように、本研究では、「線形次元削減のための関連性重み付きLDAを用いた線形次元削減法」と「線形次元削減のための関連性重み付きLDAを用いた線形次元削減法」を提案している。関連度重み付きLDAを用いた線形次元削減。南洋理工大学電気電子工学部。2005.</target>
        </trans-unit>
        <trans-unit id="4ac337776123607052d628758806e2172a140241" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimate of Species Distributions</source>
          <target state="translated">種の分布のカーネル密度推定</target>
        </trans-unit>
        <trans-unit id="1794dd0445cf0665650fb5446983f4ef8a3519d3" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation</source>
          <target state="translated">カーネル密度推定</target>
        </trans-unit>
        <trans-unit id="9837c7505d0f3a8c028c3a0430177597bb56e3e2" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation.</source>
          <target state="translated">カーネル密度推定。</target>
        </trans-unit>
        <trans-unit id="3bd4b1d4f074cf6b0f30ea849b2a75ad1d3777d9" translate="yes" xml:space="preserve">
          <source>Kernel PCA</source>
          <target state="translated">PCAカーネル</target>
        </trans-unit>
        <trans-unit id="e5cb129fc99d7ba99fe28de6d8de36380920334b" translate="yes" xml:space="preserve">
          <source>Kernel PCA was introduced in:</source>
          <target state="translated">カーネルPCAが導入されました。</target>
        </trans-unit>
        <trans-unit id="2064482c4c2332e23a8df06a915448e7e780cd46" translate="yes" xml:space="preserve">
          <source>Kernel Principal Component Analysis.</source>
          <target state="translated">カーネル主成分分析。</target>
        </trans-unit>
        <trans-unit id="ba5a4a64bda1b4288aa7730d4a3cc2a5a99cf5dc" translate="yes" xml:space="preserve">
          <source>Kernel Principal component analysis (KPCA)</source>
          <target state="translated">カーネル主成分分析(KPCA</target>
        </trans-unit>
        <trans-unit id="f9f3967ca79560e0b7bba219989bbd17450e2f6e" translate="yes" xml:space="preserve">
          <source>Kernel bandwidth.</source>
          <target state="translated">カーネルの帯域幅。</target>
        </trans-unit>
        <trans-unit id="8f8874978483d89d1eb3e15131193d11bfd798e3" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for &amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">「rbf」、「poly」、「sigmoid」のカーネル係数。</target>
        </trans-unit>
        <trans-unit id="97392135d656893f41c86b28ea3abd0d9e018bae" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf kernel.</source>
          <target state="translated">rbfカーネルのカーネル係数。</target>
        </trans-unit>
        <trans-unit id="0ae546d11d3317bcab299286e34e2f35ebfa8832" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">rbf,poly,sigmoid カーネルのカーネル係数.他のカーネルでは無視されます。</target>
        </trans-unit>
        <trans-unit id="4a5a36cb73b6fa8cb90e406a9b203038f766b3f9" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt;.</source>
          <target state="translated">rbf、poly、sigmoid、laplacian、chi2カーネルのカーネル係数。 &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt; では無視されます。</target>
        </trans-unit>
        <trans-unit id="7ed139f80ae0f25db98c92c5cce6311e8435271b" translate="yes" xml:space="preserve">
          <source>Kernel density estimation in scikit-learn is implemented in the &lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; estimator, which uses the Ball Tree or KD Tree for efficient queries (see &lt;a href=&quot;neighbors#neighbors&quot;&gt;Nearest Neighbors&lt;/a&gt; for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions.</source>
          <target state="translated">scikit-learnのカーネル密度推定は、&lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt;推定器に実装されています。推定器は、効率的なクエリにボールツリーまたはKDツリーを使用します（これらの説明については、&lt;a href=&quot;neighbors#neighbors&quot;&gt;最近傍&lt;/a&gt;を参照してください）。上記の例では簡単にするために1Dデータセットを使用していますが、カーネル密度の推定は任意の数の次元で実行できますが、実際には次元の呪いにより、高次元ではパフォーマンスが低下します。</target>
        </trans-unit>
        <trans-unit id="55e8fbe20e17e26ae0f3d4e88a1aeba0651c9393" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">対数限界尤度が評価されるカーネルハイパーパラメーター。Noneの場合、事前計算されたself.kernel_.thetaの &lt;code&gt;self.kernel_.theta&lt;/code&gt; が返されます。</target>
        </trans-unit>
        <trans-unit id="aef459d7999942524bf342d4727b96b71e8fe80a" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. In the case of multi-class classification, theta may be the hyperparameters of the compound kernel or of an individual kernel. In the latter case, all individual kernel get assigned the same theta values. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">対数限界尤度が評価されるカーネルハイパーパラメーター。マルチクラス分類の場合、thetaは複合カーネルまたは個々のカーネルのハイパーパラメーターになります。後者の場合、個々のカーネルすべてに同じシータ値が割り当てられます。Noneの場合、事前計算されたself.kernel_.thetaの &lt;code&gt;self.kernel_.theta&lt;/code&gt; が返されます。</target>
        </trans-unit>
        <trans-unit id="e5fe7d4b4a2b4b1f4287c0408af092681ae17306" translate="yes" xml:space="preserve">
          <source>Kernel k(X, Y)</source>
          <target state="translated">カーネル k(X,Y)</target>
        </trans-unit>
        <trans-unit id="3ec24bca52509370dd13e99805241c7649952db1" translate="yes" xml:space="preserve">
          <source>Kernel map to be approximated. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number.</source>
          <target state="translated">近似されるカーネルマップ。callable は、2 つの引数とこのオブジェクトに渡されたキーワード引数を kernel_params として受け取り、浮動小数点数を返す必要があります。</target>
        </trans-unit>
        <trans-unit id="819d0e343c77a54a44f85a514be1d98b92643c33" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. Set to &amp;ldquo;precomputed&amp;rdquo; in order to pass a precomputed kernel matrix to the estimator methods instead of samples.</source>
          <target state="translated">内部で使用されるカーネルマッピング。呼び出し可能オブジェクトは、2つの引数とkernel_paramsとしてこのオブジェクトに渡されたキーワード引数を受け入れ、浮動小数点数を返す必要があります。事前計算されたカーネルマトリックスをサンプルではなく推定器メソッドに渡すには、「事前計算」に設定します。</target>
        </trans-unit>
        <trans-unit id="438349cc7f220c5c2d499eba1d3d6c5414057ba4" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. This parameter is directly passed to &lt;code&gt;sklearn.metrics.pairwise.pairwise_kernel&lt;/code&gt;. If &lt;code&gt;kernel&lt;/code&gt; is a string, it must be one of the metrics in &lt;code&gt;pairwise.PAIRWISE_KERNEL_FUNCTIONS&lt;/code&gt;. If &lt;code&gt;kernel&lt;/code&gt; is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if &lt;code&gt;kernel&lt;/code&gt; is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from &lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.</source>
          <target state="translated">内部で使用されるカーネルマッピング。このパラメーターは、 &lt;code&gt;sklearn.metrics.pairwise.pairwise_kernel&lt;/code&gt; に直接渡されます。 &lt;code&gt;kernel&lt;/code&gt; が文字列の場合、 &lt;code&gt;pairwise.PAIRWISE_KERNEL_FUNCTIONS&lt;/code&gt; のメトリックの1つである必要があります。PAIRWISE_KERNEL_FUNCTIONS。場合は &lt;code&gt;kernel&lt;/code&gt; 「事前に計算」で、Xは、カーネル行列を想定しています。または、 &lt;code&gt;kernel&lt;/code&gt; が呼び出し可能な関数である場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2行を入力として受け取り、対応するカーネル値を単一の数値として返す必要があります。これは、&lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt;からの呼び出し可能オブジェクトは、単一のサンプルではなく行列で動作するため、許可されないことを意味します。代わりに、カーネルを識別する文字列を使用してください。</target>
        </trans-unit>
        <trans-unit id="5470105c2039f2210b1a2c9d8e55edfd818f2e42" translate="yes" xml:space="preserve">
          <source>Kernel matrix.</source>
          <target state="translated">カーネル行列。</target>
        </trans-unit>
        <trans-unit id="839a7f66845e964bf2afbaec5611c3402217b903" translate="yes" xml:space="preserve">
          <source>Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function \(k\) (a so called Mercer kernel), it is guaranteed that there exists a mapping \(\phi\) into a Hilbert space \(\mathcal{H}\), such that</source>
          <target state="translated">サポートベクターマシンやカーネル化PCAのようなカーネル法は,カーネルのヒルベルト空間を再現する性質に依存している.For any positive definite kernel function \(k\)(a called Mercer kernel),it is guarantee that there is a mapping 写像が存在することが保証されている。</target>
        </trans-unit>
        <trans-unit id="a3bb404582c234b1b5161269097e65342126edc8" translate="yes" xml:space="preserve">
          <source>Kernel methods to project data into alternate dimensional spaces</source>
          <target state="translated">データを異次元空間に投影するためのカーネル法</target>
        </trans-unit>
        <trans-unit id="7853e504e205e94517ed94484ade6d5285c25255" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{sum}(X, Y) = k1(X, Y) + k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{product}(X, Y) = k1(X, Y) * k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(exponent\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^\text{exponent}\).</source>
          <target state="translated">カーネルオペレーターは、1つまたは2つの基本カーネルを取り、それらを新しいカーネルに結合します。&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt;カーネルが2つのカーネルをとる\（K1 \）と\（K2 \）とを介して、コンバインそれらを\（K_ {和}（X、Y）= K1（X、Y）+ K2（X、Y）\）。&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt;カーネルは、2つのカーネルをとる\（K1 \）と\（K2 \）とを介して、コンバインそれらを\（K_ {産物}（X、Y）= K1（X、Y）* K2（X、Y）\）。&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt; &lt;code&gt;Exponentiation&lt;/code&gt; &lt;/a&gt;カーネルは（K_ {EXP}（X、Y）= K（X、Y）^ \テキスト{指数} \）\介してベースカーネルとスカラーパラメータ\（指数\）とコンバインそれらを取ります。</target>
        </trans-unit>
        <trans-unit id="6d4564c4032221fcc796901a96d1a6cf81d6aa2f" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(p\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^p\). Note that magic methods &lt;code&gt;__add__&lt;/code&gt;, &lt;code&gt;__mul___&lt;/code&gt; and &lt;code&gt;__pow__&lt;/code&gt; are overridden on the Kernel objects, so one can use e.g. &lt;code&gt;RBF() + RBF()&lt;/code&gt; as a shortcut for &lt;code&gt;Sum(RBF(), RBF())&lt;/code&gt;.</source>
          <target state="translated">カーネル演算子は、1つまたは2つの基本カーネルを取得し、それらを新しいカーネルに結合します。&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt;カーネルが2つのカーネルをとる\（K_1 \）と\（K_2 \）とを介して、コンバインそれらを\（K_ {和}（X、Y）= K_1（X、Y）+ K_2（X、Y）\）。&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt;カーネルは、2つのカーネルをとる\（K_1 \）と\（K_2 \）とを介して、コンバインそれらを\（K_ {産物}（X、Y）= K_1（X、Y）* K_2（X、Y）\）。&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt; &lt;code&gt;Exponentiation&lt;/code&gt; &lt;/a&gt;カーネルが1つの塩基カーネルと\介しスカラーパラメータ\（P \）とコンバインそれらをとる（K_ {EXP}（X、Y）= K（X、Y）^ P \）。マジックメソッド &lt;code&gt;__add__&lt;/code&gt; 、 &lt;code&gt;__mul___&lt;/code&gt; 、および &lt;code&gt;__pow__&lt;/code&gt; はカーネルオブジェクトでオーバーライドされるため、 &lt;code&gt;Sum(RBF(), RBF())&lt;/code&gt; ショートカットとして &lt;code&gt;RBF() + RBF()&lt;/code&gt; を使用できることに注意してください。</target>
        </trans-unit>
        <trans-unit id="9dc320ddac29ab60da57cafc47693079e4b6b082" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge Regression&lt;/a&gt; (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">カーネルリッジ回帰（KRR）&lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt;は、&lt;a href=&quot;linear_model#ridge-regression&quot;&gt;リッジ回帰&lt;/a&gt;（l2-norm正則化を伴う線形最小二乗）とカーネルトリックを組み合わせたものです。したがって、それぞれのカーネルとデータによって引き起こされる空間での線形関数を学習します。非線形カーネルの場合、これは元の空間の非線形関数に対応します。</target>
        </trans-unit>
        <trans-unit id="3fcbd037e0e31c66e26fa73fca7d9588d56b4cd8" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge regression and classification&lt;/a&gt; (linear least squares with l2-norm regularization) with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method&quot;&gt;kernel trick&lt;/a&gt;. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">カーネルリッジ回帰（KRR）&lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt;は、&lt;a href=&quot;linear_model#ridge-regression&quot;&gt;リッジ回帰と分類&lt;/a&gt;（l2ノルム正則化による線形最小二乗）を&lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method&quot;&gt;カーネルトリック&lt;/a&gt;と組み合わせます。したがって、それぞれのカーネルとデータによって誘導される空間内の線形関数を学習します。非線形カーネルの場合、これは元の空間の非線形関数に対応します。</target>
        </trans-unit>
        <trans-unit id="7d585be11bb912be319b898c908d63ce568dd8c0" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">カーネルリッジ回帰(KRR)は、リッジ回帰(l2ノルム正則化による線形最小二乗)とカーネルトリックを組み合わせたものです。このようにして、それぞれのカーネルとデータによって誘導される空間における線形関数を学習します。非線形カーネルの場合、これは元の空間における非線形関数に対応します。</target>
        </trans-unit>
        <trans-unit id="262cee695a2ed79939315817b4a3a26823167afe" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression combines ridge regression with the kernel trick</source>
          <target state="translated">カーネルリッジ回帰は、リッジ回帰とカーネルのトリックを組み合わせたものです。</target>
        </trans-unit>
        <trans-unit id="589ad014b6254add975c198ac204f9560e253ac1" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression.</source>
          <target state="translated">カーネルリッジ回帰。</target>
        </trans-unit>
        <trans-unit id="a797077c9a6730a652ad75f039a934d138c2b41f" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed.</source>
          <target state="translated">モデルで使用するカーネル:線形,多項式,RBF,シグモイド,または事前計算.</target>
        </trans-unit>
        <trans-unit id="407ab400408caf91955e34873fdbfe1f6ae14b07" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed. &amp;lsquo;rbf&amp;rsquo; by default.</source>
          <target state="translated">モデルで使用するカーネル：線形、多項式、RBF、シグモイド、または事前計算。デフォルトでは「rbf」。</target>
        </trans-unit>
        <trans-unit id="716837a63a81bd1da24c9f2580ff0581777fc381" translate="yes" xml:space="preserve">
          <source>Kernel which is composed of a set of other kernels.</source>
          <target state="translated">他のカーネルの集合で構成されるカーネル。</target>
        </trans-unit>
        <trans-unit id="a170413f32a293189023e0700b83d22ea6042972" translate="yes" xml:space="preserve">
          <source>Kernel. Default=&amp;rdquo;linear&amp;rdquo;.</source>
          <target state="translated">カーネル。デフォルトは「linear」です。</target>
        </trans-unit>
        <trans-unit id="e3cb275740ef8ee4f25f4b8b1bb2cb56094f01c1" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">カーネル（GPのコンテキストでは「共分散関数」とも呼ばれます）は、GPの前後の形状を決定するGPの重要な要素です。これらは、2つのデータポイントの「類似性」を定義することにより、類似のデータポイントが類似のターゲット値を持つべきであるという仮定と組み合わせて、学習される関数の仮定をエンコードします。カーネルの2つのカテゴリを区別できます。定常カーネルは、2つのデータポイントの距離にのみ依存し、絶対値\（k（x_i、x_j）= k（d（x_i、x_j））\）には依存しないため、変換に対して不変です。入力空間では、非定常カーネルはデータポイントの特定の値にも依存します。静止カーネルはさらに、等方性カーネルと異方性カーネルに細分できます。等方性カーネルは、入力空間の回転に対しても不変です。詳細については、私たちは第4章を参照します&lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="98cdf159fe1dcd2fc6aef984c01878bfe7d52c10" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;. For guidance on how to best combine different kernels, we refer to &lt;a href=&quot;#duv2014&quot; id=&quot;id6&quot;&gt;[Duv2014]&lt;/a&gt;.</source>
          <target state="translated">カーネル（GPのコンテキストでは「共分散関数」とも呼ばれます）は、GPの事前および事後の形状を決定するGPの重要な要素です。それらは、2つのデータポイントの「類似性」を定義することによって学習されている関数の仮定をエンコードし、類似したデータポイントは類似したターゲット値を持つ必要があるという仮定と組み合わせます。カーネルの2つのカテゴリを区別できます。定常カーネルは2つのデータポイントの距離のみに依存し、絶対値\（k（x_i、x_j）= k（d（x_i、x_j））\）には依存しないため、変換に対して不変です。非定常カーネルはデータポイントの特定の値にも依存しますが、入力空間では。静止カーネルはさらに、等方性カーネルと異方性カーネルに細分できます。等方性カーネルは、入力空間の回転に対しても不変です。詳細については、の第4章を参照してください&lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;。異なるカーネルを最適に組み合わせる方法のガイダンスについては、&lt;a href=&quot;#duv2014&quot; id=&quot;id6&quot;&gt;[Duv2014]&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="0ad8dd8fec70a9d46c4f724f1ce47b4b45810363" translate="yes" xml:space="preserve">
          <source>Kernels are measures of similarity, i.e. &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; if objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are considered &amp;ldquo;more similar&amp;rdquo; than objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;. A kernel must also be positive semi-definite.</source>
          <target state="translated">カーネルは、オブジェクト &lt;code&gt;a&lt;/code&gt; と &lt;code&gt;b&lt;/code&gt; がオブジェクト &lt;code&gt;a&lt;/code&gt; と &lt;code&gt;c&lt;/code&gt; よりも「類似している」と見なされる場合、類似性の尺度、つまり &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; です。カーネルも正の半定値でなければなりません。</target>
        </trans-unit>
        <trans-unit id="cd28143394596209b24bd87df6806973641c2997" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="translated">カーネルは、ハイパーパラメーターのベクトル\（\ theta \）によってパラメーター化されます。これらのハイパーパラメータは、たとえば、カーネルの長さスケールまたは周期性を制御できます（以下を参照）。すべてのカーネルは、 &lt;code&gt;eval_gradient=True&lt;/code&gt; メソッドでeval_gradient = Trueを設定することにより、\（\ theta \）に関するカーネルの自動共分散の分析勾配の計算をサポートしてい &lt;code&gt;__call__&lt;/code&gt; 。この勾配は、対数限界尤度の勾配を計算する際にガウスプロセス（リグレッサと分類子の両方）によって使用され、次に、勾配マージンを最大化する\（\ theta \）の値を決定するために使用されます勾配上昇による可能性。ハイパーパラメータごとに、カーネルのインスタンスを作成するときに初期値と境界を指定する必要があります。 \（\ theta \）の現在の値は、プロパティを介して取得および設定できます &lt;code&gt;theta&lt;/code&gt; カーネルオブジェクトのシータ。さらに、ハイパーパラメータの &lt;code&gt;bounds&lt;/code&gt; は、カーネルのプロパティの境界からアクセスできます。どちらのプロパティ（thetaおよびbounds）も、内部で使用される値の対数変換された値を返すことに注意してください。これらは通常、勾配ベースの最適化により適しているためです。各ハイパーの仕様は、のインスタンスの形式で格納される&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt; &lt;code&gt;Hyperparameter&lt;/code&gt; &lt;/a&gt;各カーネルです。「x」という名前のハイパーパラメータを使用するカーネルには、属性self.xおよびself.x_boundsが必要であることに注意してください。</target>
        </trans-unit>
        <trans-unit id="4aed51cbfb6629b3c22c46504375ed74bc60a033" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="translated">カーネルは、ハイパーパラメータのベクトル\（\ theta \）によってパラメータ化されます。これらのハイパーパラメータは、たとえば、カーネルの長さスケールや周期性を制御できます（以下を参照）。すべてのカーネルは設定を経由して（\シータ\）\に関してカーネルの自己共分散の計算解析的勾配をサポート &lt;code&gt;eval_gradient=True&lt;/code&gt; で &lt;code&gt;__call__&lt;/code&gt; 方法。この勾配は、対数周辺尤度の勾配を計算する際にガウス過程（回帰子と分類子の両方）によって使用されます。これは、対数周辺尤度を最大化する\（\ theta \）の値を決定するために使用されます。勾配上昇による可能性。カーネルのインスタンスを作成するときに、ハイパーパラメータごとに初期値と境界を指定する必要があります。 \（\ theta \）の現在の値は、プロパティを介して取得および設定できます &lt;code&gt;theta&lt;/code&gt; カーネルオブジェクトのシータ。さらに、ハイパーパラメータの &lt;code&gt;bounds&lt;/code&gt; は、カーネルのプロパティ境界からアクセスできます。両方のプロパティ（シータと境界）は、内部で使用される値の対数変換された値を返すことに注意してください。これらは通常、勾配ベースの最適化により適しているためです。各ハイパーの仕様は、のインスタンスの形式で格納される&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt; &lt;code&gt;Hyperparameter&lt;/code&gt; &lt;/a&gt;各カーネルです。「x」という名前のハイパーパラメータを使用するカーネルには、属性self.xおよびself.x_boundsが必要であることに注意してください。</target>
        </trans-unit>
        <trans-unit id="2a754d09a87b01a5043bf319d676ca0f6cb6a853" translate="yes" xml:space="preserve">
          <source>Kernels:</source>
          <target state="translated">Kernels:</target>
        </trans-unit>
        <trans-unit id="c3b9fc0d0d17c07a841795715ed044ed9e710926" translate="yes" xml:space="preserve">
          <source>Kevin P. Murphy &amp;ldquo;Machine Learning: A Probabilistic Perspective&amp;rdquo;, The MIT Press chapter 14.4.3, pp. 492-493</source>
          <target state="translated">ケビンP.マーフィー「機械学習：確率論的展望」、MITプレスの章14.4.3、pp。492-493</target>
        </trans-unit>
        <trans-unit id="1ebff3fd3bf929976eef25f0da78c334d18a2c1d" translate="yes" xml:space="preserve">
          <source>Keys are parameter names that can be passed to &lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;set_config&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">キーは、&lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;set_config&lt;/code&gt; &lt;/a&gt;に渡すことができるパラメーター名です。</target>
        </trans-unit>
        <trans-unit id="c16cf0c8b95cb6641127d4ecde39c2d13ee54107" translate="yes" xml:space="preserve">
          <source>Keyword arguments allow to adapt these defaults to specific data sets (see parameters &lt;code&gt;target_name&lt;/code&gt;, &lt;code&gt;data_name&lt;/code&gt;, &lt;code&gt;transpose_data&lt;/code&gt;, and the examples below).</source>
          <target state="translated">キーワード引数により、これらのデフォルトを特定のデータセットに適合させることができます（パラメーター &lt;code&gt;target_name&lt;/code&gt; 、 &lt;code&gt;data_name&lt;/code&gt; 、 &lt;code&gt;transpose_data&lt;/code&gt; 、および以下の例を参照してください）。</target>
        </trans-unit>
        <trans-unit id="6a687df4f73e66be23d8d5cd9810da872c9b92e2" translate="yes" xml:space="preserve">
          <source>Keyword arguments passed to the coordinate descent solver.</source>
          <target state="translated">座標降下ソルバーに渡されるキーワード引数。</target>
        </trans-unit>
        <trans-unit id="ee035295632767669037b1fd1546556e8af6cebd" translate="yes" xml:space="preserve">
          <source>Keyword arguments to be passed to matplotlib&amp;rsquo;s &lt;code&gt;plot&lt;/code&gt;.</source>
          <target state="translated">matplotlibの &lt;code&gt;plot&lt;/code&gt; 渡されるキーワード引数。</target>
        </trans-unit>
        <trans-unit id="bd2209e677c2e2331711a5337dc06706ac2ee537" translate="yes" xml:space="preserve">
          <source>Keyword arguments to pass to specified metric function.</source>
          <target state="translated">指定されたメトリック関数に渡すキーワード引数。</target>
        </trans-unit>
        <trans-unit id="b6574be8c6baa963e814d600a049a18b07924f05" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="translated">Kilian Weinberger、Anirban Dasgupta、John Langford、Alex Smola、Josh Attenberg（2009）。&lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;大規模なマルチタスク学習のための機能ハッシュ&lt;/a&gt;。手続き ICML。</target>
        </trans-unit>
        <trans-unit id="aaf2909b07b71367a7207c2f93060ee37cc58e6c" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;https://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="translated">キリアン・ワインバーガー、アニルバン・ダスグプタ、ジョン・ラングフォード、アレックス・スモーラ、ジョッシュ・アッテンバーグ（2009）。&lt;a href=&quot;https://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;大規模なマルチタスク学習のための特徴ハッシュ&lt;/a&gt;。手順 ICML。</target>
        </trans-unit>
        <trans-unit id="35a95e3949c1091022c84b09bdfaee477e2ca247" translate="yes" xml:space="preserve">
          <source>Kingma, Diederik, and Jimmy Ba. &amp;ldquo;Adam: A method for stochastic</source>
          <target state="translated">Kingma、Diederik、Jimmy Ba。「アダム：確率論的手法</target>
        </trans-unit>
        <trans-unit id="7bf0d4f9044d36fbabdb373fe028824c8f48b797" translate="yes" xml:space="preserve">
          <source>Kluger, Y., Basri, R., Chang, J. T., &amp;amp; Gerstein, M. (2003). Spectral biclustering of microarray data: coclustering genes and conditions. Genome research, 13(4), 703-716.</source>
          <target state="translated">Kluger、Y.、Basri、R.、Chang、JT、およびGerstein、M。（2003）。マイクロアレイデータのスペクトルバイクラスター化：遺伝子と条件のクラスター化。ゲノム研究、13（4）、703-716。</target>
        </trans-unit>
        <trans-unit id="454573718b795c598350a3ed3c4e500004992423" translate="yes" xml:space="preserve">
          <source>Kluger, Yuval, et. al., 2003. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;Spectral biclustering of microarray data: coclustering genes and conditions&lt;/a&gt;.</source>
          <target state="translated">クルーガー、ユヴァル、他 al。、2003。&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;マイクロアレイデータのスペクトルバイクラスタリング：遺伝子と条件の共クラスタリング&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c956cdb3811d15bc82b9ab562e4744234449e302" translate="yes" xml:space="preserve">
          <source>Knowing only the number of samples, the &lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt;&lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;&lt;/a&gt; estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection:</source>
          <target state="translated">サンプル数のみが&lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt; &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt; &lt;/a&gt;いるsklearn.random_projection.johnson_lindenstrauss_min_dimは、ランダムなサブスペースの最小サイズを控えめに見積もり、ランダムな投影によって導入される有界の歪みを保証します。</target>
        </trans-unit>
        <trans-unit id="dc8be79b794b57340c1a9b2bf6e67594910f3213" translate="yes" xml:space="preserve">
          <source>Koby Crammer, Yoram Singer. On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal of Machine Learning Research 2, (2001), 265-292</source>
          <target state="translated">コビー・クレイマー、ヨラム・シンガー 多クラスカーネルベースのベクトルマシンのアルゴリズム実装について.機械学習研究誌 2,(2001),265-292</target>
        </trans-unit>
        <trans-unit id="5c3682641cb862b7b72f47a7d095c9e12f698d72" translate="yes" xml:space="preserve">
          <source>Kullback-Leibler divergence after optimization.</source>
          <target state="translated">最適化後のKullback-Leibler発散。</target>
        </trans-unit>
        <trans-unit id="58f9065948558949c0307af59f2acaf3f9203c82" translate="yes" xml:space="preserve">
          <source>KulsinskiDistance</source>
          <target state="translated">KulsinskiDistance</target>
        </trans-unit>
        <trans-unit id="cb6565437657bdf8e9b94faf7a832064c7b5f242" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="translated">L-BFGSは、関数の2次偏微分を表すヘッセ行列を近似するソルバーです。さらに、ヘッセ行列の逆を近似して、パラメーターの更新を実行します。実装では、&lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGSの&lt;/a&gt; Scipyバージョンを使用します。</target>
        </trans-unit>
        <trans-unit id="554ef38240e48c4335936815621409310d3aac71" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="translated">L-BFGSは、関数の2次偏導関数を表すヘッセ行列を近似するソルバーです。さらに、パラメーターの更新を実行するために、ヘッセ行列の逆行列を近似します。実装では、Scipyバージョンの&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGSを使用し&lt;/a&gt;ます。</target>
        </trans-unit>
        <trans-unit id="7bcf28acc035a046a1884f5a68a7e0642aed3a3f" translate="yes" xml:space="preserve">
          <source>L-BFGS-B &amp;ndash; Software for Large-scale Bound-constrained Optimization</source>
          <target state="translated">L-BFGS-B &amp;ndash;大規模な境界制約付き最適化のためのソフトウェア</target>
        </trans-unit>
        <trans-unit id="a9d5151f1c406ba9642eb6d20ad7472462d4b8c9" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning 24, pages 123-140, 1996.</source>
          <target state="translated">L.ブライマン、「バギング予測子」、機械学習24、123〜140ページ、1996年。</target>
        </trans-unit>
        <trans-unit id="05401786a74b32c74f5aaf77879ff5fe2a1ce4dc" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning, 24(2), 123-140, 1996.</source>
          <target state="translated">L.ブライマン、「バギング予測子」、機械学習、24（2）、123-140、1996。</target>
        </trans-unit>
        <trans-unit id="ae813a657051355d781d3ca7a4417546370b5fb0" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Pasting small votes for classification in large databases and on-line&amp;rdquo;, Machine Learning, 36(1), 85-103, 1999.</source>
          <target state="translated">L.ブライマン、「大規模データベースおよびオンラインでの分類のための小票の貼り付け」、機械学習、36（1）、85-103、1999年。</target>
        </trans-unit>
        <trans-unit id="97e482bcc046e44b1e543a4a852a328e802cd962" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Random Forests&amp;rdquo;, Machine Learning, 45(1), 5-32, 2001. &lt;a href=&quot;https://doi.org/10.1023/A:1010933404324&quot;&gt;https://doi.org/10.1023/A:1010933404324&lt;/a&gt;</source>
          <target state="translated">L. Breiman、「ランダムフォレスト」、機械学習、45（1）、5-32、2001 &lt;a href=&quot;https://doi.org/10.1023/A:1010933404324&quot;&gt;https://doi.org/10.1023/A:1010933404324&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="93aaad4c8bcdef78f99bc463e879b251fb063491" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &amp;ldquo;Classification and Regression Trees&amp;rdquo;, Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L.ブライマン、J。フリードマン、R。オルシェン、およびC.ストーン、「分類と回帰木」、ワズワース、ベルモント、カリフォルニア、1984年。</target>
        </trans-unit>
        <trans-unit id="728ad1a9616394c8f19b0d53311780e8eed780ec" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L.このような場合には,「分類と回帰の木」を用いて,「分類と回帰の木」を作成することができる。分類と回帰木.Wadsworth,Belmont,CA,1984.</target>
        </trans-unit>
        <trans-unit id="26e831dbfd841f8bca5cddecddc5d95f765adc3b" translate="yes" xml:space="preserve">
          <source>L. Breiman, P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;Submodel selection and evaluation in regression: The X-random case&lt;/a&gt;, International Statistical Review 1992;</source>
          <target state="translated">L.ブライマン、P。スペクターサブ&lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;モデルの選択と回帰の評価：Xランダムの場合&lt;/a&gt;、International Statistical Review 1992;</target>
        </trans-unit>
        <trans-unit id="da524759b928a0c6c0410a2ba55315d0723efbf9" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="translated">L.ブレイマン、A。カトラー、「ランダムフォレスト」、&lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http：&lt;/a&gt; //www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</target>
        </trans-unit>
        <trans-unit id="c982df17d29f8aba32aa6a03bfa726201c066e60" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="translated">L.ブレイマン、A。カトラー、「ランダムフォレスト」、&lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;https：&lt;/a&gt; //www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</target>
        </trans-unit>
        <trans-unit id="7ecce2961bb8a3f0ec10fc321ad8811dfc6312ac" translate="yes" xml:space="preserve">
          <source>L. F. Kozachenko, N. N. Leonenko, &amp;ldquo;Sample Estimate of the Entropy of a Random Vector&amp;rdquo;, Probl. Peredachi Inf., 23:2 (1987), 9-16</source>
          <target state="translated">LF Kozachenko、NN Leonenko、「ランダムベクトルのエントロピーのサンプル推定」、Probl。Peredachi Inf。、23：2（1987）、9-16</target>
        </trans-unit>
        <trans-unit id="d97aac3a80efb6d42fc11cefc484a2c681583627" translate="yes" xml:space="preserve">
          <source>L. F. Kozachenko, N. N. Leonenko, &amp;ldquo;Sample Estimate of the Entropy of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16</source>
          <target state="translated">LF Kozachenko、NN Leonenko、「ランダムベクトルのエントロピーのサンプル推定：、Probl。Peredachi Inf。、23：2（1987）、9-16</target>
        </trans-unit>
        <trans-unit id="f170f61c9bead94cf287d881c88071c0e3a5501e" translate="yes" xml:space="preserve">
          <source>L. Hubert and P. Arabie, Comparing Partitions, Journal of Classification 1985 &lt;a href=&quot;https://link.springer.com/article/10.1007%2FBF01908075&quot;&gt;https://link.springer.com/article/10.1007%2FBF01908075&lt;/a&gt;</source>
          <target state="translated">L.HubertおよびP.Arabie、パーティションの比較、Journal of Classification 1985 &lt;a href=&quot;https://link.springer.com/article/10.1007%2FBF01908075&quot;&gt;https://link.springer.com/article/10.1007%2FBF01908075&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9026b644be7a2edc54521dca8f44e2af501befa2" translate="yes" xml:space="preserve">
          <source>L. Mosley, &lt;a href=&quot;https://lib.dr.iastate.edu/etd/13537/&quot;&gt;A balanced approach to the multi-class imbalance problem&lt;/a&gt;, IJCV 2010.</source>
          <target state="translated">L. Mosley、&lt;a href=&quot;https://lib.dr.iastate.edu/etd/13537/&quot;&gt;マルチクラス不均衡問題へのバランスの取れたアプローチ&lt;/a&gt;、IJCV2010。</target>
        </trans-unit>
        <trans-unit id="27e9c034667fd587e63afe1b6bd9ac5dd761c4eb" translate="yes" xml:space="preserve">
          <source>L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C. Moore, John DeNero.</source>
          <target state="translated">多クラスヒンジ損失モデルのためのL1とL2の正則化 Robert C.Moore,John DeNero著</target>
        </trans-unit>
        <trans-unit id="739dce23f089e2bc4737d849cf6e6812aaac6b25" translate="yes" xml:space="preserve">
          <source>L1 Penalty and Sparsity in Logistic Regression</source>
          <target state="translated">ロジスティック回帰におけるL1ペナルティとスパリティ</target>
        </trans-unit>
        <trans-unit id="8d79d7e84774c8797e94aafbcec78896f21a814d" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{i=1}^{n} |w_i|\), which leads to sparse solutions.</source>
          <target state="translated">L1ノルム。\¶(R(w):=|w_i_i|)のように、まばらな解が得られます。</target>
        </trans-unit>
        <trans-unit id="b5bea6bb158694d1df1789af92353788b4accacc" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{j=1}^{m} |w_j|\), which leads to sparse solutions.</source>
          <target state="translated">L1ノルム。\¶(R(w):=w_j||)のように、疎な解が得られます。</target>
        </trans-unit>
        <trans-unit id="ae8ca0f194d88f499adeb94f8b5c01af62268b9f" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2\),</source>
          <target state="translated">L2ノルム。\¶(R(w):=\frac{1}{2})\sum_{i=1}^{n}w_i^2).</target>
        </trans-unit>
        <trans-unit id="2f4d6f15c348b07a6c4412e8ecac45c72eb1770b" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{j=1}^{m} w_j^2 = ||w||_2^2\),</source>
          <target state="translated">L2ノルム。\¶(R(w):=\frac{1}{2})\sum_{j=1}^{m}w_j^2=||w||_2^2)。</target>
        </trans-unit>
        <trans-unit id="e55996560b375d2b1311657b3550d521d2224094" translate="yes" xml:space="preserve">
          <source>L2 penalty (regularization term) parameter.</source>
          <target state="translated">L2 ペナルティ(正則化項)パラメータ。</target>
        </trans-unit>
        <trans-unit id="512ddf6d4bbcf9517a6def433a9acfdaefb1e3cd" translate="yes" xml:space="preserve">
          <source>LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all \(k\). This reduces the log posterior to:</source>
          <target state="translated">LDAはQDAの特殊なケースで、各クラスのGaussiansが同じ共分散行列を共有すると仮定する。\(\Sigma_k=\Sigma)for all \(k\).これは 対数事後処理を減らす</target>
        </trans-unit>
        <trans-unit id="7d7eb4b58ee70885659f8b6dfa6b739d18b840b6" translate="yes" xml:space="preserve">
          <source>LIBLINEAR &amp;ndash; A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR &amp;ndash;大規模な線形分類のためのライブラリ</target>
        </trans-unit>
        <trans-unit id="23f600324ae930d885bf27049a430c382dc77087" translate="yes" xml:space="preserve">
          <source>LIBLINEAR: A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR:大規模線形分類のためのライブラリ</target>
        </trans-unit>
        <trans-unit id="919f2c891fd7b6ae4005ef3cab68511f9b26c031" translate="yes" xml:space="preserve">
          <source>LIBSVM: A Library for Support Vector Machines</source>
          <target state="translated">LIBSVM.サポートベクターマシンのためのライブラリ</target>
        </trans-unit>
        <trans-unit id="2f7204b5759b40e38407ab9bdcb1553f2d733475" translate="yes" xml:space="preserve">
          <source>LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes.</source>
          <target state="translated">LSAは、潜在的なセマンティックインデキシング、LSIとしても知られていますが、厳密には情報検索のための永続的なインデックスとして利用されています。</target>
        </trans-unit>
        <trans-unit id="f4a5095ae748443324845cf5a2f1b28d147ed2ca" translate="yes" xml:space="preserve">
          <source>LSH Forest being an approximate method, some true neighbors from the indexed dataset might be missing from the results.</source>
          <target state="translated">LSH Forestは近似的な手法であるため、インデックス化されたデータセットの真の隣人が結果から欠落している可能性があります。</target>
        </trans-unit>
        <trans-unit id="afceea8d4c81422ac802414c94f3f49075a51ec4" translate="yes" xml:space="preserve">
          <source>LSH Forest: Locality Sensitive Hashing forest [1] is an alternative method for vanilla approximate nearest neighbor search methods. LSH forest data structure has been implemented using sorted arrays and binary search and 32 bit fixed-length hashes. Random projection is used as the hash family which approximates cosine distance.</source>
          <target state="translated">LSHフォレスト。Locality Sensitive Hashing forest [1]は、バニラ近似最近傍探索法の代替手法です。LSHフォレストのデータ構造は、ソートされた配列と二値探索、32ビット固定長ハッシュを用いて実装されています。コサイン距離を近似するハッシュファミリとしてランダム射影が用いられている。</target>
        </trans-unit>
        <trans-unit id="497cbd9196f20980eefacbc5b295901fb0a6c25f" translate="yes" xml:space="preserve">
          <source>LSTAT % lower status of the population</source>
          <target state="translated">LSTAT 人口の下位ステータス</target>
        </trans-unit>
        <trans-unit id="10e8ec7cf1b34af007bc1d6b016abc85aa0b454d" translate="yes" xml:space="preserve">
          <source>Label Propagation classifier</source>
          <target state="translated">ラベル伝搬分類器</target>
        </trans-unit>
        <trans-unit id="abaf5a09ed6812e5734e77c1313bb44d953f5d5d" translate="yes" xml:space="preserve">
          <source>Label Propagation digits active learning</source>
          <target state="translated">ラベル伝搬桁数アクティブラーニング</target>
        </trans-unit>
        <trans-unit id="a45a75b5c87b437cf487b153831ce5b94e5322d0" translate="yes" xml:space="preserve">
          <source>Label Propagation digits: Demonstrating performance</source>
          <target state="translated">ラベルの伝搬桁。性能を発揮する</target>
        </trans-unit>
        <trans-unit id="f15baf6416f92a52b1527f1d28d49a335fe3d388" translate="yes" xml:space="preserve">
          <source>Label Propagation learning a complex structure</source>
          <target state="translated">複雑な構造を学習するラベル伝搬</target>
        </trans-unit>
        <trans-unit id="5f24cba3626113f57fbd8f2c1a1dac90f055831d" translate="yes" xml:space="preserve">
          <source>Label assigned to each item via the transduction.</source>
          <target state="translated">トランスダクションを介して各項目に割り当てられたラベル。</target>
        </trans-unit>
        <trans-unit id="0154541a5d5e8e0b2444f876377737f91ad447a9" translate="yes" xml:space="preserve">
          <source>Label considered as positive and others are considered negative.</source>
          <target state="translated">ポジティブとみなされ、他の人はネガティブとみなされるラベル。</target>
        </trans-unit>
        <trans-unit id="e1c383c45e91a1b41ae4aea8504e1ff71ada889a" translate="yes" xml:space="preserve">
          <source>Label is 1 for an inlier and -1 for an outlier according to the LOF score and the contamination parameter.</source>
          <target state="translated">LOFスコアとコンタミネーション・パラメータに応じて、ラベルには、内包値が1、外包値が-1となっています。</target>
        </trans-unit>
        <trans-unit id="0facd2ec455a2234134eaa8ce0e172bf077ca396" translate="yes" xml:space="preserve">
          <source>Label of the positive class. Defaults to the greater label unless y_true is all 0 or all -1 in which case pos_label defaults to 1.</source>
          <target state="translated">正のクラスのラベル。y_trueがすべて0またはすべて-1でない限り、より大きな方のラベルがデフォルトとなります。</target>
        </trans-unit>
        <trans-unit id="4a4a633c5d3b5ebf2a9c4453fb41f8475e350bc9" translate="yes" xml:space="preserve">
          <source>Label of the positive class. If None, the maximum label is used as positive class</source>
          <target state="translated">正のクラスのラベル。Noneの場合は、最大ラベルが正のクラスとして使用されます。</target>
        </trans-unit>
        <trans-unit id="91ed314c98998b774c857769b601470c2a4233d0" translate="yes" xml:space="preserve">
          <source>Label propagation denotes a few variations of semi-supervised graph inference algorithms.</source>
          <target state="translated">ラベル伝搬は、半教師付きグラフ推論アルゴリズムのいくつかのバリエーションを示す。</target>
        </trans-unit>
        <trans-unit id="3a4c36d2f1914cbaa6f86d2f3e759e05f747e6f8" translate="yes" xml:space="preserve">
          <source>Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available:</source>
          <target state="translated">ラベル伝搬モデルには、2つのカーネル法が組み込まれています。カーネルの選択は、アルゴリズムのスケーラビリティと性能の両方に影響します。以下のものが利用可能です。</target>
        </trans-unit>
        <trans-unit id="d9c8943fba1565dfa00ecc788417147c59e84b5a" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;mean reciprocal rank&lt;/a&gt;.</source>
          <target state="translated">ラベルランキング平均精度（LRAP）は、サンプル全体で次の質問に対する答えを平均します。各グラウンドトゥルースラベルについて、ランクの高いラベルのどの部分が真のラベルでしたか？このパフォーマンス測定は、各サンプルに関連付けられたラベルにより良いランクを付けることができれば、より高くなります。得られたスコアは常に厳密に0より大きく、最良の値は1です。サンプルごとに関連するラベルが1つだけある場合、ラベルランキングの平均精度は&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;平均逆数ランクに&lt;/a&gt;相当します。</target>
        </trans-unit>
        <trans-unit id="12d27d4c8cd4504c7079d27029449977fea3fa44" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score.</source>
          <target state="translated">ラベルランキング平均精度(LRAP)は、各サンプルに割り当てられた各基底真実ラベルの平均値であり、スコアの低い真のラベルと合計ラベルの比率の平均値である。</target>
        </trans-unit>
        <trans-unit id="57882529b52287495d04cf4c6bba559a970b02d4" translate="yes" xml:space="preserve">
          <source>Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected.</source>
          <target state="translated">外れ値サンプル(指定された半径上に隣人がいないサンプル)に与えられるラベル。Noneに設定すると、外れ値が検出されたときにValueErrorが発生します。</target>
        </trans-unit>
        <trans-unit id="a3ea7d5af24c9f7706e04a90b4cc006ad64537bf" translate="yes" xml:space="preserve">
          <source>LabelSpreading model for semi-supervised learning</source>
          <target state="translated">半教師付き学習のためのLabelSpreadingモデル</target>
        </trans-unit>
        <trans-unit id="82b6583f37d4a090f2277f71261de91f41eff15e" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized:</source>
          <target state="translated">すべてのクラスのメンバーを同じクラスタに割り当てるラベリングは、常に純粋であるとは限らないため、ペナルティが課せられます。</target>
        </trans-unit>
        <trans-unit id="a59d28cce33bc578e32e7790445917276a69fe16" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized:</source>
          <target state="translated">すべてのクラスのメンバーを同じクラスタに割り当てるラベリングは、完全に均質ではないので、ペナルティがあります。</target>
        </trans-unit>
        <trans-unit id="2625047637f13a503b1aa26353d53ce007980d47" translate="yes" xml:space="preserve">
          <source>Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well:</source>
          <target state="translated">同一クラスからのメンバを持つ純粋なクラスターを持つラベリングは均質ですが、不要な分割は完全性を損なうため、Vメジャーにもペナルティを与えることになります。</target>
        </trans-unit>
        <trans-unit id="040e8af7f9faa240f939c7eb15dd2f3691882d68" translate="yes" xml:space="preserve">
          <source>Labelled data.</source>
          <target state="translated">ラベル付きデータ。</target>
        </trans-unit>
        <trans-unit id="a8a910f7e8e66128e5f0f93a7ebe3b1d5812067b" translate="yes" xml:space="preserve">
          <source>Labelling a new sample is performed by finding the nearest centroid for a given sample.</source>
          <target state="translated">新しいサンプルのラベリングは、与えられたサンプルの最も近いセントロイドを見つけることによって実行されます。</target>
        </trans-unit>
        <trans-unit id="47fc9fa69e29f326a363aa6376f6761fa85e0797" translate="yes" xml:space="preserve">
          <source>Labels assigned by the first annotator.</source>
          <target state="translated">最初のアノテータによって割り当てられたラベル。</target>
        </trans-unit>
        <trans-unit id="bdb7346e56bb733f97e8f0b9d11cce2ffadf9042" translate="yes" xml:space="preserve">
          <source>Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping &lt;code&gt;y1&lt;/code&gt; and &lt;code&gt;y2&lt;/code&gt; doesn&amp;rsquo;t change the value.</source>
          <target state="translated">2番目のアノテーターによって割り当てられたラベル。カッパ統計は対称なので、 &lt;code&gt;y1&lt;/code&gt; と &lt;code&gt;y2&lt;/code&gt; を入れ替えても値は変わりません。</target>
        </trans-unit>
        <trans-unit id="202396c3dbc4d15cb0462523b4fd7f2f49834479" translate="yes" xml:space="preserve">
          <source>Labels assigned to the centroids of the subclusters after they are clustered globally.</source>
          <target state="translated">グローバルにクラスタリングされた後のサブクラスタのセントロイドに割り当てられたラベル。</target>
        </trans-unit>
        <trans-unit id="86a5303314971b15773b1ad8460967a7978fc1e6" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels are ranging from 0-39 and correspond to the Subject IDs.</source>
          <target state="translated">各顔画像に関連付けられたラベル。これらのラベルは0~39の範囲であり、被写体IDに対応しています。</target>
        </trans-unit>
        <trans-unit id="b8a8237c586e7a43e02e7a221af16786bca65b16" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels range from 0-5748 and correspond to the person IDs.</source>
          <target state="translated">各顔画像に関連付けられたラベル。これらのラベルは、0~5748の範囲で、人のIDに対応しています。</target>
        </trans-unit>
        <trans-unit id="639c7a5f12221be9fa16d4184a91d960ee8d5fb6" translate="yes" xml:space="preserve">
          <source>Labels associated to each pair of images. The two label values being different persons or the same person.</source>
          <target state="translated">各画像のペアに関連付けられたラベル。2つのラベルの値は、異なる人物であったり、同一人物であったりします。</target>
        </trans-unit>
        <trans-unit id="dd9359ae6e29bf7b087516560ad1a2e91d10cfb0" translate="yes" xml:space="preserve">
          <source>Labels for X.</source>
          <target state="translated">Xのラベル</target>
        </trans-unit>
        <trans-unit id="0b53b6571e9267409e85ff23873e0a0824df02a7" translate="yes" xml:space="preserve">
          <source>Labels of each point</source>
          <target state="translated">各ポイントのラベル</target>
        </trans-unit>
        <trans-unit id="4350a7104cda6c17ed013efe2d00ebaae03eeb73" translate="yes" xml:space="preserve">
          <source>Labels of each point (if compute_labels is set to True).</source>
          <target state="translated">各点のラベル(compute_labelsがTrueに設定されている場合).</target>
        </trans-unit>
        <trans-unit id="8c76fdcbe4be61c2bbf79d2e67413441e31eb988" translate="yes" xml:space="preserve">
          <source>Labels of each point.</source>
          <target state="translated">各ポイントのラベル。</target>
        </trans-unit>
        <trans-unit id="9caa2dbfb17c8c2f4ae17aa6bab878223c8520e3" translate="yes" xml:space="preserve">
          <source>Labels to constrain permutation within groups, i.e. &lt;code&gt;y&lt;/code&gt; values are permuted among samples with the same group identifier. When not specified, &lt;code&gt;y&lt;/code&gt; values are permuted among all samples.</source>
          <target state="translated">グループ内の置換を制限するラベル。つまり、 &lt;code&gt;y&lt;/code&gt; 値は同じグループ識別子を持つサンプル間で置換されます。指定しない場合、 &lt;code&gt;y&lt;/code&gt; の値はすべてのサンプル間で並べ替えられます。</target>
        </trans-unit>
        <trans-unit id="c21c4f0b2fc516030c767721367e1d2fba51e007" translate="yes" xml:space="preserve">
          <source>Labels.</source>
          <target state="translated">Labels.</target>
        </trans-unit>
        <trans-unit id="45efe9972f3bf7c62e3db1678d501faf12d10c1b" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_clusters&lt;/code&gt; and &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">大きな &lt;code&gt;n_clusters&lt;/code&gt; と &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28e08fa26129e68210c4b016ca6da1b08a1a37e9" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">大きな &lt;code&gt;n_samples&lt;/code&gt; と &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d40959dcecc27d1d44b2e4cffa59a304e9a052a1" translate="yes" xml:space="preserve">
          <source>Large dataset, outlier removal, data reduction.</source>
          <target state="translated">大規模データセット、外れ値除去、データ削減</target>
        </trans-unit>
        <trans-unit id="8f1784e927c9c4e578edb46d860596eed4a90b35" translate="yes" xml:space="preserve">
          <source>Large outliers</source>
          <target state="translated">大規模な外れ値</target>
        </trans-unit>
        <trans-unit id="20dfcd03ef69fe3c6c6e8549019c43956f87d5db" translate="yes" xml:space="preserve">
          <source>Lars computes a path solution only for each kink in the path. As a result, it is very efficient when there are only of few kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting any meta parameter. On the opposite, coordinate descent compute the path points on a pre-specified grid (here we use the default). Thus it is more efficient if the number of grid points is smaller than the number of kinks in the path. Such a strategy can be interesting if the number of features is really large and there are enough samples to select a large amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the coordinate descent algorithm will only sample the path on a grid.</source>
          <target state="translated">Larsはパス内の各キンクに対してのみパス解を計算します。その結果、特徴量やサンプル数が少ない場合など、キンクが少ない場合に非常に効率的です。また、メタパラメータを設定することなく、完全なパスを計算することができます。これに対して,座標降下は,あらかじめ指定されたグリッド上のパス点を計算します(ここではデフォルトを使用します).したがって、グリッド点の数がパスのキンクの数よりも少ない方が効率的です。このような手法は、特徴量が非常に多く、大量の特徴量を選択するのに十分なサンプルがある場合に興味深いものとなります。数値誤差の点では、大きく相関する変数の場合、Larsはより多くの誤差を蓄積しますが、座標降下アルゴリズムはグリッド上のパスのみをサンプルすることになります。</target>
        </trans-unit>
        <trans-unit id="fafbf93538200568ab2506c2a63168c161506b4f" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net</source>
          <target state="translated">ラッソと弾性ネット</target>
        </trans-unit>
        <trans-unit id="64045413f4cce0f6cc0a64e33254b9beab1142d8" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net for Sparse Signals</source>
          <target state="translated">疎な信号のためのラッソと弾性ネット</target>
        </trans-unit>
        <trans-unit id="02b3c1dbfc5f6c26007e2282ba4be10a77581a65" translate="yes" xml:space="preserve">
          <source>Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent.</source>
          <target state="translated">投げ縄と弾性ネット(L1とL2のペナルティ)を座標降下を用いて実装しました。</target>
        </trans-unit>
        <trans-unit id="721bb6d50a67145009b7e81abd6add7dc9980ff6" translate="yes" xml:space="preserve">
          <source>Lasso computed by least-angle regression</source>
          <target state="translated">最小角度回帰によって計算されたラッソ</target>
        </trans-unit>
        <trans-unit id="c805258f4c266592bbe9892ca4c6fe8fe41525e3" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path</source>
          <target state="translated">正則化パスに沿った反復的なフィッティングを持つラッソ線形モデル</target>
        </trans-unit>
        <trans-unit id="47657b9ded4cc2a2b0764459c9d492e6cc3f4eb7" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path.</source>
          <target state="translated">正則化パスに沿って反復的にフィットするLASSO線形モデル。</target>
        </trans-unit>
        <trans-unit id="af3dece2cf6ae684f46dbebc7279e4f62e00335d" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Lars using BIC or AIC for model selection</source>
          <target state="translated">モデル選択にBICまたはAICを使用したLarsでのLassoモデル適合</target>
        </trans-unit>
        <trans-unit id="050a0d126029facc258b43169ac1e55a978389bf" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a.</source>
          <target state="translated">LAST ANGLE Regression(最小角度回帰)を用いたLASSOモデルの適合。</target>
        </trans-unit>
        <trans-unit id="9cd5532bfae0b1e27ef3555196bbd1195b2078fe" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a. Lars</source>
          <target state="translated">最小角回帰を用いたラスソモデル適合、別名ラース</target>
        </trans-unit>
        <trans-unit id="7cbdf91f396ae23c8822ab30bdd340882655aa26" translate="yes" xml:space="preserve">
          <source>Lasso model selection: Cross-Validation / AIC / BIC</source>
          <target state="translated">ラッソモデルの選択 クロスバリデーション/AIC/BIC</target>
        </trans-unit>
        <trans-unit id="5632592b831e91b94b8f3294c0115a52d64872b7" translate="yes" xml:space="preserve">
          <source>Lasso models (see the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; User Guide section) estimates sparse coefficients. LassoCV applies cross validation in order to determine which value of the regularization parameter (&lt;code&gt;alpha&lt;/code&gt;) is best suited for the model estimation.</source>
          <target state="translated">ラッソモデル（&lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;ラッソ&lt;/a&gt;ユーザーガイドのセクションを参照）は、スパース係数を推定します。LassoCVは、正則化パラメーター（ &lt;code&gt;alpha&lt;/code&gt; ）のどの値がモデル推定に最も適しているかを判断するために交差検定を適用します。</target>
        </trans-unit>
        <trans-unit id="51c5bc73e17f640c8a180ee453dbdca923d8c408" translate="yes" xml:space="preserve">
          <source>Lasso on dense and sparse data</source>
          <target state="translated">密なデータと疎なデータに対するラッソ</target>
        </trans-unit>
        <trans-unit id="4222e17e965145615293d33dd92e1394e71c2b5b" translate="yes" xml:space="preserve">
          <source>Lasso path using LARS</source>
          <target state="translated">LARSを用いたラスソパス</target>
        </trans-unit>
        <trans-unit id="1acac83cf58491df993404acd51caed4c4458648" translate="yes" xml:space="preserve">
          <source>Lasso using coordinate descent (&lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;)</source>
          <target state="translated">座標降下を使用した投げ縄（&lt;a href=&quot;linear_model#lasso&quot;&gt;投げ縄&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="e33c33e9d593ce188f7d437b3dd829993a23358f" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.</source>
          <target state="translated">潜在ディリクレ配置は、テキストコーパスのような離散的なデータセットのコレクションのための生成的確率モデルである。また、文書の集合から抽象的なトピックを発見するために使用されるトピックモデルでもある。</target>
        </trans-unit>
        <trans-unit id="b259b9fed25933f3361602dc71394efbeb9d0882" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation with online variational Bayes algorithm</source>
          <target state="translated">オンライン変分ベイズアルゴリズムを用いた潜在ディリクレ配置</target>
        </trans-unit>
        <trans-unit id="691257140e4ed31a708c6cf301cec44aee34c69f" translate="yes" xml:space="preserve">
          <source>Latent representations of the data.</source>
          <target state="translated">データの潜在的な表現。</target>
        </trans-unit>
        <trans-unit id="7972223ce1d5a83652f334b349de24d196516da5" translate="yes" xml:space="preserve">
          <source>Later you can load back the pickled model (possibly in another Python process) with:</source>
          <target state="translated">後で、Pickededモデルを(おそらく別のPythonプロセスで)ロードし直すことができます。</target>
        </trans-unit>
        <trans-unit id="af705669290f66a0c593b0deebc97c8dff7d4996" translate="yes" xml:space="preserve">
          <source>Later, you can reload the pickled model (possibly in another Python process) with:</source>
          <target state="translated">後日、(おそらく別のPythonプロセスで)Pickedモデルをリロードすることができます。</target>
        </trans-unit>
        <trans-unit id="87b4154b3c380b9ca1fa8f1419dd8e2c1d34065a" translate="yes" xml:space="preserve">
          <source>Latitude house block latitude</source>
          <target state="translated">ラティチュードハウスブロックラティチュード</target>
        </trans-unit>
        <trans-unit id="5d6517da9252e690b07eb861ecaf7b79646512be" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">&lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; に&lt;/a&gt;渡される葉のサイズ。これは、構築とクエリの速度、およびツリーの格納に必要なメモリに影響を与える可能性があります。最適な値は、問題の性質によって異なります。</target>
        </trans-unit>
        <trans-unit id="90341c46ba90925b69433ce5faecb3e1b8c85d8c" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;code&gt;BallTree&lt;/code&gt; or &lt;code&gt;KDTree&lt;/code&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">&lt;code&gt;BallTree&lt;/code&gt; または &lt;code&gt;KDTree&lt;/code&gt; に渡されるリーフサイズ。これは、構築とクエリの速度、およびツリーの格納に必要なメモリに影響を与える可能性があります。最適な値は、問題の性質によって異なります。</target>
        </trans-unit>
        <trans-unit id="adfd1a5c3117b99a14c45a4ae06038fd4593b137" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">BallTree または KDTree に渡されるリーフサイズ。これは、ツリーを保存するのに必要なメモリだけでなく、構築や問い合わせの速度にも影響します。最適な値は、問題の性質に依存します。</target>
        </trans-unit>
        <trans-unit id="2f4f4f9d9992d30c454ebca3af5182554c5dd5d3" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">BallTree または cKDTree に渡されるリーフサイズ。これは、ツリーを保存するのに必要なメモリだけでなく、構築や問い合わせの速度にも影響を与えます。最適な値は、問題の性質に依存します。</target>
        </trans-unit>
        <trans-unit id="31743e5f5ee8b348cb24154ab26179446399d075" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X and returns the transformed data.</source>
          <target state="translated">データXのNMFモデルを学習し、変換したデータを返す。</target>
        </trans-unit>
        <trans-unit id="a49199fe15b3d192e2f8e78d2cfcc004b5bb592f" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X.</source>
          <target state="translated">データXのNMFモデルを学習します。</target>
        </trans-unit>
        <trans-unit id="f28a5a2a8197ba712162f1642134c8c32dff12de" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings and transform X.</source>
          <target state="translated">機能名のリスト-&amp;gt;インデックスのマッピングを学び、Xを変換します。</target>
        </trans-unit>
        <trans-unit id="8c410f4ecac33d5545793d5deb3e8b1121157db0" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings.</source>
          <target state="translated">機能名-&amp;gt;インデックスマッピングのリストをご覧ください。</target>
        </trans-unit>
        <trans-unit id="a753afaf1f2a5a0c1c19f381e2c844f4e69ccf16" translate="yes" xml:space="preserve">
          <source>Learn a vocabulary dictionary of all tokens in the raw documents.</source>
          <target state="translated">生の文書のすべてのトークンの語彙辞書を学ぶ。</target>
        </trans-unit>
        <trans-unit id="d9349583a45dc48570d0d3236e8faf8ecfef570b" translate="yes" xml:space="preserve">
          <source>Learn and apply the dimension reduction on the train data.</source>
          <target state="translated">学習し,訓練データに次元削減を適用します.</target>
        </trans-unit>
        <trans-unit id="c8e9cfdd99f37695b9bb2a2cf234f653fe10a376" translate="yes" xml:space="preserve">
          <source>Learn empirical variances from X.</source>
          <target state="translated">Xから経験的分散を学ぶ。</target>
        </trans-unit>
        <trans-unit id="650a6ae9c550e7f32470024973e3b36aee2841fa" translate="yes" xml:space="preserve">
          <source>Learn model for the data X with variational Bayes method.</source>
          <target state="translated">変分ベイズ法を用いてデータXのモデルを学習する。</target>
        </trans-unit>
        <trans-unit id="dacb80f7c7ce4a5db80b953f259da8b386886101" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights)</source>
          <target state="translated">idf ベクトル(グローバル項重み)を学習する</target>
        </trans-unit>
        <trans-unit id="fb7b507c119ba0834ef410110951b80a51da9b63" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights).</source>
          <target state="translated">idf ベクトル(グローバル項重み)を学習します。</target>
        </trans-unit>
        <trans-unit id="b331e0a3149fc26c2099c41ac3e9655d530c7a47" translate="yes" xml:space="preserve">
          <source>Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)</source>
          <target state="translated">非計算カーネルの逆変換を学習します。(すなわち,点の前像を求めることを学習する)</target>
        </trans-unit>
        <trans-unit id="351ef97b73f57653764681dfe2a94603d5d1cbab" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return document-term matrix.</source>
          <target state="translated">語彙辞書を覚えて、文書-タームマトリックスを返す。</target>
        </trans-unit>
        <trans-unit id="ee96e1f94ac61b3bff29cbb75afd2fdb8a437bed" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return term-document matrix.</source>
          <target state="translated">語彙辞書を学習し、用語-文書マトリックスを返す。</target>
        </trans-unit>
        <trans-unit id="65adc2e107d7d619d7107cf14005ab5e9c9cd5ef" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf from training set.</source>
          <target state="translated">トレーニングセットから語彙とidfを学ぶ。</target>
        </trans-unit>
        <trans-unit id="51c9b9cb5d5b207d3afa5b6e39e7500c1ff633ed" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return document-term matrix.</source>
          <target state="translated">語彙とidfを学び、文書-タームマトリックスを返す。</target>
        </trans-unit>
        <trans-unit id="d9ba5b6f4cc6cff1198de21973fdc3c62d64336f" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return term-document matrix.</source>
          <target state="translated">語彙とidfを学び、用語-文書マトリックスを返す。</target>
        </trans-unit>
        <trans-unit id="5b86400dde56a045e486ecc10d7618c7daf0f573" translate="yes" xml:space="preserve">
          <source>Learning a graph structure</source>
          <target state="translated">グラフ構造の学習</target>
        </trans-unit>
        <trans-unit id="4ecad9f15b8037b0486e20e0cb489ddd61caeca5" translate="yes" xml:space="preserve">
          <source>Learning an embedding</source>
          <target state="translated">エンベッディングの学習</target>
        </trans-unit>
        <trans-unit id="f89176d3f1741099f1479699aa585a0c6906b634" translate="yes" xml:space="preserve">
          <source>Learning and predicting</source>
          <target state="translated">学習と予測</target>
        </trans-unit>
        <trans-unit id="5087c606edcdf30c07ac8bd6a14c9b96c0975b25" translate="yes" xml:space="preserve">
          <source>Learning curve.</source>
          <target state="translated">学習曲線。</target>
        </trans-unit>
        <trans-unit id="af86142d107ea3e7d568509ca68cbef348748b05" translate="yes" xml:space="preserve">
          <source>Learning problems fall into a few categories:</source>
          <target state="translated">学習問題はいくつかのカテゴリーに分類されます。</target>
        </trans-unit>
        <trans-unit id="213b18cf4e4c891522544c2231435e470a8853a1" translate="yes" xml:space="preserve">
          <source>Learning rate schedule for weight updates.</source>
          <target state="translated">ウェイト更新のための学習率スケジュール。</target>
        </trans-unit>
        <trans-unit id="ad3970bc51aa8e2c82bc13dcb9d4922e01a06590" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each classifier by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">学習率は &lt;code&gt;learning_rate&lt;/code&gt; によって各分類子の寄与を縮小します。 &lt;code&gt;learning_rate&lt;/code&gt; と &lt;code&gt;n_estimators&lt;/code&gt; の間にはトレードオフがあります。</target>
        </trans-unit>
        <trans-unit id="cc055e36b16b7ea8669e5252649d8e3ee1af0b11" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each regressor by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">学習率は &lt;code&gt;learning_rate&lt;/code&gt; によって各リグレッサの寄与を縮小します。 &lt;code&gt;learning_rate&lt;/code&gt; と &lt;code&gt;n_estimators&lt;/code&gt; の間にはトレードオフがあります。</target>
        </trans-unit>
        <trans-unit id="8982fb177d3b5a895540d84670a324c9b8376572" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.</source>
          <target state="translated">予測関数のパラメーターを学習して同じデータでテストするのは方法論上の誤りです。見たばかりのサンプルのラベルを繰り返すだけのモデルでは、完全なスコアが得られますが、まだ有用なものは何も予測できません。目に見えないデータ。この状況は、&lt;strong&gt;過剰適合&lt;/strong&gt;と呼ばれ&lt;strong&gt;ます&lt;/strong&gt;。これを回避するには、（教師あり）機械学習実験を実行するときに、利用可能なデータの一部を&lt;strong&gt;テストセット&lt;/strong&gt; &lt;code&gt;X_test, y_test&lt;/code&gt; として保持するのが一般的です。 「実験」という用語は、学術的な使用のみを意味するものではないことに注意してください。商業的な設定であっても、機械学習は通常実験的に始まるためです。</target>
        </trans-unit>
        <trans-unit id="7ed56c1456833faed4202c795166989f5307ee69" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by &lt;a href=&quot;grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; techniques.</source>
          <target state="translated">予測関数のパラメーターを学習し、同じデータでそれをテストすることは、方法論的な誤りです。今見たサンプルのラベルを繰り返すだけのモデルは、完全なスコアを持ちますが、まだ有用なものを予測することはできません。見えないデータ。この状況は&lt;strong&gt;過剰適合&lt;/strong&gt;と呼ばれ&lt;strong&gt;ます&lt;/strong&gt;。これを回避するために、（教師あり）機械学習実験を実行する場合、利用可能なデータの一部を&lt;strong&gt;テストセット&lt;/strong&gt; &lt;code&gt;X_test, y_test&lt;/code&gt; として保持するのが一般的な方法です。商業環境でも機械学習は通常実験的に開始されるため、「実験」という言葉は学術的な使用のみを意味するものではないことに注意してください。これは、モデルトレーニングにおける典型的な相互検証ワークフローのフローチャートです。最適なパラメータは、次の方法で決定できます。&lt;a href=&quot;grid_search#grid-search&quot;&gt;グリッド検索&lt;/a&gt;手法。</target>
        </trans-unit>
        <trans-unit id="93c3e1794e48ba7d8637b32d813e97686cf36d4f" translate="yes" xml:space="preserve">
          <source>Learns each output independently rather than chaining.</source>
          <target state="translated">各出力を連鎖させるのではなく、独立して学習します。</target>
        </trans-unit>
        <trans-unit id="5fce8b00092369b98dfb920b76a7ee0efe5e00b1" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a.</source>
          <target state="translated">最小角回帰モデル、別名。</target>
        </trans-unit>
        <trans-unit id="3b28e26eb21f16fdbdefabf1ed5ad375edeafb8b" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a. LAR</source>
          <target state="translated">最小角回帰モデル別名LAR</target>
        </trans-unit>
        <trans-unit id="b8ab306ac662259fba4aa6725b193c75be140b61" translate="yes" xml:space="preserve">
          <source>Least Squares projection of the data onto the sparse components.</source>
          <target state="translated">データの疎な成分への最小二乗投影.</target>
        </trans-unit>
        <trans-unit id="2c3aa035aea93ac3dc79ecee5528b7c8dcfba4ab" translate="yes" xml:space="preserve">
          <source>Least absolute deviation (&lt;code&gt;'lad'&lt;/code&gt;): A robust loss function for regression. The initial model is given by the median of the target values.</source>
          <target state="translated">最小絶対偏差（ &lt;code&gt;'lad'&lt;/code&gt; ）：回帰のロバストな損失関数。初期モデルは、ターゲット値の中央値によって与えられます。</target>
        </trans-unit>
        <trans-unit id="3aeaacb76e6b5d496047f324133ddd0747e1d2c6" translate="yes" xml:space="preserve">
          <source>Least squares (&lt;code&gt;'ls'&lt;/code&gt;): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.</source>
          <target state="translated">最小二乗（ &lt;code&gt;'ls'&lt;/code&gt; ）：優れた計算特性による回帰の自然な選択。初期モデルは、ターゲット値の平均によって与えられます。</target>
        </trans-unit>
        <trans-unit id="972ad47a68ab0dd02c8d4f9c32f25ef79120c408" translate="yes" xml:space="preserve">
          <source>Least-Squares: Linear regression (Ridge or Lasso depending on \(R\)). \(L(y_i, f(x_i)) = \frac{1}{2}(y_i - f(x_i))^2\).</source>
          <target state="translated">最小二乗。線形回帰(Ridge or Lasso depending on \(R)).\(L(y_i,f(x_i))=\frac{1}{2}(y_i-f(x_i))^2).</target>
        </trans-unit>
        <trans-unit id="7963186b092849241b779637d34ce64214b0375a" translate="yes" xml:space="preserve">
          <source>Least-Squares: Ridge Regression.</source>
          <target state="translated">最小二乗.リッジ回帰。</target>
        </trans-unit>
        <trans-unit id="acf6db0396d489bb160af474285d57fb823df68a" translate="yes" xml:space="preserve">
          <source>Least-angle regression (&lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt;)</source>
          <target state="translated">最小角度回帰（&lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;最小角度回帰&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="8427893bdf84ecb2b84085547aa1cfdd8fd0e807" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.</source>
          <target state="translated">最小角回帰(LARS)は、Bradley Efron、Trevor Hastie、Iain Johnstone、Robert Tibshiraniによって開発された高次元データのための回帰アルゴリズムです。LARSは、フォワードステップワイズ回帰に似ています。各ステップで、ターゲットと最も相関のある特徴を見つけます。同等の相関を持つ複数の特徴がある場合、同じ特徴に沿って進むのではなく、特徴間で等距離の方向に進みます。</target>
        </trans-unit>
        <trans-unit id="818f02ffe71d576f8833c06d3318f5d50790be37" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the predictor most correlated with the response. When there are multiple predictors having equal correlation, instead of continuing along the same predictor, it proceeds in a direction equiangular between the predictors.</source>
          <target state="translated">最小角回帰(LARS)は、Bradley Efron、Trevor Hastie、Iain Johnstone、Robert Tibshiraniによって開発された高次元データのための回帰アルゴリズムです。LARSは、フォワードステップワイズ回帰に似ています。各ステップで、それは応答と最も相関のある予測変数を見つけます。同等の相関を持つ複数の予測変数がある場合,同じ予測変数に沿って進むのではなく,予測変数間で等角な方向に進む.</target>
        </trans-unit>
        <trans-unit id="5cc9936fd171dfb4c941611970a01e31c9182cee" translate="yes" xml:space="preserve">
          <source>Leave One Group Out cross-validator</source>
          <target state="translated">一つのグループを残す クロスバリデーター</target>
        </trans-unit>
        <trans-unit id="708b3ff9ed12b2c6f3635d37f516d672f76ad26e" translate="yes" xml:space="preserve">
          <source>Leave P Group(s) Out cross-validator</source>
          <target state="translated">P グループを残してクロスバリデータを出力する</target>
        </trans-unit>
        <trans-unit id="b1d423c90dfa79c0db1cf2e91d8b80c110d2debb" translate="yes" xml:space="preserve">
          <source>Leave P groups out.</source>
          <target state="translated">Pグループは放置しておく。</target>
        </trans-unit>
        <trans-unit id="2e788c12c63436d5bbf2b3d54792d07b4ad5906d" translate="yes" xml:space="preserve">
          <source>Leave P observations out.</source>
          <target state="translated">Pの観察は省く。</target>
        </trans-unit>
        <trans-unit id="23a4dfbb0e55172e2c29fa75763519463b465b57" translate="yes" xml:space="preserve">
          <source>Leave one observation out.</source>
          <target state="translated">一つの観察を外に出しておく。</target>
        </trans-unit>
        <trans-unit id="96e7c056605d5580183d915f0e8250d81cc4028b" translate="yes" xml:space="preserve">
          <source>Leave-One-Out cross-validator</source>
          <target state="translated">リーブワンアウトのクロスバリデーター</target>
        </trans-unit>
        <trans-unit id="a3d5fb094bf6540a5945dfebfc612a40422d0970" translate="yes" xml:space="preserve">
          <source>Leave-P-Out cross-validator</source>
          <target state="translated">リーブプアウトクロスバリデータ</target>
        </trans-unit>
        <trans-unit id="7fa92633d7eb4070a1a9e7f3ffd6a6dd808d5514" translate="yes" xml:space="preserve">
          <source>Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.</source>
          <target state="translated">Ledoit O,Wolf M.Honey,I Shrunk the Sample Covariance Matrix.ポートフォリオマネジメントのジャーナル 30(4),110-119,2004.</target>
        </trans-unit>
        <trans-unit id="b6a08e295c1dafc447ef93ac82d0e6a70b01528e" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf&amp;rsquo;s formula as described in &amp;ldquo;A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices&amp;rdquo;, Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.</source>
          <target state="translated">Ledoit-Wolfは収縮の特定の形式であり、収縮係数は「大規模共分散行列の適切な推定量」、LedoitおよびWolf、Journal of Multivariate Analysisで説明されているO. LedoitおよびM. Wolfの公式を使用して計算されます。 、88巻、2号、2004年2月、ページ365〜411。</target>
        </trans-unit>
        <trans-unit id="b450ff5574aa7547a2d2804a59fde9043d1f11e3" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf vs OAS estimation</source>
          <target state="translated">Ledoit-Wolf vs OASの見積もり</target>
        </trans-unit>
        <trans-unit id="74b56641357b357e1a04f8ba20caa0211258f1b9" translate="yes" xml:space="preserve">
          <source>LedoitWolf Estimator</source>
          <target state="translated">LedoitWolfの見積もり</target>
        </trans-unit>
        <trans-unit id="a7127a921977497178bbe9d19b374d5b3660e695" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y)</source>
          <target state="translated">返されたカーネル k(X,Y)の左引数</target>
        </trans-unit>
        <trans-unit id="db46139863e59ff060f10e61ce009637ab35e3fe" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y).</source>
          <target state="translated">返されたカーネル k(X,Y)の左引数。</target>
        </trans-unit>
        <trans-unit id="9bf558c9b2f1ab98bdd863d46c825f77b8bcb622" translate="yes" xml:space="preserve">
          <source>Left to right.</source>
          <target state="translated">左から右へ</target>
        </trans-unit>
        <trans-unit id="75bf879e9683d8e42f9cdbce4ac2378477aafa4c" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;</source>
          <target state="translated">パスの長さ。 &lt;code&gt;eps=1e-3&lt;/code&gt; は、 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt; ことを意味します</target>
        </trans-unit>
        <trans-unit id="c25fd55e85b58584519914fbcbbc8e0b71dacb4b" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;.</source>
          <target state="translated">パスの長さ。 &lt;code&gt;eps=1e-3&lt;/code&gt; は、 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt; ことを意味します。</target>
        </trans-unit>
        <trans-unit id="817bbec7d68f2acac91e1c4383d071e50b618e53" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters</source>
          <target state="translated">パラメータ数に対する感度が低い</target>
        </trans-unit>
        <trans-unit id="820f47ab7dc5f77aa60df5d42cf3669d7140be19" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters:</source>
          <target state="translated">パラメータ数に対する感度が低い</target>
        </trans-unit>
        <trans-unit id="5f1b602bd58c70400c6cc3123702692e92eee6c0" translate="yes" xml:space="preserve">
          <source>Lessons learned</source>
          <target state="translated">学んだこと</target>
        </trans-unit>
        <trans-unit id="650648dcfa58ca5d69540fc9d7c76c71c03cdd8d" translate="yes" xml:space="preserve">
          <source>Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a function mapping x to a Hilbert space. KernelCenterer centers (i.e., normalize to have zero mean) the data without explicitly computing phi(x). It is equivalent to centering phi(x) with sklearn.preprocessing.StandardScaler(with_std=False).</source>
          <target state="translated">K(x,z)を phi(x)^T phi(z)で定義されるカーネルとします。KernelCenterer は、明示的に phi(x)を計算せずにデータをセンタリングします(つまり、平均値がゼロになるように正規化します)。これは,sklearn.preprocessing.StandardScaler(with_std=False)でphi(x)をセンタリングするのと同等です.</target>
        </trans-unit>
        <trans-unit id="821c4d001d9578c562e1b0af64f8be6da39e632f" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(\sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="translated">\（S \）を相似行列、\（X \）を\（n \）入力点の座標とします。視差\（\ hat {d} _ {ij} \）は、いくつかの最適な方法で選択された類似性の変換です。応力と呼ばれる目的は、\（\ sum_ {i &amp;lt;j} d_ {ij}（X）-\ hat {d} _ {ij}（X）\）によって定義されます。</target>
        </trans-unit>
        <trans-unit id="4d3e4f22e7563805ce13fd93e247765bb5c10653" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="translated">\（S \）を類似度行列、\（X \）を\（n \）入力点の座標とします。格差\（\ hat {d} _ {ij} \）は、いくつかの最適な方法で選択された類似度の変換です。ストレスと呼ばれる目的は、\（sum_ {i &amp;lt;j} d_ {ij}（X）-\ hat {d} _ {ij}（X）\）によって定義されます。</target>
        </trans-unit>
        <trans-unit id="5ab8a1b7961470064849295cd500b6579b4fa397" translate="yes" xml:space="preserve">
          <source>Let \(X_S\) be the set of target features (i.e. the &lt;code&gt;features&lt;/code&gt; parameter) and let \(X_C\) be its complement.</source>
          <target state="translated">\（X_S \）をターゲット機能のセット（つまり、 &lt;code&gt;features&lt;/code&gt; パラメーター）とし、\（X_C \）をその補集合とします。</target>
        </trans-unit>
        <trans-unit id="c0a7cf3804b0fab7efc2f61eebe3b5930a870c77" translate="yes" xml:space="preserve">
          <source>Let the data at node \(m\) be represented by \(Q\). For each candidate split \(\theta = (j, t_m)\) consisting of a feature \(j\) and threshold \(t_m\), partition the data into \(Q_{left}(\theta)\) and \(Q_{right}(\theta)\) subsets</source>
          <target state="translated">For each candidate split 》 For each candidate split 》 For each candidate split 》 For Another Another split</target>
        </trans-unit>
        <trans-unit id="0eec5761b9ded7fa59a47d01c0fcb2883cae7dd4" translate="yes" xml:space="preserve">
          <source>Let us now try to reconstruct the original image from the patches by averaging on overlapping areas:</source>
          <target state="translated">次に、重なり合っている部分を平均化して、パッチから元の画像を再構築してみましょう。</target>
        </trans-unit>
        <trans-unit id="b5fecaca6e2e29d4dbee3904d66ce06eade247b5" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s compute the performance of this constant prediction baseline with 3 different regression metrics:</source>
          <target state="translated">3つの異なる回帰メトリックを使用して、この一定の予測ベースラインのパフォーマンスを計算してみましょう。</target>
        </trans-unit>
        <trans-unit id="253db59ca1134838191e01d302a48262e9ba6374" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s consider the following trained regression model:</source>
          <target state="translated">次の訓練された回帰モデルを考えてみましょう。</target>
        </trans-unit>
        <trans-unit id="b44b9f209d8c4175d9e9b8f1619b5304447365c9" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s fit a MLPRegressor and compute single-variable partial dependence plots</source>
          <target state="translated">MLPRegressorを適合させて、単一変数の部分依存プロットを計算してみましょう。</target>
        </trans-unit>
        <trans-unit id="c338f438f84e4277f756f203f24830c86a10546d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s load data from the newsgroups dataset which comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).</source>
          <target state="translated">トレーニング（または開発）用とテスト（またはパフォーマンス評価）用の2つのサブセットに分割された20のトピックに関する約18000のニュースグループ投稿で構成されるニュースグループデータセットからデータをロードしましょう。</target>
        </trans-unit>
        <trans-unit id="b852fb92508c2f8d11c716ed1054a5db1d558210" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s load the motor claim dataset from OpenML: &lt;a href=&quot;https://www.openml.org/d/41214&quot;&gt;https://www.openml.org/d/41214&lt;/a&gt;</source>
          <target state="translated">OpenMLからモータークレームデータセットをロードしましょう：&lt;a href=&quot;https://www.openml.org/d/41214&quot;&gt;https&lt;/a&gt;：//www.openml.org/d/41214</target>
        </trans-unit>
        <trans-unit id="ed3c266c74a01e549482a05ad99384adcdbf59a9" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s make the same partial dependence plot for the 2 features interaction, this time in 3 dimensions.</source>
          <target state="translated">今回は3次元で、2つの特徴の相互作用に対して同じ部分依存プロットを作成しましょう。</target>
        </trans-unit>
        <trans-unit id="2f393e9b32e94a400250ae55bd302c219802fcb3" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s now compute the partial dependence plots for this neural network using the model-agnostic (brute-force) method:</source>
          <target state="translated">モデルにとらわれない（ブルートフォース）方法を使用して、このニューラルネットワークの部分依存プロットを計算してみましょう。</target>
        </trans-unit>
        <trans-unit id="c401ba9464abf901fd51a8e8666504a0d45adbfb" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s now fit a GradientBoostingRegressor and compute the partial dependence plots either or one or two variables at a time.</source>
          <target state="translated">ここで、GradientBoostingRegressorを適合させ、部分依存プロットを一度に1つまたは2つの変数で計算してみましょう。</target>
        </trans-unit>
        <trans-unit id="106ecb5f7c6bb4669d70caeae32d17518696e61b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s print the first lines of the first loaded file:</source>
          <target state="translated">最初にロードされたファイルの最初の行を印刷してみましょう：</target>
        </trans-unit>
        <trans-unit id="bcd495b6fedeb570ab62363e5dbb308b08a2e969" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 25, and 50, and want to know their class name.</source>
          <target state="translated">サンプル10、25、および50に興味があり、それらのクラス名を知りたいとします。</target>
        </trans-unit>
        <trans-unit id="65595eba1f80a7173dc24674f2afc2d5837968b2" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 50, and 85, and want to know their class name.</source>
          <target state="translated">サンプル10、50、および85に興味があり、それらのクラス名を知りたいとします。</target>
        </trans-unit>
        <trans-unit id="fb46983e946ca9f3803c9b6fd00931719bb67d7b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 80, and 140, and want to know their class name.</source>
          <target state="translated">サンプル10、80、および140に興味があり、それらのクラス名を知りたいとします。</target>
        </trans-unit>
        <trans-unit id="e8b31884f71e52e12b72fcb054664d8ab80d318b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; cross-validation object:</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;相互検証オブジェクトがどのように検索されるかを見てみましょう。</target>
        </trans-unit>
        <trans-unit id="36ae1b66774b74a6fe504ba4aa0655c5c58c7d06" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;code&gt;KFold&lt;/code&gt; cross-validation object:</source>
          <target state="translated">&lt;code&gt;KFold&lt;/code&gt; 交差検定オブジェクトを探す方法を見てみましょう：</target>
        </trans-unit>
        <trans-unit id="9c035fe2592c35e4264add67f9ea318300cfbf19" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take a look at what the most informative features are:</source>
          <target state="translated">最も有益な機能は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="fc26adc7a4427a7251d50d414ace6a50d7fbe76d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents:</source>
          <target state="translated">次のカウントの例を見てみましょう。最初の用語は100％存在するため、あまり興味深いものではありません。他の2つの機能は、50％未満の時間でしか機能しないため、おそらくドキュメントの内容をより代表的に示しています。</target>
        </trans-unit>
        <trans-unit id="2f95c182cf5fa2e13ce2622defe8af992d765313" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s try again with the default setting:</source>
          <target state="translated">デフォルトの設定でもう一度試してみましょう。</target>
        </trans-unit>
        <trans-unit id="c3a6b9996c4370e6a8670703084aa093c6face20" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:</source>
          <target state="translated">それを使用して、テキストドキュメントの最小主義的コーパスの単語の出現をトークン化してカウントします。</target>
        </trans-unit>
        <trans-unit id="292ee05108cd151612c92edea6da34acc9a56662" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use pandas to load a copy of the titanic dataset. The following shows how to apply separate preprocessing on numerical and categorical features.</source>
          <target state="translated">パンダを使用して、タイタニックデータセットのコピーをロードしましょう。以下に、数値およびカテゴリの特徴に個別の前処理を適用する方法を示します。</target>
        </trans-unit>
        <trans-unit id="c77ba9dd4a1f63d6e3e47c6ccfdd6637c48fa284" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s visually compare the cross validation behavior for many scikit-learn cross-validation objects. Below we will loop through several common cross-validation objects, visualizing the behavior of each.</source>
          <target state="translated">多くのscikit-learn相互検証オブジェクトの相互検証動作を視覚的に比較してみましょう。以下では、いくつかの一般的な交差検証オブジェクトをループして、それぞれの動作を視覚化します。</target>
        </trans-unit>
        <trans-unit id="6c69807d4e78cfb8da9f8e8c21f378d88124782a" translate="yes" xml:space="preserve">
          <source>Level of verbosity.</source>
          <target state="translated">冗長性のレベル。</target>
        </trans-unit>
        <trans-unit id="339e81226696b46d7377960244af7f4dcb540176" translate="yes" xml:space="preserve">
          <source>Lewis, D. D., Yang, Y., Rose, T. G., &amp;amp; Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397.</source>
          <target state="translated">ルイス、DD、ヤン、Y。、ローズ、TG、およびリー、F。（2004）。RCV1：テキスト分類研究のための新しいベンチマークコレクション。Journal of Machine Learning Research、5、361-397。</target>
        </trans-unit>
        <trans-unit id="ee9267aef527ceaeed70f092da783571e1b2536d" translate="yes" xml:space="preserve">
          <source>Libsvm GUI</source>
          <target state="translated">Libsvm GUI</target>
        </trans-unit>
        <trans-unit id="87b3d037e844d0b272ce5bfc0fb0a06e31f13827" translate="yes" xml:space="preserve">
          <source>License: BSD 3 clause</source>
          <target state="translated">ライセンス:BSD 3節</target>
        </trans-unit>
        <trans-unit id="538c09161b8497f998404cafc34964ed3a445575" translate="yes" xml:space="preserve">
          <source>Licensed under the 3-clause BSD License.</source>
          <target state="translated">3句BSDライセンスの下でライセンスされています。</target>
        </trans-unit>
        <trans-unit id="d119b02c417272fad54f56fb5c480a5a866c4e2e" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="translated">リッチマン、M。（2013）。UCI Machine Learning Repository [ &lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt; ]。カリフォルニア州アーバイン：カリフォルニア大学情報科学部。</target>
        </trans-unit>
        <trans-unit id="e76bfa901cdd22255b64ebacf70ec6f76b7f04ab" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;https://archive.ics.uci.edu/ml&quot;&gt;https://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="translated">リッチマン、M。（2013）。UCI機械学習リポジトリ[ &lt;a href=&quot;https://archive.ics.uci.edu/ml&quot;&gt;https://archive.ics.uci.edu/ml&lt;/a&gt; ]。カリフォルニア大学アーバイン校：カリフォルニア大学情報コンピュータサイエンス学部。</target>
        </trans-unit>
        <trans-unit id="4a2cabe35d47f4d173451a3dc4bce594fc9e8434" translate="yes" xml:space="preserve">
          <source>Like &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;, forests of trees also extend to &lt;a href=&quot;tree#tree-multioutput&quot;&gt;multi-output problems&lt;/a&gt; (if Y is an array of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt;).</source>
          <target state="translated">&lt;a href=&quot;tree#tree&quot;&gt;ディシジョンツリー&lt;/a&gt;と同様に、ツリーのフォレストも&lt;a href=&quot;tree#tree-multioutput&quot;&gt;マルチ出力問題に&lt;/a&gt;拡張されます（Yがサイズの配列 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="f8c94a1d14fd76e50a18d8c5112493b7c41a75b8" translate="yes" xml:space="preserve">
          <source>Like &lt;code&gt;Pipeline&lt;/code&gt;, individual steps may be replaced using &lt;code&gt;set_params&lt;/code&gt;, and ignored by setting to &lt;code&gt;'drop'&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; と同様に、個々のステップは &lt;code&gt;set_params&lt;/code&gt; を使用して置き換え、 &lt;code&gt;'drop'&lt;/code&gt; 設定することで無視できます。</target>
        </trans-unit>
        <trans-unit id="09c07eeb7023cd495f9b67aa8c5832e9de0a3634" translate="yes" xml:space="preserve">
          <source>Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.</source>
          <target state="translated">MultinomialNBと同様に、この分類器は離散データに適している。違いは、MultinomialNBが出現回数で動作するのに対し、BernoulliNBはバイナリ/ブーリアン特徴に対応するように設計されている点です。</target>
        </trans-unit>
        <trans-unit id="33e640491ab99ccee8501c0c94b8deb9f93a420b" translate="yes" xml:space="preserve">
          <source>Like fit(X) followed by transform(X), but does not require materializing X in memory.</source>
          <target state="translated">fit(X)の後に transform(X)が続くのと似ていますが、メモリ上で X を実体化する必要はありません。</target>
        </trans-unit>
        <trans-unit id="07a7d71492c9370f4c5f214183352ca2f48a9590" translate="yes" xml:space="preserve">
          <source>Like in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using &lt;code&gt;set_params&lt;/code&gt; and searched in grid search.</source>
          <target state="translated">PipelineおよびFeatureUnionと同様に、これにより、トランスフォーマーとそのパラメーターを &lt;code&gt;set_params&lt;/code&gt; を使用して設定し、グリッド検索で検索できます。</target>
        </trans-unit>
        <trans-unit id="ace16ab25f8f0e2cb278ad02989604150a81258c" translate="yes" xml:space="preserve">
          <source>Like pipelines, feature unions have a shorthand constructor called &lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt;&lt;code&gt;make_union&lt;/code&gt;&lt;/a&gt; that does not require explicit naming of the components.</source>
          <target state="translated">パイプラインと同様に、機能ユニオンには、コンポーネントの明示的な命名を必要としない&lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt; &lt;code&gt;make_union&lt;/code&gt; &lt;/a&gt;と呼ばれる省略形のコンストラクタがあります。</target>
        </trans-unit>
        <trans-unit id="29685b73b0fbefa3dc8a3378779801b7f7266cf2" translate="yes" xml:space="preserve">
          <source>Like scalers, &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; puts all features into the same, known range or distribution. However, by performing a rank transformation, it smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.</source>
          <target state="translated">スケーラーと同様に、&lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt;はすべての機能を同じ既知の範囲または分布に配置します。ただし、ランク変換を実行することにより、異常な分布を平滑化し、スケーリング方法よりも外れ値の影響を受けにくくなります。ただし、フィーチャ内およびフィーチャ間の相関と距離を歪めます。</target>
        </trans-unit>
        <trans-unit id="73666b411bce492e14d6ba5736c0ca5eebf6eb1b" translate="yes" xml:space="preserve">
          <source>Like the Poisson GLM above, the gradient boosted trees model minimizes the Poisson deviance. However, because of a higher predictive power, it reaches lower values of Poisson deviance.</source>
          <target state="translated">上記のポアソンGLMと同様に、勾配ブーストされた木モデルはポアソンデビアンスを最小化します。しかし、予測力が高いため、ポアソンデビアンスの値が低くなります。</target>
        </trans-unit>
        <trans-unit id="0ae4ed5af04ee97eab148462e283fb7149bd9d04" translate="yes" xml:space="preserve">
          <source>Limit in bytes of the size of the cache.</source>
          <target state="translated">キャッシュのサイズをバイト単位で制限します。</target>
        </trans-unit>
        <trans-unit id="bbd76c46a461ce6867ca433ec8697501cc65b137" translate="yes" xml:space="preserve">
          <source>Limiting distance of neighbors to return. (default is the value passed to the constructor).</source>
          <target state="translated">返す隣人の距離を制限します。(デフォルトはコンストラクタに渡される値)。</target>
        </trans-unit>
        <trans-unit id="62c917554a7197d63486db913ca90de577c0bfe0" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis</source>
          <target state="translated">線形判別分析</target>
        </trans-unit>
        <trans-unit id="bb7eb231c96859c6082b4ce0d3b796b4329f6e8f" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="translated">線形判別分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）と二次判別分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）は、名前が示すように、それぞれ線形と二次の決定面を持つ2つの古典的な分類器です。</target>
        </trans-unit>
        <trans-unit id="719a12bbe391db4f9a1b1f0f22d958d133e79356" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="translated">線形判別分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）と2次判別分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）は、それぞれ名前が示すように、線形と2次の決定面を持つ2つの古典的な分類子です。</target>
        </trans-unit>
        <trans-unit id="e36f5257c349ab3d8389a5027fa29765ef7a78a4" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance &lt;em&gt;between classes&lt;/em&gt;. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.</source>
          <target state="translated">線形判別分析（LDA）は&lt;em&gt;、クラス間&lt;/em&gt;のほとんどの分散を説明する属性を特定しようとし&lt;em&gt;ます&lt;/em&gt;。特に、LDAは、PCAとは対照的に、既知のクラスラベルを使用する監視ありメソッドです。</target>
        </trans-unit>
        <trans-unit id="02924b985796944d65c857ba377ee96748a5fefe" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis and Quadratic Discriminant Analysis</source>
          <target state="translated">線形判別分析と二次判別分析</target>
        </trans-unit>
        <trans-unit id="37e8c1f7f3b5f52be8ccec8de8de9cd593660b24" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis, from the &lt;a href=&quot;../../modules/classes#module-sklearn.discriminant_analysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis&lt;/code&gt;&lt;/a&gt; module, and Neighborhood Components Analysis, from the &lt;a href=&quot;../../modules/classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; module, are supervised dimensionality reduction method, i.e. they make use of the provided labels, contrary to other methods.</source>
          <target state="translated">線形判別分析、&lt;a href=&quot;../../modules/classes#module-sklearn.discriminant_analysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis&lt;/code&gt; の&lt;/a&gt;モジュール、および近所コンポーネント分析は、から&lt;a href=&quot;../../modules/classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; の&lt;/a&gt;モジュール、すなわち、それらは他の方法に提供されるラベルの使用、逆を行い、教師次元削減方法です。</target>
        </trans-unit>
        <trans-unit id="fe99070400d8a366d4438afb34b3817ed643e76c" translate="yes" xml:space="preserve">
          <source>Linear Model trained with L1 prior as regularizer (aka the Lasso)</source>
          <target state="translated">正則化器としてL1先行で訓練された線形モデル(別名:Lasso</target>
        </trans-unit>
        <trans-unit id="b4819d272193c458d14d3c2a02b6439edb693339" translate="yes" xml:space="preserve">
          <source>Linear Regression Example</source>
          <target state="translated">線形回帰の例</target>
        </trans-unit>
        <trans-unit id="85494d31f5cd31cf05c6e37284f8e968283c0002" translate="yes" xml:space="preserve">
          <source>Linear SVC is not a probabilistic classifier by default but it has a built-in calibration option enabled in this example (&lt;code&gt;probability=True&lt;/code&gt;).</source>
          <target state="translated">線形SVCは、デフォルトでは確率的分類器ではありませんが、この例では、組み込みのキャリブレーションオプションが有効になっています（ &lt;code&gt;probability=True&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="e97d7a71e4408e1f570cb8d2ee92b68f661724af" translate="yes" xml:space="preserve">
          <source>Linear SVMs</source>
          <target state="translated">線形SVM</target>
        </trans-unit>
        <trans-unit id="73af0f0fe2656e7c704e9d2782f72d490054905e" translate="yes" xml:space="preserve">
          <source>Linear Sum - A n-dimensional vector holding the sum of all samples</source>
          <target state="translated">線形和-すべてのサンプルの和を保持するn次元ベクトル.</target>
        </trans-unit>
        <trans-unit id="1cd7978197df4491cb006d18687f0ce787689e06" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">線形サポートベクトル分類（&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;）は、最大境界法（Niculescu-MizilとCaruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]を&lt;/a&gt;比較）の典型であるRandomForestClassifierとしてさらにシグモイド曲線を示し、決定境界に近いハードサンプルに焦点を当てます（サポートベクター）。</target>
        </trans-unit>
        <trans-unit id="aa48807728eba9fefe9fd82435afe3ea7ac48643" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;1&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">線形サポートベクター分類（&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;）は、RandomForestClassifierとしてさらに多くのシグモイド曲線を示します。これは、決定境界に近いハードサンプルに焦点を当てる最大マージンメソッド（Niculescu-MizilとCaruana &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;1を&lt;/a&gt;比較）で一般的です（サポートベクトル）。</target>
        </trans-unit>
        <trans-unit id="88aaad048f30298d89bc0519c1e6f4cfbb7c20ea" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification.</source>
          <target state="translated">線形サポートベクター分類。</target>
        </trans-unit>
        <trans-unit id="4669e7bb12c975a34b6d592ccfe985850a9e31eb" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Regression.</source>
          <target state="translated">線形支持ベクトル回帰。</target>
        </trans-unit>
        <trans-unit id="299f04ebeb7ad11bec6b5498c6b639ccade4023d" translate="yes" xml:space="preserve">
          <source>Linear and Quadratic Discriminant Analysis with covariance ellipsoid</source>
          <target state="translated">共分散楕円体を用いた線形・二次判別分析</target>
        </trans-unit>
        <trans-unit id="f5530856b3075f323ba02b6ac9992d5aae8eee6e" translate="yes" xml:space="preserve">
          <source>Linear classifiers</source>
          <target state="translated">線形分類器</target>
        </trans-unit>
        <trans-unit id="c0463594ed874e4d015c682e8a6395a05e3fbd8b" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</source>
          <target state="translated">線形分類器(SVM、ロジスティック回帰、a.o.)をSGDトレーニングで使用。</target>
        </trans-unit>
        <trans-unit id="9a198808a208105876aff5b3459a1743d02b9e6c" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, etc.) with SGD training.</source>
          <target state="translated">線形分類器(SVM、ロジスティック回帰など)とSGDトレーニング。</target>
        </trans-unit>
        <trans-unit id="fa82faf2d530b479b3e87ec39c80fc313d729e93" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space.</source>
          <target state="translated">中心データの特異値分解を用いた線形次元削減、最も重要な特異ベクトルのみを保持し、データを低次元空間に投影します。</target>
        </trans-unit>
        <trans-unit id="9db7130b75e27bc47e2764b6ec7d1ce03bb7f92f" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.</source>
          <target state="translated">データを特異値分解して低次元空間に投影することで線形次元削減を実現</target>
        </trans-unit>
        <trans-unit id="6451e9b1aa60579e3e911ed4aef57d459ed7cbf1" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.</source>
          <target state="translated">データを特異値分解して低次元空間に投影することで線形次元削減を行います。SVDを適用する前に、入力データは中央揃えされますが、各特徴についてはスケーリングされません。</target>
        </trans-unit>
        <trans-unit id="c7a4096bc58f13af1ea98cce08fd73c62fb91596" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data, keeping only the most significant singular vectors to project the data to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.</source>
          <target state="translated">データの特異値分解を使用して線形次元を削減し,最も有意な特異ベクトルのみを保持して,データを低次元空間に投影します.入力データは,SVDを適用する前に,各特徴に対して中央揃えされますが,スケーリングは行われません.</target>
        </trans-unit>
        <trans-unit id="212b70af3cba5b501136f7c4f46821ce9f54ad31" translate="yes" xml:space="preserve">
          <source>Linear kernel (&lt;code&gt;kernel = 'linear'&lt;/code&gt;)</source>
          <target state="translated">線形カーネル（ &lt;code&gt;kernel = 'linear'&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="1196f0388e6edcd3bda2236746717385556b159a" translate="yes" xml:space="preserve">
          <source>Linear least squares with l2 regularization.</source>
          <target state="translated">l2正則化を用いた線形最小二乗法。</target>
        </trans-unit>
        <trans-unit id="0663410286eb390a6a91a4885ecdb0348930bc50" translate="yes" xml:space="preserve">
          <source>Linear model fitted by minimizing a regularized empirical loss with SGD</source>
          <target state="translated">正規化された経験的損失をSGDで最小化してフィットした線形モデル</target>
        </trans-unit>
        <trans-unit id="8d6556caff9af87efd1e0ccffe2463b6a45189f7" translate="yes" xml:space="preserve">
          <source>Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure.</source>
          <target state="translated">多くの回帰子のそれぞれの個別効果を検定するための線形モデル.これは,特徴選択手順で使用されるスコアリング関数であり,自由に立ち上がる特徴選択手順ではない.</target>
        </trans-unit>
        <trans-unit id="8f05719b5c26a33c08ae54e21caa15631a4bbbf1" translate="yes" xml:space="preserve">
          <source>Linear model: from regression to sparsity</source>
          <target state="translated">線形モデル:回帰から疎分散へ</target>
        </trans-unit>
        <trans-unit id="0b1d2caa3dbccbb7fc7a7a3c7fac23bacc286b81" translate="yes" xml:space="preserve">
          <source>Linear models with regularization</source>
          <target state="translated">正則化された線形モデル</target>
        </trans-unit>
        <trans-unit id="46d0fcb8f937066fa349aa94a3c710921424dd9b" translate="yes" xml:space="preserve">
          <source>Linear models with sparse coefficients</source>
          <target state="translated">疎な係数を持つ線形モデル</target>
        </trans-unit>
        <trans-unit id="2c94cc16a66b49675f2acef482a0fbcd40d606ee" translate="yes" xml:space="preserve">
          <source>Linear models: \(y = X\beta + \epsilon\)</source>
          <target state="translated">Linear models.\Y=X\beta+\epsilon)</target>
        </trans-unit>
        <trans-unit id="b501f602569674c31fc384f2cd7a29bcf6c1ce1f" translate="yes" xml:space="preserve">
          <source>Linear regression</source>
          <target state="translated">線形回帰</target>
        </trans-unit>
        <trans-unit id="d8f88b232d41c327138bbda59458fa5fc4086fff" translate="yes" xml:space="preserve">
          <source>Linear regression model that is robust to outliers.</source>
          <target state="translated">外れ値にロバストな線形回帰モデル。</target>
        </trans-unit>
        <trans-unit id="597ff76dcbb7bc322f194ba001977a736c193c2d" translate="yes" xml:space="preserve">
          <source>Linear regression with combined L1 and L2 priors as regularizer.</source>
          <target state="translated">正則化器としてL1とL2のプライオを組み合わせた線形回帰.</target>
        </trans-unit>
        <trans-unit id="0a2d386e0774637a1788b00b4abdb8b2c6c38c74" translate="yes" xml:space="preserve">
          <source>Linear ridge regression.</source>
          <target state="translated">線形リッジ回帰。</target>
        </trans-unit>
        <trans-unit id="1dc397a187bb8fae755995aa069f79a9d565d581" translate="yes" xml:space="preserve">
          <source>Linear support vector classification.</source>
          <target state="translated">線形サポートベクター分類。</target>
        </trans-unit>
        <trans-unit id="5959458e20a73276c12d61f1d64604d66daab52d" translate="yes" xml:space="preserve">
          <source>LinearRegression</source>
          <target state="translated">LinearRegression</target>
        </trans-unit>
        <trans-unit id="6ff9599a07718d87fb124205a5a88e8262436582" translate="yes" xml:space="preserve">
          <source>LinearRegression fits a linear model with coefficients w = (w1, &amp;hellip;, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.</source>
          <target state="translated">LinearRegressionは、係数w =（w1、&amp;hellip;、wp）の線形モデルに適合し、データセット内の観測されたターゲットと線形近似によって予測されたターゲットの間の残差平方和を最小化します。</target>
        </trans-unit>
        <trans-unit id="daf34c391f43051e2982c2bbeba34bf1a7727132" translate="yes" xml:space="preserve">
          <source>List containing the artists for the annotation boxes making up the tree.</source>
          <target state="translated">ツリーを構成する注釈ボックスのアーティストを含むリスト。</target>
        </trans-unit>
        <trans-unit id="c7ed3fbb6680836b95c3db482cfaf054c37a8419" translate="yes" xml:space="preserve">
          <source>List containing train-test split of inputs.</source>
          <target state="translated">入力の訓練-テスト分割を含むリスト。</target>
        </trans-unit>
        <trans-unit id="7946c78611ea79ca25491c94f60dac5182c68016" translate="yes" xml:space="preserve">
          <source>List of (name, class), where &lt;code&gt;name&lt;/code&gt; is the class name as string and &lt;code&gt;class&lt;/code&gt; is the actuall type of the class.</source>
          <target state="translated">（名前、クラス）の一覧、 &lt;code&gt;name&lt;/code&gt; 文字列としてクラス名である &lt;code&gt;class&lt;/code&gt; 、クラスのactuallタイプです。</target>
        </trans-unit>
        <trans-unit id="01f72260e79a828ac37c6e1b27f0158a5c017639" translate="yes" xml:space="preserve">
          <source>List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.</source>
          <target state="translated">連鎖 さ れている (名前 ・ 変換)タ プル (はめ込み ・ 変換を実装した)タ プルの リ ス ト (名前 ・ 変換)を、 連鎖 さ れてい る 順に並べたもので、 最後のオブジ ェ ク ト がエ ク ス ト レー タ 。</target>
        </trans-unit>
        <trans-unit id="da3552a00ac25869a883c68bd3a0b9b483a759ac" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, column(s)) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="translated">データのサブセットに適用される変換器オブジェクトを指定する(名前、変換器、列)タプルのリスト。</target>
        </trans-unit>
        <trans-unit id="d6a17bcffaea2ea2f18b9842614672499d0a26c6" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, columns) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="translated">データのサブセットに適用する変換器オブジェクトを指定する(名前、変換器、列)タプルのリスト。</target>
        </trans-unit>
        <trans-unit id="9ce9067b559ab6542ebc584f224960b4d8e01fb3" translate="yes" xml:space="preserve">
          <source>List of &lt;code&gt;n_features&lt;/code&gt;-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">&lt;code&gt;n_features&lt;/code&gt; 次元データポイントのリスト。各行は単一のデータポイントに対応します。</target>
        </trans-unit>
        <trans-unit id="5f38bb9ffb369276ed25fb7c04fb0e0e029823d8" translate="yes" xml:space="preserve">
          <source>List of all the classes that can possibly appear in the y vector.</source>
          <target state="translated">y ベクトルに現れる可能性のあるすべてのクラスのリスト.</target>
        </trans-unit>
        <trans-unit id="7cd6d854280958549421b40e7d622e1782f63df4" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If &lt;code&gt;None&lt;/code&gt; alphas are set automatically</source>
          <target state="translated">モデルを計算するアルファのリスト。 &lt;code&gt;None&lt;/code&gt; アルファが自動的に設定される場合</target>
        </trans-unit>
        <trans-unit id="917b5a956108e84a4893edaf20f4507c6507d0e2" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically</source>
          <target state="translated">モデルを計算するアルファのリスト。Noneの場合、アルファは自動的に設定されます</target>
        </trans-unit>
        <trans-unit id="a5422f4e0e412f7e68186f61afde0578ebd8abd7" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically.</source>
          <target state="translated">モデルを計算するアルファのリスト。Noneの場合、アルファは自動的に設定されます。</target>
        </trans-unit>
        <trans-unit id="d057f35a68cef6d291f5ea686ce0f4438a6a2951" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If not provided, set automatically.</source>
          <target state="translated">モデルを計算するアルファのリスト。省略した場合は自動的に設定されます。</target>
        </trans-unit>
        <trans-unit id="fb8d4641f5ca2701f801733b19cf7bb77974f371" translate="yes" xml:space="preserve">
          <source>List of arrays of terms.</source>
          <target state="translated">用語の配列のリスト。</target>
        </trans-unit>
        <trans-unit id="6ecedd8bbbc6137125014e8bb7a429cbcef11be8" translate="yes" xml:space="preserve">
          <source>List of built-in kernels.</source>
          <target state="translated">組み込みカーネルのリスト。</target>
        </trans-unit>
        <trans-unit id="36c7ba17f19f78b4b0b98a1a27cecbfd22dc65e4" translate="yes" xml:space="preserve">
          <source>List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept. For &lt;code&gt;multiclass='multinomial'&lt;/code&gt;, the shape is (n_classes, n_cs, n_features) or (n_classes, n_cs, n_features + 1).</source>
          <target state="translated">ロジスティック回帰モデルの係数のリスト。fit_interceptがTrueに設定されている場合、2番目の次元はn_features + 1となり、最後のアイテムが切片を表します。用 &lt;code&gt;multiclass='multinomial'&lt;/code&gt; 、形状（n_classes、n_cs、n_features）または（n_classes、n_cs、n_features + 1）です。</target>
        </trans-unit>
        <trans-unit id="d090495d2c127f32b67f3946c4bdcba721a89fcd" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If &lt;code&gt;None&lt;/code&gt; is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">マトリックスにインデックスを付けるラベルのリスト。これは、ラベルのサブセットを並べ替えたり選択したりするために使用できます。 &lt;code&gt;None&lt;/code&gt; が指定されている場合、 &lt;code&gt;y_true&lt;/code&gt; または &lt;code&gt;y_pred&lt;/code&gt; に少なくとも1回出現するものは、ソートされた順序で使用されます。</target>
        </trans-unit>
        <trans-unit id="568d5fc554d78a8c3f420990686843b1d52522c9" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If none is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">行列にインデックスを付けるラベルのリスト。これを使用して、ラベルのサブセットを並べ替えたり選択したりできます。何も指定されていない場合、 &lt;code&gt;y_true&lt;/code&gt; または &lt;code&gt;y_pred&lt;/code&gt; に少なくとも1回現れるものは、ソートされた順序で使用されます。</target>
        </trans-unit>
        <trans-unit id="4904457db6e3ad315971a386c35727cdd591b70f" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to select a subset of labels. If None, all labels that appear at least once in &lt;code&gt;y1&lt;/code&gt; or &lt;code&gt;y2&lt;/code&gt; are used.</source>
          <target state="translated">行列にインデックスを付けるラベルのリスト。これは、ラベルのサブセットを選択するために使用できます。Noneの場合、 &lt;code&gt;y1&lt;/code&gt; または &lt;code&gt;y2&lt;/code&gt; に少なくとも1回出現するすべてのラベルが使用されます。</target>
        </trans-unit>
        <trans-unit id="b841f355bd90388eac15a6584a26192e6c900c97" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">n_features次元のデータ点のリスト。各行は1つのデータポイントに対応します。</target>
        </trans-unit>
        <trans-unit id="af1051d092002bc2f98d27cb1ada3b5cc2dacea1" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single query.</source>
          <target state="translated">n_features-次元データポイントのリスト。各行は1つのクエリに対応します。</target>
        </trans-unit>
        <trans-unit id="85e7a2833a6b5505d28e95f0c1116dee51aa01e1" translate="yes" xml:space="preserve">
          <source>List of objects to ensure sliceability.</source>
          <target state="translated">スライス性を確保するためのオブジェクトのリスト。</target>
        </trans-unit>
        <trans-unit id="5538dc428bf1dd702d4666daf2c6801367c4f065" translate="yes" xml:space="preserve">
          <source>List of sample weights attached to the data X.</source>
          <target state="translated">データXに添付されているサンプル重みのリスト。</target>
        </trans-unit>
        <trans-unit id="af4d88e1f955adfe14752a1cab15db410dc25046" translate="yes" xml:space="preserve">
          <source>List of samples.</source>
          <target state="translated">サンプルの一覧です。</target>
        </trans-unit>
        <trans-unit id="9fa149a90ccae2cfe066dfb859bf8a7c95ef01ca" translate="yes" xml:space="preserve">
          <source>List of transformer objects to be applied to the data. The first half of each tuple is the name of the transformer.</source>
          <target state="translated">データに適用されるトランスフォーマオブジェクトのリスト。各タプルの前半はトランスフォーマの名前です。</target>
        </trans-unit>
        <trans-unit id="f2f499a9d9cf5fba3b5aa16bff4e7ad9f538a51f" translate="yes" xml:space="preserve">
          <source>List of values for the regularization parameter or integer specifying the number of regularization parameters that should be used. In this case, the parameters will be chosen in a logarithmic scale between 1e-4 and 1e4.</source>
          <target state="translated">正則化パラメータの値のリスト、または使用するべき正則化パラメータの数を指定する整数。この場合、パラメータは1e-4から1e-4の間の対数スケールで選択されます。</target>
        </trans-unit>
        <trans-unit id="d742bd356ab53d1131907c9ca41e9f89956bc677" translate="yes" xml:space="preserve">
          <source>List of weighting type to calculate the score. None means no weighted; &amp;ldquo;linear&amp;rdquo; means linear weighted; &amp;ldquo;quadratic&amp;rdquo; means quadratic weighted.</source>
          <target state="translated">スコアを計算するための重み付けタイプのリスト。なしは重み付けなしを意味します。「線形」とは、線形加重を意味します。「二次」とは、二次加重を意味します。</target>
        </trans-unit>
        <trans-unit id="ccaadae3fd2b8d525242b8298319bebc15b1d7f7" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;lsquo;08. Eighth IEEE International Conference on.</source>
          <target state="translated">Liu、Fei Tony、Ting、Kai Ming、Zhou、Zhi-Hua。「隔離林」データマイニング、2008。ICDM'08。8番目のIEEE国際会議。</target>
        </trans-unit>
        <trans-unit id="ef6455b6e1e2fee9a705bb5d5a6878e2089d032f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;rsquo;08. Eighth IEEE International Conference on.</source>
          <target state="translated">Liu、Fei Tony、Ting、Kai Ming、Zhou、Zhi-Hua。「孤立した森。」データマイニング、2008年。ICDM'08。に関する第8回IEEE国際会議。</target>
        </trans-unit>
        <trans-unit id="8d858831be3c025b5261ad0994fdd43e36106d2f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation-based anomaly detection.&amp;rdquo; ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 3.</source>
          <target state="translated">Liu、Fei Tony、Ting、Kai Ming、Zhou、Zhi-Hua。「分離ベースの異常検出。」データからの知識発見に関するACMトランザクション（TKDD）6.1（2012）：3。</target>
        </trans-unit>
        <trans-unit id="d7cbf66ae3940637cf4feeef412f2113fd5144b3" translate="yes" xml:space="preserve">
          <source>Load Data and Train a SVC</source>
          <target state="translated">データのロードとSVCのトレーニング</target>
        </trans-unit>
        <trans-unit id="5a977facf077b843c15be4adf2b27656870bbc8b" translate="yes" xml:space="preserve">
          <source>Load Data and train model</source>
          <target state="translated">データのロードとモデルのトレーニング</target>
        </trans-unit>
        <trans-unit id="bc1c89a3655919cbe107b23bf70fdaef2d59b7e4" translate="yes" xml:space="preserve">
          <source>Load a datasets as downloaded from &lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;からダウンロードしたデータセットをロードします</target>
        </trans-unit>
        <trans-unit id="10fbb828ccf5ef978744b601a27eefff85b82acd" translate="yes" xml:space="preserve">
          <source>Load and return the boston house-prices dataset (regression).</source>
          <target state="translated">ボストンの住宅価格データセット(回帰)をロードして返す。</target>
        </trans-unit>
        <trans-unit id="f0b03288037dddab02ba1bf0d814f5cc8cf63088" translate="yes" xml:space="preserve">
          <source>Load and return the breast cancer wisconsin dataset (classification).</source>
          <target state="translated">乳がんウィスコンシンのデータセット(分類)をロードして返す。</target>
        </trans-unit>
        <trans-unit id="fb9c782009d54032f572c4b8eb05f6ff3c69b6ee" translate="yes" xml:space="preserve">
          <source>Load and return the diabetes dataset (regression).</source>
          <target state="translated">糖尿病データセット(回帰)をロードして返す。</target>
        </trans-unit>
        <trans-unit id="5ee0c3f160bd1db558fab50ff07fd2d60e875939" translate="yes" xml:space="preserve">
          <source>Load and return the digits dataset (classification).</source>
          <target state="translated">桁のデータセット(分類)をロードして返します。</target>
        </trans-unit>
        <trans-unit id="91627f9a236f04bf8e67f696e6012e55dde096ca" translate="yes" xml:space="preserve">
          <source>Load and return the iris dataset (classification).</source>
          <target state="translated">虹彩データセット(分類)をロードして返す。</target>
        </trans-unit>
        <trans-unit id="08308ecd69078eb0533ddcbcb38611925dd58ae7" translate="yes" xml:space="preserve">
          <source>Load and return the linnerud dataset (multivariate regression).</source>
          <target state="translated">linnerudデータセット(多変量回帰)をロードして返します。</target>
        </trans-unit>
        <trans-unit id="24c66578782cdc888ab71f9a25a06214508e7234" translate="yes" xml:space="preserve">
          <source>Load and return the physical excercise linnerud dataset.</source>
          <target state="translated">フィジカルエクササイズのlinnerudデータセットをロードして返します。</target>
        </trans-unit>
        <trans-unit id="0a61d81b3e38cd33952ad8e4ab4da4e0afb0ac23" translate="yes" xml:space="preserve">
          <source>Load and return the wine dataset (classification).</source>
          <target state="translated">ワインのデータセット(分類)をロードして返す。</target>
        </trans-unit>
        <trans-unit id="34956b05e013a2f3d8d5817783733a9d69ea9a29" translate="yes" xml:space="preserve">
          <source>Load data from the training set</source>
          <target state="translated">トレーニングセットからデータをロード</target>
        </trans-unit>
        <trans-unit id="907ca9fec180a2f563a6eb0b2c208dd89483dfe5" translate="yes" xml:space="preserve">
          <source>Load dataset from multiple files in SVMlight format</source>
          <target state="translated">SVMlight形式で複数のファイルからデータセットを読み込む</target>
        </trans-unit>
        <trans-unit id="e0287d019fcfe4320ef71958ec3623d393a07d68" translate="yes" xml:space="preserve">
          <source>Load datasets in the svmlight / libsvm format into sparse CSR matrix</source>
          <target state="translated">svmlight/libsvm 形式のデータセットを疎な CSR マトリクスにロードします。</target>
        </trans-unit>
        <trans-unit id="15df99bbc404778e529956fb3833b1f8b300577d" translate="yes" xml:space="preserve">
          <source>Load sample images for image manipulation.</source>
          <target state="translated">画像操作用のサンプル画像を読み込みます。</target>
        </trans-unit>
        <trans-unit id="93b606a5680687306536f14272c219f02caf9a74" translate="yes" xml:space="preserve">
          <source>Load text files with categories as subfolder names.</source>
          <target state="translated">カテゴリをサブフォルダ名にしてテキストファイルを読み込みます。</target>
        </trans-unit>
        <trans-unit id="ada1ba97e9c53b7b56715b1a2824f0ae676a78c6" translate="yes" xml:space="preserve">
          <source>Load the 20 newsgroups dataset and vectorize it into token counts (classification).</source>
          <target state="translated">ニュースグループ20個のデータセットをロードし、トークンカウント(分類)にベクトル化します。</target>
        </trans-unit>
        <trans-unit id="4f7a062fa00aaafd76d451b474abbba6455b18d1" translate="yes" xml:space="preserve">
          <source>Load the California housing dataset (regression).</source>
          <target state="translated">カリフォルニア州の住宅データセットをロードする(リグレッション)。</target>
        </trans-unit>
        <trans-unit id="d369acbb02d6ae84bdcebcaf52c16540c4d5f177" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).</source>
          <target state="translated">野生の顔(Labeled Faces in the Wild (LFW)のペアデータセット(分類)をロードします。</target>
        </trans-unit>
        <trans-unit id="fdf290fe8f8a97ef39a92df3e1f665ba9f5137b7" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) people dataset (classification).</source>
          <target state="translated">Labeled Faces in the Wild (LFW)人物データセットを読み込む(分類)。</target>
        </trans-unit>
        <trans-unit id="c1d9dfefbd2137b268a0489f71dee7b704510f30" translate="yes" xml:space="preserve">
          <source>Load the Olivetti faces data-set from AT&amp;amp;T (classification).</source>
          <target state="translated">AT＆T（分類）からOlivetti Facesデータセットを読み込みます。</target>
        </trans-unit>
        <trans-unit id="2772bcfa51d7f467cdc1ff56dd2a38098daf99c8" translate="yes" xml:space="preserve">
          <source>Load the RCV1 multilabel dataset (classification).</source>
          <target state="translated">RCV1マルチラベルデータセット(分類)をロードします。</target>
        </trans-unit>
        <trans-unit id="b34ac8eb475e3d0c92e532ea1f16cafea2826dba" translate="yes" xml:space="preserve">
          <source>Load the covertype dataset (classification).</source>
          <target state="translated">カバータイプのデータセット(分類)をロードします。</target>
        </trans-unit>
        <trans-unit id="ad3fd711e27424bfdcf7677e93b2ded911f0bbc0" translate="yes" xml:space="preserve">
          <source>Load the data</source>
          <target state="translated">データの読み込み</target>
        </trans-unit>
        <trans-unit id="54322fa6d75ea033036ee5315e01f5a9e265e0ca" translate="yes" xml:space="preserve">
          <source>Load the filenames and data from the 20 newsgroups dataset (classification).</source>
          <target state="translated">ニュースグループ20のデータセット(分類)からファイル名とデータを読み込みます。</target>
        </trans-unit>
        <trans-unit id="ae3c786b5593f01e176137f6a4960d769f8b9d22" translate="yes" xml:space="preserve">
          <source>Load the kddcup99 dataset (classification).</source>
          <target state="translated">kddcup99データセット(分類)をロードします。</target>
        </trans-unit>
        <trans-unit id="820329ef76c355bc57213e87e726caebf3ec8e17" translate="yes" xml:space="preserve">
          <source>Load the numpy array of a single sample image</source>
          <target state="translated">1つのサンプル画像のnumpy配列を読み込みます.</target>
        </trans-unit>
        <trans-unit id="6565057c8bbe701655d34466bc255155c3ea2c6e" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et.</source>
          <target state="translated">Phillips らの種の分布データセットのためのローダー。</target>
        </trans-unit>
        <trans-unit id="00912c83de18e685a34ddbd42e1354c697eb14e0" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et. al. (2006)</source>
          <target state="translated">Phillips et al.(2006)</target>
        </trans-unit>
        <trans-unit id="4f514b04ed6b877534da140af8e12cab5016f713" translate="yes" xml:space="preserve">
          <source>Loaders</source>
          <target state="translated">Loaders</target>
        </trans-unit>
        <trans-unit id="1d603b233f1badee343cd4d051b0c74346bf8ab5" translate="yes" xml:space="preserve">
          <source>Loading an example dataset</source>
          <target state="translated">サンプルデータセットの読み込み</target>
        </trans-unit>
        <trans-unit id="caf6cb93de1911a37a3c4f9c173bcddd3283525a" translate="yes" xml:space="preserve">
          <source>Loading datasets, basic feature extraction and target definitions</source>
          <target state="translated">データセットの読み込み、基本的な特徴抽出とターゲット定義</target>
        </trans-unit>
        <trans-unit id="afb9453c6f5c0750a61be0390918061037ab3605" translate="yes" xml:space="preserve">
          <source>Loading from external datasets</source>
          <target state="translated">外部データセットからの読み込み</target>
        </trans-unit>
        <trans-unit id="b4240e57d982043f1f905f33f107714b1056ff0f" translate="yes" xml:space="preserve">
          <source>Loading the 20 newsgroups dataset</source>
          <target state="translated">20個のニュースグループデータセットの読み込み</target>
        </trans-unit>
        <trans-unit id="bf453b7e00694519c6d048cddce89c9acdc80f61" translate="yes" xml:space="preserve">
          <source>Loads both, &lt;code&gt;china&lt;/code&gt; and &lt;code&gt;flower&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;china&lt;/code&gt; と &lt;code&gt;flower&lt;/code&gt; 両方をロードします。</target>
        </trans-unit>
        <trans-unit id="5a8b86a7fef7215f7de926bc65cb224b10c3ccba" translate="yes" xml:space="preserve">
          <source>Locally Linear Embedding</source>
          <target state="translated">局所的に線形埋め込み</target>
        </trans-unit>
        <trans-unit id="f71746cee5cf3673e7e527aaea93ab0ac960ab66" translate="yes" xml:space="preserve">
          <source>Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.</source>
          <target state="translated">局所的線形埋め込み(LLE)は、局所的な近傍内の距離を保持するデータの低次元投影を求めます。これは、最高の非線形埋め込みを見つけるために、グローバルに比較される一連の局所主成分分析と考えることができます。</target>
        </trans-unit>
        <trans-unit id="ba721026e6725be51f569c81e87377b42c664dd5" translate="yes" xml:space="preserve">
          <source>Locally linear embedding can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">ローカル線形埋め込みは、ローカルで関数&lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt;またはオブジェクト指向の対応する&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; を使用して&lt;/a&gt;実行できます。</target>
        </trans-unit>
        <trans-unit id="120996393a2755aae459a0342f6a159574a0420b" translate="yes" xml:space="preserve">
          <source>Log likelihood of the Gaussian mixture given X.</source>
          <target state="translated">与えられたXが与えられたガウス混合物の対数尤度.</target>
        </trans-unit>
        <trans-unit id="10dac5cbd2ef695e498b42bff8cc166d8d6c8a26" translate="yes" xml:space="preserve">
          <source>Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).</source>
          <target state="translated">ログロスはp=0またはp=1では定義されていないので、確率はmax(eps,min(1-eps,p))にクリップされます。</target>
        </trans-unit>
        <trans-unit id="8742f15984971d3e598576d7cde59958d4df18a1" translate="yes" xml:space="preserve">
          <source>Log loss, aka logistic loss or cross-entropy loss.</source>
          <target state="translated">ログロス、別名ロジスティックロスまたはクロスエントロピーロス。</target>
        </trans-unit>
        <trans-unit id="3332ed47adb99d618a3191081bd1b56f7df887fc" translate="yes" xml:space="preserve">
          <source>Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs (&lt;code&gt;predict_proba&lt;/code&gt;) of a classifier instead of its discrete predictions.</source>
          <target state="translated">対数損失は、ロジスティック回帰損失またはクロスエントロピー損失とも呼ばれ、確率推定値で定義されます。これは一般に（多項）ロジスティック回帰とニューラルネットワーク、および期待値最大化のいくつかのバリアントで使用され、離散予測の代わりに分類器の確率出力（ &lt;code&gt;predict_proba&lt;/code&gt; ）を評価するために使用できます。</target>
        </trans-unit>
        <trans-unit id="f8ceba0d5dd7df5e53e4d9ba0bfe4881369ef7f1" translate="yes" xml:space="preserve">
          <source>Log of probability estimates.</source>
          <target state="translated">確率推定値の対数。</target>
        </trans-unit>
        <trans-unit id="b2dede1f561914a3bc83cf7a3f85dcff6bab8c76" translate="yes" xml:space="preserve">
          <source>Log probabilities of each data point in X.</source>
          <target state="translated">Xの各データ点の対数確率。</target>
        </trans-unit>
        <trans-unit id="ce21bba36fd356086ab08edfbf5461d606fc0046" translate="yes" xml:space="preserve">
          <source>Log probability of each class (smoothed).</source>
          <target state="translated">各クラスの対数確率(平滑化)。</target>
        </trans-unit>
        <trans-unit id="10521a3daec9ae1f69d9eb092c8ffd785e7a6414" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model</source>
          <target state="translated">現在のモデルでの各サンプルの対数尤度</target>
        </trans-unit>
        <trans-unit id="f4a66282780599e98e06ef51f0d5990ef29ec975" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model.</source>
          <target state="translated">現在のモデルでの各サンプルの対数尤度。</target>
        </trans-unit>
        <trans-unit id="9c1e8dc95e554810186fcd38490e4f1fe6e53c32" translate="yes" xml:space="preserve">
          <source>Log-likelihood score on left-out data across folds.</source>
          <target state="translated">ひだをまたいだ左端のデータでの対数尤度スコア。</target>
        </trans-unit>
        <trans-unit id="af6fc4d4c535e2fcc7787b2d2b354e641a5cdf07" translate="yes" xml:space="preserve">
          <source>Log-marginal likelihood of theta for training data.</source>
          <target state="translated">学習データのシータの対数倍尤度。</target>
        </trans-unit>
        <trans-unit id="a79f6e0f430c7ecad68ae2bba39688851de03cfd" translate="yes" xml:space="preserve">
          <source>Log: Logistic Regression.</source>
          <target state="translated">Log.ロジスティック回帰。</target>
        </trans-unit>
        <trans-unit id="710d6654ab013b68e75b864460870bdb608ee833" translate="yes" xml:space="preserve">
          <source>Log: equivalent to Logistic Regression. \(L(y_i, f(x_i)) = \log(1 + \exp (-y_i f(x_i)))\).</source>
          <target state="translated">Log:ロジスティック回帰に相当する。\(L(y_i,f(x_i))=\log(1+\exp (-y_i f(x_i)))</target>
        </trans-unit>
        <trans-unit id="667a374e42016ea0491009bae949bbc3eb5a98fe" translate="yes" xml:space="preserve">
          <source>Logistic Regression (aka logit, MaxEnt) classifier.</source>
          <target state="translated">ロジスティック回帰(別名logit、MaxEnt)の分類器。</target>
        </trans-unit>
        <trans-unit id="7553fecbacc2ab6c754b732dd2a40625b016efa3" translate="yes" xml:space="preserve">
          <source>Logistic Regression 3-class Classifier</source>
          <target state="translated">ロジスティック回帰3クラス分類器</target>
        </trans-unit>
        <trans-unit id="67b9d1bed8ce4778bb74ee8f32cf37a9f88e56b4" translate="yes" xml:space="preserve">
          <source>Logistic Regression CV (aka logit, MaxEnt) classifier.</source>
          <target state="translated">ロジスティック回帰 CV (別名 logit,MaxEnt)分類器.</target>
        </trans-unit>
        <trans-unit id="4c4251ffdad99c44ab6ab03dc44e767ed73a387b" translate="yes" xml:space="preserve">
          <source>Logistic function</source>
          <target state="translated">ロジスティック関数</target>
        </trans-unit>
        <trans-unit id="90723c3f54f2127002d5e8087f3fd75704debc54" translate="yes" xml:space="preserve">
          <source>Logistic regression is implemented in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional \(\ell_1\), \(\ell_2\) or Elastic-Net regularization.</source>
          <target state="translated">ロジスティック回帰が実装されて&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;。この実装は、オプションの\（\ ell_1 \）、\（\ ell_2 \）、またはElastic-Net正則化を使用して、バイナリ、One-vs-Rest、または多項ロジスティック回帰に適合できます。</target>
        </trans-unit>
        <trans-unit id="2c0f1438d10823ae208574adad9979316ecf1f7d" translate="yes" xml:space="preserve">
          <source>Logistic regression on raw pixel values is presented for comparison. The example shows that the features extracted by the BernoulliRBM help improve the classification accuracy.</source>
          <target state="translated">比較のために、生の画素値に対するロジスティック回帰を提示します。この例では、BernoulliRBMで抽出された特徴量が分類精度の向上に役立つことが示されています。</target>
        </trans-unit>
        <trans-unit id="f05fe21aed88fc82a5a0513559ae877673e205fc" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation</source>
          <target state="translated">クロスバリデーションを組み込んだロジスティック回帰</target>
        </trans-unit>
        <trans-unit id="96304f46c8b5deeee00fad5a320967f13f13e69f" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation.</source>
          <target state="translated">クロスバリデーションを組み込んだロジスティック回帰</target>
        </trans-unit>
        <trans-unit id="d3b2957f5500f497ec4678d49dfe4396dcf43781" translate="yes" xml:space="preserve">
          <source>Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;logistic function&lt;/a&gt;.</source>
          <target state="translated">ロジスティック回帰は、その名前にもかかわらず、回帰ではなく分類の線形モデルです。文献では、ロジスティック回帰は、ロジット回帰、最大エントロピー分類（MaxEnt）、または対数線形分類子としても知られています。このモデルでは、単一の試行の可能な結果を​​表す確率は、&lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;ロジスティック関数&lt;/a&gt;を使用してモデル化されます。</target>
        </trans-unit>
        <trans-unit id="8e225015f4826ba9beac040479565ae8cd20d1cd" translate="yes" xml:space="preserve">
          <source>Logistic regression.</source>
          <target state="translated">ロジスティック回帰。</target>
        </trans-unit>
        <trans-unit id="9cb16d85d5ce6d29fbfeaf0784f0b2e2c61ca591" translate="yes" xml:space="preserve">
          <source>LogisticRegression</source>
          <target state="translated">LogisticRegression</target>
        </trans-unit>
        <trans-unit id="de456a9443564fc60f026f7b3757765c6c521491" translate="yes" xml:space="preserve">
          <source>LogisticRegression returns well calibrated predictions as it directly optimizes log-loss. In contrast, the other methods return biased probabilities, with different biases per method:</source>
          <target state="translated">LogisticRegressionは、対数損失を直接最適化するので、よくキャリブレーションされた予測値を返します。対照的に、他の手法は、手法ごとに異なるバイアスを持つ偏った確率を返します。</target>
        </trans-unit>
        <trans-unit id="25965de326ab0450ae299a566621b897c09262e7" translate="yes" xml:space="preserve">
          <source>Long-awaited Generalized Linear Models with non-normal loss functions are now available. In particular, three new regressors were implemented: &lt;a href=&quot;../../modules/generated/sklearn.linear_model.poissonregressor#sklearn.linear_model.PoissonRegressor&quot;&gt;&lt;code&gt;PoissonRegressor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.linear_model.gammaregressor#sklearn.linear_model.GammaRegressor&quot;&gt;&lt;code&gt;GammaRegressor&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.tweedieregressor#sklearn.linear_model.TweedieRegressor&quot;&gt;&lt;code&gt;TweedieRegressor&lt;/code&gt;&lt;/a&gt;. The Poisson regressor can be used to model positive integer counts, or relative frequencies. Read more in the &lt;a href=&quot;../../modules/linear_model#generalized-linear-regression&quot;&gt;User Guide&lt;/a&gt;. Additionally, &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; supports a new &amp;lsquo;poisson&amp;rsquo; loss as well.</source>
          <target state="translated">非正規損失関数を備えた待望の一般化線形モデルが利用可能になりました。特に、&lt;a href=&quot;../../modules/generated/sklearn.linear_model.poissonregressor#sklearn.linear_model.PoissonRegressor&quot;&gt; &lt;code&gt;PoissonRegressor&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;../../modules/generated/sklearn.linear_model.gammaregressor#sklearn.linear_model.GammaRegressor&quot;&gt; &lt;code&gt;GammaRegressor&lt;/code&gt; &lt;/a&gt;、および&lt;a href=&quot;../../modules/generated/sklearn.linear_model.tweedieregressor#sklearn.linear_model.TweedieRegressor&quot;&gt; &lt;code&gt;TweedieRegressor&lt;/code&gt; の&lt;/a&gt;3つの新しいリグレッサーが実装されました。ポアソン回帰子は、正の整数カウントまたは相対度数をモデル化するために使用できます。詳細については、&lt;a href=&quot;../../modules/linear_model#generalized-linear-regression&quot;&gt;ユーザーガイド&lt;/a&gt;をご覧ください。さらに、&lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;は、新しい「ポアソン」損失もサポートします。</target>
        </trans-unit>
        <trans-unit id="83ec89bbbb1925d31612bf071115de8d555d9924" translate="yes" xml:space="preserve">
          <source>Longitude house block longitude</source>
          <target state="translated">家屋ブロック経度</target>
        </trans-unit>
        <trans-unit id="900f1aa8e30fd6bc29b8e8d4cae98f421ef462cd" translate="yes" xml:space="preserve">
          <source>Looking at the coefficient plot to gauge feature importance can be misleading as some of them vary on a small scale, while others, like AGE, varies a lot more, several decades.</source>
          <target state="translated">特徴の重要性を測るために係数プロットを見ると、いくつかは小さなスケールで変化し、他のものはAGEのように数十年単位で変化するので、誤解を招く可能性があります。</target>
        </trans-unit>
        <trans-unit id="65fb0d7c39bbac75b47a97a31a7e2974bcfa2753" translate="yes" xml:space="preserve">
          <source>Looking closely at the WAGE distribution reveals that it has a long tail. For this reason, we should take its logarithm to turn it approximately into a normal distribution (linear models such as ridge or lasso work best for a normal distribution of error).</source>
          <target state="translated">WAGE 分布をよく見ると、長い尾を持っていることがわかります。この理由から、その対数を取って、ほぼ正規分布にする必要があります(線形モデルでは、リッジやラッソのような誤差の正規分布に最も適しています)。</target>
        </trans-unit>
        <trans-unit id="e9aa6c9b011905252d25083008c9809acc3f4160" translate="yes" xml:space="preserve">
          <source>Loss function used by the algorithm.</source>
          <target state="translated">アルゴリズムで使用される損失関数。</target>
        </trans-unit>
        <trans-unit id="f2ebf0012d7d593bf1ef0d0a316102397c08a9f0" translate="yes" xml:space="preserve">
          <source>Low-level methods</source>
          <target state="translated">低レベルメソッド</target>
        </trans-unit>
        <trans-unit id="43b8e239b3dbfa96de18c459c0e716f889fbf1ff" translate="yes" xml:space="preserve">
          <source>Lower bound on the lowest predicted value (the minimum value may still be higher). If not set, defaults to -inf.</source>
          <target state="translated">予測値の下限値(最小値の方がまだ高いかもしれません)。設定されていない場合、デフォルトは-infです。</target>
        </trans-unit>
        <trans-unit id="ea609f61be1ccbae7413cc55d9401ee01dff16e3" translate="yes" xml:space="preserve">
          <source>Lower bound value on the likelihood (of the training data with respect to the model) of the best fit of inference.</source>
          <target state="translated">推論の最適適合度の尤度(モデルに対する訓練データの)の下界値。</target>
        </trans-unit>
        <trans-unit id="af301438554e0ee8815f3548a50754545e52e051" translate="yes" xml:space="preserve">
          <source>Lower bound value on the log-likelihood (of the training data with respect to the model) of the best fit of EM.</source>
          <target state="translated">EM の最適適合度の対数尤度(モデルに対する訓練データの)の下界値.</target>
        </trans-unit>
        <trans-unit id="4ada54abc98e483baeab7ae15def52027a7aae96" translate="yes" xml:space="preserve">
          <source>Lower-triangular Cholesky decomposition of the kernel in &lt;code&gt;X_train_&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;X_train_&lt;/code&gt; のカーネルの下三角コレスキー分解</target>
        </trans-unit>
        <trans-unit id="b93b2eafc7fea724b9bcb08bb1fb9109b8d1d561" translate="yes" xml:space="preserve">
          <source>M. Bawa, T. Condie and P. Ganesan, &amp;ldquo;LSH Forest: Self-Tuning Indexes for Similarity Search&amp;rdquo;, WWW &amp;lsquo;05 Proceedings of the 14th international conference on World Wide Web, 651-660, 2005.</source>
          <target state="translated">M.バワ、T。コンディー、およびP.ガネサン、「LSHフォレスト：類似検索のための自己調整インデックス」、WWW '05 Proceedings of the 14th International Conference on World Wide Web、651-660、2005。</target>
        </trans-unit>
        <trans-unit id="e8445854a0cf4ad63f8ee64cb2fc2359051f4c85" translate="yes" xml:space="preserve">
          <source>M. Dumont et al, &lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;Fast multi-class image annotation with random subwindows and multiple output randomized trees&lt;/a&gt;, International Conference on Computer Vision Theory and Applications 2009</source>
          <target state="translated">M. Dumontら&lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;、ランダムサブウィンドウと複数の出力ランダム化ツリーを使用した高速マルチクラスイメージアノテーション、&lt;/a&gt;コンピュータービジョン理論とアプリケーションに関する国際会議2009</target>
        </trans-unit>
        <trans-unit id="4f0f168494bf38e0f99da8c5a97b71596101a871" translate="yes" xml:space="preserve">
          <source>M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine, Journal of Machine Learning Research, Vol. 1, 2001.</source>
          <target state="translated">M.E.Tipping,スパースベイズ学習と関連性ベクトルマシン,機械学習研究ジャーナル,Vol.1,2001.</target>
        </trans-unit>
        <trans-unit id="2422710e8cdc4f555670a3606a875134eadd99fe" translate="yes" xml:space="preserve">
          <source>M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;The Pascal Visual Object Classes (VOC) Challenge&lt;/a&gt;, IJCV 2010.</source>
          <target state="translated">M. Everingham、L。Van Gool、CKI Williams、J。Winn、A。Zisserman、&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;The Pascal Visual Object Classes（VOC）Challenge&lt;/a&gt;、IJCV 2010。</target>
        </trans-unit>
        <trans-unit id="aa09c5b3704d1cab7c2f9d80f35ab989ea05cba1" translate="yes" xml:space="preserve">
          <source>MAE output is non-negative floating point. The best value is 0.0.</source>
          <target state="translated">MAE出力は非負の浮動小数点です。最良値は0.0です。</target>
        </trans-unit>
        <trans-unit id="dcac13e5386ab7554d99f42d2774ea6dec7dc033" translate="yes" xml:space="preserve">
          <source>MARR</source>
          <target state="translated">MARR</target>
        </trans-unit>
        <trans-unit id="203b5e4f3efe3b38e1b9f876ab3923dffadb1fa1" translate="yes" xml:space="preserve">
          <source>MARR_Unmarried</source>
          <target state="translated">MARR_Unmarried</target>
        </trans-unit>
        <trans-unit id="f4d1d18b18dbadb43dc94aafefb0919124514bb4" translate="yes" xml:space="preserve">
          <source>MEDV Median value of owner-occupied homes in $1000&amp;rsquo;s</source>
          <target state="translated">MEDV 1000ドルでの所有者が居住する住宅の中央値</target>
        </trans-unit>
        <trans-unit id="33379c640ef1bcb7b4dbc3ceb61d0f9854342e44" translate="yes" xml:space="preserve">
          <source>MKL</source>
          <target state="translated">MKL</target>
        </trans-unit>
        <trans-unit id="a8e1fd8b99167af6d3e02ac86d0a101dabaf0e42" translate="yes" xml:space="preserve">
          <source>MLP can fit a non-linear model to the training data. &lt;code&gt;clf.coefs_&lt;/code&gt; contains the weight matrices that constitute the model parameters:</source>
          <target state="translated">MLPは非線形モデルをトレーニングデータに適合させることができます。 &lt;code&gt;clf.coefs_&lt;/code&gt; には、モデルパラメータを構成する重み行列が含まれています。</target>
        </trans-unit>
        <trans-unit id="7fc5f2a7a15f6ccd1641b37c2fb96c6ce75018c2" translate="yes" xml:space="preserve">
          <source>MLP is sensitive to feature scaling.</source>
          <target state="translated">MLPは特徴量のスケーリングに敏感です。</target>
        </trans-unit>
        <trans-unit id="08431dee59de79a71b4718dbf6ee28e75fee38c3" translate="yes" xml:space="preserve">
          <source>MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.</source>
          <target state="translated">MLPでは、隠れニューロンの数、層の数、反復回数など、多くのハイパーパラメータを調整する必要があります。</target>
        </trans-unit>
        <trans-unit id="e82dbcf8b443c94c79e54d7d53faa6f5db46762a" translate="yes" xml:space="preserve">
          <source>MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples:</source>
          <target state="translated">MLPは,浮動小数点特徴ベクトルとして表現された学習サンプルを保持するサイズの配列X(n_samples,n_features)と,学習サンプルの目標値(クラスラベル)を保持するサイズの配列y(n_samples,)の2つの配列を用いて学習を行います.</target>
        </trans-unit>
        <trans-unit id="f0c27305c85163e665d40daa0f2ca2e458a2e63e" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="translated">MLPは、&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;確率的勾配降下法&lt;/a&gt;、&lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;、または&lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;を使用してトレーニングします。確率的勾配降下法（SGD）は、適応を必要とするパラメーターに関する損失関数の勾配を使用してパラメーターを更新します。</target>
        </trans-unit>
        <trans-unit id="4fc94508d8948819a05a769d08877751f90a9958" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="translated">MLPは、&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;確率的勾配降下法&lt;/a&gt;、&lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;アダム&lt;/a&gt;、または&lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;を使用してトレーニングします。確率的勾配降下法（SGD）は、適応が必要なパラメーターに関する損失関数の勾配を使用してパラメーターを更新します。</target>
        </trans-unit>
        <trans-unit id="f27922032032bfc1325082b9b33d5f3a9228ddf6" translate="yes" xml:space="preserve">
          <source>MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates \(P(y|x)\) per sample \(x\):</source>
          <target state="translated">MLPはバックプロパゲーションを使って学習します。より正確には,ある種の勾配降下を用いて訓練を行い,勾配はBackpropagationを用いて計算される.分類では、クロスエントロピー損失関数を最小化し、サンプルごとの確率推定値のベクトルを与える。</target>
        </trans-unit>
        <trans-unit id="c8c1d3b7c59691465cb0496f22bcb3604bca5a60" translate="yes" xml:space="preserve">
          <source>MLP uses different loss functions depending on the problem type. The loss function for classification is Cross-Entropy, which in binary case is given as,</source>
          <target state="translated">MLPでは、問題の種類によって異なる損失関数を使用します。分類のための損失関数はCross-Entropyであり、バイナリの場合は次のように与えられる。</target>
        </trans-unit>
        <trans-unit id="d03f0b750d6ad970862b8ceab4b82a667eb1bc44" translate="yes" xml:space="preserve">
          <source>MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.</source>
          <target state="translated">隠れた層を持つMLPは、局所的な最小値が複数存在する非凸の損失関数を持っています。そのため、ランダムな重みの初期設定が異なると、検証精度が異なることがあります。</target>
        </trans-unit>
        <trans-unit id="14160d0f2e53b28f2f5c2a7702cb0510220c29b8" translate="yes" xml:space="preserve">
          <source>MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPClassifierは反復的に学習します.これは,各時間ステップでモデルパラメータに対する損失関数の部分導関数が計算され,パラメータが更新されるからです.</target>
        </trans-unit>
        <trans-unit id="a1925f1c916c80accddbe48b0d0e8da75d102083" translate="yes" xml:space="preserve">
          <source>MLPRegressor</source>
          <target state="translated">MLPRegressor</target>
        </trans-unit>
        <trans-unit id="8b27d0c0c6a8a44ae6f32f660e2bfb892d109024" translate="yes" xml:space="preserve">
          <source>MLPRegressor trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPRegressorは,各時間ステップでモデルパラメータに対する損失関数の部分導関数が計算され,パラメータを更新するために反復的に学習します.</target>
        </trans-unit>
        <trans-unit id="8e70290c1fc16432a8f4b616e4fd6fbaa4abfbea" translate="yes" xml:space="preserve">
          <source>MNIST classfification using multinomial logistic + L1</source>
          <target state="translated">多項ロジスティック+L1を用いたMNISTクラスフィフィケーション</target>
        </trans-unit>
        <trans-unit id="fbb70ceca40f03de1a52a87cdebacfd0c2426668" translate="yes" xml:space="preserve">
          <source>MNIST classification using multinomial logistic + L1</source>
          <target state="translated">多項ロジスティック+L1を用いたMNIST分類</target>
        </trans-unit>
        <trans-unit id="25845da185fe3a02cb60c18fcf84202e8f31d1e7" translate="yes" xml:space="preserve">
          <source>Machine learning algorithms need data. Go to each &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; sub-folder and run the &lt;code&gt;fetch_data.py&lt;/code&gt; script from there (after having read them first).</source>
          <target state="translated">機械学習アルゴリズムにはデータが必要です。各 &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; サブフォルダーに移動し、そこから &lt;code&gt;fetch_data.py&lt;/code&gt; スクリプトを実行します（最初に読んだ後）。</target>
        </trans-unit>
        <trans-unit id="45f2bd27f62f0226a5b3177e6a59d79cc23fea68" translate="yes" xml:space="preserve">
          <source>Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the &lt;strong&gt;training set&lt;/strong&gt;, on which we learn some properties; we call the other set the &lt;strong&gt;testing set&lt;/strong&gt;, on which we test the learned properties.</source>
          <target state="translated">機械学習とは、データセットのいくつかのプロパティを学習し、それらのプロパティを別のデータセットに対してテストすることです。機械学習の一般的な方法は、データセットを2つに分割してアルゴリズムを評価することです。これらのセットの1つを&lt;strong&gt;トレーニングセット&lt;/strong&gt;と呼び、その上でいくつかのプロパティを学習します。もう1つのセットを&lt;strong&gt;テストセット&lt;/strong&gt;と呼び、学習したプロパティをテストします。</target>
        </trans-unit>
        <trans-unit id="17dc705c260bdc393406dc006335656d8788b655" translate="yes" xml:space="preserve">
          <source>Machine learning: the problem setting</source>
          <target state="translated">機械学習:問題設定</target>
        </trans-unit>
        <trans-unit id="e6a69273199992ddfe41f469dda4cc1f6b79ceb0" translate="yes" xml:space="preserve">
          <source>Magnesium</source>
          <target state="translated">Magnesium</target>
        </trans-unit>
        <trans-unit id="2bb08573261ae718ebb52db49951821b64f9a80c" translate="yes" xml:space="preserve">
          <source>Magnesium:</source>
          <target state="translated">Magnesium:</target>
        </trans-unit>
        <trans-unit id="ca3ae45b6eafdd0a6dfea23df60842bdb389e9fc" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;a href=&quot;#sklearn.covariance.EllipticEnvelope.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is called) observations.</source>
          <target state="translated">トレーニングセット（&lt;a href=&quot;#sklearn.covariance.EllipticEnvelope.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;と呼ばれる）観測のマハラノビス距離。</target>
        </trans-unit>
        <trans-unit id="55e9152468183c8a16be469ac6ea3dbb0e42dd62" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;a href=&quot;#sklearn.covariance.MinCovDet.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is called) observations.</source>
          <target state="translated">トレーニングセット（&lt;a href=&quot;#sklearn.covariance.MinCovDet.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;と呼ばれる）観測のマハラノビス距離。</target>
        </trans-unit>
        <trans-unit id="91059cae8d3b76142d7879b78c0a2ccaab268e7d" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;code&gt;fit&lt;/code&gt; is called) observations.</source>
          <target state="translated">トレーニングセット（ &lt;code&gt;fit&lt;/code&gt; が呼び出される）観測のマハラノビス距離。</target>
        </trans-unit>
        <trans-unit id="d469f730cc4a1b58c5ef61209e446f59013c3c80" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances to centers</source>
          <target state="translated">センターへのマハラノビスの距離</target>
        </trans-unit>
        <trans-unit id="6f98cc22ed52c0a1e40fae778fadcd35627c25b5" translate="yes" xml:space="preserve">
          <source>MahalanobisDistance</source>
          <target state="translated">MahalanobisDistance</target>
        </trans-unit>
        <trans-unit id="62bce9422ff2d14f69ab80a154510232fc8a9afd" translate="yes" xml:space="preserve">
          <source>Main</source>
          <target state="translated">Main</target>
        </trans-unit>
        <trans-unit id="7a412cc8631eaef465ac98b25829da66ced3b43d" translate="yes" xml:space="preserve">
          <source>Main takeaways</source>
          <target state="translated">主な持ち物</target>
        </trans-unit>
        <trans-unit id="8d6381188443dad8aa5d016fb4ec69dd96237200" translate="yes" xml:space="preserve">
          <source>Make a copy of input data.</source>
          <target state="translated">入力データのコピーを作成します。</target>
        </trans-unit>
        <trans-unit id="f11963f5d19078a49cfab3cc41da5922accd3330" translate="yes" xml:space="preserve">
          <source>Make a large circle containing a smaller circle in 2d.</source>
          <target state="translated">2次元で小さな円を含む大きな円を作ります。</target>
        </trans-unit>
        <trans-unit id="1cce5fef6c99c293eee32e22f026e768f4b5d892" translate="yes" xml:space="preserve">
          <source>Make a scorer from a performance metric or loss function.</source>
          <target state="translated">パフォーマンスメトリックまたは損失関数からスコアラーを作成します。</target>
        </trans-unit>
        <trans-unit id="723bb825ac5b7b14f789cce44f7b667d39fe6543" translate="yes" xml:space="preserve">
          <source>Make and use a deep copy of X and Y (if Y exists)</source>
          <target state="translated">XとYのディープコピーを作成して使用する(Yが存在する場合</target>
        </trans-unit>
        <trans-unit id="bdd3abd6a5ef3ffb2c4167fd4f1b6dc60d7a55bd" translate="yes" xml:space="preserve">
          <source>Make arrays indexable for cross-validation.</source>
          <target state="translated">クロスバリデーションのために配列にインデックスを付けられるようにします。</target>
        </trans-unit>
        <trans-unit id="5f9f781fbc2f44502a26f1a8945369508ac6ebd5" translate="yes" xml:space="preserve">
          <source>Make pipeline to preprocess the data</source>
          <target state="translated">データを前処理するためのパイプラインを作成</target>
        </trans-unit>
        <trans-unit id="2d28cad808149ac4eb3c924434a0979417f24a68" translate="yes" xml:space="preserve">
          <source>Make sure that X has a minimum number of samples in its first axis (rows for a 2D array).</source>
          <target state="translated">Xの第1軸のサンプル数が最小であることを確認してください(2D配列の場合は行数)。</target>
        </trans-unit>
        <trans-unit id="b2b1ee415b35f3ec33d28dbc91a8479edeb8d71e" translate="yes" xml:space="preserve">
          <source>Make sure that array is 2D, square and symmetric.</source>
          <target state="translated">配列が2次元、正方形、対称であることを確認してください。</target>
        </trans-unit>
        <trans-unit id="6885424e5e7bfa46a7e7c7cb1bd5d6e804bbccd9" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when X has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">2D配列に最小数のフィーチャ（列）があることを確認してください。デフォルト値の1は、空のデータセットを拒否します。このチェックは、Xが実質的に2次元であるか、元は1Dで、 &lt;code&gt;ensure_2d&lt;/code&gt; がTrueの場合にのみ適用されます。0に設定すると、このチェックが無効になります。</target>
        </trans-unit>
        <trans-unit id="37a276f69711964822e7fcec88111a5f6d2f84a0" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when the input data has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">2D配列に最小数のフィーチャ（列）があることを確認してください。デフォルト値の1は、空のデータセットを拒否します。このチェックは、入力データが実質的に2次元であるか、 &lt;code&gt;ensure_2d&lt;/code&gt; は1Dで、確実に_2dがTrueである場合にのみ適用されます。0に設定すると、このチェックが無効になります。</target>
        </trans-unit>
        <trans-unit id="c24d8a1e4fcddcfde73957adfbe65fb40c76c786" translate="yes" xml:space="preserve">
          <source>Make sure that the array has a minimum number of samples in its first axis (rows for a 2D array). Setting to 0 disables this check.</source>
          <target state="translated">配列の第1軸(2次元配列の場合は行)のサンプル数が最小であることを確認してください。0に設定すると、このチェックは無効になります。</target>
        </trans-unit>
        <trans-unit id="fdc3084b2db3fff3561874bdbe81f2954a4b0ffc" translate="yes" xml:space="preserve">
          <source>Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise. See &lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt; for convenient ways of scaling heterogeneous data.</source>
          <target state="translated">すべての機能で同じスケールが使用されていることを確認してください。多様体学習法は最近傍探索に基づいているため、そうでなければアルゴリズムのパフォーマンスが低下する可能性があります。異種データをスケーリングする便利な方法については、&lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="bd58d70c4390b61fe411e7823adf47cfb5052e9e" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration (used by default). Also, ideally, features should be standardized using e.g. &lt;code&gt;make_pipeline(StandardScaler(), SGDClassifier())&lt;/code&gt; (see &lt;a href=&quot;compose#combining-estimators&quot;&gt;Pipelines&lt;/a&gt;).</source>
          <target state="translated">モデルをフィッティングする前にトレーニングデータを並べ替え（シャッフル）するか、 &lt;code&gt;shuffle=True&lt;/code&gt; を使用して各反復後にシャッフルします（デフォルトで使用）。また、理想的には、機能は &lt;code&gt;make_pipeline(StandardScaler(), SGDClassifier())&lt;/code&gt; を使用して標準化する必要があります（&lt;a href=&quot;compose#combining-estimators&quot;&gt;パイプラインを&lt;/a&gt;参照）。</target>
        </trans-unit>
        <trans-unit id="7f7e62e13bb8885a4df4d0d5a8e1dba7c3c65c15" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration.</source>
          <target state="translated">モデルを近似する前にトレーニングデータを並べ替える（シャッフルする）か、各反復後に &lt;code&gt;shuffle=True&lt;/code&gt; を使用してシャッフルしてください。</target>
        </trans-unit>
        <trans-unit id="f48f3a474378f962985a41275dc98355541c63d2" translate="yes" xml:space="preserve">
          <source>Make two interleaving half circles</source>
          <target state="translated">2つの半円を重ねる</target>
        </trans-unit>
        <trans-unit id="69015b0f2ce90a52d27e40a08b160550cfb23a90" translate="yes" xml:space="preserve">
          <source>Making predictions</source>
          <target state="translated">予測を立てる</target>
        </trans-unit>
        <trans-unit id="0a0a9871e0af603535e4f6104cfca3266e203a87" translate="yes" xml:space="preserve">
          <source>Malic Acid:</source>
          <target state="translated">リンゴ酸です。</target>
        </trans-unit>
        <trans-unit id="245748b8f3a70aaac9759204b7d3978b9d337db8" translate="yes" xml:space="preserve">
          <source>Malic acid</source>
          <target state="translated">リンゴ酸</target>
        </trans-unit>
        <trans-unit id="a81a721fb7e702ed0a37d056ec4a9d2f925e70b0" translate="yes" xml:space="preserve">
          <source>ManhattanDistance</source>
          <target state="translated">ManhattanDistance</target>
        </trans-unit>
        <trans-unit id="ccbe2127be70aaa7c3514b3155dd29902fae6143" translate="yes" xml:space="preserve">
          <source>Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.</source>
          <target state="translated">マニホールド学習は、PCAのような線形フレームワークを一般化し、データの非線形構造に敏感になるようにしようとする試みと考えることができます。教師ありのバリエーションが存在しますが、典型的なマニホールド学習問題は教師なしです:それは、事前に決められた分類を使用せずに、データ自体からデータの高次元構造を学習します。</target>
        </trans-unit>
        <trans-unit id="7a0e60acb472080022463866637a1ea7c0251335" translate="yes" xml:space="preserve">
          <source>Manifold Learning methods on a severed sphere</source>
          <target state="translated">切断された球体上でのマニホールド学習法</target>
        </trans-unit>
        <trans-unit id="5f7bca3c10846eb5854c536c3448fedf998ffbcf" translate="yes" xml:space="preserve">
          <source>Manifold learning</source>
          <target state="translated">マニホールド学習</target>
        </trans-unit>
        <trans-unit id="aca365adba00c10f7a3cc50cff4a88afd0947dd9" translate="yes" xml:space="preserve">
          <source>Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.</source>
          <target state="translated">マニホールド学習は、非線形次元削減へのアプローチです。このタスクのためのアルゴリズムは、多くのデータセットの次元性が人為的に高いだけであるという考えに基づいています。</target>
        </trans-unit>
        <trans-unit id="1e718f0bccbec4566b4c8536fdd24c184318a8a9" translate="yes" xml:space="preserve">
          <source>Manifold learning on handwritten digits: Locally Linear Embedding, Isomap&amp;hellip;</source>
          <target state="translated">手書き数字の多様体学習：ローカル線形埋め込み、Isomap&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="ad97dd09eb7e528a0b4debe1b9c745e650b7307a" translate="yes" xml:space="preserve">
          <source>Manually setting one of the environment variables (&lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt;, &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt;, or &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt;) will take precedence over what joblib tries to do. The total number of threads will be &lt;code&gt;n_jobs * &amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt;. Note that setting this limit will also impact your computations in the main process, which will only use &lt;code&gt;&amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt;. Joblib exposes a context manager for finer control over the number of threads in its workers (see joblib docs linked below).</source>
          <target state="translated">環境変数（ &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt; 、 &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt; 、 &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt; 、または &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt; ）の1つを手動で設定することは、joblibが実行しようとすることよりも優先されます。スレッドの総数は &lt;code&gt;n_jobs * &amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt; ます。この制限を設定すると、 &lt;code&gt;&amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt; のみを使用するメインプロセスでの計算にも影響することに注意してください。 Joblibは、ワーカー内のスレッド数をより細かく制御するためのコンテキストマネージャーを公開します（以下にリンクされているjoblibドキュメントを参照）。</target>
        </trans-unit>
        <trans-unit id="0471386edfe0bf057e3ed471b66689189fdc892c" translate="yes" xml:space="preserve">
          <source>Manufacturing</source>
          <target state="translated">Manufacturing</target>
        </trans-unit>
        <trans-unit id="0d3695eb907329bab9f0e9752d7ff00d200420c9" translate="yes" xml:space="preserve">
          <source>Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an &lt;em&gt;inlier&lt;/em&gt;), or should be considered as different (it is an &lt;em&gt;outlier&lt;/em&gt;). Often, this ability is used to clean real data sets. Two important distinctions must be made:</source>
          <target state="translated">多くのアプリケーションでは、新しい観測値が既存の観測値と同じ分布に属している（&lt;em&gt;inlierである&lt;/em&gt;）か、異なると見なす必要がある（&lt;em&gt;外れ値である&lt;/em&gt;）かを判断できる必要があります。多くの場合、この機能は実際のデータセットのクリーンアップに使用されます。次の2つの重要な違いがあります。</target>
        </trans-unit>
        <trans-unit id="626f0980ad5cd6d2b6f18a99ff094a7bf141dc9a" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints</source>
          <target state="translated">多くのクラスタ、接続性の制約がある可能性がある</target>
        </trans-unit>
        <trans-unit id="9d1190903d42ddc70f3db2311f157c56b6260b92" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints, non Euclidean distances</source>
          <target state="translated">多数のクラスター、接続性制約、非ユークリッド距離の可能性</target>
        </trans-unit>
        <trans-unit id="ac65e2f8a158fa7cc404d708906171f5ea9f26fd" translate="yes" xml:space="preserve">
          <source>Many clusters, uneven cluster size, non-flat geometry</source>
          <target state="translated">多数のクラスター、不均一なクラスターサイズ、非平坦な形状</target>
        </trans-unit>
        <trans-unit id="241eda779a46dbe514b8b7f2a96e98aed4d935af" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="translated">多くのデータセットには、テキスト、フロート、日付など、さまざまなタイプの特徴が含まれています。各タイプの特徴には、個別の前処理または特徴抽出ステップが必要です。多くの場合、たとえば&lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;を使用するなど、scikit-learnメソッドを適用する前にデータを前処理するのが最も簡単です。 scikit-learnに渡す前にデータを処理すると、次のいずれかの理由で問題が発生する可能性があります。</target>
        </trans-unit>
        <trans-unit id="d589144c9193e129ab4721a317a3dc3eaa909106" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="translated">多くのデータセットには、テキスト、フロート、日付など、さまざまなタイプの特徴が含まれています。各タイプの特徴には、個別の前処理または特徴抽出ステップが必要です。多くの場合、たとえば&lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;を使用して、scikit-learnメソッドを適用する前にデータを前処理するのが最も簡単です。データをscikit-learnに渡す前に処理すると、次のいずれかの理由で問題が発生する可能性があります。</target>
        </trans-unit>
        <trans-unit id="192c25a6ed904d1327958fde4c93098505cb86b8" translate="yes" xml:space="preserve">
          <source>Many metrics are not given names to be used as &lt;code&gt;scoring&lt;/code&gt; values, sometimes because they require additional parameters, such as &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;. In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;. That function converts metrics into callables that can be used for model evaluation.</source>
          <target state="translated">多くのメトリックには、 &lt;code&gt;scoring&lt;/code&gt; 値として使用する名前が付けられていません。これは、&lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt;などの追加のパラメーターが必要になる場合があるためです。このような場合、適切なスコアリングオブジェクトを生成する必要があります。スコアリング用の呼び出し可能オブジェクトを生成する最も簡単な方法は、&lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt;を使用することです。この関数は、メトリックをモデル評価に使用できる呼び出し可能オブジェクトに変換します。</target>
        </trans-unit>
        <trans-unit id="c6eff6cfb2b81378a219da9247d45ca528d18b55" translate="yes" xml:space="preserve">
          <source>Many scikit-learn estimators rely on nearest neighbors: Several classifiers and regressors such as &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.kneighborsregressor#sklearn.neighbors.KNeighborsRegressor&quot;&gt;&lt;code&gt;KNeighborsRegressor&lt;/code&gt;&lt;/a&gt;, but also some clustering methods such as &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;SpectralClustering&lt;/code&gt;&lt;/a&gt;, and some manifold embeddings such as &lt;a href=&quot;generated/sklearn.manifold.tsne#sklearn.manifold.TSNE&quot;&gt;&lt;code&gt;TSNE&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;Isomap&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">多くのscikit-learn推定量は、最近傍に依存しています&lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt;や&lt;a href=&quot;generated/sklearn.neighbors.kneighborsregressor#sklearn.neighbors.KNeighborsRegressor&quot;&gt; &lt;code&gt;KNeighborsRegressor&lt;/code&gt; &lt;/a&gt;などのいくつかの分類器と回帰子だけでなく、&lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt;や&lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;SpectralClustering&lt;/code&gt; &lt;/a&gt;などのいくつかのクラスタリング手法、および&lt;a href=&quot;generated/sklearn.manifold.tsne#sklearn.manifold.TSNE&quot;&gt; &lt;code&gt;TSNE&lt;/code&gt; &lt;/a&gt;や&lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;Isomap&lt;/code&gt; &lt;/a&gt;などのいくつかの多様体埋め込みもあります。</target>
        </trans-unit>
        <trans-unit id="3a89fb6cdb80688c2fbf1190407f9abec89dd4f3" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="translated">多くの統計的問題では、母集団の共分散行列の推定が必要です。これは、データセットの散布図の形状の推定と見なすことができます。ほとんどの場合、このような推定は、プロパティ（サイズ、構造、均一性）が推定の品質に大きな影響を与えるサンプルに対して実行する必要があります。&lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; の&lt;/a&gt;パッケージには、正確にさまざまな設定の下で、人口の共分散行列を推定するためのツールを提供します。</target>
        </trans-unit>
        <trans-unit id="dacd10610ea3bac36f91971d47d3019273ca964d" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;code&gt;sklearn.covariance&lt;/code&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="translated">多くの統計的問題では、母集団の共分散行列の推定が必要です。これは、データセットの散布図の形状の推定と見なすことができます。ほとんどの場合、そのような推定は、特性（サイズ、構造、均一性）が推定の品質に大きな影響を与えるサンプルに対して行われる必要があります。 &lt;code&gt;sklearn.covariance&lt;/code&gt; のパッケージには、正確にさまざまな設定の下で、人口の共分散行列を推定するためのツールを提供します。</target>
        </trans-unit>
        <trans-unit id="be7bf3b7e371f4bec9a03a7522f6dcf31d112a68" translate="yes" xml:space="preserve">
          <source>Many, many more &amp;hellip;</source>
          <target state="translated">もっとたくさん&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="01a4f781a04bf81d6d3609180ff5b158082bf232" translate="yes" xml:space="preserve">
          <source>Map data to a normal distribution</source>
          <target state="translated">データを正規分布にマッピング</target>
        </trans-unit>
        <trans-unit id="16409bc40b2df043ac11786860ad0f327aa511b9" translate="yes" xml:space="preserve">
          <source>Maps data to a normal distribution using a power transformation.</source>
          <target state="translated">電力変換を用いてデータを正規分布にマッピングします。</target>
        </trans-unit>
        <trans-unit id="05aecccd2b32722fa423ccbd7840d48763834385" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">パラメーター &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt; 使用して、データを標準正規分布にマップします。</target>
        </trans-unit>
        <trans-unit id="12f61571505f27a87deaf94928434363c3add704" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution='normal'&lt;/code&gt;.</source>
          <target state="translated">パラメータ &lt;code&gt;output_distribution='normal'&lt;/code&gt; を使用して、データを標準正規分布にマップします。</target>
        </trans-unit>
        <trans-unit id="2546740d19a0cb39e3dfa40dd82138fa02a22968" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list.</source>
          <target state="translated">各カテゴリ特徴名を値のリストにマップし、iとしてエンコードされた値がリストのithになるようにします。</target>
        </trans-unit>
        <trans-unit id="67b185421bf5d928c6a7d5bf74ee20d677f50228" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list. If &lt;code&gt;as_frame&lt;/code&gt; is True, this is None.</source>
          <target state="translated">iとしてエンコードされた値がリストのi番目になるように、各カテゴリ機能名を値のリストにマップします。 &lt;code&gt;as_frame&lt;/code&gt; がTrueの場合、これはNoneです。</target>
        </trans-unit>
        <trans-unit id="601b228138151f5d614818578f5a990f06465ee3" translate="yes" xml:space="preserve">
          <source>Marginal distribution for the transformed data. The choices are &amp;lsquo;uniform&amp;rsquo; (default) or &amp;lsquo;normal&amp;rsquo;.</source>
          <target state="translated">変換されたデータの周辺分布。選択肢は、 'uniform'（デフォルト）または 'normal'です。</target>
        </trans-unit>
        <trans-unit id="32cec489ab51eb304acc5d56c34e0b5894817af1" translate="yes" xml:space="preserve">
          <source>Mark Schmidt, Nicolas Le Roux, and Francis Bach: &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;Minimizing Finite Sums with the Stochastic Average Gradient.&lt;/a&gt;</source>
          <target state="translated">Mark Schmidt、Nicolas Le Roux、Francis Bach：&lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;確率的平均勾配による有限和の最小化。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c75a2b42a0ab364e58b54559c45fcda50d1973f4" translate="yes" xml:space="preserve">
          <source>Married</source>
          <target state="translated">Married</target>
        </trans-unit>
        <trans-unit id="b66f191d027329ba9273c4c5f9be765f9b3745f5" translate="yes" xml:space="preserve">
          <source>Mask to be used on X.</source>
          <target state="translated">Xで使用するマスク。</target>
        </trans-unit>
        <trans-unit id="54a21a4d94fa24c6c092fd6e4c2ec05359c76c09" translate="yes" xml:space="preserve">
          <source>MatchingDistance</source>
          <target state="translated">MatchingDistance</target>
        </trans-unit>
        <trans-unit id="38c6b835ca8294e13538ac219d64ae1244beb7fc" translate="yes" xml:space="preserve">
          <source>Matern kernel.</source>
          <target state="translated">マテルンカーネル。</target>
        </trans-unit>
        <trans-unit id="c2846fd5b2a8440131137c07fea40912afc701b7" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with \(\ell_1\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">数学的には、線形モデルを正則化するために、\(\ell_1)priorを用いて訓練する。最小化する目的関数は</target>
        </trans-unit>
        <trans-unit id="bf1818d1c45b1997515a16368907c8cf902bab56" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior and \(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">Mathematically,it consists of a linear model trained with a mixed \(\(\ell_1))previous and \(\(\ell_2)previous as regularizer.最小化する目的関数は</target>
        </trans-unit>
        <trans-unit id="1d9fe275a9038555cabcfe46b4a675067788d84f" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">Mathematically,it consists of a linear model trained with a mixed \(\(2_2\)prior as regularizer.最小化する目的関数は</target>
        </trans-unit>
        <trans-unit id="85bbf115a5892290f61e157be8a21f18c7185291" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\)-norm and \(\ell_2\)-norm for regularization. The objective function to minimize is:</source>
          <target state="translated">Mathematically,it consists of a linear model trained with a mixed \(\(\ell_1)o\(\(\\ell_2)oorm and \(\(\ell_2)oorm for regularization.最小化する目的関数は</target>
        </trans-unit>
        <trans-unit id="a22bf31bc3080adb481367d05d21298b6681e5ee" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\)-norm for regularization. The objective function to minimize is:</source>
          <target state="translated">Mathematically,it consists of a linear model trained with a mixed \(\(2_2)norm for regularization.最小化する目的関数は</target>
        </trans-unit>
        <trans-unit id="47b46d2e2bec7b475ce40ef0c30d61c9f27a62f1" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:</source>
          <target state="translated">数学的には、正則化項を追加した線形モデルで構成されます。最小化する目的関数は</target>
        </trans-unit>
        <trans-unit id="461064fec990b9f56bd78c5da4699263962dc67b" translate="yes" xml:space="preserve">
          <source>Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation : \(\Sigma_{\rm shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm Tr}\hat{\Sigma}}{p}\rm Id\).</source>
          <target state="translated">数学的には,この収縮は,経験的共分散行列の最小の固有値と最大の固有値の間の比率を減らすことからなる.これは,与えられたオフセットに従ってすべての固有値を単純にシフトすることで行うことができ,これは共分散行列のl2罰化された最尤推定量を求めることに相当します.実際には、収縮は、単純な凸変換に煮詰まっています。\\\Sigma_{\rm shrunk}=(1-\alpha)Attack{Sigma}+\alpha\frac{\rm Tr}hat{\Rm Tr}hat{\Sigma}}{p}Rm Id).</target>
        </trans-unit>
        <trans-unit id="7f2fa948973686599d9719cd22bf9c261bdbf5a5" translate="yes" xml:space="preserve">
          <source>Mathematically, truncated SVD applied to training samples \(X\) produces a low-rank approximation \(X\):</source>
          <target state="translated">Mathematically,truncated SVD applied to training samplesに適用すると、低ランク近似が得られる。</target>
        </trans-unit>
        <trans-unit id="b8c6141893596b10260b39727bf4a66986a56a95" translate="yes" xml:space="preserve">
          <source>Matrices:</source>
          <target state="translated">Matrices:</target>
        </trans-unit>
        <trans-unit id="878abbe8708b2c0d949ede6590fbf80f3b3ca712" translate="yes" xml:space="preserve">
          <source>Matrix \(C\) such that \(C_{i, j}\) is the number of samples in true class \(i\) and in predicted class \(j\). If &lt;code&gt;eps is None&lt;/code&gt;, the dtype of this array will be integer. If &lt;code&gt;eps&lt;/code&gt; is given, the dtype will be float. Will be a &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; if &lt;code&gt;sparse=True&lt;/code&gt;.</source>
          <target state="translated">\（C_ {i、j} \）が真のクラス\（i \）と予測されたクラス\（j \）のサンプル数になるような行列\（C \）。 &lt;code&gt;eps is None&lt;/code&gt; 場合、この配列のdtypeは整数になります。場合は &lt;code&gt;eps&lt;/code&gt; 与えられ、dtypeはフロートになります。 &lt;code&gt;sparse=True&lt;/code&gt; の場合、 &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; になります。</target>
        </trans-unit>
        <trans-unit id="9a9d3bf25623c95ec103c9ba9ebefafc39b31e10" translate="yes" xml:space="preserve">
          <source>Matrix of similarities between points</source>
          <target state="translated">点間の類似度のマトリックス</target>
        </trans-unit>
        <trans-unit id="fe09cc11ed56c787dbf583e1d3c86930e73d13b7" translate="yes" xml:space="preserve">
          <source>Matrix to be scaled.</source>
          <target state="translated">スケーリングされるマトリックス。</target>
        </trans-unit>
        <trans-unit id="f581a8973d13aee9e6d301f776c7ac0aca9937df" translate="yes" xml:space="preserve">
          <source>Matrix to decompose</source>
          <target state="translated">分解するマトリックス</target>
        </trans-unit>
        <trans-unit id="64c58969af0dfb121c9b8582a353fed07f3ae81a" translate="yes" xml:space="preserve">
          <source>Matrix to normalize using the variance of the features.</source>
          <target state="translated">特徴量の分散を用いて正規化する行列.</target>
        </trans-unit>
        <trans-unit id="dc67599b55e21aadb81c15a2c42c0d243f238c7f" translate="yes" xml:space="preserve">
          <source>Matrix whose two columns are to be swapped.</source>
          <target state="translated">2つの列が入れ替わる行列.</target>
        </trans-unit>
        <trans-unit id="2ea8698954891f702cc6556af5a70f4a4777910c" translate="yes" xml:space="preserve">
          <source>Matrix whose two rows are to be swapped.</source>
          <target state="translated">2つの行が入れ替わる行列.</target>
        </trans-unit>
        <trans-unit id="91c27cd36373d9f3e97d3e8652e4d5420038195e" translate="yes" xml:space="preserve">
          <source>Max number of iterations for updating document topic distribution in the E-step.</source>
          <target state="translated">Eステップで文書トピック分布を更新するための最大反復回数。</target>
        </trans-unit>
        <trans-unit id="00c71f39eb3784f568496e9b201bef34cf1fba1e" translate="yes" xml:space="preserve">
          <source>MaxAbsScaler</source>
          <target state="translated">MaxAbsScaler</target>
        </trans-unit>
        <trans-unit id="b19f6ae06ce0301b0f2f115ace4b151976f71361" translate="yes" xml:space="preserve">
          <source>Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between \(q(z,\theta,\beta)\) and the true posterior \(p(z, \theta, \beta |w, \alpha, \eta)\).</source>
          <target state="translated">ELBOを最大化することは、Kullback-Leibler(KL)divergence between \(q(z,\theta,\beta)and the true posterior ﾟ(p(z,\theta,\beta |w,\alpha,)</target>
        </trans-unit>
        <trans-unit id="c7118c6c94bd33474c6bd73b2a0ef4d05bd61b9a" translate="yes" xml:space="preserve">
          <source>Maximizing the log-marginal-likelihood after subtracting the target&amp;rsquo;s mean yields the following kernel with an LML of -83.214:</source>
          <target state="translated">ターゲットの平均を差し引いた後、対数マージン尤度を最大化すると、LMLが-83.214の次のカーネルが生成されます。</target>
        </trans-unit>
        <trans-unit id="3cc68e53e734e045e272d9b61d6ce440314a7c5a" translate="yes" xml:space="preserve">
          <source>Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma. If an array is given, it will compute a safe number of components array-wise.</source>
          <target state="translated">ジョンソン-リンデンストラウス・リーマで定義された最大の歪み率。配列が与えられた場合,配列ごとに安全な数の成分を計算します.</target>
        </trans-unit>
        <trans-unit id="2648af65469bf6064127630edb239d63fa9087cb" translate="yes" xml:space="preserve">
          <source>Maximum likelihood covariance estimator</source>
          <target state="translated">最尤共分散推定量</target>
        </trans-unit>
        <trans-unit id="c549d82a160dc50758b33cda113fa1dc7a80727c" translate="yes" xml:space="preserve">
          <source>Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</source>
          <target state="translated">残差の最大ノルム。Noneでない場合は、n_nonzero_coefsをオーバーライドします。</target>
        </trans-unit>
        <trans-unit id="c4d8a154588727ab7c620ef2088eb03d03fdb2cd" translate="yes" xml:space="preserve">
          <source>Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes.</source>
          <target state="translated">各ノードにおける CF サブクラスタの最大数.サブクラスターの数が分岐係数を超えるような新しいサンプルが入ってきた場合,そのノードは2つのノードに分割され,それぞれのノードにサブクラスターが再分配されます.そのノードの親サブクラスターは削除され,分割された2つのノードの親として2つの新しいサブクラスターが追加されます.</target>
        </trans-unit>
        <trans-unit id="ffcbfb393ae9341f5e6cf4dea80093dbb65c654d" translate="yes" xml:space="preserve">
          <source>Maximum number of epochs to not meet &lt;code&gt;tol&lt;/code&gt; improvement. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">&lt;code&gt;tol&lt;/code&gt; の改善を満たさないエポックの最大数。solver = 'sgd'または 'adam'の場合にのみ有効</target>
        </trans-unit>
        <trans-unit id="7d6dde474581154e61a4c14f0f2c50aef90aa524" translate="yes" xml:space="preserve">
          <source>Maximum number of imputation rounds to perform before returning the imputations computed during the final round. A round is a single imputation of each feature with missing values. The stopping criterion is met once &lt;code&gt;abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))&lt;/code&gt; &amp;lt; tol, where &lt;code&gt;X_t&lt;/code&gt; is &lt;code&gt;X&lt;/code&gt; at iteration &lt;code&gt;t. Note that early stopping is only
applied if ``sample_posterior=False`&lt;/code&gt;.</source>
          <target state="translated">最終ラウンド中に計算された代入を返す前に実行する代入ラウンドの最大数。ラウンドは、値が欠落している各機能の単一の代入です。停止基準は、 &lt;code&gt;abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))&lt;/code&gt; &amp;lt;tolで満たされます &lt;code&gt;t. Note that early stopping is only applied if ``sample_posterior=False`&lt;/code&gt; ここで、 &lt;code&gt;X_t&lt;/code&gt; は反復tでの &lt;code&gt;X&lt;/code&gt; です。早期停止は、 `` sample_posterior = False`の場合にのみ適用されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="2d31095f21fc709b8362fbfbc8701ee49bed43f4" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations</source>
          <target state="translated">最大反復回数</target>
        </trans-unit>
        <trans-unit id="ec0c7c7cfd1a4dd46776f2839b3e2ee30db0ceb0" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations allowed.</source>
          <target state="translated">許容される反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="8d0f629c611a546c50fbd29c0a5c09d14523502f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations before timing out.</source>
          <target state="translated">タイミングアウトするまでの最大反復回数。</target>
        </trans-unit>
        <trans-unit id="e5ab15aeae2ebd19c6cc8dd9b722001d143407e7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations during fit.</source>
          <target state="translated">はめ込み中の最大反復回数。</target>
        </trans-unit>
        <trans-unit id="4bd775e3e4801f1199d0d4b78f5390120406b7db" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack.</source>
          <target state="translated">arpackの最大反復回数を指定します。Noneの場合はarpackが最適な値を選択します。</target>
        </trans-unit>
        <trans-unit id="f374a3956c9375a530255caa54ee43ca08273ef7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; solver, the default value is 1000.</source>
          <target state="translated">共役勾配ソルバーの最大反復回数。「sparse_cg」および「lsqr」ソルバーの場合、デフォルト値はscipy.sparse.linalgによって決定されます。「サグ」ソルバーの場合、デフォルト値は1000です。</target>
        </trans-unit>
        <trans-unit id="11f341e8f36dbf9c1af7cfa8e66dcc9686e34f6b" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For the &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; and saga solver, the default value is 1000.</source>
          <target state="translated">共役勾配ソルバーの最大反復回数。'sparse_cg'および 'lsqr'ソルバーの場合、デフォルト値はscipy.sparse.linalgによって決定されます。'sag'およびsagaソルバーの場合、デフォルト値は1000です。</target>
        </trans-unit>
        <trans-unit id="c5354ceb6cfff4ad460f2a35427697293dda616c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.</source>
          <target state="translated">共役勾配ソルバーの最大反復回数を指定します。デフォルト値は scipy.sparse.linalg によって決定されます。</target>
        </trans-unit>
        <trans-unit id="1e7dfd80e629f3bb34ea64892d98c69b612b79e1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for random sample selection.</source>
          <target state="translated">ランダムサンプル選択の最大反復回数。</target>
        </trans-unit>
        <trans-unit id="ecd130d87b8a2f3d02f709f684a50be372cae2c9" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the arpack solver. not used if eigen_solver == &amp;lsquo;dense&amp;rsquo;.</source>
          <target state="translated">arpackソルバーの最大反復回数。eigen_solver == 'dense'の場合は使用されません。</target>
        </trans-unit>
        <trans-unit id="9de38c6ff2395ad8506cb6d44f0cafffcf62c788" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the calculation of spatial median.</source>
          <target state="translated">空間中央値の計算のための最大反復回数。</target>
        </trans-unit>
        <trans-unit id="07f904d8faecfe25c803428c8e7a88d0070e3e3e" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the optimization. Should be at least 250.</source>
          <target state="translated">最適化の最大反復回数。少なくとも250である必要があります。</target>
        </trans-unit>
        <trans-unit id="dce17045503a4e7dfc48c40d90d1aeae843b7e1f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the solver.</source>
          <target state="translated">ソルバーの最大反復回数を指定します。</target>
        </trans-unit>
        <trans-unit id="23888143df53d4407b7d5db42e819ad8fc40bfb6" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations in the optimization.</source>
          <target state="translated">最適化の最大反復回数。</target>
        </trans-unit>
        <trans-unit id="e1c1739cc631f47e83bf7484461b685fd99cca3d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the SMACOF algorithm for a single run.</source>
          <target state="translated">SMACOFアルゴリズムの1回の実行における最大反復回数。</target>
        </trans-unit>
        <trans-unit id="7f1e71a3c23990192b71291657a3bae2f490d4ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm for a single run.</source>
          <target state="translated">1回の実行のためのk-meansアルゴリズムの最大反復回数。</target>
        </trans-unit>
        <trans-unit id="f0e6e9653318c3bc8385e39576298e438fdcd759" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm to run.</source>
          <target state="translated">実行するk-meansアルゴリズムの最大反復回数。</target>
        </trans-unit>
        <trans-unit id="368dd40a437636dbd3f559d65e7725494d2a9fb1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the optimization algorithm.</source>
          <target state="translated">最適化アルゴリズムの最大反復回数。</target>
        </trans-unit>
        <trans-unit id="63c06831f070b2a52428ee45e49cb4b88ea5a09d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.</source>
          <target state="translated">早期停止基準ヒューリスティックとは独立して停止するまでの完全なデータセットに対する最大反復回数.</target>
        </trans-unit>
        <trans-unit id="a1da02f682252ffa124e7544bc23e0152d7ce8c8" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations performed on each seed.</source>
          <target state="translated">各シードに対して実行される反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="ff75c0f1937b00a0d18d3456b10469179a13bfd5" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations run across all classes.</source>
          <target state="translated">すべてのクラスで実行される反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="5d873ede9710528d1162698e1b4121133119149c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations taken for the solvers to converge.</source>
          <target state="translated">ソルバーが収束するまでの最大反復回数.</target>
        </trans-unit>
        <trans-unit id="0f2d5ee22794f6faeed2ed56167e69832301d9fb" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that &lt;code&gt;scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)&lt;/code&gt; should run for.</source>
          <target state="translated">&lt;code&gt;scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)&lt;/code&gt; を実行する必要がある反復の最大数。</target>
        </trans-unit>
        <trans-unit id="7899fd5b3a78738f0401cfc3b35e8ce4d2309778" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by &lt;code&gt;is_data_valid&lt;/code&gt; or invalid models defined by &lt;code&gt;is_model_valid&lt;/code&gt;.</source>
          <target state="translated">ゼロのインライアまたは &lt;code&gt;is_data_valid&lt;/code&gt; で定義された無効なデータ、または &lt;code&gt;is_model_valid&lt;/code&gt; で定義された無効なモデルが見つかったためにスキップできる反復の最大数。</target>
        </trans-unit>
        <trans-unit id="e7fba252520d1990cf2d4eb5716def2f32bbae99" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for.</source>
          <target state="translated">scipy.optimize.fmin_l_bfgs_bが実行すべき反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="4e1098501827a192b62ebb4f1cab51c2d422cf00" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 場合に実行する反復の最大数。</target>
        </trans-unit>
        <trans-unit id="f8b9c126c90c88245cf150185cad9a95bb9ec437" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm='lasso_cd'&lt;/code&gt; or &lt;code&gt;lasso_lars&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;algorithm='lasso_cd'&lt;/code&gt; または &lt;code&gt;lasso_lars&lt;/code&gt; の場合に実行する最大反復回数。</target>
        </trans-unit>
        <trans-unit id="aa990833ac4f020e2d41da9d6169624866ecf0ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform in the Lars algorithm.</source>
          <target state="translated">Larsアルゴリズムで実行する反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="67f387c33c051b181969f68bc7a00e313e621f7a" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform when solving the lasso problem.</source>
          <target state="translated">投げ縄問題を解く際に実行する反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="d0f4ce7794b613699161c8c6e0e45882e1e59e63" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform, set to infinity for no limit.</source>
          <target state="translated">実行する反復回数の最大値で、無限大に設定されています。</target>
        </trans-unit>
        <trans-unit id="6484f135db2cde17daa0042bcd9839216d734460" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform.</source>
          <target state="translated">実行する反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="db2ca83257c5e157920232d66349b60febf20184" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform. Can be used for early stopping.</source>
          <target state="translated">実行する反復回数の最大値。早期停止に使用できる。</target>
        </trans-unit>
        <trans-unit id="3ae0883a212da2486dca35b829a405dbbd2b8c29" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.</source>
          <target state="translated">最適化を中止するまでに進捗がない場合の最大反復回数を指定します。進捗状況は 50 回の反復ごとにのみチェックされるため、この値は 50 の次の倍数に丸められます。</target>
        </trans-unit>
        <trans-unit id="5919ba2c46521b64537304f167c62803da4391df" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet.</source>
          <target state="translated">クラスタリング操作が終了する前に(そのシードポイントに対して)シードポイントごとに、まだ収束していない場合の最大反復回数。</target>
        </trans-unit>
        <trans-unit id="3fcea6ff6580050eb63d7142d23e7d7896de7626" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations.</source>
          <target state="translated">反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="2bd71b5c83b9f5da0d4a94baa31a035271906ce7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300</source>
          <target state="translated">反復の最大数。デフォルトは300</target>
        </trans-unit>
        <trans-unit id="6740f9eed55c5399b0f5fe47513d42802b2d72aa" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300.</source>
          <target state="translated">反復の最大数。デフォルトは300です。</target>
        </trans-unit>
        <trans-unit id="78adce28aba1dcec634d8308251161f963f008c1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Should be greater than or equal to 1.</source>
          <target state="translated">反復回数の最大値。1以上でなければなりません。</target>
        </trans-unit>
        <trans-unit id="1d3427b734648d0ef9c00f4282011f095b732bba" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. The solver iterates until convergence (determined by &amp;lsquo;tol&amp;rsquo;) or this number of iterations. For stochastic solvers (&amp;lsquo;sgd&amp;rsquo;, &amp;lsquo;adam&amp;rsquo;), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.</source>
          <target state="translated">反復の最大数。ソルバーは、収束（ 'tol'によって決定）またはこの反復回数まで反復します。確率的ソルバー（ 'sgd'、 'adam'）の場合、これはエポック数（各データポイントが使用される回数）を決定し、勾配ステップの数ではないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="647156dc0e4267e82660e321d855d0ab57ee6ce8" translate="yes" xml:space="preserve">
          <source>Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices.</source>
          <target state="translated">計算効率のために,分位数を推定するために使用される最大サンプル数.値が同一の疎な行列と密な行列では,サブサンプリング手順が異なる場合があることに注意してください.</target>
        </trans-unit>
        <trans-unit id="415a2ec1c451656db8760ffe077b89b191d3a2b3" translate="yes" xml:space="preserve">
          <source>Maximum numbers of iterations to perform, therefore maximum features to include. 10% of &lt;code&gt;n_features&lt;/code&gt; but at least 5 if available.</source>
          <target state="translated">実行する反復の最大数、つまり含める機能の最大数。 &lt;code&gt;n_features&lt;/code&gt; の 10％、使用可能な場合は少なくとも5つ。</target>
        </trans-unit>
        <trans-unit id="631012abffd401f8346d1251260aa1bdd321bf8a" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt; or the number of nodes in the path with &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">各反復での共分散の最大値（絶対値）。 &lt;code&gt;n_alphas&lt;/code&gt; は、 &lt;code&gt;max_iter&lt;/code&gt; 、 &lt;code&gt;n_features&lt;/code&gt; 、またはパスが &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt; であるパスのノード数のいずれか小さい方です。</target>
        </trans-unit>
        <trans-unit id="705f01a5b973480d43f7edb8b4f1d8b46afffacc" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt;, or the number of nodes in the path with correlation greater than &lt;code&gt;alpha&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">各反復での共分散の最大値（絶対値）。 &lt;code&gt;n_alphas&lt;/code&gt; は、 &lt;code&gt;max_iter&lt;/code&gt; 、 &lt;code&gt;n_features&lt;/code&gt; 、または &lt;code&gt;alpha&lt;/code&gt; より大きい相関を持つパスのノード数のいずれか小さい方です。</target>
        </trans-unit>
        <trans-unit id="62989d4a3b259ca439d6192faa2834aa0e75a2e0" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;n_nonzero_coefs&lt;/code&gt; or &lt;code&gt;n_features&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">各反復での共分散の最大値（絶対値）。 &lt;code&gt;n_alphas&lt;/code&gt; は、 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; または &lt;code&gt;n_features&lt;/code&gt; のいずれか小さい方です。</target>
        </trans-unit>
        <trans-unit id="2519f5f4a0d8bcad0dcee25fb3c2cfe0d8efda04" translate="yes" xml:space="preserve">
          <source>Maximum possible imputed value. Broadcast to shape (n_features,) if scalar. If array-like, expects shape (n_features,), one max value for each feature. &lt;code&gt;None&lt;/code&gt; (default) is converted to np.inf.</source>
          <target state="translated">可能な最大の代入値。スカラーの場合、シェイプ（n_features、）にブロードキャストします。配列のような場合、形状（n_features、）、各フィーチャに1つの最大値が必要です。 &lt;code&gt;None&lt;/code&gt; （デフォルト）はnp.infに変換されます。</target>
        </trans-unit>
        <trans-unit id="e56eb85aade57d415023e1a3a8ae03f5b942a0cd" translate="yes" xml:space="preserve">
          <source>Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">インライアとして分類されるデータサンプルの最大残差。デフォルトでは、しきい値はターゲット値 &lt;code&gt;y&lt;/code&gt; のMAD（中央絶対偏差）として選択されます。</target>
        </trans-unit>
        <trans-unit id="6dd6334c9c1bb29cde2ace0d7ca9c039e7de14bd" translate="yes" xml:space="preserve">
          <source>Maximum size for a single training set.</source>
          <target state="translated">1つのトレーニングセットの最大サイズ。</target>
        </trans-unit>
        <trans-unit id="3feffec2bfb871f0142dbd2d75d410b437245309" translate="yes" xml:space="preserve">
          <source>Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.</source>
          <target state="translated">サンプル上のXの最大二乗和。SAGソルバーでのみ使用されます。Noneの場合は、すべてのサンプルを対象に計算されます。クロスバリデーションを高速化するために、この値を事前に計算しておく必要があります。</target>
        </trans-unit>
        <trans-unit id="229948f9503f6467f2a53d61f4254093e7ca3738" translate="yes" xml:space="preserve">
          <source>Maximum step size (regularization). Defaults to 1.0.</source>
          <target state="translated">最大ステップサイズ(正則化)。デフォルトは1.0です。</target>
        </trans-unit>
        <trans-unit id="843c61e3ff0f74449c911dbb02faa43821e29850" translate="yes" xml:space="preserve">
          <source>Maximum value of a bicluster.</source>
          <target state="translated">二重クラスタの最大値。</target>
        </trans-unit>
        <trans-unit id="ce9a39e13a20e687ebff9fcf4496175bdfa0afbb" translate="yes" xml:space="preserve">
          <source>Maximum value of input array &lt;code&gt;X_&lt;/code&gt; for right bound.</source>
          <target state="translated">右境界の入力配列 &lt;code&gt;X_&lt;/code&gt; の最大値。</target>
        </trans-unit>
        <trans-unit id="9fa80bb15d05b082522b43fcb83b05beb6f022c6" translate="yes" xml:space="preserve">
          <source>May be the string &amp;ldquo;jaccard&amp;rdquo; to use the Jaccard coefficient, or any function that takes four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows, b_columns).</source>
          <target state="translated">Jaccard係数を使用する文字列「jaccard」、または4つの引数を取る関数で、それぞれが1dインジケーターベクトル（a_rows、a_columns、b_rows、b_columns）です。</target>
        </trans-unit>
        <trans-unit id="4fad1e9d11d435bd5f0db307b217272d94f19197" translate="yes" xml:space="preserve">
          <source>May contain any subset of (&amp;lsquo;headers&amp;rsquo;, &amp;lsquo;footers&amp;rsquo;, &amp;lsquo;quotes&amp;rsquo;). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata.</source>
          <target state="translated">（「ヘッダー」、「フッター」、「引用」）のサブセットを含めることができます。これらはそれぞれ、ニュースグループの投稿から検出および削除される種類のテキストであり、分類子がメタデータに適合しすぎるのを防ぎます。</target>
        </trans-unit>
        <trans-unit id="5b986034f147ad60f742da340ca1bd20b5f08ce7" translate="yes" xml:space="preserve">
          <source>McCullagh, Peter; Nelder, John (1989). Generalized Linear Models, Second Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.</source>
          <target state="translated">マッカラー,ピーター;ネルダー,ジョン(1989).一般化線形モデル、第2版。ボカラトン。チャップマン・アンド・ホール/CRC.ISBN 0-412-31760-5。</target>
        </trans-unit>
        <trans-unit id="72639b42074abb6c8ea20684337c75e653e13eb2" translate="yes" xml:space="preserve">
          <source>McSherry, F., &amp;amp; Najork, M. (2008, March). Computing information retrieval performance measures efficiently in the presence of tied scores. In European conference on information retrieval (pp. 414-421). Springer, Berlin, Heidelberg.</source>
          <target state="translated">McSherry、F。、およびNajork、M。（2008年3月）。同点のスコアが存在する場合、情報検索のパフォーマンス測定を効率的に計算します。情報検索に関するヨーロッパの会議（pp.414-421）。スプリンガー、ベルリン、ハイデルベルク。</target>
        </trans-unit>
        <trans-unit id="4f0935dfe9ab3f30e90c245d2338ed727682177f" translate="yes" xml:space="preserve">
          <source>Mean Absolute Error:</source>
          <target state="translated">平均絶対誤差。</target>
        </trans-unit>
        <trans-unit id="90a417c7a65441a9ebd4508d554460a1437266a0" translate="yes" xml:space="preserve">
          <source>Mean Gamma deviance regression loss.</source>
          <target state="translated">平均ガンマ・デビアンス回帰損失。</target>
        </trans-unit>
        <trans-unit id="b5e71e9559d855f0bb974ab566c48b140de9b95a" translate="yes" xml:space="preserve">
          <source>Mean Poisson deviance regression loss.</source>
          <target state="translated">平均ポアソンデビアンス回帰損失。</target>
        </trans-unit>
        <trans-unit id="007ffde203dd83c4c710b22da1cbe0b3700a98d1" translate="yes" xml:space="preserve">
          <source>Mean Silhouette Coefficient for all samples.</source>
          <target state="translated">全サンプルの平均シルエット係数。</target>
        </trans-unit>
        <trans-unit id="2762f10f75116f5e4a70c10eb14cf5f478eb498f" translate="yes" xml:space="preserve">
          <source>Mean Squared Error:</source>
          <target state="translated">平均二乗誤差。</target>
        </trans-unit>
        <trans-unit id="04bada6d1e51be3ec0e69f413da51c5f62d2f9fc" translate="yes" xml:space="preserve">
          <source>Mean Tweedie deviance regression loss.</source>
          <target state="translated">平均ツイーディー偏差値回帰損失。</target>
        </trans-unit>
        <trans-unit id="43559adecf21dbddbbe17afba52b16b4a67e4402" translate="yes" xml:space="preserve">
          <source>Mean absolute error regression loss</source>
          <target state="translated">平均絶対誤差回帰損失</target>
        </trans-unit>
        <trans-unit id="640f16564a014e166473dc43ba616e7e2ca59c7f" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) w.r.t. y.</source>
          <target state="translated">self.predict(X)の平均精度 w.r.t.y.</target>
        </trans-unit>
        <trans-unit id="ec9517dd8574c2a6b45d6a307e2a503f5e6d275d" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) wrt. y.</source>
          <target state="translated">self.predict(X)の平均精度(y.に対する)。</target>
        </trans-unit>
        <trans-unit id="7493a61b1729d0e0247689ac97555930f03706df" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator</source>
          <target state="translated">best_estimatorの平均交差検証スコア</target>
        </trans-unit>
        <trans-unit id="140100875ffefa42eddb6a75afe4c55e05443030" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator.</source>
          <target state="translated">best_estimatorの平均交差検証スコア。</target>
        </trans-unit>
        <trans-unit id="905638cd4671af1b6b218d93d25fb805a7548993" translate="yes" xml:space="preserve">
          <source>Mean of feature importance over &lt;code&gt;n_repeats&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;n_repeats&lt;/code&gt; よりも特徴の重要性の平均。</target>
        </trans-unit>
        <trans-unit id="dd2a669704ec2ab03a0e4ca8e2abc9c2d42f71c6" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution a query points</source>
          <target state="translated">予測分布の平均値 a クエリポイント</target>
        </trans-unit>
        <trans-unit id="b609d0f7d96a7b9c8e60baf85298ffc173cbc6f7" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution of query points.</source>
          <target state="translated">クエリ点の予測分布の平均。</target>
        </trans-unit>
        <trans-unit id="e7f5a133eabd3f3a470b8b7cb54eeb045b64973a" translate="yes" xml:space="preserve">
          <source>Mean or median or quantile of the training targets or constant value given by the user.</source>
          <target state="translated">学習目標の平均値または中央値または分位値、またはユーザーが与えた定数値。</target>
        </trans-unit>
        <trans-unit id="9ede03e41402c8eec43dd01264f8f822af5fb92b" translate="yes" xml:space="preserve">
          <source>Mean shift clustering aims to discover &amp;ldquo;blobs&amp;rdquo; in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.</source>
          <target state="translated">平均シフトクラスタリングは、滑らかな密度のサンプルで「ブロブ」を発見することを目的としています。これはセントロイドベースのアルゴリズムであり、セントロイドの候補を特定の領域内のポイントの平均になるように更新することで機能します。次に、これらの候補は、後処理段階でフィルター処理されて、ほぼ重複するものを排除して、重心の最終セットを形成します。</target>
        </trans-unit>
        <trans-unit id="08b2e6d37eec1f5ceae1376ffba9071609b6547f" translate="yes" xml:space="preserve">
          <source>Mean shift clustering using a flat kernel.</source>
          <target state="translated">フラットカーネルを用いた平均シフトクラスタリング。</target>
        </trans-unit>
        <trans-unit id="4484f1a9abfaeee06549ff0a6b75712b44fd35f2" translate="yes" xml:space="preserve">
          <source>Mean square error for the test set on each fold, varying l1_ratio and alpha.</source>
          <target state="translated">l1_ratio と alpha を変化させた各折り目のテストセットの平均二乗誤差.</target>
        </trans-unit>
        <trans-unit id="4a0031a2d59450a58aeaa638a064cb2e2c9a0da5" translate="yes" xml:space="preserve">
          <source>Mean squared error regression loss</source>
          <target state="translated">平均二乗誤差回帰損失</target>
        </trans-unit>
        <trans-unit id="831bfb250ab69b773c25fae60f230a00bfdc7239" translate="yes" xml:space="preserve">
          <source>Mean squared logarithmic error regression loss</source>
          <target state="translated">平均二乗対数誤差回帰損失</target>
        </trans-unit>
        <trans-unit id="2db7f6881ab1082c632822482db18fa9fc34ed90" translate="yes" xml:space="preserve">
          <source>Mean-shift</source>
          <target state="translated">Mean-shift</target>
        </trans-unit>
        <trans-unit id="896bb25ed00af769d9d9cdb21af7655bd0a22654" translate="yes" xml:space="preserve">
          <source>Measure and plot the results</source>
          <target state="translated">結果を測定し、プロットする</target>
        </trans-unit>
        <trans-unit id="df21241945c8fd60618f888d81f315be3f7af674" translate="yes" xml:space="preserve">
          <source>Measure the similarity of two clusterings of a set of points.</source>
          <target state="translated">点の集合の2つのクラスタリングの類似度を測定します。</target>
        </trans-unit>
        <trans-unit id="e49da6a85d81735a7b28ef79fc256dec989d2443" translate="yes" xml:space="preserve">
          <source>Measurement errors in X</source>
          <target state="translated">Xの測定誤差</target>
        </trans-unit>
        <trans-unit id="471fba4dfe2d4f61d0ae5efc77acb722551abb7b" translate="yes" xml:space="preserve">
          <source>Measurement errors in y</source>
          <target state="translated">yの測定誤差</target>
        </trans-unit>
        <trans-unit id="d59aa4a9911bb1573c0ba0a2779ea96a4989f27f" translate="yes" xml:space="preserve">
          <source>MedInc median income in block</source>
          <target state="translated">MedInc ブロック中央値所得</target>
        </trans-unit>
        <trans-unit id="ca6bd4b635d61f1c13fd0394e55000bb30738881" translate="yes" xml:space="preserve">
          <source>Median absolute error output is non-negative floating point. The best value is 0.0. Read more in the &lt;a href=&quot;../model_evaluation#median-absolute-error&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">中央絶対誤差出力は非負の浮動小数点です。最適な値は0.0です。詳細については、&lt;a href=&quot;../model_evaluation#median-absolute-error&quot;&gt;ユーザーガイド&lt;/a&gt;をご覧ください。</target>
        </trans-unit>
        <trans-unit id="bb82014fc42479d50d7886c5d05c20bd8db97f56" translate="yes" xml:space="preserve">
          <source>Median absolute error regression loss</source>
          <target state="translated">中央値絶対誤差回帰損失</target>
        </trans-unit>
        <trans-unit id="9e7082d8eb8f2409deaa71605e5d6dbf5b190217" translate="yes" xml:space="preserve">
          <source>Medium &lt;code&gt;n_samples&lt;/code&gt;, small &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">中規模の &lt;code&gt;n_samples&lt;/code&gt; 、小規模の &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="da13fe6da16d1b0d3601ee1416010215c9da2b3d" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;coef_&lt;/code&gt; holds the weights \(w\)</source>
          <target state="translated">メンバー &lt;code&gt;coef_&lt;/code&gt; は重み\（w \）を保持します</target>
        </trans-unit>
        <trans-unit id="b26ca7073281a8f4da3f64aafb4932ce0dc723cc" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds \(b\)</source>
          <target state="translated">メンバー &lt;code&gt;intercept_&lt;/code&gt; _は\（b \）を保持します</target>
        </trans-unit>
        <trans-unit id="8e7e4ea63f467ef992e1b5515c3662fd092327fd" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds the intercept (aka offset or bias):</source>
          <target state="translated">メンバー &lt;code&gt;intercept_&lt;/code&gt; は切片（別名オフセットまたはバイアス）を保持します。</target>
        </trans-unit>
        <trans-unit id="b6ef7f0fdf735583a61dbfc359227479e43bf842" translate="yes" xml:space="preserve">
          <source>Memmapping mode for numpy arrays passed to workers. See &amp;lsquo;max_nbytes&amp;rsquo; parameter documentation for more details.</source>
          <target state="translated">ワーカーに渡されるnumpy配列のメマッピングモード。詳細については、「max_nbytes」パラメータのドキュメントをご覧ください。</target>
        </trans-unit>
        <trans-unit id="424b610ebd2af23e4bb8a29dcabbc0551e9c3d87" translate="yes" xml:space="preserve">
          <source>Memory consumption for large sample sizes</source>
          <target state="translated">大規模なサンプルサイズのためのメモリ消費</target>
        </trans-unit>
        <trans-unit id="5418f36b831a9c825f5d841c5ee3b1bdb2dd3e28" translate="yes" xml:space="preserve">
          <source>Meta-estimator to regress on a transformed target.</source>
          <target state="translated">変換されたターゲットに回帰するメタ推定器。</target>
        </trans-unit>
        <trans-unit id="79599678d3d5e2500fd2a7f727461a2dac0b6cd4" translate="yes" xml:space="preserve">
          <source>Meta-estimators for building composite models with transformers</source>
          <target state="translated">トランスを用いた複合モデル構築のためのメタ推定器</target>
        </trans-unit>
        <trans-unit id="d5a7b3579e10eeaa00ced1884380b174708caebd" translate="yes" xml:space="preserve">
          <source>Meta-transformer for selecting features based on importance weights.</source>
          <target state="translated">重要度に基づいて特徴を選択するためのメタ変換器。</target>
        </trans-unit>
        <trans-unit id="88306943fea7e76f9cd57cae0ea6d8b32d2e8434" translate="yes" xml:space="preserve">
          <source>Method</source>
          <target state="translated">Method</target>
        </trans-unit>
        <trans-unit id="bebb8fb7cc6768e9928f39fcd0184089c7f19d03" translate="yes" xml:space="preserve">
          <source>Method for initialization</source>
          <target state="translated">初期化のための方法</target>
        </trans-unit>
        <trans-unit id="3dad9226be4bd937f8a455ee0badad4ee6cceff1" translate="yes" xml:space="preserve">
          <source>Method for initialization of k-means algorithm; defaults to &amp;lsquo;k-means++&amp;rsquo;.</source>
          <target state="translated">k-meansアルゴリズムの初期化方法。デフォルトは 'k-means ++'です。</target>
        </trans-unit>
        <trans-unit id="2c3f3dc3ba37d9b2f1af18176cea15628b89a09c" translate="yes" xml:space="preserve">
          <source>Method for initialization, default to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">初期化の方法、デフォルトは「k-means ++」：</target>
        </trans-unit>
        <trans-unit id="62b8a3dc56fc8229948d624cc5b38920d22e2365" translate="yes" xml:space="preserve">
          <source>Method for initialization, defaults to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">初期化の方法、デフォルトは「k-means ++」：</target>
        </trans-unit>
        <trans-unit id="0e3feb124243dbfe777425d1e7de652a8a95432b" translate="yes" xml:space="preserve">
          <source>Method for initialization:</source>
          <target state="translated">初期化のためのメソッドです。</target>
        </trans-unit>
        <trans-unit id="3b1389e0e832a05337d6ccb31e50ea1425ca91a8" translate="yes" xml:space="preserve">
          <source>Method name</source>
          <target state="translated">メソッド名</target>
        </trans-unit>
        <trans-unit id="f2ddbb16b4269f001b2886168e6ee73674985a74" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;.</source>
          <target state="translated">特異ベクトルを正規化してバイクラスターに変換する方法。'scale'、 'bistochastic'、または 'log'のいずれかである可能性があります。著者は「ログ」の使用を推奨しています。ただし、データがスパースの場合、ログの正規化は機能しません。そのため、デフォルトは「二重確率」です。</target>
        </trans-unit>
        <trans-unit id="b78ea13fd7ce3e01d82dac91ffffedca9d6a516f" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;. CAUTION: if &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt;, the data must not be sparse.</source>
          <target state="translated">特異ベクトルをバイクラスターに正規化および変換する方法。'scale'、 'bistochastic'、または 'log'のいずれかになります。著者は「ログ」の使用を推奨しています。ただし、データがスパースの場合、ログの正規化は機能しません。そのため、デフォルトは 'bistochastic'です。注意： &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt; 場合、データはスパースであってはなりません。</target>
        </trans-unit>
        <trans-unit id="7e7b59d1db0b41f1f7de6a768474fa98a959edfd" translate="yes" xml:space="preserve">
          <source>Method to use in finding shortest path.</source>
          <target state="translated">最短パスを見つける際に使用する方法。</target>
        </trans-unit>
        <trans-unit id="46674c498c855af96974ed544b15ae6396d6f74f" translate="yes" xml:space="preserve">
          <source>Method used to encode the transformed result.</source>
          <target state="translated">変換された結果をエンコードするために使用されるメソッド。</target>
        </trans-unit>
        <trans-unit id="155758829048f282b684f453070e5e32e8a3b098" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: &amp;lsquo;nndsvd&amp;rsquo; if n_components &amp;lt; n_features, otherwise random. Valid options:</source>
          <target state="translated">プロシージャの初期化に使用されるメソッド。デフォルト：n_components &amp;lt;n_featuresの場合は「nndsvd」、それ以外の場合はランダム。有効なオプション：</target>
        </trans-unit>
        <trans-unit id="2babbe784e2e75bf6bc5c5da588a447b4ed201c9" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: None.</source>
          <target state="translated">プロシージャを初期化するために使われるメソッド。既定値。なし。</target>
        </trans-unit>
        <trans-unit id="7506fd4c85f1f80b13ff77585a3eea447916c5af" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: None. Valid options:</source>
          <target state="translated">プロシージャを初期化するために使われるメソッド。既定値。なし。有効なオプション。</target>
        </trans-unit>
        <trans-unit id="6e9cbcdd1d5058381751f008677bca0c5dd1dd9b" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;a href=&quot;#sklearn.decomposition.LatentDirichletAllocation.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="translated">&lt;code&gt;_component&lt;/code&gt; の更新に使用されるメソッド。&lt;a href=&quot;#sklearn.decomposition.LatentDirichletAllocation.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;法でのみ使用されます。一般に、データサイズが大きい場合、オンライン更新はバッチ更新よりもはるかに高速になります。</target>
        </trans-unit>
        <trans-unit id="2c32a8fabfe7118c5a18a023b129a09f5774d869" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;code&gt;fit&lt;/code&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="translated">&lt;code&gt;_component&lt;/code&gt; の更新に使用されるメソッド。 &lt;code&gt;fit&lt;/code&gt; 法でのみ使用されます。一般に、データサイズが大きい場合、オンライン更新はバッチ更新よりもはるかに高速になります。</target>
        </trans-unit>
        <trans-unit id="7e4ac6803c9159c694f63d089cb06b2519c16aba" translate="yes" xml:space="preserve">
          <source>Methods</source>
          <target state="translated">Methods</target>
        </trans-unit>
        <trans-unit id="8de2b023f4bb12cf6f4720283ae55f1dda2214ee" translate="yes" xml:space="preserve">
          <source>Methods called for each base estimator. It can be:</source>
          <target state="translated">各基底推定器に呼び出されるメソッド とすることができます。</target>
        </trans-unit>
        <trans-unit id="66a03315f429c2912a3083f4347509dcbe2b4de3" translate="yes" xml:space="preserve">
          <source>Metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.</source>
          <target state="translated">距離計算に使用するメトリック。scikit-learnやscipy.spatial.distanceの任意のメトリックを使用することができます。</target>
        </trans-unit>
        <trans-unit id="a01ea489bdc9c9d6a182edc027242ff94374f848" translate="yes" xml:space="preserve">
          <source>Metric used to compute distances to neighbors.</source>
          <target state="translated">隣人との距離を計算するために使用されるメトリック。</target>
        </trans-unit>
        <trans-unit id="223dc067df313c03c1e792b17b98d4c951add488" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;ldquo;precomputed&amp;rdquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted. If &amp;ldquo;precomputed&amp;rdquo;, a distance matrix (instead of a similarity matrix) is needed as input for the fit method.</source>
          <target state="translated">リンケージの計算に使用されるメトリック。「ユークリッド」、「l1」、「l2」、「マンハッタン」、「コサイン」、または「事前計算」にすることができます。リンケージが「ワード」の場合、「ユークリッド」のみが受け入れられます。「事前計算」されている場合、近似法の入力として（類似度行列ではなく）距離行列が必要です。</target>
        </trans-unit>
        <trans-unit id="ef01ecfb88c8d650a45a85cec9ebc18d89f4ecbc" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;lsquo;precomputed&amp;rsquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted.</source>
          <target state="translated">リンケージの計算に使用されるメトリック。「ユークリッド」、「l1」、「l2」、「マンハッタン」、「コサイン」、「事前計算済み」のいずれかです。リンケージが「ワード」の場合、「ユークリッド」のみが受け入れられます。</target>
        </trans-unit>
        <trans-unit id="b996dbf9b464efe667f55d3c7b947b9e2ffb345f" translate="yes" xml:space="preserve">
          <source>Metrics available for various machine learning tasks are detailed in sections below.</source>
          <target state="translated">様々な機械学習タスクで利用可能なメトリクスは、以下のセクションで詳細に説明されています。</target>
        </trans-unit>
        <trans-unit id="6333551e93ef2383df0508df89bd1ca423b74bc9" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping, &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;, 2001.</source>
          <target state="translated">Michael E. Tipping、&lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;スパースベイジアン学習と関連性ベクトルマシン&lt;/a&gt;、2001年。</target>
        </trans-unit>
        <trans-unit id="276b36ad13c4507935dcfa6095085df1bf048be3" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping: &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;</source>
          <target state="translated">マイケルE.ティッピング：&lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;スパースベイジアン学習と関連性ベクトルマシン&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee22c86ee428b82d33b13bdebced2deed71d63a1" translate="yes" xml:space="preserve">
          <source>Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</source>
          <target state="translated">マイケル・マーシャル (MARSHALL%PLU@io.arc.nasa.gov)</target>
        </trans-unit>
        <trans-unit id="f33a348553a7d85d27424bb525b1eca4fb8a5155" translate="yes" xml:space="preserve">
          <source>MinMaxScaler</source>
          <target state="translated">MinMaxScaler</target>
        </trans-unit>
        <trans-unit id="6fad9f3e5fbaefddf87807ab8e89f2398827a6e0" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering</source>
          <target state="translated">ミニバッチK-Meansクラスタリング</target>
        </trans-unit>
        <trans-unit id="f81af70401b9fbaba1cc8f3d806f31408afbc851" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering.</source>
          <target state="translated">ミニバッチK-Meansクラスタリング。</target>
        </trans-unit>
        <trans-unit id="8a7343b748199980306d06faf24494c5fb233c16" translate="yes" xml:space="preserve">
          <source>Mini-batch Sparse Principal Components Analysis</source>
          <target state="translated">ミニバッチ疎な主成分分析</target>
        </trans-unit>
        <trans-unit id="4a04231399e807603297c55fa730ae6cac785e8b" translate="yes" xml:space="preserve">
          <source>Mini-batch dictionary learning</source>
          <target state="translated">ミニバッチ辞書学習</target>
        </trans-unit>
        <trans-unit id="b36d4bb746673223e9f3eddf90497433b367472a" translate="yes" xml:space="preserve">
          <source>Mini-batch sparse PCA (&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt;&lt;code&gt;MiniBatchSparsePCA&lt;/code&gt;&lt;/a&gt;) is a variant of &lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt;&lt;code&gt;SparsePCA&lt;/code&gt;&lt;/a&gt; that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.</source>
          <target state="translated">ミニバッチスパースPCA（&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt; &lt;code&gt;MiniBatchSparsePCA&lt;/code&gt; &lt;/a&gt;）はSparsePCAのバリアントで&lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt; &lt;code&gt;SparsePCA&lt;/code&gt; &lt;/a&gt;、高速ですが精度は低くなります。速度の向上は、指定された反復回数の間、一連の機能の小さなチャンクを反復することによって達成されます。</target>
        </trans-unit>
        <trans-unit id="acc629f9bc13af6fe4ccc30d47949b9a29f4708a" translate="yes" xml:space="preserve">
          <source>Minimal cost complexity pruning recursively finds the node with the &amp;ldquo;weakest link&amp;rdquo;. The weakest link is characterized by an effective alpha, where the nodes with the smallest effective alpha are pruned first. To get an idea of what values of &lt;code&gt;ccp_alpha&lt;/code&gt; could be appropriate, scikit-learn provides &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path&quot;&gt;&lt;code&gt;DecisionTreeClassifier.cost_complexity_pruning_path&lt;/code&gt;&lt;/a&gt; that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves.</source>
          <target state="translated">最小限のコストの複雑さのプルーニングは、「最も弱いリンク」を持つノードを再帰的に見つけます。最も弱いリンクは、有効アルファによって特徴付けられ、有効アルファが最小のノードが最初にプルーニングされます。 &lt;code&gt;ccp_alpha&lt;/code&gt; のどの値が適切であるかを知るために、scikit-learnは、剪定プロセスの各ステップで有効なアルファと対応する総葉不純物を返す&lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path&quot;&gt; &lt;code&gt;DecisionTreeClassifier.cost_complexity_pruning_path&lt;/code&gt; &lt;/a&gt;を提供します。アルファが増加すると、より多くの木が剪定され、葉の総不純物が増加します。</target>
        </trans-unit>
        <trans-unit id="8f8dae4007afe0299bfaa99f8bc74594de701fdf" translate="yes" xml:space="preserve">
          <source>Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of &lt;a href=&quot;#bre&quot; id=&quot;id2&quot;&gt;[BRE]&lt;/a&gt;. This algorithm is parameterized by \(\alpha\ge0\) known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, \(R_\alpha(T)\) of a given tree \(T\):</source>
          <target state="translated">最小限のコストの複雑さの剪定は、&lt;a href=&quot;#bre&quot; id=&quot;id2&quot;&gt;[BRE]の&lt;/a&gt;第3章で説明されているように、過剰適合を回避するためにツリーを剪定するために使用されるアルゴリズムです。このアルゴリズムは、複雑度パラメーターと呼ばれる\（\ alpha \ ge0 \）によってパラメーター化されます。複雑さパラメーターは、特定のツリー\（T \）のコスト-複雑さの尺度\（R_ \ alpha（T）\）を定義するために使用されます。</target>
        </trans-unit>
        <trans-unit id="338b69eb058f4b8de205ae0e6a0b364261aebe7e" translate="yes" xml:space="preserve">
          <source>Minimizes the objective function:</source>
          <target state="translated">目的関数を最小化します。</target>
        </trans-unit>
        <trans-unit id="84c971787220fb3e13d325cba22644ed7cfc6396" translate="yes" xml:space="preserve">
          <source>Minimizing Finite Sums with the Stochastic Average Gradient &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</source>
          <target state="translated">確率的平均勾配による有限和の最小化&lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="40d428add0b0bb2b15690324c7a65b1e95d21444" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant (MCD): robust estimator of covariance.</source>
          <target state="translated">最小共分散決定量(MCD):共分散のロバストな推定器。</target>
        </trans-unit>
        <trans-unit id="f0d923ebaec99475dba3ff68622a8b582426df2b" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant Estimator</source>
          <target state="translated">最小共分散決定量推定器</target>
        </trans-unit>
        <trans-unit id="acdf76216ef7494ca3a405d1a4760970f1dcb045" translate="yes" xml:space="preserve">
          <source>Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.</source>
          <target state="translated">パスに沿った最小相関。これはLassoの正則化パラメータαパラメータに対応します。</target>
        </trans-unit>
        <trans-unit id="45261c0e2275ffe7c782578b7e04c16d232cec64" translate="yes" xml:space="preserve">
          <source>Minimum number of candidates evaluated per estimator, assuming enough items meet the &lt;code&gt;min_hash_match&lt;/code&gt; constraint.</source>
          <target state="translated">十分なアイテムが &lt;code&gt;min_hash_match&lt;/code&gt; 制約を満たすと仮定して、推定量ごとに評価される候補の最小数。</target>
        </trans-unit>
        <trans-unit id="1f6032c543b0bedd4ef1a29303943c7332e81e08" translate="yes" xml:space="preserve">
          <source>Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt;, treated as a relative number &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt;) for &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt;. This is typically chosen as the minimal number of samples necessary to estimate the given &lt;code&gt;base_estimator&lt;/code&gt;. By default a &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; estimator is assumed and &lt;code&gt;min_samples&lt;/code&gt; is chosen as &lt;code&gt;X.shape[1] + 1&lt;/code&gt;.</source>
          <target state="translated">元のデータからランダムに選択されたサンプルの最小数。 &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt; サンプルの絶対数として扱われ、 &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt; 相対数 &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt; ）として扱われます。これは通常、指定された &lt;code&gt;base_estimator&lt;/code&gt; を推定するために必要なサンプルの最小数として選択されます。デフォルトでは、 &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; 推定器が想定され、 &lt;code&gt;min_samples&lt;/code&gt; は &lt;code&gt;X.shape[1] + 1&lt;/code&gt; として選択されます。</target>
        </trans-unit>
        <trans-unit id="7055b8e6fba3c22d096f09a773f8fa2a0f2c2a45" translate="yes" xml:space="preserve">
          <source>Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2). If &lt;code&gt;None&lt;/code&gt;, the value of &lt;code&gt;min_samples&lt;/code&gt; is used instead.</source>
          <target state="translated">OPTICSクラスター内のサンプルの最小数。絶対数またはサンプル数の一部として表されます（少なくとも2に丸められます）。 &lt;code&gt;None&lt;/code&gt; の場合、代わりに &lt;code&gt;min_samples&lt;/code&gt; の値が使用されます。</target>
        </trans-unit>
        <trans-unit id="143330e48296c9730a85d5042f3f4108bc450e1a" translate="yes" xml:space="preserve">
          <source>Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2). If &lt;code&gt;None&lt;/code&gt;, the value of &lt;code&gt;min_samples&lt;/code&gt; is used instead. Used only when &lt;code&gt;cluster_method='xi'&lt;/code&gt;.</source>
          <target state="translated">OPTICSクラスター内のサンプルの最小数。絶対数またはサンプル数の一部として表されます（少なくとも2に丸められます）。 &lt;code&gt;None&lt;/code&gt; の場合、代わりに &lt;code&gt;min_samples&lt;/code&gt; の値が使用されます。 &lt;code&gt;cluster_method='xi'&lt;/code&gt; の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="1c28e0e254bdc137eed30061d47cd3e6720cfe75" translate="yes" xml:space="preserve">
          <source>Minimum possible imputed value. Broadcast to shape (n_features,) if scalar. If array-like, expects shape (n_features,), one min value for each feature. &lt;code&gt;None&lt;/code&gt; (default) is converted to -np.inf.</source>
          <target state="translated">可能な最小の代入値。スカラーの場合、シェイプ（n_features、）にブロードキャストします。配列のような場合、形状（n_features、）、各フィーチャに1つの最小値が必要です。 &lt;code&gt;None&lt;/code&gt; （デフォルト）は-np.infに変換されます。</target>
        </trans-unit>
        <trans-unit id="3a6bb55043794a0a93e8ff0524f08e79cbc35225" translate="yes" xml:space="preserve">
          <source>Minimum value of a bicluster.</source>
          <target state="translated">バイクラスターの最小値。</target>
        </trans-unit>
        <trans-unit id="59e81ca4cbc76e95e029c93b9fa76bb8c2828a22" translate="yes" xml:space="preserve">
          <source>Minimum value of input array &lt;code&gt;X_&lt;/code&gt; for left bound.</source>
          <target state="translated">左境界の入力配列 &lt;code&gt;X_&lt;/code&gt; の最小値。</target>
        </trans-unit>
        <trans-unit id="2b5d457149fe5be167ed99387c99dd4725835fe8" translate="yes" xml:space="preserve">
          <source>MinkowskiDistance</source>
          <target state="translated">MinkowskiDistance</target>
        </trans-unit>
        <trans-unit id="8984ca78ae6f645a8da0469517e3e68a7c22986d" translate="yes" xml:space="preserve">
          <source>Mirroring the example above in grid search, we can specify a continuous random variable that is log-uniformly distributed between &lt;code&gt;1e0&lt;/code&gt; and &lt;code&gt;1e3&lt;/code&gt;:</source>
          <target state="translated">上記のグリッド検索の例を反映して、 &lt;code&gt;1e0&lt;/code&gt; と &lt;code&gt;1e3&lt;/code&gt; の間で対数均一に分布する連続確率変数を指定できます。</target>
        </trans-unit>
        <trans-unit id="f3547bc4550b1de5a83615a3b3bc38cc23770ce0" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;class_log_prior_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">MultinomialNBを線形モデルとして解釈するために、 &lt;code&gt;class_log_prior_&lt;/code&gt; をミラーリングします。</target>
        </trans-unit>
        <trans-unit id="7814bd45bfd54f380e5f0cd3f0461e627752ef7b" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;feature_log_prob_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">MultinomialNBを線形モデルとして解釈するための &lt;code&gt;feature_log_prob_&lt;/code&gt; をミラー化します。</target>
        </trans-unit>
        <trans-unit id="5f2cbd107037ed23248e5058a7a64cd6bae05468" translate="yes" xml:space="preserve">
          <source>Miscellaneous</source>
          <target state="translated">Miscellaneous</target>
        </trans-unit>
        <trans-unit id="0d2ecb69e7b12979a3e40a5ab8b5183911f1a3c3" translate="yes" xml:space="preserve">
          <source>Miscellaneous and introductory examples for scikit-learn.</source>
          <target state="translated">scikit-learnのための雑学・入門例。</target>
        </trans-unit>
        <trans-unit id="26655e342820eb1000893c259228858eef67a34d" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values</source>
          <target state="translated">欠落している属性値</target>
        </trans-unit>
        <trans-unit id="e446df504bb1a7ba9afc2f86aa7e483abdbc1937" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values:</source>
          <target state="translated">属性値がありません。</target>
        </trans-unit>
        <trans-unit id="905705cdb93f7b29194485a892d91da6c1cdc874" translate="yes" xml:space="preserve">
          <source>Missing Value Imputation</source>
          <target state="translated">欠落した値のインピュテーション</target>
        </trans-unit>
        <trans-unit id="67cc34b1cd58b9ef03f7ff376e8541732aac180b" translate="yes" xml:space="preserve">
          <source>Missing information</source>
          <target state="translated">欠落している情報</target>
        </trans-unit>
        <trans-unit id="0e00e76132a4d6b917901e5526c250a336a29108" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">欠落している値は、基本的な&lt;a href=&quot;../../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; &lt;/a&gt;を使用して、平均値、中央値、または最も頻度の高い値に置き換えることができます。</target>
        </trans-unit>
        <trans-unit id="391b82fea55e1ed8699fe6e91400e8d6055ab0df" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;. The median is a more robust estimator for data with high magnitude variables which could dominate results (otherwise known as a &amp;lsquo;long tail&amp;rsquo;).</source>
          <target state="translated">欠損値は、基本的な&lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; &lt;/a&gt;を使用して、平均値、中央値、または最も頻度の高い値に置き換えることができます。中央値は、結果を支配する可能性のある大きさの大きな変数（そうでなければ「ロングテール」として知られている）を含むデータのより堅牢な推定量です。</target>
        </trans-unit>
        <trans-unit id="7657a2d6545adb4955b10f53c4131bc5602e90eb" translate="yes" xml:space="preserve">
          <source>Missing values in the &amp;lsquo;data&amp;rsquo; are represented as NaN&amp;rsquo;s. Missing values in &amp;lsquo;target&amp;rsquo; are represented as NaN&amp;rsquo;s (numerical target) or None (categorical target)</source>
          <target state="translated">「データ」の欠損値はNaNとして表されます。'target'の欠損値は、NaN（数値ターゲット）またはNone（カテゴリーターゲット）として表されます</target>
        </trans-unit>
        <trans-unit id="6a6932c856f91eed3dd44780fa6b9fe69490c4b8" translate="yes" xml:space="preserve">
          <source>Mixin class for all bicluster estimators in scikit-learn</source>
          <target state="translated">scikit-learn のすべての双クラスター推定器のための Mixin クラス</target>
        </trans-unit>
        <trans-unit id="2c10e3ce37d297d342507e753914a98859330d14" translate="yes" xml:space="preserve">
          <source>Mixin class for all classifiers in scikit-learn.</source>
          <target state="translated">scikit-learnの全ての分類器のためのmixinクラス。</target>
        </trans-unit>
        <trans-unit id="5fa39e3354bc95759f1ac752182b15d93d7771cd" translate="yes" xml:space="preserve">
          <source>Mixin class for all cluster estimators in scikit-learn.</source>
          <target state="translated">scikit-learnのすべてのクラスタ推定器のためのMixinクラス。</target>
        </trans-unit>
        <trans-unit id="eb8addc65b16d7fa21479da43bfb0ac745b8fd54" translate="yes" xml:space="preserve">
          <source>Mixin class for all density estimators in scikit-learn.</source>
          <target state="translated">scikit-learnのすべての密度推定量のためのmixinクラス。</target>
        </trans-unit>
        <trans-unit id="6ac045bb154d5d0dbd1cc9d29eba911e1ea2781a" translate="yes" xml:space="preserve">
          <source>Mixin class for all regression estimators in scikit-learn.</source>
          <target state="translated">scikit-learn のすべての回帰推定器のための Mixin クラス。</target>
        </trans-unit>
        <trans-unit id="f73bd7177b7212616f9ae4cd764e784bac6a7ce1" translate="yes" xml:space="preserve">
          <source>Mixin class for all transformers in scikit-learn.</source>
          <target state="translated">scikit-learnのすべてのトランスフォーマーのためのミックスクラス。</target>
        </trans-unit>
        <trans-unit id="4d9a44acff48ccb4a2b026d4835ebe86a95495fc" translate="yes" xml:space="preserve">
          <source>Model Complexity Influence</source>
          <target state="translated">モデルの複雑さの影響</target>
        </trans-unit>
        <trans-unit id="9c567347b8af7d91b331f07af5f77ec0a361f505" translate="yes" xml:space="preserve">
          <source>Model Selection</source>
          <target state="translated">モデルの選択</target>
        </trans-unit>
        <trans-unit id="7d5e06ce8e5a0fb1e8e99aee47dbf3426de865fe" translate="yes" xml:space="preserve">
          <source>Model Selection Interface</source>
          <target state="translated">モデル選択インターフェース</target>
        </trans-unit>
        <trans-unit id="d9b7f2bb0f8fc0d29940e1aefd1565fcc5b449f7" translate="yes" xml:space="preserve">
          <source>Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.</source>
          <target state="translated">モデルブレンディング。アンサンブル法で、ある教師付き推定量の予測値を別の推定量の訓練に使用する場合。</target>
        </trans-unit>
        <trans-unit id="c3b027b1bc55171725d0853107d2cd63b70cf1b0" translate="yes" xml:space="preserve">
          <source>Model complexity</source>
          <target state="translated">モデルの複雑さ</target>
        </trans-unit>
        <trans-unit id="088cfdc97cd06f5c2647d7bc4d07170997a1804d" translate="yes" xml:space="preserve">
          <source>Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.</source>
          <target state="translated">scikit-learnでのモデル圧縮は、今のところ線形モデルのみを対象としています。この文脈では、モデルのスパース度(モデルベクトルに含まれるゼロ以外の座標の数)を制御したいことを意味します。一般的に、モデルのスパース性と入力データのスパース表現を組み合わせるのは良いアイデアです。</target>
        </trans-unit>
        <trans-unit id="12cb4d758358636a28aab0195639a05c4c6adf09" translate="yes" xml:space="preserve">
          <source>Model persistence</source>
          <target state="translated">モデルの永続性</target>
        </trans-unit>
        <trans-unit id="c38101ec23202ddb2bcf9ed4925ad6d1c9181511" translate="yes" xml:space="preserve">
          <source>Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in &lt;code&gt;CSR&lt;/code&gt; format), it is generally sufficient to not generate the relevant features, leaving their columns empty.</source>
          <target state="translated">モデルの再形成は、モデルに適合する利用可能な機能の一部のみを選択することで構成されます。言い換えると、モデルが学習段階で特徴を破棄した場合、入力からそれらを取り除くことができます。これにはいくつかの利点があります。まず、モデル自体のメモリ（つまり時間）のオーバーヘッドを削減します。また、前回の実行から保持する機能がわかったら、パイプライン内の明示的な機能選択コンポーネントを破棄することもできます。最後に、モデルによって破棄された機能を収集および構築しないことで、データアクセス層と機能抽出層の上流での処理時間とI / O使用量の削減に役立ちます。たとえば、生データがデータベースからのものである場合、クエリがより軽いレコードを返すようにすることで、シンプルで高速なクエリを記述したり、I / O使用量を削減したりできます。現時点では、整形はscikit-learnで手動で実行する必要があります。まばらな入力の場合（特に &lt;code&gt;CSR&lt;/code&gt; 形式）、通常は関連する機能を生成せず、列を空のままにしておくだけで十分です。</target>
        </trans-unit>
        <trans-unit id="28eeecfcba5c4e3a6c993b8bf2c6736730dfbd15" translate="yes" xml:space="preserve">
          <source>Model selection</source>
          <target state="translated">モデルの選択</target>
        </trans-unit>
        <trans-unit id="12aba00cd9b6d07b68e1c4795de07ac9d7b738d7" translate="yes" xml:space="preserve">
          <source>Model selection and evaluation using tools, such as &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;model_selection.cross_val_score&lt;/code&gt;&lt;/a&gt;, take a &lt;code&gt;scoring&lt;/code&gt; parameter that controls what metric they apply to the estimators evaluated.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt;や&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;model_selection.cross_val_score&lt;/code&gt; &lt;/a&gt;などのツールを使用したモデルの選択と評価は、評価される推定量に適用するメトリックを制御する &lt;code&gt;scoring&lt;/code&gt; パラメーターを取ります。</target>
        </trans-unit>
        <trans-unit id="855cd5c7a76f661266d80a6648a2f964caf6a19e" translate="yes" xml:space="preserve">
          <source>Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to &amp;ldquo;train&amp;rdquo; the parameters of the grid.</source>
          <target state="translated">さまざまなパラメーター設定を評価することによるモデル選択は、ラベル付きデータを使用してグリッドのパラメーターを「トレーニング」する方法と見なすことができます。</target>
        </trans-unit>
        <trans-unit id="89917070f2baaaaf3d7a4bc37b23fe9e19c05135" translate="yes" xml:space="preserve">
          <source>Model selection with Probabilistic PCA and Factor Analysis (FA)</source>
          <target state="translated">確率的PCAと因子分析(FA)によるモデル選択</target>
        </trans-unit>
        <trans-unit id="96389a0f02a3826ae6017961d0301435c315a116" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; for an analysis of these issues.</source>
          <target state="translated">ネストされたCVを使用しないモデル選択では、同じデータを使用してモデルパラメーターを調整し、モデルのパフォーマンスを評価します。したがって、情報がモデルに「漏れ」、データに過剰適合する可能性があります。この影響の大きさは、主にデータセットのサイズとモデルの安定性に依存します。Cawleyとタルボット参照&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1を&lt;/a&gt;これらの問題の分析のため。</target>
        </trans-unit>
        <trans-unit id="9c731d5ab6adf4a98df3f1383f23cf8f7eed3338" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; for an analysis of these issues.</source>
          <target state="translated">ネストされたCVのないモデル選択では、同じデータを使用してモデルパラメーターを調整し、モデルのパフォーマンスを評価します。したがって、情報がモデルに「漏洩」し、データが過剰に適合します。この影響の大きさは、主にデータセットのサイズとモデルの安定性に依存します。これらの問題の分析については、CawleyとTalbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="74392d3518eac75d4c193fc75b5f4945cf9be3f9" translate="yes" xml:space="preserve">
          <source>Model selection: choosing estimators and their parameters</source>
          <target state="translated">モデル選択:推定量とそのパラメータの選択</target>
        </trans-unit>
        <trans-unit id="ed1ba8eabae7e8d7de3d25705f2020f1428a1a11" translate="yes" xml:space="preserve">
          <source>Model the number of claims with a Poisson distribution, and the average claim amount per claim, also known as severity, as a Gamma distribution and multiply the predictions of both in order to get the total claim amount.</source>
          <target state="translated">請求件数をポアソン分布でモデル化し、重症度とも呼ばれる1件あたりの平均請求額をガンマ分布でモデル化し、両者の予測値を乗算して総請求額を求める。</target>
        </trans-unit>
        <trans-unit id="7c9e82e3e8aa374bb01cb1f0583b83a3f30a27a0" translate="yes" xml:space="preserve">
          <source>Model the total claim amount per exposure directly, typically with a Tweedie distribution of Tweedie power \(p \in (1, 2)\).</source>
          <target state="translated">曝露あたりの総請求額を直接モデル化して、典型的には、Tweedie powerのTweedie分布を用いて、Tweedie power \(p \in (1,2)Immunology)とする。</target>
        </trans-unit>
        <trans-unit id="ad79a801df8015a66d5501cf36f7ffcd2a41ddf8" translate="yes" xml:space="preserve">
          <source>Model validation</source>
          <target state="translated">モデルの検証</target>
        </trans-unit>
        <trans-unit id="44e8839819f969bae0c352bffb18b8d9aae37a64" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;https://matplotlib.org/basemap/&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">種の地理的分布のモデル化は、保全生物学における重要な問題です。この例では、過去の観測と14の環境変数を考慮して、2つの南米の哺乳類の地理的分布をモデル化します。肯定的な例しかないため（失敗した観測はありません）、この問題を密度推定問題としてキャストし、モデリングツールとして&lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt; &lt;/a&gt;を使用します。データセットはPhillipsらによって提供されています。al。（2006）。可能な場合、この例では&lt;a href=&quot;https://matplotlib.org/basemap/&quot;&gt;ベースマップ&lt;/a&gt;を使用して南アメリカの海岸線と国境をプロットします。</target>
        </trans-unit>
        <trans-unit id="04c7998384d3cc95ade6e27711f83c95af26806e" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;code&gt;OneClassSVM&lt;/code&gt; provided by the package &lt;code&gt;sklearn.svm&lt;/code&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">種の地理的分布をモデル化することは、保全生物学において重要な問題です。この例では、過去の観測と14の環境変数が与えられた2つの南アメリカの哺乳類の地理的分布をモデル化します。肯定的な例しかないため（失敗した観測はありません）、この問題を密度推定問題としてキャストし、 &lt;code&gt;OneClassSVM&lt;/code&gt; パッケージで提供される &lt;code&gt;sklearn.svm&lt;/code&gt; をモデリングツールとして使用します。データセットはフィリップスらによって提供されます。al。（2006）。可能な場合、この例では&lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;ベースマップ&lt;/a&gt;を使用して、南アメリカの海岸線と国境をプロットします。</target>
        </trans-unit>
        <trans-unit id="3432d8d9b44052d02847746ae08d1ac381072a89" translate="yes" xml:space="preserve">
          <source>Modified Huber: \(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))^2\) if \(y_i f(x_i) &amp;gt; 1\), and \(L(y_i, f(x_i)) = -4 y_i f(x_i)\) otherwise.</source>
          <target state="translated">変更されたHuber：\（L（y_i、f（x_i））= \ max（0、1-y_i f（x_i））^ 2 \）if \（y_i f（x_i）&amp;gt; 1 \）、および\（L（ y_i、f（x_i））= -4 y_i f（x_i）\）それ以外の場合。</target>
        </trans-unit>
        <trans-unit id="41be465b762359b2fa053c297894959404d2b4b5" translate="yes" xml:space="preserve">
          <source>Module &lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt;&lt;code&gt;sklearn.kernel_ridge&lt;/code&gt;&lt;/a&gt; implements kernel ridge regression.</source>
          <target state="translated">モジュール&lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt; &lt;code&gt;sklearn.kernel_ridge&lt;/code&gt; &lt;/a&gt;は、カーネルリッジ回帰を実装します。</target>
        </trans-unit>
        <trans-unit id="7c19bb73223842069c348f5ce2be56f6bdc47336" translate="yes" xml:space="preserve">
          <source>Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">勾配降下法の更新の勢い。0から1の間でなければなりません。solver= 'sgd'の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="08837633f9d15f78a0ca0401b5e29aae44a3cb25" translate="yes" xml:space="preserve">
          <source>Monotonic Constraints</source>
          <target state="translated">単調制約</target>
        </trans-unit>
        <trans-unit id="afdb29f8a2a5c8088f948d58228ab92ec4b8dbc8" translate="yes" xml:space="preserve">
          <source>Moosmann, F. and Triggs, B. and Jurie, F. &amp;ldquo;Fast discriminative visual codebooks using randomized clustering forests&amp;rdquo; NIPS 2007</source>
          <target state="translated">Moosmann、F.およびTriggs、B.およびJurie、F.「ランダム化されたクラスタリングフォレストを使用した高速の識別ビジュアルコードブ​​ック」NIPS 2007</target>
        </trans-unit>
        <trans-unit id="3f683b2b5fe59dc7e9963e0b844a2be959abe1bd" translate="yes" xml:space="preserve">
          <source>More details about the losses formulas can be found in the &lt;a href=&quot;../sgd#sgd-mathematical-formulation&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">損失の計算式の詳細については、&lt;a href=&quot;../sgd#sgd-mathematical-formulation&quot;&gt;ユーザーガイドを参照してください&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ea951c164724999b1e82491617fa7550c41c4ea4" translate="yes" xml:space="preserve">
          <source>More details can be found in the article &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Bayesian Interpolation&lt;/a&gt; by MacKay, David J. C.</source>
          <target state="translated">詳細については、MacKayによる&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;ベイズ補間&lt;/a&gt;、David JCの記事を参照してください。</target>
        </trans-unit>
        <trans-unit id="3dd8319d03052df7074a6e0643293f5dc781d510" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;http://scikit-learn.org/stable/modules/sgd.html&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="translated">詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/modules/sgd.html&quot;&gt;SGD&lt;/a&gt;のドキュメントをご覧ください。</target>
        </trans-unit>
        <trans-unit id="5be68ba5f49b8c23c2004cef7b8f1738816fd7ff" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="translated">詳細については、&lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;のドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="eeddded239db4ba79d40ed239189aa60eea25acb" translate="yes" xml:space="preserve">
          <source>More details on tools available for model selection can be found in the sections on &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;Cross-validation: evaluating estimator performance&lt;/a&gt; and &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">モデル選択のために利用できるツールについての詳細は、上のセクションに記載されています&lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;クロスバリデーション：推定性能を評価&lt;/a&gt;し、&lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;推定のハイパーパラメータをチューニング&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d431b615f9733586de025b4f9872bf1a8badc9bd" translate="yes" xml:space="preserve">
          <source>More formally, the responsibility of a sample \(k\) to be the exemplar of sample \(i\) is given by:</source>
          <target state="translated">より正式には、sample \(i\)の模範となるsampleの責任は、次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="e5fcb8eb05185b2ae6a7561ecfd54e7e78d1825d" translate="yes" xml:space="preserve">
          <source>More formally, we define a core sample as being a sample in the dataset such that there exist &lt;code&gt;min_samples&lt;/code&gt; other samples within a distance of &lt;code&gt;eps&lt;/code&gt;, which are defined as &lt;em&gt;neighbors&lt;/em&gt; of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of &lt;em&gt;their&lt;/em&gt; neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.</source>
          <target state="translated">より正式には、コアサンプルをデータセット内のサンプルとして定義し、コアサンプルの&lt;em&gt;近傍&lt;/em&gt;として定義されている &lt;code&gt;eps&lt;/code&gt; の距離内に他のサンプル &lt;code&gt;min_samples&lt;/code&gt; が存在するようにします。これは、コアサンプルがベクトル空間の密集した領域にあることを示しています。クラスタは、再帰的に、コア試料を採取コア試料である隣国のすべてを見つけること、の全て見つけることによって構築することができますコアサンプルのセットである&lt;em&gt;彼らの&lt;/em&gt;ようにコアサンプルで隣人を、と。クラスターには、非コアサンプルのセットもあります。これは、クラスター内のコアサンプルに隣接するサンプルですが、それ自体はコアサンプルではありません。直観的には、これらのサンプルはクラスターの周辺にあります。&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="39d3fe53d51c5218d78f036015830e2502c27f2b" translate="yes" xml:space="preserve">
          <source>More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc&amp;hellip;</source>
          <target state="translated">より一般的には、分類子の精度がランダムに近すぎる場合、おそらく問題が発生したことを意味します。機能が役立たない、ハイパーパラメーターが正しく調整されていない、分類子がクラスの不均衡などに苦しんでいます&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="63b4a4241c78c35e803f9a1e6a808d1813f2fb30" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="translated">詳細については、&lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipyのインストールページ&lt;/a&gt;と、Daniel Nouriからのこの&lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;ブログ投稿を&lt;/a&gt;ご覧ください。Debian / Ubuntuのインストール手順がわかりやすく説明されています。</target>
        </trans-unit>
        <trans-unit id="c3cc4720813506dbf91cef9b68d3a09728559160" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;https://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="translated">詳細については、&lt;a href=&quot;https://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipyのインストールページ&lt;/a&gt;と、Debian / Ubuntuのインストール手順を段階的に説明しているDanielNouriの&lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;ブログ投稿を&lt;/a&gt;ご覧ください。</target>
        </trans-unit>
        <trans-unit id="5d4e98f0a8d8595ea60691c38a1427dece6499fb" translate="yes" xml:space="preserve">
          <source>More metadata from OpenML</source>
          <target state="translated">OpenML のその他のメタデータ</target>
        </trans-unit>
        <trans-unit id="12db8232292ca8d1cf35bc6b9168f2c8b63d47ca" translate="yes" xml:space="preserve">
          <source>More precisely its the expectation of the target response after accounting for the initial model; partial dependence plots do not include the &lt;code&gt;init&lt;/code&gt; model.</source>
          <target state="translated">より正確には、初期モデルを考慮した後のターゲット応答の期待。部分依存プロットには、 &lt;code&gt;init&lt;/code&gt; モデルは含まれていません。</target>
        </trans-unit>
        <trans-unit id="73794f226fb348eb5da6ad63afeb16cb41727d95" translate="yes" xml:space="preserve">
          <source>More readable code, in particular since it avoids constructing list of arguments.</source>
          <target state="translated">特に引数のリストを構築する必要がないので、より読みやすいコードになりました。</target>
        </trans-unit>
        <trans-unit id="a22dda2285328f04695cb396d42b64909dfc0d90" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(X|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="translated">具体的には、線形・二次判別分析では、密度を持つ多変量ガウス分布としてモデル化されます。</target>
        </trans-unit>
        <trans-unit id="2b3cb69cb34819cd8834522c11758ddc50e1f474" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(x|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="translated">具体的には、線形・二次判別分析では、密度を持つ多変量ガウス分布としてモデル化されます。</target>
        </trans-unit>
        <trans-unit id="79ecb6d9275fdfeccdbd69d6aa3919b92952032e" translate="yes" xml:space="preserve">
          <source>Most commonly, disparities are set to \(\hat{d}_{ij} = b S_{ij}\).</source>
          <target state="translated">最も一般的なのは、格差を「\(hat{d}_{ij}=b S_{ij})」に設定することです。</target>
        </trans-unit>
        <trans-unit id="242544fc56d0b5c7cafd51d576a4b175219e1770" translate="yes" xml:space="preserve">
          <source>Most estimators based on nearest neighbors graphs now accept precomputed sparse graphs as input, to reuse the same graph for multiple estimator fits. To use this feature in a pipeline, one can use the &lt;code&gt;memory&lt;/code&gt; parameter, along with one of the two new transformers, &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt;&lt;code&gt;neighbors.KNeighborsTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt;&lt;code&gt;neighbors.RadiusNeighborsTransformer&lt;/code&gt;&lt;/a&gt;. The precomputation can also be performed by custom estimators to use alternative implementations, such as approximate nearest neighbors methods. See more details in the &lt;a href=&quot;../../modules/neighbors#neighbors-transformer&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">最近傍グラフに基づくほとんどの推定量は、事前に計算されたスパースグラフを入力として受け入れ、同じグラフを複数の推定量近似に再利用するようになりました。パイプラインでこの機能を使用するには、2つの新しいトランスフォーマー&lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt; &lt;code&gt;neighbors.KNeighborsTransformer&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;../../modules/generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt; &lt;code&gt;neighbors.RadiusNeighborsTransformer&lt;/code&gt; の&lt;/a&gt;いずれかとともに &lt;code&gt;memory&lt;/code&gt; パラメーターを使用できます。事前計算は、カスタム推定器によって実行して、近似最近傍法などの代替実装を使用することもできます。詳細については、&lt;a href=&quot;../../modules/neighbors#neighbors-transformer&quot;&gt;ユーザーガイドを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="44814fa11b1099a09b484290756712e9a8679a34" translate="yes" xml:space="preserve">
          <source>Most of the parameters are unchanged from &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;. One exception is the &lt;code&gt;max_iter&lt;/code&gt; parameter that replaces &lt;code&gt;n_estimators&lt;/code&gt;, and controls the number of iterations of the boosting process:</source>
          <target state="translated">ほとんどのパラメーターは、&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;から変更されていません。1つの例外は、 &lt;code&gt;n_estimators&lt;/code&gt; を置き換え、ブースティングプロセスの反復回数を制御する &lt;code&gt;max_iter&lt;/code&gt; パラメーターです。</target>
        </trans-unit>
        <trans-unit id="13411f05832555677b503d8db5e64b4930c99086" translate="yes" xml:space="preserve">
          <source>Most of the variance can be explained by a bell-shaped curve of width effective_rank: the low rank part of the singular values profile is:</source>
          <target state="translated">分散の大部分は、幅のベル型曲線 effect_rank:特異値プロファイルの低ランク部分で説明できます。</target>
        </trans-unit>
        <trans-unit id="9a311c70d6fa85e99fb6533c84253a4d2c760cf7" translate="yes" xml:space="preserve">
          <source>Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model.</source>
          <target state="translated">ほとんどのscikit-learnモデルは、コンパイルされたCython拡張機能か最適化されたコンピューティングライブラリで実装されているため、通常はかなり高速です。一方、多くの実世界のアプリケーションでは、特徴抽出プロセス(データベースの行やネットワークパケットなどの生データをnumpy配列に変換すること)が全体的な予測時間を左右します。例えば、ロイターのテキスト分類タスクでは、全体の準備(SGMLファイルの読み込みと解析、テキストのトークン化、共通のベクトル空間へのハッシュ化)には、選択したモデルにもよりますが、実際の予測コードの100倍から500倍の時間がかかっています。</target>
        </trans-unit>
        <trans-unit id="1f57c7d2294fbf421c865e0ff805433c9e9164a6" translate="yes" xml:space="preserve">
          <source>Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix \(X\) so that it has shape &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt;. We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same.</source>
          <target state="translated">自然言語処理（NLP）および情報検索（IR）の文献におけるLSAのほとんどの処理は、行列\（X \）の軸を入れ替えて、形状が &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt; になるようにします。LSAはscikit-learn APIによく一致する別の方法で提示しますが、見つかった特異値は同じです。</target>
        </trans-unit>
        <trans-unit id="56ac69cc3d5e8e713d723baf0656a6eefef8f81b" translate="yes" xml:space="preserve">
          <source>Multi target classification</source>
          <target state="translated">マルチターゲット分類</target>
        </trans-unit>
        <trans-unit id="b9b406b23aa7207ecf1f2aef41fc5c5ad0ba0c31" translate="yes" xml:space="preserve">
          <source>Multi target regression</source>
          <target state="translated">マルチターゲット回帰</target>
        </trans-unit>
        <trans-unit id="332c064d1606c8de1a2522f9e4ee668dee56478e" translate="yes" xml:space="preserve">
          <source>Multi-class AdaBoosted Decision Trees</source>
          <target state="translated">マルチクラスAdaBoosted決定木</target>
        </trans-unit>
        <trans-unit id="d384b7095ac166d1b587c1dafb0cadad28beb4c2" translate="yes" xml:space="preserve">
          <source>Multi-class targets.</source>
          <target state="translated">マルチクラスのターゲット。</target>
        </trans-unit>
        <trans-unit id="3243798e9c1a783043187bb0ea60ba4b8d0dfc62" translate="yes" xml:space="preserve">
          <source>Multi-class targets. An indicator matrix turns on multilabel classification.</source>
          <target state="translated">マルチクラスのターゲット。指標マトリックスは、マルチラベル分類をオンにします。</target>
        </trans-unit>
        <trans-unit id="552ba9a8fb8ef0b9cf8d9ea68e7f0c182ae9af5e" translate="yes" xml:space="preserve">
          <source>Multi-dimensional scaling</source>
          <target state="translated">多次元スケーリング</target>
        </trans-unit>
        <trans-unit id="9815dac6e8971893d838904dc7e1cfe16372af94" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron classifier.</source>
          <target state="translated">多層パーセプトロン分類器.</target>
        </trans-unit>
        <trans-unit id="b994a134c1a31489af71fc772bdcadb38a217ddf" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. Note that you must apply the &lt;em&gt;same&lt;/em&gt; scaling to the test set for meaningful results. You can use &lt;code&gt;StandardScaler&lt;/code&gt; for standardization.</source>
          <target state="translated">多層パーセプトロンは機能のスケーリングの影響を受けやすいため、データをスケーリングすることを強くお勧めします。たとえば、入力ベクトルXの各属性を[0、1]または[-1、+1]にスケーリングするか、平均0と分散1になるように標準化します。以下のテストセットに&lt;em&gt;同じ&lt;/em&gt;スケーリングを適用する必要があります。意味のある結果。 &lt;code&gt;StandardScaler&lt;/code&gt; を使用して標準化できます。</target>
        </trans-unit>
        <trans-unit id="8b22895cdf3840f5acfe1ac32cbc8961e8fd336a" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron regressor.</source>
          <target state="translated">多層パーセプトロン回帰器</target>
        </trans-unit>
        <trans-unit id="dc72474a07afc8bb8057ac9bf6d8fbc65e56a63e" translate="yes" xml:space="preserve">
          <source>Multi-output Decision Tree Regression</source>
          <target state="translated">多出力決定木回帰</target>
        </trans-unit>
        <trans-unit id="2627f8f7a5d9294ea8dcfe47a04508977edd8f7c" translate="yes" xml:space="preserve">
          <source>Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor.</source>
          <target state="translated">複数の予測変数にまたがって予測された複数の出力ターゲット。注:各予測子に対して別々のモデルが生成されます。</target>
        </trans-unit>
        <trans-unit id="4da1e42d60732d934b60458ac859ed1253e6bfbd" translate="yes" xml:space="preserve">
          <source>Multi-output targets.</source>
          <target state="translated">複数の出力ターゲット。</target>
        </trans-unit>
        <trans-unit id="d25d7d780166f0481648cccd463a78a5e417f6f3" translate="yes" xml:space="preserve">
          <source>Multi-output targets. An indicator matrix turns on multilabel estimation.</source>
          <target state="translated">マルチ出力ターゲット。インジケータ行列は、マルチラベル推定をオンにします。</target>
        </trans-unit>
        <trans-unit id="775030d60513b2f729789206b06d021b6661e16d" translate="yes" xml:space="preserve">
          <source>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">L1/L2混合ノルムを正則化して学習したマルチタスクElasticNetモデル</target>
        </trans-unit>
        <trans-unit id="7259143bf01ac8062e2b5725644f1e005f808315" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 ElasticNet with built-in cross-validation.</source>
          <target state="translated">クロスバリデーションを内蔵したマルチタスクL1/L2 ElasticNet。</target>
        </trans-unit>
        <trans-unit id="5c1ad40e838b03e514631adae1736168414d6605" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation</source>
          <target state="translated">クロスバリデーションを内蔵したマルチタスクL1/L2 Lasso</target>
        </trans-unit>
        <trans-unit id="a743aa48cf046a15087b0f4886f436159c359dd5" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation.</source>
          <target state="translated">クロスバリデーションを内蔵したマルチタスクL1/L2 Lasso。</target>
        </trans-unit>
        <trans-unit id="6377873684d0ac47f9792cf0130c074e6b5d5c8f" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">L1/L2混合ノルムを正則化して学習したマルチタスクLASSOモデル</target>
        </trans-unit>
        <trans-unit id="162889b9c309e59105e244387d111bbb75ed4cc7" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.</source>
          <target state="translated">L1/L2混合ノルムを正則化して学習したマルチタスクラスクモデル。</target>
        </trans-unit>
        <trans-unit id="0119eef45392f9d57273a8cd6ca2fcc5f0003969" translate="yes" xml:space="preserve">
          <source>Multi-task linear regressors with variable selection</source>
          <target state="translated">変数選択を伴うマルチタスク線形回帰器</target>
        </trans-unit>
        <trans-unit id="669e809a0e7044a9302d0da3188c44feddafe180" translate="yes" xml:space="preserve">
          <source>Multiclass and multilabel classification strategies</source>
          <target state="translated">マルチクラスとマルチラベルの分類戦略</target>
        </trans-unit>
        <trans-unit id="8ddfaa46f2a114c89c7cad99fd0ddf4dc7314399" translate="yes" xml:space="preserve">
          <source>Multiclass case:</source>
          <target state="translated">マルチクラスの場合。</target>
        </trans-unit>
        <trans-unit id="957cc5ae23e389ffa9c767fc16d7ac37036b0153" translate="yes" xml:space="preserve">
          <source>Multiclass classification</source>
          <target state="translated">マルチクラス分類</target>
        </trans-unit>
        <trans-unit id="868117baea7dbed0e92972aac1d890c07c8ae48f" translate="yes" xml:space="preserve">
          <source>Multiclass data will be treated as if binarized under a one-vs-rest transformation. Returned confusion matrices will be in the order of sorted unique labels in the union of (y_true, y_pred).</source>
          <target state="translated">多重クラスのデータは,1vsレスト変換の下で2値化されたかのように扱われる.返された混同行列は,(y_true,y_pred)の和の中でソートされた一意のラベルの順になります.</target>
        </trans-unit>
        <trans-unit id="2d8780a18f5ba3e6cb5bb5a579ea67a8f2550bb3" translate="yes" xml:space="preserve">
          <source>Multiclass only. Determines the type of configuration to use. The default value raises an error, so either &lt;code&gt;'ovr'&lt;/code&gt; or &lt;code&gt;'ovo'&lt;/code&gt; must be passed explicitly.</source>
          <target state="translated">マルチクラスのみ。使用する構成のタイプを決定します。デフォルト値ではエラーが発生するため、 &lt;code&gt;'ovr'&lt;/code&gt; または &lt;code&gt;'ovo'&lt;/code&gt; いずれかを明示的に渡す必要があります。</target>
        </trans-unit>
        <trans-unit id="f6acefc7f4185f0f4df5b1178aaed55afe1147ed" translate="yes" xml:space="preserve">
          <source>Multiclass only. List of labels that index the classes in &lt;code&gt;y_score&lt;/code&gt;. If &lt;code&gt;None&lt;/code&gt;, the numerical or lexicographical order of the labels in &lt;code&gt;y_true&lt;/code&gt; is used.</source>
          <target state="translated">マルチクラスのみ。 &lt;code&gt;y_score&lt;/code&gt; のクラスにインデックスを付けるラベルのリスト。 &lt;code&gt;None&lt;/code&gt; の場合、 &lt;code&gt;y_true&lt;/code&gt; のラベルの番号順または辞書式順序が使用されます。</target>
        </trans-unit>
        <trans-unit id="f43fb647f0e5eccf5a3760b6eafe5e21b79a50a6" translate="yes" xml:space="preserve">
          <source>Multiclass probability estimates are derived from binary (one-vs.-rest) estimates by simple normalization, as recommended by Zadrozny and Elkan.</source>
          <target state="translated">多クラス確率推定値は、ZadroznyとElkanによって推奨されているように、単純な正規化によってバイナリ(一対一)推定値から導出される。</target>
        </trans-unit>
        <trans-unit id="39d0ca41499d6b7f83a17678aedf5fae33705c06" translate="yes" xml:space="preserve">
          <source>Multiclass problems are binarized and treated like the corresponding multilabel problem:</source>
          <target state="translated">マルチクラス問題は二値化され、対応するマルチラベル問題のように扱われます。</target>
        </trans-unit>
        <trans-unit id="3debc5753cd55840fa65a540929e237591ad2faf" translate="yes" xml:space="preserve">
          <source>Multiclass settings</source>
          <target state="translated">マルチクラスの設定</target>
        </trans-unit>
        <trans-unit id="e3f8736465f26b4a50bfa9739f8adbcfb24ccc56" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logisitic regression on newgroups20</source>
          <target state="translated">ニューグループの多クラススパース対数回帰20</target>
        </trans-unit>
        <trans-unit id="abed0a03e9d2180975b0a68aad7dbbccddc0d2d0" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logistic regression on 20newgroups</source>
          <target state="translated">20newgroupsの多クラス疎ロジスティック回帰</target>
        </trans-unit>
        <trans-unit id="38a70920d0cd2001f4ef8f9ee41dfa6b122f018c" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="translated">マルチクラススペクトルクラスタリング、2003 Stella X.Yu、Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4386a05880a13b860ce6d4571568b773373f22e3" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="translated">マルチクラススペクトルクラスタリング、2003 Stella X. Yu、Jianbo Shi &lt;a href=&quot;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c5d0b14c4e8dd95e44e1cd37847a8b6674049750" translate="yes" xml:space="preserve">
          <source>Multiclass vs. multilabel fitting</source>
          <target state="translated">マルチクラス対マルチラベルフィッティング</target>
        </trans-unit>
        <trans-unit id="dbc4079d7d6495ef3cfc4fdaba01141075b60d89" translate="yes" xml:space="preserve">
          <source>Multidimensional scaling</source>
          <target state="translated">多次元スケーリング</target>
        </trans-unit>
        <trans-unit id="7c33b81ffc3ca04c62af4f5074ba33e510028ebd" translate="yes" xml:space="preserve">
          <source>Multilabel classification</source>
          <target state="translated">マルチラベル分類</target>
        </trans-unit>
        <trans-unit id="c720ba81272f13af125e464e56bd5648c8146ada" translate="yes" xml:space="preserve">
          <source>Multilabel ranking metrics</source>
          <target state="translated">マルチラベルのランキング指標</target>
        </trans-unit>
        <trans-unit id="ce79d912af81c5a374445c22e6e3cfe9ecb9a93c" translate="yes" xml:space="preserve">
          <source>Multilabel-indicator case:</source>
          <target state="translated">マルチラベルインジケータの場合。</target>
        </trans-unit>
        <trans-unit id="b6031d58e46d313eca93045d9598faea168256f9" translate="yes" xml:space="preserve">
          <source>Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See &lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;Using multiple metric evaluation&lt;/a&gt; for more details.</source>
          <target state="translated">マルチメトリックスコアリングは、事前定義されたスコア名の文字列のリストとして、またはスコアラー名をスコアラー関数および/または事前定義されたスコアラー名にマッピングする辞書として指定できます。詳細については、&lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;複数のメトリック評価の使用を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="71e5ed6f7fb13f64a7d1e47fd6ffef12dfd5580e" translate="yes" xml:space="preserve">
          <source>Multinomial + L1 penalty</source>
          <target state="translated">多項式+L1ペナルティ</target>
        </trans-unit>
        <trans-unit id="82d79c421161a2e0a31300a79127c9804ae62ed5" translate="yes" xml:space="preserve">
          <source>Multinomial + L2 penalty</source>
          <target state="translated">多項式+L2ペナルティ</target>
        </trans-unit>
        <trans-unit id="efccef2252a812759badf849dd9e2acd4cd7eb95" translate="yes" xml:space="preserve">
          <source>Multinomial deviance (&lt;code&gt;'deviance'&lt;/code&gt;): The negative multinomial log-likelihood loss function for multi-class classification with &lt;code&gt;n_classes&lt;/code&gt; mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration &lt;code&gt;n_classes&lt;/code&gt; regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.</source>
          <target state="translated">多項偏差（ &lt;code&gt;'deviance'&lt;/code&gt; ）： &lt;code&gt;n_classes&lt;/code&gt; 相互に排他的なクラスを持つマルチクラス分類の負の多項対数尤度損失関数。確率の推定値を提供します。初期モデルは、各クラスの事前確率によって与えられます。各反復で &lt;code&gt;n_classes&lt;/code&gt; 回帰木を構築する必要があります。これにより、多数のクラスを持つデータセットに対してGBRTがかなり非効率になります。</target>
        </trans-unit>
        <trans-unit id="313293589005fec34a4137f7e7a462e44753a91e" translate="yes" xml:space="preserve">
          <source>Multioutput classification support can be added to any classifier with &lt;code&gt;MultiOutputClassifier&lt;/code&gt;. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3&amp;hellip;,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3&amp;hellip;,yn).</source>
          <target state="translated">マルチ出力分類のサポートは、 &lt;code&gt;MultiOutputClassifier&lt;/code&gt; を使用して任意の分類子に追加できます。この戦略は、ターゲットごとに1つの分類子を当てはめることで構成されます。これにより、複数のターゲット変数分類が可能になります。このクラスの目的は、一連の応答（y1、y2、y3 &amp;hellip;、yn）。</target>
        </trans-unit>
        <trans-unit id="8ec2d1e390ee85463a8b9edc1df8f6a33597454a" translate="yes" xml:space="preserve">
          <source>Multioutput methods</source>
          <target state="translated">マルチ出力方式</target>
        </trans-unit>
        <trans-unit id="8e7bfb83db794fa15648cd7ab3b23c509cc8018c" translate="yes" xml:space="preserve">
          <source>Multioutput regression</source>
          <target state="translated">多出力回帰</target>
        </trans-unit>
        <trans-unit id="086b68ade408f93caaac80f71e1eabb4cc44f3fd" translate="yes" xml:space="preserve">
          <source>Multioutput regression support can be added to any regressor with &lt;code&gt;MultiOutputRegressor&lt;/code&gt;. This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As &lt;code&gt;MultiOutputRegressor&lt;/code&gt; fits one regressor per target it can not take advantage of correlations between targets.</source>
          <target state="translated">多出力の回帰サポートが持つ任意の回帰に追加することができ &lt;code&gt;MultiOutputRegressor&lt;/code&gt; 。この戦略は、ターゲットごとに1つのリグレッサを当てはめることで構成されます。各ターゲットは正確に1つのリグレッサで表されるため、対応するリグレッサを検査することで、ターゲットに関する知識を得ることができます。以下のよう &lt;code&gt;MultiOutputRegressor&lt;/code&gt; は、ターゲットごとに1つの回帰をフィットすることは、ターゲット間の相関関係を利用することはできません。</target>
        </trans-unit>
        <trans-unit id="96d87119823da5637cea208be6276fad5c922737" translate="yes" xml:space="preserve">
          <source>Multioutput- multiclass classification</source>
          <target state="translated">多出力-多クラス分類</target>
        </trans-unit>
        <trans-unit id="96e252b1f2ecf6cba5d585af259eddb308663e2e" translate="yes" xml:space="preserve">
          <source>Multiple metric evaluation using &lt;code&gt;cross_validate&lt;/code&gt; (please refer the &lt;code&gt;scoring&lt;/code&gt; parameter doc for more information)</source>
          <target state="translated">&lt;code&gt;cross_validate&lt;/code&gt; を使用した複数のメトリック評価（詳細については、 &lt;code&gt;scoring&lt;/code&gt; パラメーターのドキュメントを参照してください）</target>
        </trans-unit>
        <trans-unit id="629b6c06ee9b92eec539c00c0d5b033d1b11a26d" translate="yes" xml:space="preserve">
          <source>Multiple metric parameter search can be done by setting the &lt;code&gt;scoring&lt;/code&gt; parameter to a list of metric scorer names or a dict mapping the scorer names to the scorer callables.</source>
          <target state="translated">&lt;code&gt;scoring&lt;/code&gt; パラメーターをメトリックスコアラー名のリストまたはスコアラー名をスコアラー呼び出し可能オブジェクトにマッピングするdictに設定することにより、複数のメトリックパラメーター検索を実行できます。</target>
        </trans-unit>
        <trans-unit id="24f6a3478d65f4dead755fd18d3792f81fa6260d" translate="yes" xml:space="preserve">
          <source>Multiple stacking layers can be achieved by assigning &lt;code&gt;final_estimator&lt;/code&gt; to a &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor&quot;&gt;&lt;code&gt;StackingRegressor&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">複数のスタッキングレイヤーは、 &lt;code&gt;final_estimator&lt;/code&gt; を&lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt; &lt;code&gt;StackingClassifier&lt;/code&gt; &lt;/a&gt;または&lt;a href=&quot;generated/sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor&quot;&gt; &lt;code&gt;StackingRegressor&lt;/code&gt; に&lt;/a&gt;割り当てることで実現できます。</target>
        </trans-unit>
        <trans-unit id="85ae0fa16a20d62aa04d6b821cea2aa9547567a2" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. Keys are transformer names, values the weights.</source>
          <target state="translated">トランスフォーマーごとの機能の乗算的な重み。キーはトランスの名前、値は重みです。</target>
        </trans-unit>
        <trans-unit id="0634d761605b2ffdac7cc3b47cb937d2911bb7fc" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.</source>
          <target state="translated">トランスごとの特徴の乗算重み。トランスの出力は、これらの重みに乗算されます。キーはトランスの名前、値は重みです。</target>
        </trans-unit>
        <trans-unit id="afa1ae58a55a69631c4c27e76dfc260c619f73c7" translate="yes" xml:space="preserve">
          <source>Multipliers of parameter C for each class. Computed based on the &lt;code&gt;class_weight&lt;/code&gt; parameter.</source>
          <target state="translated">各クラスのパラメーターCの乗数。 &lt;code&gt;class_weight&lt;/code&gt; パラメーターに基づいて計算されます。</target>
        </trans-unit>
        <trans-unit id="b0e900a51b93880c89d198a9d719bd1f20cdb329" translate="yes" xml:space="preserve">
          <source>Multipliers of parameter C of each class. Computed based on the &lt;code&gt;class_weight&lt;/code&gt; parameter.</source>
          <target state="translated">各クラスのパラメーターCの乗数。 &lt;code&gt;class_weight&lt;/code&gt; パラメーターに基づいて計算されます。</target>
        </trans-unit>
        <trans-unit id="7f4f1f6c0e0110908215d6d402a5fd0376794171" translate="yes" xml:space="preserve">
          <source>Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting.</source>
          <target state="translated">指定された値で特徴量を乗算します。Noneの場合、特徴量は[1,100]で描画されたランダムな値でスケーリングされます。シフト後にスケーリングが行われることに注意してください。</target>
        </trans-unit>
        <trans-unit id="d54881ba1eca5e77240b1b917c4b4af86c4398ba" translate="yes" xml:space="preserve">
          <source>Multiplying the coefficients by the standard deviation of the related feature would reduce all the coefficients to the same unit of measure. As we will see &lt;a href=&quot;#scaling-num&quot;&gt;after&lt;/a&gt; this is equivalent to normalize numerical variables to their standard deviation, as \(y = \sum{coef_i \times X_i} = \sum{(coef_i \times std_i) \times (X_i / std_i)}\).</source>
          <target state="translated">係数に関連する特徴の標準偏差を掛けると、すべての係数が同じ測定単位になります。これからわか​​るように&lt;a href=&quot;#scaling-num&quot;&gt;、&lt;/a&gt;これは数値変数を標準偏差に正規化することと同じです。\（y = \ sum {coef_i \ times X_i} = \ sum {（coef_i \ times std_i）\ times（X_i / std_i）} \） 。</target>
        </trans-unit>
        <trans-unit id="773db00cec71fc706de69e832dff6b23a68d6b97" translate="yes" xml:space="preserve">
          <source>Multithreaded BLAS libraries sometimes conflict with Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; module, which is used by e.g. &lt;code&gt;GridSearchCV&lt;/code&gt; and most other estimators that take an &lt;code&gt;n_jobs&lt;/code&gt; argument (with the exception of &lt;code&gt;SGDClassifier&lt;/code&gt;, &lt;code&gt;SGDRegressor&lt;/code&gt;, &lt;code&gt;Perceptron&lt;/code&gt;, &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; and tree-based methods such as random forests). This is true of Apple&amp;rsquo;s Accelerate and OpenBLAS when built with OpenMP support.</source>
          <target state="translated">マルチスレッドBLASは時々 Pythonのライブラリとの競合 &lt;code&gt;multiprocessing&lt;/code&gt; などで使用されているモジュール、 &lt;code&gt;GridSearchCV&lt;/code&gt; および取る他のほとんどの推定 &lt;code&gt;n_jobs&lt;/code&gt; の（の例外を除いて、引数 &lt;code&gt;SGDClassifier&lt;/code&gt; 、 &lt;code&gt;SGDRegressor&lt;/code&gt; 、 &lt;code&gt;Perceptron&lt;/code&gt; 、 &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; とランダム森林などのツリーベースの方法を）。これは、OpenMPサポートを使用してビルドした場合のAppleのAccelerateおよびOpenBLASに当てはまります。</target>
        </trans-unit>
        <trans-unit id="3483a919f49e511ca829411c285a74281e005ef3" translate="yes" xml:space="preserve">
          <source>Multivariate imputation of missing values.</source>
          <target state="translated">欠損値の多変量インputation。</target>
        </trans-unit>
        <trans-unit id="3e386b49678343bab915e96a29e16b2a1aa09993" translate="yes" xml:space="preserve">
          <source>Multivariate imputer that estimates each feature from all the others.</source>
          <target state="translated">他のすべての特徴から各特徴を推定する多変量インピュター。</target>
        </trans-unit>
        <trans-unit id="425dc1fa519b0f6261993bae28e1ad51c131bb66" translate="yes" xml:space="preserve">
          <source>Must be provided at the first call to partial_fit, can be omitted in subsequent calls.</source>
          <target state="translated">partial_fitの最初の呼び出し時に指定する必要があります.</target>
        </trans-unit>
        <trans-unit id="b6845c300d4f945b800f2e50de745703cc5cc891" translate="yes" xml:space="preserve">
          <source>Must fulfill the input assumptions of the underlying estimator.</source>
          <target state="translated">基礎となる推定器の入力前提条件を満たす必要があります。</target>
        </trans-unit>
        <trans-unit id="214188886e4a84a8788bdd82b6f8744f5146fead" translate="yes" xml:space="preserve">
          <source>Mutual Information (not adjusted for chance)</source>
          <target state="translated">相互情報(偶然性を調整していない</target>
        </trans-unit>
        <trans-unit id="16b7cc0e7a5234ba809ed1e09a3a8960dff39693" translate="yes" xml:space="preserve">
          <source>Mutual Information between two clusterings.</source>
          <target state="translated">2つのクラスタリング間の相互情報。</target>
        </trans-unit>
        <trans-unit id="81e08bee8a8968c08bd07aed8cef44d9fb7a13f3" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">2つの確率変数間の相互情報量（MI）&lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;は、変数間の依存関係を測定する非負の値です。2つの確率変数が独立していて、値が大きいほど依存性が高い場合にのみ、ゼロになります。</target>
        </trans-unit>
        <trans-unit id="92d0e5dc6672a19ad8c5b9523b6a6d1a9b82c89e" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">2つの確率変数間の相互情報量（MI）&lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;は、変数間の依存関係を測定する非負の値です。2つの確率変数が独立していて、値が大きいほど依存性が高い場合にのみ、ゼロになります。</target>
        </trans-unit>
        <trans-unit id="4276bd70be44db9c6fb9548906a97ab826aba297" translate="yes" xml:space="preserve">
          <source>Mutual information between features and the target.</source>
          <target state="translated">特徴と対象の間の相互情報。</target>
        </trans-unit>
        <trans-unit id="33ca9360bf5453bcb4bf9aed03e658f161f23932" translate="yes" xml:space="preserve">
          <source>Mutual information for a continuous target.</source>
          <target state="translated">継続的な目標のための相互情報。</target>
        </trans-unit>
        <trans-unit id="aa199ad103c044c23c4e2e0edbe572bad088827c" translate="yes" xml:space="preserve">
          <source>Mutual information for a contnuous target.</source>
          <target state="translated">コンティニュアスターゲットのための相互情報。</target>
        </trans-unit>
        <trans-unit id="ef9610a089a978dd0d661be292e2bde712e413d1" translate="yes" xml:space="preserve">
          <source>Mutual information for a discrete target.</source>
          <target state="translated">離散的なターゲットのための相互情報。</target>
        </trans-unit>
        <trans-unit id="2555b04ef28112b324874c1cc2f3bf2b5b5c384e" translate="yes" xml:space="preserve">
          <source>Mutual information, a non-negative value</source>
          <target state="translated">相互情報、非負の値</target>
        </trans-unit>
        <trans-unit id="b51a60734da64be0e618bacbea2865a8a7dcd669" translate="yes" xml:space="preserve">
          <source>N</source>
          <target state="translated">N</target>
        </trans-unit>
        <trans-unit id="4e1221dedd7ee34eb6931a44dc15d9a84ca69a81" translate="yes" xml:space="preserve">
          <source>N : number of dimensions</source>
          <target state="translated">N:寸法数</target>
        </trans-unit>
        <trans-unit id="8daf5ce04352d841160e980446540ca50cce58e4" translate="yes" xml:space="preserve">
          <source>N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.</source>
          <target state="translated">N-gramsの登場です。単純なユニグラム(n=1)のコレクションを作るよりも、連続した単語のペアの出現をカウントするビグラム(n=2)のコレクションの方がいいかもしれません。</target>
        </trans-unit>
        <trans-unit id="0d5c48bb908535393359e08969f6193dc43fff44" translate="yes" xml:space="preserve">
          <source>NCA can be seen as learning a (squared) Mahalanobis distance metric:</source>
          <target state="translated">NCAは、(2乗の)マハラノビス距離メトリックを学習していると見ることができます。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
