<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="aaa6add3e7cf1ec084105f029a9e3050a4d18bde" translate="yes" xml:space="preserve">
          <source>The loss function to use in the boosting process. &amp;lsquo;binary_crossentropy&amp;rsquo; (also known as logistic loss) is used for binary classification and generalizes to &amp;lsquo;categorical_crossentropy&amp;rsquo; for multiclass classification. &amp;lsquo;auto&amp;rsquo; will automatically choose either loss depending on the nature of the problem.</source>
          <target state="translated">ブースティングプロセスで使用する損失関数。'binary_crossentropy'（ロジスティック損失とも呼ばれます）は、バイナリ分類に使用され、マルチクラス分類の場合は 'categorical_crossentropy'に一般化されます。「auto」は、問題の性質に応じて、どちらかの損失を自動的に選択します。</target>
        </trans-unit>
        <trans-unit id="d058a2ad82a18d6b6a9ed5c1f1cacf9ddbfdb363" translate="yes" xml:space="preserve">
          <source>The loss function to use in the boosting process. Note that the &amp;ldquo;least squares&amp;rdquo; and &amp;ldquo;poisson&amp;rdquo; losses actually implement &amp;ldquo;half least squares loss&amp;rdquo; and &amp;ldquo;half poisson deviance&amp;rdquo; to simplify the computation of the gradient. Furthermore, &amp;ldquo;poisson&amp;rdquo; loss internally uses a log-link and requires &lt;code&gt;y &amp;gt;= 0&lt;/code&gt;</source>
          <target state="translated">ブースティングプロセスで使用する損失関数。「最小二乗」および「ポアソン」損失は、勾配の計算を単純化するために、実際には「半分の最小二乗損失」および「半分のポアソン逸脱」を実装することに注意してください。さらに、「ポアソン」損失は内部でログリンクを使用し、 &lt;code&gt;y &amp;gt;= 0&lt;/code&gt; を必要とします</target>
        </trans-unit>
        <trans-unit id="efb9498c0013e0aa10110f406847617e3f7359e7" translate="yes" xml:space="preserve">
          <source>The loss function to use when updating the weights after each boosting iteration.</source>
          <target state="translated">ブースティングの各イテレーション後に重みを更新する際に使用する損失関数。</target>
        </trans-unit>
        <trans-unit id="76e41cb6fdfbaf660e2380953e681777b0e6378c" translate="yes" xml:space="preserve">
          <source>The loss function used is binomial deviance. Regularization via shrinkage (&lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt;) improves performance considerably. In combination with shrinkage, stochastic gradient boosting (&lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;) can produce more accurate models by reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in Random Forests (via the &lt;code&gt;max_features&lt;/code&gt; parameter).</source>
          <target state="translated">使用される損失関数は二項偏差です。収縮による正則化（ &lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt; ）により、パフォーマンスが大幅に向上します。収縮と組み合わせて、確率的勾配ブースティング（ &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; ）は、バギングによって分散を減らすことにより、より正確なモデルを生成できます。通常、収縮なしのサブサンプリングは不十分です。分散を減らすもう1つの方法は、（ &lt;code&gt;max_features&lt;/code&gt; パラメーターを介して）ランダムフォレストのランダム分割に類似した機能をサブサンプリングすることです。</target>
        </trans-unit>
        <trans-unit id="b776e738f1fc829ad5cd97ce27a8c60ba5e6ea09" translate="yes" xml:space="preserve">
          <source>The low rank part of the profile can be considered the structured signal part of the data while the tail can be considered the noisy part of the data that cannot be summarized by a low number of linear components (singular vectors).</source>
          <target state="translated">プロファイルの低ランク部分は、データの構造化された信号部分と考えることができ、テール部分は、データのノイズの多い部分と考えることができ、少ない数の線形成分(特異ベクトル)ではまとめられない部分と考えることができる。</target>
        </trans-unit>
        <trans-unit id="798401f91692e498e70c4cd9edead7945caa2484" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;alpha&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;alpha&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="translated">'alpha'の下限と上限。「fixed」に設定されている場合、ハイパーパラメータの調整中に「alpha」を変更することはできません。</target>
        </trans-unit>
        <trans-unit id="d36bb18704838afeb9c91e3cf29995f34fe370d6" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;gamma&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;gamma&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="translated">'ガンマ'の下限と上限。「固定」に設定すると、ハイパーパラメータの調整中に「ガンマ」を変更できません。</target>
        </trans-unit>
        <trans-unit id="05c76bb31dceb610d87492d4b267252a46764fae" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;length_scale&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;length_scale&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="translated">'length_scale'の下限と上限。「fixed」に設定すると、ハイパーパラメータの調整中に「length_scale」を変更できません。</target>
        </trans-unit>
        <trans-unit id="971e4cf6ca3d178a182a1d45d26bbec9249f02e4" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;noise_level&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;noise_level&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="translated">'noise_level'の下限と上限。「fixed」に設定すると、ハイパーパラメータの調整中に「noise_level」を変更できません。</target>
        </trans-unit>
        <trans-unit id="f18d5554a9b373b29b2cfd43681990a3c136384e" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;periodicity&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;periodicity&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="translated">「周期性」の下限と上限。「固定」に設定すると、ハイパーパラメータの調整中に「周期性」を変更できません。</target>
        </trans-unit>
        <trans-unit id="9c4988467c8a93656299039dfe058cd67baf3941" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;sigma_0&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;sigma_0&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="translated">'sigma_0'の下限と上限。「fixed」に設定すると、ハイパーパラメータの調整中に「sigma_0」を変更できません。</target>
        </trans-unit>
        <trans-unit id="bb2363bfa3c4e13746c6b4233da08b1f95f83117" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &lt;code&gt;constant_value&lt;/code&gt;. If set to &amp;ldquo;fixed&amp;rdquo;, &lt;code&gt;constant_value&lt;/code&gt; cannot be changed during hyperparameter tuning.</source>
          <target state="translated">&lt;code&gt;constant_value&lt;/code&gt; の下限と上限。「fixed」に設定すると、ハイパーパラメータの調整中に &lt;code&gt;constant_value&lt;/code&gt; を変更できません。</target>
        </trans-unit>
        <trans-unit id="eab337b7facddd1f9b82eece11282a55e0039c48" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on alpha</source>
          <target state="translated">アルファの下界と上界</target>
        </trans-unit>
        <trans-unit id="59acfd562a002dfced1122413eab7f832a55dd1c" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on constant_value</source>
          <target state="translated">constant_valueの下界と上界</target>
        </trans-unit>
        <trans-unit id="3a719a94ceaa10893529e6c7cb2929f1cad3fed1" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on gamma</source>
          <target state="translated">ガンマの下界と上界</target>
        </trans-unit>
        <trans-unit id="1ed15fdef2e92d07e56ad7fd40f1816e9ce5774a" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on l</source>
          <target state="translated">の下界と上界</target>
        </trans-unit>
        <trans-unit id="5a4d3b39beb7cd8132b2abe44c4adfccffd03b85" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on length_scale</source>
          <target state="translated">length_scale の下界と上界</target>
        </trans-unit>
        <trans-unit id="23389af6ff1a664526029d031ba30d8bce5de030" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on noise_level</source>
          <target state="translated">ノイズレベルの下界と上界</target>
        </trans-unit>
        <trans-unit id="5d396b2f8516f35fa726d028e889486242bc7cef" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on periodicity</source>
          <target state="translated">周期性の下限と上限</target>
        </trans-unit>
        <trans-unit id="ca7705b5107aa6db64b47bb244d2b0d4033a135a" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on the parameter. If n_elements&amp;gt;1, a pair of 1d array with n_elements each may be given alternatively. If the string &amp;ldquo;fixed&amp;rdquo; is passed as bounds, the hyperparameter&amp;rsquo;s value cannot be changed.</source>
          <target state="translated">パラメータの下限と上限。n_elements&amp;gt; 1の場合、それぞれn_elementsを持つ1d配列のペアを代わりに指定できます。文字列「fixed」が境界として渡された場合、ハイパーパラメータの値は変更できません。</target>
        </trans-unit>
        <trans-unit id="67f0743ba9ff76a5a36032511baa5e62618c04ec" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &amp;lt;= n &amp;lt;= max_n will be used.</source>
          <target state="translated">抽出されるさまざまなn-gramのn値の範囲の下限と上限。min_n &amp;lt;= n &amp;lt;= max_nのようなnのすべての値が使用されます。</target>
        </trans-unit>
        <trans-unit id="1a06e2593892f38e2c7f98d2aaf16f88ee8f121a" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &amp;lt;= n &amp;lt;= max_n will be used. For example an &lt;code&gt;ngram_range&lt;/code&gt; of &lt;code&gt;(1, 1)&lt;/code&gt; means only unigrams, &lt;code&gt;(1, 2)&lt;/code&gt; means unigrams and bigrams, and &lt;code&gt;(2, 2)&lt;/code&gt; means only bigrams. Only applies if &lt;code&gt;analyzer is not callable&lt;/code&gt;.</source>
          <target state="translated">抽出されるさまざまなn-gramのn値の範囲の下限と上限。min_n &amp;lt;= n &amp;lt;= max_nとなるようなnのすべての値が使用されます。例えば &lt;code&gt;ngram_range&lt;/code&gt; の &lt;code&gt;(1, 1)&lt;/code&gt; 手段のみ、ユニグラム &lt;code&gt;(1, 2)&lt;/code&gt; 手段のユニグラム及びバイグラム、および &lt;code&gt;(2, 2)&lt;/code&gt; 手段のみバイグラム。 &lt;code&gt;analyzer is not callable&lt;/code&gt; 場合にのみ適用されます。</target>
        </trans-unit>
        <trans-unit id="cfc6e4d4438c9aec5ef09350cb9a99015ebb32f0" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n &amp;lt;= n &amp;lt;= max_n will be used. For example an &lt;code&gt;ngram_range&lt;/code&gt; of &lt;code&gt;(1, 1)&lt;/code&gt; means only unigrams, &lt;code&gt;(1, 2)&lt;/code&gt; means unigrams and bigrams, and &lt;code&gt;(2, 2)&lt;/code&gt; means only bigrams. Only applies if &lt;code&gt;analyzer is not callable&lt;/code&gt;.</source>
          <target state="translated">抽出されるさまざまな単語n-gramまたはcharn-gramのn値の範囲の下限と上限。min_n &amp;lt;= n &amp;lt;= max_nとなるようなnのすべての値が使用されます。例えば &lt;code&gt;ngram_range&lt;/code&gt; の &lt;code&gt;(1, 1)&lt;/code&gt; 手段のみ、ユニグラム &lt;code&gt;(1, 2)&lt;/code&gt; 手段のユニグラム及びバイグラム、および &lt;code&gt;(2, 2)&lt;/code&gt; 手段のみバイグラム。 &lt;code&gt;analyzer is not callable&lt;/code&gt; 場合にのみ適用されます。</target>
        </trans-unit>
        <trans-unit id="1cf3abbfa7f1a3041db626cf750d4ae3ee4a5f71" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used create the extreme values for the &lt;code&gt;grid&lt;/code&gt;. Only if &lt;code&gt;X&lt;/code&gt; is not None.</source>
          <target state="translated">使用される下限と上限のパーセンタイルは、 &lt;code&gt;grid&lt;/code&gt; 極値を作成します。 &lt;code&gt;X&lt;/code&gt; がNoneでない場合のみ。</target>
        </trans-unit>
        <trans-unit id="ff88012ee3d2491153687a1e07b6eefe04629c89" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the PDP axes.</source>
          <target state="translated">PDP軸の極端な値を作成するために使用される下限パーセンタイルと上限パーセンタイル。</target>
        </trans-unit>
        <trans-unit id="8e5bebe8622375d11e17f863f40a564811e9afd3" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the PDP axes. Must be in [0, 1].</source>
          <target state="translated">PDP軸の極端な値を作成するために使用される下限パーセンタイルと上限パーセンタイル。0,1]でなければなりません。</target>
        </trans-unit>
        <trans-unit id="9186aa58b7ce14e687d5b2e114470d0ce75e6d07" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].</source>
          <target state="translated">グリッドの極端な値を作成するために使用される下部および上部のパーセンタイル。0,1]でなければなりません。</target>
        </trans-unit>
        <trans-unit id="ca509231087e3737e2d90c6b71a37c419327789b" translate="yes" xml:space="preserve">
          <source>The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around &lt;code&gt;0.01&lt;/code&gt;.</source>
          <target state="translated">左下の図は、単一の決定木の予想平均二乗誤差の点ごとの分解をプロットしています。これにより、バイアス項（青色）が低く、分散が大きい（緑色）ことがわかります。また、エラーのノイズ部分も示しています。これは、予想どおり、定数で &lt;code&gt;0.01&lt;/code&gt; 前後であるように見えます。</target>
        </trans-unit>
        <trans-unit id="caf1b50ddc9e3b679e122c659dff7510d4ed2966" translate="yes" xml:space="preserve">
          <source>The lower the better.</source>
          <target state="translated">低ければ低いほど良い。</target>
        </trans-unit>
        <trans-unit id="71c561a637b01dde1b3c6166ce3bd23db956f72f" translate="yes" xml:space="preserve">
          <source>The machine-learning pipeline</source>
          <target state="translated">機械学習パイプライン</target>
        </trans-unit>
        <trans-unit id="bfd900c27bac872165454194e9ba0284baa1f3aa" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.</source>
          <target state="translated">コレスキー対角因子の計算における機械精度の正則化.非常に条件の悪い系のためにこれを増やす。</target>
        </trans-unit>
        <trans-unit id="d9100545597c68bda47e361ae3fa5acd8ebd846b" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. By default, &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; is used.</source>
          <target state="translated">コレスキー対角因子の計算における機械精​​度の正則化。非常に悪条件のシステムの場合は、これを増やします。デフォルトでは、 &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; が使用されます。</target>
        </trans-unit>
        <trans-unit id="48832baaa50854fed8a3f45f6240f92c4370d802" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Default is &lt;code&gt;np.finfo(np.float64).eps&lt;/code&gt;.</source>
          <target state="translated">コレスキー対角因子の計算における機械精​​度の正則化。非常に悪条件のシステムの場合は、これを増やします。デフォルトは &lt;code&gt;np.finfo(np.float64).eps&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="69abd1713b39f884bea4ffeed0d502e131117e74" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &amp;lsquo;tol&amp;rsquo; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">コレスキー対角因子の計算における機械精​​度の正則化。非常に悪条件のシステムでは、これを増やします。一部の反復最適化ベースのアルゴリズムの「tol」パラメーターとは異なり、このパラメーターは最適化の許容範囲を制御しません。</target>
        </trans-unit>
        <trans-unit id="03f1e7333f5d863384674ba69d2200c51c214771" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">コレスキー対角因子の計算における機械精​​度の正則化。非常に悪条件のシステムでは、これを増やします。一部の反復最適化ベースのアルゴリズムの &lt;code&gt;tol&lt;/code&gt; パラメーターとは異なり、このパラメーターは最適化の許容範囲を制御しません。</target>
        </trans-unit>
        <trans-unit id="638a11bb66c553576b68621fdefa92568dded5ac" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; is used</source>
          <target state="translated">コレスキー対角因子の計算における機械精​​度の正則化。非常に悪条件のシステムの場合は、これを増やします。一部の反復最適化ベースのアルゴリズムの &lt;code&gt;tol&lt;/code&gt; パラメーターとは異なり、このパラメーターは最適化の許容範囲を制御しません。デフォルトでは、 &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; が使用されます</target>
        </trans-unit>
        <trans-unit id="a355ae348914d14284e933a2a39b838c82d13102" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; is used.</source>
          <target state="translated">コレスキー対角因子の計算における機械精​​度の正則化。非常に悪条件のシステムの場合は、これを増やします。一部の反復最適化ベースのアルゴリズムの &lt;code&gt;tol&lt;/code&gt; パラメーターとは異なり、このパラメーターは最適化の許容範囲を制御しません。デフォルトでは、 &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; が使用されます。</target>
        </trans-unit>
        <trans-unit id="686cc4307ca584641912f9ca8288a53bb10ce6de" translate="yes" xml:space="preserve">
          <source>The main advantage for Factor Analysis over &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is that it can model the variance in every direction of the input space independently (heteroscedastic noise):</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;に対する因子分析の主な利点は、入力空間のあらゆる方向の分散を独立してモデル化できることです（異分散ノイズ）。</target>
        </trans-unit>
        <trans-unit id="c6108525766abe52c974030991b5e8a7ddcb6e28" translate="yes" xml:space="preserve">
          <source>The main difficulty in learning Gaussian mixture models from unlabeled data is that it is one usually doesn&amp;rsquo;t know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;Expectation-maximization&lt;/a&gt; is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum.</source>
          <target state="translated">ラベル付けされていないデータから混合ガウスモデルを学習する際の主な問題は、通常、どの潜在的成分からどのポイントが生じたのかわからないことです（この情報にアクセスできる場合、個別のガウス分布を各セットに当てはめることが非常に簡単になります）。ポイント）。&lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;期待値最大化&lt;/a&gt;は、反復プロセスによってこの問題を回避するための十分に確立された統計アルゴリズムです。 1つ目は、ランダムコンポーネント（ランダムにデータポイントを中心、k平均から学習、または原点の周りに通常分布する）を想定し、モデルの各コンポーネントによって生成される確率を各ポイントについて計算します。次に、パラメーターを調整して、それらの割り当てが与えられたデータの可能性を最大化します。このプロセスを繰り返すと、常に局所最適に収束することが保証されます。</target>
        </trans-unit>
        <trans-unit id="d2139f3f89c20ed8a9cf480b63f0750e12fef92b" translate="yes" xml:space="preserve">
          <source>The main documentation. This contains an in-depth description of all algorithms and how to apply them.</source>
          <target state="translated">主なドキュメントです。これには、すべてのアルゴリズムとその適用方法についての詳細な説明が含まれています。</target>
        </trans-unit>
        <trans-unit id="5f94da6f46e5f5e5d800fbb67f1c2ae40caf9c89" translate="yes" xml:space="preserve">
          <source>The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\) is the number of samples and \(T\) is the number of iterations until convergence. Further, the memory complexity is of the order \(O(N^2)\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets.</source>
          <target state="translated">Affinity Propagationの最大の欠点は、その複雑さです。このアルゴリズムの時間的な複雑さは、\(O(N^2 T)\(N\)はサンプル数、\(T\)は収束までの反復回数です。さらに,メモリの複雑さは,密な類似度行列を用いた場合には\(O(N^2)T)のオーダーになりますが,疎な類似度行列を用いた場合には軽減されます.このことから,Affinity Propagationは中小規模のデータセットに最も適している.</target>
        </trans-unit>
        <trans-unit id="f23633268affa3b7ac5da4b687edb4096985bac5" translate="yes" xml:space="preserve">
          <source>The main factors that influence the prediction latency are</source>
          <target state="translated">予測待ち時間に影響を与える主な要因は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="26167f635fda3c2035f2a73e215b9329a59f7a43" translate="yes" xml:space="preserve">
          <source>The main observations to make are:</source>
          <target state="translated">主な観察事項は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="36a91e6ee1a9ee310e3b9c8a52ba666f014296cb" translate="yes" xml:space="preserve">
          <source>The main parameters to adjust when using these methods is &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt;. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are &lt;code&gt;max_features=None&lt;/code&gt; (always considering all features instead of a random subset) for regression problems, and &lt;code&gt;max_features=&quot;sqrt&quot;&lt;/code&gt; (using a random subset of size &lt;code&gt;sqrt(n_features)&lt;/code&gt;) for classification tasks (where &lt;code&gt;n_features&lt;/code&gt; is the number of features in the data). Good results are often achieved when setting &lt;code&gt;max_depth=None&lt;/code&gt; in combination with &lt;code&gt;min_samples_split=2&lt;/code&gt; (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (&lt;code&gt;bootstrap=True&lt;/code&gt;) while the default strategy for extra-trees is to use the whole dataset (&lt;code&gt;bootstrap=False&lt;/code&gt;). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting &lt;code&gt;oob_score=True&lt;/code&gt;.</source>
          <target state="translated">これらのメソッドを使用するときに調整する主なパラメーターは、 &lt;code&gt;n_estimators&lt;/code&gt; と &lt;code&gt;max_features&lt;/code&gt; です。前者は森の中の木の数です。大きいほど良いですが、計算に時間がかかります。さらに、クリティカル数のツリーを超えると、結果が大幅に改善されなくなることに注意してください。後者は、ノードを分割するときに考慮すべき機能のランダムなサブセットのサイズです。低いほど分散の減少は大きくなりますが、バイアスの増加も大きくなります。経験的に適切なデフォルト値は、回帰問題の場合は &lt;code&gt;max_features=None&lt;/code&gt; （常にランダムサブセットではなくすべての機能を考慮）、および &lt;code&gt;max_features=&quot;sqrt&quot;&lt;/code&gt; （サイズ &lt;code&gt;sqrt(n_features)&lt;/code&gt; のランダムサブセットを使用）です。）分類タスクの場合（ &lt;code&gt;n_features&lt;/code&gt; はデータ内の特徴の数です）。 &lt;code&gt;max_depth=None&lt;/code&gt; を &lt;code&gt;min_samples_split=2&lt;/code&gt; と組み合わせて設定すると（つまり、ツリーを完全に開発する場合）、良好な結果が得られることがよくあります。ただし、これらの値は通常最適ではなく、モデルが大量のRAMを消費する可能性があることに注意してください。最適なパラメータ値は常に相互検証する必要があります。さらに、ランダムフォレストでは、ブートストラップサンプルがデフォルトで使用され（ &lt;code&gt;bootstrap=True&lt;/code&gt; ）、エクストラツリーのデフォルトの戦略はデータセット全体を使用することです（ &lt;code&gt;bootstrap=False&lt;/code&gt; )。）。ブートストラップサンプリングを使用する場合、一般化の精度は、除外されたサンプルまたはバッグ外のサンプルで推定できます。これは、 &lt;code&gt;oob_score=True&lt;/code&gt; を設定することで有効にできます。</target>
        </trans-unit>
        <trans-unit id="b5b8962f0fe90f98d375c2c689eb4ed0da37d3b3" translate="yes" xml:space="preserve">
          <source>The main parameters to adjust when using these methods is &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt;. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are &lt;code&gt;max_features=n_features&lt;/code&gt; for regression problems, and &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; for classification tasks (where &lt;code&gt;n_features&lt;/code&gt; is the number of features in the data). Good results are often achieved when setting &lt;code&gt;max_depth=None&lt;/code&gt; in combination with &lt;code&gt;min_samples_split=2&lt;/code&gt; (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (&lt;code&gt;bootstrap=True&lt;/code&gt;) while the default strategy for extra-trees is to use the whole dataset (&lt;code&gt;bootstrap=False&lt;/code&gt;). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting &lt;code&gt;oob_score=True&lt;/code&gt;.</source>
          <target state="translated">これらのメソッドを使用するときに調整する主なパラメーターは、 &lt;code&gt;n_estimators&lt;/code&gt; および &lt;code&gt;max_features&lt;/code&gt; です。前者は森の中の木の本数です。大きいほど良いですが、計算に時間がかかります。さらに、ツリーの数が臨界数を超えると、結果が大幅に向上しなくなることに注意してください。後者は、ノードを分割するときに考慮する機能のランダムなサブセットのサイズです。値が小さいほど、分散の減少が大きくなりますが、バイアスの増加も大きくなります。経験的に適切なデフォルト値は、回帰問題の場合は &lt;code&gt;max_features=n_features&lt;/code&gt; 、分類タスクの &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; です（ここで &lt;code&gt;n_features&lt;/code&gt; はデータ内の特徴の数です）。 &lt;code&gt;max_depth=None&lt;/code&gt; を &lt;code&gt;min_samples_split=2&lt;/code&gt; と組み合わせて設定すると（つまり、ツリーを完全に開発する場合）、多くの場合、良い結果が得られます。ただし、これらの値は通常最適ではなく、RAMを大量に消費するモデルになる可能性があることに注意してください。最適なパラメータ値は常に相互検証する必要があります。さらに、ランダムフォレストでは、デフォルトでブートストラップサンプルが使用され（ &lt;code&gt;bootstrap=True&lt;/code&gt; ）、エクストラツリーのデフォルト戦略はデータセット全体を使用する（ &lt;code&gt;bootstrap=False&lt;/code&gt; ）ことに注意してください。ブートストラップサンプリングを使用する場合、一般化の精度は左またはアウトオブバッグサンプルで推定できます。これは &lt;code&gt;oob_score=True&lt;/code&gt; を設定することで有効にできます。</target>
        </trans-unit>
        <trans-unit id="842b51b29e297ee475304c463fec6849d760da97" translate="yes" xml:space="preserve">
          <source>The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions.</source>
          <target state="translated">t-SNEの主な目的は、高次元データの可視化です。したがって、データが2次元または3次元に埋め込まれている場合に最適です。</target>
        </trans-unit>
        <trans-unit id="85533a7e662fe3db2975f7f26edf978cd22f568d" translate="yes" xml:space="preserve">
          <source>The main theoretical result behind the efficiency of random projection is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma (quoting Wikipedia)&lt;/a&gt;:</source>
          <target state="translated">ランダム射影の効率の背後にある主な理論的結果は、&lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss補題（Wikipediaを引用）&lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="c6fec5e2fce31198cf0394ae7c6624fc74feba7a" translate="yes" xml:space="preserve">
          <source>The main usage of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; is to compute the GP&amp;rsquo;s covariance between datapoints. For this, the method &lt;code&gt;__call__&lt;/code&gt; of the kernel can be called. This method can either be used to compute the &amp;ldquo;auto-covariance&amp;rdquo; of all pairs of datapoints in a 2d array X, or the &amp;ldquo;cross-covariance&amp;rdquo; of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt;): &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;の主な用途は、データポイント間のGPの共分散を計算することです。この &lt;code&gt;__call__&lt;/code&gt; に、カーネルのメソッド__call__を呼び出すことができます。この方法は、2D配列Xのデータポイントのすべてのペアの「自動共分散」、または2D配列Xのデータポイントと2D配列Yのデータポイントのすべての組み合わせの「相互共分散」を計算するために使用できます。次の同一性は、すべてのカーネルk（&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt;を除く）に当てはまります &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4418da68cc8e1590c7a6e820d42fa84b7f1fff49" translate="yes" xml:space="preserve">
          <source>The main use-case of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt; kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt;カーネルの主な使用例は、信号のノイズ成分を説明するsum-kernelの一部です。パラメータ\（noise \ _level \）の調整は、ノイズレベルの推定に対応します。これは次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="54123ac391766adafa62d20ad77d558f0edc6a11" translate="yes" xml:space="preserve">
          <source>The main use-case of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt; kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:e</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt;カーネルの主な使用例は、信号のノイズ成分を説明する合計カーネルの一部です。パラメータ\（noise \ _level \）の調整は、ノイズレベルの推定に相当します。それは次のように定義されます：e</target>
        </trans-unit>
        <trans-unit id="9e4c246aa3bcb240193dfa5bc5ce95df373e50e0" translate="yes" xml:space="preserve">
          <source>The main use-case of this kernel is as part of a sum-kernel where it explains the noise of the signal as independently and identically normally-distributed. The parameter noise_level equals the variance of this noise.</source>
          <target state="translated">このカーネルの主な使用例は、信号のノイズを独立して同一の正規分布として説明する和カーネルの一部として使用されます。パラメータ noise_level はこのノイズの分散に等しくなります。</target>
        </trans-unit>
        <trans-unit id="3b785a852cf47604fe35c8c001dbd6b426026b75" translate="yes" xml:space="preserve">
          <source>The main use-case of this kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter corresponds to estimating the noise-level.</source>
          <target state="translated">このカーネルの主な使用例は、信号のノイズ成分を説明する和カーネルの一部としてです。このカーネルのパラメータを調整することは、ノイズレベルを推定することに相当します。</target>
        </trans-unit>
        <trans-unit id="b8d24bce3b0ca4f2f608d76c9973e497dd5a4966" translate="yes" xml:space="preserve">
          <source>The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of \(O(k n \bar p)\), where k is the number of iterations (epochs) and \(\bar p\) is the average number of non-zero attributes per sample.</source>
          <target state="translated">SGD の最大の利点は、基本的に訓練例の数が直線的であるという効率性です。X が大きさ (n,p)の行列であるとすると、トレーニングのコストは、\(O(k n \bar p)のように、k は反復回数 (epochs)、\(\bar p)はサンプルあたりの非ゼロ属性の平均数です。</target>
        </trans-unit>
        <trans-unit id="c142aa304b117907c4ec82e7c8f2e0e1debc825c" translate="yes" xml:space="preserve">
          <source>The manifold learning implementations available in scikit-learn are summarized below</source>
          <target state="translated">scikit-learn で利用可能な多様な学習実装は以下のようにまとめられています。</target>
        </trans-unit>
        <trans-unit id="97fe69cbbfc59d33068b7f1700f458b4fb09dc06" translate="yes" xml:space="preserve">
          <source>The mapping from the value \(F_M(x_i)\) to a class or a probability is loss-dependent. For the deviance (or log-loss), the probability that \(x_i\) belongs to the positive class is modeled as \(p(y_i = 1 | x_i) = \sigma(F_M(x_i))\) where \(\sigma\) is the sigmoid function.</source>
          <target state="translated">値からクラスや確率へのマッピングは、損失依存性があります。deviance(or log-loss)については、正のクラスに属する確率を、\(p(y_i=1 | x_i)=\sigma(F_M(x_i))のようにモデル化し、ここで、\(sigma)はシグモイド関数です。</target>
        </trans-unit>
        <trans-unit id="d450abcdfe5ff938dd90f9482f51850eeea5e6fe" translate="yes" xml:space="preserve">
          <source>The mapping relies on a Monte Carlo approximation to the kernel values. The &lt;code&gt;fit&lt;/code&gt; function performs the Monte Carlo sampling, whereas the &lt;code&gt;transform&lt;/code&gt; method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the &lt;code&gt;fit&lt;/code&gt; function.</source>
          <target state="translated">マッピングは、カーネル値へのモンテカルロ近似に依存しています。 &lt;code&gt;fit&lt;/code&gt; 関数は、一方、モンテカルロサンプリングを行う &lt;code&gt;transform&lt;/code&gt; 方法を行うデータのマッピング。プロセスには固有のランダム性があるため、 &lt;code&gt;fit&lt;/code&gt; 関数の呼び出しごとに結果が異なる場合があります。</target>
        </trans-unit>
        <trans-unit id="1292a72996c3834d41499b1c7abdc4ace6de6204" translate="yes" xml:space="preserve">
          <source>The mask of selected features.</source>
          <target state="translated">選択されたフィーチャーのマスク。</target>
        </trans-unit>
        <trans-unit id="c45c8ea0546b1c79f5dc6ae49b8b9eba3b94ab9e" translate="yes" xml:space="preserve">
          <source>The mathematical formulation is the following:</source>
          <target state="translated">数学的な定式化は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="d7ffadc6e201c4cc579f6014a4ba6a553d3e6d97" translate="yes" xml:space="preserve">
          <source>The matrix</source>
          <target state="translated">行列</target>
        </trans-unit>
        <trans-unit id="2cec1c9e3ed6a74b7dbd8e5442d20aacdeb523d2" translate="yes" xml:space="preserve">
          <source>The matrix dimension.</source>
          <target state="translated">行列の次元。</target>
        </trans-unit>
        <trans-unit id="e194b944fcebb1be4c3463284dee201bd8108680" translate="yes" xml:space="preserve">
          <source>The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as &lt;em&gt;covariance selection&lt;/em&gt;.</source>
          <target state="translated">共分散行列の逆行列は、しばしば精度行列と呼ばれ、偏相関行列に比例します。それは部分的な独立関係を与えます。つまり、2つの特徴が他の特徴に対して条件付きで独立している場合、精度行列の対応する係数はゼロになります。これが、スパース精度行列を推定することが理にかなっている理由です。共分散行列の推定は、データから独立関係を学習することによってより適切に条件付けられます。これは、&lt;em&gt;共分散選択&lt;/em&gt;と呼ばれます。</target>
        </trans-unit>
        <trans-unit id="e578a80d83c508825e0e59f2ea21cfd3dab91a77" translate="yes" xml:space="preserve">
          <source>The matrix of features, where NP is the number of polynomial features generated from the combination of inputs.</source>
          <target state="translated">特徴量の行列であり、ここでNPは入力の組み合わせから生成される多項式特徴量の数である。</target>
        </trans-unit>
        <trans-unit id="64f7c7c3f44e0d1f1fa6c715782355e1f1d2054c" translate="yes" xml:space="preserve">
          <source>The matrix.</source>
          <target state="translated">マトリックスです。</target>
        </trans-unit>
        <trans-unit id="9e68c52b3ee0713ce10cfb911b2833fc7e9a1df1" translate="yes" xml:space="preserve">
          <source>The maximal number of iterations for the solver.</source>
          <target state="translated">ソルバーの最大反復回数を指定します.</target>
        </trans-unit>
        <trans-unit id="f3c27fa93df86412f11493765849dd6aeb05082d" translate="yes" xml:space="preserve">
          <source>The maximum depth of each tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">各木の最大深度。Noneの場合は、すべての葉が純粋になるか、すべての葉がmin_samples_splitサンプル以下になるまでノードが展開されます。</target>
        </trans-unit>
        <trans-unit id="826fe580f0140d0a1b7e9876f557d09be8bbd041" translate="yes" xml:space="preserve">
          <source>The maximum depth of each tree. The depth of a tree is the number of edges to go from the root to the deepest leaf. Depth isn&amp;rsquo;t constrained by default.</source>
          <target state="translated">各木の最大の深さ。木の深さは、根から最も深い葉までのエッジの数です。デフォルトでは、深さは制限されていません。</target>
        </trans-unit>
        <trans-unit id="52e0ef4a9206a897434b4d55c59a4824cb11d988" translate="yes" xml:space="preserve">
          <source>The maximum depth of the representation. If None, the tree is fully generated.</source>
          <target state="translated">表現の最大深度。Noneの場合、木は完全に生成されます。</target>
        </trans-unit>
        <trans-unit id="f5406d9d68ce630037091594fec01e2950e41c42" translate="yes" xml:space="preserve">
          <source>The maximum depth of the tree.</source>
          <target state="translated">木の最大深度。</target>
        </trans-unit>
        <trans-unit id="e0ae7481b9fb370fd4191031ea9f70e91832a43a" translate="yes" xml:space="preserve">
          <source>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">ツリーの最大深度。Noneの場合、すべてのリーフが純粋になるまで、またはすべてのリーフがmin_samples_splitサンプル以下になるまでノードが展開されます。</target>
        </trans-unit>
        <trans-unit id="a55adc01e4f423a3103fea57c0a2cfa41d8471f9" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for one to be considered as in the neighborhood of the other. By default it assumes the same value as &lt;code&gt;max_eps&lt;/code&gt;. Used only when &lt;code&gt;cluster_method='dbscan'&lt;/code&gt;.</source>
          <target state="translated">一方が他方の近傍にあると見なされる2つのサンプル間の最大距離。デフォルトでは、 &lt;code&gt;max_eps&lt;/code&gt; と同じ値を想定しています。 &lt;code&gt;cluster_method='dbscan'&lt;/code&gt; の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="2785d22ac4386c9273ddac709630eaca73ae74f0" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for one to be considered as in the neighborhood of the other. Default value of &lt;code&gt;np.inf&lt;/code&gt; will identify clusters across all scales; reducing &lt;code&gt;max_eps&lt;/code&gt; will result in shorter run times.</source>
          <target state="translated">一方が他方の近傍にあると見なされる2つのサンプル間の最大距離。 &lt;code&gt;np.inf&lt;/code&gt; のデフォルト値は、すべてのスケールにわたるクラスターを識別します。 &lt;code&gt;max_eps&lt;/code&gt; を減らすと、実行時間が短くなります。</target>
        </trans-unit>
        <trans-unit id="86a132d622f603d09919856c8916a271668d069a" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.</source>
          <target state="translated">一方のサンプルが他方のサンプルの近傍にあるとみなされるための2つのサンプル間の最大距離。これはクラスタ内の点の距離の最大値ではありません。これは、データセットと距離関数のために適切に選択するための最も重要なDBSCANパラメータです。</target>
        </trans-unit>
        <trans-unit id="0cec94a1183be1885b8ca965cd0a310a0d6ef03e" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for them to be considered as in the same neighborhood.</source>
          <target state="translated">2つのサンプルが同じ近傍にあるとみなされるための2つのサンプル間の最大距離。</target>
        </trans-unit>
        <trans-unit id="2d03a3cd1f8dea91e35b793f66d9300ac80fe58d" translate="yes" xml:space="preserve">
          <source>The maximum number of bins to use for non-missing values. Before training, each feature of the input array &lt;code&gt;X&lt;/code&gt; is binned into integer-valued bins, which allows for a much faster training stage. Features with a small number of unique values may use less than &lt;code&gt;max_bins&lt;/code&gt; bins. In addition to the &lt;code&gt;max_bins&lt;/code&gt; bins, one more bin is always reserved for missing values. Must be no larger than 255.</source>
          <target state="translated">欠落していない値に使用するビンの最大数。トレーニングの前に、入力配列 &lt;code&gt;X&lt;/code&gt; の各機能は整数値のビンにビニングされます。これにより、トレーニング段階がはるかに高速になります。一意の値の数が少ない機能は、 &lt;code&gt;max_bins&lt;/code&gt; 未満のビンを使用する場合があります。 &lt;code&gt;max_bins&lt;/code&gt; ビンに加えて、もう1つのビンが常に欠落値用に予約されています。255以下である必要があります。</target>
        </trans-unit>
        <trans-unit id="78f8fd023fb60eb04dc691f81a2e17712a336e66" translate="yes" xml:space="preserve">
          <source>The maximum number of columns in the grid plot. Only active when &lt;code&gt;ax&lt;/code&gt; is a single axes or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">グリッドプロットの列の最大数。 &lt;code&gt;ax&lt;/code&gt; が単一軸または &lt;code&gt;None&lt;/code&gt; の場合にのみアクティブになります。</target>
        </trans-unit>
        <trans-unit id="78e415aabcf40a1b66551486961973263113520d" translate="yes" xml:space="preserve">
          <source>The maximum number of columns in the grid plot. Only active when &lt;code&gt;ax&lt;/code&gt; is a single axis or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">グリッドプロットの列の最大数。 &lt;code&gt;ax&lt;/code&gt; が単一軸または &lt;code&gt;None&lt;/code&gt; の場合にのみアクティブになります。</target>
        </trans-unit>
        <trans-unit id="72469eac4bccf834885ed51dab58c805d8cdc971" translate="yes" xml:space="preserve">
          <source>The maximum number of concurrently running jobs, such as the number of Python worker processes when backend=&amp;rdquo;multiprocessing&amp;rdquo; or the size of the thread-pool when backend=&amp;rdquo;threading&amp;rdquo;. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. None is a marker for &amp;lsquo;unset&amp;rsquo; that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a parallel_backend context manager that sets another value for n_jobs.</source>
          <target state="translated">同時に実行されるジョブの最大数。たとえば、backend =&amp;rdquo; multiprocessing&amp;rdquo;の場合のPythonワーカープロセスの数や、backend =&amp;rdquo; threading&amp;rdquo;の場合のスレッドプールのサイズ。-1の場合、すべてのCPUが使用されます。1を指定すると、並列計算コードはまったく使用されなくなり、デバッグに役立ちます。-1未満のn_jobsの場合、（n_cpus + 1 + n_jobs）が使用されます。したがって、n_jobs = -2の場合、1つを除くすべてのCPUが使用されます。Noneは、n_jobsに別の値を設定するparallel_backendコンテキストマネージャーの下で呼び出しが実行されない限り、n_jobs = 1（順次実行）として解釈される「設定解除」のマーカーです。</target>
        </trans-unit>
        <trans-unit id="d68dbcc29192b28b1c0e16950a4955da0229e9e9" translate="yes" xml:space="preserve">
          <source>The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.</source>
          <target state="translated">ブースティングが終了する推定量の最大数。完全適合の場合は、学習を早期に停止する。</target>
        </trans-unit>
        <trans-unit id="0ace226674121a7119418c464f4452694b61318d" translate="yes" xml:space="preserve">
          <source>The maximum number of features selected scoring above &lt;code&gt;threshold&lt;/code&gt;. To disable &lt;code&gt;threshold&lt;/code&gt; and only select based on &lt;code&gt;max_features&lt;/code&gt;, set &lt;code&gt;threshold=-np.inf&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;threshold&lt;/code&gt; スコアを選択した特徴の最大数。 &lt;code&gt;threshold&lt;/code&gt; を無効にし、 &lt;code&gt;max_features&lt;/code&gt; に基づいてのみ選択するには、 &lt;code&gt;threshold=-np.inf&lt;/code&gt; を設定します。</target>
        </trans-unit>
        <trans-unit id="305b61ba0832c48af4c31e220047663d92a85155" translate="yes" xml:space="preserve">
          <source>The maximum number of features to select. To only select based on &lt;code&gt;max_features&lt;/code&gt;, set &lt;code&gt;threshold=-np.inf&lt;/code&gt;.</source>
          <target state="translated">選択する機能の最大数。 &lt;code&gt;max_features&lt;/code&gt; のみに基づいて選択するには、 &lt;code&gt;threshold=-np.inf&lt;/code&gt; を設定します。</target>
        </trans-unit>
        <trans-unit id="1360fbfbf92ec231e131ca0c0a387fb9e5b6a69f" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations</source>
          <target state="translated">最大反復回数</target>
        </trans-unit>
        <trans-unit id="07cf3627f06a0c97fd8b5fd02bbb00e02fc4d154" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations in Newton&amp;rsquo;s method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results.</source>
          <target state="translated">予測中に事後を近似するためのニュートン法の最大反復回数。値を小さくすると、計算時間が短縮されますが、結果は悪くなります。</target>
        </trans-unit>
        <trans-unit id="5ecb214f49e2d4dbf3814aaa5686d1fc9fb18e01" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten&amp;rsquo;s FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results.</source>
          <target state="translated">反復の最大数は通常十分に高く、チューニングは必要ありません。最適化は、初期の誇張フェーズと最終的な最適化の2つのフェーズで構成されます。初期の誇張中に、元の空間の結合確率は、所定の係数との乗算によって人工的に増加します。より大きな因子は、データ内の自然なクラスター間のギャップを大きくします。係数が高すぎると、このフェーズでKLの発散が増加する可能性があります。通常は調整する必要はありません。重要なパラメータは学習率です。それが低すぎる場合、勾配降下は悪い局所最小値で動けなくなります。それが高すぎる場合、KLダイバージェンスは最適化中に増加します。その他のヒントは、Laurens van der MaatenのFAQ（参考文献を参照）にあります。最後のパラメータである角度は、パフォーマンスと精度の間のトレードオフです。角度が大きいほど、より大きな領域を1点で近似できるため、速度は向上しますが、結果の精度は低下します。</target>
        </trans-unit>
        <trans-unit id="ae7ecafb1fbb8659823714d61be48b0de277df78" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations of the boosting process, i.e. the maximum number of trees for binary classification. For multiclass classification, &lt;code&gt;n_classes&lt;/code&gt; trees per iteration are built.</source>
          <target state="translated">ブースティングプロセスの最大反復回数、つまり二項分類のツリーの最大数。マルチクラス分類の場合、反復ごとに &lt;code&gt;n_classes&lt;/code&gt; ツリーが作成されます。</target>
        </trans-unit>
        <trans-unit id="8d7badc19d2ae32c74e6847ba0481766700bc0d4" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations of the boosting process, i.e. the maximum number of trees.</source>
          <target state="translated">ブースティング処理の最大反復回数、すなわち木の最大数。</target>
        </trans-unit>
        <trans-unit id="3a263d39eeb80bbc61890473f9e4f7de61b380fb" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations to be run.</source>
          <target state="translated">実行する反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="42a16ed4baf475d1973848504fece7b7ddcdacc6" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations.</source>
          <target state="translated">反復回数の最大値。</target>
        </trans-unit>
        <trans-unit id="828c8485a088b2e8fca34b650a5b350287a8d14c" translate="yes" xml:space="preserve">
          <source>The maximum number of leaves for each tree. Must be strictly greater than 1. If None, there is no maximum limit.</source>
          <target state="translated">各木の最大葉数。厳密には1より大きくなければなりません。 Noneの場合、最大数の制限はありません。</target>
        </trans-unit>
        <trans-unit id="7623dada54053128bc10f5932064e15f34e6e533" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.PassiveAggressiveClassifier.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">トレーニングデータ（別名エポック）のパスの最大数。これは、 &lt;code&gt;fit&lt;/code&gt; メソッドの動作にのみ影響し、&lt;a href=&quot;#sklearn.linear_model.PassiveAggressiveClassifier.partial_fit&quot;&gt; &lt;code&gt;partial_fit&lt;/code&gt; &lt;/a&gt;メソッドには影響しません。</target>
        </trans-unit>
        <trans-unit id="a3d89c5f2bce98870adcd2b6143612a20116ca64" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.Perceptron.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">トレーニングデータ（別名エポック）のパスの最大数。これは、 &lt;code&gt;fit&lt;/code&gt; メソッドの動作にのみ影響し、&lt;a href=&quot;#sklearn.linear_model.Perceptron.partial_fit&quot;&gt; &lt;code&gt;partial_fit&lt;/code&gt; &lt;/a&gt;メソッドには影響しません。</target>
        </trans-unit>
        <trans-unit id="fcd0f20b2b6a22d8d1fd2812b58e7d3f318eed78" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.SGDClassifier.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">トレーニングデータ（別名エポック）のパスの最大数。これは、 &lt;code&gt;fit&lt;/code&gt; メソッドの動作にのみ影響し、&lt;a href=&quot;#sklearn.linear_model.SGDClassifier.partial_fit&quot;&gt; &lt;code&gt;partial_fit&lt;/code&gt; &lt;/a&gt;メソッドには影響しません。</target>
        </trans-unit>
        <trans-unit id="728ec87f7057b93a5bbf338c5362cf41c1855cde" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.SGDRegressor.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">トレーニングデータ（別名エポック）のパスの最大数。これは、 &lt;code&gt;fit&lt;/code&gt; メソッドの動作にのみ影響し、&lt;a href=&quot;#sklearn.linear_model.SGDRegressor.partial_fit&quot;&gt; &lt;code&gt;partial_fit&lt;/code&gt; &lt;/a&gt;メソッドには影響しません。</target>
        </trans-unit>
        <trans-unit id="fd38afe4aeee3f58ba57bcb27ec36aa15fe40a58" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;code&gt;partial_fit&lt;/code&gt; method.</source>
          <target state="translated">トレーニングデータ（別名エポック）のパスの最大数。これは、 &lt;code&gt;fit&lt;/code&gt; メソッドの動作にのみ影響し、 &lt;code&gt;partial_fit&lt;/code&gt; メソッドには影響しません。</target>
        </trans-unit>
        <trans-unit id="06bd569f5fc7509f2f4591004e3b0ffe2a685cf9" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;code&gt;partial_fit&lt;/code&gt;. Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</source>
          <target state="translated">トレーニングデータのパスの最大数（エポックとも呼ばれます）。これは、 &lt;code&gt;fit&lt;/code&gt; メソッドの動作にのみ影響し、 &lt;code&gt;partial_fit&lt;/code&gt; には影響しません。デフォルトは5です。デフォルトは0.21から1000です。tolがNoneでない場合。</target>
        </trans-unit>
        <trans-unit id="73890f2d8730349939758bb99469c8dd6b7e6151" translate="yes" xml:space="preserve">
          <source>The maximum number of patches per image to extract. If max_patches is a float in (0, 1), it is taken to mean a proportion of the total number of patches.</source>
          <target state="translated">抽出する画像あたりのパッチの最大数。max_patches が (0,1)の float の場合は、 パッチの総数に対する割合を意味します。</target>
        </trans-unit>
        <trans-unit id="52d2f42a039085572a1dcc0b5530d4df0b8b6859" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If &lt;code&gt;max_patches&lt;/code&gt; is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="translated">抽出するパッチの最大数。場合 &lt;code&gt;max_patches&lt;/code&gt; は、 0と1の間にフロートである、パッチの合計数の割合であると解釈されます。</target>
        </trans-unit>
        <trans-unit id="9914613b7b3d16fc7caab060022c123f66024339" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="translated">抽出するパッチの最大数。max_patches が 0 から 1 の間の浮動小数点数の場合は、パッチの総数に対する割合として扱われます。</target>
        </trans-unit>
        <trans-unit id="353b6da890fd9a7a3db82e5fc166d25d61607aa3" translate="yes" xml:space="preserve">
          <source>The maximum number of points on the path used to compute the residuals in the cross-validation</source>
          <target state="translated">クロスバリデーションで残差を計算するために使用されるパス上の最大ポイント数</target>
        </trans-unit>
        <trans-unit id="36c1ffc6cd6501f0a66093a4a420f28ce4311341" translate="yes" xml:space="preserve">
          <source>The maximum valid value the parameter can take. If None (default) it is implied that the parameter does not have an upper bound.</source>
          <target state="translated">パラメータが取ることのできる有効な最大値。None (デフォルト)の場合は、パラメータに上限がないことを意味します。</target>
        </trans-unit>
        <trans-unit id="c53181fb6c1656f1967e529a2fe72fd9bc29ff60" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the full dataset, which break down as soon as there are outliers in the data set</source>
          <target state="translated">完全データセットの平均と経験的共分散は,データセットに外れ値があるとすぐに分解する.</target>
        </trans-unit>
        <trans-unit id="f962fad68fff332002b42ee3dcc1e06f41fcfe6c" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the observations that are known to be good ones. This can be considered as a &amp;ldquo;perfect&amp;rdquo; MCD estimation, so one can trust our implementation by comparing to this case.</source>
          <target state="translated">良いものとして知られている観測値の平均と経験的共分散。これは「完全な」MCD推定と見なすことができるため、このケースと比較することで、実装を信頼できます。</target>
        </trans-unit>
        <trans-unit id="24b29efa5c5ad7917d67d95c19c0b6440c20e3d8" translate="yes" xml:space="preserve">
          <source>The mean claim amount or severity (&lt;code&gt;AvgClaimAmount&lt;/code&gt;) can be empirically shown to follow approximately a Gamma distribution. We fit a GLM model for the severity with the same features as the frequency model.</source>
          <target state="translated">平均請求額または重大度（ &lt;code&gt;AvgClaimAmount&lt;/code&gt; ）は、ほぼガンマ分布に従うことを経験的に示すことができます。頻度モデルと同じ機能を使用して、重大度にGLMモデルを適合させます。</target>
        </trans-unit>
        <trans-unit id="f98043dc9f229ba1c70dcb8021abddb5d793d8af" translate="yes" xml:space="preserve">
          <source>The mean of each mixture component.</source>
          <target state="translated">各混合成分の平均値。</target>
        </trans-unit>
        <trans-unit id="05329e8678ffdabb505c75140f9a49322a5129f1" translate="yes" xml:space="preserve">
          <source>The mean of the multi-dimensional normal distribution. If None then use the origin (0, 0, &amp;hellip;).</source>
          <target state="translated">多次元正規分布の平均。Noneの場合、原点（0、0、&amp;hellip;）を使用します。</target>
        </trans-unit>
        <trans-unit id="bde1358a0967a15325e4e7cc6fcf95bc7ef0f700" translate="yes" xml:space="preserve">
          <source>The mean over features. Only set if &lt;code&gt;self.whiten&lt;/code&gt; is True.</source>
          <target state="translated">機能の平均。 &lt;code&gt;self.whiten&lt;/code&gt; がTrueの場合にのみ設定されます。</target>
        </trans-unit>
        <trans-unit id="3598a929d1a64528ce62560f1aa02f9fb9981b30" translate="yes" xml:space="preserve">
          <source>The mean predicted probability in each bin.</source>
          <target state="translated">各ビンの平均予測確率。</target>
        </trans-unit>
        <trans-unit id="8aaf5602eb2242bddc9912d47746326b31b0a1d5" translate="yes" xml:space="preserve">
          <source>The mean score and the 95% confidence interval of the score estimate are hence given by:</source>
          <target state="translated">したがって、平均スコアとスコア推定値の95%信頼区間は次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="fb645170ce6174deae19b9d9f752a59168ce3c91" translate="yes" xml:space="preserve">
          <source>The mean squared error (&lt;code&gt;power=0&lt;/code&gt;) is very sensitive to the prediction difference of the second point,:</source>
          <target state="translated">平均二乗誤差（ &lt;code&gt;power=0&lt;/code&gt; ）は、2番目の点の予測差に非常に敏感です。</target>
        </trans-unit>
        <trans-unit id="94d27e1df24bf9a05c82ae625a1197b415bc3fdc" translate="yes" xml:space="preserve">
          <source>The mean value for each feature in the training set. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_mean=False&lt;/code&gt;.</source>
          <target state="translated">トレーニングセットの各特徴の平均値。 &lt;code&gt;with_mean=False&lt;/code&gt; の場合は &lt;code&gt;None&lt;/code&gt; と同じです。</target>
        </trans-unit>
        <trans-unit id="b5e7dcb7d882c8689297cb339a998a6e74140e5f" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.</source>
          <target state="translated">これらの特徴の平均、標準誤差、および「最悪」または最大（3つの最大値の平均）が各画像について計算され、30の特徴が得られました。たとえば、フィールド3は平均半径、フィールド13は半径SE、フィールド23は最悪半径です。</target>
        </trans-unit>
        <trans-unit id="8d0066f714eed86dea1c0b26b34c2da2cae60854" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.</source>
          <target state="translated">これらの特徴の平均、標準誤差、および「最悪」または最大（3つの最悪/最大値の平均）が各画像に対して計算され、30の特徴が得られました。たとえば、フィールド0は平均半径、フィールド10は半径SE、フィールド20は最悪の半径です。</target>
        </trans-unit>
        <trans-unit id="9952f9f2a02ad0002415a0bd8b6c9bae196d7ee8" translate="yes" xml:space="preserve">
          <source>The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.</source>
          <target state="translated">木が与えられたオブザベーションの正規性の尺度は,このオブザベーションを含むリーフの深さであり,この点を分離するのに必要な分割の数に相当する.リーフ内に複数のオブザベーションn_leftがある場合、n_leftサンプルの分離木の平均パス長が追加されます。</target>
        </trans-unit>
        <trans-unit id="3ff66cd9c701474c3d9f795cee9d82f5e1659bc8" translate="yes" xml:space="preserve">
          <source>The median absolute deviation to non corrupt new data is used to judge the quality of the prediction.</source>
          <target state="translated">予測の質を判断するために、非破損の新しいデータへの絶対偏差の中央値を使用しています。</target>
        </trans-unit>
        <trans-unit id="970571bcac487854f4caea3e516e12da8ac34c22" translate="yes" xml:space="preserve">
          <source>The median value for each feature in the training set.</source>
          <target state="translated">学習セットの各特徴の中央値。</target>
        </trans-unit>
        <trans-unit id="c8bf90a15bbbe280812d1ade2a9f9077adb9d41d" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments.</source>
          <target state="translated">キャッシュnumpy配列からロードする際に使用するmemmappingモード。引数の意味は numpy.load を参照してください。</target>
        </trans-unit>
        <trans-unit id="4971ea4c8c64c5bc8a4bdd1e4563c9705f8166bf" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments. By default that of the memory object is used.</source>
          <target state="translated">キャッシュnumpy配列からロードする際に使用するmemmappingモード。引数の意味は numpy.load を参照してください。デフォルトではメモリオブジェクトのものが使われます。</target>
        </trans-unit>
        <trans-unit id="85874f620128fa58a2e359ad1c9c828fcfd537bb" translate="yes" xml:space="preserve">
          <source>The memory footprint of randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is also proportional to \(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\) instead of \(n_{\max} \cdot n_{\min}\) for the exact method.</source>
          <target state="translated">ランダム化された&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;のメモリフットプリントは、\（n _ {\ max} \ cdot n _ {\ min} \）ではなく、\（2 \ cdot n _ {\ max} \ cdot n _ {\ mathrm {components}} \）にも比例します。正確な方法。</target>
        </trans-unit>
        <trans-unit id="ad92e195504975d239e8efd05966b747d9d3adc0" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier, and discretize the [0, 1] interval into bins.</source>
          <target state="translated">この手法は,入力がバイナリ分類器からのものであると仮定し,[0,1]区間をビンに離散化します.</target>
        </trans-unit>
        <trans-unit id="4fa2b59a5a9d6863d86f91b2a847ac2c39bc204e" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier.</source>
          <target state="translated">この方法は,入力がバイナリ分類器からのものであることを前提としています.</target>
        </trans-unit>
        <trans-unit id="ff8ec8205e374ddf520735804ecbfa39550f37bd" translate="yes" xml:space="preserve">
          <source>The method fits the model &lt;code&gt;n_init&lt;/code&gt; times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. If &lt;code&gt;warm_start&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then &lt;code&gt;n_init&lt;/code&gt; is ignored and a single initialization is performed upon the first call. Upon consecutive calls, training starts where it left off.</source>
          <target state="translated">このメソッドは、モデルを &lt;code&gt;n_init&lt;/code&gt; 回適合させ、モデルが最尤または下限をもつパラメーターを設定します。各試行内で、メソッドは尤度または下限の変化が &lt;code&gt;tol&lt;/code&gt; 未満になるまで &lt;code&gt;max_iter&lt;/code&gt; とM-stepの間をmax_iter回繰り返します。そうでない場合は、 &lt;code&gt;ConvergenceWarning&lt;/code&gt; が発生します。場合 &lt;code&gt;warm_start&lt;/code&gt; がある &lt;code&gt;True&lt;/code&gt; 、そして &lt;code&gt;n_init&lt;/code&gt; 無視され、単一の初期化は、最初の呼び出し時に行われます。連続した呼び出しで、トレーニングは中断したところから始まります。</target>
        </trans-unit>
        <trans-unit id="64dcb6bda2581c1c56dbef7b8447b61db8e804dd" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;a href=&quot;sklearn.exceptions.convergencewarning#sklearn.exceptions.ConvergenceWarning&quot;&gt;&lt;code&gt;ConvergenceWarning&lt;/code&gt;&lt;/a&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="translated">このメソッドは、モデルのn_init回に適合し、モデルが最大の尤度または下限を持つパラメーターを設定します。各試行内で、メソッドは、尤度または下限の変化が &lt;code&gt;tol&lt;/code&gt; 未満になるまで、EステップとMステップを &lt;code&gt;max_iter&lt;/code&gt; 回繰り返します。それ以外の場合は、&lt;a href=&quot;sklearn.exceptions.convergencewarning#sklearn.exceptions.ConvergenceWarning&quot;&gt; &lt;code&gt;ConvergenceWarning&lt;/code&gt; &lt;/a&gt;が発生します。フィッティング後、入力データポイントの最も可能性の高いラベルを予測します。</target>
        </trans-unit>
        <trans-unit id="5aeb4fb178bdd7f679ad7ab31606d57c02d18da8" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="translated">このメソッドは、モデルをn_init回適合させ、モデルが最尤または下限をもつパラメーターを設定します。各試行内で、メソッドは尤度または下限の変化が &lt;code&gt;tol&lt;/code&gt; 未満になるまで &lt;code&gt;max_iter&lt;/code&gt; とM-stepの間をmax_iter回繰り返します。そうでない場合は、 &lt;code&gt;ConvergenceWarning&lt;/code&gt; が発生します。フィッティング後、入力データポイントの最も可能性の高いラベルを予測します。</target>
        </trans-unit>
        <trans-unit id="02e13e01ea0f52a6e082ed53d9636e409ba7f22e" translate="yes" xml:space="preserve">
          <source>The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.</source>
          <target state="translated">この方法は、独立したRBMの重みでディープニューラルネットワークを初期化するために人気を博した。この方法は教師なし事前学習と呼ばれている。</target>
        </trans-unit>
        <trans-unit id="867b88e44ce337abe62bbd2070ac4235fc6ec53b" translate="yes" xml:space="preserve">
          <source>The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.</source>
          <target state="translated">サポートベクター分類の手法を拡張して、回帰問題を解くことができます。この方法は、サポートベクトル回帰と呼ばれています。</target>
        </trans-unit>
        <trans-unit id="0ea5095ab7a2f7c4087eef18fc7667a5380ac2de" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method (i.e. a logistic regression model) or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit.</source>
          <target state="translated">キャリブレーションに使用する方法。プラットの方法（つまり、ロジスティック回帰モデル）に対応する「シグモイド」またはノンパラメトリックアプローチである「等張」にすることができます。過剰適合する傾向があるため、キャリブレーションサンプルが少なすぎる &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; 等張キャリブレーションを使用することはお勧めしません。</target>
        </trans-unit>
        <trans-unit id="0b9b2478ee76aa8f8a62d64db00bfa346a8108cd" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit. Use sigmoids (Platt&amp;rsquo;s calibration) in this case.</source>
          <target state="translated">キャリブレーションに使用する方法。プラットの方法に対応する「シグモイド」またはノンパラメトリックアプローチである「等張性」のいずれかになります。キャリブレーションサンプルが少なすぎる &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; 等張キャリブレーションを使用することはお勧めしません。この場合、シグモイド（プラットの校正）を使用します。</target>
        </trans-unit>
        <trans-unit id="1d0e566bfd264bfe0a9882ca94efb92a9385b867" translate="yes" xml:space="preserve">
          <source>The method used by each base estimator.</source>
          <target state="translated">各ベース推定器で使用される方法。</target>
        </trans-unit>
        <trans-unit id="87ca64fde3cf11f4163bb009ef5ad4abbcdc8e98" translate="yes" xml:space="preserve">
          <source>The method used to calculate the averaged predictions:</source>
          <target state="translated">予測値の平均値を算出するために使用した方法です。</target>
        </trans-unit>
        <trans-unit id="21e55011234a7eafaab581faf9cf56cabba5a5c2" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the covariances. Must be one of:</source>
          <target state="translated">重み,平均,共分散の初期化に用いられるメソッド.のいずれかでなければならない.</target>
        </trans-unit>
        <trans-unit id="963cb60032e3efabe5bef9b8ad76de7b8282f830" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the precisions. Must be one of:</source>
          <target state="translated">重み、平均、精度を初期化するために使用されるメソッド.のいずれかでなければならない.</target>
        </trans-unit>
        <trans-unit id="8015d2d76c1dfbbe3de68f3d3f8aa78fbf89dd67" translate="yes" xml:space="preserve">
          <source>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">このメソッドは、ネストされたオブジェクト（パイプラインなど）だけでなく、単純な推定量でも機能します。後者には &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; という形式のパラメーターがあるため、ネストされたオブジェクトの各コンポーネントを更新できます。</target>
        </trans-unit>
        <trans-unit id="5d1673cba254e117532409bf5f2a151ed75e6f39" translate="yes" xml:space="preserve">
          <source>The method works on simple kernels as well as on nested kernels. The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">この方法は、ネストされたカーネルだけでなく、単純なカーネルでも機能します。後者には &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; という形式のパラメーターがあるため、ネストされたオブジェクトの各コンポーネントを更新できます。</target>
        </trans-unit>
        <trans-unit id="653c073b67dfbefaf963d2a9e7b682aaf13d10c5" translate="yes" xml:space="preserve">
          <source>The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.</source>
          <target state="translated">F-検定に基づく手法は、2つのランダム変数間の線形依存性の度合いを推定する。一方、相互情報法は、あらゆる種類の統計的依存関係を捉えることができますが、ノンパラメトリックであるため、正確な推定のためにはより多くのサンプルを必要とします。</target>
        </trans-unit>
        <trans-unit id="86d449abad7d30ccc370e439f66e761a4c276339" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;Glossary&lt;/a&gt;, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="translated">フィーチャ配列内のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列または呼び出し可能である場合、それはそのメトリックパラメータに対して&lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt; &lt;/a&gt;で許可されているオプションの1つである必要があります。メトリックが「事前計算」されている場合、Xは距離行列であると見なされ、正方形である必要があります。Xは&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;用語集の&lt;/a&gt;場合があり、その場合、「ゼロ以外の」要素のみがDBSCANのネイバーと見なされます。</target>
        </trans-unit>
        <trans-unit id="9346206e9d06ed06060a101f1e51a8e80f2445f7" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">フィーチャ配列内のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列または呼び出し可能である場合、それはそのメトリックパラメータに対して&lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt; &lt;/a&gt;で許可されているオプションの1つである必要があります。メトリックが「事前計算」されている場合、Xは距離行列であると見なされ、正方形である必要があります。Xは&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;用語集の&lt;/a&gt;場合があります。</target>
        </trans-unit>
        <trans-unit id="42e3e79f7ef752ca5a9bd963af37b6abd9f4c61f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。指標が文字列または呼び出し可能である場合、それはそのmetricパラメーターに対して&lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt; &lt;/a&gt;によって許可されるオプションの1つである必要があります。メトリックが「事前計算」されている場合、Xは距離行列と見なされ、正方でなければなりません。 Xはスパース行列である場合があります。その場合、「非ゼロ」要素のみがDBSCANの隣接要素と見なされます。</target>
        </trans-unit>
        <trans-unit id="379ba7f47f97569c7bd4b20c4c77667f05344897" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples corresponding to each class is the point from which the sum of the distances (according to the metric) of all samples that belong to that particular class are minimized. If the &amp;ldquo;manhattan&amp;rdquo; metric is provided, this centroid is the median and for all other metrics, the centroid is now set to be the mean.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列または呼び出し可能である場合、それは、そのメトリックパラメーターのmetrics.pairwise.pairwise_distancesによって許可されたオプションの1つである必要があります。各クラスに対応するサンプルの重心は、その特定のクラスに属するすべてのサンプルの距離の合計（メトリックに基づく）が最小化されるポイントです。 「マンハッタン」メトリックが提供されている場合、この重心は中央値であり、他のすべてのメトリックでは、重心は平均に設定されています。</target>
        </trans-unit>
        <trans-unit id="c83cb22e1c81dc697b4c0637e95b8aeb91bdccce" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt;.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。metricが文字列の場合は、 &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt; で許可されているオプションの1つである必要があります。Xが距離配列自体の場合は、 &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; を使用します。</target>
        </trans-unit>
        <trans-unit id="76b4b37055a409fe2e4e93cf3ad5227beac3187f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列の場合、それは &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt; で許可されているオプションの1つである必要があります。Xが距離配列自体の場合、メトリックとして「事前計算済み」を使用します。</target>
        </trans-unit>
        <trans-unit id="74dc2788261a34d78d8ba4595625c4b4c3a263d6" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric. Precomputed distance matrices must have 0 along the diagonal.</source>
          <target state="translated">フィーチャ配列内のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列の場合、 &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt; で許可されているオプションの1つである必要があります。Xが距離配列自体である場合は、メトリックとして「事前計算済み」を使用します。事前に計算された距離行列は、対角線に沿って0でなければなりません。</target>
        </trans-unit>
        <trans-unit id="f0f209bf70493187675786c136d5f63b20f1b34d" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列の場合、それはscipy.spatial.distance.pdistによってそのメトリックパラメータに対して許可されているオプションの1つ、またはpairwise.PAIRWISE_DISTANCE_FUNCTIONSにリストされているメトリックでなければなりません。メトリックが「事前計算」されている場合、Xは距離行列であると見なされます。または、メトリックが呼び出し可能な関数の場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2つの配列を入力として取り、それらの間の距離を示す値を返す必要があります。</target>
        </trans-unit>
        <trans-unit id="5d6bfb4b6dd59ce295c63dba458c016e3505d6e2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is &amp;ldquo;euclidean&amp;rdquo; which is interpreted as squared euclidean distance.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。メトリックが文字列の場合、それはscipy.spatial.distance.pdistによってそのメトリックパラメータに対して許可されているオプションの1つ、またはpairwise.PAIRWISE_DISTANCE_FUNCTIONSにリストされているメトリックでなければなりません。メトリックが「事前計算」されている場合、Xは距離行列であると見なされます。または、メトリックが呼び出し可能な関数の場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2つの配列を入力として取り、それらの間の距離を示す値を返す必要があります。デフォルトは「ユークリッド」で、ユークリッド距離の2乗として解釈されます。</target>
        </trans-unit>
        <trans-unit id="1d21e688cc862f7d70c6767af76fb5452c5fdcd0" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options specified in PAIRED_DISTANCES, including &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, or &amp;ldquo;cosine&amp;rdquo;. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">フィーチャ配列のインスタンス間の距離を計算するときに使用するメトリック。指標が文字列の場合、「ユークリッド」、「マンハッタン」、「コサイン」など、PAIRED_DISTANCESで指定されたオプションの1つである必要があります。または、メトリックが呼び出し可能な関数の場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2つの配列を入力として取り、それらの間の距離を示す値を返す必要があります。</target>
        </trans-unit>
        <trans-unit id="f36b3ebd6c1e0314a19882dfd7c792465ced8cb2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">機能配列のインスタンス間のカーネルを計算するときに使用するメトリック。指標が文字列の場合は、pairwise.PAIRWISE_KERNEL_FUNCTIONSの指標の1つである必要があります。メトリックが「事前計算」されている場合、Xはカーネル行列であると見なされます。または、メトリックが呼び出し可能な関数の場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2つの配列を入力として取り、それらの間の距離を示す値を返す必要があります。</target>
        </trans-unit>
        <trans-unit id="fc294d94430f6d524884c3e3bbe7bf2fd080ba19" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from &lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.</source>
          <target state="translated">機能配列内のインスタンス間のカーネルを計算するときに使用するメトリック。メトリックが文字列の場合、pairwise.PAIRWISE_KERNEL_FUNCTIONSのメトリックの1つである必要があります。メトリックが「事前計算」されている場合、Xはカーネル行列であると見なされます。または、メトリックが呼び出し可能な関数である場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、Xから2行を入力として受け取り、対応するカーネル値を単一の数値として返す必要があります。これは、&lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt;からの呼び出し可能オブジェクトは、単一のサンプルではなく行列で動作するため、許可されないことを意味します。代わりに、カーネルを識別する文字列を使用してください。</target>
        </trans-unit>
        <trans-unit id="f4d18e9d9a4e3ba27fc7d43cfc9aa960df294a66" translate="yes" xml:space="preserve">
          <source>The minimal number of components to guarantee with good probability an eps-embedding with n_samples.</source>
          <target state="translated">n_samplesを持つeps-embeddingを高い確率で保証するための最小構成要素数。</target>
        </trans-unit>
        <trans-unit id="bffda997c9866bdeb3072a563a59e0dfd40b41cb" translate="yes" xml:space="preserve">
          <source>The minimization problem becomes:</source>
          <target state="translated">最小化問題になります。</target>
        </trans-unit>
        <trans-unit id="dcfd1fcfc3ab5a126126fe7777d3f592dbce3714" translate="yes" xml:space="preserve">
          <source>The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.</source>
          <target state="translated">コンセンサススコアの最小値0は、すべてのバイクラスターのペアが完全に異質な場合に発生します。最大スコアである1は、両方のセットが同一の場合に発生します。</target>
        </trans-unit>
        <trans-unit id="546685a327b5ecf09c20bafa81b23b9f31e36223" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantee the eps-embedding is given by:</source>
          <target state="translated">eps-embeddingを保証するための最小構成要素数は次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="d171bb6d353676c30b749e2ca85c8811f4027e78" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantees the eps-embedding is given by:</source>
          <target state="translated">eps-embeddingを保証するためのコンポーネントの最小数は次のように与えられる。</target>
        </trans-unit>
        <trans-unit id="78f0aa92767e958a159a7201fe662ff62e3924a6" translate="yes" xml:space="preserve">
          <source>The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and &lt;code&gt;min_features_to_select&lt;/code&gt; isn&amp;rsquo;t divisible by &lt;code&gt;step&lt;/code&gt;.</source>
          <target state="translated">選択する機能の最小数。元のフィーチャ数と &lt;code&gt;min_features_to_select&lt;/code&gt; の差が &lt;code&gt;step&lt;/code&gt; 割り切れない場合でも、この数のフィーチャは常にスコアリングされます。</target>
        </trans-unit>
        <trans-unit id="dc2c2ff1c0458b789a8dbc35522b80adbfc0fde6" translate="yes" xml:space="preserve">
          <source>The minimum number of samples per leaf. For small datasets with less than a few hundred samples, it is recommended to lower this value since only very shallow trees would be built.</source>
          <target state="translated">葉1枚あたりのサンプル数の最小値.サンプル数が数百以下の小さなデータセットでは,非常に浅い木しか作られないので,この値を下げることが推奨される.</target>
        </trans-unit>
        <trans-unit id="1db45f422759f4529cb388eaa249fdf3a381a4d3" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least &lt;code&gt;min_samples_leaf&lt;/code&gt; training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</source>
          <target state="translated">リーフノードに存在するために必要なサンプルの最小数。深さのある分割ポイントは、左右のブランチのそれぞれに少なくとも &lt;code&gt;min_samples_leaf&lt;/code&gt; トレーニングサンプルが残っている場合にのみ考慮されます。これは、特に回帰において、モデルを平滑化する効果がある場合があります。</target>
        </trans-unit>
        <trans-unit id="754bd23e1994f02a9f11fc7f2013baebc017ca4d" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to split an internal node:</source>
          <target state="translated">内部ノードを分割するのに必要な最小サンプル数。</target>
        </trans-unit>
        <trans-unit id="28930c1108db91ea501be86c776a39ad33671bd9" translate="yes" xml:space="preserve">
          <source>The minimum score is zero, with lower values indicating better clustering.</source>
          <target state="translated">最小スコアは0で、低い値はより良いクラスタリングを示します。</target>
        </trans-unit>
        <trans-unit id="cad4521e1577b76d3fbf78851e9af149127ba4db" translate="yes" xml:space="preserve">
          <source>The minimum valid value the parameter can take. If None (default) it is implied that the parameter does not have a lower bound.</source>
          <target state="translated">パラメータが取ることができる有効な最小値。None (デフォルト)の場合は、パラメータに下限がないことを意味します。</target>
        </trans-unit>
        <trans-unit id="34dec0ced4163b0b0c5f6b2dfda2c2ae915251bf" translate="yes" xml:space="preserve">
          <source>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</source>
          <target state="translated">リーフノードに存在するために必要な(すべての入力サンプルの)重みの合計の最小重み分数。sample_weightが指定されていない場合は,サンプルの重みは等しくなります.</target>
        </trans-unit>
        <trans-unit id="adbfb8d83e84eef2c959ef548acb01276646831a" translate="yes" xml:space="preserve">
          <source>The missing indicator for input data. The data type of &lt;code&gt;Xt&lt;/code&gt; will be boolean.</source>
          <target state="translated">入力データの欠落している標識。 &lt;code&gt;Xt&lt;/code&gt; のデータ型はブール値になります。</target>
        </trans-unit>
        <trans-unit id="2b5249d3ed9eb260ab7aedd15c61af2c7df4b9f1" translate="yes" xml:space="preserve">
          <source>The mixing matrix to be used to initialize the algorithm.</source>
          <target state="translated">アルゴリズムの初期化に使用される混合行列。</target>
        </trans-unit>
        <trans-unit id="bc7bce3b2af118e0efad437caf7d72239aa18a90" translate="yes" xml:space="preserve">
          <source>The mixing matrix.</source>
          <target state="translated">混合マトリックス。</target>
        </trans-unit>
        <trans-unit id="d9852ae9396dc8e73ffef0a95fb9e0d330c7fbbc" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.</source>
          <target state="translated">このモデルは、すべてのクラスが同じ共分散行列を共有していると仮定して、各クラスにガウス密度を適合させます。</target>
        </trans-unit>
        <trans-unit id="c1fbc40d4a13f1c2e80dae47c2199d2a0ef37a24" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class.</source>
          <target state="translated">モデルはガウス密度を各クラスにフィットさせます。</target>
        </trans-unit>
        <trans-unit id="c383334c8db0249de2e6de6266d715cf757d0e4f" translate="yes" xml:space="preserve">
          <source>The model learnt is far from being a good model making accurate predictions: this is obvious when looking at the plot above, where good predictions should lie on the red line.</source>
          <target state="translated">学習したモデルは、正確な予測を行う優れたモデルとは程遠いものです:これは、上のプロットを見れば明らかです。</target>
        </trans-unit>
        <trans-unit id="744fe4c01d7aac1fa2b93515c5b761f9f450f5fc" translate="yes" xml:space="preserve">
          <source>The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.</source>
          <target state="translated">モデルは、入力の分布に関する仮定を行います。現時点では、scikit-learnは&lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt;のみを提供します。これは、入力がバイナリ値または0と1の間の値のいずれかであると想定し、それぞれが特定の機能がオンになる確率をエンコードします。</target>
        </trans-unit>
        <trans-unit id="3e7d91224c848a3199f89b8ed290703400860de0" translate="yes" xml:space="preserve">
          <source>The model need to have probability information computed at training time: fit with attribute &lt;code&gt;probability&lt;/code&gt; set to True.</source>
          <target state="translated">モデルは、トレーニング時に計算された確率情報を持つ必要があります。属性 &lt;code&gt;probability&lt;/code&gt; をTrueに設定してフィットします。</target>
        </trans-unit>
        <trans-unit id="f6f2922bb9e8bafadd9354168cde2a4c9535aa4f" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; attributes: &lt;code&gt;coef_&lt;/code&gt; holds the weights \(w\) and &lt;code&gt;intercept_&lt;/code&gt; holds \(b\).</source>
          <target state="translated">モデルパラメータには、 &lt;code&gt;coef_&lt;/code&gt; 属性と &lt;code&gt;intercept_&lt;/code&gt; 属性を介してアクセスできます &lt;code&gt;coef_&lt;/code&gt; は重み\（w \）を保持し、 &lt;code&gt;intercept_&lt;/code&gt; は\（b \）を保持します。</target>
        </trans-unit>
        <trans-unit id="532fcfc5fc4303622a7e9b0379fb5dd6d278b658" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the members &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;:</source>
          <target state="translated">モデルパラメーターには、メンバー &lt;code&gt;coef_&lt;/code&gt; および &lt;code&gt;intercept_&lt;/code&gt; を介してアクセスできます。</target>
        </trans-unit>
        <trans-unit id="87fcd2dcd4a9683677aa4a82e264db34c1778c7c" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.</source>
          <target state="translated">サポート・ベクトル分類(上述のように)によって生成されたモデルは,モデルを構築するためのコスト関数が,マージンを超えたところにある訓練点を気にしないので,訓練データのサブセットだけに依存する.同様に,サポート・ベクトル回帰によって生成されるモデルは,モデルを構築するためのコスト関数が,モデルの予測に近い訓練データを無視するので,訓練データのサブセットにのみ依存する.</target>
        </trans-unit>
        <trans-unit id="14cc80df7b1b8b9f6ebfb922c7e5dfe3e7692df9" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.</source>
          <target state="translated">サポート・ベクトル分類(上述のように)によって生成されたモデルは、モデルを構築するためのコスト関数が、マージンを超えたところにある訓練点を気にしないので、訓練データのサブセットだけに依存する。同様に,サポート・ベクトル回帰によって生成されるモデルは,予測がターゲットに近いサンプルを無視するコスト関数があるため,訓練データのサブセットにのみ依存します.</target>
        </trans-unit>
        <trans-unit id="4911f4659c5f4e900454be23c5d2d4b6dba06b2b" translate="yes" xml:space="preserve">
          <source>The model will stay unchanged.</source>
          <target state="translated">機種変更はありません。</target>
        </trans-unit>
        <trans-unit id="ffaf4f38449ad493c6f07021545ef1cc0f916269" translate="yes" xml:space="preserve">
          <source>The models are ordered from strongest regularized to least regularized. The 4 coefficients of the models are collected and plotted as a &amp;ldquo;regularization path&amp;rdquo;: on the left-hand side of the figure (strong regularizers), all the coefficients are exactly 0. When regularization gets progressively looser, coefficients can get non-zero values one after the other.</source>
          <target state="translated">モデルは、最も強い正則化から最も低い正則化へと並べられます。モデルの4つの係数が収集され、「正規化パス」としてプロットされます。図の左側（強力な正則化器）では、すべての係数が正確に0になります。正則化が徐々に緩くなると、係数はゼロ以外になる可能性があります次々と値。</target>
        </trans-unit>
        <trans-unit id="41864e959b3c3945768dfd483a7ad2160aff5a56" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;.</source>
          <target state="translated">モジュール&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; に&lt;/a&gt;は、Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;によって1995年に導入された一般的なブースティングアルゴリズムAdaBoostが含まれています。</target>
        </trans-unit>
        <trans-unit id="1ad1575f52119d57fdeed35bbf3eb9cde06a6188" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted decision trees.</source>
          <target state="translated">モジュール&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;は、勾配ブーストされた決定木を介して分類と回帰の両方のメソッドを提供します。</target>
        </trans-unit>
        <trans-unit id="06014352d795d21d24d63a43012b2cdda688123a" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted regression trees.</source>
          <target state="translated">モジュール&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;は、勾配ブースト回帰ツリーによる分類と回帰の両方のメソッドを提供します。</target>
        </trans-unit>
        <trans-unit id="c3b04b84598eb19b700a6f5a28a6ebcc8d4034a0" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; also exposes a set of simple functions measuring a prediction error given ground truth and prediction:</source>
          <target state="translated">モジュール&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt;は、グラウンドトゥルースと予測が与えられた場合の予測エラーを測定する一連の単純な関数も公開します。</target>
        </trans-unit>
        <trans-unit id="a4ba63f9b8e0d9fa0a182e0bb4dc5760121e3e0e" translate="yes" xml:space="preserve">
          <source>The module &lt;code&gt;partial_dependence&lt;/code&gt; provides a convenience function &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; and a two-way PDP between the two features:</source>
          <target state="translated">モジュール &lt;code&gt;partial_dependence&lt;/code&gt; は、一方向および双方向の部分依存プロットを作成するための便利な関数&lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt;を提供します。以下の例では、部分依存プロットのグリッドを作成する方法を示します &lt;code&gt;0&lt;/code&gt; フィーチャ0と &lt;code&gt;1&lt;/code&gt; の2つの一方向PDPと、2つのフィーチャ間の双方向PDP：</target>
        </trans-unit>
        <trans-unit id="5b2518b0fdafd75a6658a4d2a018bccb79345ce3" translate="yes" xml:space="preserve">
          <source>The module contains the public attributes &lt;code&gt;coefs_&lt;/code&gt; and &lt;code&gt;intercepts_&lt;/code&gt;. &lt;code&gt;coefs_&lt;/code&gt; is a list of weight matrices, where weight matrix at index \(i\) represents the weights between layer \(i\) and layer \(i+1\). &lt;code&gt;intercepts_&lt;/code&gt; is a list of bias vectors, where the vector at index \(i\) represents the bias values added to layer \(i+1\).</source>
          <target state="translated">このモジュールには、パブリック属性 &lt;code&gt;coefs_&lt;/code&gt; および &lt;code&gt;intercepts_&lt;/code&gt; が含まれています。 &lt;code&gt;coefs_&lt;/code&gt; は重み行列のリストです。インデックス\（i \）の重み行列は、レイヤー\（i \）とレイヤー\（i + 1 \）の間の重みを表します。 &lt;code&gt;intercepts_&lt;/code&gt; はバイアスベクトルのリストで、インデックス\（i \）のベクトルはレイヤー\（i + 1 \）に追加されたバイアス値を表します。</target>
        </trans-unit>
        <trans-unit id="b31fa7fd3ac8f2a64f0dd29549e8dd9abb96ee19" translate="yes" xml:space="preserve">
          <source>The module: &lt;code&gt;random_projection&lt;/code&gt; provides several tools for data reduction by random projections. See the relevant section of the documentation: &lt;a href=&quot;random_projection#random-projection&quot;&gt;Random Projection&lt;/a&gt;.</source>
          <target state="translated">モジュール： &lt;code&gt;random_projection&lt;/code&gt; は、ランダム投影によるデータ削減のためのいくつかのツールを提供します。ドキュメントの関連セクションを参照してください：&lt;a href=&quot;random_projection#random-projection&quot;&gt;ランダムプロジェクション&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="81f532ac90d157aeffd690ee0c3b7aa6b7bbd149" translate="yes" xml:space="preserve">
          <source>The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of &lt;code&gt;_fit_stages&lt;/code&gt; as keyword arguments &lt;code&gt;callable(i, self,
locals())&lt;/code&gt;. If the callable returns &lt;code&gt;True&lt;/code&gt; the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.</source>
          <target state="translated">モニターは、各反復の後に、現在の反復、推定器への参照、および &lt;code&gt;_fit_stages&lt;/code&gt; のローカル変数をキーワード引数 &lt;code&gt;callable(i, self, locals())&lt;/code&gt; として呼び出します。呼び出し可能オブジェクトが &lt;code&gt;True&lt;/code&gt; を返す場合、フィッティング手順は停止されます。モニターは、延期された見積もりの​​計算、早期停止、モデルのイントロスペクト、スナップショットなど、さまざまな目的に使用できます。</target>
        </trans-unit>
        <trans-unit id="52aa15c9df5da45110f63c8c26b8cfa477557d62" translate="yes" xml:space="preserve">
          <source>The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the &lt;strong&gt;regularization path&lt;/strong&gt; of the estimator.</source>
          <target state="translated">この戦略を適用できる最も一般的なパラメーターは、レギュラライザーの強度をエンコードするパラメーターです。この場合、推定量の&lt;strong&gt;正則化パス&lt;/strong&gt;を計算するとします。</target>
        </trans-unit>
        <trans-unit id="3156312e704bdb91fdff12aa2e110d4f21e21302" translate="yes" xml:space="preserve">
          <source>The most intuitive way to do so is to use a bags of words representation:</source>
          <target state="translated">最も直感的な方法は、言葉の表現の袋を使うことです。</target>
        </trans-unit>
        <trans-unit id="399fde41b63b2ea676c5145583a3950879f6c9fa" translate="yes" xml:space="preserve">
          <source>The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.</source>
          <target state="translated">このスケーリングを使用する動機は、特徴量の非常に小さな標準偏差に対するロバスト性と、疎なデータのゼロエントリを保存することです。</target>
        </trans-unit>
        <trans-unit id="3ec6281766fab2b8f9e9f9f6e92da7d4704e6316" translate="yes" xml:space="preserve">
          <source>The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable.</source>
          <target state="translated">マルチタスク・レーザは、選択された特徴がタスク間で同じであることを強制することで、複数の回帰問題を共同で適合させることができます。この例は、連続した測定をシミュレートし、各タスクは時間の瞬間であり、関連する特徴は同じでありながら時間の経過とともに振幅が変化します。マルチタスクラッソは、ある時点で選択された特徴がすべての時点で選択されることを強制します。これにより、ラッソによる特徴の選択がより安定したものとなる。</target>
        </trans-unit>
        <trans-unit id="7b55b776834f44186e095c0df9ee2b704ce3630c" translate="yes" xml:space="preserve">
          <source>The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature:</source>
          <target state="translated">ここでのマルチクラスの定義は、文献には一定のコンセンサスはありませんが、二値分類で使用されるメトリックの最も合理的な拡張であると思われます。</target>
        </trans-unit>
        <trans-unit id="5fdb408d7bc477f0762da630aa0f3aea39db3f13" translate="yes" xml:space="preserve">
          <source>The multiclass support is handled according to a one-vs-one scheme.</source>
          <target state="translated">マルチクラス対応は、1対1のスキームに従って処理されます。</target>
        </trans-unit>
        <trans-unit id="559112a07c580b9f5163f4eaab25d31b6f252bc0" translate="yes" xml:space="preserve">
          <source>The multilabel_confusion_matrix calculates class-wise or sample-wise multilabel confusion matrices, and in multiclass tasks, labels are binarized under a one-vs-rest way; while confusion_matrix calculates one confusion matrix for confusion between every two classes.</source>
          <target state="translated">multilabel_confusion_matrixは、クラス単位またはサンプル単位のマルチラベル混同行列を計算し、多クラスタスクでは、ラベルは1vsレストの方法で2値化されます。</target>
        </trans-unit>
        <trans-unit id="66359e593d021aa5ae2d568f4acc056ec0bf4a32" translate="yes" xml:space="preserve">
          <source>The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.</source>
          <target state="translated">多項式ナイーブベイズ分類器は、離散的な特徴(例えば、テキスト分類のための単語数)を持つ分類に適しています。多項分布は、通常、整数の特徴数を必要とします。しかし、実際には、tf-idfのような小数のカウントも有効です。</target>
        </trans-unit>
        <trans-unit id="9fcb660998dc7edb2d34f5dc5854df445bad9a92" translate="yes" xml:space="preserve">
          <source>The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:</source>
          <target state="translated">複数のメトリクスは、リスト、タプル、または定義済みスコアラー名のセットのいずれかで指定することができます。</target>
        </trans-unit>
        <trans-unit id="0f55ab2d94390a312af5800dfb617d9ab1f868e3" translate="yes" xml:space="preserve">
          <source>The name of the hyperparameter. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds</source>
          <target state="translated">ハイパーパラメータの名前。「x」という名前のハイパーパラメータを使用するカーネルには、属性self.xおよびself.x_boundsが必要であることに注意してください。</target>
        </trans-unit>
        <trans-unit id="66cdcb45ef3965759b13ccc7a57841383638bd06" translate="yes" xml:space="preserve">
          <source>The name of the parameter to be printed in error messages.</source>
          <target state="translated">エラーメッセージに表示されるパラメータの名前です。</target>
        </trans-unit>
        <trans-unit id="a09e250651300d47ed9cd74b128d0a12a5aa8e3e" translate="yes" xml:space="preserve">
          <source>The name of the sample image loaded</source>
          <target state="translated">読み込んだサンプル画像の名前</target>
        </trans-unit>
        <trans-unit id="b853190860f83bcb94f9f4edb130e6f04adf4ae5" translate="yes" xml:space="preserve">
          <source>The names &lt;code&gt;vect&lt;/code&gt;, &lt;code&gt;tfidf&lt;/code&gt; and &lt;code&gt;clf&lt;/code&gt; (classifier) are arbitrary. We will use them to perform grid search for suitable hyperparameters below. We can now train the model with a single command:</source>
          <target state="translated">&lt;code&gt;vect&lt;/code&gt; 、 &lt;code&gt;tfidf&lt;/code&gt; 、および &lt;code&gt;clf&lt;/code&gt; （分類子）という名前は任意です。これらを使用して、以下の適切なハイパーパラメータのグリッド検索を実行します。これで、1つのコマンドでモデルをトレーニングできます。</target>
        </trans-unit>
        <trans-unit id="37788c74066a15702915a906173e5747b775560c" translate="yes" xml:space="preserve">
          <source>The names of features</source>
          <target state="translated">特徴の名前</target>
        </trans-unit>
        <trans-unit id="4a6d82313da18df77f363e5439a6b3ce08c6680a" translate="yes" xml:space="preserve">
          <source>The names of target classes.</source>
          <target state="translated">対象となるクラスの名前です。</target>
        </trans-unit>
        <trans-unit id="9146e93a1405c3d2cac65e0084d1b1c7a951b3b3" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns</source>
          <target state="translated">データセットの列の名前</target>
        </trans-unit>
        <trans-unit id="9c71b3cc12d889ddb9194bd05e135e431b48c18f" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns.</source>
          <target state="translated">データセットの列の名前。</target>
        </trans-unit>
        <trans-unit id="5e5fcf3b8aec8853bca895bccf214ac28ac4d27e" translate="yes" xml:space="preserve">
          <source>The names of the target columns</source>
          <target state="translated">対象となるカラムの名前</target>
        </trans-unit>
        <trans-unit id="40ffca0689344b78c3e10b40b262e0c45ea9b50f" translate="yes" xml:space="preserve">
          <source>The names of the target columns.</source>
          <target state="translated">対象となる列の名前。</target>
        </trans-unit>
        <trans-unit id="1942d0e2fca00caa96a5a7573d350784bbfcadf1" translate="yes" xml:space="preserve">
          <source>The new backend can then be selected by passing its name as the backend argument to the Parallel class. Moreover, the default backend can be overwritten globally by setting make_default=True.</source>
          <target state="translated">そして、Parallelクラスの引数にバックエンド名を渡すことで、新しいバックエンドを選択することができます。さらに、make_default=Trueを設定することで、デフォルトのバックエンドをグローバルに上書きすることができます。</target>
        </trans-unit>
        <trans-unit id="a0fb0e8bf0410fa55523d36fa2a7a49ac4415117" translate="yes" xml:space="preserve">
          <source>The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.</source>
          <target state="translated">新しいdtypeは、元の型に応じてnp.float32またはnp.float64になります。この関数は、引数のコピーに応じて、コピーを作成したり、引数を修正したりすることができます。</target>
        </trans-unit>
        <trans-unit id="6d7b951e7afa9271dd7834467cb67e175d7e626f" translate="yes" xml:space="preserve">
          <source>The new entry \(d(u,v)\) is computed as follows,</source>
          <target state="translated">The new entry \(d(u,v)\)は、以下のように計算されます。</target>
        </trans-unit>
        <trans-unit id="c771668f5d4b4d7aa145f31455a67e2ba21af3ce" translate="yes" xml:space="preserve">
          <source>The next figure compares the results obtained for the different type of the weight concentration prior (parameter &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;) for different values of &lt;code&gt;weight_concentration_prior&lt;/code&gt;. Here, we can see the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is &amp;lsquo;dirichlet_distribution&amp;rsquo; while this is not necessarily the case for the &amp;lsquo;dirichlet_process&amp;rsquo; type (used by default).</source>
          <target state="translated">次の図は、前重量濃度の異なるタイプについて得られた結果（パラメータ比較 &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; の異なる値に対して） &lt;code&gt;weight_concentration_prior&lt;/code&gt; を。ここでは、 &lt;code&gt;weight_concentration_prior&lt;/code&gt; パラメータの値が、取得されるアクティブなコンポーネントの有効数に強い影響を与えることがわかります。また、事前のタイプが「dirichlet_distribution」の場合、事前の濃度重みの値が大きいと重みがより均一になることにも注意できますが、これは必ずしも「dirichlet_process」タイプの場合とは限りません（デフォルトで使用されます）。</target>
        </trans-unit>
        <trans-unit id="c25f5050f6fd3011382e4c50ae76e75315ba1a26" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; scales better. With regard to prediction time, &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="translated">次の図は、トレーニングセットのさまざまなサイズに対する&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; の&lt;/a&gt;フィッティングと予測の時間を比較しています。&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; の&lt;/a&gt;フィッティングは、中規模のトレーニングセット（1000サンプル未満）の&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;よりも高速です。ただし、トレーニングセットが大きい場合は、&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; の&lt;/a&gt;スケーリングが向上します。予測時間に関しては、学習されたスパースソリューションのため、&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;はトレーニングセットのすべてのサイズで&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;よりも高速です。スパース性の程度、したがって予測時間は、&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;のパラメーター\（\ epsilon \）および\（C \）に依存することに注意してください。 \（\ epsilon = 0 \）は、密なモデルに対応します。</target>
        </trans-unit>
        <trans-unit id="b6d38fd34e131451ba8b974153991556ff8b9398" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;code&gt;SVR&lt;/code&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;code&gt;SVR&lt;/code&gt; scales better. With regard to prediction time, &lt;code&gt;SVR&lt;/code&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;code&gt;SVR&lt;/code&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="translated">次の図は、トレーニングセットのさまざまなサイズに対する&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;と &lt;code&gt;SVR&lt;/code&gt; のフィッティングと予測の時間を比較しています。&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; の&lt;/a&gt;フィッティングは、中規模のトレーニングセット（1000サンプル未満）の &lt;code&gt;SVR&lt;/code&gt; よりも高速です。ただし、トレーニングセットが大きいほど、 &lt;code&gt;SVR&lt;/code&gt; のスケーリングが向上します。&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;時間の点では、学習されたスパースソリューションのため、トレーニングセットのすべてのサイズで &lt;code&gt;SVR&lt;/code&gt; はKernelRidgeよりも高速です。スパース性の程度、したがって予測時間は、 &lt;code&gt;SVR&lt;/code&gt; のパラメーター\（\ epsilon \）および\（C \）に依存することに注意してください。 \（\ epsilon = 0 \）は密なモデルに対応します。</target>
        </trans-unit>
        <trans-unit id="7fb8a1fd588ff5df76899e15821355218c8c70df" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of KRR and SVR for different sizes of the training set. Fitting KRR is faster than SVR for medium- sized training sets (less than 1000 samples); however, for larger training sets SVR scales better. With regard to prediction time, SVR is faster than KRR for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters epsilon and C of the SVR.</source>
          <target state="translated">次の図は、訓練セットのサイズが異なる場合のKRRとSVRのフィッティングと予測の時間を比較したものです。中規模の訓練セット(1000サンプル以下)では、KRRのフィッティングはSVRよりも速いですが、大規模な訓練セットではSVRの方がスケールが大きくなります。予測時間に関しては、学習された疎な解のため、学習された訓練セットのすべてのサイズにおいて、SVRの方がKRRよりも高速です。疎さの程度と予測時間は、SVRのパラメータεとCに依存することに注意してください。</target>
        </trans-unit>
        <trans-unit id="b8b8c72913234ec998c925f50d9fa1a29ef7082f" translate="yes" xml:space="preserve">
          <source>The next image illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">次の画像は、シグモイド校正が3クラスの分類問題の予測確率をどのように変化させるかを示しています。図示されているのは、3つの角が3つのクラスに対応する標準の2-simplexです。矢印は、校正されていない分類器によって予測された確率ベクトルから、ホールドアウト検証セットでのシグモイド校正後の同じ分類器によって予測された確率ベクトルを指しています。色は、インスタンスの真のクラスを示します(赤:クラス1、緑:クラス2、青:クラス3)。</target>
        </trans-unit>
        <trans-unit id="d16a780143f0785e9248e298dec17c9fe8de9d1e" translate="yes" xml:space="preserve">
          <source>The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity.</source>
          <target state="translated">ノードは、その状態が接続されている他のノードの状態に依存するランダム変数である。したがって,モデルは,接続の重みと,可視単位と非表示単位のそれぞれについての切片(バイアス)項によってパラメータ化されています(図では簡略化のために省略しています).</target>
        </trans-unit>
        <trans-unit id="55f688a90e745dd5020f6b5c567bbe8f6396fa6e" translate="yes" xml:space="preserve">
          <source>The noise level in the targets can be specified by passing it via the parameter &lt;code&gt;alpha&lt;/code&gt;, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below).</source>
          <target state="translated">ターゲットのノイズレベルは、パラメーター &lt;code&gt;alpha&lt;/code&gt; を介してグローバルに、またはデータポイントごとに渡すことで指定できます。適度なノイズレベルは、ティコノフの正則化として効果的に実装されるため、フィッティング中に数値の問題を処理する場合にも役立ちます。つまり、カーネルマトリックスの対角に追加します。ノイズレベルを明示的に指定する代わりに、カーネルにWhiteKernelコンポーネントを含めることもできます。これにより、データからグローバルノイズレベルを推定できます（以下の例を参照）。</target>
        </trans-unit>
        <trans-unit id="99b58f648d173c7793dc0e913318a8835572f0c2" translate="yes" xml:space="preserve">
          <source>The non-fixed, log-transformed hyperparameters of the kernel</source>
          <target state="translated">カーネルの非固定対数変換されたハイパーパラメータ</target>
        </trans-unit>
        <trans-unit id="8ea3cf8563db967f688cb46c0883a9d020905a15" translate="yes" xml:space="preserve">
          <source>The nonmetric algorithm adds a monotonic regression step before computing the stress.</source>
          <target state="translated">非メトリックアルゴリズムは、応力を計算する前に単調回帰ステップを追加します。</target>
        </trans-unit>
        <trans-unit id="8cecdfcc92ccd83125ecaa6c4beb7447e43f92cb" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).</source>
          <target state="translated">各非ゼロサンプル(軸が0の場合は各非ゼロ特徴量)を正規化するために使用するノルム。</target>
        </trans-unit>
        <trans-unit id="39e8a7c3ff72c23082d4ee16a84d74279c8584ba" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample.</source>
          <target state="translated">各非ゼロサンプルを正規化するために使用するノルム。</target>
        </trans-unit>
        <trans-unit id="6d3c61aac7334b6f6f54f73be53f19b464b34cbe" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample. If norm=&amp;rsquo;max&amp;rsquo; is used, values will be rescaled by the maximum of the absolute values.</source>
          <target state="translated">ゼロ以外の各サンプルを正規化するために使用する基準。norm = 'max'が使用されている場合、値は絶対値の最大値によって再スケーリングされます。</target>
        </trans-unit>
        <trans-unit id="b4510db24d586496f5cd27e9cd8024ffe16a368e" translate="yes" xml:space="preserve">
          <source>The normalized mutual information is defined as</source>
          <target state="translated">正規化された相互情報は</target>
        </trans-unit>
        <trans-unit id="ad22e0beb8a3a9f5574d757b3678d39d78057ba3" translate="yes" xml:space="preserve">
          <source>The normalizer instance can then be used on sample vectors as any transformer:</source>
          <target state="translated">ノーマライザのインスタンスは、他の変換器と同様にサンプルベクトル上で使用することができます。</target>
        </trans-unit>
        <trans-unit id="82d79be22b3b3fa23f79d393f8a5b628a27ddb08" translate="yes" xml:space="preserve">
          <source>The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below).</source>
          <target state="translated">考慮される隣人の数k(別名パラメータn_neighbors)は、一般的に、1)他のオブジェクトがこのクラスタに対して局所的な外れ値となり得るように、クラスタが含まなければならないオブジェクトの最小数よりも大きく、2)局所的な外れ値となり得る近くのオブジェクトの最大数よりも小さい値が選ばれます。実際には、このような情報は一般的には得られず、n_neighbors=20とすることが一般的にはうまくいくようです。外れ値の割合が高い場合(以下の例のように10%よりも大きい場合)、n_neighborsはより大きくすべきです(以下の例ではn_neighbors=35)。</target>
        </trans-unit>
        <trans-unit id="9650bdc07aaa5281bfc3be8ecd4292573a8743c6" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to compute the partial dependences. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">部分的な依存関係を計算するために使用するCPUの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="4a35a9d90f2abd9af864433913570f3dbe7f2765" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">OVA（マルチクラス問題の場合は1つとすべて）の計算に使用するCPUの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="985a6706e191013209fa45542581d0ffa377ae50" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">OVA（マルチクラス問題の場合は1対すべて）の計算に使用するCPUの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="36ae328d3607f79ec631b3fe327da7ed8cf8098b" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するCPUの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="2e29d4d145cd20eb08e9ea5571074f934df7e0d8" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算を行うために使用するCPUの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="720879c36f3df22b5ec2b112039efcd8b5ba8b1b" translate="yes" xml:space="preserve">
          <source>The number of EM iterations to perform.</source>
          <target state="translated">実行するEMの反復回数。</target>
        </trans-unit>
        <trans-unit id="df46ce41d5f6eae9d9abc12693e64e5ffe0f042c" translate="yes" xml:space="preserve">
          <source>The number of OpenMP threads to use for the computation. Parallelism is sample-wise on the main cython loop which assigns each sample to its closest center.</source>
          <target state="translated">計算に使用する OpenMP スレッド数。並列処理はメインのcythonループ上でサンプル単位で行われ、各サンプルは最も近い中心に割り当てられます。</target>
        </trans-unit>
        <trans-unit id="40648db4f1524a67ed66976c14cc75bac6eed386" translate="yes" xml:space="preserve">
          <source>The number of atomic tasks to dispatch at once to each worker. When individual evaluations are very fast, dispatching calls to workers can be slower than sequential computation because of the overhead. Batching fast computations together can mitigate this. The &lt;code&gt;'auto'&lt;/code&gt; strategy keeps track of the time it takes for a batch to complete, and dynamically adjusts the batch size to keep the time on the order of half a second, using a heuristic. The initial batch size is 1. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; with &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; will dispatch batches of a single task at a time as the threading backend has very little overhead and using larger batch size has not proved to bring any gain in that case.</source>
          <target state="translated">各ワーカーに一度にディスパッチするアトミックタスクの数。個々の評価が非常に高速である場合、オーバーヘッドのため、ワーカーへの呼び出しのディスパッチは順次計算よりも遅くなる可能性があります。高速計算をバッチ処理すると、これを軽減できます。 &lt;code&gt;'auto'&lt;/code&gt; の戦略は、それが完全にバッチにかかる時間を追跡し、動的ヒューリスティックを使用して、半秒のオーダーの時間を保つためにバッチサイズを調整します。最初のバッチサイズは1です。 &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; と &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; スレッドのバックエンドは非常にわずかなオーバーヘッドがあるとして、一度に単一のタスクのバッチを派遣し、より大きなバッチサイズを使用すると、その中の任意の利得をもたらすことが証明されていません場合。</target>
        </trans-unit>
        <trans-unit id="3ae39cb6a942b138204dbb141781bfb4c75e3760" translate="yes" xml:space="preserve">
          <source>The number of base estimators in the ensemble.</source>
          <target state="translated">アンサンブルに含まれる基底推定器の数。</target>
        </trans-unit>
        <trans-unit id="9fe1c6517ea4c36af295e91a2e2410361ff8432c" translate="yes" xml:space="preserve">
          <source>The number of batches (of tasks) to be pre-dispatched. Default is &amp;lsquo;2*n_jobs&amp;rsquo;. When batch_size=&amp;rdquo;auto&amp;rdquo; this is reasonable default and the workers should never starve.</source>
          <target state="translated">事前ディスパッチされる（タスクの）バッチの数。デフォルトは「2 * n_jobs」です。batch_size =&amp;rdquo; auto&amp;rdquo;の場合、これは妥当なデフォルトであり、ワーカーが決して飢えてはいけません。</target>
        </trans-unit>
        <trans-unit id="f9d0a981566d58378881b2cabe57053da9d34fb6" translate="yes" xml:space="preserve">
          <source>The number of biclusters to find.</source>
          <target state="translated">ビックラスターの数字の語呂合わせを見つけます。</target>
        </trans-unit>
        <trans-unit id="b9c52ec2125ed0e2339a6eae69fb246b4dc5348e" translate="yes" xml:space="preserve">
          <source>The number of biclusters.</source>
          <target state="translated">バイクラスターの数です。</target>
        </trans-unit>
        <trans-unit id="d76125e41f0deb521acf9ed71af5267a484f2d30" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="translated">生成するビンの数。 &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt; 場合、ValueErrorを発生させます。</target>
        </trans-unit>
        <trans-unit id="3424019fa75a2f96f62b30f8336ea4cebfa5c4fe" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. The intervals for the bins are determined by the minimum and maximum of the input data. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="translated">生成するビンの数。ビンの間隔は、入力データの最小値と最大値によって決まります。 &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt; 場合、ValueErrorを発生させます。</target>
        </trans-unit>
        <trans-unit id="b2f078045a64e3b870ff56d7d5abb73dda7d8a43" translate="yes" xml:space="preserve">
          <source>The number of bins used to bin the data is controlled with the &lt;code&gt;max_bins&lt;/code&gt; parameter. Using less bins acts as a form of regularization. It is generally recommended to use as many bins as possible, which is the default.</source>
          <target state="translated">データをビン化するために使用されるビンの数は、 &lt;code&gt;max_bins&lt;/code&gt; パラメーターで制御されます。使用するビンの数を減らすことは、正則化の一形態として機能します。通常、デフォルトであるできるだけ多くのビンを使用することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="016567b86fa6f6a4b4c043763b4e841bd445fc32" translate="yes" xml:space="preserve">
          <source>The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.</source>
          <target state="translated">ブーストするステージの数を指定します。勾配ブーストはオーバーフィットにかなり強いので、通常は数が多い方がパフォーマンスが向上します。</target>
        </trans-unit>
        <trans-unit id="d5d9cefb2fff3dbdd417050b8ce7c297d42aaa9f" translate="yes" xml:space="preserve">
          <source>The number of claims (&lt;code&gt;ClaimNb&lt;/code&gt;) is a positive integer (0 included). Thus, this target can be modelled by a Poisson distribution. It is then assumed to be the number of discrete events occuring with a constant rate in a given time interval (&lt;code&gt;Exposure&lt;/code&gt;, in units of years). Here we model the frequency &lt;code&gt;y = ClaimNb / Exposure&lt;/code&gt;, which is still a (scaled) Poisson distribution, and use &lt;code&gt;Exposure&lt;/code&gt; as &lt;code&gt;sample_weight&lt;/code&gt;.</source>
          <target state="translated">クレームの数（ &lt;code&gt;ClaimNb&lt;/code&gt; ）は正の整数（0を含む）です。したがって、このターゲットはポアソン分布でモデル化できます。次に、特定の時間間隔（ &lt;code&gt;Exposure&lt;/code&gt; 、年単位）で一定の割合で発生する離散イベントの数であると想定されます。ここでは、頻度 &lt;code&gt;y = ClaimNb / Exposure&lt;/code&gt; をモデル化します。これは、依然として（スケーリングされた）ポアソン分布であり、 &lt;code&gt;Exposure&lt;/code&gt; を &lt;code&gt;sample_weight&lt;/code&gt; として使用します。</target>
        </trans-unit>
        <trans-unit id="a186b2a8600030ec87eb172dc6f657942fddfcfb" translate="yes" xml:space="preserve">
          <source>The number of claims (&lt;code&gt;ClaimNb&lt;/code&gt;) is a positive integer that can be modeled as a Poisson distribution. It is then assumed to be the number of discrete events occurring with a constant rate in a given time interval (&lt;code&gt;Exposure&lt;/code&gt;, in units of years).</source>
          <target state="translated">クレームの数（ &lt;code&gt;ClaimNb&lt;/code&gt; ）は、ポアソン分布としてモデル化できる正の整数です。次に、特定の時間間隔（ &lt;code&gt;Exposure&lt;/code&gt; 、年単位）で一定の割合で発生する離散イベントの数であると想定されます。</target>
        </trans-unit>
        <trans-unit id="d31e8a7065eae23aeb71500302a31ebb485560ec" translate="yes" xml:space="preserve">
          <source>The number of classes</source>
          <target state="translated">クラスの数</target>
        </trans-unit>
        <trans-unit id="77a0204799f583fccb414a4215d0dc841510a2f7" translate="yes" xml:space="preserve">
          <source>The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).</source>
          <target state="translated">クラスの数(単一出力問題の場合)、または各出力(複数出力問題の場合)のクラスの数を含むリスト。</target>
        </trans-unit>
        <trans-unit id="221daa93ea83049b71eff667b2da7d0ce3fa89ae" translate="yes" xml:space="preserve">
          <source>The number of classes (or labels) of the classification problem.</source>
          <target state="translated">分類問題のクラス(またはラベル)の数。</target>
        </trans-unit>
        <trans-unit id="7b75bf8cc583f50195e96e51d244ce57f23360a0" translate="yes" xml:space="preserve">
          <source>The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).</source>
          <target state="translated">クラスの数(単一出力問題)、または各出力(複数出力問題)のクラス数を含むリスト。</target>
        </trans-unit>
        <trans-unit id="8b5fc0d622c5abb76da13c6a8f50c84a211eca46" translate="yes" xml:space="preserve">
          <source>The number of classes in the training data</source>
          <target state="translated">学習データのクラス数</target>
        </trans-unit>
        <trans-unit id="a096e01744ef01b9d139303dfd4a94a83569986b" translate="yes" xml:space="preserve">
          <source>The number of classes of the classification problem.</source>
          <target state="translated">分類問題のクラス数。</target>
        </trans-unit>
        <trans-unit id="21583bce2023d80fa6dedc801ccace76e8a2445f" translate="yes" xml:space="preserve">
          <source>The number of classes to return.</source>
          <target state="translated">返すクラスの数。</target>
        </trans-unit>
        <trans-unit id="7defdc95bb1ddc0580ac0be76fc251278b72fb84" translate="yes" xml:space="preserve">
          <source>The number of classes.</source>
          <target state="translated">クラスの数です。</target>
        </trans-unit>
        <trans-unit id="9298de4c7bbca93c3d80fc1d9af6567cb0b29b0a" translate="yes" xml:space="preserve">
          <source>The number of clusters found by the algorithm. If &lt;code&gt;distance_threshold=None&lt;/code&gt;, it will be equal to the given &lt;code&gt;n_clusters&lt;/code&gt;.</source>
          <target state="translated">アルゴリズムによって検出されたクラスターの数。 &lt;code&gt;distance_threshold=None&lt;/code&gt; の場合、指定された &lt;code&gt;n_clusters&lt;/code&gt; と等しくなります。</target>
        </trans-unit>
        <trans-unit id="062fe5ee9cf896a5c74f9ba057f89b8b6de5fa8a" translate="yes" xml:space="preserve">
          <source>The number of clusters per class.</source>
          <target state="translated">クラスごとのクラスター数。</target>
        </trans-unit>
        <trans-unit id="21f23a23e06d5c295fade98ab37ab55d16e621ca" translate="yes" xml:space="preserve">
          <source>The number of clusters to find.</source>
          <target state="translated">見つけるべきクラスターの数。</target>
        </trans-unit>
        <trans-unit id="703c8a00edcb110a0abc7bd2f2de73653cb3b8bc" translate="yes" xml:space="preserve">
          <source>The number of clusters to find. It must be &lt;code&gt;None&lt;/code&gt; if &lt;code&gt;distance_threshold&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">検索するクラスターの数。 &lt;code&gt;distance_threshold&lt;/code&gt; が &lt;code&gt;None&lt;/code&gt; でない場合は、 &lt;code&gt;None&lt;/code&gt; である必要があります。</target>
        </trans-unit>
        <trans-unit id="b4184f0399110f5f8e690a26513dd6f78511c0a5" translate="yes" xml:space="preserve">
          <source>The number of clusters to form as well as the number of centroids to generate.</source>
          <target state="translated">形成するクラスターの数だけでなく、生成するセントロイドの数を指定します。</target>
        </trans-unit>
        <trans-unit id="35d84f4a157603ef94bd9baafa0d524311104205" translate="yes" xml:space="preserve">
          <source>The number of columns in the grid plot (default: 3).</source>
          <target state="translated">グリッドプロットの列数(デフォルトは3)。</target>
        </trans-unit>
        <trans-unit id="1ab9cd0e34ad15344a0623bebcdec3c277a5d196" translate="yes" xml:space="preserve">
          <source>The number of components. It is same as the &lt;code&gt;n_components&lt;/code&gt; parameter if it was given. Otherwise, it will be same as the number of features.</source>
          <target state="translated">コンポーネントの数。指定された場合、 &lt;code&gt;n_components&lt;/code&gt; パラメーターと同じです。それ以外の場合は、機能の数と同じになります。</target>
        </trans-unit>
        <trans-unit id="25441c707266953707d0c752071de32d0051d235" translate="yes" xml:space="preserve">
          <source>The number of connected components in the graph.</source>
          <target state="translated">グラフ内の接続された構成要素の数。</target>
        </trans-unit>
        <trans-unit id="5d17146f816382e55c9752b437d1e0a90ca8e256" translate="yes" xml:space="preserve">
          <source>The number of cross-validation splits (folds/iterations).</source>
          <target state="translated">クロスバリデーションの分割数(ひだ/反復)。</target>
        </trans-unit>
        <trans-unit id="4e96a6f0802b675664ce18cac028e5815cdfcfc0" translate="yes" xml:space="preserve">
          <source>The number of data features.</source>
          <target state="translated">データの特徴の数。</target>
        </trans-unit>
        <trans-unit id="aa32f92ae0454e6e9ee52207fe93d7eb6600c995" translate="yes" xml:space="preserve">
          <source>The number of degrees of freedom of each components in the model.</source>
          <target state="translated">モデルの各構成要素の自由度数。</target>
        </trans-unit>
        <trans-unit id="b2ef77453518fcb650d3ae2f90fcfda15611a36f" translate="yes" xml:space="preserve">
          <source>The number of duplicated features, drawn randomly from the informative and the redundant features.</source>
          <target state="translated">情報特徴量と冗長特徴量からランダムに抽出された重複特徴量の数。</target>
        </trans-unit>
        <trans-unit id="81a86627bf928c8685cc6768d089212f83e7a3d6" translate="yes" xml:space="preserve">
          <source>The number of elements of the hyperparameter value. Defaults to 1, which corresponds to a scalar hyperparameter. n_elements &amp;gt; 1 corresponds to a hyperparameter which is vector-valued, such as, e.g., anisotropic length-scales.</source>
          <target state="translated">ハイパーパラメータ値の要素の数。デフォルトは1で、これはスカラーハイパーパラメーターに対応します。n_elements&amp;gt; 1は、異方性の長さスケールなど、ベクトル値のハイパーパラメータに対応します。</target>
        </trans-unit>
        <trans-unit id="f3756e321fd8c5762ae8ac31516f2ecb21110717" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 上の等間隔のポイントの数。</target>
        </trans-unit>
        <trans-unit id="1ea7fab9bca4ccb2b6b305c06e165e2e51c05101" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes of the plots, for each target feature.</source>
          <target state="translated">各ターゲット特徴について、プロットの軸上に等間隔に配置された点の数。</target>
        </trans-unit>
        <trans-unit id="b29bfd2ea8e3e1682dd4a79d1706f446dfb38a05" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes.</source>
          <target state="translated">軸上に等間隔に配置された点の数。</target>
        </trans-unit>
        <trans-unit id="6843d07b6a565b2c9c3cc44410aeae6ef1353131" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the grid, for each target feature.</source>
          <target state="translated">グリッド上に等間隔に配置されたポイントの数を、各ターゲットフィーチャごとに指定します。</target>
        </trans-unit>
        <trans-unit id="a2df871c8f4e92cb59ed15324ccda38fad42ea75" translate="yes" xml:space="preserve">
          <source>The number of estimators as selected by early stopping (if &lt;code&gt;n_iter_no_change&lt;/code&gt; is specified). Otherwise it is set to &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">早期停止によって選択された推定子の数（ &lt;code&gt;n_iter_no_change&lt;/code&gt; が指定されている場合）。それ以外の場合は &lt;code&gt;n_estimators&lt;/code&gt; に設定されます。</target>
        </trans-unit>
        <trans-unit id="1c41aa32f1f0f6cf009b98065236eda5fafde67a" translate="yes" xml:space="preserve">
          <source>The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.</source>
          <target state="translated">出力行列の特徴量(列)の数。特徴の数が少ないとハッシュ衝突が起こりやすくなりますが、数が多いと線形学習者の係数次元が大きくなります。</target>
        </trans-unit>
        <trans-unit id="586a175a91db906a71d554d2394ac768c54a011d" translate="yes" xml:space="preserve">
          <source>The number of features for each sample.</source>
          <target state="translated">各サンプルの特徴量の数。</target>
        </trans-unit>
        <trans-unit id="7aa2f8ce84af9869f7a766d49383cc26f37d08c4" translate="yes" xml:space="preserve">
          <source>The number of features has to be &amp;gt;= 5.</source>
          <target state="translated">機能の数は5以上でなければなりません。</target>
        </trans-unit>
        <trans-unit id="129d21ac97bd672e4633231039dccb80b794a16c" translate="yes" xml:space="preserve">
          <source>The number of features to consider when looking for the best split:</source>
          <target state="translated">最適なスプリットを探す際に考慮すべき機能の数です。</target>
        </trans-unit>
        <trans-unit id="c3e18b2792aee2f260136dbfb16e08ac8ac5d9d0" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator ( without replacement by default, see &lt;code&gt;bootstrap_features&lt;/code&gt; for more details).</source>
          <target state="translated">各基本推定量をトレーニングするためにXから描画する特徴の数（デフォルトでは置換なし。詳細については、 &lt;code&gt;bootstrap_features&lt;/code&gt; を参照してください）。</target>
        </trans-unit>
        <trans-unit id="7bfa1c66757d06ac8d59fc4bf744ebce5057ba13" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator.</source>
          <target state="translated">各基底推定器を訓練するためにXから描画する特徴量の数。</target>
        </trans-unit>
        <trans-unit id="d51bb9401f1843316f34a353f5592cb098582ce3" translate="yes" xml:space="preserve">
          <source>The number of features to select. If &lt;code&gt;None&lt;/code&gt;, half of the features are selected.</source>
          <target state="translated">選択する機能の数。 &lt;code&gt;None&lt;/code&gt; の場合、機能の半分が選択されます。</target>
        </trans-unit>
        <trans-unit id="9a2e8b8c9f83e59315eadeb30286733009f73579" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred from the maximum column index occurring in any of the files.</source>
          <target state="translated">使用する機能の数。Noneの場合は、いずれかのファイルに存在する最大カラムインデックスから推測されます。</target>
        </trans-unit>
        <trans-unit id="98598b839c3ae52a8bef761a8c292e4971f87fa1" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. n_features is only required if &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed a non-default value.</source>
          <target state="translated">使用する機能の数。Noneの場合、推測されます。この引数は、より大きなスライスデータセットのサブセットである複数のファイルをロードするのに役立ちます。各サブセットにはすべての機能の例がない場合があるため、推定される形状はスライスごとに異なる場合があります。n_featuresは、 &lt;code&gt;offset&lt;/code&gt; または &lt;code&gt;length&lt;/code&gt; デフォルト以外の値が渡された場合にのみ必要です。</target>
        </trans-unit>
        <trans-unit id="3f0c441872fc889415f948d3a194dda00a747f75" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is performed.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;が実行されたときのフィーチャの数。</target>
        </trans-unit>
        <trans-unit id="65e177587b4163893d6713a7981471e039d2ab1a" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is performed.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;が実行されたときのフィーチャの数。</target>
        </trans-unit>
        <trans-unit id="deca4fdfd1ca37b7e04bbcf3985c1b98fb8a3a95" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; が実行されたときの特徴の数。</target>
        </trans-unit>
        <trans-unit id="000ec70bd77b0bf4abd3e711fc085eb14c1166a9" translate="yes" xml:space="preserve">
          <source>The number of features.</source>
          <target state="translated">特徴の数です。</target>
        </trans-unit>
        <trans-unit id="9910eba7c229f9255e2c9649c38205ffc855802c" translate="yes" xml:space="preserve">
          <source>The number of features. Should be at least 5.</source>
          <target state="translated">機能の数。5以上である必要があります。</target>
        </trans-unit>
        <trans-unit id="6b858bf37b4f474c4599d32f8587786621c82cda" translate="yes" xml:space="preserve">
          <source>The number of informative features, i.e., the number of features used to build the linear model used to generate the output.</source>
          <target state="translated">情報的特徴量の数、すなわち、出力を生成するために使用される線形モデルを構築するために使用される特徴量の数。</target>
        </trans-unit>
        <trans-unit id="1a77292033054c608772a804dab5a2e1fdf27c5c" translate="yes" xml:space="preserve">
          <source>The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension &lt;code&gt;n_informative&lt;/code&gt;. For each cluster, informative features are drawn independently from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube.</source>
          <target state="translated">有益な機能の数。各クラスは、次元 &lt;code&gt;n_informative&lt;/code&gt; の部分空間内の超立方体の頂点の周りにそれぞれ配置されたいくつかのガウスクラスターで構成されています。各クラスターについて、有益な特徴がN（0、1）から独立して描画され、共分散を追加するために各クラスター内でランダムに線形結合されます。次に、クラスターをハイパーキューブの頂点に配置します。</target>
        </trans-unit>
        <trans-unit id="8860e037fbe039fe78d5e3c48f8e1848feb12312" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The best results are kept.</source>
          <target state="translated">実行する初期化の数です。最良の結果を保持します。</target>
        </trans-unit>
        <trans-unit id="51a26d62cae82295bdfa00f11021a221252f4d93" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The result with the highest lower bound value on the likelihood is kept.</source>
          <target state="translated">実行する初期化の数。尤度の下限値が最も高い結果が保持されます。</target>
        </trans-unit>
        <trans-unit id="38ff309a985e30ab2e29b9afc192f79c8b9a0c6a" translate="yes" xml:space="preserve">
          <source>The number of integer to sample.</source>
          <target state="translated">サンプルする整数の数。</target>
        </trans-unit>
        <trans-unit id="a52a9529854ceea9a883fc2df829925e52c70f47" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before this call to partial_fit. This is optional: if no number is passed, the memory of the object is used.</source>
          <target state="translated">このpartial_fitの呼び出しの前に実行されたデータバッチの反復回数。これはオプションです:数値が渡されなかった場合は、オブジェクトのメモリが使用されます。</target>
        </trans-unit>
        <trans-unit id="99c30bd94e39fd4b9ed3f7e2a7d5b85b28ed521a" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before.</source>
          <target state="translated">以前に実行したデータバッチの反復回数。</target>
        </trans-unit>
        <trans-unit id="a86dc5cbf697553ae74ea1f2c0f3574967cc15f4" translate="yes" xml:space="preserve">
          <source>The number of iterations as selected by early stopping, depending on the &lt;code&gt;early_stopping&lt;/code&gt; parameter. Otherwise it corresponds to max_iter.</source>
          <target state="translated">&lt;code&gt;early_stopping&lt;/code&gt; パラメーターに応じて、早期停止によって選択された反復回数。それ以外の場合はmax_iterに対応します。</target>
        </trans-unit>
        <trans-unit id="a3bc7b28196b3922c89415e845096f6cf78b8faf" translate="yes" xml:space="preserve">
          <source>The number of iterations corresponding to the best stress. Returned only if &lt;code&gt;return_n_iter&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">最高のストレスに対応する反復回数。 &lt;code&gt;return_n_iter&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="09e334ba47a4a0e3959de08638739eb9eaaab635" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by lars_path to find the grid of alphas for each target.</source>
          <target state="translated">各ターゲットのアルファのグリッドを見つけるために lars_path によって取られた反復回数.</target>
        </trans-unit>
        <trans-unit id="b1098a1922ae37c291de7345b0094c64c3a02834" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha.</source>
          <target state="translated">座標降下オプティマイザが、各アルファについて指定された許容範囲に到達するまでに要した反復回数。</target>
        </trans-unit>
        <trans-unit id="cb53eef7959113120386cfa7066166d0b269478f" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when &lt;code&gt;return_n_iter&lt;/code&gt; is set to True).</source>
          <target state="translated">座標降下オプティマイザが各アルファの指定された許容値に到達するために行う反復の数。（ &lt;code&gt;return_n_iter&lt;/code&gt; がTrueに設定されている場合に返されます）。</target>
        </trans-unit>
        <trans-unit id="33de6e55bb60c2bc5ac457a31b105deefc3f996b" translate="yes" xml:space="preserve">
          <source>The number of iterations the solver has ran.</source>
          <target state="translated">ソルバーが実行した反復回数。</target>
        </trans-unit>
        <trans-unit id="28e3bc9f7f876eba064c747240fb8d2ff652b8df" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel all &lt;code&gt;estimators&lt;/code&gt;&lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;code&gt;joblib.parallel_backend&lt;/code&gt; context. -1 means using all processors. See Glossary for more details.</source>
          <target state="translated">すべての &lt;code&gt;estimators&lt;/code&gt; を並行して実行するジョブの数は &lt;code&gt;fit&lt;/code&gt; ます。 &lt;code&gt;joblib.parallel_backend&lt;/code&gt; コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。-1は、すべてのプロセッサを使用することを意味します。詳細については、用語集を参照してください。</target>
        </trans-unit>
        <trans-unit id="2412be87a40770d353886ae29c8c16a95b728197" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;ために並行して実行するジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="bbb843cd2b1d5904ac077f6964fae31def7713e0" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt; of all &lt;code&gt;estimators&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;code&gt;joblib.parallel_backend&lt;/code&gt; context. -1 means using all processors. See Glossary for more details.</source>
          <target state="translated">すべての &lt;code&gt;estimators&lt;/code&gt; を &lt;code&gt;fit&lt;/code&gt; ために並行して実行するジョブの数。 &lt;code&gt;joblib.parallel_backend&lt;/code&gt; コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。-1は、すべてのプロセッサを使用することを意味します。詳細については、用語集を参照してください。</target>
        </trans-unit>
        <trans-unit id="db80deec4964c14c90bbb2ba0d38dab8d92c99f4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; のために並行して実行するジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="f140b3227d03089cb6e6210a5db41bedf6b9aa63" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; ために並行して実行するジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="e9c5c499bb768493660dc4f85003a1f155b88c8c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.predict&quot;&gt; &lt;code&gt;predict&lt;/code&gt; の&lt;/a&gt;両方で並行して実行するジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="7e98b60a4f63df4b45688240803363e4326e4c3c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.predict&quot;&gt; &lt;code&gt;predict&lt;/code&gt; の&lt;/a&gt;両方で並行して実行するジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="631e50edfdf734f4505b382652be3db79f846fe9" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;a href=&quot;#sklearn.ensemble.IsolationForest.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.IsolationForest.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.ensemble.IsolationForest.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;#sklearn.ensemble.IsolationForest.predict&quot;&gt; &lt;code&gt;predict&lt;/code&gt; の&lt;/a&gt;両方で並行して実行するジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="828e81cc3e32985aa18f25c0aa8cb224a18c0ff7" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; と &lt;code&gt;predict&lt;/code&gt; 両方で並行して実行するジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="c43ba6eb14ab669b0479493b6caf6c135c124b5c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None`&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; と &lt;code&gt;predict&lt;/code&gt; 両方で並行して実行するジョブの数。 &lt;code&gt;None`&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="b1734bf8c02c7376c103570d8af07531d90ec5dd" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">並行して実行するジョブの数。&lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.predict&quot;&gt; &lt;code&gt;predict&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.decision_path&quot;&gt; &lt;code&gt;decision_path&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.apply&quot;&gt; &lt;code&gt;apply&lt;/code&gt; &lt;/a&gt;はすべてツリー上で並列化されます。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="30ff6a93f265905272a4a92b5eefd941a956fb0c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">並行して実行するジョブの数。&lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.predict&quot;&gt; &lt;code&gt;predict&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.decision_path&quot;&gt; &lt;code&gt;decision_path&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.apply&quot;&gt; &lt;code&gt;apply&lt;/code&gt; &lt;/a&gt;はすべてツリー上で並列化されます。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="e8618a7401b1bd61324e6d152687e714d9dfac99" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">並行して実行するジョブの数。&lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.predict&quot;&gt; &lt;code&gt;predict&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.decision_path&quot;&gt; &lt;code&gt;decision_path&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.apply&quot;&gt; &lt;code&gt;apply&lt;/code&gt; &lt;/a&gt;はすべてツリー上で並列化されます。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="c242a688459caf97557bbb76035f6cc722015df4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">並行して実行するジョブの数。&lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.predict&quot;&gt; &lt;code&gt;predict&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.decision_path&quot;&gt; &lt;code&gt;decision_path&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.apply&quot;&gt; &lt;code&gt;apply&lt;/code&gt; &lt;/a&gt;はすべてツリー上で並列化されます。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="91298d7ca6e21328dab1603b8e897f672cc46e7d" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">並行して実行するジョブの数。&lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.transform&quot;&gt; &lt;code&gt;transform&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.decision_path&quot;&gt; &lt;code&gt;decision_path&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.apply&quot;&gt; &lt;code&gt;apply&lt;/code&gt; &lt;/a&gt;はすべてツリー上で並列化されます。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="1c706846846b90bc0cd14952c05d5f24ee8d014f" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="2f8f980d4703b1ec3df64fe85f466c6c6170bcab" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="6833984bce80d83c32632eedfcb38fdab54d1b8b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. If multiple initializations are used (&lt;code&gt;n_init&lt;/code&gt;), each run of the algorithm is computed in parallel.</source>
          <target state="translated">計算に使用するジョブの数。複数の初期化が使用されている場合（ &lt;code&gt;n_init&lt;/code&gt; ）、アルゴリズムの各実行は並列で計算されます。</target>
        </trans-unit>
        <trans-unit id="6691b8acedaea4810fafdb230140a5bee73c0f13" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するジョブの数。yの各ターゲット変数を並行して実行します。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="be06d9a31cbe7a4bf21b6f4456cf1df2e9b8ee45" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するジョブの数。yの各ターゲット変数を並行して実行します。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="e3d27815195706836eb73a8c598c8129b01e46ca" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するジョブの数。これは、n_targets&amp;gt; 1および十分に大きな問題に対してのみ高速化を提供します。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="ba376dc0fa282138bebfa832fa05fd19bdc385f3" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">計算に使用するジョブの数。これは、n_targets&amp;gt; 1および十分に大きな問題のスピードアップのみを提供します。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="aea0fe9213de8cdb23ff0f2fe182f38ade8bf1a8" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.</source>
          <target state="translated">計算に使用するジョブの数。これは、ペアワイズ行列をn_jobsの偶数スライスに分解し、並列に計算することで動作します。</target>
        </trans-unit>
        <trans-unit id="355b7ccfb662137f73492d9f4ce45fbb16afbbbc" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.</source>
          <target state="translated">計算に使用するジョブの数を指定します。これは、n_initの各実行を並列に計算することで動作します。</target>
        </trans-unit>
        <trans-unit id="a5a46de70d97dcbce1cd57e62c5d0b48ff30934b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">Eステップで使用するジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="b51be92d99796ca039296711c164e0b4cf278e0a" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">Eステップで使用するジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="27ef8dc8b90e4adb5cf60c4b5861107cfc3fcd66" translate="yes" xml:space="preserve">
          <source>The number of leaves in the tree</source>
          <target state="translated">木の葉の数</target>
        </trans-unit>
        <trans-unit id="aa280d8fa86c4668dcae50e829dc6fe4b82c5c09" translate="yes" xml:space="preserve">
          <source>The number of longitudes (x) and latitudes (y) in the grid</source>
          <target state="translated">グリッド内の長さ(x)と緯度(y)の数</target>
        </trans-unit>
        <trans-unit id="c1079f8f5554d74d12065ae30d0d0d5ad6da869f" translate="yes" xml:space="preserve">
          <source>The number of mixture components.</source>
          <target state="translated">混合成分の数です。</target>
        </trans-unit>
        <trans-unit id="1b5111fc1060a675cec3a1f3743c5b7235e4d74d" translate="yes" xml:space="preserve">
          <source>The number of mixture components. Depending on the data and the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; the model can decide to not use all the components by setting some component &lt;code&gt;weights_&lt;/code&gt; to values very close to zero. The number of effective components is therefore smaller than n_components.</source>
          <target state="translated">混合コンポーネントの数。データの値に応じて、 &lt;code&gt;weight_concentration_prior&lt;/code&gt; モデルは、いくつかのコンポーネント設定することで、すべてのコンポーネントを使用しないことを決定することができ &lt;code&gt;weights_&lt;/code&gt; を非常にゼロに近い値に。したがって、有効なコンポーネントの数はn_componentsよりも少なくなります。</target>
        </trans-unit>
        <trans-unit id="add0998b97eff8ce13b181824a749694c3eae657" translate="yes" xml:space="preserve">
          <source>The number of nearest neighbors to return</source>
          <target state="translated">返すべき最も近い隣人の数</target>
        </trans-unit>
        <trans-unit id="15fc684195ef8537dde92583b88b14e2ce9227a5" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">考慮される隣人の数(パラメータ n_neighbors)は、一般的に、1)他のサンプルがこのクラスターに対して局所的な外れ値となり得るように、クラスターが含まなければならないサンプルの最小数よりも大きく、2)局所的な外れ値となり得る近くのサンプルの最大数よりも小さく設定されています。実際には、このような情報は一般的には得られず、n_neighbors=20とするのが一般的にはうまくいくようです。</target>
        </trans-unit>
        <trans-unit id="acccc5b01db683b034e704a8a9a779e161564526" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered, (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">考慮される隣人の数(パラメータ n_neighbors)は、一般的に、1)クラスターに含まれるサンプルの最小数よりも大きく設定され、他のサンプルがこのクラスターに対して局所的な外れ値になる可能性があるため、2)局所的な外れ値になる可能性がある近くのサンプルの最大数よりも小さく設定されます。実際には、このような情報は一般的には得られず、n_neighbors=20とするのが一般的にはうまくいくようです。</target>
        </trans-unit>
        <trans-unit id="877763fdbf37d108bc07acba2c3c7a43a6b1f5d0" translate="yes" xml:space="preserve">
          <source>The number of occurrences of each label in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;y_true&lt;/code&gt; 内の各ラベルの出現回数。</target>
        </trans-unit>
        <trans-unit id="d77e127c483a951faddd4898060a91624b32c7e2" translate="yes" xml:space="preserve">
          <source>The number of outlying points matters, but also how much they are outliers.</source>
          <target state="translated">外れ点の数も重要ですが、どれだけ外れているかも重要です。</target>
        </trans-unit>
        <trans-unit id="28c74611a0fbfe9d7c9bc15166aba9285108f840" translate="yes" xml:space="preserve">
          <source>The number of outputs when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; が実行されたときの出力の数。</target>
        </trans-unit>
        <trans-unit id="bb79176b795c17c39b28af054f09df09e008175b" translate="yes" xml:space="preserve">
          <source>The number of outputs.</source>
          <target state="translated">出力数です。</target>
        </trans-unit>
        <trans-unit id="b09418506b739dbda708239c423400be69d20b7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search.</source>
          <target state="translated">近隣探索のために実行する並列ジョブの数。</target>
        </trans-unit>
        <trans-unit id="f8c9b6b47de0b026f28768b5a0afa5c012cbf5fc" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">ネイバー検索で実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="7704c4671581e69d735cac67f4c8350e13fe7ef2" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Affects only &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; methods.</source>
          <target state="translated">ネイバー検索で実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。&lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;kneighbors_graph&lt;/code&gt; &lt;/a&gt;メソッドのみに影響します。</target>
        </trans-unit>
        <trans-unit id="361059c7cf2c1088b100ce86f03251d9b9b3606a" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">ネイバー検索で実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。&lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;方法には影響しません。</target>
        </trans-unit>
        <trans-unit id="0d88911bb44ab8ec6ef4a44ece33bbbd3a1ce8ce" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">ネイバー検索で実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。&lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;方法には影響しません。</target>
        </trans-unit>
        <trans-unit id="1e03b6a0d6693562cf04d81d449658ec683196ea" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">ネイバー検索のために実行する並列ジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="ab2a0224fab84a593d45a145e0638371b6911fd8" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">ネイバー検索のために実行する並列ジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。&lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;方法には影響しません。</target>
        </trans-unit>
        <trans-unit id="ecd1b3d7da18bb6dc421488bb596e855ecb80e43" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">ネイバー検索のために実行する並列ジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。&lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;方法には影響しません。</target>
        </trans-unit>
        <trans-unit id="5f8cf8af9be5d62ff8935fbef652326c9adba1d1" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. If &lt;code&gt;-1&lt;/code&gt;, then the number of jobs is set to the number of CPU cores.</source>
          <target state="translated">ネイバー検索のために実行する並列ジョブの数。 &lt;code&gt;-1&lt;/code&gt; の場合、ジョブの数はCPUコアの数に設定されます。</target>
        </trans-unit>
        <trans-unit id="e352fe55bf07363046d5192af0f663c879039cfb" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. This parameter has no impact when &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; or (&lt;code&gt;metric=&quot;euclidean&quot;&lt;/code&gt; and &lt;code&gt;method=&quot;exact&quot;&lt;/code&gt;). &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">ネイバー検索のために実行する並列ジョブの数。このパラメーターは、 &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; または（ &lt;code&gt;metric=&quot;euclidean&quot;&lt;/code&gt; および &lt;code&gt;method=&quot;exact&quot;&lt;/code&gt; ）の場合は影響しません。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="92152098131975c56771bce4e909262b46d5eb7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">実行する並列ジョブの数。 &lt;code&gt;None&lt;/code&gt; は、&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="d31f80403a0bc612aeee728ef96a0071578ad09f" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">実行する並列ジョブの数。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;コンテキストでない限り、 &lt;code&gt;None&lt;/code&gt; は1を意味します。 &lt;code&gt;-1&lt;/code&gt; は、すべてのプロセッサを使用することを意味します。詳細については、&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;用語集&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="99143dd67a79337f4ad9e3dbea2cd1e515a938df" translate="yes" xml:space="preserve">
          <source>The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21.</source>
          <target state="translated">学習データを通過する回数(エポック)。デフォルトは None です。非推奨。0.21で削除される予定です。</target>
        </trans-unit>
        <trans-unit id="ec5cdfb884714451da411c4ff8cb1db87b4e7636" translate="yes" xml:space="preserve">
          <source>The number of redundant features. These features are generated as random linear combinations of the informative features.</source>
          <target state="translated">冗長特徴量の数である。これらの特徴は、情報的特徴のランダムな線形の組み合わせとして生成される。</target>
        </trans-unit>
        <trans-unit id="47b1c5caf88dbd07fabbf6968de5281c23c7d24f" translate="yes" xml:space="preserve">
          <source>The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar.</source>
          <target state="translated">回帰ターゲットの数、すなわち、サンプルに関連するy出力ベクトルの次元。デフォルトでは,出力はスカラです.</target>
        </trans-unit>
        <trans-unit id="338ec305a58ac58159822d262713a3e274e16037" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed.</source>
          <target state="translated">対数限界尤度を最大化するカーネルのパラメーターを見つけるためのオプティマイザーの再起動回数。オプティマイザの最初の実行は、カーネルの初期パラメータから実行され、残りのパラメータは（もしあれば）許可されたtheta値のスペースからランダムに抽出されたthetasから対数均一にサンプリングされます。0より大きい場合、すべての境界は有限でなければなりません。n_restarts_optimizer == 0は、1回の実行が実行されることを意味することに注意してください。</target>
        </trans-unit>
        <trans-unit id="a4b3a5a4d2704e96fc904a7b55b3f1c5b2c8d507" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed.</source>
          <target state="translated">対数限界尤度を最大化するカーネルのパラメーターを見つけるためのオプティマイザーの再起動回数。オプティマイザの最初の実行は、カーネルの初期パラメータから実行され、残りのパラメータ（ある場合）は、許可されたtheta値のスペースからランダムに抽出されたthetasから対数均一にサンプリングされます。0より大きい場合、すべての境界は有限でなければなりません。n_restarts_optimizer = 0は、1回の実行が実行されることを意味することに注意してください。</target>
        </trans-unit>
        <trans-unit id="8f64e7c256c29c0afec3f69dafbe77018e516c64" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters in the checkerboard structure.</source>
          <target state="translated">チェッカーボード構造の行と列のクラスタ数。</target>
        </trans-unit>
        <trans-unit id="1d47b9ac186fe69411b80e5cb5c0bc35de2b1552" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters.</source>
          <target state="translated">行と列のクラスタ数。</target>
        </trans-unit>
        <trans-unit id="a93f4e954bb39f85a0cd6723eb5913eb20116e8e" translate="yes" xml:space="preserve">
          <source>The number of sample points on the S curve.</source>
          <target state="translated">S曲線上のサンプル点の数。</target>
        </trans-unit>
        <trans-unit id="e771006aa290a36177ef8a2fd15f85ed45a9f7ce" translate="yes" xml:space="preserve">
          <source>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.</source>
          <target state="translated">中心点とみなされる点の近傍のサンプル数(または総重量)。これには点自体も含まれる。</target>
        </trans-unit>
        <trans-unit id="d8023265c3023688c589a292c9dfac5ffedb5a44" translate="yes" xml:space="preserve">
          <source>The number of samples drawn from the Gaussian process</source>
          <target state="translated">ガウス過程から引き出されるサンプル数</target>
        </trans-unit>
        <trans-unit id="c514b9b07551e0f76131a511ab26abfabcd4aa8a" translate="yes" xml:space="preserve">
          <source>The number of samples in a neighborhood for a point to be considered as a core point. Also, up and down steep regions can&amp;rsquo;t have more then &lt;code&gt;min_samples&lt;/code&gt; consecutive non-steep points. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).</source>
          <target state="translated">コアポイントと見なされるポイントの近隣のサンプル数。また、急勾配の領域の上下には、 &lt;code&gt;min_samples&lt;/code&gt; を超える連続した非急勾配のポイントを含めることはできません。絶対数またはサンプル数の端数として表されます（少なくとも2に丸められます）。</target>
        </trans-unit>
        <trans-unit id="b9e5e159d14938dc3482cf1b66194258f9217ba8" translate="yes" xml:space="preserve">
          <source>The number of samples in a neighborhood for a point to be considered as a core point. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).</source>
          <target state="translated">中心点とみなされる点の近傍のサンプル数。絶対数またはサンプル数の端数として表現される(少なくとも2になるように四捨五入される)。</target>
        </trans-unit>
        <trans-unit id="1589955effed900fd5766b276491de7c2d2b9bf4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator for each feature. If there are not missing samples, the &lt;code&gt;n_samples_seen&lt;/code&gt; will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">各機能の推定器によって処理されたサンプルの数。欠落しているサンプルがない場合、 &lt;code&gt;n_samples_seen&lt;/code&gt; は整数になり、それ以外の場合は配列になります。新しい呼び出しでリセットされてフィットしますが、 &lt;code&gt;partial_fit&lt;/code&gt; 呼び出しで増分されます。</target>
        </trans-unit>
        <trans-unit id="8b8db8f325de293d35a9c6b4d95ec39fb2cca6ee" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. It will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">推定器によって処理されたサンプルの数。これは、fitへの新しい呼び出しでリセットされますが、 &lt;code&gt;partial_fit&lt;/code&gt; 呼び出し全体で増加します。</target>
        </trans-unit>
        <trans-unit id="5282c1e1c469ae21abb6fc766981198bf00b68c4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">推定器によって処理されたサンプルの数。新しい呼び出しでリセットされてフィットしますが、 &lt;code&gt;partial_fit&lt;/code&gt; 呼び出しで増分されます。</target>
        </trans-unit>
        <trans-unit id="baac00734c9f0ef53943bb384323fdc39b43973f" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator (with replacement by default, see &lt;code&gt;bootstrap&lt;/code&gt; for more details).</source>
          <target state="translated">各基本推定量をトレーニングするためにXから抽出するサンプルの数（デフォルトでは置換あり。詳細については、 &lt;code&gt;bootstrap&lt;/code&gt; を参照してください）。</target>
        </trans-unit>
        <trans-unit id="1793fd9997ff1f0552981d710a775a55fe6d48a0" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator.</source>
          <target state="translated">各基底推定器を訓練するために X から描画するサンプル数。</target>
        </trans-unit>
        <trans-unit id="bb3a932b7568c921848c0d1cfe6540980845aa8f" translate="yes" xml:space="preserve">
          <source>The number of samples to take in each batch.</source>
          <target state="translated">各バッチで採取するサンプル数。</target>
        </trans-unit>
        <trans-unit id="45c4c8ec08a2e1d782bf77a47114ec6ebd4dca5e" translate="yes" xml:space="preserve">
          <source>The number of samples to use for each batch. Only used when calling &lt;code&gt;fit&lt;/code&gt;. If &lt;code&gt;batch_size&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, then &lt;code&gt;batch_size&lt;/code&gt; is inferred from the data and set to &lt;code&gt;5 * n_features&lt;/code&gt;, to provide a balance between approximation accuracy and memory consumption.</source>
          <target state="translated">各バッチに使用するサンプルの数。 &lt;code&gt;fit&lt;/code&gt; を呼び出すときにのみ使用されます。場合 &lt;code&gt;batch_size&lt;/code&gt; しない &lt;code&gt;None&lt;/code&gt; 、次いで &lt;code&gt;batch_size&lt;/code&gt; にデータセットから推定される &lt;code&gt;5 * n_features&lt;/code&gt; 近似精度とメモリ消費のバランスを提供します。</target>
        </trans-unit>
        <trans-unit id="cc9506bec678834c1e3b7d03354e1ff3a3f08ddc" translate="yes" xml:space="preserve">
          <source>The number of samples to use. If not given, all samples are used.</source>
          <target state="translated">使用するサンプル数。指定しない場合は、すべてのサンプルが使用されます。</target>
        </trans-unit>
        <trans-unit id="06d71b7513bf6cfbd6babc125146f20e54cecd08" translate="yes" xml:space="preserve">
          <source>The number of samples.</source>
          <target state="translated">サンプル数です。</target>
        </trans-unit>
        <trans-unit id="8ed3bb3b4a6145be5f5af9755587163fa8bebbaa" translate="yes" xml:space="preserve">
          <source>The number of seconds contained in delta</source>
          <target state="translated">デルタに含まれる秒数</target>
        </trans-unit>
        <trans-unit id="fd2b02981da97aea3b937c6f113a2aa5413af4a0" translate="yes" xml:space="preserve">
          <source>The number of selected features with cross-validation.</source>
          <target state="translated">クロスバリデーションで選択された特徴量の数。</target>
        </trans-unit>
        <trans-unit id="f83194da71427b41aa36e9d801ef17814b93c795" translate="yes" xml:space="preserve">
          <source>The number of selected features.</source>
          <target state="translated">選択された機能の数。</target>
        </trans-unit>
        <trans-unit id="ecf3b2d99c2ce29bc7f1b5f51d42517d75e68e85" translate="yes" xml:space="preserve">
          <source>The number of stages of the final model is available at the attribute &lt;code&gt;n_estimators_&lt;/code&gt;.</source>
          <target state="translated">最終モデルのステージ数は、属性 &lt;code&gt;n_estimators_&lt;/code&gt; で取得できます。</target>
        </trans-unit>
        <trans-unit id="d8f394636e1a93d63e3434a6391ec5ef2f73741a" translate="yes" xml:space="preserve">
          <source>The number of threads used by the OpenBLAS, MKL or BLIS libraries can be set via the &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt;, &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt;, and &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt; environment variables.</source>
          <target state="translated">OpenBLAS、MKL、またはBLISライブラリーが使用するスレッドの数は、 &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt; 、 &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt; 、および &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt; 環境変数を介して設定できます。</target>
        </trans-unit>
        <trans-unit id="40db2965d8f58e28ab3da3567d512db3201d5fa4" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed.</source>
          <target state="translated">グリッドがリファインされる回数を指定します。アルファの明示的な値が渡された場合は使用されません。</target>
        </trans-unit>
        <trans-unit id="642e04b02615d230b9669f1186145c403a47fbce" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed. Range is [1, inf).</source>
          <target state="translated">グリッドがリファインされる回数を指定します。アルファの明示的な値が渡された場合は使用されません。範囲は [1,inf)です。</target>
        </trans-unit>
        <trans-unit id="089ef760cb35bf7968ad0395345a0886662c0be1" translate="yes" xml:space="preserve">
          <source>The number of tree that are built at each iteration. For regressors, this is always 1.</source>
          <target state="translated">各反復で構築されるツリーの数。レグレッサーの場合、これは常に1です。</target>
        </trans-unit>
        <trans-unit id="20cb5259cf581cee4993b651401d828750438d9c" translate="yes" xml:space="preserve">
          <source>The number of tree that are built at each iteration. This is equal to 1 for binary classification, and to &lt;code&gt;n_classes&lt;/code&gt; for multiclass classification.</source>
          <target state="translated">各反復で構築されるツリーの数。これは、バイナリ分類の場合は1に等しく、マルチクラス分類の場合は &lt;code&gt;n_classes&lt;/code&gt; に等しくなります。</target>
        </trans-unit>
        <trans-unit id="9b5e4a652e0296cb387fe06bf82a965799e670b1" translate="yes" xml:space="preserve">
          <source>The number of trees in the forest.</source>
          <target state="translated">森の中にある木の数。</target>
        </trans-unit>
        <trans-unit id="b609a37dc2d7220a5b1e0ac4ad20a19f7617a90e" translate="yes" xml:space="preserve">
          <source>The number of weak learners (i.e. regression trees) is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;; &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;The size of each tree&lt;/a&gt; can be controlled either by setting the tree depth via &lt;code&gt;max_depth&lt;/code&gt; or by setting the number of leaf nodes via &lt;code&gt;max_leaf_nodes&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;shrinkage&lt;/a&gt; .</source>
          <target state="translated">弱学習器（つまり、回帰ツリー）の数は、パラメーター &lt;code&gt;n_estimators&lt;/code&gt; によって制御されます。&lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;各ツリーのサイズは、&lt;/a&gt;を介してツリーの深さを設定することによってのいずれかで制御することができる &lt;code&gt;max_depth&lt;/code&gt; 又は介しリーフノードの数を設定することにより &lt;code&gt;max_leaf_nodes&lt;/code&gt; 。 &lt;code&gt;learning_rate&lt;/code&gt; は、コントロールを介してオーバーフィットする範囲（0.0、1.0]でハイパーパラメータで&lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;収縮&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="68e68bbcb31faf3cd13be5ec90f20eecf860f6d7" translate="yes" xml:space="preserve">
          <source>The number of weak learners is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the &lt;code&gt;base_estimator&lt;/code&gt; parameter. The main parameters to tune to obtain good results are &lt;code&gt;n_estimators&lt;/code&gt; and the complexity of the base estimators (e.g., its depth &lt;code&gt;max_depth&lt;/code&gt; or minimum required number of samples to consider a split &lt;code&gt;min_samples_split&lt;/code&gt;).</source>
          <target state="translated">弱学習器の数は、パラメーター &lt;code&gt;n_estimators&lt;/code&gt; によって制御されます。 &lt;code&gt;learning_rate&lt;/code&gt; パラメータは、最終的な組み合わせにおける弱識別器の寄与を制御します。デフォルトでは、弱学習器は意思決定の切り株です。 &lt;code&gt;base_estimator&lt;/code&gt; パラメータを使用して、さまざまな弱学習器を指定できます。良い結果を得るために調整する主なパラメーターは、 &lt;code&gt;n_estimators&lt;/code&gt; と基本推定量の複雑さです（たとえば、その深さ &lt;code&gt;max_depth&lt;/code&gt; または分割 &lt;code&gt;min_samples_split&lt;/code&gt; を考慮するために必要なサンプルの最小数）。</target>
        </trans-unit>
        <trans-unit id="a6d1422bf72f5a61faa255a9a1207169e4a394c6" translate="yes" xml:space="preserve">
          <source>The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets.</source>
          <target state="translated">このオブジェクトは,LassoCV オブジェクトと同じ問題を解決します.ただし,LassoCV とは異なり,自分で関連するアルファ値を見つけます.一般的に,この性質のため,より安定しています.しかし,重度のマルチコリニアなデータセットに対しては,より脆弱です.</target>
        </trans-unit>
        <trans-unit id="909e016dde1f323cbfe3daf2401dd2bee2829e0c" translate="yes" xml:space="preserve">
          <source>The object to use to fit the data.</source>
          <target state="translated">データをはめ込むのに使うオブジェクト。</target>
        </trans-unit>
        <trans-unit id="d5c8a5f63b0e142ed66ba0f0aa5318c460719fdf" translate="yes" xml:space="preserve">
          <source>The object&amp;rsquo;s &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; attributes store the best mean score and the parameters setting corresponding to that score:</source>
          <target state="translated">オブジェクトの &lt;code&gt;best_score_&lt;/code&gt; および &lt;code&gt;best_params_&lt;/code&gt; 属性には、最良の平均スコアと、そのスコアに対応するパラメーター設定が格納されます。</target>
        </trans-unit>
        <trans-unit id="512dd720938772db73af76fb4221b6596459608c" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H.</source>
          <target state="translated">目的関数は、WとHを交互に最小化することで最小化されます。</target>
        </trans-unit>
        <trans-unit id="f16ae566357b7a1e9f3616f2cbcfd46a6904b9f5" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H. If H is given and update_H=False, it solves for W only.</source>
          <target state="translated">目的関数は、WとHを交互に最小化します。 Hが与えられ、update_H=Falseの場合は、Wのみを解いています。</target>
        </trans-unit>
        <trans-unit id="a139874cf1d9e2c13462d6567d8563d658f0907b" translate="yes" xml:space="preserve">
          <source>The objective function is:</source>
          <target state="translated">目的関数は</target>
        </trans-unit>
        <trans-unit id="bf56422fecab64934517c1fdfbcaed66ab27df08" translate="yes" xml:space="preserve">
          <source>The objective function to minimize is in this case</source>
          <target state="translated">最小化する目的関数は、この場合</target>
        </trans-unit>
        <trans-unit id="e546a5e9110cb4077ff5ab03d80b93cb6429070c" translate="yes" xml:space="preserve">
          <source>The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.</source>
          <target state="translated">オブザベーションが,低次元の潜在因子の線形変換と追加されたガウス・ノイズによって引き起こされると仮定される.一般性を損なうことなく、因子は、ゼロ平均と単位共分散を持つガウスに従って分布する。ノイズもまたゼロ平均で,任意の対角共分散行列を持つ.</target>
        </trans-unit>
        <trans-unit id="f8b5be5464139b25f15f97e4d9d483501d55af35" translate="yes" xml:space="preserve">
          <source>The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="translated">クラスタリングするオブザベーション。データはC順に変換され、与えられたデータがC連続でない場合はメモリコピーが発生することに注意しなければなりません。</target>
        </trans-unit>
        <trans-unit id="5af240b8e218ff237309779b56f83d5e52d1af7b" translate="yes" xml:space="preserve">
          <source>The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.</source>
          <target state="translated">オブザベーション、我々が計算するマハラノビス距離。観測は、フィットで使用されるデータと同じ分布から描かれていると仮定しています。</target>
        </trans-unit>
        <trans-unit id="eb71736c8c9acd5b1d75a4e7144f466f3b859a53" translate="yes" xml:space="preserve">
          <source>The obtained score is always strictly greater than 0 and the best value is 1.</source>
          <target state="translated">得られたスコアは常に厳密には0よりも大きく、最良値は1である。</target>
        </trans-unit>
        <trans-unit id="b7dea734e0307eec19c3b5341e6f81839af6082a" translate="yes" xml:space="preserve">
          <source>The one-vs-the-rest meta-classifier also implements a &lt;code&gt;predict_proba&lt;/code&gt; method, so long as such a method is implemented by the base classifier. This method returns probabilities of class membership in both the single label and multilabel case. Note that in the multilabel case, probabilities are the marginal probability that a given sample falls in the given class. As such, in the multilabel case the sum of these probabilities over all possible labels for a given sample &lt;em&gt;will not&lt;/em&gt; sum to unity, as they do in the single label case.</source>
          <target state="translated">one-vs-the-restメタ分類子は、ベース分類子によって実装される限り、 &lt;code&gt;predict_proba&lt;/code&gt; メソッドも実装します。このメソッドは、単一ラベルとマルチラベルの両方のケースでクラスメンバーシップの確率を返します。マルチラベルの場合、確率は、特定のサンプルが特定のクラスに分類される限界確率であることに注意してください。そのため、マルチラベルの場合には、与えられたサンプルについて、すべての可能なラベルの上にこれらの確率の合計は&lt;em&gt;ないだろう&lt;/em&gt;、彼らは単一ラベルの場合にはそうであるように、団結を合計します。</target>
        </trans-unit>
        <trans-unit id="f36ee9570dbac199195d25256879fa51a751bbaa" translate="yes" xml:space="preserve">
          <source>The opposite LOF of the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (&lt;code&gt;negative_outlier_factor_&lt;/code&gt; close to -1), while outliers tend to have a larger LOF score.</source>
          <target state="translated">トレーニングサンプルの反対のLOF。高いほど、正常です。外れ値のLOFスコアは大きくなる傾向がありますが、インライアのLOFスコアは1に近い傾向があります（ &lt;code&gt;negative_outlier_factor_&lt;/code&gt; は-1に近い）。</target>
        </trans-unit>
        <trans-unit id="df3e454f1090a47509e70dbea510891595829b28" translate="yes" xml:space="preserve">
          <source>The opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal.</source>
          <target state="translated">各入力サンプルの局所外れ要因の逆。低いほど異常。</target>
        </trans-unit>
        <trans-unit id="4312ae849a138f7db034f6fd7f5a1ea0b817b171" translate="yes" xml:space="preserve">
          <source>The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:</source>
          <target state="translated">与えられたデータセットに対する最適なアルゴリズムは複雑な選択であり、多くの要因に依存します。</target>
        </trans-unit>
        <trans-unit id="e994897b226c2495f8cea3f1e46f2c7029c61954" translate="yes" xml:space="preserve">
          <source>The optimal lambda parameter for minimizing skewness is estimated on each feature independently using maximum likelihood.</source>
          <target state="translated">歪度を最小化するための最適なラムダパラメータは、最尤法を用いて各特徴に独立して推定されます。</target>
        </trans-unit>
        <trans-unit id="033713ef73311880bab79fd88513749d67a56824" translate="yes" xml:space="preserve">
          <source>The optimization objective for Lasso is:</source>
          <target state="translated">Lassoの最適化目的は</target>
        </trans-unit>
        <trans-unit id="64214421bf615c105d2891f743437d06253a6ade" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskElasticNet is:</source>
          <target state="translated">MultiTaskElasticNetの最適化目標は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="f43132c3f7097ed8020d37840c051e421e41e300" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskLasso is:</source>
          <target state="translated">MultiTaskLassoの最適化の目的は</target>
        </trans-unit>
        <trans-unit id="3bee4ea6ed3cad366ecf4d67dfc05469de43ad46" translate="yes" xml:space="preserve">
          <source>The optimization objective for the case method=&amp;rsquo;lasso&amp;rsquo; is:</source>
          <target state="translated">case method = 'lasso'の最適化の目的は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="8d89fe15a9f2092d4f8a7ef569d775bf0279d26d" translate="yes" xml:space="preserve">
          <source>The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:</source>
          <target state="translated">オプションの extra 引数は、非推奨のメッセージと docstring に追加されます。注意:extra のデフォルト値で使用するには、括弧を空にしてください。</target>
        </trans-unit>
        <trans-unit id="0c86ba504c90ec1412c0ccc3d7f0b2f074294dc1" translate="yes" xml:space="preserve">
          <source>The optional parameter &lt;code&gt;whiten=True&lt;/code&gt; makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.</source>
          <target state="translated">オプションのパラメーター &lt;code&gt;whiten=True&lt;/code&gt; を使用すると、各コンポーネントを単位分散にスケーリングしながら、データを特異空間に投影できます。これは、下流のモデルが信号の等方性を強く想定している場合に役立ちます。これは、たとえば、RBFカーネルとK-Meansクラスタリングアルゴリズムを備えたサポートベクターマシンの場合です。</target>
        </trans-unit>
        <trans-unit id="c77f2b2e59e77aff75fd77a2825914acd9eb1242" translate="yes" xml:space="preserve">
          <source>The order in which the features will be imputed. Possible values:</source>
          <target state="translated">特徴を入力する順序.可能な値。</target>
        </trans-unit>
        <trans-unit id="846ba284e005e0c78a1fd443a812172705bbc8f3" translate="yes" xml:space="preserve">
          <source>The order of labels in the classifier chain.</source>
          <target state="translated">分類器チェーンのラベルの順序。</target>
        </trans-unit>
        <trans-unit id="7b082acdda498538bd71b7e32b0b230c2144c284" translate="yes" xml:space="preserve">
          <source>The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.:</source>
          <target state="translated">鎖の順序は、整数のリストを指定することで明示的に設定することができます。例えば、長さ5の鎖の場合...</target>
        </trans-unit>
        <trans-unit id="f9d11a930ecaf4c38c05b12592342da86a8c77ff" translate="yes" xml:space="preserve">
          <source>The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the &lt;code&gt;transformers&lt;/code&gt; list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the &lt;code&gt;passthrough&lt;/code&gt; keyword. Those columns specified with &lt;code&gt;passthrough&lt;/code&gt; are added at the right to the output of the transformers.</source>
          <target state="translated">変換された特徴マトリックスの列の順序は、 &lt;code&gt;transformers&lt;/code&gt; リストで列が指定された順序に従います。指定されていない元の特徴マトリックスの列は、 &lt;code&gt;passthrough&lt;/code&gt; キーワードで指定されていない限り、変換後の変換された特徴マトリックスから削除されます。 &lt;code&gt;passthrough&lt;/code&gt; 指定された列は、トランスフォーマーの出力の右側に追加されます。</target>
        </trans-unit>
        <trans-unit id="0b5c0a29228cf2f99af9ecdff2f5b2a0dc08ed2d" translate="yes" xml:space="preserve">
          <source>The original data</source>
          <target state="translated">オリジナルデータ</target>
        </trans-unit>
        <trans-unit id="77422b337aacebf704cab947c2765e7ef78426db" translate="yes" xml:space="preserve">
          <source>The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.</source>
          <target state="translated">元のデータセットは92×112枚で構成されていましたが、ここで利用できるバージョンは64×64枚の画像で構成されています。</target>
        </trans-unit>
        <trans-unit id="11407c8a67a7b9eefbd2b65794ca1ecbfa48a97d" translate="yes" xml:space="preserve">
          <source>The original formulation of the hashing trick by Weinberger et al. used two separate hash functions \(h\) and \(\xi\) to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits.</source>
          <target state="translated">Weinbergerらによるハッシュ・トリックのオリジナルの定式化では、特徴の列のインデックスと符号を決定するために、2つの別々のハッシュ関数を使用していました。現在の実装では、MurmurHash3の符号ビットが他のビットから独立しているという仮定の下で動作している。</target>
        </trans-unit>
        <trans-unit id="62507dafeac3a5da5ac8979b39262545fa83f997" translate="yes" xml:space="preserve">
          <source>The original image data. For color images, the last dimension specifies the channel: a RGB image would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="translated">元の画像データ。カラー画像の場合、最後の次元でチャネルを指定します。RGB画像の &lt;code&gt;n_channels=3&lt;/code&gt; になります。</target>
        </trans-unit>
        <trans-unit id="8eec938f1b6ddec4315f3fac08b8dc8e8e67d5d5" translate="yes" xml:space="preserve">
          <source>The original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 47.</source>
          <target state="translated">元の画像は250×250ピクセルですが、デフォルトのスライスとリサイズの引数で62×47に縮小されます。</target>
        </trans-unit>
        <trans-unit id="a555f40ab758aba55bcba7bb9ac36af67b065d6a" translate="yes" xml:space="preserve">
          <source>The other kernels</source>
          <target state="translated">他のカーネル</target>
        </trans-unit>
        <trans-unit id="5978e5bbc3d0aa56b4c3321d1d9ccbd8b149a1a9" translate="yes" xml:space="preserve">
          <source>The outer product of the row and column label vectors shows a representation of the checkerboard structure.</source>
          <target state="translated">行と列のラベルベクトルの外積は、市松模様の構造の表現を示しています。</target>
        </trans-unit>
        <trans-unit id="4e7d9f946f101134a65fd7d38a82dce953516bd7" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">出力 &lt;code&gt;y&lt;/code&gt; は次の式に従って作成されます。</target>
        </trans-unit>
        <trans-unit id="7348f35a4e7d1a3450461b231ee28ef95517ba57" translate="yes" xml:space="preserve">
          <source>The output is generated by applying a (potentially biased) random linear regression model with &lt;code&gt;n_informative&lt;/code&gt; nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.</source>
          <target state="translated">出力は、 &lt;code&gt;n_informative&lt;/code&gt; 非ゼロリグレッサを持つ（潜在的にバイアスされた）ランダム線形回帰モデルを以前に生成された入力といくつかの調整可能なスケールを持ついくつかのガウス中心ノイズに適用することによって生成されます。</target>
        </trans-unit>
        <trans-unit id="0c62eccd54e33fb989ef31175162303c11637d7c" translate="yes" xml:space="preserve">
          <source>The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If &lt;code&gt;flip_sign&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.</source>
          <target state="translated">特異値分解の出力は、特異ベクトルの符号の順列までのみ一意です。 &lt;code&gt;flip_sign&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合、左の特異ベクトルの各コンポーネントの最大の負荷を正にすることにより、符号のあいまいさが解決されます。</target>
        </trans-unit>
        <trans-unit id="ab5b2ab18fdef999b51de35c5e9c33e7d3ac7cc7" translate="yes" xml:space="preserve">
          <source>The output of the 3 models are combined in a 2D graph where nodes represents the stocks and edges the:</source>
          <target state="translated">3つのモデルの出力は、ノードが株式とエッジを表す2Dグラフに結合されます。</target>
        </trans-unit>
        <trans-unit id="c9b52f3c910ded5afcb915db265ed0583f446660" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to as the 1-of-K coding scheme.</source>
          <target state="translated">変換の出力は、1-of-K符号化方式と呼ばれることがあります。</target>
        </trans-unit>
        <trans-unit id="f172299244e465e38a0859a0349f26df08f75701" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to by some authors as the 1-of-K coding scheme.</source>
          <target state="translated">変換の出力は、いくつかの著者によって1-of-K符号化方式と呼ばれることがあります。</target>
        </trans-unit>
        <trans-unit id="58983c4b722f7fa5c156e992e47b9ca634d33156" translate="yes" xml:space="preserve">
          <source>The output values.</source>
          <target state="translated">出力値です。</target>
        </trans-unit>
        <trans-unit id="f348d2a86dcf3bc20a82250a4d5068aa50c67aad" translate="yes" xml:space="preserve">
          <source>The overall complexity of Isomap is \(O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]\).</source>
          <target state="translated">Isomapの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[N^2(k+\log(N))]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="9cec4ae56de9cf1cd9cf2f8a0ff2e2e24864f054" translate="yes" xml:space="preserve">
          <source>The overall complexity of MLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\).</source>
          <target state="translated">MLLE の全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[N (k-D)k^2]+O[d N^2]となります。)</target>
        </trans-unit>
        <trans-unit id="7fa546000c6a79308906d0c040bf81047f6b149e" translate="yes" xml:space="preserve">
          <source>The overall complexity of spectral embedding is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">スペクトル埋め込みの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="808f28c633edaf7537ca8395b6bc52f0086ed496" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard HLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\).</source>
          <target state="translated">標準的なHLLEの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[N d^6]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="63617858af3d41882021a1ab37e78e76cce39983" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">標準的なLLEの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="b88c53c3806a592f7e00e19aef010a599cfaf4dc" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LTSA is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]\).</source>
          <target state="translated">標準的なLTSAの全体的な複雑さは、\(O[D \log(k)N \log(N)]+O[D N k^3]+O[k^2 d]+O[d N^2]です。)</target>
        </trans-unit>
        <trans-unit id="fc12a97c8a559f9672542a072947ef2eae0adcac" translate="yes" xml:space="preserve">
          <source>The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:</source>
          <target state="translated">スコアが偶然に得られる確率を近似したp値。として計算されます。</target>
        </trans-unit>
        <trans-unit id="7f9aaf22278cea2b541fbd8b88b09de54aad3e99" translate="yes" xml:space="preserve">
          <source>The parallel version of K-Means is broken on OS X when &lt;code&gt;numpy&lt;/code&gt; uses the &lt;code&gt;Accelerate&lt;/code&gt; Framework. This is expected behavior: &lt;code&gt;Accelerate&lt;/code&gt; can be called after a fork but you need to execv the subprocess with the Python binary (which multiprocessing does not do under posix).</source>
          <target state="translated">&lt;code&gt;numpy&lt;/code&gt; が &lt;code&gt;Accelerate&lt;/code&gt; Frameworkを使用すると、K-MeansのパラレルバージョンがOS Xで壊れます。これは予想される動作です。フォークの後に &lt;code&gt;Accelerate&lt;/code&gt; を呼び出すことができますが、サブプロセスをPythonバイナリで実行する必要があります（posixではマルチプロセッシングは行いません）。</target>
        </trans-unit>
        <trans-unit id="6d1486e3a09b61694eecc888bffbc6ccf12d8263" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf&quot; id=&quot;id20&quot;&gt;[HTF]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id21&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="translated">パラメーター &lt;code&gt;learning_rate&lt;/code&gt; は、パラメーター &lt;code&gt;n_estimators&lt;/code&gt; （適合させる弱い学習者の数）と強く相互作用します。 &lt;code&gt;learning_rate&lt;/code&gt; 値が小さいほど、一定のトレーニングエラーを維持するために、より多くの弱い学習者が必要になります。経験的証拠は、 &lt;code&gt;learning_rate&lt;/code&gt; の値が小さいほど、テストエラーが改善されることを示しています。&lt;a href=&quot;#htf&quot; id=&quot;id20&quot;&gt;[HTF]&lt;/a&gt;は、学習率を小さな定数（たとえば、 &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt; ）に &lt;code&gt;n_estimators&lt;/code&gt; 、早期停止によってn_estimatorsを選択することをお勧めします。 &lt;code&gt;learning_rate&lt;/code&gt; と &lt;code&gt;n_estimators&lt;/code&gt; の間の相互作用の詳細については、&lt;a href=&quot;#r2007&quot; id=&quot;id21&quot;&gt;[ &lt;/a&gt;R2007 ]を参照してください。</target>
        </trans-unit>
        <trans-unit id="26cab4ba2ce5f7f1aa5cff1bedcda88264af63ea" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="translated">パラメータ &lt;code&gt;learning_rate&lt;/code&gt; は、適合させる弱学習器の数であるパラメータ &lt;code&gt;n_estimators&lt;/code&gt; と強く相互作用します。 &lt;code&gt;learning_rate&lt;/code&gt; の値が小さいほど、一定のトレーニングエラーを維持するために多数の弱学習器が必要になります。経験的証拠は、 &lt;code&gt;learning_rate&lt;/code&gt; の値が小さいほど、テストエラーが改善されることを示しています。&lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt;学習率を小さな定数（たとえば &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt; ）に &lt;code&gt;n_estimators&lt;/code&gt; 、早期に停止してn_estimatorsを選択することをお勧めします。 &lt;code&gt;learning_rate&lt;/code&gt; と &lt;code&gt;n_estimators&lt;/code&gt; の相互作用の詳細については、&lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[ &lt;/a&gt;R2007 ]を参照してください。</target>
        </trans-unit>
        <trans-unit id="0e6ab9c66156a9d2feae55ed2060a2c6f7d6986c" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;memory&lt;/code&gt; is needed in order to cache the transformers. &lt;code&gt;memory&lt;/code&gt; can be either a string containing the directory where to cache the transformers or a &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; object:</source>
          <target state="translated">トランスフォーマーをキャッシュするためにパラメーター・ &lt;code&gt;memory&lt;/code&gt; が必要です。 &lt;code&gt;memory&lt;/code&gt; は、トランスフォーマーをキャッシュするディレクトリを含む文字列、または&lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt;オブジェクトのいずれかです。</target>
        </trans-unit>
        <trans-unit id="171048c7012440ddda965396141222fe52434637" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;normalize&lt;/code&gt; allows to report ratios instead of counts. The confusion matrix can be normalized in 3 different ways: &lt;code&gt;'pred'&lt;/code&gt;, &lt;code&gt;'true'&lt;/code&gt;, and &lt;code&gt;'all'&lt;/code&gt; which will divide the counts by the sum of each columns, rows, or the entire matrix, respectively.</source>
          <target state="translated">パラメータ &lt;code&gt;normalize&lt;/code&gt; を使用すると、カウントではなく比率をレポートできます。混同行列は、 &lt;code&gt;'pred'&lt;/code&gt; 、 &lt;code&gt;'true'&lt;/code&gt; 、および &lt;code&gt;'all'&lt;/code&gt; 3つの異なる方法で正規化できます。これらは、それぞれ、各列、行、または行列全体の合計でカウントを除算します。</target>
        </trans-unit>
        <trans-unit id="a19846a3c7376460662acabedd23579aba492f87" translate="yes" xml:space="preserve">
          <source>The parameter \(\nu\) is also called the &lt;strong&gt;learning rate&lt;/strong&gt; because it scales the step length the gradient descent procedure; it can be set via the &lt;code&gt;learning_rate&lt;/code&gt; parameter.</source>
          <target state="translated">パラメータ\（\ nu \）は、勾配降下法のステップ長をスケーリングするため、&lt;strong&gt;学習率&lt;/strong&gt;とも呼ばれます。 &lt;code&gt;learning_rate&lt;/code&gt; パラメータで設定できます。</target>
        </trans-unit>
        <trans-unit id="76b14eb14f0e0be16176b9f0182e58f523e33b2d" translate="yes" xml:space="preserve">
          <source>The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers.</source>
          <target state="translated">パラメータεは,外れ値として分類されるべきサンプルの数を制御します.イプシロンが小さければ小さいほど、外れ値に対してよりロバストです。</target>
        </trans-unit>
        <trans-unit id="33a23a95b6f6e50b11cce3982dee22dc3afbeaef" translate="yes" xml:space="preserve">
          <source>The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.</source>
          <target state="translated">探索するパラメータグリッドで、エスティメー タのパラメータを許容値のシーケンスにマッピングするディクショナリとして使用します。</target>
        </trans-unit>
        <trans-unit id="95ad4f92539995843fbdd8a8c8263fdd9652cae5" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. More specifically, the optimization objective is:</source>
          <target state="translated">パラメータl1_ratioはglmnet Rパッケージのαに相当し、αはglmnetのラムダパラメータに相当します。より具体的には、最適化の目的は</target>
        </trans-unit>
        <trans-unit id="444f64a3bffea2bee3d3973fdc03c3dbbbbf54d9" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio &amp;lt;= 0.01 is not reliable, unless you supply your own sequence of alpha.</source>
          <target state="translated">パラメータl1_ratioはglmnet Rパッケージのアルファに対応し、アルファはglmnetのラムダパラメータに対応します。具体的には、l1_ratio = 1は投げ縄ペナルティです。現在、独自のアルファシーケンスを提供しない限り、l1_ratio &amp;lt;= 0.01は信頼できません。</target>
        </trans-unit>
        <trans-unit id="3d75707f4ee017e7f35982b539c7d0e6825a3d30" translate="yes" xml:space="preserve">
          <source>The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized.</source>
          <target state="translated">学習した関数の滑らかさを制御するパラメータν.nu が小さいほど,近似関数の平滑度は低くなります.nu=infの場合,カーネルはRBFカーネルと等価になり,nu=0.5の場合は絶対指数カーネルと等価になります.重要な中間値は,nu=1.5 (1回微分可能な関数)とnu=2.5 (2回微分可能な関数)です.0.5,1.5,2.5,inf]以外の値は,修正ベッセル関数を評価する必要があるため,かなり高い計算コストがかかることに注意してください (約10倍)。さらに、lとは対照的に、nuは初期値に固定され、最適化されていません。</target>
        </trans-unit>
        <trans-unit id="fa3c09390eafe53f0b58881f0d7db646d5796fe2" translate="yes" xml:space="preserve">
          <source>The parameters \(\sigma_y\) and \(\mu_y\) are estimated using maximum likelihood.</source>
          <target state="translated">パラメーターは、最尤度を用いて推定されている。</target>
        </trans-unit>
        <trans-unit id="d8334dfb20de589f58500a915aef89a4fab7377d" translate="yes" xml:space="preserve">
          <source>The parameters \(\theta_y\) is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:</source>
          <target state="translated">The parameters \(\theta_y)は、最尤の平滑化されたバージョン、つまり相対的な周波数カウントで推定される。</target>
        </trans-unit>
        <trans-unit id="c10d9b59570fee3848efafd20062e78feab2baf1" translate="yes" xml:space="preserve">
          <source>The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model, the regularization parameters \(\alpha\) and \(\lambda\) being estimated by maximizing the &lt;em&gt;log marginal likelihood&lt;/em&gt;. The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters \(\alpha\) and \(\lambda\) is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters &lt;code&gt;alpha_init&lt;/code&gt; and &lt;code&gt;lambda_init&lt;/code&gt;.</source>
          <target state="translated">パラメータ\（w \）、\（\ alpha \）、および\（\ lambda \）は、モデルの適合中に共同で推定され、正則化パラメータ\（\ alpha \）および\（\ lambda \）はによって推定されます。&lt;em&gt;対数周辺尤度の&lt;/em&gt;最大化。scikit-learnの実装は、（Tipping、2001）の付録Aで説明されているアルゴリズムに基づいており、パラメーター\（\ alpha \）と\（\ lambda \）の更新は（MacKay、1992）で提案されているように行われます。最大化プロシージャの初期値は、ハイパー &lt;code&gt;alpha_init&lt;/code&gt; および &lt;code&gt;lambda_init&lt;/code&gt; を使用して設定できます。</target>
        </trans-unit>
        <trans-unit id="162d2ed9f70c881f87134a87c0c741cb23832c22" translate="yes" xml:space="preserve">
          <source>The parameters implementation of the &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; &lt;/a&gt;クラスのパラメーター実装は、重み分布に対して2つのタイプの事前分布を提案します。ディリクレ分布を伴う有限混合モデルと、ディリクレプロセスを伴う無限混合モデルです。実際には、ディリクレプロセスの推論アルゴリズムは近似され、固定された最大数のコンポーネント（スティック破壊表現と呼ばれます）で切り捨てられた分布を使用します。実際に使用されるコンポーネントの数は、ほとんどの場合、データに依存します。</target>
        </trans-unit>
        <trans-unit id="957eda9a79d6f1f87ea7b634983b57c94627c96b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.</source>
          <target state="translated">これらの方法を適用するために使用される推定器のパラメータは、パラメータグリッド上で交差検証されたグリッドサーチによって最適化されます。</target>
        </trans-unit>
        <trans-unit id="6f20d8d61d83d6376fa9caf869f91f4640e0cc5b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.</source>
          <target state="translated">これらの手法を適用するために使用されているエスティメー タのパラメータは、パラメータ設定の上で交差検証された検索によって最適化されています。</target>
        </trans-unit>
        <trans-unit id="1d4a34fa151b21edbbd9fa634476deabc1cb44cf" translate="yes" xml:space="preserve">
          <source>The parameters of the power transformation for the selected features.</source>
          <target state="translated">選択された特徴量の電力変換のパラメータ。</target>
        </trans-unit>
        <trans-unit id="62f294aa209942b08fddf4e74d29c13f78a9e67d" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter.</source>
          <target state="translated">選択されたパラメータは、スコアリングパラメータに応じて、ホールドアウトされたデータのスコアを最大化するパラメータである。</target>
        </trans-unit>
        <trans-unit id="b6d7555a36c15ae2ffe9751024a0eebcf2421d57" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead.</source>
          <target state="translated">選択されたパラメータは、明示的なスコアが渡された場合はそれが代わりに使用されますが、明示的なスコアが渡されない限り、残されたデータのスコアを最大にするパラメータが選択されます。</target>
        </trans-unit>
        <trans-unit id="34d594dbc00b197d06cf788b61a5dfe36db335bf" translate="yes" xml:space="preserve">
          <source>The parameters that have been evaluated.</source>
          <target state="translated">評価されたパラメータ。</target>
        </trans-unit>
        <trans-unit id="f139c5090bf57fe74c58422c7266093265927a67" translate="yes" xml:space="preserve">
          <source>The parent of each node. Only returned when a connectivity matrix is specified, elsewhere &amp;lsquo;None&amp;rsquo; is returned.</source>
          <target state="translated">各ノードの親。接続マトリックスが指定されている場合にのみ返されます。それ以外の場合は「なし」が返されます。</target>
        </trans-unit>
        <trans-unit id="491910df08517ded9cf6622a10e93f79d8d7b51e" translate="yes" xml:space="preserve">
          <source>The partial depdendence curves can be plotted for the multi-layer perceptron. In this case, &lt;code&gt;line_kw&lt;/code&gt; is passed to &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to change the color of the curve.</source>
          <target state="translated">多層パーセプトロンの部分依存曲線をプロットできます。この場合、 &lt;code&gt;line_kw&lt;/code&gt; はに渡される&lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt;曲線の色を変更します。</target>
        </trans-unit>
        <trans-unit id="89bfd729c83f5c14808628511ada19e6e2b220e5" translate="yes" xml:space="preserve">
          <source>The partial dependence function evaluated on the &lt;code&gt;grid&lt;/code&gt;. For regression and binary classification &lt;code&gt;n_classes==1&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 評価された部分依存関数。回帰およびバイナリ分類の場合 &lt;code&gt;n_classes==1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="95b5d0f6fa140b6ee1f70868d5a14424c22e2d4c" translate="yes" xml:space="preserve">
          <source>The partial dependence of the response \(f\) at a point \(x_S\) is defined as:</source>
          <target state="translated">応答のpartial dependence of response \(f\)at a point (x_S\)は、次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="053dbcb3edc2e230ef9a4eff0313b65bae4689ca" translate="yes" xml:space="preserve">
          <source>The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">パッシブアグレッシブアルゴリズムは、大規模な学習のためのアルゴリズムのファミリーです。学習率を必要としない点でパーセプトロンに似ています。ただし、パーセプトロンとは異なり、正則化パラメーター &lt;code&gt;C&lt;/code&gt; が含まれています。</target>
        </trans-unit>
        <trans-unit id="cf6ba8c5f2e782ae102fc993ff0f91af87853f26" translate="yes" xml:space="preserve">
          <source>The path of the base directory to use as a data store or None. If None is given, no caching is done and the Memory object is completely transparent. This option replaces cachedir since version 0.12.</source>
          <target state="translated">データストアとして使用するベースディレクトリのパス、または None。None を指定した場合、キャッシュは行われず、Memory オブジェクトは完全に透過的になります。このオプションはバージョン 0.12 以降では cachedir に置き換わります。</target>
        </trans-unit>
        <trans-unit id="e60c1638cc188f171e30720c51f9233ac8e0591d" translate="yes" xml:space="preserve">
          <source>The path to scikit-learn data dir.</source>
          <target state="translated">scikit-learnのデータディレクトリへのパス。</target>
        </trans-unit>
        <trans-unit id="d92bdbe34c5616d0c8598712ff33d12e5d0daade" translate="yes" xml:space="preserve">
          <source>The path to the location of the data.</source>
          <target state="translated">データの場所へのパスです。</target>
        </trans-unit>
        <trans-unit id="b2f9a75941863d91af30a0efa7c5bf671f8aabb3" translate="yes" xml:space="preserve">
          <source>The path to the location of the target.</source>
          <target state="translated">対象の場所までのパス。</target>
        </trans-unit>
        <trans-unit id="7e58c61969eaf91a150b8c119238c04ee82b41f7" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used.</source>
          <target state="translated">使用するペナルティ(別名正則化用語)。</target>
        </trans-unit>
        <trans-unit id="780a6be1d44fa6c4bc32e0e6a573d3b4ad24b942" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to &amp;lsquo;l2&amp;rsquo; which is the standard regularizer for linear SVM models. &amp;lsquo;l1&amp;rsquo; and &amp;lsquo;elasticnet&amp;rsquo; might bring sparsity to the model (feature selection) not achievable with &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="translated">使用するペナルティ（正則化用語）。線形SVMモデルの標準正則化子である 'l2'がデフォルトです。「l1」と「elasticnet」は、「l2」では実現できないスパース性をモデル（機能選択）にもたらす可能性があります。</target>
        </trans-unit>
        <trans-unit id="5ec62ab0e251a48390ce125b16975a3fa4c7ca91" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to None.</source>
          <target state="translated">使用するペナルティ(別名正則化項)。デフォルトは None です。</target>
        </trans-unit>
        <trans-unit id="3874ef3081cc70f2fd57e0725f8bff89bf9a04cd" translate="yes" xml:space="preserve">
          <source>The performance is may slightly worse for the randomized search, and is likely due to a noise effect and would not carry over to a held-out test set.</source>
          <target state="translated">パフォーマンスは、ランダム化された検索では若干悪くなる可能性があり、ノイズの影響が原因である可能性が高く、ホールドアウトされたテストセットに持ち越されることはありません。</target>
        </trans-unit>
        <trans-unit id="195880b4735384ca5f4a3196f2d3109be0fc5155" translate="yes" xml:space="preserve">
          <source>The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.</source>
          <target state="translated">これはノイズの影響である可能性が高く、テストセットを保持したままの状態には持ち越されませんが、無作為化探索ではパフォーマンスがわずかに低下します。</target>
        </trans-unit>
        <trans-unit id="66758ccad6c0faa66ee36e2739cca75d7cb80119" translate="yes" xml:space="preserve">
          <source>The performance measure reported by &lt;em&gt;k&lt;/em&gt;-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</source>
          <target state="translated">&lt;em&gt;k&lt;/em&gt;分割交差検証によって報告されるパフォーマンス測定は、ループで計算された値の平均です。このアプローチは計算コストが高くなる可能性がありますが、（任意の検証セットを修正する場合のように）データを無駄にしないため、サンプル数が非常に少ない逆推論などの問題で大きな利点になります。</target>
        </trans-unit>
        <trans-unit id="773c4b551d63eb1e8cc16b04d1476c48de12db0a" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="translated">SAMMEとSAMME.Rのパフォーマンス&lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;1つの&lt;/a&gt;アルゴリズムが比較されます。 SAMME.Rは確率推定を使用して加法モデルを更新しますが、SAMMEは分類のみを使用します。例が示すように、SAMME.Rアルゴリズムは通常、SAMMEよりも速く収束し、より少ないブースト反復でより低いテストエラーを達成します。各ブースティング反復後のテストセットの各アルゴリズムのエラーが左側に表示され、各ツリーのテストセットの分類エラーが中央に表示され、各ツリーのブースト重みが右側に表示されます。 SAMME.Rアルゴリズムでは、すべてのツリーの重みが1であるため、表示されていません。</target>
        </trans-unit>
        <trans-unit id="3b50299f6e3a476dff9e3b98d46cee16ed70fcdb" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="translated">SAMMEおよびSAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;アルゴリズムのパフォーマンスが比較されます。SAMME.Rは確率推定を使用して加法モデルを更新しますが、SAMME.Rは分類のみを使用します。例が示すように、SAMME.Rアルゴリズムは通常、SAMMEよりも速く収束し、ブースティングの反復回数を減らしてテストエラーを低減します。各ブースティング反復後のテストセットの各アルゴリズムのエラーは左側に表示され、各ツリーのテストセットの分類エラーは中央に表示され、各ツリーのブーストウェイトは右側に表示されます。SAMME.Rアルゴリズムでは、すべてのツリーの重みが1であるため、表示されていません。</target>
        </trans-unit>
        <trans-unit id="54c9475018dc0386efaf6207cbf2038791834d22" translate="yes" xml:space="preserve">
          <source>The performance of the models can be evaluated by their ability to yield well-calibrated predictions and a good ranking.</source>
          <target state="translated">モデルの性能は、よくキャリブレーションされた予測値が得られることと、良いランキングが得られることで評価することができます。</target>
        </trans-unit>
        <trans-unit id="2f1cd52ca3b14d7125644c3724ffbf78c03b2ab3" translate="yes" xml:space="preserve">
          <source>The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step.</source>
          <target state="translated">次に、選択されたハイパーパラメータと訓練されたモデルの性能を、モデル選択ステップでは使用されなかった専用の評価セットで測定します。</target>
        </trans-unit>
        <trans-unit id="dd106efb0f7012bff421a0ad8f3697f8283515ed" translate="yes" xml:space="preserve">
          <source>The periodicity of the kernel.</source>
          <target state="translated">カーネルの周期性。</target>
        </trans-unit>
        <trans-unit id="5dce1e0dbd7277c2f73689bcac70f69df5d2fc02" translate="yes" xml:space="preserve">
          <source>The perplexity is defined as \(k=2^{(S)}\) where \(S\) is the Shannon entropy of the conditional probability distribution. The perplexity of a \(k\)-sided die is \(k\), so that \(k\) is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.</source>
          <target state="translated">錯乱度は、条件付き確率分布のシャノンエントロピーと定義されます。\(k\)sided dieのpleplexityは、\(k\)であるから、条件付き確率を生成する際に、t-SNEが考慮する最寄りの隣人の数は、実質的にはT-SNEが考慮した数となる。パープルクシティが大きいほど、より多くの最 近隣人が存在し、小さな構造の影響を受けにくくなります。逆に、より低いパープレキシシティでは、より少ない数の最近傍を考慮するため、局所的な近傍を優先してより多くの大域的な情報を無視することになります。データセットのサイズが大きくなればなるほど,局所近傍の妥当なサンプルを得るためには,より多くのポイントが必要となり,そのため,より大きな過重度が必要となることがある.同様に,ノイズの多いデータセットでは,バックグラウンドノイズを超えて十分なローカル近傍を網羅するために,より大きなパープレキシシティ値が必要となる.</target>
        </trans-unit>
        <trans-unit id="61a03c034b8bc8a8d92d85436ffe8ab0dc0dda5b" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significanlty different results.</source>
          <target state="translated">錯綜度は、他のマニホールド学習アルゴリズムで使用される最近傍数に関連しています。大きなデータセットでは、通常、より大きなペルプレキシシティが必要になります。5から50の間の値を選択することを検討してください。値が異なると、結果が大きく異なる場合があります。</target>
        </trans-unit>
        <trans-unit id="08299376158415917838ffdf1e208cdfe76446d4" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter.</source>
          <target state="translated">錯綜度は、他のマニホールド学習アルゴリズムで使用される最近傍数に関連しています。大きなデータセットでは、通常、より大きなペルプレキシシティが必要になります。5から50の間の値を選択することを検討してください。t-SNEはこのパラメータに対して非常に鈍感なので、この選択は非常に重要ではありません。</target>
        </trans-unit>
        <trans-unit id="042f3b8255d615a1ffcd846018bae060090b690e" translate="yes" xml:space="preserve">
          <source>The physical location of boston csv dataset.</source>
          <target state="translated">ボストンcsvデータセットの物理的な位置。</target>
        </trans-unit>
        <trans-unit id="b0881df888192122fb3f0dbde6c8cd7ff6f0357c" translate="yes" xml:space="preserve">
          <source>The pipeline below extracts the subject and body from each post using &lt;code&gt;SubjectBodyExtractor&lt;/code&gt;, producing a (n_samples, 2) array. This array is then used to compute standard bag-of-words features for the subject and body as well as text length and number of sentences on the body, using &lt;code&gt;ColumnTransformer&lt;/code&gt;. We combine them, with weights, then train a classifier on the combined set of features.</source>
          <target state="translated">以下のパイプラインは、 &lt;code&gt;SubjectBodyExtractor&lt;/code&gt; を使用して各投稿から件名と本文を抽出し、（n_samples、2）配列を生成します。次に、この配列を使用して、 &lt;code&gt;ColumnTransformer&lt;/code&gt; を使用して、件名と本文の標準的なbag-of-words機能、および本文のテキストの長さと文の数を計算します。それらを重みと組み合わせてから、組み合わせた機能のセットで分類器をトレーニングします。</target>
        </trans-unit>
        <trans-unit id="3d1257b149fa9d88b1966c8dbdc55bf21ea6a0db" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed.</source>
          <target state="translated">欠損値のプレースホルダー。 &lt;code&gt;missing_values&lt;/code&gt; のすべての出現が帰属されます。</target>
        </trans-unit>
        <trans-unit id="9c95ae315d785709e445ae217c5567fb1ce65f07" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For missing values encoded as np.nan, use the string value &amp;ldquo;NaN&amp;rdquo;.</source>
          <target state="translated">欠損値のプレースホルダー。 &lt;code&gt;missing_values&lt;/code&gt; のすべての出現が帰属されます。np.nanとしてエンコードされた欠損値については、文字列値「NaN」を使用します。</target>
        </trans-unit>
        <trans-unit id="7d178852ef63f5a60aca24eb2ef7cd366b0501af" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For pandas&amp;rsquo; dataframes with nullable integer dtypes with missing values, &lt;code&gt;missing_values&lt;/code&gt; should be set to &lt;code&gt;np.nan&lt;/code&gt;, since &lt;code&gt;pd.NA&lt;/code&gt; will be converted to &lt;code&gt;np.nan&lt;/code&gt;.</source>
          <target state="translated">欠落している値のプレースホルダー。 &lt;code&gt;missing_values&lt;/code&gt; すべての出現が帰属されます。欠損値とNULL可能整数dtypesとパンダデータフレームの場合は、 &lt;code&gt;missing_values&lt;/code&gt; をに設定する必要があり &lt;code&gt;np.nan&lt;/code&gt; 以来、 &lt;code&gt;pd.NA&lt;/code&gt; はに変換されます &lt;code&gt;np.nan&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="182f79373d80c85b96a18c9f29af644fec99f0de" translate="yes" xml:space="preserve">
          <source>The plot above tells us about dependencies between a specific feature and the target when all other features remain constant, i.e., &lt;strong&gt;conditional dependencies&lt;/strong&gt;. An increase of the AGE will induce a decrease of the WAGE when all other features remain constant. On the contrary, an increase of the EXPERIENCE will induce an increase of the WAGE when all other features remain constant. Also, AGE, EXPERIENCE and EDUCATION are the three variables that most influence the model.</source>
          <target state="translated">上記のプロットは、他のすべての機能が一定のままである場合の特定の機能とターゲット間の依存関係、つまり&lt;strong&gt;条件付き依存関係について示してい&lt;/strong&gt;ます。他のすべての機能が一定のままである場合、AGEの増加は、WAGEの減少を引き起こします。それどころか、他のすべての機能が一定のままである場合、経験値の増加は賃金の増加を引き起こします。また、AGE、EXPERIENCE、EDUCATIONは、モデルに最も影響を与える3つの変数です。</target>
        </trans-unit>
        <trans-unit id="50d8f581357d70f0923b3a8bf25734f11fbbdc88" translate="yes" xml:space="preserve">
          <source>The plot represents the learning curve of the classifier: the evolution of classification accuracy over the course of the mini-batches. Accuracy is measured on the first 1000 samples, held out as a validation set.</source>
          <target state="translated">プロットは、分類器の学習曲線を表しています:ミニバッチの間の分類精度の進化です。精度は、最初の1000サンプルで測定され、検証セットとして保存されます。</target>
        </trans-unit>
        <trans-unit id="2108780ce5e305bd6eefeccab3f1f12e712db7ba" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.</source>
          <target state="translated">プロットは、線形判別分析と二次判別分析の決定境界を示しています。下の行は、線形判別分析が線形境界しか学習できないのに対し、二次判別分析は二次境界を学習できるため、より柔軟性が高いことを示しています。</target>
        </trans-unit>
        <trans-unit id="b078f7cb7ea97d6d1e43a3acd4d110579a0862d7" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes.</source>
          <target state="translated">プロットは、視覚化のために2つの特徴のみでトレーニングとスコアリングを行った場合の、虹彩データセット上での最近傍分類と近傍成分分析の分類の決定境界を示しています。</target>
        </trans-unit>
        <trans-unit id="17f022b1b904616a8feb650bfceadf60a7908c45" translate="yes" xml:space="preserve">
          <source>The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), avg. occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and avg. rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="translated">このプロットは、4つの一方向および1つの双方向の部分依存プロットを示しています。一方向PDPのターゲット変数は、収入の中央値（ &lt;code&gt;MedInc&lt;/code&gt; ）、avgです。世帯あたりの居住者（ &lt;code&gt;AvgOccup&lt;/code&gt; ）、家の年齢の中央値（ &lt;code&gt;HouseAge&lt;/code&gt; ）、および平均。世帯あたりの部屋数（ &lt;code&gt;AveRooms&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="d1f9d8f4302df476eee3089e3160330e3191c729" translate="yes" xml:space="preserve">
          <source>The plot shows the regions where the discretized encoding is constant.</source>
          <target state="translated">プロットは、離散化された符号化が一定である領域を示しています。</target>
        </trans-unit>
        <trans-unit id="a1523290796b6a8ba4024eaf4b48817249c66e13" translate="yes" xml:space="preserve">
          <source>The plots below illustrate the effect the parameter &lt;code&gt;C&lt;/code&gt; has on the separation line. A large value of &lt;code&gt;C&lt;/code&gt; basically tells our model that we do not have that much faith in our data&amp;rsquo;s distribution, and will only consider points close to line of separation.</source>
          <target state="translated">以下のプロットは、パラメーター &lt;code&gt;C&lt;/code&gt; が分離線に及ぼす影響を示しています。 &lt;code&gt;C&lt;/code&gt; の値が大きいと、基本的にモデルにデータの分布をそれほど信用していないことを伝え、分離線に近い点のみを考慮します。</target>
        </trans-unit>
        <trans-unit id="faaabc25a5f9dd9badc9ef9f18d9770e507882de" translate="yes" xml:space="preserve">
          <source>The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.</source>
          <target state="translated">プロットは、最初に3つのクラスタを使用して、K-meansアルゴリズムがどのような結果をもたらすかを表示します。次に、初期化が悪かった場合の分類プロセスへの影響が示されています。n_initを1(デフォルトは10)だけに設定することで、異なるセントロイドシードでアルゴリズムが実行される回数が減少します。次のプロットは、8つのクラスターを使用した場合の結果と、最終的な真実を示しています。</target>
        </trans-unit>
        <trans-unit id="3ad47df403d87eb821e2edd090b9e74720bc0be8" translate="yes" xml:space="preserve">
          <source>The plots represent the distribution of the prediction latency as a boxplot.</source>
          <target state="translated">プロットは、予測待ち時間の分布をボックスプロットで表したものです。</target>
        </trans-unit>
        <trans-unit id="9d34744e2f178eb734dfa28bde9ee7e67cdcc28f" translate="yes" xml:space="preserve">
          <source>The plots show four 1-way and two 1-way partial dependence plots (omitted for &lt;a href=&quot;../../modules/generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt;&lt;code&gt;MLPRegressor&lt;/code&gt;&lt;/a&gt; due to computation time). The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), average occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and average rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="translated">プロットは、4つの1方向および（のために省略2つの1ウェイ部分依存性プロットを示し&lt;a href=&quot;../../modules/generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt; &lt;code&gt;MLPRegressor&lt;/code&gt; &lt;/a&gt;による計算時間を）。一方向PDPのターゲット変数は、収入の中央値（ &lt;code&gt;MedInc&lt;/code&gt; ）、世帯あたりの平均居住者（ &lt;code&gt;AvgOccup&lt;/code&gt; ）、世帯年齢の中央値（ &lt;code&gt;HouseAge&lt;/code&gt; ）、および世帯あたりの平均部屋数（ &lt;code&gt;AveRooms&lt;/code&gt; ）です。</target>
        </trans-unit>
        <trans-unit id="ee583f2ef106ae04159c6d135c18b8bf01049699" translate="yes" xml:space="preserve">
          <source>The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.</source>
          <target state="translated">プロットは学習点を実線で、テスト点を半透明で示しています。右下は、テストセットでの分類精度を示しています。</target>
        </trans-unit>
        <trans-unit id="a10b5c6300ecb650c0782e5a6bff1ffc576100bb" translate="yes" xml:space="preserve">
          <source>The point cloud spanned by the observations above is very flat in one direction: one of the three univariate features can almost be exactly computed using the other two. PCA finds the directions in which the data is not &lt;em&gt;flat&lt;/em&gt;</source>
          <target state="translated">上記の観測にまたがる点群は、一方向に非常に平坦です。3つの一変量特徴の1つは、他の2つを使用してほぼ正確に計算できます。PCAはデータが&lt;em&gt;フラット&lt;/em&gt;でない方向を見つける&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="9874b880b8ee39ca50e864dab01006b5213a2351" translate="yes" xml:space="preserve">
          <source>The points.</source>
          <target state="translated">ポイントは</target>
        </trans-unit>
        <trans-unit id="0cdbbfd6a3924811880d5b83f6e26dafaaff0147" translate="yes" xml:space="preserve">
          <source>The polynomial kernel is defined as:</source>
          <target state="translated">多項式カーネルは次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="8b34b53d18875cd269c1341b177c3bcbb9230141" translate="yes" xml:space="preserve">
          <source>The pooled values for each feature cluster.</source>
          <target state="translated">各特徴クラスタのプールされた値。</target>
        </trans-unit>
        <trans-unit id="6285f4c6cbbb9a8676e04ca09291d710e5d4992f" translate="yes" xml:space="preserve">
          <source>The possible options are &amp;lsquo;hinge&amp;rsquo;, &amp;lsquo;log&amp;rsquo;, &amp;lsquo;modified_huber&amp;rsquo;, &amp;lsquo;squared_hinge&amp;rsquo;, &amp;lsquo;perceptron&amp;rsquo;, or a regression loss: &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;.</source>
          <target state="translated">可能なオプションは、「hinge」、「log」、「modified_huber」、「squared_hinge」、「perceptron」、または回帰損失です：「squared_loss」、「huber」、「epsilon_insensitive」、または「squared_epsilon_insensitive」。</target>
        </trans-unit>
        <trans-unit id="6a0542e47ae26fdcd6f04cfe1be082ea4e8e2319" translate="yes" xml:space="preserve">
          <source>The power determines the underlying target distribution according to the following table:</source>
          <target state="translated">パワーは、以下の表に従って、基礎となるターゲット分布を決定します。</target>
        </trans-unit>
        <trans-unit id="5a756ae9d1d42224e6a27c29b135c093bd570bbe" translate="yes" xml:space="preserve">
          <source>The power of the Minkowski metric to be used to calculate distance between points.</source>
          <target state="translated">点間距離を計算するために使用するミンコフスキーメトリックのパワー。</target>
        </trans-unit>
        <trans-unit id="581738f7dfe64a35233afe8207a1e82379bfb8cd" translate="yes" xml:space="preserve">
          <source>The power transform is useful as a transformation in modeling problems where homoscedasticity and normality are desired. Below are examples of Box-Cox and Yeo-Johnwon applied to six different probability distributions: Lognormal, Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.</source>
          <target state="translated">べき乗変換は、同種確率と正規性が望まれるモデル化問題における変換として有用である。以下は、6つの異なる確率分布に適用されたBox-CoxとYeo-Johnwonの例です。対数正規分布、カイ二乗分布、ワイブル分布、ガウス分布、一様分布、バイモーダル分布です。</target>
        </trans-unit>
        <trans-unit id="837e2e1ea652ea6696a91670a638bda69523a446" translate="yes" xml:space="preserve">
          <source>The power transform method. Available methods are:</source>
          <target state="translated">電力変換法です。利用可能な方法は以下の通りです。</target>
        </trans-unit>
        <trans-unit id="1605256f2d1d73782f41770777e3757d50960cf1" translate="yes" xml:space="preserve">
          <source>The power transform method. Currently, &amp;lsquo;box-cox&amp;rsquo; (Box-Cox transform) is the only option available.</source>
          <target state="translated">パワー変換メソッド。現在、「box-cox」（Box-Cox変換）のみが使用可能なオプションです。</target>
        </trans-unit>
        <trans-unit id="9e3ef4e072a07501cd2ec598f0a1011b6178f209" translate="yes" xml:space="preserve">
          <source>The precision is the ratio &lt;code&gt;tp / (tp + fp)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fp&lt;/code&gt; the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.</source>
          <target state="translated">精度は比であり、 &lt;code&gt;tp / (tp + fp)&lt;/code&gt; &lt;code&gt;tp&lt;/code&gt; 真陽性の数であり、 &lt;code&gt;fp&lt;/code&gt; を偽陽性の数を。精度は直観的には、分類子が負のサンプルを正としてラベル付けしない能力です。</target>
        </trans-unit>
        <trans-unit id="5e318fd9419ebb0c7685cd2fcf271eb0864ec5bb" translate="yes" xml:space="preserve">
          <source>The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">混合物の各成分の精度行列。精度行列は、共分散行列の逆です。共分散行列は対称正定行列であるため、ガウス分布の混合は、精度行列によって同等にパラメーター化できます。共分散行列の代わりに精度行列を格納すると、テスト時に新しいサンプルの対数尤度を計算するのがより効率的になります。形状は &lt;code&gt;covariance_type&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="027e42693446de348f6d01928068eb7453ef8a97" translate="yes" xml:space="preserve">
          <source>The precision matrix associated to the current covariance object.</source>
          <target state="translated">現在の共分散オブジェクトに関連付けられた精度行列.</target>
        </trans-unit>
        <trans-unit id="d78de957c617714f761992022534f1c5cbc37dc0" translate="yes" xml:space="preserve">
          <source>The precision of each components on the mean distribution (Gaussian).</source>
          <target state="translated">平均分布(ガウス分布)上の各成分の精度。</target>
        </trans-unit>
        <trans-unit id="208aaa08d6ec7d945a950063ea09933d0bd5ac66" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;.</source>
          <target state="translated">平均分布の事前精度（ガウス）。手段を配置できる場所までの延長を制御します。値が小さいほど、各クラスターの &lt;code&gt;mean_prior&lt;/code&gt; 周りに集中します。</target>
        </trans-unit>
        <trans-unit id="b72296dd4015be0343e22e7dc45bdf0f7e18f582" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to 1.</source>
          <target state="translated">平均分布の事前精度（ガウス）。手段を配置できる場所までの延長を制御します。値が小さいほど、各クラスターの &lt;code&gt;mean_prior&lt;/code&gt; 周りに集中します。パラメータの値は0より大きくなければなりません。Noneの場合、1に設定されます。</target>
        </trans-unit>
        <trans-unit id="99b687233f264a106cecce1d5b60aee20c688782" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. Larger values concentrate the cluster means around &lt;code&gt;mean_prior&lt;/code&gt;. If mean_precision_prior is set to None, &lt;code&gt;mean_precision_prior_&lt;/code&gt; is set to 1.</source>
          <target state="translated">平均分布の事前精度（ガウス）。手段を配置できる場所の範囲を制御します。値が大きいほど、クラスター平均は &lt;code&gt;mean_prior&lt;/code&gt; の周りに集中します。mean_precision_priorがNoneに設定されている場合、 &lt;code&gt;mean_precision_prior_&lt;/code&gt; は1に設定されます。</target>
        </trans-unit>
        <trans-unit id="88b322c6c16cb10f58188170d43f7d8832a9357f" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. Larger values concentrate the cluster means around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it is set to 1.</source>
          <target state="translated">平均分布の事前精度（ガウス）。手段を配置できる場所の範囲を制御します。値が大きいほど、クラスター平均は &lt;code&gt;mean_prior&lt;/code&gt; の周りに集中します。パラメータの値は0より大きくなければなりません。Noneの場合は1に設定されます。</target>
        </trans-unit>
        <trans-unit id="03d920e00078b8bd9b4faa0b1aa14a8c65b257bb" translate="yes" xml:space="preserve">
          <source>The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</source>
          <target state="translated">精度-リコール曲線は、異なる閾値に対する精度とリコールのトレードオフを示しています。曲線の下の高い領域は、高い再現性と高い精度の両方を表し、高い精度は低い偽陽性率に関連し、高い再現性は低い偽陰性率に関連します。両方とも高いスコアは、分類器が正確な結果を返していること(高精度)と、すべての陽性結果の大部分を返していること(高リコール)を示しています。</target>
        </trans-unit>
        <trans-unit id="b03d17dd4f0a50b42b8760dd1442678e73495423" translate="yes" xml:space="preserve">
          <source>The predicted class C for each sample in X is returned.</source>
          <target state="translated">Xの各サンプルについて予測されたクラスCが返されます.</target>
        </trans-unit>
        <trans-unit id="bfeb54e7bff0bb15dd098a7327cddb86a1801899" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the base estimators in the ensemble.</source>
          <target state="translated">入力サンプルの予測クラス対数確率は、アンサンブル内の基本推定子の平均予測クラス確率の対数として計算される。</target>
        </trans-unit>
        <trans-unit id="cd67555dc49cf41a5e5df91097299e3752772d2d" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.</source>
          <target state="translated">入力サンプルの予測クラス対数確率は,森林内の木の平均予測クラス確率の対数として計算されます.</target>
        </trans-unit>
        <trans-unit id="eab6e37cbb9069ff9afad75097cea6fc5d34926b" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.</source>
          <target state="translated">入力サンプルの予測クラス対数確率は,アンサンブル内の分類器の加重平均予測クラス対数確率として計算される.</target>
        </trans-unit>
        <trans-unit id="47d729548b0a8e226d9e4c530c0297fbe0f5665f" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.</source>
          <target state="translated">入力標本の予測されたクラスは,森林内の木による投票であり,その確率推定値で重み付けされている.つまり,予測されたクラスは,木全体の平均確率推定値が最も高いものである.</target>
        </trans-unit>
        <trans-unit id="228bf14c189aaaaad2dc0e65c7c1dff58773904b" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting.</source>
          <target state="translated">入力サンプルの予測クラスは、平均予測確率が最も高いクラスとして計算されます。ベースエスティメータが &lt;code&gt;predict_proba&lt;/code&gt; メソッドを実装しない場合、投票に頼ります。</target>
        </trans-unit>
        <trans-unit id="8652ca51db9ef5a2b15410ff134f217292f6ef55" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.</source>
          <target state="translated">入力サンプルの予測クラスは,アンサンブル内の分類器の重み付き平均予測値として計算されます.</target>
        </trans-unit>
        <trans-unit id="46c6bfcd5571fcbc8ac1e94c5ec5ef7b9ca3bcfc" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">入力サンプルの予測クラス確率は、森林内の木の平均予測クラス確率として計算されます。1本の木のクラス確率は,葉の中の同じクラスのサンプルの割合である.</target>
        </trans-unit>
        <trans-unit id="90e3cd15b2277da446a86ed04d9b97d9385dbcbd" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the base estimators in the ensemble. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting and the predicted class probabilities of an input sample represents the proportion of estimators predicting each class.</source>
          <target state="translated">入力サンプルの予測クラス確率は、集団内の基本推定量の平均予測クラス確率として計算されます。ベース推定器が &lt;code&gt;predict_proba&lt;/code&gt; メソッドを実装しない場合、それは投票に頼り、入力サンプルの予測クラス確率は各クラスを予測する推定器の割合を表します。</target>
        </trans-unit>
        <trans-unit id="f43014d849d28fe556560f6e74f971c02171e223" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.</source>
          <target state="translated">入力サンプルの予測クラス確率は,アンサンブル内の分類器の加重平均予測クラス確率として計算される.</target>
        </trans-unit>
        <trans-unit id="232fb255d370f3424586a7b0c3f74d049bd6d607" translate="yes" xml:space="preserve">
          <source>The predicted class probability is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">予測クラス確率は、葉の中の同じクラスのサンプルの割合です。</target>
        </trans-unit>
        <trans-unit id="45d2cef0bd7682bfccc693d1957f2c12cdb9bae8" translate="yes" xml:space="preserve">
          <source>The predicted class.</source>
          <target state="translated">予測されたクラス。</target>
        </trans-unit>
        <trans-unit id="d01b8d940e0e34c3c7942798bc90fe03e260970d" translate="yes" xml:space="preserve">
          <source>The predicted classes, or the predict values.</source>
          <target state="translated">予測されたクラス、つまり予測値。</target>
        </trans-unit>
        <trans-unit id="b68f3b27cbad35418e1a35aab9bbe1837a195e37" translate="yes" xml:space="preserve">
          <source>The predicted classes.</source>
          <target state="translated">予測されたクラス。</target>
        </trans-unit>
        <trans-unit id="f9c849804a5f2e4c77a6bd9f1a72aeb60a174b11" translate="yes" xml:space="preserve">
          <source>The predicted log-probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;. Equivalent to log(predict_proba(X))</source>
          <target state="translated">モデル内の各クラスのサンプルの予測対数確率。クラスは &lt;code&gt;self.classes_&lt;/code&gt; にあるとおりに順序付けされます。log（predict_proba（X））と同等</target>
        </trans-unit>
        <trans-unit id="b73f981e520b253c83fbe81831bba280a3db569a" translate="yes" xml:space="preserve">
          <source>The predicted probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;.</source>
          <target state="translated">モデル内の各クラスのサンプルの予測確率。クラスは &lt;code&gt;self.classes_&lt;/code&gt; にあるとおりに順序付けされます。</target>
        </trans-unit>
        <trans-unit id="a286cd68d524dde2fbaba23c990f981ec35b78f3" translate="yes" xml:space="preserve">
          <source>The predicted probas.</source>
          <target state="translated">予測されたプロバ</target>
        </trans-unit>
        <trans-unit id="53b10e885ebd4a72b834dc0bd6649d47d09469b6" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.</source>
          <target state="translated">入力サンプルの予測回帰目標は、アンサンブル内の推定子の平均予測回帰目標として計算される。</target>
        </trans-unit>
        <trans-unit id="d575d8b045eb46db33f9cbaec364a954b218830a" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.</source>
          <target state="translated">入力サンプルの予測回帰目標は、森林内の木の平均予測回帰目標として計算される。</target>
        </trans-unit>
        <trans-unit id="21875214f30fbe829fb7e391b984beb01fcc0748" translate="yes" xml:space="preserve">
          <source>The predicted regression value of an input sample is computed as the weighted median prediction of the classifiers in the ensemble.</source>
          <target state="translated">入力サンプルの予測回帰値は,アンサンブル内の分類器の重み付き中央値予測として計算される.</target>
        </trans-unit>
        <trans-unit id="d728cdaebb39fe6a42651804a843b92d06b095fc" translate="yes" xml:space="preserve">
          <source>The predicted regression values.</source>
          <target state="translated">予測された回帰値。</target>
        </trans-unit>
        <trans-unit id="eb2e0fb384bae49f48977b71692091d27df9ee48" translate="yes" xml:space="preserve">
          <source>The predicted target values.</source>
          <target state="translated">予測された目標値。</target>
        </trans-unit>
        <trans-unit id="16c2ec05bdd825f5e946bffd1661391a4efc1866" translate="yes" xml:space="preserve">
          <source>The predicted value of the input samples.</source>
          <target state="translated">入力サンプルの予測値。</target>
        </trans-unit>
        <trans-unit id="71f7a8826bad533ae16d312cbce730009c206751" translate="yes" xml:space="preserve">
          <source>The predicted values.</source>
          <target state="translated">予測値です。</target>
        </trans-unit>
        <trans-unit id="d3f70f498146a996702d8957eebe13ff93b2e60c" translate="yes" xml:space="preserve">
          <source>The prediction interpolates the observations (at least for regular kernels).</source>
          <target state="translated">予測はオブザベーションを補間する(少なくとも正規のカーネルの場合)。</target>
        </trans-unit>
        <trans-unit id="0a5a460e70a5f18f6e563f520727c4e907b64c8a" translate="yes" xml:space="preserve">
          <source>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.</source>
          <target state="translated">予測は確率的(ガウス分布)なので、経験的信頼区間を計算し、それに基づいて、関心のある領域で予測を再フィット(オンライン・フィッティング、適応的フィッティング)すべきかどうかを決定することができます。</target>
        </trans-unit>
        <trans-unit id="888b741b5759a3600a521b02ebf591af7172583e" translate="yes" xml:space="preserve">
          <source>The prediction is:</source>
          <target state="translated">予言は、です。</target>
        </trans-unit>
        <trans-unit id="92250eb4257ffd9737b8e4be4f65018174b8c07a" translate="yes" xml:space="preserve">
          <source>The predictions for all the points in the grid, averaged over all samples in X (or over the training data if &lt;code&gt;method&lt;/code&gt; is &amp;lsquo;recursion&amp;rsquo;). &lt;code&gt;n_outputs&lt;/code&gt; corresponds to the number of classes in a multi-class setting, or to the number of tasks for multi-output regression. For classical regression and binary classification &lt;code&gt;n_outputs==1&lt;/code&gt;. &lt;code&gt;n_values_feature_j&lt;/code&gt; corresponds to the size &lt;code&gt;values[j]&lt;/code&gt;.</source>
          <target state="translated">Xのすべてのサンプル（または &lt;code&gt;method&lt;/code&gt; が「再帰」の場合はトレーニングデータ）で平均化された、グリッド内のすべてのポイントの予測。 &lt;code&gt;n_outputs&lt;/code&gt; は、マルチクラス設定のクラスの数、またはマルチ出力回帰のタスクの数に対応します。古典的な回帰と二項分類の場合 &lt;code&gt;n_outputs==1&lt;/code&gt; 。 &lt;code&gt;n_values_feature_j&lt;/code&gt; はサイズ &lt;code&gt;values[j]&lt;/code&gt; 対応します。</target>
        </trans-unit>
        <trans-unit id="4f4304f9d52554d908e981e82f60b25cc189525a" translate="yes" xml:space="preserve">
          <source>The present version of SpectralClustering requires the number of clusters to be specified in advance. It works well for a small number of clusters, but is not advised for many clusters.</source>
          <target state="translated">現在のバージョンのSpectralClusteringでは、クラスタの数を事前に指定する必要があります。これは少数のクラスタではうまく動作しますが、多くのクラスタではお勧めできません。</target>
        </trans-unit>
        <trans-unit id="22369282bb204be005e939800f6b06d8c430453c" translate="yes" xml:space="preserve">
          <source>The previously introduced metrics are &lt;strong&gt;not normalized with regards to random labeling&lt;/strong&gt;: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular &lt;strong&gt;random labeling won&amp;rsquo;t yield zero scores especially when the number of clusters is large&lt;/strong&gt;.</source>
          <target state="translated">以前に導入されたメトリックは&lt;strong&gt;、ランダムラベリングに関して正規化&lt;/strong&gt;されて&lt;strong&gt;いません&lt;/strong&gt;。つまり、サンプル、クラスター、グラウンドトゥルースクラスの数によっては、完全にランダムなラベリングでは、均一性、完全性、したがってvメジャーが同じ値になるとは限りません。特に&lt;strong&gt;、クラスターの数が多い場合は特に、ランダムラベリングではスコアがゼロになりません&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="b298c69bd549b9b894cc39801befe9de3ee0bc57" translate="yes" xml:space="preserve">
          <source>The primal problem can be equivalently formulated as</source>
          <target state="translated">原始問題は等価的に次のように定式化することができます。</target>
        </trans-unit>
        <trans-unit id="cd0e1bbfb34ee27f92c4a3d8c327812d7edee1f8" translate="yes" xml:space="preserve">
          <source>The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as &lt;em&gt;non-generalizing&lt;/em&gt; machine learning methods, since they simply &amp;ldquo;remember&amp;rdquo; all of its training data (possibly transformed into a fast indexing structure such as a &lt;a href=&quot;#ball-tree&quot;&gt;Ball Tree&lt;/a&gt; or &lt;a href=&quot;#kd-tree&quot;&gt;KD Tree&lt;/a&gt;).</source>
          <target state="translated">最近傍法の背後にある原則は、新しい点に最も近い距離にある事前定義済みのトレーニングサンプルを見つけ、それらからラベルを予測することです。サンプル数は、ユーザー定義の定数（k最近傍学習）にすることも、点の局所密度に基づいて変化させることもできます（半径ベースの近傍学習）。距離は、一般に、任意のメートル法の尺度にすることができます。標準のユークリッド距離が最も一般的な選択です。近傍ベースの方法は、すべてのトレーニングデータを単に「記憶」しているため（&lt;a href=&quot;#ball-tree&quot;&gt;ボールツリー&lt;/a&gt;や&lt;a href=&quot;#kd-tree&quot;&gt;KDツリー&lt;/a&gt;などの高速なインデックス構造に変換される可能性があるため）、&lt;em&gt;非一般化&lt;/em&gt;機械学習方法として知られています。</target>
        </trans-unit>
        <trans-unit id="947ce8b8522668844673470c4e6efaad3bb4bafb" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel are shown in the following figure:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt;カーネルから生じるGPの前後を次の図に示します。</target>
        </trans-unit>
        <trans-unit id="43f31a8e2502a5518bdd46434b1800e86463052e" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:</source>
          <target state="translated">ExpSineSquaredカーネルから得られるGPの先行値と事後値を次の図に示します。</target>
        </trans-unit>
        <trans-unit id="f92d48b9507eee6a0b35b438f1fb3d6ff89e93fd" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart).</source>
          <target state="translated">共分散分布上の自由度数の事前分布(Wishart)。</target>
        </trans-unit>
        <trans-unit id="f920ca641ad93ad7a821ae4139b67430b9eddb8f" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart). If it is None, it&amp;rsquo;s set to &lt;code&gt;n_features&lt;/code&gt;.</source>
          <target state="translated">共分散分布の自由度の事前分布（ウィシャート）。Noneの場合、 &lt;code&gt;n_features&lt;/code&gt; に設定されます。</target>
        </trans-unit>
        <trans-unit id="cf6f1da4c11f5b6aa97c72a194e67d10417600f3" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). If it is None, the emiprical covariance prior is initialized using the covariance of X. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">共分散分布の事前分布（ウィシャート）。Noneの場合、準共分散事前分布はXの共分散を使用して初期化されます。形状は &lt;code&gt;covariance_type&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="449bf6ea1f50ae651db1aabfda8187878d697851" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">共分散分布の事前分布（ウィシャート）。形状は &lt;code&gt;covariance_type&lt;/code&gt; に依存します。</target>
        </trans-unit>
        <trans-unit id="a7d0d50e2fe2007735b69660533329d841055128" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian).</source>
          <target state="translated">平均分布(ガウス分布)の優先順位。</target>
        </trans-unit>
        <trans-unit id="330376751a07bab7a9ae7af5823384716a02e1f4" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it is set to the mean of X.</source>
          <target state="translated">平均分布(ガウス分布)の優先順位。Noneの場合はXの平均に設定されます。</target>
        </trans-unit>
        <trans-unit id="70a81456bc5946b8879a5b610e7810c1053668bd" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it&amp;rsquo;s set to the mean of X.</source>
          <target state="translated">平均分布の事前分布（ガウス）。Noneの場合は、Xの平均に設定されます。</target>
        </trans-unit>
        <trans-unit id="0c84abbb5dade5fc9d4b47758b34408cb97bc08f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian.</source>
          <target state="translated">\（\ alpha \）および\（\ lambda \）の事前&lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;分布&lt;/a&gt;は、ガウス分布の精度の共役事前分布であるガンマ分布になるように選択されます。</target>
        </trans-unit>
        <trans-unit id="4f2acfb8ac15388ba72cc76c4859e9942703930f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian. The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">\（\ alpha \）および\（\ lambda \）の事前&lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;分布&lt;/a&gt;は、ガウス分布の精度の共役事前分布であるガンマ分布になるように選択されます。結果として得られるモデルは&lt;em&gt;ベイジアンリッジ回帰&lt;/em&gt;と呼ばれ、古典的な&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;似ています。</target>
        </trans-unit>
        <trans-unit id="69c3bd38e0a5cd7a1c6a9d7a1b4382f7891ca245" translate="yes" xml:space="preserve">
          <source>The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.</source>
          <target state="translated">確率モデルはクロスバリデーションを用いて作成されているため、 predictで得られる結果とは若干異なる結果になる可能性があります。また、非常に小さなデータセットでは意味のない結果が出てしまいます。</target>
        </trans-unit>
        <trans-unit id="a6cf9fb7eddd86de2e07f31468eef83947604765" translate="yes" xml:space="preserve">
          <source>The probability of category \(t\) in feature \(i\) given class \(c\) is estimated as:</source>
          <target state="translated">the probability of category ¶(t\(t)in feature ¶(i\(i))given class ¶(c\(c))は、次のように推定されます。</target>
        </trans-unit>
        <trans-unit id="5a547056b7368b638d698d05d3f24b68fc3e86df" translate="yes" xml:space="preserve">
          <source>The probability of each class being drawn. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">描画される各クラスの確率。 &lt;code&gt;return_distributions=True&lt;/code&gt; の場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="91c6b31a68e2490c2f85bd0b221deab07bc16086" translate="yes" xml:space="preserve">
          <source>The probability of each feature being drawn given each class. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">各クラスを指定して描画される各フィーチャの確率。 &lt;code&gt;return_distributions=True&lt;/code&gt; の場合にのみ返されます。</target>
        </trans-unit>
        <trans-unit id="e0b4e863c306f0bcfef6f345210b44e3fb52ffa9" translate="yes" xml:space="preserve">
          <source>The probability that a coefficient is zero (see notes). Larger values enforce more sparsity.</source>
          <target state="translated">係数がゼロである確率(注を参照)。値が大きいほど疎分散が強くなります。</target>
        </trans-unit>
        <trans-unit id="f5e975aa46dbf2f661a2bf284eb823b950bfb65a" translate="yes" xml:space="preserve">
          <source>The problem of correlated variables</source>
          <target state="translated">相関変数の問題</target>
        </trans-unit>
        <trans-unit id="4678dc803cddccad6a08944794f2a1c194908b2a" translate="yes" xml:space="preserve">
          <source>The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.</source>
          <target state="translated">最適な決定木を学習する問題は、最適性のいくつかの側面において、また単純な概念においてもNP完全であることが知られている。その結果、実用的な決定木学習アルゴリズムは、各ノードで局所的に最適な決定を行う貪欲アルゴリズムのようなヒューリスティックアルゴリズムに基づいている。このようなアルゴリズムは、グローバルに最適な決定木を返すことを保証できません。これは、アンサンブル学習器で複数の木を学習することで緩和することができ、ここでは、特徴とサンプルは置換でランダムにサンプリングされる。</target>
        </trans-unit>
        <trans-unit id="6c819ed9fb97cd33099d0eff9842389e345641fd" translate="yes" xml:space="preserve">
          <source>The problem solved in clustering</source>
          <target state="translated">クラスタリングで解決した問題</target>
        </trans-unit>
        <trans-unit id="2c5b556d82e8f03aa8505b7b204354187717fb43" translate="yes" xml:space="preserve">
          <source>The problem solved in supervised learning</source>
          <target state="translated">教師付き学習で解決した問題</target>
        </trans-unit>
        <trans-unit id="ab484ff296e26d980b5f93762f848e40818065e8" translate="yes" xml:space="preserve">
          <source>The progress meter: the higher the value of &lt;code&gt;verbose&lt;/code&gt;, the more messages:</source>
          <target state="translated">進行状況メーター： &lt;code&gt;verbose&lt;/code&gt; の値が高いほど、メッセージが多くなります。</target>
        </trans-unit>
        <trans-unit id="4163ad4952f27df98667c245d9d8133cfc5f4bae" translate="yes" xml:space="preserve">
          <source>The project mailing list</source>
          <target state="translated">プロジェクトのメーリングリスト</target>
        </trans-unit>
        <trans-unit id="c7689a2c8d3ddf3814b537c25de9417bcceff1dc" translate="yes" xml:space="preserve">
          <source>The projected data.</source>
          <target state="translated">予測されたデータです。</target>
        </trans-unit>
        <trans-unit id="8bfacd813c2539118e4e65b3f249a31e3f90cba3" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;(n_sample + n_features + 1) / 2&lt;/code&gt;. The parameter must be in the range (0, 1).</source>
          <target state="translated">生のMCD見積もりの​​サポートに含まれるポイントの割合。デフォルトはNoneです。これは、support_fractionの最小値がアルゴリズム内で使用されることを意味します： &lt;code&gt;(n_sample + n_features + 1) / 2&lt;/code&gt; 。パラメータは（0、1）の範囲内である必要があります。</target>
        </trans-unit>
        <trans-unit id="1179732ddee68f2a030de922cca4a7f3acc9e816" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: [n_sample + n_features + 1] / 2</source>
          <target state="translated">生のMCD推定値のサポートに含まれるポイントの割合。デフォルトはNoneで、アルゴリズム内ではsupport_fractionの最小値が使用されることを意味します。n_sample+n_features+1]/2</target>
        </trans-unit>
        <trans-unit id="5f706b185c5fa578abf1be5586afd62f084c2356" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;.</source>
          <target state="translated">生のMCD推定値のサポートに含まれるポイントの割合。Noneの場合、アルゴリズム内でsupport_fractionの最小値が使用されます： &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2ecaf585d98315eb2e879bcbfec7a3d49600f7b8" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;. Range is (0, 1).</source>
          <target state="translated">生のMCD見積もりの​​サポートに含まれるポイントの割合。Noneの場合、support_fractionの最小値がアルゴリズム内で使用されます： &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt; 。範囲は（0、1）です。</target>
        </trans-unit>
        <trans-unit id="dd542c4196f5720eadadf90803a23b09f997f270" translate="yes" xml:space="preserve">
          <source>The proportion of samples whose class is the positive class, in each bin (fraction of positives).</source>
          <target state="translated">各ビンにおけるクラスが正のクラスであるサンプルの割合(陽性の割合)。</target>
        </trans-unit>
        <trans-unit id="6be8a7dea2b475115d00cec72a7cea2ea475dbdd" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;early_stopping&lt;/code&gt; is True.</source>
          <target state="translated">早期打ち切りの検証セットとして確保するトレーニングデータの割合。0から1の間でなければなりません &lt;code&gt;early_stopping&lt;/code&gt; がTrueの場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="c5ef332af0dc668a636cc813bd9c0d699622c033" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;n_iter_no_change&lt;/code&gt; is set to an integer.</source>
          <target state="translated">早期停止の検証セットとして確保しておくトレーニングデータの割合。0から1の間でなければなりません &lt;code&gt;n_iter_no_change&lt;/code&gt; が整数に設定されている場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="e7040634736e41a179ebec81405480b75c6b1afc" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True</source>
          <target state="translated">早期停止のための検証セットとして設定する学習データの割合。0 から 1 の間でなければなりません。 early_stopping が True の場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="22eea34be82cd504d8c4cf88134dfff15444db48" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.</source>
          <target state="translated">早期停止のための検証セットとして設定するトレーニングデータの割合。0から1の間でなければなりません。 early_stoppingがTrueの場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="3b21c544ac1d6b2edc3be7d45619235a28b43e4c" translate="yes" xml:space="preserve">
          <source>The proportions of samples assigned to each class. If None, then classes are balanced. Note that if &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt;, then the last class weight is automatically inferred. More than &lt;code&gt;n_samples&lt;/code&gt; samples may be returned if the sum of &lt;code&gt;weights&lt;/code&gt; exceeds 1.</source>
          <target state="translated">各クラスに割り当てられたサンプルの比率。Noneの場合、クラスはバランスされます。（注）その &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt; 、最後のクラスの重みを自動的に推測されます。以上 &lt;code&gt;n_samples&lt;/code&gt; の合計場合にサンプルが返されてもよい &lt;code&gt;weights&lt;/code&gt; 1を超えます。</target>
        </trans-unit>
        <trans-unit id="e063e4585132e12e4e3b8bc3e0e5f8f9333735ee" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of &lt;code&gt;components_&lt;/code&gt;. It is the linear operator that maps independent sources to the data.</source>
          <target state="translated">&lt;code&gt;components_&lt;/code&gt; の疑似反転。独立したソースをデータにマッピングするのは線形演算子です。</target>
        </trans-unit>
        <trans-unit id="6b5cae22ba8a7c5a2306b5698bd81974f62f5c35" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting it to &amp;lsquo;passthrough&amp;rsquo; or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">パイプラインの目的は、さまざまなパラメーターを設定しながら相互検証できるいくつかのステップを組み立てることです。このため、以下の例のように、名前と「__」で区切られたパラメーター名を使用して、さまざまなステップのパラメーターを設定できます。ステップの推定器は、その名前のパラメーターを別の推定器に設定することで完全に置き換えることができます。または、トランスフォーマーを「パススルー」または「 &lt;code&gt;None&lt;/code&gt; 」に設定することで削除できます。</target>
        </trans-unit>
        <trans-unit id="824b6a86b9524b4fdb9e2919749fc7db8e534162" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.</source>
          <target state="translated">パイプラインの目的は、さまざまなパラメーターを設定しながら、相互検証できるいくつかのステップを組み立てることです。このため、以下の例のように、名前と「__」で区切られたパラメーター名を使用して、さまざまなステップのパラメーターを設定できます。ステップの推定器は、その名前のパラメーターを別の推定器に設定することによって完全に置き換えるか、Noneに設定することによってトランスフォーマーを削除することができます。</target>
        </trans-unit>
        <trans-unit id="7aabe2d97cf9256a5a6bd2e4c38d95ce8f939f1c" translate="yes" xml:space="preserve">
          <source>The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.</source>
          <target state="translated">これら2つのランダム性の源の目的は、フォレスト推定器の分散を減少させることである。実際、個々の決定木は一般的に高い分散を示し、オーバーフィットする傾向があります。フォレストに注入されたランダム性により、予測誤差が多少分離した決定木が得られます。これらの予測の平均を取ることで、いくつかの誤差が相殺されます。ランダムフォレストでは、多様な木を組み合わせることで分散を低減しますが、その代償としてバイアスが若干増加することがあります。実際には、分散の減少はしばしば顕著であり、その結果、全体的に優れたモデルが得られます。</target>
        </trans-unit>
        <trans-unit id="cd8f7005fb1918c1319c0b334e68b25127992a8d" translate="yes" xml:space="preserve">
          <source>The python source code used to generate the model</source>
          <target state="translated">モデルの生成に使用した python のソースコード</target>
        </trans-unit>
        <trans-unit id="a971c90794b3ab81f6079922073a4f8be4eaa91d" translate="yes" xml:space="preserve">
          <source>The qualitative difference between these models can also be visualized by comparing the histogram of observed target values with that of predicted values:</source>
          <target state="translated">これらのモデル間の質的な違いは、観測された目標値のヒストグラムと予測値のヒストグラムを比較することによっても可視化することができます。</target>
        </trans-unit>
        <trans-unit id="f7347085b6a9f16c7df450dca9dbc878bc14f5cc" translate="yes" xml:space="preserve">
          <source>The quantile to predict using the &amp;ldquo;quantile&amp;rdquo; strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.</source>
          <target state="translated">「分位」戦略を使用して予測する分位。分位点0.5は中央値に対応し、0.0は最小値、1.0は最大値に対応します。</target>
        </trans-unit>
        <trans-unit id="eabfb5a10c049cb3056c46f05114ad3228125c8e" translate="yes" xml:space="preserve">
          <source>The quantity \(\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}\) is the derivative of the loss with respect to its second parameter, evaluated at \(F_{m-1}(x)\). It is easy to compute for any given \(F_{m - 1}(x_i)\) in a closed form since the loss is differentiable. We will denote it by \(g_i\).</source>
          <target state="translated">量の\(左:\frac{\partial l(y_i,F(x_i))}{\partial F(x_i)}}right]_{F=F_{m-1}}}は、2番目のパラメータに対する損失の導関数である。損失は微分可能なので,閉じた形で計算するのは簡単です。ここでは、それを \(g_i\)と呼ぶことにします。</target>
        </trans-unit>
        <trans-unit id="38be089aaf53753add8a6e12d9d6e104537de3bf" translate="yes" xml:space="preserve">
          <source>The quantity that we use is the daily variation in quote price: quotes that are linked tend to cofluctuate during a day.</source>
          <target state="translated">使用する量は、気配値の日次変動:連動する気配値が1日の間にコフルクトする傾向があります。</target>
        </trans-unit>
        <trans-unit id="96cf03fa14346bfc08bd6f97110fef23b56f72d1" translate="yes" xml:space="preserve">
          <source>The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.</source>
          <target state="translated">クエリ点または点。提供されない場合は、インデックス化された各点の隣人が返されます。この場合、問い合わせ点はそれ自身の隣人とはみなされません。</target>
        </trans-unit>
        <trans-unit id="30a7cdee7bb9697635b16e9d03b7cdd4b400cabd" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. the training samples.</source>
          <target state="translated">学習サンプルに関連して局所外れ値因子を計算するためのクエリサンプルまたはサンプル.</target>
        </trans-unit>
        <trans-unit id="e9edc4411fbea590103c5860156a749b07db92a2" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. to the training samples.</source>
          <target state="translated">局所外れ値因子を計算するためのクエリサンプルまたはサンプル.</target>
        </trans-unit>
        <trans-unit id="2f63e59d1f59e9a5989ccb3ba903c403d9c311f4" translate="yes" xml:space="preserve">
          <source>The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.</source>
          <target state="translated">新しいサンプルと最も近いサブクラスターをマージして得られたサブクラスターの半径は、しきい値よりも小さくなければなりません。そうでなければ、新しいサブクラスターが開始されます。この値を非常に低く設定すると分割が促進され,逆に分割が促進されます.</target>
        </trans-unit>
        <trans-unit id="c1fb03bbf68de1d40f0cd10af0722ed019408483" translate="yes" xml:space="preserve">
          <source>The random forest regressor will only ever predict values within the range of observations or closer to zero for each of the targets. As a result the predictions are biased towards the centre of the circle.</source>
          <target state="translated">ランダム・フォレスト回帰器は、オブザベーションの範囲内の値か、ターゲットのそれぞれについてゼロに近い値だけを予測します。その結果、予測は円の中心に偏ります。</target>
        </trans-unit>
        <trans-unit id="ff0bc6a79a727353502babbe6e55a993a0f80508" translate="yes" xml:space="preserve">
          <source>The random number generator is used to generate random chain orders.</source>
          <target state="translated">乱数発生器は、ランダムチェーンオーダーを発生させるために使用されます。</target>
        </trans-unit>
        <trans-unit id="31617e37a4673ca35baf50a5963330e598289ccc" translate="yes" xml:space="preserve">
          <source>The random symmetric, positive-definite matrix.</source>
          <target state="translated">ランダムな対称正定値行列。</target>
        </trans-unit>
        <trans-unit id="0ae670050b82bcbccc27a159ae39c91afa76e218" translate="yes" xml:space="preserve">
          <source>The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.</source>
          <target state="translated">ランダム探索とグリッド探索は、全く同じパラメータ空間を探索します。パラメータ設定の結果は非常に似ていますが、ランダム化探索の方が実行時間が大幅に短くなっています。</target>
        </trans-unit>
        <trans-unit id="6f118fa702c125f64290c65f38abc4f6dddff279" translate="yes" xml:space="preserve">
          <source>The raw (unadjusted) Rand index is then given by:</source>
          <target state="translated">そして、生の(調整されていない)ランド指数は次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="6b2fe9bd420d4a6948923a1a40e37c6224028547" translate="yes" xml:space="preserve">
          <source>The raw RI score is then &amp;ldquo;adjusted for chance&amp;rdquo; into the ARI score using the following scheme:</source>
          <target state="translated">次に、次のスキームを使用して、未加工のRIスコアがARIスコアに「調整」されます。</target>
        </trans-unit>
        <trans-unit id="c2c7c9eff2c5366f7dc8bd26b18e5e786e16eeff" translate="yes" xml:space="preserve">
          <source>The raw image data.</source>
          <target state="translated">生の画像データです。</target>
        </trans-unit>
        <trans-unit id="31fb3fd5063abf3dc0c0004645bb280ed7e9b6d1" translate="yes" xml:space="preserve">
          <source>The raw predicted values (i.e. the sum of the trees leaves) for each sample. n_trees_per_iteration is equal to the number of classes in multiclass classification.</source>
          <target state="translated">n_trees_per_iteration はマルチクラス分類のクラス数に等しい。</target>
        </trans-unit>
        <trans-unit id="884b3f9d013732b636db9c9b452d7d3559d50f2d" translate="yes" xml:space="preserve">
          <source>The raw robust estimated covariance before correction and re-weighting.</source>
          <target state="translated">補正および再加重前の生のロバスト推定共分散。</target>
        </trans-unit>
        <trans-unit id="70b59f0ad9598471a02599d6a34b12e103ef557b" translate="yes" xml:space="preserve">
          <source>The raw robust estimated location before correction and re-weighting.</source>
          <target state="translated">補正・再加重前の生のロバスト推定位置。</target>
        </trans-unit>
        <trans-unit id="246acb046d6b9bd62c3702633fac1169a8d836d0" translate="yes" xml:space="preserve">
          <source>The real data lies in the &lt;code&gt;filenames&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; attributes. The target attribute is the integer index of the category:</source>
          <target state="translated">実際のデータは、 &lt;code&gt;filenames&lt;/code&gt; と &lt;code&gt;target&lt;/code&gt; 属性にあります。ターゲット属性は、カテゴリの整数インデックスです。</target>
        </trans-unit>
        <trans-unit id="480e842bb5c6e42d98f06aa4f6c096c0c7aba356" translate="yes" xml:space="preserve">
          <source>The recall is the ratio &lt;code&gt;tp / (tp + fn)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fn&lt;/code&gt; the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</source>
          <target state="translated">リコールは、比率である &lt;code&gt;tp / (tp + fn)&lt;/code&gt; ここで、 &lt;code&gt;tp&lt;/code&gt; 真陽性の数であり、 &lt;code&gt;fn&lt;/code&gt; の偽陰性の数。再現率は直感的に、すべての陽性サンプルを見つける分類子の能力です。</target>
        </trans-unit>
        <trans-unit id="6f99b6066a1cdf81f95a9e89b923c52bb3877c2c" translate="yes" xml:space="preserve">
          <source>The reconstructed image.</source>
          <target state="translated">再構成された画像です。</target>
        </trans-unit>
        <trans-unit id="65f6a86aee18eed07b01f195c73d746d5f2ca909" translate="yes" xml:space="preserve">
          <source>The reconstructed points using the metric MDS and non metric MDS are slightly shifted to avoid overlapping.</source>
          <target state="translated">メトリックデータシートと非メトリックデータシートを用いて再構成された点は、重複を避けるためにわずかにシフトされています。</target>
        </trans-unit>
        <trans-unit id="cefc093c3489c62b159a3ce090f6d8b10ecaa3b1" translate="yes" xml:space="preserve">
          <source>The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a \(d\)-dimensional manifold embedded in a \(D\)-dimensional parameter space, the reconstruction error will decrease as &lt;code&gt;n_components&lt;/code&gt; is increased until &lt;code&gt;n_components == d&lt;/code&gt;.</source>
          <target state="translated">各ルーチンで計算された再構成誤差を使用して、最適な出力次元を選択できます。\（D \）に埋め込まれた次元のマニホールド- - \（D \）のような次元のパラメータ空間、再構成誤差が減少する &lt;code&gt;n_components&lt;/code&gt; にまで増加さ &lt;code&gt;n_components == d&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cd5de02ec688b7a05331073a7d6e7032a5c96c24" translate="yes" xml:space="preserve">
          <source>The reconstruction with L1 penalization gives a result with zero error (all pixels are successfully labeled with 0 or 1), even if noise was added to the projections. In comparison, an L2 penalization (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt;&lt;/a&gt;) produces a large number of labeling errors for the pixels. Important artifacts are observed on the reconstructed image, contrary to the L1 penalization. Note in particular the circular artifact separating the pixels in the corners, that have contributed to fewer projections than the central disk.</source>
          <target state="translated">L1ペナルティによる再構成では、ノイズが投影に追加された場合でも、エラーがゼロ（すべてのピクセルが0または1で正常にラベル付けされる）の結果が得られます。比較すると、L2 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; &lt;/a&gt;（sklearn.linear_model.Ridge）は、ピクセルに多数のラベリングエラーを生成します。L1ペナルティとは対照的に、再構成された画像には重要なアーティファクトが見られます。特に、中央のディスクよりも少ない投影に寄与している、コーナーのピクセルを分離する円形のアーティファクトに注意してください。</target>
        </trans-unit>
        <trans-unit id="cc89055f829a16401789a0501b3aaaa652548713" translate="yes" xml:space="preserve">
          <source>The reduced distance, defined for some metrics, is a computationally more efficient measure which preserves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is the squared-euclidean distance.</source>
          <target state="translated">軽減された距離は、いくつかのメトリックで定義され、真の距離のランクを保持する計算効率の高い尺度です。例えば、ユークリッド距離メトリックでは、縮小された距離は2乗ユークリッド距離です。</target>
        </trans-unit>
        <trans-unit id="d3411437239f8f2dc8ee2706670c4e4a91db1791" translate="yes" xml:space="preserve">
          <source>The reduced samples.</source>
          <target state="translated">縮小されたサンプル。</target>
        </trans-unit>
        <trans-unit id="67c2217cec61f41257c896a393db0ce694f3f635" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;GridSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">再 &lt;code&gt;best_estimator_&lt;/code&gt; された推定量は、best_estimator_属性で利用可能になり、この &lt;code&gt;GridSearchCV&lt;/code&gt; インスタンスで直接 &lt;code&gt;predict&lt;/code&gt; を使用できるようになります。</target>
        </trans-unit>
        <trans-unit id="1178e040e21448dfc3d87635a64add70ee2fc71f" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;RandomizedSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">再 &lt;code&gt;best_estimator_&lt;/code&gt; された推定器は、best_estimator_属性で使用可能になり、この &lt;code&gt;RandomizedSearchCV&lt;/code&gt; インスタンスで直接 &lt;code&gt;predict&lt;/code&gt; を使用できるようになります。</target>
        </trans-unit>
        <trans-unit id="d35d9dbef92f31bfc55b28e05410d9f2634f5c0d" translate="yes" xml:space="preserve">
          <source>The regression target for each sample.</source>
          <target state="translated">各サンプルの回帰目標。</target>
        </trans-unit>
        <trans-unit id="eb2b5c40285ce2a0d935c8174b2b1df487e2e554" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical.</source>
          <target state="translated">該当する場合は、回帰対象または分類ラベル。Dtypeは数値の場合はfloat、カテゴリカルな場合はobjectです。</target>
        </trans-unit>
        <trans-unit id="f49726f7b2ee9ab1b3928455718be0471fabc15a" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical. If &lt;code&gt;as_frame&lt;/code&gt; is True, &lt;code&gt;target&lt;/code&gt; is a pandas object.</source>
          <target state="translated">該当する場合、回帰ターゲットまたは分類ラベル。Dtypeは、数値の場合はfloatであり、カテゴリの場合はobjectです。 &lt;code&gt;as_frame&lt;/code&gt; がTrueの場合、 &lt;code&gt;target&lt;/code&gt; はpandasオブジェクトです。</target>
        </trans-unit>
        <trans-unit id="3833ac2bfe2b88ee813e1c7e696adf8c3874fe96" translate="yes" xml:space="preserve">
          <source>The regression target.</source>
          <target state="translated">回帰目標。</target>
        </trans-unit>
        <trans-unit id="b42b60d45f4e29bdab4ac89a5e2270a5774add83" translate="yes" xml:space="preserve">
          <source>The regression target. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas Series.</source>
          <target state="translated">回帰ターゲット。 &lt;code&gt;as_frame=True&lt;/code&gt; の場合、 &lt;code&gt;target&lt;/code&gt; はパンダシリーズになります。</target>
        </trans-unit>
        <trans-unit id="c39a60b37913113f18fb812e5aaafb85e77ce6aa" translate="yes" xml:space="preserve">
          <source>The regression targets. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="translated">回帰ターゲット。 &lt;code&gt;as_frame=True&lt;/code&gt; の場合、 &lt;code&gt;target&lt;/code&gt; はpandasDataFrameになります。</target>
        </trans-unit>
        <trans-unit id="444eaf2365e5ec07e25a0ed19fde31ede0366773" translate="yes" xml:space="preserve">
          <source>The regressor is used to predict and the &lt;code&gt;inverse_func&lt;/code&gt; or &lt;code&gt;inverse_transform&lt;/code&gt; is applied before returning the prediction.</source>
          <target state="translated">&lt;code&gt;inverse_transform&lt;/code&gt; は予測に使用され、 &lt;code&gt;inverse_func&lt;/code&gt; またはreverse_transformは予測を返す前に適用されます。</target>
        </trans-unit>
        <trans-unit id="edcb36e5cf0943282d84be3c17128a4f09595f3b" translate="yes" xml:space="preserve">
          <source>The regressor that is used for calibration depends on the &lt;code&gt;method&lt;/code&gt; parameter. &lt;code&gt;'sigmoid'&lt;/code&gt; corresponds to a parametric approach based on Platt&amp;rsquo;s logistic model &lt;a href=&quot;#id8&quot; id=&quot;id4&quot;&gt;3&lt;/a&gt;, i.e. \(p(y_i = 1 | f_i)\) is modeled as \(\sigma(A f_i + B)\) where \(\sigma\) is the logistic function, and \(A\) and \(B\) are real numbers to be determined when fitting the regressor via maximum likelihood. &lt;code&gt;'isotonic'&lt;/code&gt; will instead fit a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function (see &lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt;&lt;code&gt;sklearn.isotonic&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">キャリブレーションに使用されるリグレッサは、 &lt;code&gt;method&lt;/code&gt; パラメータによって異なります。 &lt;code&gt;'sigmoid'&lt;/code&gt; は、Plattのロジスティックモデル&lt;a href=&quot;#id8&quot; id=&quot;id4&quot;&gt;3&lt;/a&gt;に基づくパラメトリックアプローチに対応します。つまり、\（p（y_i = 1 | f_i）\）は\（\ sigma（A f_i + B）\）としてモデル化されます。ここで\（\ sigma \）はロジスティック関数であり、\（A \）と\（B \）は、最尤法でリグレッサを近似するときに決定される実数です。 &lt;code&gt;'isotonic'&lt;/code&gt; は、代わりにノンパラメトリック等張回帰子に適合し、段階的な非減少関数を出力します（&lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt; &lt;code&gt;sklearn.isotonic&lt;/code&gt; を&lt;/a&gt;参照）。</target>
        </trans-unit>
        <trans-unit id="2ab5d35ab966f753794908f22950ec4d3c66e8fc" translate="yes" xml:space="preserve">
          <source>The regressor to stacked the base estimators fitted.</source>
          <target state="translated">フィットした基底推定値を積み重ねるための回帰器。</target>
        </trans-unit>
        <trans-unit id="4f20e2edcb6e4e7d6bdbe8de69b9b24c7725d7e1" translate="yes" xml:space="preserve">
          <source>The regularised covariance is:</source>
          <target state="translated">正則化共分散は</target>
        </trans-unit>
        <trans-unit id="4dfecc4cc3d2dd0e68757701d8e0aef7bde77c4d" translate="yes" xml:space="preserve">
          <source>The regularization mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 &amp;lt; l1_ratio &amp;lt; 1, the penalty is a combination of L1 and L2.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1の正則化混合パラメーター。l1_ratio= 0の場合、ペナルティは要素ごとのL2ペナルティ（別名フロベニウスノルム）です。l1_ratio = 1の場合、要素ごとのL1ペナルティです。0 &amp;lt;l1_ratio &amp;lt;1の場合、ペナルティはL1とL2の組み合わせです。</target>
        </trans-unit>
        <trans-unit id="f1711bca786b8ddb98e81cd17b706bc6925eee44" translate="yes" xml:space="preserve">
          <source>The regularization parameter C in the LogisticRegression. When C is an array, fit will take each regularization parameter in C one by one for LogisticRegression and store results for each one in &lt;code&gt;all_scores_&lt;/code&gt;, where columns and rows represent corresponding reg_parameters and features.</source>
          <target state="translated">LogisticRegressionの正則化パラメーターC。Cが配列の場合、fitはCの各正則化パラメーターを1つずつLogisticRegressionに &lt;code&gt;all_scores_&lt;/code&gt; 、それぞれの結果をall_scores_に格納します。列と行は対応するreg_parametersと機能を表します。</target>
        </trans-unit>
        <trans-unit id="7865779166b38ce9dcfe2e41f3eba5295cbd3874" translate="yes" xml:space="preserve">
          <source>The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling.</source>
          <target state="translated">Lassoの正則化パラメータαパラメータです。警告:これは安定性選択記事のαパラメータではなく、スケーリングです。</target>
        </trans-unit>
        <trans-unit id="0ed37f5cbd4d43ac0f2b6fe6eac86f942c91256a" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance.</source>
          <target state="translated">正則化パラメータ:αが高いほど正則化が進み、逆共分散が疎かになります。</target>
        </trans-unit>
        <trans-unit id="e124bf83498b6636f4567895eaf37c151f534f9f" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. Range is (0, inf].</source>
          <target state="translated">正則化パラメータ:アルファが高いほど正則化が進み,逆共分散がより鮮明になります.範囲は (0,inf]です.</target>
        </trans-unit>
        <trans-unit id="688891a69c745428fa6d147d1a8f3e33c50a1bf6" translate="yes" xml:space="preserve">
          <source>The regularization reduces the influence of correlated variables on the model because the weight is shared between the two predictive variables, so neither alone would have strong weights.</source>
          <target state="translated">正則化は,重みが2つの予測変数の間で共有されるので,モデル上の相関変数の影響を軽減する.</target>
        </trans-unit>
        <trans-unit id="7a1b8c0c02e4613adc42b58f49402bda71930b51" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is given by:</source>
          <target state="translated">正則化された(縮小された)共分散は次式で与えられます。</target>
        </trans-unit>
        <trans-unit id="0a5a9291b6a07681a32efc9a88d6f2d7eab96435" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is:</source>
          <target state="translated">正則化された(縮小された)共分散は</target>
        </trans-unit>
        <trans-unit id="5061099398d935daccb5dfd0421b959c5f17fce1" translate="yes" xml:space="preserve">
          <source>The regularized covariance is given by:</source>
          <target state="translated">正則化された共分散は次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="bba73ee876f52c89494b029f9c7d24e1239abc01" translate="yes" xml:space="preserve">
          <source>The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.</source>
          <target state="translated">レギュラライザは、二乗ユークリッドノルムL2または絶対ノルムL1、またはその両方の組み合わせ(Elastic Net)を使用して、モデルパラメータをゼロベクトルに向かって縮小する損失関数に追加されるペナルティです。レギュラライザーのためにパラメータの更新が0.0を超えると、更新は0.0に切り捨てられ、疎なモデルの学習とオンライン特徴選択が可能になります。</target>
        </trans-unit>
        <trans-unit id="dccfd8ff5de1af25843574f310f33b70fd7f2fcc" translate="yes" xml:space="preserve">
          <source>The relationship between recall and precision can be observed in the stairstep area of the plot - at the edges of these steps a small change in the threshold considerably reduces precision, with only a minor gain in recall.</source>
          <target state="translated">リコールと精度の関係は、プロットの階段状の領域で観察することができます-これらのステップの端では、しきい値の小さな変化が精度を大幅に低下させ、リコールはわずかに増加するだけです。</target>
        </trans-unit>
        <trans-unit id="e4d930448408dd13aba80cfbcc12949bce572769" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile if &lt;code&gt;effective_rank&lt;/code&gt; is not None.</source>
          <target state="translated">&lt;code&gt;effective_rank&lt;/code&gt; がNoneでない場合の特異値プロファイルの太いノイズの多いテールの相対的な重要性。</target>
        </trans-unit>
        <trans-unit id="5b4ee2ad411363b437ce1738486767191903cc20" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile.</source>
          <target state="translated">特異値プロファイルの太いノイジーテールの相対的な重要性。</target>
        </trans-unit>
        <trans-unit id="1bf8ab4629789502195bd390130a1b2aa7e78e4e" translate="yes" xml:space="preserve">
          <source>The relative increment in the results before declaring convergence.</source>
          <target state="translated">収束を宣言する前の結果の相対的な増分。</target>
        </trans-unit>
        <trans-unit id="ae073568b2b2b27a72266212b55fc2cb2d4cd169" translate="yes" xml:space="preserve">
          <source>The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The &lt;strong&gt;expected fraction of the samples&lt;/strong&gt; they contribute to can thus be used as an estimate of the &lt;strong&gt;relative importance of the features&lt;/strong&gt;. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.</source>
          <target state="translated">ツリーの決定ノードとして使用される特徴の相対ランク（つまり、深さ）を使用して、ターゲット変数の予測可能性に関するその特徴の相対的重要度を評価できます。ツリーの最上部で使用される機能は、入力サンプルの大部分の最終予測決定に役立ちます。したがって&lt;strong&gt;、サンプル&lt;/strong&gt;が寄与する&lt;strong&gt;サンプル&lt;/strong&gt;の&lt;strong&gt;予想される割合&lt;/strong&gt;&lt;strong&gt;は、特徴の相対的重要度の&lt;/strong&gt;推定値として使用でき&lt;strong&gt;ます&lt;/strong&gt;。scikit-learnでは、機能が寄与するサンプルの割合と、それらを分割することによる不純物の減少を組み合わせて、その機能の予測力の正規化された推定値を作成します。</target>
        </trans-unit>
        <trans-unit id="81bcb12f84ef974910f69bd2abe477a2b6a71621" translate="yes" xml:space="preserve">
          <source>The remaining columns can be used to predict the frequency of claim events. Those columns are very heterogeneous with a mix of categorical and numeric variables with different scales, possibly very unevenly distributed.</source>
          <target state="translated">残りの列は、クレーム・イベントの頻度を予測するために使用できます。これらの列は、異なるスケールを持つカテゴリカル変数と数値変数が混在しており、非常に不均一に分布している可能性があります。</target>
        </trans-unit>
        <trans-unit id="0fc6d12d2c584bef7c5a793374c798219700e42f" translate="yes" xml:space="preserve">
          <source>The remaining singular values&amp;rsquo; tail is fat, decreasing as:</source>
          <target state="translated">残りの特異値のテールは太く、次のように減少します。</target>
        </trans-unit>
        <trans-unit id="ec1c6c51e9847818ba009f4ace94e61e7f83ab56" translate="yes" xml:space="preserve">
          <source>The reported averages include macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label), and sample average (only for multilabel classification). Micro average (averaging the total true positives, false negatives and false positives) is only shown for multi-label or multi-class with a subset of classes, because it corresponds to accuracy otherwise. See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="translated">報告される平均には、マクロ平均（ラベルごとの非加重平均の平均）、加重平均（ラベルごとのサポート加重平均の平均）、およびサンプル平均（マルチラベル分類のみ）が含まれます。マイクロアベレージ（真陽性、偽陰性、偽陽性の合計の平均）は、マルチラベルまたはクラスのサブセットを持つマルチクラスに対してのみ表示されます。それ以外の場合は精度に対応するためです。平均の詳細については、&lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="006806051caab55f94063393a09eaea2afaeb022" translate="yes" xml:space="preserve">
          <source>The reported averages include micro average (averaging the total true positives, false negatives and false positives), macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label) and sample average (only for multilabel classification). See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="translated">報告された平均には、ミクロ平均（真陽性、偽陰性、偽陽性の合計の平均）、マクロ平均（ラベルごとの加重平均の平均）、加重平均（ラベルごとのサポート加重平均の平均）、およびサンプル平均（マルチラベルの場合のみ）が含まれます分類）。平均の詳細については、&lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt;も参照してください。</target>
        </trans-unit>
        <trans-unit id="8f7b2580f38f68e2972536327271d77d45324cd3" translate="yes" xml:space="preserve">
          <source>The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.</source>
          <target state="translated">X(Xk+1)ブロックの残差行列は、現在のXスコア:x_scoureに対するデフレによって得られる。</target>
        </trans-unit>
        <trans-unit id="18a2377c2d81e0ebb00644fcb1d33204e8d98249" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented.</source>
          <target state="translated">Y(Yk+1)ブロックの残差行列は、現在のXスコアに対するデフレによって得られる。これにより、PLS2として知られるPLS回帰が実行される。このモードは予測指向である。</target>
        </trans-unit>
        <trans-unit id="953a5ab9533846fbfb5f76815143b2efa25824fa" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score.</source>
          <target state="translated">Y(Yk+1)ブロックの残差行列は、現在のYスコアにデフレをかけて得られる。</target>
        </trans-unit>
        <trans-unit id="4786c768ceb1dac6ca78b9b9b4bd8e26183b7ef9" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.</source>
          <target state="translated">Y (Yk+1)ブロックの残差行列は、現在の Y スコアでのデフレによって得られる。これはPLS回帰の正準対称版を実行する。しかし、CCAとはわずかに異なる。これは主にモデリングに用いられます。</target>
        </trans-unit>
        <trans-unit id="f3c685007c5e417136c9d43a236103e7a7414e81" translate="yes" xml:space="preserve">
          <source>The result is quite similar to the non-normalized case.</source>
          <target state="translated">結果は正規化されていない場合とかなり似ています。</target>
        </trans-unit>
        <trans-unit id="3050c2e87895e094a17bdbd4ce41119b71fa1e6b" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; is much more strongly biased: the difference is reminiscent of the local intensity value of the original image.</source>
          <target state="translated">&lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;最小角度回帰&lt;/a&gt;の結果ははるかに強く偏っています。その差は、元の画像の局所的な強度値を連想させます。</target>
        </trans-unit>
        <trans-unit id="2844a2c76b7226f0533d494b8e60503f59b9c4e8" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; may be different from those obtained using &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; as the elements are grouped in different ways. The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; takes an average over cross-validation folds, whereas &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; is not an appropriate measure of generalisation error.</source>
          <target state="translated">結果&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; を&lt;/a&gt;使用して得られたものとは異なっていてもよい&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; の&lt;/a&gt;要素が異なる方法でグループ化されているように。関数&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; は&lt;/a&gt;に対し、交差検定フォールドの平均をとる&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; は、&lt;/a&gt;単にいくつかの異なるモデルからの標識（又は確率）は平凡返します。したがって、&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt;は、汎化誤差の適切な尺度ではありません。</target>
        </trans-unit>
        <trans-unit id="a51791cdd2f13d5121648ef37a39961811acb402" translate="yes" xml:space="preserve">
          <source>The result of calling &lt;code&gt;fit&lt;/code&gt; on a &lt;code&gt;GridSearchCV&lt;/code&gt; object is a classifier that we can use to &lt;code&gt;predict&lt;/code&gt;:</source>
          <target state="translated">呼び出しの結果 &lt;code&gt;fit&lt;/code&gt; 上 &lt;code&gt;GridSearchCV&lt;/code&gt; のオブジェクトは、我々がするために使用できる分類器である &lt;code&gt;predict&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="88a0848975f5bdc5b1a985f762b1245644b8930f" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to &lt;code&gt;np.diag(self(X))&lt;/code&gt;; however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="translated">このメソッドの結果は、 &lt;code&gt;np.diag(self(X))&lt;/code&gt; と同じです。ただし、対角線のみが評価されるため、より効率的に評価できます。</target>
        </trans-unit>
        <trans-unit id="b0c66fbb583b621a3ed191db16f52e8d9fa96072" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="translated">この方法の結果はnp.diag(self(X))と同じですが、対角線のみを評価するので、より効率的に評価できます。</target>
        </trans-unit>
        <trans-unit id="629d23e2107cca0c4a5d2061641bb34723082f2c" translate="yes" xml:space="preserve">
          <source>The result points are &lt;em&gt;not&lt;/em&gt; necessarily sorted by distance to their query point.</source>
          <target state="translated">結果ポイントは、クエリポイントまでの距離でソートされるとは&lt;em&gt;限りません&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="97bd456bf5cbdca967559f8f3d5ed5bef7911efc" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabasz score.</source>
          <target state="translated">結果的にカリンスキー・ハラバースのスコア。</target>
        </trans-unit>
        <trans-unit id="dc861dceeba49e88611e2b04a0b3ec8cf37af89c" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabaz score.</source>
          <target state="translated">結果的にカリンスキー-ハラバズのスコア。</target>
        </trans-unit>
        <trans-unit id="91d97654f948931244cdd794d37fd9ac58fe871c" translate="yes" xml:space="preserve">
          <source>The resulting Davies-Bouldin score.</source>
          <target state="translated">結果としてのデイヴィス=ボルダンのスコア。</target>
        </trans-unit>
        <trans-unit id="cfd8b506a89d0c11900fefa11e6003ff64d7a508" translate="yes" xml:space="preserve">
          <source>The resulting Fowlkes-Mallows score.</source>
          <target state="translated">結果的に得られたファウルクス・マロウズのスコア。</target>
        </trans-unit>
        <trans-unit id="82343548ff27f39a450415f81d355404a35e8f50" translate="yes" xml:space="preserve">
          <source>The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.</source>
          <target state="translated">各行と各列は正確に1つの双クラスタに属しているため、結果として得られる双クラスタ構造はブロック対角です。</target>
        </trans-unit>
        <trans-unit id="a10cedad8ec8047b7f7badb13dd314c46b9b1d6c" translate="yes" xml:space="preserve">
          <source>The resulting counts are normalized using &lt;a href=&quot;sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;sklearn.preprocessing.normalize&lt;/code&gt;&lt;/a&gt; unless normalize is set to False.</source>
          <target state="translated">正規化がFalseに設定されていない限り、結果のカウントは&lt;a href=&quot;sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt; &lt;code&gt;sklearn.preprocessing.normalize&lt;/code&gt; &lt;/a&gt;を使用して正規化されます。</target>
        </trans-unit>
        <trans-unit id="af7844c7ade28bb6e966130c36de65d0b613a4b9" translate="yes" xml:space="preserve">
          <source>The resulting dataset contains ordinal attributes which can be further used in a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">結果のデータセットには、&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; で&lt;/a&gt;さらに使用できる序数属性が含まれています。</target>
        </trans-unit>
        <trans-unit id="2b9ee0d5d80ffdad0cbeed5368095411cec46727" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_exp(X, Y) = k(X, Y) ** exponent</source>
          <target state="translated">結果として得られるカーネルは k_exp(X,Y)=k(X,Y)**指数として定義されます。</target>
        </trans-unit>
        <trans-unit id="5f2e205585c93945d55d6e2cae73c2d9f60e4b99" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_prod(X, Y) = k1(X, Y) * k2(X, Y)</source>
          <target state="translated">結果として得られるカーネルは k_prod(X,Y)=k1(X,Y)*k2(X,Y)と定義されます。</target>
        </trans-unit>
        <trans-unit id="65c4e9abf2f65b5e239efef34833d44fb903de78" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_sum(X, Y) = k1(X, Y) + k2(X, Y)</source>
          <target state="translated">結果として得られるカーネルは k_sum(X,Y)=k1(X,Y)+k2(X,Y)と定義されます。</target>
        </trans-unit>
        <trans-unit id="fb9ca9651a528caa2f6deb96ee39233de0fa48d4" translate="yes" xml:space="preserve">
          <source>The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;. The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. The parameters are estimated by maximizing the &lt;em&gt;marginal log likelihood&lt;/em&gt;.</source>
          <target state="translated">結果のモデルは&lt;em&gt;ベイジアンリッジ回帰&lt;/em&gt;と呼ばれ、従来の&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;似ています。パラメータ\（w \）、\（\ alpha \）、および\（\ lambda \）は、モデルの適合中に一緒に推定されます。残りのハイパーパラメーターは、\（\ alpha \）と\（\ lambda \）に対するガンマ事前分布のパラメーターです。これらは通常、&lt;em&gt;情報を提供&lt;/em&gt;しないように選択されます。パラメータは、&lt;em&gt;限界対数尤度を&lt;/em&gt;最大化することによって推定され&lt;em&gt;ます&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="2ff5ad65586245c919e0a8e1ef11c4948b5606a5" translate="yes" xml:space="preserve">
          <source>The resulting patches are allocated in a dedicated array.</source>
          <target state="translated">結果として得られたパッチは、専用の配列に割り当てられます。</target>
        </trans-unit>
        <trans-unit id="64dfac2f5dc0c167f2eb731c8522fdbbf80022e7" translate="yes" xml:space="preserve">
          <source>The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data.</source>
          <target state="translated">結果として得られた変換器は、その後、データの教師付き、疎な、高次元のカテゴリ埋め込みを学習します。</target>
        </trans-unit>
        <trans-unit id="52f668ebefc79801d78ca36ee52d3c9f5a955a8d" translate="yes" xml:space="preserve">
          <source>The results from OPTICS &lt;code&gt;cluster_optics_dbscan&lt;/code&gt; method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise.</source>
          <target state="translated">OPTICS &lt;code&gt;cluster_optics_dbscan&lt;/code&gt; メソッドとDBSCANの結果は非常に似ていますが、常に同じであるとは限りません。具体的には、周辺およびノイズポイントのラベル付け。これは、OPTICSによって処理された各密集領域の最初のサンプルは、到達可能性の値が大きく、その領域の他のポイントに近いため、周辺ではなくノイズとしてマークされる場合があるためです。これは、隣接するポイントが周辺またはノイズとしてマークされる候補と見なされる場合に影響します。</target>
        </trans-unit>
        <trans-unit id="3f9dd224b05fada5edc12dcb7a4c8d5421d61c2d" translate="yes" xml:space="preserve">
          <source>The return value is a cross-validator which generates the train/test splits via the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">戻り値は、 &lt;code&gt;split&lt;/code&gt; メソッドを介してトレイン/テストスプリットを生成するクロスバリデーターです。</target>
        </trans-unit>
        <trans-unit id="63a7df0fd8542a7b486adfe1466de16955c3587a" translate="yes" xml:space="preserve">
          <source>The returned dataset is a &lt;code&gt;scikit-learn&lt;/code&gt; &amp;ldquo;bunch&amp;rdquo;: a simple holder object with fields that can be both accessed as python &lt;code&gt;dict&lt;/code&gt; keys or &lt;code&gt;object&lt;/code&gt; attributes for convenience, for instance the &lt;code&gt;target_names&lt;/code&gt; holds the list of the requested category names:</source>
          <target state="translated">返されるデータセットは &lt;code&gt;scikit-learn&lt;/code&gt; の「束」です &lt;code&gt;dict&lt;/code&gt; キーまたは &lt;code&gt;object&lt;/code&gt; 属性としてアクセスできるフィールドを持つ単純なホルダーオブジェクトです。たとえば、 &lt;code&gt;target_names&lt;/code&gt; はリクエストされたカテゴリ名のリストを保持しています。</target>
        </trans-unit>
        <trans-unit id="c5d262a3b65ccb350b8a1d10b0eaf6eba41fc62d" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by label of classes.</source>
          <target state="translated">返された全クラスの推定値は、クラスのラベル順に並べられています。</target>
        </trans-unit>
        <trans-unit id="8520c10c8edb7f58383ef36900c902f5398bf785" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by the label of classes.</source>
          <target state="translated">返された全クラスの推定値は、クラスのラベル順に並べられています。</target>
        </trans-unit>
        <trans-unit id="9fa1a308376d0d44aa58fe9b54fd102c1f0b956a" translate="yes" xml:space="preserve">
          <source>The returned object is a MemorizedFunc object, that is callable (behaves like a function), but offers extra methods for cache lookup and management. See the documentation for &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt;&lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返されるオブジェクトは、呼び出し可能な（関数のように動作する）MemorizedFuncオブジェクトですが、キャッシュの検索と管理のための追加のメソッドを提供します。&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt; &lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt; &lt;/a&gt;のドキュメントを参照してください。</target>
        </trans-unit>
        <trans-unit id="4a1e44716730f4f771067bf0b441674e2034e883" translate="yes" xml:space="preserve">
          <source>The richer dictionary on the right is not larger in size, heavier subsampling is performed in order to stay on the same order of magnitude.</source>
          <target state="translated">右側のリッチな辞書はサイズが大きくないので、同じオーダーにとどまるように重いサブサンプリングが行われます。</target>
        </trans-unit>
        <trans-unit id="6f396634dd18eef11c3d60404319f2831b13040b" translate="yes" xml:space="preserve">
          <source>The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around &lt;code&gt;x=2&lt;/code&gt;). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance.</source>
          <target state="translated">右の図は同じプロットに対応していますが、代わりに決定木のバギング集団を使用しています。どちらの図でも、バイアス項が前のケースよりも大きいことがわかります。右上の図では、平均予測（シアン色）と可能な限り最良のモデルとの差が大きくなっています（たとえば、 &lt;code&gt;x=2&lt;/code&gt; あたりのオフセットに注意してください））。右下の図では、バイアス曲線も左下の図よりわずかに高くなっています。ただし、分散に関しては、予測のビームは狭く、分散が低いことを示唆しています。確かに、右下の図で確認できるように、（緑色の）分散項は単一決定木の場合よりも低くなっています。したがって、全体として、バイアス分散分解は同じではなくなります。トレードオフはバギングに適しています。データセットのブートストラップコピーに適合するいくつかの決定木を平均化すると、バイアス項がわずかに増加しますが、分散の大幅な削減が可能になり、全体的な平均二乗誤差が低くなります（下の赤い曲線を比較してください）図）。スクリプト出力もこの直感を確認します。バギング集団の総誤差は、単一の決定木の総誤差よりも低く、そしてこの違いは確かに主に分散の減少に起因しています。</target>
        </trans-unit>
        <trans-unit id="4281560832d700a912bb9768ad9253748f3986db" translate="yes" xml:space="preserve">
          <source>The right plot shows the mean squared error between the coefficients found by the model and the chosen vector w. Less regularised models retrieve the exact coefficients (error is equal to 0), stronger regularised models increase the error.</source>
          <target state="translated">右のプロットは,モデルによって発見された係数と選択されたベクトルwとの間の平均二乗誤差を示しています.</target>
        </trans-unit>
        <trans-unit id="e91440f2fdf83c048c6ec145aa4ba8b2408d7a77" translate="yes" xml:space="preserve">
          <source>The robust MCD, that has a low error provided \(n_\text{samples} &amp;gt; 5n_\text{features}\)</source>
          <target state="translated">提供されるエラーの少ない堅牢なMCD \（n_ \ text {samples}&amp;gt; 5n_ \ text {features} \）</target>
        </trans-unit>
        <trans-unit id="ba79522dd51eaaef6a47e20f449051b56df7c07d" translate="yes" xml:space="preserve">
          <source>The roc curve requires either the probabilities or the non-thresholded decision values from the estimator. Since the logistic regression provides a decision function, we will use it to plot the roc curve:</source>
          <target state="translated">roc曲線は、推定器からの確率または閾値のない決定値のいずれかを必要とする。ロジスティック回帰は決定関数を提供するので、それを使用してROC曲線をプロットします。</target>
        </trans-unit>
        <trans-unit id="17f09e161fdb50b4bf88ea2669cf0485f026fd48" translate="yes" xml:space="preserve">
          <source>The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.</source>
          <target state="translated">行はサンプル、列は以下の通りです。セパルの長さ、セパルの幅、花びらの長さ、花びらの幅。</target>
        </trans-unit>
        <trans-unit id="075c19426795ecca36fadcd5142db0c818eae0c2" translate="yes" xml:space="preserve">
          <source>The s parameter used to randomly scale the penalty of different features. Should be between 0 and 1.</source>
          <target state="translated">s パラメータは,異なる特徴のペナルティをランダムにスケーリングするために使用されます.0から1の間でなければなりません.</target>
        </trans-unit>
        <trans-unit id="04a6b9de3c4814c0e0ce533f438ce9705b113ee4" translate="yes" xml:space="preserve">
          <source>The same as the min_samples given to OPTICS. Up and down steep regions can&amp;rsquo;t have more then &lt;code&gt;min_samples&lt;/code&gt; consecutive non-steep points. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).</source>
          <target state="translated">OPTICSに与えられたmin_samplesと同じです。急勾配の領域の上下には、 &lt;code&gt;min_samples&lt;/code&gt; を超える連続した非急勾配のポイントを含めることはできません。絶対数またはサンプル数の端数として表されます（少なくとも2に丸められます）。</target>
        </trans-unit>
        <trans-unit id="8e103a284fd35ca6afde93378ca71e413fedf538" translate="yes" xml:space="preserve">
          <source>The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).</source>
          <target state="translated">同じグループが二つ折りになっても出てこない(はっきりとしたグループの数は、少なくとも二つ折りの数と同じでなければならない)。</target>
        </trans-unit>
        <trans-unit id="0bd48a9cd63a739602e7de633cedbe34c0721a4f" translate="yes" xml:space="preserve">
          <source>The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:</source>
          <target state="translated">変換器の同じ イ ン ス タ ン ス を、 はめ込みの呼び出 し の際には見られなかった新しいテ ス ト デー タ に適用す る こ と がで き ます:訓練デー タ で実行 さ れた変換 と 一致す る よ う 、 同じ ス ケー リ ン グ と シ フ ト 演算が適用 さ れます。</target>
        </trans-unit>
        <trans-unit id="9b63d107562f8bc53dcee73bc223f241497e945e" translate="yes" xml:space="preserve">
          <source>The same parameter &lt;code&gt;target&lt;/code&gt; is used to specify the target in multi-output regression settings.</source>
          <target state="translated">同じパラメーター &lt;code&gt;target&lt;/code&gt; を使用して、多出力回帰設定でターゲットを指定します。</target>
        </trans-unit>
        <trans-unit id="c4fc830eaa32e3715aa0bbc8cdaaf20cd8fc44d0" translate="yes" xml:space="preserve">
          <source>The same probability calibration procedure is available for all estimators via the &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; (see &lt;a href=&quot;calibration#calibration&quot;&gt;Probability calibration&lt;/a&gt;). In the case of &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt;, this procedure is builtin in &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; which is used under the hood, so it does not rely on scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;CalibratedClassifierCV&lt;/code&gt; &lt;/a&gt;を介して、すべての推定器で同じ確率キャリブレーション手順を使用できます（&lt;a href=&quot;calibration#calibration&quot;&gt;確率キャリブレーションを&lt;/a&gt;参照）。&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; の&lt;/a&gt;場合、このプロシージャは内部で使用される&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvmに&lt;/a&gt;組み込まれているため、scikit-learnの&lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;CalibratedClassifierCV&lt;/code&gt; に&lt;/a&gt;依存しません。</target>
        </trans-unit>
        <trans-unit id="c16a50cac2b1dc1101733e88917cf565ad0ad55a" translate="yes" xml:space="preserve">
          <source>The sample counts that are shown are weighted with any sample_weights that might be present.</source>
          <target state="translated">表示されるサンプル数は、存在する可能性のあるsample_weightsで重み付けされています。</target>
        </trans-unit>
        <trans-unit id="da56cc1e3278be97e394d14d7afaac125d975593" translate="yes" xml:space="preserve">
          <source>The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible.</source>
          <target state="translated">サンプルの重み付けは,Cパラメータを再スケーリングします.これは,分類器がこれらのポイントを正しく取得することをより重視することを意味します.この効果は,しばしば微妙なものかもしれません.ここでは,この効果を強調するために,特に外れ値に重みをつけて,決定境界の変形を非常に目に見えるようにしています.</target>
        </trans-unit>
        <trans-unit id="67f0f1888d07b20765c25e213bac379b2147854b" translate="yes" xml:space="preserve">
          <source>The sampled subsets of integer. The subset of selected integer might not be randomized, see the method argument.</source>
          <target state="translated">整数のサンプリングされた部分集合。選択された整数の部分集合はランダム化されないかもしれません。</target>
        </trans-unit>
        <trans-unit id="582c77a6e6e223c6a451ff779c949c2d01a1e4b1" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="translated">このデータセットのサンプルは、米国の30&amp;times;30mの森林のパッチに対応しており、各パッチのカバータイプ、つまり主要な樹種を予測するために収集されています。7つのカバータイプがあるため、これはマルチクラス分類問題になります。各サンプルには、&lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;データセットのホームページに&lt;/a&gt;記載されている54の機能があります。一部の機能はブールインジケーターですが、その他の機能は離散測定または連続測定です。</target>
        </trans-unit>
        <trans-unit id="5425b30895b660513d41a7b82e02c9858a9f43c3" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="translated">このデータセットのサンプルは、米国の30&amp;times;30mの森林パッチに対応しており、各パッチのカバータイプ、つまり主要な樹種を予測するタスクのために収集されています。7つのカバータイプがあるため、これはマルチクラス分類の問題になります。各サンプルには54の機能があり、&lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;データセットのホームページに&lt;/a&gt;記載されています。一部の機能はブールインジケーターですが、その他の機能は離散的または連続的な測定です。</target>
        </trans-unit>
        <trans-unit id="5340a26524629a68efc3f526b09b13eeae9b4043" translate="yes" xml:space="preserve">
          <source>The samples that are used to train the calibrator should not be used to train the target classifier.</source>
          <target state="translated">キャリブレーターを訓練するために使用されるサンプルは、ターゲット分類器を訓練するために使用されるべきではありません。</target>
        </trans-unit>
        <trans-unit id="3faf112740a094590f64233617c65a5fa097ee8f" translate="yes" xml:space="preserve">
          <source>The samples.</source>
          <target state="translated">サンプルです。</target>
        </trans-unit>
        <trans-unit id="7217248266c27250d74fa132fb5cef7e3435697a" translate="yes" xml:space="preserve">
          <source>The scalar parameter to validate.</source>
          <target state="translated">検証するスカラーパラメータ。</target>
        </trans-unit>
        <trans-unit id="1aa98782517735bf609d139ad8030ba79e53e38f" translate="yes" xml:space="preserve">
          <source>The scaler instance can then be used on new data to transform it the same way it did on the training set:</source>
          <target state="translated">スケーラーのインスタンスを新しいデータ上で使用して、学習セットと同じように変換することができます。</target>
        </trans-unit>
        <trans-unit id="fae25cf01c011421f9b169e383c16cc5b0e7dc5d" translate="yes" xml:space="preserve">
          <source>The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data:</source>
          <target state="translated">scikit-learnプロジェクトは、新規性や外れ値検出の両方に使用できる機械学習ツールのセットを提供します。この戦略は、データから教師なしの方法でオブジェクトを学習して実装されています。</target>
        </trans-unit>
        <trans-unit id="b2bcbd53d39d84e38eaccf61f40a5b7e3d9f1072" translate="yes" xml:space="preserve">
          <source>The scikit-learn provides an object &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt; that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.</source>
          <target state="translated">scikit-learnは、ロバストな共分散推定をデータに適合させるオブジェクト&lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt; &lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt; &lt;/a&gt;を提供し、中央モードの外側の点を無視して楕円を中央データポイントに適合させます。</target>
        </trans-unit>
        <trans-unit id="c6f20ecec2f178819b742529ff67a9012c4e74b9" translate="yes" xml:space="preserve">
          <source>The score above which features should be selected.</source>
          <target state="translated">どの機能を選択すべきかの上のスコア。</target>
        </trans-unit>
        <trans-unit id="fbc2c7bdd77bdb62a30109845f32d883a40f3289" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split.</source>
          <target state="translated">各 cv 分割のテストスコアを表すスコア配列.</target>
        </trans-unit>
        <trans-unit id="a1547f496d2e14d7ede805b073598361b82ebf35" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split. Suffix &lt;code&gt;_score&lt;/code&gt; in &lt;code&gt;test_score&lt;/code&gt; changes to a specific metric like &lt;code&gt;test_r2&lt;/code&gt; or &lt;code&gt;test_auc&lt;/code&gt; if there are multiple scoring metrics in the scoring parameter.</source>
          <target state="translated">各cv分割のテストスコアのスコア配列。サフィックス &lt;code&gt;_score&lt;/code&gt; で &lt;code&gt;test_score&lt;/code&gt; のようなメトリックの特定に変わり &lt;code&gt;test_r2&lt;/code&gt; または &lt;code&gt;test_auc&lt;/code&gt; スコアリングパラメータで複数のスコアリング指標がある場合。</target>
        </trans-unit>
        <trans-unit id="7e1ecd73a82717f9c044c3e124c2c92104db3c6b" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. Suffix &lt;code&gt;_score&lt;/code&gt; in &lt;code&gt;train_score&lt;/code&gt; changes to a specific metric like &lt;code&gt;train_r2&lt;/code&gt; or &lt;code&gt;train_auc&lt;/code&gt; if there are multiple scoring metrics in the scoring parameter. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">各cv分割の列車スコアのスコア配列。サフィックス &lt;code&gt;_score&lt;/code&gt; で &lt;code&gt;train_score&lt;/code&gt; のようなメトリックの特定に変わり &lt;code&gt;train_r2&lt;/code&gt; または &lt;code&gt;train_auc&lt;/code&gt; スコアリングパラメータで複数のスコアリング指標が存在する場合。これは、 &lt;code&gt;return_train_score&lt;/code&gt; パラメーターが &lt;code&gt;True&lt;/code&gt; の場合にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="21374122c0da50ea660a65ea3274fa15ca9abbe5" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">各CV分割のトレーニングスコアのスコア配列。これは、 &lt;code&gt;return_train_score&lt;/code&gt; パラメータが &lt;code&gt;True&lt;/code&gt; の場合にのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="f9f0e8fbf15571f500f19345d44e98db663f1949" translate="yes" xml:space="preserve">
          <source>The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.</source>
          <target state="translated">スコアは,正しくないクラスタリングの場合は-1,非常に密なクラスタリングの場合は+1の間に制限される.ゼロ付近のスコアは,クラスタが重複していることを示す.</target>
        </trans-unit>
        <trans-unit id="10419dac5247bd799d768931a57ece7edc36ff14" translate="yes" xml:space="preserve">
          <source>The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.</source>
          <target state="translated">スコアは、クラスタ内分散とクラスタ間分散の比率として定義されます。</target>
        </trans-unit>
        <trans-unit id="24504b3de92e08246a270b0dc2b57eafc2174bab" translate="yes" xml:space="preserve">
          <source>The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.</source>
          <target state="translated">スコアは、各クラスタと最も類似しているクラスタの平均類似度の尺度として定義され、類似度はクラスタ間距離に対するクラスタ内距離の比率である。したがって、離れていて分散していないクラスタほどスコアが高くなります。</target>
        </trans-unit>
        <trans-unit id="a5fc2e46656049d91b95dd6363e2ef13687eb710" translate="yes" xml:space="preserve">
          <source>The score is defined as the ratio of within-cluster distances to between-cluster distances.</source>
          <target state="translated">スコアは、クラスタ内距離とクラスタ間距離の比として定義されます。</target>
        </trans-unit>
        <trans-unit id="a9ccc42adfd31440d43d06c5c4aab96f5e8222f0" translate="yes" xml:space="preserve">
          <source>The score is fast to compute</source>
          <target state="translated">スコアの計算が速い</target>
        </trans-unit>
        <trans-unit id="ef88d78d9d28409ef934e001658916bd55503917" translate="yes" xml:space="preserve">
          <source>The score is fast to compute.</source>
          <target state="translated">スコアの計算が早い。</target>
        </trans-unit>
        <trans-unit id="298450881e1d83a617f17a47cba5b823081f8332" translate="yes" xml:space="preserve">
          <source>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.</source>
          <target state="translated">クラスターが密集していて、よく分離されているときにスコアが高くなり、これはクラスターの標準的な概念に関係しています。</target>
        </trans-unit>
        <trans-unit id="8ebec04d9506729ea096f793265427e501ac6359" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - \text{n\_classes}}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="translated">スコアの範囲は0〜1です。または、 &lt;code&gt;adjusted=True&lt;/code&gt; を使用すると、ランダムなパフォーマンスで、\（\ frac {1} {1-\ text {n \ _classes}} \）〜1の範囲に再スケーリングされます。スコア0。</target>
        </trans-unit>
        <trans-unit id="f66ce641ead1aaeebf85e08eeb401e8169ff494e" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - n\_classes}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="translated">スコアの範囲は0〜1です。または、 &lt;code&gt;adjusted=True&lt;/code&gt; を使用すると、\（\ frac {1} {1-n \ _classes} \）から1までの範囲に再スケーリングされ、ランダムスコア0でのパフォーマンスが得られます。</target>
        </trans-unit>
        <trans-unit id="99f201771620c880cb8fcf0a4290517d17eb61e7" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.</source>
          <target state="translated">スコアは0から1の範囲です。高い値は、2つのクラスタ間の類似性が高いことを示します。</target>
        </trans-unit>
        <trans-unit id="67703aed8c056d27307c0c36c3685d8f65de746b" translate="yes" xml:space="preserve">
          <source>The scorer callable object / function must have its signature as &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">スコアラー呼び出し可能オブジェクト/関数には、 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; としての署名が必要です。</target>
        </trans-unit>
        <trans-unit id="5e77d6fe812bc8f046068c5fa078dda3a1146e44" translate="yes" xml:space="preserve">
          <source>The scorer.</source>
          <target state="translated">点取り屋さん。</target>
        </trans-unit>
        <trans-unit id="97894cb494476d0c2ed229f8ae81e55aa78ff1c3" translate="yes" xml:space="preserve">
          <source>The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the &lt;code&gt;scoring&lt;/code&gt; parameter. Empty if no early stopping or if &lt;code&gt;validation_fraction&lt;/code&gt; is None.</source>
          <target state="translated">差し出された検証データの各反復でのスコア。最初のエントリは、最初の反復前のアンサンブルのスコアです。スコアは、 &lt;code&gt;scoring&lt;/code&gt; パラメーターに従って計算されます。早期停止がない場合、または &lt;code&gt;validation_fraction&lt;/code&gt; がNoneの場合は、空にします。</target>
        </trans-unit>
        <trans-unit id="04400e5c44d656378bb2e0f77c0e1b05794d2b51" translate="yes" xml:space="preserve">
          <source>The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the &lt;code&gt;scoring&lt;/code&gt; parameter. If &lt;code&gt;scoring&lt;/code&gt; is not &amp;lsquo;loss&amp;rsquo;, scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.</source>
          <target state="translated">トレーニングデータの各反復でのスコア。最初のエントリは、最初の反復前のアンサンブルのスコアです。スコアは、 &lt;code&gt;scoring&lt;/code&gt; パラメーターに従って計算されます。 &lt;code&gt;scoring&lt;/code&gt; が「損失」でない場合、スコアは最大10000サンプルのサブセットで計算されます。早期打ち切りがない場合は空にします。</target>
        </trans-unit>
        <trans-unit id="dd1c18a79b397962bdc5e0312d35f12a484f084a" translate="yes" xml:space="preserve">
          <source>The scores for each feature along the path.</source>
          <target state="translated">パスに沿った各特徴のスコア。</target>
        </trans-unit>
        <trans-unit id="81a1b1406082100f034b3df6c2ec24562a123854" translate="yes" xml:space="preserve">
          <source>The scores obtained for each permutations.</source>
          <target state="translated">各パーマネテーションについて得られたスコア。</target>
        </trans-unit>
        <trans-unit id="12753b21f50efe6accfab886a283f46c3614c6fe" translate="yes" xml:space="preserve">
          <source>The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does not attempt to completely filter the outliers but lessen their effect.</source>
          <target state="translated">HuberRegressorのスコアは、外れ値を完全にフィルタリングするのではなく、その影響を軽減しようとしているため、TheilSenとRANSACの両方と直接比較することはできません。</target>
        </trans-unit>
        <trans-unit id="48d352a6767e745c328aec9195da7218a62bd613" translate="yes" xml:space="preserve">
          <source>The scores of all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at keys ending in &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; (&lt;code&gt;'mean_test_precision'&lt;/code&gt;, &lt;code&gt;'rank_test_precision'&lt;/code&gt;, etc&amp;hellip;)</source>
          <target state="translated">すべてのスコアラーのスコアは、 &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; 終わるキーの &lt;code&gt;cv_results_&lt;/code&gt; dictで利用できます（ &lt;code&gt;'mean_test_precision'&lt;/code&gt; 、 &lt;code&gt;'rank_test_precision'&lt;/code&gt; など...）</target>
        </trans-unit>
        <trans-unit id="1676248dcc2b8fd0688c9d8da4bc193152185788" translate="yes" xml:space="preserve">
          <source>The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on.</source>
          <target state="translated">最適なペナルティパラメータ(α)の探索は、反復的に洗練されたグリッド上で行われます:最初にグリッド上のクロスバリデートされたスコアが計算され、次に最大値を中心とした新たな洗練されたグリッドが計算されます。</target>
        </trans-unit>
        <trans-unit id="929b69ae47a09aabafb756d8ff0633d79e90c985" translate="yes" xml:space="preserve">
          <source>The searched parameter.</source>
          <target state="translated">検索されたパラメータ。</target>
        </trans-unit>
        <trans-unit id="0d9a4b24aef1596fe1105c7b9e28ffdd6e589d45" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the product-kernel</source>
          <target state="translated">製品カーネルの第二ベースカーネル</target>
        </trans-unit>
        <trans-unit id="2d0afedea31e6e04d617f4edd60a150835c5916d" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the sum-kernel</source>
          <target state="translated">和カーネルの第二基底カーネル</target>
        </trans-unit>
        <trans-unit id="544eb81771c23c2f620b00755dbcd12f00cacc66" translate="yes" xml:space="preserve">
          <source>The second example shows the ability of the Minimum Covariance Determinant robust estimator of covariance to concentrate on the main mode of the data distribution: the location seems to be well estimated, although the covariance is hard to estimate due to the banana-shaped distribution. Anyway, we can get rid of some outlying observations. The One-Class SVM is able to capture the real data structure, but the difficulty is to adjust its kernel bandwidth parameter so as to obtain a good compromise between the shape of the data scatter matrix and the risk of over-fitting the data.</source>
          <target state="translated">2番目の事例は、データ分布のメイン・モードに集中する共分散の最小共分散決定子ロバスト推定器の能力を示しています:共分散はバナナ型分布のために推定しにくいですが、位置はよく推定されているようです。いずれにしても、いくつかの外れたオブザベーションを取り除くことができます。ワンクラスSVMは実際のデータ構造を捉えることができますが、困難なのは、データ散乱行列の形状とデータにオーバーフィットするリスクとの間で良い妥協点を得るために、そのカーネル帯域幅パラメータを調整することです。</target>
        </trans-unit>
        <trans-unit id="5a46c67db9268b64cb76670e9e1b845e906b608f" translate="yes" xml:space="preserve">
          <source>The second figure shows the calibration curve of a linear support-vector classifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian naive Bayes: the calibration curve has a sigmoid curve, which is typical for an under-confident classifier. In the case of LinearSVC, this is caused by the margin property of the hinge loss, which lets the model focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">2番目の図は、線形支持ベクトル分類器(LinearSVC)の検量線を示しています。LinearSVCは、ガウスナイーブベイズとは逆の挙動を示します:校正曲線はシグモイド曲線を持ち、これは信頼度の低い分類器に典型的なものです。LinearSVCの場合、これはヒンジ損失のマージン特性によって引き起こされ、モデルは決定境界に近いハードサンプル(支持ベクトル)に焦点を合わせることができます。</target>
        </trans-unit>
        <trans-unit id="e56142dda4889d67d8f5448567e56cb678c48e3c" translate="yes" xml:space="preserve">
          <source>The second figure shows the log-marginal-likelihood for different choices of the kernel&amp;rsquo;s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.</source>
          <target state="translated">2番目の図は、カーネルのハイパーパラメーターのさまざまな選択肢の対数限界尤度を示しており、最初の図で使用されているハイパーパラメーターの2つの選択肢を黒い点で強調しています。</target>
        </trans-unit>
        <trans-unit id="69a6b8988bb4a9da22ae7a8129c60d4bfa19ab9d" translate="yes" xml:space="preserve">
          <source>The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:</source>
          <target state="translated">第2のローダは、典型的には顔認証タスクに使用されます:各サンプルは、同一人物に属するか否かを問わず、2枚の画像のペアです。</target>
        </trans-unit>
        <trans-unit id="80a536b57ba05b05dbab01bd549517eccd6caab7" translate="yes" xml:space="preserve">
          <source>The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process prior fit with variational inference. The low value of the concentration prior makes the model favor a lower number of active components. This models &amp;ldquo;decides&amp;rdquo; to focus its modeling power on the big picture of the structure of the dataset: groups of points with alternating directions modeled by non-diagonal covariance matrices. Those alternating directions roughly capture the alternating nature of the original sine signal.</source>
          <target state="translated">2番目のモデルは、変分推論で事前にディリクレプロセスで近似したベイジアンガウス混合モデルです。以前の濃度の値が低いと、モデルはより少ない数のアクティブなコンポーネントを優先します。このモデルは、データセットの構造の全体像（非対角共分散行列でモデル化された方向が交互に変化する点のグループ）にモデリング力を集中させるように「決定」します。これらの交互の方向は、元のサイン信号の交互の性質を大まかに捉えます。</target>
        </trans-unit>
        <trans-unit id="ce63f012cd3f9d5ba6717aef4dc1ee5e80e67c9d" translate="yes" xml:space="preserve">
          <source>The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">2番目のモデルは、ノイズレベルが小さく、長さのスケールが短いため、ノイズフリーの関数関係による変動のほとんどを説明することができます。2番目のモデルの方が尤度が高いですが、ハイパーパラメータの初期値によっては、勾配ベースの最適化も高ノイズ解に収束する可能性があります。そのため、異なる初期化に対して数回の最適化を繰り返すことが重要です。</target>
        </trans-unit>
        <trans-unit id="d5dcf1b11e556ea9365934aae9c750b0508ecb89" translate="yes" xml:space="preserve">
          <source>The second plot demonstrate one single run of the &lt;code&gt;MiniBatchKMeans&lt;/code&gt; estimator using a &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; and &lt;code&gt;n_init=1&lt;/code&gt;. This run leads to a bad convergence (local optimum) with estimated centers stuck between ground truth clusters.</source>
          <target state="translated">2番目のプロットは、 &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; と &lt;code&gt;n_init=1&lt;/code&gt; を使用した &lt;code&gt;MiniBatchKMeans&lt;/code&gt; 推定器の1回の実行を示しています。この実行により、グランドトゥルースクラスター間にスタックされた推定中心で、収束が悪くなります（局所最適）。</target>
        </trans-unit>
        <trans-unit id="41338f596d871499307d96f69f697b91afdfa1c4" translate="yes" xml:space="preserve">
          <source>The second plot is a heatmap of the classifier&amp;rsquo;s cross-validation accuracy as a function of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from \(10^{-3}\) to \(10^3\) is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.</source>
          <target state="translated">2番目のプロットは、 &lt;code&gt;C&lt;/code&gt; および &lt;code&gt;gamma&lt;/code&gt; の関数としての分類器の交差検定精度のヒートマップです。この例では、説明のために比較的大きなグリッドを調べます。実際には、通常、\（10 ^ {-3} \）から\（10 ^ 3 \）までの対数グリッドで十分です。最適なパラメータがグリッドの境界にある場合、後続の検索でその方向に拡張できます。</target>
        </trans-unit>
        <trans-unit id="57fb8e025f7ec94b6d1e30748cbff5561f1e9901" translate="yes" xml:space="preserve">
          <source>The second plot shows that an increase of the admissible distortion &lt;code&gt;eps&lt;/code&gt; allows to reduce drastically the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; for a given number of samples &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">2番目のプロットは、許容可能な歪み &lt;code&gt;eps&lt;/code&gt; の増加により、特定のサンプル数 &lt;code&gt;n_components&lt;/code&gt; 最小次元数n_componentsを大幅に削減できることを示しています &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9c6c0ec72e2e2da2def0a20a979b162b66b4f25f" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span.</source>
          <target state="translated">第2のプロットは、RBFカーネルSVMと近似カーネルマップを持つ線形SVMの決定面を可視化したものである。このプロットは、データの最初の2つの主成分に投影された分類器の決定面を示しています。この可視化は、64次元での決定面の興味深いスライスなので、大目に見てください。特に、データポイント(ドットで表される)は、最初の2つの主成分が横たわる平面上にはないので、必ずしもそのデータポイントが横たわっている領域に分類されるとは限らないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="0b0b4cc48e44d58bc4674ef40664981912a3f333" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span. The usage of &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="translated">2番目のプロットは、RBFカーネルSVMと線形SVMの決定面を近似カーネルマップで視覚化しました。プロットは、データの最初の2つの主成分に投影された分類器の決定面を示しています。この視覚化は、64次元の決定面を通る興味深いスライスであるため、一粒の塩で行う必要があります。特に、データポイント（ドットとして表される）は、最初の2つの主成分がまたがる平面上にないため、必ずしもデータポイントが存在する領域に分類されるとは限らないことに注意してください。&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;の使用法については、&lt;a href=&quot;../../modules/kernel_approximation#kernel-approximation&quot;&gt;カーネル近似&lt;/a&gt;で詳しく説明されています。</target>
        </trans-unit>
        <trans-unit id="a7aa029b23a556153d8e76aebf5393705fc5a931" translate="yes" xml:space="preserve">
          <source>The second use case is to build a completely custom scorer object from a simple python function using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;, which can take several parameters:</source>
          <target state="translated">2番目の使用例は、&lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt;を使用して単純なpython関数から完全にカスタムのスコアラーオブジェクトを構築することです。これは、いくつかのパラメーターを取ることができます。</target>
        </trans-unit>
        <trans-unit id="c00cf63770db0bb2225569e421e3076426129a55" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">繰り返し値を削除するために連続変数に小さなノイズを追加するための疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="06cdf003d2aac9179d5700a91f249ff0a94d6f8e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;</source>
          <target state="translated">更新するランダムな機能を選択する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;selection&lt;/code&gt; == 'random'のときに使用されます</target>
        </trans-unit>
        <trans-unit id="0bc2f2e46810cbdc913e04d68694f60b2b83e0d8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;.</source>
          <target state="translated">更新するランダムな機能を選択する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;selection&lt;/code&gt; == 'random'の場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="fceac838e81f76b0b51f388983ff2416b1824960" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">更新するランダム機能を選択する疑似乱数ジェネレーターのシード。 &lt;code&gt;selection&lt;/code&gt; == 'random'の場合に使用されます。複数の関数呼び出しにわたって再現可能な出力のためにintを渡します。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="c9b5b4205f687d2590f4c80ac04919e87b2e36db" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data for the dual coordinate descent (if &lt;code&gt;dual=True&lt;/code&gt;). When &lt;code&gt;dual=False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">二重座標降下のデータをシャッフルするときに使用する疑似乱数ジェネレータのシード（ &lt;code&gt;dual=True&lt;/code&gt; の場合）。 &lt;code&gt;dual=False&lt;/code&gt; の場合、&lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;の基本的な実装はランダムではなく、 &lt;code&gt;random_state&lt;/code&gt; は結果に影響を与えません。 intの場合、random_stateは乱数ジェネレータによって使用されるシードです。 RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。 Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="213c180fc69b83cb51964cde95f090b02ff40db1" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data, i.e. getting the random vectors to initialize the algorithm. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">データをシャッフルするとき、つまりアルゴリズムを初期化するための乱数ベクトルを取得するときに使用する疑似乱数ジェネレーターのシード。複数の関数呼び出しにわたって再現可能な結果を​​得るためにintを渡します。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="f24e839d9f0c07b405eb078c6f50d58ca84b7fe6" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">データをシャッフルするときに使用する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="58318e33f8140e9303ba15931c5e93b11c5df63e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo; or &amp;lsquo;liblinear&amp;rsquo;.</source>
          <target state="translated">データをシャッフルするときに使用する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;solver&lt;/code&gt; == 'sag'または 'liblinear'の場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="1e64edd7c479eb06717df1495b4d044bfaa961d0" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo;.</source>
          <target state="translated">データをシャッフルするときに使用する疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;solver&lt;/code&gt; == 'サグ'のときに使用されます。</target>
        </trans-unit>
        <trans-unit id="3a35c2036466dbf3b68131c5150065896eadbbdd" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use. Randomizes selection of estimator features if n_nearest_features is not None, the &lt;code&gt;imputation_order&lt;/code&gt; if &lt;code&gt;random&lt;/code&gt;, and the sampling from posterior if &lt;code&gt;sample_posterior&lt;/code&gt; is True. Use an integer for determinism. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">使用する疑似乱数ジェネレーターのシード。 n_nearest_featuresは、Noneでない場合は推定機能の選択をランダムに &lt;code&gt;imputation_order&lt;/code&gt; 場合は &lt;code&gt;random&lt;/code&gt; た場合、および後部からのサンプリング &lt;code&gt;sample_posterior&lt;/code&gt; が真です。決定論には整数を使用します。&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="db7139d208134a3c43811feaafd4124e4ee369a8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator used when shuffling the data for probability estimates. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">確率推定のためにデータをシャッフルするときに使用される疑似乱数ジェネレータのシード。intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレータです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="f1665069fc9c4c9d9dabb36f1931e17db4945528" translate="yes" xml:space="preserve">
          <source>The set of F values.</source>
          <target state="translated">Fの値の集合。</target>
        </trans-unit>
        <trans-unit id="d03a062cac2973dfcc9173b5fc93f7520db21987" translate="yes" xml:space="preserve">
          <source>The set of labels can be different for each output variable. For instance, a sample could be assigned &amp;ldquo;pear&amp;rdquo; for an output variable that takes possible values in a finite set of species such as &amp;ldquo;pear&amp;rdquo;, &amp;ldquo;apple&amp;rdquo;; and &amp;ldquo;blue&amp;rdquo; or &amp;ldquo;green&amp;rdquo; for a second output variable that takes possible values in a finite set of colors such as &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;blue&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo;&amp;hellip;</source>
          <target state="translated">ラベルのセットは、出力変数ごとに異なる場合があります。たとえば、サンプルには、「pear」、「apple」などの種の有限セットで可能な値を取る出力変数の「pear」を割り当てることができます。そして「青」または「緑」は、「緑」、「赤」、「青」、「黄色」などの色の有限セットで可能な値を取る2番目の出力変数です。</target>
        </trans-unit>
        <trans-unit id="143c46009e14705cc3449ca19bc2f349662d5283" translate="yes" xml:space="preserve">
          <source>The set of labels for each sample such that &lt;code&gt;y[i]&lt;/code&gt; consists of &lt;code&gt;classes_[j]&lt;/code&gt; for each &lt;code&gt;yt[i, j] == 1&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;y[i]&lt;/code&gt; &lt;code&gt;classes_[j]&lt;/code&gt; が各 &lt;code&gt;yt[i, j] == 1&lt;/code&gt; のclasses_ [j]で構成されるような各サンプルのラベルのセット。</target>
        </trans-unit>
        <trans-unit id="2d10d69a1c09af3eb9fb75910b7cd4ebd942ac2e" translate="yes" xml:space="preserve">
          <source>The set of labels to include when &lt;code&gt;average != 'binary'&lt;/code&gt;, and their order if &lt;code&gt;average is None&lt;/code&gt;. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">&lt;code&gt;average != 'binary'&lt;/code&gt; ときに含めるラベルのセットと、 &lt;code&gt;average is None&lt;/code&gt; 場合の順序。データに存在するラベルは除外できます。たとえば、過半数の否定的なクラスを無視してマルチクラス平均を計算できますが、データに存在しないラベルは、マクロ平均のコンポーネントが0になります。マルチラベルターゲットの場合、ラベルは列インデックスです。デフォルトでは、 &lt;code&gt;y_true&lt;/code&gt; と &lt;code&gt;y_pred&lt;/code&gt; のすべてのラベルがソートされた順序で使用されます。</target>
        </trans-unit>
        <trans-unit id="fc018ce1a1ad3dcbb813a9b943d602142a80bfe5" translate="yes" xml:space="preserve">
          <source>The set of p-values.</source>
          <target state="translated">p値の集合。</target>
        </trans-unit>
        <trans-unit id="ae935a83c454932c18aabaf5527bf9d40a05884a" translate="yes" xml:space="preserve">
          <source>The set of regressors that will be tested sequentially.</source>
          <target state="translated">順次テストされるレグレッサーのセット。</target>
        </trans-unit>
        <trans-unit id="418cd7fd5c32b01bfeb33989472034a267c4e833" translate="yes" xml:space="preserve">
          <source>The shape (Nx, Ny) array of pairwise distances between points in X and Y.</source>
          <target state="translated">XとYの点間の対になる距離の形状(Nx,Ny)の配列。</target>
        </trans-unit>
        <trans-unit id="313d9c9325eeb4a6e4291910f1f9fa2e1a31f92e" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;(n_classes-1, n_SV)&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_classes - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_classes - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="translated">&lt;code&gt;dual_coef_&lt;/code&gt; の形状は &lt;code&gt;(n_classes-1, n_SV)&lt;/code&gt; で、レイアウトを把握するのがやや困難です。列は、 &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; 「one-vs-one」分類器のいずれかに含まれるサポートベクターに対応します。各サポートベクターは、 &lt;code&gt;n_classes - 1&lt;/code&gt; 分類器で使用されます。 &lt;code&gt;n_classes - 1&lt;/code&gt; これらの分類のための二重係数に各行対応のエントリ。</target>
        </trans-unit>
        <trans-unit id="be8e19650b3e56af8959e9d1bdb9305ca252ca97" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_class - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_class - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="translated">&lt;code&gt;dual_coef_&lt;/code&gt; の形状は &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; で、レイアウトを把握するのがやや難しいです。列は、 &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; 「1対1」の分類子のいずれかに含まれるサポートベクターに対応しています。各サポートベクターは、 &lt;code&gt;n_class - 1&lt;/code&gt; 分類器で使用されます。 &lt;code&gt;n_class - 1&lt;/code&gt; これらの分類のための二重係数に各行対応のエントリ。</target>
        </trans-unit>
        <trans-unit id="79705c107a83df8d45ef47d82375b7c707f4d5ae" translate="yes" xml:space="preserve">
          <source>The shape of the result.</source>
          <target state="translated">結果の形。</target>
        </trans-unit>
        <trans-unit id="770a88634d7b4928209cc52a1f8aea4bce68a8e8" translate="yes" xml:space="preserve">
          <source>The shift offset allows a zero threshold for being an outlier. Only available for novelty detection (when novelty is set to True). The argument X is supposed to contain &lt;em&gt;new data&lt;/em&gt;: if X contains a point from training, it considers the later in its own neighborhood. Also, the samples in X are not considered in the neighborhood of any point.</source>
          <target state="translated">シフトオフセットにより、外れ値となるしきい値をゼロにすることができます。新規性検出にのみ使用できます（新規性がTrueに設定されている場合）。引数Xには&lt;em&gt;新しいデータ&lt;/em&gt;が含まれることになっています。Xにトレーニングからのポイントが含まれている場合、Xは自身の近傍の後者を考慮します。また、Xのサンプルは、どの点の近傍でも考慮されません。</target>
        </trans-unit>
        <trans-unit id="fd58d8b5ba5725b529e01c853ef09676528984ae" translate="yes" xml:space="preserve">
          <source>The shifted opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">各入力サンプルの局所外れ値係数の反対側にシフトした値。低いほど異常であることを示す。否定的なスコアは外れ値を表し、肯定的なスコアはインライアを表す。</target>
        </trans-unit>
        <trans-unit id="432ee6647b81dbc5a7cbb417e7251dacced20941" translate="yes" xml:space="preserve">
          <source>The signed distance to the hyperplane (computed as the dot product between the coefficients and the input sample, plus the intercept) is given by &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt;&lt;code&gt;SGDClassifier.decision_function&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">超平面までの符号付き距離（係数と入力サンプル間の内積、および切片として計算）は、&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt; &lt;code&gt;SGDClassifier.decision_function&lt;/code&gt; &lt;/a&gt;によって与えられます。</target>
        </trans-unit>
        <trans-unit id="13c37fa5739748416ca3f9521f51dec2de533ef6" translate="yes" xml:space="preserve">
          <source>The similarity of two sets of biclusters.</source>
          <target state="translated">バイクラスターの2つのセットの類似性。</target>
        </trans-unit>
        <trans-unit id="7becf18fe4f5e5f9bf982cb2425cc3a834e0c404" translate="yes" xml:space="preserve">
          <source>The simplest metric &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; model, called &lt;em&gt;absolute MDS&lt;/em&gt;, disparities are defined by \(\hat{d}_{ij} = S_{ij}\). With absolute MDS, the value \(S_{ij}\) should then correspond exactly to the distance between point \(i\) and \(j\) in the embedding point.</source>
          <target state="translated">&lt;em&gt;絶対MDS&lt;/em&gt;と呼ばれる最も単純なメトリック&lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt;モデルで、視差は\（\ hat {d} _ {ij} = S_ {ij} \）によって定義されます。絶対MDSでは、値\（S_ {ij} \）は、埋め込みポイントのポイント\（i \）と\（j \）の間の距離に正確に対応する必要があります。&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="1d805d3b9d3166d024fed65dbe7dc1ddecd0fb40" translate="yes" xml:space="preserve">
          <source>The simplest possible classifier is the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;nearest neighbor&lt;/a&gt;: given a new observation &lt;code&gt;X_test&lt;/code&gt;, find in the training set (i.e. the data used to train the estimator) the observation with the closest feature vector. (Please see the &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors section&lt;/a&gt; of the online Scikit-learn documentation for more information about this type of classifier.)</source>
          <target state="translated">最も簡単な分類子は&lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;最近傍&lt;/a&gt;です。新しい観測値 &lt;code&gt;X_test&lt;/code&gt; が与えられた場合、トレーニングセット（つまり、推定器の学習に使用されるデータ）で、最も近い特徴ベクトルを使用して観測値を見つけます。（このタイプの分類器の詳細については、Scikit-learnのオンラインドキュメントの「&lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors」セクション&lt;/a&gt;を参照してください。）</target>
        </trans-unit>
        <trans-unit id="1fac5c5ae1360f0509b6fb6c9bee7a89fd16eea5" translate="yes" xml:space="preserve">
          <source>The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost.</source>
          <target state="translated">この次元削減を達成する最も単純な方法は、データのランダムな投影を取ることです。これはデータ構造のある程度の可視化を可能にしますが、選択のランダム性は望まれる多くのことを残します。ランダム投影では、データ内のより興味深い構造が失われる可能性があります。</target>
        </trans-unit>
        <trans-unit id="a8c7e68caf35057fba3785bb16a14da344816784" translate="yes" xml:space="preserve">
          <source>The simplest way to use cross-validation is to call the &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper function on the estimator and the dataset.</source>
          <target state="translated">交差検証を使用する最も簡単な方法は、推定器とデータセットで&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;ヘルパー関数を呼び出すことです。</target>
        </trans-unit>
        <trans-unit id="6b61610397fd4bcbb9699a5ba6221c6a9dfd8212" translate="yes" xml:space="preserve">
          <source>The singular value decomposition, \(A_n = U \Sigma V^\top\), provides the partitions of the rows and columns of \(A\). A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.</source>
          <target state="translated">特異値分解は、「\(A_n=U \(A)」の行と列の分割を与える。)左の特異値の部分集合が行の分割を、右の特異値の部分集合が列の分割を与える。</target>
        </trans-unit>
        <trans-unit id="f785df3fd21ed481c16151f3b91e613616eec382" translate="yes" xml:space="preserve">
          <source>The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the &lt;code&gt;n_components&lt;/code&gt; variables in the lower-dimensional space.</source>
          <target state="translated">選択した各コンポーネントに対応する特異値。特異値は、低次元空間の &lt;code&gt;n_components&lt;/code&gt; 変数の2ノルムに等しくなります。</target>
        </trans-unit>
        <trans-unit id="fda37f3c2ceb4ddf1d93860ac5de12a5ffc950e8" translate="yes" xml:space="preserve">
          <source>The size of &lt;code&gt;grid_scores_&lt;/code&gt; is equal to &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt;, where step is the number of features removed at each iteration.</source>
          <target state="translated">&lt;code&gt;grid_scores_&lt;/code&gt; のサイズは &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt; に等しく、stepは各反復で削除されるフィーチャの数です。</target>
        </trans-unit>
        <trans-unit id="2f08a32d3411460a5643fb826528c94534f8d8f2" translate="yes" xml:space="preserve">
          <source>The size of the image that will be reconstructed.</source>
          <target state="translated">再構築される画像のサイズ。</target>
        </trans-unit>
        <trans-unit id="9867bd15789365ba5d58a7dbdcd5815e87ddbf26" translate="yes" xml:space="preserve">
          <source>The size of the model with the default parameters is \(O( M * N * log (N) )\), where \(M\) is the number of trees and \(N\) is the number of samples. In order to reduce the size of the model, you can change these parameters: &lt;code&gt;min_samples_split&lt;/code&gt;, &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt; and &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="translated">既定のパラメーターを使用したモデルのサイズは\（O（M * N * log（N））\）です。ここで、\（M \）はツリーの数、\（N \）はサンプルの数です。モデルのサイズを小さくするために、これらのパラメーターを変更できます： &lt;code&gt;min_samples_split&lt;/code&gt; 、 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 、 &lt;code&gt;max_depth&lt;/code&gt; 、および &lt;code&gt;min_samples_leaf&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1f17f4e69474d346e8934f03ff8be8d8add88c9f" translate="yes" xml:space="preserve">
          <source>The size of the random matrix to generate.</source>
          <target state="translated">生成するランダム行列のサイズ。</target>
        </trans-unit>
        <trans-unit id="b31452681b2b7098990045ccc11256312f76473c" translate="yes" xml:space="preserve">
          <source>The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth &lt;code&gt;h&lt;/code&gt; can capture interactions of order &lt;code&gt;h&lt;/code&gt; . There are two ways in which the size of the individual regression trees can be controlled.</source>
          <target state="translated">回帰木の基本学習器のサイズは、勾配ブースティングモデルで取得できる変数の相互作用のレベルを定義します。一般に、深さ &lt;code&gt;h&lt;/code&gt; のツリーは、次数 &lt;code&gt;h&lt;/code&gt; の相互作用をキャプチャできます。個々の回帰ツリーのサイズを制御する方法は2つあります。</target>
        </trans-unit>
        <trans-unit id="eb0fc3f910e5dcbfcfe6af4540bbfd0a452530ef" translate="yes" xml:space="preserve">
          <source>The size of the sample to use when computing the Silhouette Coefficient on a random subset of the data. If &lt;code&gt;sample_size is None&lt;/code&gt;, no sampling is used.</source>
          <target state="translated">データのランダムなサブセットでシルエット係数を計算するときに使用するサンプルのサイズ。 &lt;code&gt;sample_size is None&lt;/code&gt; 場合、サンプリングは使用されません。</target>
        </trans-unit>
        <trans-unit id="b3605bfba06cab15c1207c4102bf5bbb9eee0a53" translate="yes" xml:space="preserve">
          <source>The size of the set to sample from.</source>
          <target state="translated">サンプリングするセットのサイズ。</target>
        </trans-unit>
        <trans-unit id="bec91be0bb08923b4b038e4efb20c2c584713078" translate="yes" xml:space="preserve">
          <source>The size of the trees can be controlled through the &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt;, and &lt;code&gt;min_samples_leaf&lt;/code&gt; parameters.</source>
          <target state="translated">ツリーのサイズは、 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 、 &lt;code&gt;max_depth&lt;/code&gt; 、および &lt;code&gt;min_samples_leaf&lt;/code&gt; パラメーターを介して制御できます。</target>
        </trans-unit>
        <trans-unit id="d5051ed3b169b87ed97be7e20bda0cdf9d82ecae" translate="yes" xml:space="preserve">
          <source>The size, the distance and the shape of clusters may vary upon initialization, perplexity values and does not always convey a meaning.</source>
          <target state="translated">クラスターの大きさ、距離、形状は、初期化時に変化する可能性があり、錯乱値であり、必ずしも意味を伝えるものではありません。</target>
        </trans-unit>
        <trans-unit id="6830208b16eb851287384293d35fab1e64fb8f9a" translate="yes" xml:space="preserve">
          <source>The skewed chi squared kernel is given by:</source>
          <target state="translated">歪んだカイ二乗カーネルは次式で与えられる。</target>
        </trans-unit>
        <trans-unit id="b37812e3809837f255e9952013df77f9cb7868cc" translate="yes" xml:space="preserve">
          <source>The smaller the Brier score, the better, hence the naming with &amp;ldquo;loss&amp;rdquo;. Across all items in a set N predictions, the Brier score measures the mean squared difference between (1) the predicted probability assigned to the possible outcomes for item i, and (2) the actual outcome. Therefore, the lower the Brier score is for a set of predictions, the better the predictions are calibrated. Note that the Brier score always takes on a value between zero and one, since this is the largest possible difference between a predicted probability (which must be between zero and one) and the actual outcome (which can take on values of only 0 and 1). The Brier loss is composed of refinement loss and calibration loss. The Brier score is appropriate for binary and categorical outcomes that can be structured as true or false, but is inappropriate for ordinal variables which can take on three or more values (this is because the Brier score assumes that all possible outcomes are equivalently &amp;ldquo;distant&amp;rdquo; from one another). Which label is considered to be the positive label is controlled via the parameter pos_label, which defaults to 1. Read more in the &lt;a href=&quot;../calibration#calibration&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">ブライアスコアが小さいほど良いため、「損失」という名前が付けられます。セットN予測のすべての項目にわたって、ブライアスコアは、（1）項目iの可能な結果に割り当てられた予測確率と、（2）実際の結果の間の平均二乗差を測定します。したがって、一連の予測のブライアスコアが低いほど、予測の調整が向上します。ブライアスコアは常に0から1の間の値をとることに注意してください。これは、予測された確率（0から1の間でなければならない）と実際の結果（0と1の値のみをとることができる）の間の可能な最大の差であるためです。 ）。ブライアー損失は、精製損失と校正損失で構成されます。ブライアスコアは、真または偽として構造化できるバイナリおよびカテゴリの結果に適しています。ただし、3つ以上の値をとることができる順序変数には不適切です（これは、ブライアスコアがすべての可能な結果が互いに同等に「離れている」と想定しているためです）。どのラベルがポジティブラベルと見なされるかは、パラメータpos_labelを介して制御されます。デフォルトは1です。&lt;a href=&quot;../calibration#calibration&quot;&gt;ユーザーガイド&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="bdf0bd9101d435249fe017ac2196fd5049ca1688" translate="yes" xml:space="preserve">
          <source>The smoothing priors \(\alpha \ge 0\) accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting \(\alpha = 1\) is called Laplace smoothing, while \(\alpha &amp;lt; 1\) is called Lidstone smoothing.</source>
          <target state="translated">事前平滑化\（\ alpha \ ge 0 \）は、学習サンプルに存在しない特徴を考慮に入れ、以降の計算で確率がゼロになるのを防ぎます。\（\ alpha = 1 \）の設定はラプラス平滑化と呼ばれ、\（\ alpha &amp;lt;1 \）はリッドストーン平滑化と呼ばれます。</target>
        </trans-unit>
        <trans-unit id="4842d9fce84143997f4f4321a8717acf3b670822" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For L1 penalization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="translated">ソルバー「liblinear」は座標降下（CD）アルゴリズムを使用し、scikit-learnに付属する優れたC ++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEARライブラリ&lt;/a&gt;に依存しています。ただし、liblinearで実装されたCDアルゴリズムは、真の多項式（マルチクラス）モデルを学習できません。代わりに、最適化問題は「1対レスト」の方法で分解されるため、個別のバイナリ分類子がすべてのクラスに対してトレーニングされます。これは内部で発生するため、このソルバーを使用する&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;インスタンスはマルチクラス分類子として動作します。 L1 &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt; &lt;/a&gt;場合、sklearn.svm.l1_min_cは、Cの下限を計算して、非「null」（すべての機能の重みがゼロ）モデルを取得できます。</target>
        </trans-unit>
        <trans-unit id="26f79d036c2795565ba6a21b798f1790f5f71dab" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For \(\ell_1\) regularization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="translated">ソルバー「liblinear」は、座標降下（CD）アルゴリズムを使用し、scikit-learnに付属している優れたC ++ &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEARライブラリ&lt;/a&gt;に依存しています。ただし、liblinearに実装されたCDアルゴリズムは、真の多項（マルチクラス）モデルを学習できません。代わりに、最適化問題は「one-vs-rest」方式で分解されるため、すべてのクラスに対して個別のバイナリ分類器がトレーニングされます。これは内部で発生するため、このソルバーを使用する&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;インスタンスはマルチクラス分類子として動作します。 \（\ ell_1 \）正則化の場合、&lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt; を&lt;/a&gt;使用すると、非「null」（すべての特徴の重みがゼロ）モデルを取得するためにCの下限を計算できます。</target>
        </trans-unit>
        <trans-unit id="ed7150245f4b6e455142137cba3f8af716071c7a" translate="yes" xml:space="preserve">
          <source>The solver for weight optimization.</source>
          <target state="translated">重量最適化のためのソルバーです。</target>
        </trans-unit>
        <trans-unit id="7622dc087d4f210bd1e051afc82030dfdfb796a8" translate="yes" xml:space="preserve">
          <source>The solver is selected by a default policy based on &lt;code&gt;X.shape&lt;/code&gt; and &lt;code&gt;n_components&lt;/code&gt;: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient &amp;lsquo;randomized&amp;rsquo; method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards.</source>
          <target state="translated">ソルバーは、 &lt;code&gt;X.shape&lt;/code&gt; と &lt;code&gt;n_components&lt;/code&gt; に基づくデフォルトのポリシーによって選択されます。入力データが500x500より大きく、抽出するコンポーネントの数がデータの最小次元の80％未満の場合、より効率的な 'ランダム化'メソッドが有効になります。それ以外の場合は、正確な完全なSVDが計算され、オプションで後で切り捨てられます。</target>
        </trans-unit>
        <trans-unit id="7be0c6e2677e69b218f8a95f391cfbcac99c36a1" translate="yes" xml:space="preserve">
          <source>The solvers implemented in the class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; are &amp;ldquo;liblinear&amp;rdquo;, &amp;ldquo;newton-cg&amp;rdquo;, &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;saga&amp;rdquo;:</source>
          <target state="translated">クラス&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; に&lt;/a&gt;実装されているソルバーは、「liblinear」、「newton-cg」、「lbfgs」、「sag」、「saga」です。</target>
        </trans-unit>
        <trans-unit id="2562620e10231c5093b1e84475f4d077e3e41917" translate="yes" xml:space="preserve">
          <source>The sought maximum memory for temporary distance matrix chunks. When None (default), the value of &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; is used.</source>
          <target state="translated">一時的な距離行列のチャンクに求められる最大メモリ。なし（デフォルト）の場合、 &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; の値が使用されます。</target>
        </trans-unit>
        <trans-unit id="d35f1b775d7d16bbabc524099db10f7048897e6d" translate="yes" xml:space="preserve">
          <source>The source can also be found &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;on Github&lt;/a&gt;.</source>
          <target state="translated">ソースは&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;Githubに&lt;/a&gt;もあります。</target>
        </trans-unit>
        <trans-unit id="351456aed7b7d0fc1d0034839135c70dde192f05" translate="yes" xml:space="preserve">
          <source>The source of this tutorial can be found within your scikit-learn folder:</source>
          <target state="translated">このチュートリアルのソースは scikit-learn フォルダ内にあります。</target>
        </trans-unit>
        <trans-unit id="aa3459bc05f5ce02810c5dff6ad6cfbeb1ca9a04" translate="yes" xml:space="preserve">
          <source>The spacing between points of the grid, in degrees</source>
          <target state="translated">グリッドの点間の間隔を度単位で指定します。</target>
        </trans-unit>
        <trans-unit id="464028094b69433f3db44d7a263916deccf86cb6" translate="yes" xml:space="preserve">
          <source>The sparse code factor in the matrix factorization.</source>
          <target state="translated">行列因数分解における疎な符号因子。</target>
        </trans-unit>
        <trans-unit id="f9d9c0a7d6ff3b6246478d922234360e15ed6ab9" translate="yes" xml:space="preserve">
          <source>The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-zero items (X).</source>
          <target state="translated">この行列の各列が正確に n_nonzero_coefs 非ゼロ項目(X)を持つような疎なコード。</target>
        </trans-unit>
        <trans-unit id="ee7d500960f94d10a937d21e436dcc2adbbc9a2a" translate="yes" xml:space="preserve">
          <source>The sparse codes</source>
          <target state="translated">疎なコード</target>
        </trans-unit>
        <trans-unit id="98979bbd88c557cc69074b91f3080d6fb357dded" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results from the dense implementation, due to a shrunk learning rate for the intercept. See &lt;a href=&quot;#implementation-details&quot;&gt;Implementation details&lt;/a&gt;.</source>
          <target state="translated">スパース実装は、インターセプトの学習率が低下するため、デンス実装とはわずかに異なる結果を生成します。参照&lt;a href=&quot;#implementation-details&quot;&gt;実装の詳細を&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0e1ce62165b4e0dbd0b6b1199e83c88e1c88959d" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results than the dense implementation due to a shrunk learning rate for the intercept.</source>
          <target state="translated">疎な実装では,切片の学習率が縮小されているため,密な実装とはわずかに異なる結果が得られます.</target>
        </trans-unit>
        <trans-unit id="e4890de0c56a0e7645deea9088355f84adac629f" translate="yes" xml:space="preserve">
          <source>The sparse vector</source>
          <target state="translated">疎なベクトル</target>
        </trans-unit>
        <trans-unit id="df0fd56f5b70016c4466d03cfc8859f7c3ea7663" translate="yes" xml:space="preserve">
          <source>The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself.</source>
          <target state="translated">スパーシティは実際には行列のコレスキー係数に課せられています.したがって,アルファは行列自体の充填率には直接変換されません.</target>
        </trans-unit>
        <trans-unit id="56d99fb198a4da207bde97c2ceeeb3653451f496" translate="yes" xml:space="preserve">
          <source>The sparsity-inducing \(\ell_1\) norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter &lt;code&gt;alpha&lt;/code&gt;. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero.</source>
          <target state="translated">スパース性を誘発する\（\ ell_1 \）ノルムはまた、利用可能なトレーニングサンプルが少ない場合にノイズからコンポーネントを学習することを防ぎます。ペナルティの程度（およびスパース性）は、ハイパーパラメーター &lt;code&gt;alpha&lt;/code&gt; を使用して調整できます。値が小さいと緩やかに正則化された因数分解が行われ、値が大きいと多くの係数がゼロに縮小されます。</target>
        </trans-unit>
        <trans-unit id="1e3f3420100e85d456b50524681a1e1c7bbc338a" translate="yes" xml:space="preserve">
          <source>The split code for a single sample has length &lt;code&gt;2 * n_components&lt;/code&gt; and is constructed using the following rule: First, the regular code of length &lt;code&gt;n_components&lt;/code&gt; is computed. Then, the first &lt;code&gt;n_components&lt;/code&gt; entries of the &lt;code&gt;split_code&lt;/code&gt; are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative.</source>
          <target state="translated">単一のサンプルの分割コードの長さは &lt;code&gt;2 * n_components&lt;/code&gt; で、次のルールを使用して作成されます。最初に、長さ &lt;code&gt;n_components&lt;/code&gt; の通常のコードが計算されます。そして、第1 &lt;code&gt;n_components&lt;/code&gt; 用のエントリ &lt;code&gt;split_code&lt;/code&gt; は、通常のコードベクトルの正の部分が充填されています。分割コードの後半は、コードベクトルの負の部分で満たされ、正の符号のみが含まれます。したがって、split_codeは負ではありません。</target>
        </trans-unit>
        <trans-unit id="c55afca5a7a433d8d42b73a1df4af26f8dc38160" translate="yes" xml:space="preserve">
          <source>The stacked regressor will combine the strengths of the different regressors. However, we also see that training the stacked regressor is much more computationally expensive.</source>
          <target state="translated">積層型レグレッサーは、異なるレグレッサーの長所を組み合わせます。しかし、スタック型レグレッサーを学習すると計算量が多くなることもわかります。</target>
        </trans-unit>
        <trans-unit id="619ea10ad996c929a97e389d99c020b2211157d4" translate="yes" xml:space="preserve">
          <source>The standard LLE algorithm comprises three stages:</source>
          <target state="translated">標準的なLLEアルゴリズムは3つのステージから構成されています。</target>
        </trans-unit>
        <trans-unit id="a5bdc246aecc7e3bec9121428d3f9c450814299f" translate="yes" xml:space="preserve">
          <source>The standard deviation of the clusters.</source>
          <target state="translated">クラスターの標準偏差。</target>
        </trans-unit>
        <trans-unit id="b61516300476f785958503bfbc63e8e96de11d18" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise applied to the output.</source>
          <target state="translated">出力に適用されるガウスノイズの標準偏差。</target>
        </trans-unit>
        <trans-unit id="e5c05e9131e22b6718043e99a2bae1b92b538981" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise.</source>
          <target state="translated">ガウスノイズの標準偏差。</target>
        </trans-unit>
        <trans-unit id="0d973eb25a07dcdadcaeac90be3869c3ac64dae0" translate="yes" xml:space="preserve">
          <source>The standard score of a sample &lt;code&gt;x&lt;/code&gt; is calculated as:</source>
          <target state="translated">サンプル &lt;code&gt;x&lt;/code&gt; の標準スコアは次のように計算されます。</target>
        </trans-unit>
        <trans-unit id="3689d32cff3e62dded279fb6b657e94942cdc7dc" translate="yes" xml:space="preserve">
          <source>The stepwise interpolating function that covers the input domain &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">入力ドメイン &lt;code&gt;X&lt;/code&gt; をカバーする段階的な内挿関数。</target>
        </trans-unit>
        <trans-unit id="a2cf8fe1dea678ae31186ca36392deb4d997cfe8" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol).</source>
          <target state="translated">停止基準。Noneでない場合、反復は（loss&amp;gt; previous_loss --tol）のときに停止します。</target>
        </trans-unit>
        <trans-unit id="4d8cf8d82bcb0bc25727f81a7a8d626fa0a70639" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.</source>
          <target state="translated">停止基準。Noneでない場合、（loss&amp;gt; previous_loss-tol）のときに反復が停止します。デフォルトはNoneです。デフォルトは0.21から1e-3です。</target>
        </trans-unit>
        <trans-unit id="b0435766401c2c671f3d0dbc7668ffa0a2dc3aba" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, training will stop when (loss &amp;gt; best_loss - tol) for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs.</source>
          <target state="translated">停止基準。Noneでない場合、 &lt;code&gt;n_iter_no_change&lt;/code&gt; の連続するエポックの場合（loss&amp;gt; best_loss --tol）にトレーニングが停止します。</target>
        </trans-unit>
        <trans-unit id="ba1aa4a75fb51f2479b447aa001f5e6386a1f799" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization.</source>
          <target state="translated">埋め込み空間でラベルを割り当てるために使用する戦略。ラプラシアン埋め込み後にラベルを割り当てるには2つの方法がありますが、k-meansを適用することができ、一般的な選択です。しかし、初期化に敏感になることもあります。離散化は、ランダムな初期化の影響を受けにくいもう一つの方法です。</target>
        </trans-unit>
        <trans-unit id="0a02a9e0399d776c0fdc23972d432af0d079552e" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the &amp;lsquo;Multiclass spectral clustering&amp;rsquo; paper referenced below for more details on the discretization approach.</source>
          <target state="translated">埋め込みスペースでラベルを割り当てるために使用する戦略。ラプラシアン埋め込み後にラベルを割り当てる方法は2つあります。k-meansは適用可能であり、一般的な選択肢です。ただし、初期化の影響を受けやすい場合もあります。離散化は、ランダムな初期化の影響を受けにくい別のアプローチです。離散化アプローチの詳細については、以下で参照される「マルチクラススペクトルクラスタリング」ペーパーを参照してください。</target>
        </trans-unit>
        <trans-unit id="067b32fbfa4aafe8139a50a2cd0dc62d48f6ce7c" translate="yes" xml:space="preserve">
          <source>The strategy used to choose the split at each node. Supported strategies are &amp;ldquo;best&amp;rdquo; to choose the best split and &amp;ldquo;random&amp;rdquo; to choose the best random split.</source>
          <target state="translated">各ノードで分割を選択するために使用される戦略。サポートされている戦略は、最良の分割を選択する「最善」と、最良のランダム分割を選択する「ランダム」です。</target>
        </trans-unit>
        <trans-unit id="e4b87188ae01dfb2ea23e8aa9287a121677cbc35" translate="yes" xml:space="preserve">
          <source>The strength of recall versus precision in the F-score.</source>
          <target state="translated">Fスコアにおけるリコール対精度の強さ。</target>
        </trans-unit>
        <trans-unit id="9380c762fbcadf9826438d5ff503ef39938c2642" translate="yes" xml:space="preserve">
          <source>The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.</source>
          <target state="translated">LOFアルゴリズムの強みは、データセットのローカル特性とグローバル特性の両方を考慮に入れていることです。問題は、サンプルがどれだけ孤立しているかではなく、そのサンプルが周囲の近傍からどれだけ孤立しているかということです。</target>
        </trans-unit>
        <trans-unit id="ef559a266ab9339add9416970528df4288b9fe07" translate="yes" xml:space="preserve">
          <source>The string to decode</source>
          <target state="translated">デコードする文字列</target>
        </trans-unit>
        <trans-unit id="3b937bbc7005ed347df9fefe128290b88cb139ff" translate="yes" xml:space="preserve">
          <source>The string to decode.</source>
          <target state="translated">デコードする文字列。</target>
        </trans-unit>
        <trans-unit id="71af025de90c113777cd083adeb4a7dc41c643ae" translate="yes" xml:space="preserve">
          <source>The string value &amp;ldquo;auto&amp;rdquo; determines whether y should increase or decrease based on the Spearman correlation estimate&amp;rsquo;s sign.</source>
          <target state="translated">文字列値「auto」は、スピアマン相関推定の符号に基づいて、yを増減するかどうかを決定します。</target>
        </trans-unit>
        <trans-unit id="68913ceeaf791c0f89f5da0e6ea267a6c64604e5" translate="yes" xml:space="preserve">
          <source>The submatrix corresponding to bicluster i.</source>
          <target state="translated">バイクラスターiに対応する部分行列。</target>
        </trans-unit>
        <trans-unit id="a10436d8ac7a1230da5ccfb4c02351e0bfbb4701" translate="yes" xml:space="preserve">
          <source>The subset of drawn features for each base estimator.</source>
          <target state="translated">各ベース推定器の描画特徴量のサブセット。</target>
        </trans-unit>
        <trans-unit id="57675bd8615ef838a99f713d239feb9810e82664" translate="yes" xml:space="preserve">
          <source>The subset of drawn samples for each base estimator.</source>
          <target state="translated">各基底推定量のための描画サンプルのサブセット。</target>
        </trans-unit>
        <trans-unit id="fce75057d74e03086c8b482c64f0a007dbeb6ebe" translate="yes" xml:space="preserve">
          <source>The sum of all predictions also confirms the calibration issue of the &lt;code&gt;Ridge&lt;/code&gt; model: it under-estimates by more than 3% the total number of claims in the test set while the other three models can approximately recover the total number of claims of the test portfolio.</source>
          <target state="translated">すべての予測の合計は、 &lt;code&gt;Ridge&lt;/code&gt; モデルのキャリブレーションの問題も確認します。これは、テストセットのクレームの総数を3％以上過小評価しますが、他の3つのモデルはテストポートフォリオのクレームの総数をほぼ回復できます。 。</target>
        </trans-unit>
        <trans-unit id="58b06737b1ee81d6af2156c3790929db0bc0c464" translate="yes" xml:space="preserve">
          <source>The sum of the features (number of words if documents) is drawn from a Poisson distribution with this expected value.</source>
          <target state="translated">この期待値でポアソン分布から特徴量(文書の場合は単語数)の和を描きます。</target>
        </trans-unit>
        <trans-unit id="690aa5752a21a529c84a7d2bed72d6956dfbf2f1" translate="yes" xml:space="preserve">
          <source>The support is the number of occurrences of each class in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">サポートは、 &lt;code&gt;y_true&lt;/code&gt; の各クラスの出現回数です。</target>
        </trans-unit>
        <trans-unit id="b8f29f24bf343b8a8c6a9702bbb3373c30b3886a" translate="yes" xml:space="preserve">
          <source>The support vector machines in scikit-learn support both dense (&lt;code&gt;numpy.ndarray&lt;/code&gt; and convertible to that by &lt;code&gt;numpy.asarray&lt;/code&gt;) and sparse (any &lt;code&gt;scipy.sparse&lt;/code&gt;) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered &lt;code&gt;numpy.ndarray&lt;/code&gt; (dense) or &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (sparse) with &lt;code&gt;dtype=float64&lt;/code&gt;.</source>
          <target state="translated">scikit-learnのサポートベクターマシンは、入力として密（ &lt;code&gt;numpy.ndarray&lt;/code&gt; と &lt;code&gt;numpy.asarray&lt;/code&gt; によって変換可能）および疎（任意の &lt;code&gt;scipy.sparse&lt;/code&gt; ）サンプルベクトルの両方をサポートします。ただし、SVMを使用してスパースデータを予測するには、そのようなデータに適合している必要があります。最適なパフォーマンスを得るには、Cタイプの &lt;code&gt;numpy.ndarray&lt;/code&gt; （密）または &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; （疎）を &lt;code&gt;dtype=float64&lt;/code&gt; で使用します。</target>
        </trans-unit>
        <trans-unit id="b8e93494175fc764f9336519319d2c72af2d3469" translate="yes" xml:space="preserve">
          <source>The target features for which the partial dependecy should be computed (size should be smaller than 3 for visual renderings).</source>
          <target state="translated">部分依存性が計算されるべきターゲット特徴(視覚的なレンダリングの場合は3よりも小さいサイズでなければなりません)。</target>
        </trans-unit>
        <trans-unit id="1676d735c85e09d0307a407a6c8fc0482ea454c1" translate="yes" xml:space="preserve">
          <source>The target features for which to create the PDPs. If features[i] is an int or a string, a one-way PDP is created; if features[i] is a tuple, a two-way PDP is created. Each tuple must be of size 2. if any entry is a string, then it must be in &lt;code&gt;feature_names&lt;/code&gt;.</source>
          <target state="translated">PDPを作成する対象の機能。features [i]がintまたは文字列の場合、一方向PDPが作成されます。features [i]がタプルの場合、双方向PDPが作成されます。各タプルのサイズは2である必要があります。エントリが文字列の場合は、 &lt;code&gt;feature_names&lt;/code&gt; に含まれている必要があります。</target>
        </trans-unit>
        <trans-unit id="c6694d34ee3bb1ae73f71f475a9064ea6bb5aa61" translate="yes" xml:space="preserve">
          <source>The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.</source>
          <target state="translated">目標は、訓練セット内の最も近い隣人に関連付けられた目標を局所的に補間することで予測されます。</target>
        </trans-unit>
        <trans-unit id="fd8f31b4b903aeb83268043a3b52655fbcfeb540" translate="yes" xml:space="preserve">
          <source>The target labels (integer index).</source>
          <target state="translated">対象となるラベル(整数のインデックス)。</target>
        </trans-unit>
        <trans-unit id="9040adccd40d7354a58b523dff53abedde8f2d61" translate="yes" xml:space="preserve">
          <source>The target labels.</source>
          <target state="translated">ターゲットのラベル。</target>
        </trans-unit>
        <trans-unit id="836251639a1d2dd67d806e7910ad6656af016095" translate="yes" xml:space="preserve">
          <source>The target values (class labels in classification, real numbers in regression).</source>
          <target state="translated">目標値(分類ではクラスラベル、回帰では実数)。</target>
        </trans-unit>
        <trans-unit id="581ab16d01401b52de3a8a770522ae3215b0d2a3" translate="yes" xml:space="preserve">
          <source>The target values (class labels) as integers or strings.</source>
          <target state="translated">対象となる値(クラスラベル)を整数または文字列として指定します。</target>
        </trans-unit>
        <trans-unit id="c42151a6b9abff52d7ec8ba07a5200409a48a3c6" translate="yes" xml:space="preserve">
          <source>The target values (class labels).</source>
          <target state="translated">対象となる値(クラスラベル)。</target>
        </trans-unit>
        <trans-unit id="af111c076ceef9b210aa6e236368271feda238ba" translate="yes" xml:space="preserve">
          <source>The target values (integers that correspond to classes in classification, real numbers in regression).</source>
          <target state="translated">目標値(分類ではクラスに対応する整数、回帰では実数)。</target>
        </trans-unit>
        <trans-unit id="94e263fd180ba7303394b0318de33e42ec7bd528" translate="yes" xml:space="preserve">
          <source>The target values (real numbers).</source>
          <target state="translated">目標値(実数)です。</target>
        </trans-unit>
        <trans-unit id="4f52bd7c58bfce68aafb35b3e2f5dd531ddc572f" translate="yes" xml:space="preserve">
          <source>The target values (real numbers). Use &lt;code&gt;dtype=np.float64&lt;/code&gt; and &lt;code&gt;order='C'&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">ターゲット値（実数）。使用 &lt;code&gt;dtype=np.float64&lt;/code&gt; 及び &lt;code&gt;order='C'&lt;/code&gt; 最大効率のために。</target>
        </trans-unit>
        <trans-unit id="53a447e32fbe96637e9d7be27a641b24353eba6e" translate="yes" xml:space="preserve">
          <source>The target values.</source>
          <target state="translated">目標値です。</target>
        </trans-unit>
        <trans-unit id="863f0df40c91ba7090e8eb769163050f217d7bc5" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems.</source>
          <target state="translated">教師付き学習問題の対象変数。</target>
        </trans-unit>
        <trans-unit id="f60efda845afa3f0dc7adc040cd9b2450fbcc2aa" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems. Stratification is done based on the y labels.</source>
          <target state="translated">教師付き学習問題の対象変数。層化はyラベルに基づいて行われる.</target>
        </trans-unit>
        <trans-unit id="aa20ad6bc67663b59dda9b76a4a065ca1f86af8f" translate="yes" xml:space="preserve">
          <source>The target variable is the median house value for California districts.</source>
          <target state="translated">対象変数は、カリフォルニア州の地区の住宅価値の中央値である。</target>
        </trans-unit>
        <trans-unit id="40a696d29fc2e13fd302babe7b309a6f769a208a" translate="yes" xml:space="preserve">
          <source>The target variable to try to predict in the case of supervised learning.</source>
          <target state="translated">教師付き学習の場合に予測しようとする対象変数。</target>
        </trans-unit>
        <trans-unit id="a34872d2673c11b79098f51c73d69310c6a49da3" translate="yes" xml:space="preserve">
          <source>The task at hand is to predict disease progression from physiological variables.</source>
          <target state="translated">目下の課題は、生理学的変数から疾患の進行を予測することである。</target>
        </trans-unit>
        <trans-unit id="609ac3008273ee503e4809fb5a54a64b3f8be85d" translate="yes" xml:space="preserve">
          <source>The ten features are standard independent Gaussian and the target &lt;code&gt;y&lt;/code&gt; is defined by:</source>
          <target state="translated">10個の特徴は標準の独立ガウス分布であり、ターゲット &lt;code&gt;y&lt;/code&gt; は次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="b7581ec7a4d469cfac096612cf94e3958274d6aa" translate="yes" xml:space="preserve">
          <source>The term &amp;ldquo;discrete features&amp;rdquo; is used instead of naming them &amp;ldquo;categorical&amp;rdquo;, because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that.</source>
          <target state="translated">「離散的特徴」という用語は、本質をより正確に説明するため、「カテゴリー的」という名前の代わりに使用されます。たとえば、画像のピクセル強度は個別の特徴であり（ほとんどカテゴリではありません）、そのようにマークを付けるとより良い結果が得られます。また、連続変数を離散として、またはその逆として扱うと、通常は正しくない結果が得られるので注意してください。</target>
        </trans-unit>
        <trans-unit id="2014676021228002e8224c1f06ee12bb4596d24b" translate="yes" xml:space="preserve">
          <source>The term \((x-\mu_k)^t \Sigma^{-1} (x-\mu_k)\) corresponds to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mahalanobis_distance&quot;&gt;Mahalanobis Distance&lt;/a&gt; between the sample \(x\) and the mean \(\mu_k\). The Mahalanobis distance tells how close \(x\) is from \(\mu_k\), while also accounting for the variance of each feature. We can thus interpret LDA as assigning \(x\) to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities.</source>
          <target state="translated">用語\（（x- \ mu_k）^ t \ Sigma ^ {-1}（x- \ mu_k）\）は、サンプル\（x \）と平均\（\ mu_k \）の間の&lt;a href=&quot;https://en.wikipedia.org/wiki/Mahalanobis_distance&quot;&gt;マハラノビス距離に&lt;/a&gt;対応します。マハラノビス距離は、\（x \）が\（\ mu_k \）からどれだけ近いかを示し、各特徴の分散も考慮します。したがって、LDAは、クラスの事前確率も考慮しながら、平均がマハラノビス距離の観点から最も近いクラスに\（x \）を割り当てるものとして解釈できます。</target>
        </trans-unit>
        <trans-unit id="1748689c159b7c280435a293a84c60a8fa11ddf6" translate="yes" xml:space="preserve">
          <source>The test points for the data. Same format as the training data.</source>
          <target state="translated">データのテストポイント。訓練データと同じ形式。</target>
        </trans-unit>
        <trans-unit id="c4412981e13c1e09172f9e595c07a27a82a32abb" translate="yes" xml:space="preserve">
          <source>The testing set indices for that split.</source>
          <target state="translated">その分割のためのテストセットのインデックス。</target>
        </trans-unit>
        <trans-unit id="c079148b8f468ab34a1c911c5b93e7fb1e4fbf43" translate="yes" xml:space="preserve">
          <source>The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; takes an &lt;code&gt;encoding&lt;/code&gt; parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (&lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt;).</source>
          <target state="translated">scikit-learnのテキスト機能エクストラクターは、テキストファイルをデコードする方法を知っていますが、ファイルのエンコーディングをユーザーに通知する場合に限られます&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;は、この目的のために &lt;code&gt;encoding&lt;/code&gt; パラメーターを取ります。最新のテキストファイルの場合、正しいエンコーディングはおそらくUTF-8です。したがって、これがデフォルトです（ &lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="5da204cc914d9d1b370d2a7e3d3f9fed2399ad38" translate="yes" xml:space="preserve">
          <source>The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow.</source>
          <target state="translated">予測の一貫性を得るためには、サンプル数の増加に応じてペナルティパラメータを一定に保つ必要があるという理論です。</target>
        </trans-unit>
        <trans-unit id="ed8c4048d538066672a56cef623687584e4d51a1" translate="yes" xml:space="preserve">
          <source>The third figure compares kernel density estimates for a distribution of 100 samples in 1 dimension. Though this example uses 1D distributions, kernel density estimation is easily and efficiently extensible to higher dimensions as well.</source>
          <target state="translated">3番目の図は、1次元の100サンプルの分布のカーネル密度推定値を比較したものです。この例では1次元分布を使用していますが、カーネル密度推定は高次元にも簡単かつ効率的に拡張できます。</target>
        </trans-unit>
        <trans-unit id="0742a25a1bc73abd7670f1b33a5e8f933c99aa55" translate="yes" xml:space="preserve">
          <source>The third model is also a Bayesian Gaussian mixture model with a Dirichlet process prior but this time the value of the concentration prior is higher giving the model more liberty to model the fine-grained structure of the data. The result is a mixture with a larger number of active components that is similar to the first model where we arbitrarily decided to fix the number of components to 10.</source>
          <target state="translated">3番目のモデルは、同じくディリクレ過程先行を持つベイズ・ガウス混合モデルですが、今回は濃度先行の値がより高くなっており、データの微細な構造をモデルに与える自由度が高くなっています。結果は、我々が任意に成分数を10に固定することを決定した最初のモデルと同様に、より多くの活性成分を持つ混合物です。</target>
        </trans-unit>
        <trans-unit id="f1f7cd9ed7ac82273a71a8430edb1e09f61b8cab" translate="yes" xml:space="preserve">
          <source>The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If &amp;ldquo;median&amp;rdquo; (resp. &amp;ldquo;mean&amp;rdquo;), then the &lt;code&gt;threshold&lt;/code&gt; value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., &amp;ldquo;1.25*mean&amp;rdquo;) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, &amp;ldquo;mean&amp;rdquo; is used by default.</source>
          <target state="translated">機能の選択に使用するしきい値。重要性がそれ以上の機能は保持され、他の機能は破棄されます。「中央値」（または「平均」）の場合、 &lt;code&gt;threshold&lt;/code&gt; は機能の重要度の中央値（または平均）です。スケーリング係数（たとえば、「1.25 *平均」）も使用できます。Noneで、推定量のパラメーターペナルティがl1に明示的または暗黙的に（たとえばLasso）設定されている場合、使用されるしきい値は1e-5です。それ以外の場合は、デフォルトで「平均」が使用されます。</target>
        </trans-unit>
        <trans-unit id="91a1a785c1bc7a11d4e20e6e119c885bf4142111" translate="yes" xml:space="preserve">
          <source>The threshold value used for feature selection.</source>
          <target state="translated">特徴の選択に使用される閾値。</target>
        </trans-unit>
        <trans-unit id="5a48126d50b83afd18e220a4fcf0cc7ffc7180d9" translate="yes" xml:space="preserve">
          <source>The time complexity of this implementation is &lt;code&gt;O(d ** 2)&lt;/code&gt; assuming d ~ n_features ~ n_components.</source>
          <target state="translated">この実装の時間の複雑さは、d〜n_features〜n_componentsを想定した &lt;code&gt;O(d ** 2)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="ff8097e0ec52614ae168c4cd444f04e3f78b14e2" translate="yes" xml:space="preserve">
          <source>The time for fitting the estimator on the train set for each cv split.</source>
          <target state="translated">cv 分割ごとの列車集合に推定器を適合させるための時間.</target>
        </trans-unit>
        <trans-unit id="dc0535c66e14595ad26a449dc475b0e83c5091ba" translate="yes" xml:space="preserve">
          <source>The time for scoring the estimator on the test set for each cv split. (Note time for scoring on the train set is not included even if &lt;code&gt;return_train_score&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">各CV分割のテストセットで推定量をスコアリングする時間。（ &lt;code&gt;return_train_score&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されている場合でも、トレインセットのスコアリングの時間は含まれません。</target>
        </trans-unit>
        <trans-unit id="1cd8d7a025bee860396ab665c8f7316a009f2f5a" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;.</source>
          <target state="translated">降下方向の計算に使用されるエラスティックネットソルバーの許容誤差。このパラメーターは、全体的なパラメーター推定値ではなく、特定の列更新の検索方向の精度を制御します。mode = 'cd'にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="1c748ca3956e12feeccc0881efb2e7b986b52b0e" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;. Range is (0, inf].</source>
          <target state="translated">降下方向の計算に使用されるエラスティックネットソルバーの許容誤差。このパラメーターは、パラメーター推定全体ではなく、特定の列更新の検索方向の精度を制御します。mode = 'cd'にのみ使用されます。範囲は（0、inf]です。</target>
        </trans-unit>
        <trans-unit id="54800ee92ded7e21b226653650e94d2e245f5dc0" translate="yes" xml:space="preserve">
          <source>The tolerance for the optimization: if the updates are smaller than &lt;code&gt;tol&lt;/code&gt;, the optimization code checks the dual gap for optimality and continues until it is smaller than &lt;code&gt;tol&lt;/code&gt;.</source>
          <target state="translated">最適化の許容範囲：更新が &lt;code&gt;tol&lt;/code&gt; よりも小さい場合、最適化コードは最適化についてデュアルギャップをチェックし、 &lt;code&gt;tol&lt;/code&gt; より小さくなるまで続行します。</target>
        </trans-unit>
        <trans-unit id="7abdf76511114d1a589dafaf8c3b9fea94410d4e" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped.</source>
          <target state="translated">収束を宣言するための許容範囲:デュアルギャップがこの値を下回った場合、反復処理を停止します。</target>
        </trans-unit>
        <trans-unit id="c8679f3dc5795753fd4b3924c7477d6451516504" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. Range is (0, inf].</source>
          <target state="translated">収束を宣言するための許容範囲:デュアルギャップがこの値を下回った場合、反復処理を停止します。範囲は(0,inf)です。</target>
        </trans-unit>
        <trans-unit id="1174cbfc6c5cd8bb9e51e269de526360c060c73a" translate="yes" xml:space="preserve">
          <source>The tomography projection operation is a linear transformation. In addition to the data-fidelity term corresponding to a linear regression, we penalize the L1 norm of the image to account for its sparsity. The resulting optimization problem is called the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;. We use the class &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt;&lt;/a&gt;, that uses the coordinate descent algorithm. Importantly, this implementation is more computationally efficient on a sparse matrix, than the projection operator used here.</source>
          <target state="translated">トモグラフィー投影操作は線形変換です。線形回帰に対応するデータ忠実度の項に加えて、画像のL1ノルムにペナルティを課して、スパース性を考慮します。結果として生じる最適化問題は、&lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;投げ縄&lt;/a&gt;と呼ばれます。座標降下アルゴリズムを使用するクラス&lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt; &lt;/a&gt;を使用します。重要なのは、この実装は、ここで使用されている射影演算子よりも、スパース行列での計算効率が高いことです。</target>
        </trans-unit>
        <trans-unit id="6067d7bbaca20238e188da5c4c1f4f9b915cbe46" translate="yes" xml:space="preserve">
          <source>The total number of features.</source>
          <target state="translated">機能の総数。</target>
        </trans-unit>
        <trans-unit id="52b059ac9ef2b76cd7b57a438578db960faf18a1" translate="yes" xml:space="preserve">
          <source>The total number of features. These comprise &lt;code&gt;n_informative&lt;/code&gt; informative features, &lt;code&gt;n_redundant&lt;/code&gt; redundant features, &lt;code&gt;n_repeated&lt;/code&gt; duplicated features and &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; useless features drawn at random.</source>
          <target state="translated">機能の総数。これらは、 &lt;code&gt;n_informative&lt;/code&gt; されるn_informative有益な機能、 &lt;code&gt;n_redundant&lt;/code&gt; 冗長機能、 &lt;code&gt;n_repeated&lt;/code&gt; 複製機能、および &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; 役に立たない機能で構成されます。</target>
        </trans-unit>
        <trans-unit id="1b95194d002b1fa90370f5ac460e11ae527d46c4" translate="yes" xml:space="preserve">
          <source>The total number of input features.</source>
          <target state="translated">入力特徴量の総数。</target>
        </trans-unit>
        <trans-unit id="75f4c14304ea0320f6751402bd1abbcf764fb2d4" translate="yes" xml:space="preserve">
          <source>The total number of points equally divided among classes.</source>
          <target state="translated">クラス間で等分されたポイント数の合計。</target>
        </trans-unit>
        <trans-unit id="6fd1a66b2c352f2c105828a73ec9201a20677da3" translate="yes" xml:space="preserve">
          <source>The total number of points generated.</source>
          <target state="translated">生成されたポイントの合計数です。</target>
        </trans-unit>
        <trans-unit id="b463685bc7194f4e1bd9c9e9566855d8af2442c3" translate="yes" xml:space="preserve">
          <source>The total number of points generated. If odd, the inner circle will have one point more than the outer circle.</source>
          <target state="translated">生成されたポイントの総数です。奇数の場合、内側の円は外側の円よりも1点多くなります。</target>
        </trans-unit>
        <trans-unit id="30df1d74b9688e22831b35a50354a3af5ea3a9b9" translate="yes" xml:space="preserve">
          <source>The total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features.</source>
          <target state="translated">多項式出力特徴量の総数。出力特徴量の数は,入力特徴量のすべての適切なサイズの組み合わせを反復して計算されます.</target>
        </trans-unit>
        <trans-unit id="2341cd15b4f720c4e4ca39627124f453602ba043" translate="yes" xml:space="preserve">
          <source>The traditional way to compute the principal eigenvector is to use the power iteration method:</source>
          <target state="translated">主固有ベクトルを計算する伝統的な方法は、パワーイテレーション法を使用しています。</target>
        </trans-unit>
        <trans-unit id="486bc8b3be25b906a521777296790c0e7db3392b" translate="yes" xml:space="preserve">
          <source>The training algorithm implemented in &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt; is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; に&lt;/a&gt;実装されているトレーニングアルゴリズムは、確率的最尤（SML）または持続的コントラスト発散（PCD）として知られています。最大尤度を直接最適化することは、データ尤度の形式のために実行不可能です。</target>
        </trans-unit>
        <trans-unit id="03936b9df8c7a04402a186159fb53bde128b3907" translate="yes" xml:space="preserve">
          <source>The training data</source>
          <target state="translated">学習データ</target>
        </trans-unit>
        <trans-unit id="a7dac9ee1d012d3a04e96a076bd4bb0255f4fa3a" translate="yes" xml:space="preserve">
          <source>The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.</source>
          <target state="translated">訓練データは,他のものから離れたオブザベーションとして定義される外れ値を含む.したがって,外れ値検出推定器は,逸脱したオブザベーションを無視して,訓練データが最も集中している領域に適合しようとする.</target>
        </trans-unit>
        <trans-unit id="58b640148d7ae1e7f191ee9d800aaef902a6aef0" translate="yes" xml:space="preserve">
          <source>The training data is not polluted by outliers and we are interested in detecting whether a &lt;strong&gt;new&lt;/strong&gt; observation is an outlier. In this context an outlier is also called a novelty.</source>
          <target state="translated">トレーニングデータは外れ値によって汚染されておらず、&lt;strong&gt;新しい&lt;/strong&gt;観測値が外れ値であるかどうかを検出することに関心があります。この文脈では、外れ値は新規性とも呼ばれます。</target>
        </trans-unit>
        <trans-unit id="431a9d8e158a3b53d56cef19b322f4a781297a84" translate="yes" xml:space="preserve">
          <source>The training data, e.g. a reference to an immutable snapshot</source>
          <target state="translated">学習データ、例えば不変のスナップショットへの参照</target>
        </trans-unit>
        <trans-unit id="44e706b0239b6ac2fad3df8ff059d735dbfbf296" translate="yes" xml:space="preserve">
          <source>The training input samples.</source>
          <target state="translated">学習入力サンプル。</target>
        </trans-unit>
        <trans-unit id="92c6c3d94fd9608db44f0ede45f9bbe4de664d1d" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">トレーニング入力サンプル。内部的に、それは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換され、疎行列が疎 &lt;code&gt;csc_matrix&lt;/code&gt; に提供される場合。</target>
        </trans-unit>
        <trans-unit id="fdce812a7f2c7f82334342af14e6e75d01b61ef1" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">トレーニング入力サンプル。内部的には、そのdtypeは &lt;code&gt;dtype=np.float32&lt;/code&gt; に変換されます。スパース行列が指定されている場合は、スパース &lt;code&gt;csc_matrix&lt;/code&gt; に変換されます。</target>
        </trans-unit>
        <trans-unit id="549582a79c03c6cf3a88a3f5ae8ab3477fe6f4f0" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.</source>
          <target state="translated">訓練入力サンプル。疎な行列は、基底推定器でサポートされている場合にのみ受け入れられる。</target>
        </trans-unit>
        <trans-unit id="cc9cba55947c2b2da5cd927ed2d1f239e6fce1e8" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.</source>
          <target state="translated">学習入力サンプル.スパース行列は、CSC,CSR,COO,DOK,LIL のいずれかになります。COO,DOK,LIL は CSR に変換される.</target>
        </trans-unit>
        <trans-unit id="ed9fac6749e01b46bdd0e4eebbecd7ed42fc3bac" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.</source>
          <target state="translated">学習入力サンプル.スパース行列は、CSC,CSR,COO,DOK,LIL のいずれかになります。DOK と LIL は CSR に変換されます.</target>
        </trans-unit>
        <trans-unit id="dba95905d10a3a6d61cb924560b16f35840a95de" translate="yes" xml:space="preserve">
          <source>The training points for the data. Each point has three fields:</source>
          <target state="translated">データのトレーニングポイントです。各ポイントには3つのフィールドがあります。</target>
        </trans-unit>
        <trans-unit id="768a232e21b799d7ab4a21a202aa20c0c2da04e0" translate="yes" xml:space="preserve">
          <source>The training samples.</source>
          <target state="translated">訓練用のサンプルです。</target>
        </trans-unit>
        <trans-unit id="49afe52f19d67077f586b0d86a3a80bb7b9051b3" translate="yes" xml:space="preserve">
          <source>The training set has size &lt;code&gt;i * n_samples // (n_splits + 1)
+ n_samples % (n_splits + 1)&lt;/code&gt; in the &lt;code&gt;i``th split,
with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">トレーニングセットのサイズは、 &lt;code&gt;i * n_samples // (n_splits + 1) + n_samples % (n_splits + 1)&lt;/code&gt; で、 &lt;code&gt;i``th split, with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt; あり、 &lt;code&gt;n_samples&lt;/code&gt; サンプル数です。</target>
        </trans-unit>
        <trans-unit id="7d0d65f5b4df90d39c92efe2c541dfa70477a3f3" translate="yes" xml:space="preserve">
          <source>The training set indices for that split.</source>
          <target state="translated">その分割のためのトレーニングセットのインデックス。</target>
        </trans-unit>
        <trans-unit id="1fe1c9ebc66c3c60358a4b7d7ce6958b71f5be6a" translate="yes" xml:space="preserve">
          <source>The transformation can be triggered by setting either &lt;code&gt;transformer&lt;/code&gt; or the pair of functions &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt;. However, setting both options will raise an error.</source>
          <target state="translated">変換は、 &lt;code&gt;transformer&lt;/code&gt; または関数 &lt;code&gt;func&lt;/code&gt; と &lt;code&gt;inverse_func&lt;/code&gt; ペアを設定することでトリガーできます。ただし、両方のオプションを設定するとエラーが発生します。</target>
        </trans-unit>
        <trans-unit id="cb596196fe310821d43e26f57899432a8af2c024" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="translated">変換は,各特徴量に対して独立して適用されます.まず,特徴量の累積分布関数の推定値が,元の値を一様分布にマッピングするために使用される.次に,得られた値が,関連する分位関数を使用して,所望の出力分布にマッピングされます.フィットされた範囲よりも下または上にある新しい/未見のデータの特徴値は,出力分布の境界にマップされます.この変換は非線形であることに注意してください。これは,同じスケールで測定された変数間の線形相関を歪めるかもしれないが,異なるスケールで測定された変数をより直接的に比較可能にする.</target>
        </trans-unit>
        <trans-unit id="c1ce2cd56f04ea729576a873f5ef18af794ea62c" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. The cumulative density function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="translated">変換は、各特徴量に対して独立して適用されます。特徴量の累積密度関数は、元の値を投影するために使用されます。フィットされた範囲よりも下または上にある新しい/未見のデータの特徴量は,出力分布の境界にマッピングされます.この変換は非線形であることに注意してください。これは,同じスケールで測定された変数間の線形相関を歪めるかもしれないが,異なるスケールで測定された変数をより直接的に比較可能にする.</target>
        </trans-unit>
        <trans-unit id="019c47d3f3ad8207e8989b103cc42bdf22c099af" translate="yes" xml:space="preserve">
          <source>The transformation is calculated as (when &lt;code&gt;axis=0&lt;/code&gt;):</source>
          <target state="translated">変換は次のように計算されます（ &lt;code&gt;axis=0&lt;/code&gt; の場合）。</target>
        </trans-unit>
        <trans-unit id="ac658687b2a734f60273ed8ccd4ce7f47f7a43fa" translate="yes" xml:space="preserve">
          <source>The transformation is given by (when &lt;code&gt;axis=0&lt;/code&gt;):</source>
          <target state="translated">変換は次の式で与えられます（ &lt;code&gt;axis=0&lt;/code&gt; の場合）。</target>
        </trans-unit>
        <trans-unit id="ddd72b82186574150d900e6dfd8721345fa62073" translate="yes" xml:space="preserve">
          <source>The transformation is given by:</source>
          <target state="translated">変換は次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="1db1e441acce63afd6d696933447c0b2d453bb8c" translate="yes" xml:space="preserve">
          <source>The transformed data</source>
          <target state="translated">変換されたデータ</target>
        </trans-unit>
        <trans-unit id="f7cd2d51dfcb247e9b3665d7843f0082b5c854bd" translate="yes" xml:space="preserve">
          <source>The transformed data is a sparse graph as returned by kneighbors_graph.</source>
          <target state="translated">変換されたデータは kneighbors_graph が返す疎なグラフです。</target>
        </trans-unit>
        <trans-unit id="5e2ab39a27e391675e94304e8d0476203d9b90e4" translate="yes" xml:space="preserve">
          <source>The transformed data is a sparse graph as returned by radius_neighbors_graph.</source>
          <target state="translated">変換されたデータは radius_neighbors_graph で返される疎なグラフです。</target>
        </trans-unit>
        <trans-unit id="0a52b9359f518099e81b711eeaccc5e0c7c17eb0" translate="yes" xml:space="preserve">
          <source>The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.</source>
          <target state="translated">変換されたデータはナイーブベイズ分類器の訓練に使用され、PCAの前にスケーリングされたデータセットは、スケーリングされていないバージョンを大幅に上回るという、予測精度の明確な違いが観察されます。</target>
        </trans-unit>
        <trans-unit id="80e852f03673351dd5ee00932a57dc4a209cdf2d" translate="yes" xml:space="preserve">
          <source>The transformed data.</source>
          <target state="translated">変換されたデータです。</target>
        </trans-unit>
        <trans-unit id="831c10597508e086776cd00760f56c708f8d2bba" translate="yes" xml:space="preserve">
          <source>The tree algorithm to use. Valid options are [&amp;lsquo;kd_tree&amp;rsquo;|&amp;rsquo;ball_tree&amp;rsquo;|&amp;rsquo;auto&amp;rsquo;]. Default is &amp;lsquo;auto&amp;rsquo;.</source>
          <target state="translated">使用するツリーアルゴリズム。有効なオプションは['kd_tree' | 'ball_tree' | 'auto']です。デフォルトは「auto」です。</target>
        </trans-unit>
        <trans-unit id="d745501ed4c4af3a67688bd307560f44fb29c904" translate="yes" xml:space="preserve">
          <source>The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node.</source>
          <target state="translated">木データ構造は、各ノードがいくつかのサブクラスターからなるノードで構成されています。ノード内のサブクラスターの最大数は分岐係数によって決定されます。各サブクラスタは、線形和、二乗和、およびそのサブクラスタ内のサンプル数を保持する。さらに、各サブクラスタは、サブクラスタがリーフノードのメンバーでない場合、その子としてノードを持つこともできる。</target>
        </trans-unit>
        <trans-unit id="e124a1b9cc4902e35946b8c1931bf2b92e9cefc7" translate="yes" xml:space="preserve">
          <source>The tree-based model is significantly better at ranking policyholders by risk while the two linear models perform similarly.</source>
          <target state="translated">ツリーベースのモデルは、2つの線形モデルが同様の性能を発揮するのに対し、リスク別の契約者のランク付けでは有意に優れています。</target>
        </trans-unit>
        <trans-unit id="fbc6ef1abc08faa2debda525a78475d34850618e" translate="yes" xml:space="preserve">
          <source>The true probability in each bin (fraction of positives).</source>
          <target state="translated">各ビンの真の確率(陽性の割合)。</target>
        </trans-unit>
        <trans-unit id="89aec1004fa9767eced66561c3ed3161000c45a1" translate="yes" xml:space="preserve">
          <source>The true score without permuting targets.</source>
          <target state="translated">ターゲットをパーマネントしない真のスコア。</target>
        </trans-unit>
        <trans-unit id="78ab726a9fdf47809b691c3859f0df06c5754377" translate="yes" xml:space="preserve">
          <source>The trustworthiness is within [0, 1]. It is defined as</source>
          <target state="translated">信頼度は[0,1]の範囲内である。として定義されています。</target>
        </trans-unit>
        <trans-unit id="311da69a1a719737450cc586668ba277c4bde011" translate="yes" xml:space="preserve">
          <source>The tutorial folder should contain the following sub-folders:</source>
          <target state="translated">チュートリアルフォルダには、以下のサブフォルダが含まれている必要があります。</target>
        </trans-unit>
        <trans-unit id="2e8b6623a8185bbb598cfdf6b3f71f1ee2c2a837" translate="yes" xml:space="preserve">
          <source>The two figures below plot the values of &lt;code&gt;C&lt;/code&gt; on the &lt;code&gt;x-axis&lt;/code&gt; and the corresponding cross-validation scores on the &lt;code&gt;y-axis&lt;/code&gt;, for several different fractions of a generated data-set.</source>
          <target state="translated">以下の2つの図は、生成されたデータセットのいくつかの異なる部分について、 &lt;code&gt;x-axis&lt;/code&gt; に &lt;code&gt;C&lt;/code&gt; の値を、 &lt;code&gt;y-axis&lt;/code&gt; に対応する相互検証スコアをプロットしています。</target>
        </trans-unit>
        <trans-unit id="726ed01e2c2e9b244f4346dd4c7877757cc3867c" translate="yes" xml:space="preserve">
          <source>The two linear regressors &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt; now support sample weights.</source>
          <target state="translated">2つの線形&lt;a href=&quot;../../modules/generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt; &lt;code&gt;ElasticNet&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;とElasticNetがサンプルの重みをサポートするようになりました。</target>
        </trans-unit>
        <trans-unit id="85a91cdcd45557b2849d30b64bc99a3236c9a910" translate="yes" xml:space="preserve">
          <source>The two plots differ only in the area in the middle where the classes are tied. If &lt;code&gt;break_ties=False&lt;/code&gt;, all input in that area would be classified as one class, whereas if &lt;code&gt;break_ties=True&lt;/code&gt;, the tie-breaking mechanism will create a non-convex decision boundary in that area.</source>
          <target state="translated">2つのプロットは、クラスが結び付けられている中央の領域でのみ異なります。 &lt;code&gt;break_ties=False&lt;/code&gt; の場合、その領域のすべての入力は1つのクラスとして分類され &lt;code&gt;break_ties=True&lt;/code&gt; 、break_ties = Trueの場合、タイブレークメカニズムはその領域に非凸の決定境界を作成します。</target>
        </trans-unit>
        <trans-unit id="04a77900ed6f82fb8a154b287d08593a39a5648c" translate="yes" xml:space="preserve">
          <source>The two sample image.</source>
          <target state="translated">2つのサンプル画像です。</target>
        </trans-unit>
        <trans-unit id="3ed5804ea6261422d9ec471fffa8f8a41a307d64" translate="yes" xml:space="preserve">
          <source>The two species are:</source>
          <target state="translated">2種です。</target>
        </trans-unit>
        <trans-unit id="4aa36c5d8992165f7895075f7b16a37f9864b092" translate="yes" xml:space="preserve">
          <source>The type of criterion to use.</source>
          <target state="translated">使用する基準の種類。</target>
        </trans-unit>
        <trans-unit id="6bc8093d847369045cfca5abe49cd94f035f902d" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument.</source>
          <target state="translated">特徴量の型を指定します.Numpy array/scipy.sparse matrix constructors に dtype 引数として渡されます.</target>
        </trans-unit>
        <trans-unit id="51cef5c60b8823e16a67a0821a4b5311abdcd5ed" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.</source>
          <target state="translated">特徴量の型.dtype 引数として scipy.sparse matrix constructors に渡されます。bool、np.boolean、または任意の符号なし整数型には設定しないでください。</target>
        </trans-unit>
        <trans-unit id="25d83e9ee3b7a100418b9b6d2bb7936470b09825" translate="yes" xml:space="preserve">
          <source>The type of norm used to compute the error. Available error types: - &amp;lsquo;frobenius&amp;rsquo; (default): sqrt(tr(A^t.A)) - &amp;lsquo;spectral&amp;rsquo;: sqrt(max(eigenvalues(A^t.A)) where A is the error &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt;.</source>
          <target state="translated">エラーの計算に使用されるノルムのタイプ。利用可能なエラータイプ：-'frobenius'（デフォルト）：sqrt（tr（A ^ tA））-'spectral'：sqrt（max（eigenvalues（A ^ tA）））ここで、Aはエラー &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="8b60d3555697082fbd56cd614358874d006ccd25" translate="yes" xml:space="preserve">
          <source>The type of the hyperparameter. Currently, only &amp;ldquo;numeric&amp;rdquo; hyperparameters are supported.</source>
          <target state="translated">ハイパーパラメータのタイプ。現在、「数値」ハイパーパラメータのみがサポートされています。</target>
        </trans-unit>
        <trans-unit id="f0b5e941b7e074477ab9a734aa8fbc101aeb347c" translate="yes" xml:space="preserve">
          <source>The unchanged dictionary atoms</source>
          <target state="translated">変更されていない辞書の原子</target>
        </trans-unit>
        <trans-unit id="17cd05cd0020cc8838dc1f102077fa8c28f81758" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="translated">基盤となる&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;実装は、乱数ジェネレーターを使用して、モデルをデュアル座標降下法でフィッティングするとき（つまり、 &lt;code&gt;dual&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されているとき）に機能を選択します。したがって、同じ入力データに対してわずかに異なる結果が得られることは珍しくありません。その場合は、 &lt;code&gt;tol&lt;/code&gt; パラメータを小さくしてみてください。このランダム性は、 &lt;code&gt;random_state&lt;/code&gt; パラメーターを使用して制御することもできます。 &lt;code&gt;dual&lt;/code&gt; が &lt;code&gt;False&lt;/code&gt; に設定されている場合、&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;の基本的な実装はランダムではなく、 &lt;code&gt;random_state&lt;/code&gt; は結果に影響を与えません。</target>
        </trans-unit>
        <trans-unit id="776a52daee9321c0497d40c79109e87f34af6b7e" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="translated">基本となる&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;実装は、モデルをデュアル座標降下で近似するとき（つまり、 &lt;code&gt;dual&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; に設定されているとき）、乱数ジェネレータを使用して機能を選択します。したがって、同じ入力データに対してわずかに異なる結果が得られることは珍しくありません。その場合は、tolパラメータを小さくしてみてください。このランダム性は、 &lt;code&gt;random_state&lt;/code&gt; パラメータでも制御できます。 &lt;code&gt;dual&lt;/code&gt; が &lt;code&gt;False&lt;/code&gt; に設定されている場合、&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;の基本的な実装はランダムではなく、 &lt;code&gt;random_state&lt;/code&gt; は結果に影響を与えません。</target>
        </trans-unit>
        <trans-unit id="68c40938fee4ae1976f2ebe09bc5c5a404f12d37" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter.</source>
          <target state="translated">基礎となるCの実装では、モデルを近似するときに、乱数ジェネレータを使用して特徴を選択します。したがって、同じ入力データに対してわずかに異なる結果が得られることは珍しくありません。その場合は、 &lt;code&gt;tol&lt;/code&gt; パラメータを小さくしてみてください。</target>
        </trans-unit>
        <trans-unit id="2f30e1e8a65635cf877334fb55d57ec3cedb0460" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.</source>
          <target state="translated">基礎となるC言語の実装では、モデルのフィッティング時に特徴を選択するために乱数発生器を使用します。そのため、同じ入力データに対してわずかに異なる結果が出ることは珍しくありません。そのような場合は、tolパラメータを小さくしてみてください。</target>
        </trans-unit>
        <trans-unit id="4ed849766bc74a6b6038e401c289ce378db99dcc" translate="yes" xml:space="preserve">
          <source>The underlying Tree object. Please refer to &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; for attributes of Tree object and &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;Understanding the decision tree structure&lt;/a&gt; for basic usage of these attributes.</source>
          <target state="translated">基になるTreeオブジェクト。Treeオブジェクトの属性については、 &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; を参照してください。これらの属性の基本的な使用法&lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;については、意思決定ツリーの構造&lt;/a&gt;を理解してください。</target>
        </trans-unit>
        <trans-unit id="eaa9f70240ca573ce446579a8e3077ce3fd3b2de" translate="yes" xml:space="preserve">
          <source>The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.</source>
          <target state="translated">ベースとなる実装はMurmurHash3_x86_32で、ルックアップテーブル、ブルームフィルタ、カウントミニスケッチ、フィーチャーハッシュの実装に適した低レイテンシの32ビットハッシュを生成します。</target>
        </trans-unit>
        <trans-unit id="8b27403493b3e1cf67d088cabd49f20f60beabb5" translate="yes" xml:space="preserve">
          <source>The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy.</source>
          <target state="translated">基礎となる実装であるliblinearは、メモリコピーが発生するデータに対して疎な内部表現を使用しています。</target>
        </trans-unit>
        <trans-unit id="7c5718e72f8aabec82565a4976b3e3b79f8616d0" translate="yes" xml:space="preserve">
          <source>The unique classes labels.</source>
          <target state="translated">ユニークなクラスのラベル。</target>
        </trans-unit>
        <trans-unit id="22bad6cd0a0fca07f198954a6d755d20a0363412" translate="yes" xml:space="preserve">
          <source>The univariate position of the sample according to the main dimension of the points in the manifold.</source>
          <target state="translated">マニホールド内の点の主次元に応じた試料の一変位置。</target>
        </trans-unit>
        <trans-unit id="e1bff9cf5ae34029868daf8dfe6afee04fc2666d" translate="yes" xml:space="preserve">
          <source>The unmixing matrix.</source>
          <target state="translated">非混合マトリックス。</target>
        </trans-unit>
        <trans-unit id="4c5425dc309e3cab0153df4bbf4dcfe21bae1aa0" translate="yes" xml:space="preserve">
          <source>The unsupervised data reduction and the supervised estimator can be chained in one step. See &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline: chaining estimators&lt;/a&gt;.</source>
          <target state="translated">教師なしデータ削減と教師あり推定量は、1つのステップで連鎖できます。&lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline：chaining estimatorsを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="fca2372915cd9dbf5bde8febd27812d65a387223" translate="yes" xml:space="preserve">
          <source>The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for &lt;code&gt;x&lt;/code&gt; to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide).</source>
          <target state="translated">左上の図は、おもちゃの1d回帰問題のランダムなデータセットLS（青い点）でトレーニングされた単一の決定木の予測（濃い赤）を示しています。また、問題の他の（および異なる）ランダムに描画されたインスタンスLSでトレーニングされた他の単一の決定木の予測（薄い赤）も示しています。直感的に、ここでの分散項は、個々の推定量の予測のビームの幅（薄い赤）に対応しています。分散が大きいほど、 &lt;code&gt;x&lt;/code&gt; の予測は敏感になりますトレーニングセットの小さな変更に。バイアス項は、推定器（シアン）の平均予測と可能な限り最高のモデル（紺色）の差に対応します。したがって、この問題では、分散が大きい（赤いビームはかなり広い）一方で、バイアスが非常に低い（シアンと青の両方の曲線が互いに近い）ことがわかります。</target>
        </trans-unit>
        <trans-unit id="30828e1dddfa129352e0424777c8d521d86c8855" translate="yes" xml:space="preserve">
          <source>The usage and the parameters of &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; are described below. The 2 most important parameters of these estimators are &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;learning_rate&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;の使用法とパラメーターを以下に説明します。これらの推定量の2つの最も重要なパラメーターは、 &lt;code&gt;n_estimators&lt;/code&gt; と &lt;code&gt;learning_rate&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="1125be6cb70e3f587339b27b5931d4f81c0d0d9b" translate="yes" xml:space="preserve">
          <source>The usage of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;の使用法は、&lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;カーネル近似&lt;/a&gt;で詳しく説明されています。</target>
        </trans-unit>
        <trans-unit id="7012369902c9ebdfcf4a40c832356073db31ae9a" translate="yes" xml:space="preserve">
          <source>The usage of centroid distance limits the distance metric to Euclidean space.</source>
          <target state="translated">セントロイド距離の使用は、距離メトリックをユークリッド空間に限定します。</target>
        </trans-unit>
        <trans-unit id="4f2ed911398b7e07d993b089b964fc574c420c21" translate="yes" xml:space="preserve">
          <source>The usage of the &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt;&lt;code&gt;SkewedChi2Sampler&lt;/code&gt;&lt;/a&gt; is the same as the usage described above for the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;. The only difference is in the free parameter, that is called \(c\). For a motivation for this mapping and the mathematical details see &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt; &lt;code&gt;SkewedChi2Sampler&lt;/code&gt; &lt;/a&gt;の使用法は、RBFSamplerについて上記で説明した使用法と同じ&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;。唯一の違いは、\（c \）と呼ばれるfreeパラメータにあります。このマッピングの動機と数学的詳細については、&lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="e4fafa67af5e9d647520b3a289f2e58cae35673f" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/miscellaneous/plot_multioutput_face_completion#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">回帰のための多出力最近傍の使用は&lt;a href=&quot;../auto_examples/miscellaneous/plot_multioutput_face_completion#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py&quot;&gt;、多出力推定量を使用した顔の補完で&lt;/a&gt;示されます。この例では、入力Xは面の上半分のピクセルであり、出力Yはそれらの面の下半分のピクセルです。</target>
        </trans-unit>
        <trans-unit id="fabe9be99adc410c1907ea1daa7576972e4b19e8" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">回帰のための多出力最近傍の使用は&lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;、多出力推定量を使用した面補完で&lt;/a&gt;示されています。この例では、入力Xは顔の上半分のピクセルで、出力Yは顔の下半分のピクセルです。</target>
        </trans-unit>
        <trans-unit id="810478c2cd15f9d623ca524c5ee19e348c649a7e" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/miscellaneous/plot_multioutput_face_completion#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">分類のためのマルチ出力ツリーの使用は&lt;a href=&quot;../auto_examples/miscellaneous/plot_multioutput_face_completion#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py&quot;&gt;、マルチ出力推定量を使用した顔の完成で&lt;/a&gt;示されます。この例では、入力Xは面の上半分のピクセルであり、出力Yはそれらの面の下半分のピクセルです。</target>
        </trans-unit>
        <trans-unit id="adaab44ae672254ed2b0f1474d8d83b9d0ff364a" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">分類に多出力ツリーを使用する方法は&lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;、多出力推定量を使用した面補完に&lt;/a&gt;示されています。この例では、入力Xは顔の上半分のピクセルで、出力Yは顔の下半分のピクセルです。</target>
        </trans-unit>
        <trans-unit id="e18a96fd5f8412fd8540943bfa9bb84448ef6eef" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for regression is demonstrated in &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;Multi-output Decision Tree Regression&lt;/a&gt;. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.</source>
          <target state="translated">回帰にマルチ出力ツリーを使用する方法は、マルチ&lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;出力ディシジョンツリー回帰に&lt;/a&gt;示されています。この例では、入力Xは単一の実数値で、出力YはXの正弦と余弦です。</target>
        </trans-unit>
        <trans-unit id="6d9188616eccce81f2896ac591395f1642a23655" translate="yes" xml:space="preserve">
          <source>The used categories can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute.</source>
          <target state="translated">使用されている &lt;code&gt;categories_&lt;/code&gt; は、categories_属性にあります。</target>
        </trans-unit>
        <trans-unit id="ada4ba90a5864aca46a2fd4279e91da24ee2ef32" translate="yes" xml:space="preserve">
          <source>The user-provided initial means, defaults to None, If it None, means are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">ユーザーが指定した初期手段は、デフォルトでNone です &lt;code&gt;init_params&lt;/code&gt; 場合は、init_paramsメソッドを使用して初期化されます。</target>
        </trans-unit>
        <trans-unit id="a1cbbec0505b4f4802950c392df8579b38a3220d" translate="yes" xml:space="preserve">
          <source>The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the &amp;lsquo;init_params&amp;rsquo; method. The shape depends on &amp;lsquo;covariance_type&amp;rsquo;:</source>
          <target state="translated">ユーザー提供の初期精度（共分散行列の逆数）のデフォルトはNoneです。Noneの場合、精度は「init_params」メソッドを使用して初期化されます。形状は「covariance_type」に依存します：</target>
        </trans-unit>
        <trans-unit id="930a8f090bbd3f5ab357e6a55436cb80968a37a5" translate="yes" xml:space="preserve">
          <source>The user-provided initial weights, defaults to None. If it None, weights are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">ユーザーが指定した初期の重み。デフォルトはNoneです。Noneの場合、重みは &lt;code&gt;init_params&lt;/code&gt; メソッドを使用して初期化されます。</target>
        </trans-unit>
        <trans-unit id="845cdaf2c42f719deb3141928cabec5996b4eedb" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.</source>
          <target state="translated">通常の共分散最尤推定値は、収縮を用いて正則化することができる。Ledoit と Wolf は、漸近的に最適な収縮パラメータ(MSE 基準を最小化する)を計算するための近似式を提案し、これにより Ledoit-Wolf 共分散推定値が得られる。</target>
        </trans-unit>
        <trans-unit id="5d9f8abe9cd1fbb176b951540baae1f3defa7962" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set.</source>
          <target state="translated">通常の共分散最尤推定値は、データセット内の外れ値の存在に非常に敏感です。このような場合は、共分散のロバスト推定量を使用して、推定がデータセット内の「誤った」観測に耐性があることを保証することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="b7108164734a4610e8bf46680745d5c565a5fe9c" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set. &lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;#id5&quot; id=&quot;id2&quot;&gt;2&lt;/a&gt;</source>
          <target state="translated">通常の共分散最尤推定は、データセット内の外れ値の存在に非常に敏感です。このような場合、共分散のロバスト推定量を使用して、推定がデータセット内の「誤った」観測値に耐性があることを保証する方がよいでしょう。&lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;、&lt;a href=&quot;#id5&quot; id=&quot;id2&quot;&gt;2&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bf61b06542d3e7e313e8463ad6cb36679683eb16" translate="yes" xml:space="preserve">
          <source>The utility function &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt;&lt;code&gt;make_pipeline&lt;/code&gt;&lt;/a&gt; is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:</source>
          <target state="translated">ユーティリティ関数&lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt; &lt;code&gt;make_pipeline&lt;/code&gt; &lt;/a&gt;は、パイプラインを構築するための省略形です。可変数の推定器を受け取り、パイプラインを返し、名前を自動的に入力します。</target>
        </trans-unit>
        <trans-unit id="e6fa170ec90d321cecf4d3db13ca13c99c71342b" translate="yes" xml:space="preserve">
          <source>The valid distance metrics, and the function they map to, are:</source>
          <target state="translated">有効な距離メトリクスと、それらがマッピングする関数は、次のとおりです。</target>
        </trans-unit>
        <trans-unit id="7aae4519886038a4fe27266a449c263068305fb0" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.</source>
          <target state="translated">値2は最も高いスコアを持っています:それは1.5と2の重みで2回現れます:これらの合計は3です。</target>
        </trans-unit>
        <trans-unit id="1614b1f299556b57f6975870b8797f657adfc906" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.5.</source>
          <target state="translated">値2は最も高いスコアを持っています:それは1.5と2の重みで2回現れます:これらの合計は3.5です。</target>
        </trans-unit>
        <trans-unit id="cf2aaa6a69c13c7a1da598bbc487a099c24264e9" translate="yes" xml:space="preserve">
          <source>The value 4 appears three times: with uniform weights, the result is simply the mode of the distribution.</source>
          <target state="translated">値4は3回出現します:一様な重みでは、結果は単に分布のモードです。</target>
        </trans-unit>
        <trans-unit id="ee4d7bc4aea75382ac7c13c2d3ccdb7c85b05695" translate="yes" xml:space="preserve">
          <source>The value by which &lt;code&gt;|y - X'w - c|&lt;/code&gt; is scaled down.</source>
          <target state="translated">値 &lt;code&gt;|y - X'w - c|&lt;/code&gt; 縮小されます。</target>
        </trans-unit>
        <trans-unit id="208f4b20d150446e32489b5141e41ca0c231e069" translate="yes" xml:space="preserve">
          <source>The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor.</source>
          <target state="translated">選択されたパーティションに関連付けられた慣性基準の値(compute_labels が True に設定されている場合).慣性は,サンプルの最も近い隣人までの二乗距離の和として定義されます.</target>
        </trans-unit>
        <trans-unit id="605e5e84334ac11a6b1bcf93811952a1f4c93232" translate="yes" xml:space="preserve">
          <source>The value of the information criteria (&amp;lsquo;aic&amp;rsquo;, &amp;lsquo;bic&amp;rsquo;) across all alphas. The alpha which has the smallest information criterion is chosen. This value is larger by a factor of &lt;code&gt;n_samples&lt;/code&gt; compared to Eqns. 2.15 and 2.16 in (Zou et al, 2007).</source>
          <target state="translated">すべてのアルファにわたる情報基準（ 'aic'、 'bic'）の値。最小の情報基準を持つアルファが選択されます。この値は、 &lt;code&gt;n_samples&lt;/code&gt; と比較してn_samples倍大きくなっています。2.15および2.16（Zou et al、2007）。</target>
        </trans-unit>
        <trans-unit id="31a3294fc25a32d76657f5de9f45e5deb67968f8" translate="yes" xml:space="preserve">
          <source>The value of the largest coefficient.</source>
          <target state="translated">最大係数の値。</target>
        </trans-unit>
        <trans-unit id="35a39b75c4cf1051868175c0eb7c4d08a5d8c4c6" translate="yes" xml:space="preserve">
          <source>The value of the smallest coefficient.</source>
          <target state="translated">最小の係数の値。</target>
        </trans-unit>
        <trans-unit id="77ad2ae374f9b7e58f24db15820679db8e02f6eb" translate="yes" xml:space="preserve">
          <source>The values at which the partial dependence should be evaluated are directly generated from &lt;code&gt;X&lt;/code&gt;. For 2-way partial dependence, a 2D-grid of values is generated. The &lt;code&gt;values&lt;/code&gt; field returned by &lt;a href=&quot;generated/sklearn.inspection.partial_dependence#sklearn.inspection.partial_dependence&quot;&gt;&lt;code&gt;sklearn.inspection.partial_dependence&lt;/code&gt;&lt;/a&gt; gives the actual values used in the grid for each target feature. They also correspond to the axis of the plots.</source>
          <target state="translated">部分的な依存関係を評価する必要がある値は、 &lt;code&gt;X&lt;/code&gt; から直接生成されます。2方向の部分依存の場合、値の2Dグリッドが生成されます。 &lt;code&gt;values&lt;/code&gt; によって返されるフィールド&lt;a href=&quot;generated/sklearn.inspection.partial_dependence#sklearn.inspection.partial_dependence&quot;&gt; &lt;code&gt;sklearn.inspection.partial_dependence&lt;/code&gt; は、&lt;/a&gt;各対象地物のためのグリッドで使用される実際の値を与えます。それらは、プロットの軸にも対応しています。</target>
        </trans-unit>
        <trans-unit id="d764819e9c8551a1ae19a24b5d4cd3ac77d8b2aa" translate="yes" xml:space="preserve">
          <source>The values corresponding the quantiles of reference.</source>
          <target state="translated">基準となるクォンタイルに対応する値。</target>
        </trans-unit>
        <trans-unit id="e64c591fdc6bfea9ce8a9d182cdb9d3354d8a90b" translate="yes" xml:space="preserve">
          <source>The values listed by the ValueError exception correspond to the functions measuring prediction accuracy described in the following sections. The scorer objects for those functions are stored in the dictionary &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt;.</source>
          <target state="translated">ValueError例外によってリストされる値は、次のセクションで説明する予測精度を測定する関数に対応しています。これらの関数のスコアラーオブジェクトは、辞書 &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt; に格納されています。</target>
        </trans-unit>
        <trans-unit id="ad8b80eff2782593fcb8941c5fc504eacc683633" translate="yes" xml:space="preserve">
          <source>The values of the parameter that will be evaluated.</source>
          <target state="translated">評価されるパラメータの値。</target>
        </trans-unit>
        <trans-unit id="380d80dd5b496982ade02999d670873884707a35" translate="yes" xml:space="preserve">
          <source>The values of this array sum to 1, unless all trees are single node trees consisting of only the root node, in which case it will be an array of zeros.</source>
          <target state="translated">すべての木がルートノードのみからなるシングルノードの木でない限り、この配列の値の合計は1になります。</target>
        </trans-unit>
        <trans-unit id="896e61b11bd1cccb5014a25cacfbd49baded2da1" translate="yes" xml:space="preserve">
          <source>The values to be assigned to each cluster of samples</source>
          <target state="translated">サンプルの各クラスタに割り当てられる値</target>
        </trans-unit>
        <trans-unit id="15b198c6985fde1d17e7089abe8db26ba2c88b68" translate="yes" xml:space="preserve">
          <source>The values with which the grid has been created. The generated grid is a cartesian product of the arrays in &lt;code&gt;values&lt;/code&gt;. &lt;code&gt;len(values) ==
len(features)&lt;/code&gt;. The size of each array &lt;code&gt;values[j]&lt;/code&gt; is either &lt;code&gt;grid_resolution&lt;/code&gt;, or the number of unique values in &lt;code&gt;X[:, j]&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">グリッドが作成された値。生成されたグリッドは、 &lt;code&gt;values&lt;/code&gt; の配列のデカルト積です。 &lt;code&gt;len(values) == len(features)&lt;/code&gt; 。各配列 &lt;code&gt;values[j]&lt;/code&gt; のサイズは、 &lt;code&gt;grid_resolution&lt;/code&gt; 、または &lt;code&gt;X[:, j]&lt;/code&gt; の一意の値の数のいずれか小さい方です。</target>
        </trans-unit>
        <trans-unit id="de1379c1aa59c6a0f9d415d70cec6205d70e58b1" translate="yes" xml:space="preserve">
          <source>The variance for each feature in the training set. Used to compute &lt;code&gt;scale_&lt;/code&gt;. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_std=False&lt;/code&gt;.</source>
          <target state="translated">トレーニングセット内の各特徴の分散。 &lt;code&gt;scale_&lt;/code&gt; の計算に使用されます。 &lt;code&gt;with_std=False&lt;/code&gt; の場合は &lt;code&gt;None&lt;/code&gt; と同じです。</target>
        </trans-unit>
        <trans-unit id="6909f327165fbbe5fe9d7a24773b9839e3b76030" translate="yes" xml:space="preserve">
          <source>The variance of the training samples transformed by a projection to each component.</source>
          <target state="translated">各成分への射影によって変換された訓練サンプルの分散。</target>
        </trans-unit>
        <trans-unit id="73778e0f44de95a7d306fdc5c4bc68ceb4bf8f71" translate="yes" xml:space="preserve">
          <source>The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">パスに沿った係数の変動値。 &lt;code&gt;fit_path&lt;/code&gt; パラメータが &lt;code&gt;False&lt;/code&gt; の場合は存在しません。</target>
        </trans-unit>
        <trans-unit id="f193fe45abdd49c251c120544e6bc124439f7f88" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;lsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;lsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="translated">ベクトル\（h_i \）は観測されないため、「潜在」と呼ばれます。\（\ epsilon \）は、平均が0、共分散が\（\ Psi \）のガウス分布に従って分布したノイズ項と見なされます（つまり、\（\ epsilon \ sim \ mathcal {N}（0、\ Psi）\））、 \（\ mu \）は任意のオフセットベクトルです。このようなモデルは、\（h_i \）から\（x_i \）がどのように生成されるかを表すため、「生成的」と呼ばれます。すべての\（x_i \）を列として使用して行列\（\ mathbf {X} \）を作成し、すべての\（h_i \）を行列\（\ mathbf {H} \の列として使用する場合）次に、（適切に定義された\（\ mathbf {M} \）および\（\ mathbf {E} \）を使用して）次のように記述できます。</target>
        </trans-unit>
        <trans-unit id="928beebc49e69bf6a914c81639fd968b851e773a" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;rsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;rsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="translated">ベクトル\（h_i \）は、監視されていないため、「潜在的」と呼ばれます。\（\ epsilon \）は、平均が0で共分散が\（\ Psi \）（つまり、\（\ epsilon \ sim \ mathcal {N}（0、\ Psi）\））のガウス分布に従って分布するノイズ項と見なされます。 \（\ mu \）は任意のオフセットベクトルです。このようなモデルは、\（x_i \）が\（h_i \）からどのように生成されるかを説明するため、「生成」と呼ばれます。すべての\（x_i \）を列として使用して行列\（\ mathbf {X} \）を形成し、すべての\（h_i \）を行列\（\ mathbf {H} \）の列として使用する場合）次に、（適切に定義された\（\ mathbf {M} \）および\（\ mathbf {E} \））を記述できます。</target>
        </trans-unit>
        <trans-unit id="4545cfee48c8d7d2034c72caee40a8d82cfcf4ce" translate="yes" xml:space="preserve">
          <source>The verbose setting on the MiniBatchKMeans enables us to see that some clusters are reassigned during the successive calls to partial-fit. This is because the number of patches that they represent has become too low, and it is better to choose a random new cluster.</source>
          <target state="translated">MiniBatchKMeansの冗長な設定により、部分フィットの連続した呼び出しの間に、いくつかのクラスタが再割り当てされることがわかります。これは、それらが表現するパッチの数が少なくなりすぎたためで、ランダムな新しいクラスタを選択する方が良いからです。</target>
        </trans-unit>
        <trans-unit id="e4c09c360935c392206a8639f0a0b8f4c48b519d" translate="yes" xml:space="preserve">
          <source>The verbosity level</source>
          <target state="translated">冗長性のレベル</target>
        </trans-unit>
        <trans-unit id="4b4be8a44f0a986b95a00a99e1db7cc81cee43d8" translate="yes" xml:space="preserve">
          <source>The verbosity level.</source>
          <target state="translated">冗長性のレベル。</target>
        </trans-unit>
        <trans-unit id="d073e4bf5a007c3a6f9ea54d0f642d54d81c6316" translate="yes" xml:space="preserve">
          <source>The verbosity level. If not zero, print some information about the fitting process.</source>
          <target state="translated">冗長度レベル。ゼロでない場合は,フィット処理に関する情報を表示します.</target>
        </trans-unit>
        <trans-unit id="bca220a25d0b85cbc18bcc37bc0c66babd623c39" translate="yes" xml:space="preserve">
          <source>The verbosity level. The default, zero, means silent mode.</source>
          <target state="translated">冗長度のレベル。デフォルトのゼロは、サイレントモードを意味します。</target>
        </trans-unit>
        <trans-unit id="b790263c39903969cb477edb620406690c93cb40" translate="yes" xml:space="preserve">
          <source>The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported.</source>
          <target state="translated">冗長度レベル:ゼロでない場合、進行状況のメッセージを出力します。50を超えると、出力は標準出力に送られます。メッセージの頻度は冗長度に応じて増加します。10を超えると、すべての繰り返しが報告されます。</target>
        </trans-unit>
        <trans-unit id="ac67f0eccad920e701e068f47d28e090c55e23b1" translate="yes" xml:space="preserve">
          <source>The verbosity mode of the function. By default that of the memory object is used.</source>
          <target state="translated">関数の冗長モード。デフォルトではメモリオブジェクトの冗長モードが使用されます。</target>
        </trans-unit>
        <trans-unit id="31fbaba32a3bec24c49d9ccad5d6e7edcbc904dc" translate="yes" xml:space="preserve">
          <source>The versions of scikit-learn and its dependencies</source>
          <target state="translated">scikit-learn のバージョンとその依存関係</target>
        </trans-unit>
        <trans-unit id="588dcf117cf46b2a6deec167a78bc5c1266a3688" translate="yes" xml:space="preserve">
          <source>The visualization is fit automatically to the size of the axis. Use the &lt;code&gt;figsize&lt;/code&gt; or &lt;code&gt;dpi&lt;/code&gt; arguments of &lt;code&gt;plt.figure&lt;/code&gt; to control the size of the rendering.</source>
          <target state="translated">視覚化は、軸のサイズに自動的に適合します。使用 &lt;code&gt;figsize&lt;/code&gt; や &lt;code&gt;dpi&lt;/code&gt; の引数 &lt;code&gt;plt.figure&lt;/code&gt; をレンダリングのサイズを制御します。</target>
        </trans-unit>
        <trans-unit id="6a1686cc9631522f215a91033f5d185a9b750f85" translate="yes" xml:space="preserve">
          <source>The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns:</source>
          <target state="translated">このベクタライザによって抽出された語彙はより大きくなり、局所的な位置決めパターンで符号化された曖昧さを解決することができるようになりました。</target>
        </trans-unit>
        <trans-unit id="16d6455593e440b1ece6b5d9b609b990b16728ff" translate="yes" xml:space="preserve">
          <source>The weighted average probabilities for a sample would then be calculated as follows:</source>
          <target state="translated">その後、標本の加重平均確率は次のように計算されます。</target>
        </trans-unit>
        <trans-unit id="cf3727918257ef88c9160136d7b98b7188c886ca" translate="yes" xml:space="preserve">
          <source>The weighted impurity decrease equation is the following:</source>
          <target state="translated">加重不純物減少式は以下の通りである。</target>
        </trans-unit>
        <trans-unit id="b71363fa16752021c27a44ccc8c03e740b0054e3" translate="yes" xml:space="preserve">
          <source>The weights \(w\) of the model can be access:</source>
          <target state="translated">モデルの重みにアクセスできるようになりました。</target>
        </trans-unit>
        <trans-unit id="dff6c2479ef55aa391ea0314f017ab45501554fd" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight</source>
          <target state="translated">Xの各オブザベーションの重み。</target>
        </trans-unit>
        <trans-unit id="fd37cf293f530a725eccba4fc386300500a89671" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None)</source>
          <target state="translated">Noneの場合、すべてのオブザベーションに等しい重みが割り当てられる (デフォルト:None)。</target>
        </trans-unit>
        <trans-unit id="89b091775b7b9350f3e527eec283b3139c887c00" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None).</source>
          <target state="translated">Noneの場合、すべてのオブザベーションに等しい重みが割り当てられる(デフォルト:None)。</target>
        </trans-unit>
        <trans-unit id="7cdf718f4170abd3f842260826cc1d9da2bb111a" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight.</source>
          <target state="translated">Xの各オブザベーションの重み。</target>
        </trans-unit>
        <trans-unit id="0fd9c75eb2401f4a1ca174b965b1602b9d82941e" translate="yes" xml:space="preserve">
          <source>The weights of each feature computed by the &lt;code&gt;fit&lt;/code&gt; method call are stored in a model attribute:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; メソッド呼び出しによって計算された各特徴の重みは、モデル属性に格納されます。</target>
        </trans-unit>
        <trans-unit id="44235eb4c2d368f2a7e225131bb791a1dc59a457" translate="yes" xml:space="preserve">
          <source>The weights of each mixture components.</source>
          <target state="translated">各混合成分の重さ。</target>
        </trans-unit>
        <trans-unit id="dd426f25816730ede96a98b838186f9fc1553a50" translate="yes" xml:space="preserve">
          <source>The wine dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">ワインデータセットは、古典的で非常に簡単な多クラス分類データセットです。</target>
        </trans-unit>
        <trans-unit id="9f15574c628488edf549f3131c3e1a8bfce37b9c" translate="yes" xml:space="preserve">
          <source>The word &amp;ldquo;article&amp;rdquo; is a significant feature, based on how often people quote previous posts like this: &amp;ldquo;In article [article ID], [name] &amp;lt;[e-mail address]&amp;gt; wrote:&amp;rdquo;</source>
          <target state="translated">「記事」という言葉は重要な特徴であり、「記事[記事ID]、[名前] &amp;lt;[電子メールアドレス]&amp;gt;が次のように書いている：」のように、以前の投稿を引用する頻度に基づいています。</target>
        </trans-unit>
        <trans-unit id="364df8b6c4c1dfcb405abf5ac32289b3f8338a10" translate="yes" xml:space="preserve">
          <source>The word &lt;em&gt;restricted&lt;/em&gt; refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:</source>
          <target state="translated">&lt;em&gt;制限付き&lt;/em&gt;という用語は、モデルの2部構成を指し、非表示のユニット間または表示されているユニット間の直接的な相互作用を禁止します。つまり、次の条件付き独立性が想定されます。</target>
        </trans-unit>
        <trans-unit id="92b782ef9bfc8a9813d7ea0d8efb9106d8f1896e" translate="yes" xml:space="preserve">
          <source>The word boundaries-aware variant &lt;code&gt;char_wb&lt;/code&gt; is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw &lt;code&gt;char&lt;/code&gt; variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations.</source>
          <target state="translated">単語境界を認識するバリアント &lt;code&gt;char_wb&lt;/code&gt; は、単語の分離に空白を使用する言語では特に興味深いです。この場合、生の &lt;code&gt;char&lt;/code&gt; バリアントよりもノイズの少ない機能が大幅に生成されないためです。このような言語では、スペルミスや単語の派生に関する堅牢性を維持しながら、そのような機能を使用してトレーニングされた分類子の予測精度と収束速度の両方を向上させることができます。</target>
        </trans-unit>
        <trans-unit id="8e8bd120180a9cd30000d6c7bc4b945e5f585fc6" translate="yes" xml:space="preserve">
          <source>The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, &amp;lsquo;How slow is the k-means method?&amp;rsquo; SoCG2006)</source>
          <target state="translated">最悪の場合の複雑性は、n = n_samples、p = n_featuresのO（n ^（k + 2 / p））によって与えられます。（D. ArthurおよびS. Vassilvitskii、「k-meansメソッドはどのくらい遅いのですか？」SoCG2006）</target>
        </trans-unit>
        <trans-unit id="08b4bc51642b52ec53b43a5b2dca7586d296a859" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimator: robust multivariate regression model.</source>
          <target state="translated">Theil-Sen 推定器:ロバスト多変量回帰モデル。</target>
        </trans-unit>
        <trans-unit id="26357ed7a886288385aec0522bda7ee91031a5a9" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</source>
          <target state="translated">多重線形回帰モデルのTheil-Sen推定量、2009 Xin Dang、Hanxiang Peng、Xueqin Wang、およびHeping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bfbe9912c49db3d4af2258ea17cbacd11611139b" translate="yes" xml:space="preserve">
          <source>Theil-Sen Regression</source>
          <target state="translated">テイルセン回帰</target>
        </trans-unit>
        <trans-unit id="43407a9ed591c227c94e72cd0fbe8ae10a8a282d" translate="yes" xml:space="preserve">
          <source>TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs worse than OLS.</source>
          <target state="translated">TheilSen は、方向 X と方向 y の両方で、小さな外れ値には適していますが、それ以上のブレークポイントがあり、OLS よりもパフォーマンスが悪くなります。</target>
        </trans-unit>
        <trans-unit id="bae71614b2af83560543a8e9bca47722a78d80c8" translate="yes" xml:space="preserve">
          <source>Their harmonic mean called &lt;strong&gt;V-measure&lt;/strong&gt; is computed by &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt;&lt;code&gt;v_measure_score&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;strong&gt;V-measure&lt;/strong&gt;と呼ばれる調和平均は、&lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt; &lt;code&gt;v_measure_score&lt;/code&gt; &lt;/a&gt;によって計算されます。</target>
        </trans-unit>
        <trans-unit id="de0b6ab722b9ec4be95cb7086fea273ea373e1de" translate="yes" xml:space="preserve">
          <source>Then fire an ipython shell and run the work-in-progress script with:</source>
          <target state="translated">次にipythonシェルを起動して、work-in-progressスクリプトを実行します。</target>
        </trans-unit>
        <trans-unit id="8f78bb6cc31961e9507c2d8e418f53fe822a9ecd" translate="yes" xml:space="preserve">
          <source>Then one can show that to classify a data point after scaling is equivalent to finding the estimated class mean \(\mu^*_k\) which is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the \(K-1\) affine subspace \(H_K\) generated by all the \(\mu^*_k\) for all classes. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a \(K-1\) dimensional space.</source>
          <target state="translated">そうすると、スケーリング後のデータ点を分類することは、ユークリッド距離で最も近いクラス平均を見つけることと等価であることがわかる。しかし、これは、すべてのクラスのためのすべての\(K-1\)で生成されたアフィネ部分空間に投影した後、同様に行うことができます。これは,LDA 分類器には,次元空間への線形投影による次元削減があることを示している.</target>
        </trans-unit>
        <trans-unit id="37e493a483dcc9fefbfec81c47f8c835f7340e3f" translate="yes" xml:space="preserve">
          <source>Then the Davies-Bouldin index is defined as:</source>
          <target state="translated">すると、Davies-Bouldin指数は次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="54059dffef3b64a1ba00cd6fbe5ba85d85229195" translate="yes" xml:space="preserve">
          <source>Then the metrics are defined as:</source>
          <target state="translated">すると、メトリクスは次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="8da3ae858e8aee4768b456c38c7b9a98b999a912" translate="yes" xml:space="preserve">
          <source>Then the multiclass MCC is defined as:</source>
          <target state="translated">そうすると、マルチクラスMCCは次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="826d143749061228ef1caa51bd344b5a543a3ad8" translate="yes" xml:space="preserve">
          <source>Then the rows of \(Z\) are clustered using &lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;. The first &lt;code&gt;n_rows&lt;/code&gt; labels provide the row partitioning, and the remaining &lt;code&gt;n_columns&lt;/code&gt; labels provide the column partitioning.</source>
          <target state="translated">次に、\（Z \）の行が&lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;を使用してクラスター化されます。最初の &lt;code&gt;n_rows&lt;/code&gt; ラベルは行パーティションを提供し、残りの &lt;code&gt;n_columns&lt;/code&gt; ラベルは列パーティションを提供します。</target>
        </trans-unit>
        <trans-unit id="35fd57830f62bfb9e9a1c9481c01f75fad3f7e92" translate="yes" xml:space="preserve">
          <source>Then we check the performance of the computed model plotting its predictions on the test set and computing, for example, the median absolute error of the model.</source>
          <target state="translated">次に、計算されたモデルの性能をチェックし、その予測値をテスト集合上にプロットし、例えばモデルの絶対誤差の中央値を計算します。</target>
        </trans-unit>
        <trans-unit id="04ebdc92c95e8a854e544219afce2fe077f07d46" translate="yes" xml:space="preserve">
          <source>Then we check the quality of the predictions.</source>
          <target state="translated">そして、予測の質をチェックします。</target>
        </trans-unit>
        <trans-unit id="a7548beecedd6ae525f17db272cd47fef5592b2e" translate="yes" xml:space="preserve">
          <source>Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:</source>
          <target state="translated">そして、ユークリッド(L2)ノルムを適用すると、以下の文書1のtf-idfsが得られる。</target>
        </trans-unit>
        <trans-unit id="743e0b74d42270bf2752b1121cf66d48c177b1df" translate="yes" xml:space="preserve">
          <source>Then, the &lt;code&gt;raw_X&lt;/code&gt; to be fed to &lt;code&gt;FeatureHasher.transform&lt;/code&gt; can be constructed using:</source>
          <target state="translated">次に、 &lt;code&gt;raw_X&lt;/code&gt; にフィードされる &lt;code&gt;FeatureHasher.transform&lt;/code&gt; は、以下を使用して構築できます。</target>
        </trans-unit>
        <trans-unit id="36e93c1a3cbd0ad36880c0132d911de26dc0ca83" translate="yes" xml:space="preserve">
          <source>Then, we identify features &lt;code&gt;X&lt;/code&gt; and targets &lt;code&gt;y&lt;/code&gt;: the column WAGE is our target variable (i.e., the variable which we want to predict).</source>
          <target state="translated">次に、特徴 &lt;code&gt;X&lt;/code&gt; とターゲット &lt;code&gt;y&lt;/code&gt; を特定します。列WAGEは、ターゲット変数（つまり、予測する変数）です。</target>
        </trans-unit>
        <trans-unit id="c2abd121a4ed8d06cf0817807518539ab09e24fe" translate="yes" xml:space="preserve">
          <source>Then, we introspect the information regarding each column data type.</source>
          <target state="translated">そして、各カラムのデータ型に関する情報を内観する。</target>
        </trans-unit>
        <trans-unit id="4665e6f004c55fa84e60f1409a5b03c15037dd8c" translate="yes" xml:space="preserve">
          <source>Theoretical bounds</source>
          <target state="translated">理論的な境界</target>
        </trans-unit>
        <trans-unit id="3ebe4a41a0b35192395aac8a80293b4ff462a23b" translate="yes" xml:space="preserve">
          <source>There are 3 different APIs for evaluating the quality of a model&amp;rsquo;s predictions:</source>
          <target state="translated">モデルの予測の品質を評価するための3つの異なるAPIがあります。</target>
        </trans-unit>
        <trans-unit id="96e1915039a69b2e8b64f25478a0431f9e517cc9" translate="yes" xml:space="preserve">
          <source>There are \(K\) topics in the corpus.</source>
          <target state="translated">コーパスの中には、「\(K)」の話題があります。</target>
        </trans-unit>
        <trans-unit id="aa910de5e10f2b04644e63996efaedb310779ee6" translate="yes" xml:space="preserve">
          <source>There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let &lt;code&gt;D&lt;/code&gt; be the distance, and &lt;code&gt;S&lt;/code&gt; be the kernel:</source>
          <target state="translated">カーネルなど、距離メトリックと類似性メジャーの間で変換する方法はいくつかあります。してみましょう &lt;code&gt;D&lt;/code&gt; は距離であり、 &lt;code&gt;S&lt;/code&gt; は、カーネルのこと：</target>
        </trans-unit>
        <trans-unit id="daf4e822cec5d40489080bfb83cea014bb21271c" translate="yes" xml:space="preserve">
          <source>There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):</source>
          <target state="translated">また、いくつかの欠点もあります(対メモリ内の語彙を持つCountVectorizerの使用)。</target>
        </trans-unit>
        <trans-unit id="17151dd0dcece68b8abecb55a500335bd2f90b73" translate="yes" xml:space="preserve">
          <source>There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.</source>
          <target state="translated">XORやパリティ、マルチプレクサの問題など、決定木では表現しにくいため、学習しにくい概念があります。</target>
        </trans-unit>
        <trans-unit id="fbd19b439fe6ba80531cd23629fc7a9a883a8dcf" translate="yes" xml:space="preserve">
          <source>There are different things to keep in mind when dealing with data corrupted by outliers:</source>
          <target state="translated">外れ値によって破損したデータを扱う際には、さまざまな点に注意が必要です。</target>
        </trans-unit>
        <trans-unit id="8e7f48473f9869b2775936845c434aa7ef66ad5e" translate="yes" xml:space="preserve">
          <source>There are four more hyperparameters, \(\alpha_1\), \(\alpha_2\), \(\lambda_1\) and \(\lambda_2\) of the gamma prior distributions over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. By default \(\alpha_1 = \alpha_2 = \lambda_1 = \lambda_2 = 10^{-6}\).</source>
          <target state="translated">\（\ alpha \）と\（\ lambda \）上のガンマ事前分布には、さらに4つのハイパーパラメーター\（\ alpha_1 \）、\（\ alpha_2 \）、\（\ lambda_1 \）、\（\ lambda_2 \）があります。 \）。これらは通常、&lt;em&gt;情報を提供&lt;/em&gt;しないように選択されます。デフォルトでは、\（\ alpha_1 = \ alpha_2 = \ lambda_1 = \ lambda_2 = 10 ^ {-6} \）。</target>
        </trans-unit>
        <trans-unit id="0fcc62fa36cf231416a60ac162100095676c743b" translate="yes" xml:space="preserve">
          <source>There are many learning routines which rely on nearest neighbors at their core. One example is &lt;a href=&quot;density#kernel-density&quot;&gt;kernel density estimation&lt;/a&gt;, discussed in the &lt;a href=&quot;density#density-estimation&quot;&gt;density estimation&lt;/a&gt; section.</source>
          <target state="translated">中心にある最近傍に依存する多くの学習ルーチンがあります。1つの例は、&lt;a href=&quot;density#density-estimation&quot;&gt;密度推定&lt;/a&gt;セクションで説明する&lt;a href=&quot;density#kernel-density&quot;&gt;カーネル密度推定&lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="de49a8d62debf6dd1584d9f929c607a2c0abdd42" translate="yes" xml:space="preserve">
          <source>There are many well-established imputation packages in the R data science ecosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns out to be a particular instance of different sequential imputation algorithms that can all be implemented with &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; by passing in different regressors to be used for predicting missing feature values. In the case of missForest, this regressor is a Random Forest. See &lt;a href=&quot;../auto_examples/impute/plot_iterative_imputer_variants_comparison#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py&quot;&gt;Imputing missing values with variants of IterativeImputer&lt;/a&gt;.</source>
          <target state="translated">Rデータ科学生態系の多くの十分に確立帰属パッケージがあります：アメリア、MI、マウスは、missForestは、などmissForestは人気があり、すべてを実現可能なシーケンシャル帰属アルゴリズムの特定のインスタンスであることが判明&lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;IterativeImputer&lt;/code&gt; &lt;/a&gt;で欠落している特徴値を予測するために使用されるさまざまなリグレッサーを渡します。missForestの場合、このリグレッサーはランダムフォレストです。&lt;a href=&quot;../auto_examples/impute/plot_iterative_imputer_variants_comparison#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py&quot;&gt;IterativeImputerのバリアント&lt;/a&gt;を使用した欠落値の代入を参照してください。</target>
        </trans-unit>
        <trans-unit id="afc5c09497df5828bed5e53fb9ba7340fbd12b56" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. It does not aim to be a general, &amp;lsquo;one-size-fits-all&amp;rsquo; solution as some tasks may require a more custom solution. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt; for more details.</source>
          <target state="translated">提供されている「英語」のストップワードリストには、いくつかの既知の問題があります。一部のタスクではよりカスタムなソリューションが必要になる場合があるため、一般的な「万能」ソリューションを目指すものではありません。詳細については、&lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="84513801465c2346e62e2a9ecaecce1cb0c3994b" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt;.</source>
          <target state="translated">提供されている「英語」のストップワードリストには、いくつかの既知の問題があります。&lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="69da86c48a6fab38a24a0a096ce607f0a2966cc1" translate="yes" xml:space="preserve">
          <source>There are several possibilities to do that, two of which are:</source>
          <target state="translated">そのためにはいくつかの可能性がありますが、そのうちの2つです。</target>
        </trans-unit>
        <trans-unit id="87d60889b216a9f6a8543438325d8902e749b92a" translate="yes" xml:space="preserve">
          <source>There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).</source>
          <target state="translated">40人の個性的な被写体のそれぞれについて、10種類の画像が用意されています。被験者によっては、照明、顔の表情(目を開いている/閉じている、笑っている/笑っていない)、顔の細部(眼鏡/眼鏡なし)を変えて、異なる時間に撮影されたものもあります。すべての画像は、暗い均質な背景の下で、被験者を正面から見て直立した状態で撮影されました(多少の横移動は許容範囲内)。</target>
        </trans-unit>
        <trans-unit id="840b342f8bab010ed9a33830388b79c022d751bb" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers linear kernels, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="translated">サポートベクター回帰には、&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; の&lt;/a&gt; 3つの異なる実装があります。&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt;は&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;より高速な実装を提供しますが、線形カーネルのみを考慮しますが、&lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt;は&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt;とは少し異なる定式化を実装します。詳細については、&lt;a href=&quot;#svm-implementation-details&quot;&gt;実装の詳細&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="7c46376033bb18a7479bbb076a0191da5ac66026" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers the linear kernel, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="translated">サポートベクター回帰には、&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; の&lt;/a&gt;3つの異なる実装があります。&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt;は&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;よりも高速な実装を提供しますが、線形カーネルのみを考慮しますが、&lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt;は&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt;とはわずかに異なる定式化を実装します。詳細については、&lt;a href=&quot;#svm-implementation-details&quot;&gt;実装の詳細&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="6325cb8ad2d750db2de1f50457999e3ed60c95ad" translate="yes" xml:space="preserve">
          <source>There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.</source>
          <target state="translated">データセットの種類に応じて、データセットを取得するために使用できる主に3種類のデータセットインタフェースがあります。</target>
        </trans-unit>
        <trans-unit id="f738d3558005f6c63ce087749d6fa9a898307784" translate="yes" xml:space="preserve">
          <source>There are two main methods to approximate the integral above, namely the &amp;lsquo;brute&amp;rsquo; and &amp;lsquo;recursion&amp;rsquo; methods. The &lt;code&gt;method&lt;/code&gt; parameter controls which method to use.</source>
          <target state="translated">上記の積分を近似する主な方法は2つあります。つまり、「brute」と「recursion」の方法です。 &lt;code&gt;method&lt;/code&gt; 方法は、使用するパラメータを制御します。</target>
        </trans-unit>
        <trans-unit id="8d867308d6cfc04930e6d66867b250110233d07e" translate="yes" xml:space="preserve">
          <source>There are two options to assign labels:</source>
          <target state="translated">ラベルを割り当てるには2つのオプションがあります。</target>
        </trans-unit>
        <trans-unit id="60a8f9c96bc7f93958ce08005db84d1ac5e8a2b4" translate="yes" xml:space="preserve">
          <source>There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.</source>
          <target state="translated">バイクラスタリングの結果を評価するには、内部的なものと外部的なものの2つの方法があります。クラスタの安定性のような内部的な評価は、データと結果そのものにのみ依存します。現在のところ、scikit-learnには内部的なバイクラスタリングの評価方法はありません。外部指標は、真の解のような外部の情報源を参照します。実際のデータを扱う場合、真の解は通常未知ですが、人工的なデータの二重クラスタリングは、真の解が既知であるため、アルゴリズムの評価に役立つかもしれません。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
