<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="4fae2d49ee8a5e8d8ea52343db3e2b05ff45988e" translate="yes" xml:space="preserve">
          <source>Fit the ridge classifier.</source>
          <target state="translated">リッジ分類器をフィットさせます。</target>
        </trans-unit>
        <trans-unit id="2b96765d688b574c756064537ec2686c8ac36571" translate="yes" xml:space="preserve">
          <source>Fit the transformer on X.</source>
          <target state="translated">トランスをXに装着します。</target>
        </trans-unit>
        <trans-unit id="acd485cd941415835a11d2180278c2146ce5888c" translate="yes" xml:space="preserve">
          <source>Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)</source>
          <target state="translated">ARD事前分布を用いて,回帰モデルの重みを適合させる.回帰モデルの重みは,ガウス分布にあると仮定する.また,パラメータ λ (重みの分布の精度)と alpha (ノイズの分布の精度)を推定する.推定は,繰り返し処理(Evidence Maximization)によって行われる.</target>
        </trans-unit>
        <trans-unit id="ea18404b90123396c3f41537d11afad54c507780" translate="yes" xml:space="preserve">
          <source>Fit to data, then transform it.</source>
          <target state="translated">データにフィットし、変換します。</target>
        </trans-unit>
        <trans-unit id="c2cf341635a3875675d46dd618b9a1757d9a6c7c" translate="yes" xml:space="preserve">
          <source>Fit transformer by checking X.</source>
          <target state="translated">Xを確認してトランスをフィットさせます。</target>
        </trans-unit>
        <trans-unit id="eb75d7eb91b3dadf245e1b3bf8f4c37a27824085" translate="yes" xml:space="preserve">
          <source>Fit underlying estimators.</source>
          <target state="translated">基礎となる推定子をフィットさせます。</target>
        </trans-unit>
        <trans-unit id="c093c0cee7f3b9fa93ffb32acb026baea322889f" translate="yes" xml:space="preserve">
          <source>Fits a Minimum Covariance Determinant with the FastMCD algorithm.</source>
          <target state="translated">FastMCDアルゴリズムを用いて最小共分散決定量にフィットします。</target>
        </trans-unit>
        <trans-unit id="f30506afc59d08eae550fdeb02ae856731ea996b" translate="yes" xml:space="preserve">
          <source>Fits all the transforms one after the other and transforms the data, then uses fit_transform on transformed data with the final estimator.</source>
          <target state="translated">すべての変換を次々にはめ込み、データを変換してから、変換されたデータ上で fit_transform を使用して最終的な推定量を指定します。</target>
        </trans-unit>
        <trans-unit id="c5c1cc9c0352ec3e2d5fcc0df63b71ce152f382f" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso covariance model to X.</source>
          <target state="translated">GraphicalLassoの共分散モデルをXにフィットします.</target>
        </trans-unit>
        <trans-unit id="268ae9ae0a9043d80b2e575c7e344f83f78e662d" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso model to X.</source>
          <target state="translated">GraphicalLassoモデルをXにフィットさせます。</target>
        </trans-unit>
        <trans-unit id="643e04850030e1e1aeeaf619a88e7dab7e6a06be" translate="yes" xml:space="preserve">
          <source>Fits the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">与えられた学習データとパラメータに従って,Ledoit-Wolf縮退共分散モデルを適合させます.</target>
        </trans-unit>
        <trans-unit id="60a2c06b8221e361c36d5fdce8ee644d8456bfce" translate="yes" xml:space="preserve">
          <source>Fits the Maximum Likelihood Estimator covariance model according to the given training data and parameters.</source>
          <target state="translated">与えられた訓練データとパラメータに従って最尤推定器の共分散モデルを適合させます.</target>
        </trans-unit>
        <trans-unit id="9b00c787fd8f13356df0bf86c478f2b7848ac4d7" translate="yes" xml:space="preserve">
          <source>Fits the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.</source>
          <target state="translated">与えられた訓練データとパラメータに従って、Oracle近似収縮共分散モデルをフィットします。</target>
        </trans-unit>
        <trans-unit id="2fb3d8548147a6429a7eeed9e3fbe0456049bc8c" translate="yes" xml:space="preserve">
          <source>Fits the estimator.</source>
          <target state="translated">見積もりにフィットします。</target>
        </trans-unit>
        <trans-unit id="98f6beb1ac87a74b86b8c1fedd5ae0ed0ac89461" translate="yes" xml:space="preserve">
          <source>Fits the shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">与えられた学習データとパラメータに従って,縮退共分散モデルを適合させます.</target>
        </trans-unit>
        <trans-unit id="f072640da94e1ad56e67373f3632aeaebdd8b854" translate="yes" xml:space="preserve">
          <source>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</source>
          <target state="translated">変換器を X と y に、 オプシ ョ ンの引数 fit_params ではめ込み、 変換後の X を返します。</target>
        </trans-unit>
        <trans-unit id="fd568de48dadd27cd4e3ca2395da082642e8f8ec" translate="yes" xml:space="preserve">
          <source>Fitted regressor.</source>
          <target state="translated">フィットしたレグレッサー。</target>
        </trans-unit>
        <trans-unit id="14c6da7cd39661e1b0c6468a3c6a5907d4f99a9c" translate="yes" xml:space="preserve">
          <source>Fitting transformers may be computationally expensive. With its &lt;code&gt;memory&lt;/code&gt; parameter set, &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; will cache each transformer after calling &lt;code&gt;fit&lt;/code&gt;. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration.</source>
          <target state="translated">フィッティングトランスフォーマーは、計算コストがかかる場合があります。そのでは &lt;code&gt;memory&lt;/code&gt; パラメータセット、&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;呼び出した後、各トランスをキャッシュします &lt;code&gt;fit&lt;/code&gt; 。この機能は、パラメーターと入力データが同一の場合に、パイプライン内のフィットトランスフォーマーの計算を回避するために使用されます。典型的な例は、変圧器を1回だけ取り付けて、構成ごとに再利用できるグリッド検索の場合です。</target>
        </trans-unit>
        <trans-unit id="01fe05c223cb56d84d085e38ca62de1932a87e50" translate="yes" xml:space="preserve">
          <source>Flag indicating if the cross-validation values corresponding to each alpha should be stored in the &lt;code&gt;cv_values_&lt;/code&gt; attribute (see below). This flag is only compatible with &lt;code&gt;cv=None&lt;/code&gt; (i.e. using Generalized Cross-Validation).</source>
          <target state="translated">各アルファに対応する相互検証値を &lt;code&gt;cv_values_&lt;/code&gt; 属性に格納する必要があるかどうかを示すフラグ（以下を参照）。このフラグは &lt;code&gt;cv=None&lt;/code&gt; とのみ互換性があります（つまり、一般化交差検証を使用）。</target>
        </trans-unit>
        <trans-unit id="1f498759924682e2363e94f7b83282b97429fcf5" translate="yes" xml:space="preserve">
          <source>Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are:</source>
          <target state="translated">一般化クロスバリデーションを実行する際に使用するストラテジーを示すフラグ。オプションは以下の通りです。</target>
        </trans-unit>
        <trans-unit id="3407c4421a1f6ede0cab565dc5123546e65ddde6" translate="yes" xml:space="preserve">
          <source>Flat geometry, good for density estimation</source>
          <target state="translated">フラットな形状、密度推定に適しています。</target>
        </trans-unit>
        <trans-unit id="748a38982c93bb25fbfeb18b34277c35439ac98c" translate="yes" xml:space="preserve">
          <source>Flavanoids</source>
          <target state="translated">Flavanoids</target>
        </trans-unit>
        <trans-unit id="f55beb472c3b08362b7861294963760ddd037d08" translate="yes" xml:space="preserve">
          <source>Flavanoids:</source>
          <target state="translated">Flavanoids:</target>
        </trans-unit>
        <trans-unit id="1bf94453d6aa9e9092828eefafa6394692100339" translate="yes" xml:space="preserve">
          <source>Flexible pickling control for the communication to and from the worker processes.</source>
          <target state="translated">作業者工程とのやり取りに柔軟な酸洗制御が可能です。</target>
        </trans-unit>
        <trans-unit id="2d83a2dbf42ef510856c4fe5eb69b3efa4599763" translate="yes" xml:space="preserve">
          <source>Flow Chart</source>
          <target state="translated">フローチャート</target>
        </trans-unit>
        <trans-unit id="87698cca8f914c77b735bad53fe489d2af135e70" translate="yes" xml:space="preserve">
          <source>Folder to be used by the pool for memmapping large arrays for sharing memory with worker processes. If None, this will try in order:</source>
          <target state="translated">ワーカープロセスとメモリを共有するための大規模な配列をメモマップするためにプールが使用するフォルダ。Noneの場合は、順番に試行します。</target>
        </trans-unit>
        <trans-unit id="313fc38f449c398563f152e4416c92af47202923" translate="yes" xml:space="preserve">
          <source>Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</source>
          <target state="translated">ランダム性のある構造を見つけるのアルゴリズム4.3に従う。近似行列分解を構築するための確率的アルゴリズム Halko,et al.,2009 (arXiv:909)http://arxiv.org/pdf/0909.4061</target>
        </trans-unit>
        <trans-unit id="ec0c3b76630fd745381cc215a284820af75a683a" translate="yes" xml:space="preserve">
          <source>Footnotes</source>
          <target state="translated">Footnotes</target>
        </trans-unit>
        <trans-unit id="41e2c901c13d4daacf7c9adad4d559c077a8e51e" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;one-vs-rest&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class, n_features]&lt;/code&gt; and &lt;code&gt;[n_class]&lt;/code&gt; respectively. Each row of the coefficients corresponds to one of the &lt;code&gt;n_class&lt;/code&gt; many &amp;ldquo;one-vs-rest&amp;rdquo; classifiers and similar for the intercepts, in the order of the &amp;ldquo;one&amp;rdquo; class.</source>
          <target state="translated">「one-vs-rest」の&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; の場合&lt;/a&gt;、属性 &lt;code&gt;coef_&lt;/code&gt; および &lt;code&gt;intercept_&lt;/code&gt; の形状はそれぞれ &lt;code&gt;[n_class, n_features]&lt;/code&gt; および &lt;code&gt;[n_class]&lt;/code&gt; です。係数の各行は、「1」クラスの順序で、切片の &lt;code&gt;n_class&lt;/code&gt; 多くの「1対rest」分類子の 1つに対応し、類似しています。</target>
        </trans-unit>
        <trans-unit id="c6cc35fe8de003f58f84a7c02bbc9d4b652dc227" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;pairwise&amp;rdquo; metrics, between &lt;em&gt;samples&lt;/em&gt; and not estimators or predictions, see the &lt;a href=&quot;metrics#metrics&quot;&gt;Pairwise metrics, Affinities and Kernels&lt;/a&gt; section.</source>
          <target state="translated">推定値や予測ではなく&lt;em&gt;サンプル&lt;/em&gt;間の「ペアワイズ」メトリックについては、&lt;a href=&quot;metrics#metrics&quot;&gt;ペアワイズメトリック、アフィニティとカーネルの&lt;/a&gt;セクションをご覧ください。</target>
        </trans-unit>
        <trans-unit id="935f9d2961eec1b64a8d3f62208c3a16e0e7ef63" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the &lt;code&gt;n_estimators&lt;/code&gt; parameter of &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;（例えばランダムフォレスト、GBT、ExtraTreesなど）の木の木の数とその深さが最も重要な役割を果たしています。レイテンシとスループットは、ツリーの数に比例してスケーリングする必要があります。このケースでは、直接使用 &lt;code&gt;n_estimators&lt;/code&gt; のパラメータ &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt; を。</target>
        </trans-unit>
        <trans-unit id="c2e53d45d0579b4b39658069206cb04a03ac3808" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge &amp;amp; RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression&amp;hellip;) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; &lt;/a&gt;（例えば投げ縄、ElasticNet、SGDClassifier /回帰、リッジ＆RidgeClassifier、PassiveAggressiveClassifier /回帰、LinearSVC、ロジスティック回帰...）予測時に適用される決定関数はそれほどの待ち時間と同等である必要があり、（内積）と同じです。</target>
        </trans-unit>
        <trans-unit id="8582a7ae6ed830b76bb2d9fd21363d4d3995f59c" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input we suggest to use the &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; class instead. The objective function can be configured to be almost the same as the &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">ため&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;（及び&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;）numpyの配列をコピーしてliblinear内部疎データ表現（倍精度浮動小数点数と非ゼロ成分のINT32インデックス）に変換されるように渡される入力。密集した巨大なC連続倍精度配列を入力としてコピーせずに大規模な線形分類子を適合させたい場合は、代わりに&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;クラスを使用することをお勧めします。目的関数は、&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;モデルとほぼ同じになるように構成できます。</target>
        </trans-unit>
        <trans-unit id="e56c72d645a0047396221ed2eb5407914a9b6bf6" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;make_classification&lt;/code&gt;, three binary and two multi-class classification datasets are generated, with different numbers of informative features and clusters per class.</source>
          <target state="translated">&lt;code&gt;make_classification&lt;/code&gt; 、3進と2つのマルチクラス分類データセットは、有益な機能やクラスごとのクラスタの数が異なると、生成されます。</target>
        </trans-unit>
        <trans-unit id="dda7c631740b122861937f9ebbfdde73fd44b016" translate="yes" xml:space="preserve">
          <source>For Gaussian distributed data, the distance of an observation \(x_i\) to the mode of the distribution can be computed using its Mahalanobis distance: \(d_{(\mu,\Sigma)}(x_i)^2 = (x_i - \mu)'\Sigma^{-1}(x_i - \mu)\) where \(\mu\) and \(\Sigma\) are the location and the covariance of the underlying Gaussian distribution.</source>
          <target state="translated">Gaussian分布データの場合、分布のモードへの観測の距離は、そのマハラノビス距離を使って計算することができます。\(d_{(\mu,\Sigma)}(x_i)^2=(x_i-\mu)'\Sigma^{-1}(x_i-ﾞ\mu))ここで、\(\(i)と\(i)Sigmaは、位置と分布の共分散である。</target>
        </trans-unit>
        <trans-unit id="3fb903a20f5aad7e20f9123d2edfa2a0638dc6bc" translate="yes" xml:space="preserve">
          <source>For \(k\) clusters, the Calinski-Harabaz score \(s\) is given as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:</source>
          <target state="translated">クラスターの場合、Calinski-Harabazのスコアは、クラスター間分散の平均とクラスター内分散の比として与えられます。</target>
        </trans-unit>
        <trans-unit id="d27bcc7c91650762beefd01dc3089ec6978d5c87" translate="yes" xml:space="preserve">
          <source>For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.</source>
          <target state="translated">分類モデルでは,X の各標本の予測されたクラスが返されます.回帰モデルでは,Xに基づく予測値が返されます.</target>
        </trans-unit>
        <trans-unit id="e6fd66f776dfd09a091bc857e8c9d10d50ac3ba8" translate="yes" xml:space="preserve">
          <source>For a comparison of the different scalers, transformers, and normalizers, see &lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples/preprocessing/plot_all_scaling.py&lt;/a&gt;.</source>
          <target state="translated">さまざまなスケーラー、トランスフォーマー、ノーマライザの比較については、&lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples / preprocessing / plot_all_scaling.pyを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="33f57a2a03940da66a0808ac0a9d7e45bc98afe2" translate="yes" xml:space="preserve">
          <source>For a complete probabilistic model we also need a prior distribution for the latent variable \(h\). The most straightforward assumption (based on the nice properties of the Gaussian distribution) is \(h \sim \mathcal{N}(0, \mathbf{I})\). This yields a Gaussian as the marginal distribution of \(x\):</source>
          <target state="translated">完全な確率モデルのためには、潜在変数のための事前分布も必要です。最も簡単な仮定は(ガウス分布の良い性質に基づいて)、 これは、ガウス分布の限界分布となります。</target>
        </trans-unit>
        <trans-unit id="7e3b25cfbbacb17bf9ce066c3983b6610e3fab10" translate="yes" xml:space="preserve">
          <source>For a constant learning rate use &lt;code&gt;learning_rate='constant'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the learning rate.</source>
          <target state="translated">一定の学習率の場合、 &lt;code&gt;learning_rate='constant'&lt;/code&gt; を使用し、 &lt;code&gt;eta0&lt;/code&gt; を使用して学習率を指定します。</target>
        </trans-unit>
        <trans-unit id="76a9227cf28fa05c9bb19673e15a2774476a035c" translate="yes" xml:space="preserve">
          <source>For a description of the implementation and details of the algorithms used, please refer to</source>
          <target state="translated">実装の説明と使用されているアルゴリズムの詳細については、以下を参照してください。</target>
        </trans-unit>
        <trans-unit id="ccaff5036b34bf3986d666eb2f98e4ef943f3dfd" translate="yes" xml:space="preserve">
          <source>For a discussion and comparison of these algorithms, see the &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;manifold module page&lt;/a&gt;</source>
          <target state="translated">これらのアルゴリズムの説明と比較については、&lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;多様体モジュールのページを参照してください&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7b7d7f03d534f8340da15e8579a79cb680de77bd" translate="yes" xml:space="preserve">
          <source>For a document generated from multiple topics, all topics are weighted equally in generating its bag of words.</source>
          <target state="translated">複数のトピックから生成されたドキュメントの場合、すべてのトピックは、その単語袋を生成する際に等しく重み付けされます。</target>
        </trans-unit>
        <trans-unit id="b87483db50bfd800e7f61326ccd19592abcc3547" translate="yes" xml:space="preserve">
          <source>For a few of the best biclusters, its most common document categories and its ten most important words get printed. The best biclusters are determined by their normalized cut. The best words are determined by comparing their sums inside and outside the bicluster.</source>
          <target state="translated">最高のバイクラスターのいくつかについては、その最も一般的な文書カテゴリとその10の最も重要な単語が印刷されます。最良のバイクラスターは、その正規化されたカットによって決定されます。最良の単語は、双クラスタの内側と外側の合計を比較することで決定されます。</target>
        </trans-unit>
        <trans-unit id="859f5c51d38da3ad244e41ebf28b507a4a99bc62" translate="yes" xml:space="preserve">
          <source>For a full code example that demonstrates using a &lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt;&lt;code&gt;FunctionTransformer&lt;/code&gt;&lt;/a&gt; to do custom feature selection, see &lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;Using FunctionTransformer to select columns&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt; &lt;code&gt;FunctionTransformer&lt;/code&gt; &lt;/a&gt;を使用してカスタム機能の選択を行う方法を示す完全なコード例については、FunctionTransformerを&lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;使用した列の&lt;/a&gt;選択を参照してください。</target>
        </trans-unit>
        <trans-unit id="a29c02b5f33160de772eacbd74337a53f6625181" translate="yes" xml:space="preserve">
          <source>For a full-fledged example of out-of-core scaling in a text classification task see &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;.</source>
          <target state="translated">テキスト分類タスクでのコア外スケーリングの完全な例については&lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;、テキストドキュメントのコア外&lt;/a&gt;分類を参照してください。</target>
        </trans-unit>
        <trans-unit id="d78aafa74d01fdbbdb67982f22aabb8fb92e6131" translate="yes" xml:space="preserve">
          <source>For a given value of &lt;code&gt;n_components&lt;/code&gt;&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is often less accurate as &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is cheaper to compute, though, making use of larger feature spaces more efficient.</source>
          <target state="translated">与えられた値については &lt;code&gt;n_components&lt;/code&gt; &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;しばしば以下のように正確である&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;。ただし、&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; の&lt;/a&gt;方が計算コストが安く、より大きな特徴空間をより効率的に利用できます。</target>
        </trans-unit>
        <trans-unit id="46cec00c813e8ea8e2f5bd58264262a28ac481cf" translate="yes" xml:space="preserve">
          <source>For a good choice of alpha, the &lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be &amp;ldquo;sufficiently large&amp;rdquo;, or L1 models will perform at random, where &amp;ldquo;sufficiently large&amp;rdquo; depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated.</source>
          <target state="translated">アルファを適切に選択するために、特定の特定の条件が満たされていれば、&lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;は少数の観測のみを使用して、ゼロ以外の変数の正確なセットを完全に回復できます。特に、サンプルの数は「十分に大きい」必要があります。そうでない場合、L1モデルはランダムに実行されます。「十分に大きい」は、非ゼロ係数の数、特徴の数の対数、ノイズの量、非ゼロ係数の最小絶対値、および計画行列Xの構造。さらに、計画行列は、過度に相関していないなど、特定の特定のプロパティを表示する必要があります。</target>
        </trans-unit>
        <trans-unit id="283fe9d87c4a4faac62d4b9cee8a3089a1cb638f" translate="yes" xml:space="preserve">
          <source>For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number.</source>
          <target state="translated">N個のクラスを持つマルチラベル分類問題では、N個のバイナリ分類器に0からN-1の間の整数が割り当てられます。これらの整数は,連鎖のモデルの順序を定義します.各分類器は,利用可能な学習データに加えて,モデルがより低い数値を割り当てられたクラスの真のラベルを加えて適合させます.</target>
        </trans-unit>
        <trans-unit id="73e6ca6403df9166903acb4326e48adf2d2e8f55" translate="yes" xml:space="preserve">
          <source>For a multi_class problem, if multi_class is set to be &amp;ldquo;multinomial&amp;rdquo; the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.</source>
          <target state="translated">multi_class問題の場合、multi_classが「多項式」に設定されている場合、softmax関数を使用して、各クラスの予測確率が検索されます。それ以外の場合は、1対残りのアプローチを使用します。つまり、ロジスティック関数を使用して、各クラスが正であると仮定して確率を計算します。そして、すべてのクラスにわたってこれらの値を正規化します。</target>
        </trans-unit>
        <trans-unit id="642c44d27e63bf2bd3e040832cd67d4f4b97be3d" translate="yes" xml:space="preserve">
          <source>For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.</source>
          <target state="translated">マルチクラス問題では,各クラスのハイパーパラメタは,すべてのひだとクラスを平行して1対残りを行って得られた最高のスコアを用いて計算されます.したがって,これは真の多項損失ではありません.</target>
        </trans-unit>
        <trans-unit id="3c102da8b9e1a48c3e9639790d9170902789d884" translate="yes" xml:space="preserve">
          <source>For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated.</source>
          <target state="translated">ルートに入る新しい点については,それに最も近いサブクラスタとマージされ,そのサブクラスタの線形和,二乗和,サンプル数が更新されます.これは、リーフノードのプロパティが更新されるまで再帰的に行われます。</target>
        </trans-unit>
        <trans-unit id="d36945b7ba93218440d9a3fb3620ffe9bc7ebec1" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to a sphere dataset, see &lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;Manifold Learning methods on a severed sphere&lt;/a&gt;</source>
          <target state="translated">メソッドが球のデータセットに適用される同様の例については&lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;、切断された球での多様体学習メソッドを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="945c76e7faeb6fe6d013381d6851dfb972b0f8ae" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to the S-curve dataset, see &lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;Comparison of Manifold Learning methods&lt;/a&gt;</source>
          <target state="translated">メソッドがSカーブデータセットに適用される同様の例については&lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;、多様体学習メソッドの比較を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="5693da1a0a426bf5e6f357791de67cea3dcb709e" translate="yes" xml:space="preserve">
          <source>For an adaptively decreasing learning rate, use &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6.</source>
          <target state="translated">適応的に減少する学習率の場合、 &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; を使用し、 &lt;code&gt;eta0&lt;/code&gt; を使用して開始学習率を指定します。停止基準に達すると、学習率は5で除算され、アルゴリズムは停止しません。アルゴリズムは、学習率が1e-6を下回ると停止します。</target>
        </trans-unit>
        <trans-unit id="289eda38dfb97e5735805aabc918de776eb35066" translate="yes" xml:space="preserve">
          <source>For an estimator to be effective, you need the distance between neighboring points to be less than some value \(d\), which depends on the problem. In one dimension, this requires on average \(n \sim 1/d\) points. In the context of the above \(k\)-NN example, if the data is described by just one feature with values ranging from 0 to 1 and with \(n\) training observations, then new data will be no further away than \(1/n\). Therefore, the nearest neighbor decision rule will be efficient as soon as \(1/n\) is small compared to the scale of between-class feature variations.</source>
          <target state="translated">推定器を有効にするためには、隣り合う点間の距離が、問題に依存するいくつかの値よりも小さくなる必要がある。In one dimension,this requires on average.上記の「\(k)-\(k)-NN」の例では,データが0から1までの値を持つ1つの特徴量で記述され,かつ,訓練観測がある場合には,新しいデータは「\(1/n)」よりも遠くには存在しないことになる.したがって,クラス間の特徴変動の規模に比べて,\(1/n\)が小さくなれば,最近傍決定則は効率的になる.</target>
        </trans-unit>
        <trans-unit id="cc92ccd33be99f33389b25ab23e30ca5c4b36d12" translate="yes" xml:space="preserve">
          <source>For an example of using this dataset with scikit-learn, see &lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples/applications/plot_species_distribution_modeling.py&lt;/a&gt;.</source>
          <target state="translated">このデータセットをscikit-learnで使用する例については、&lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples / applications / plot_species_distribution_modeling.pyを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="84cc57ff6bd3680e44c549367baf76fa37633107" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples/cluster/plot_affinity_propagation.py&lt;/a&gt;.</source>
          <target state="translated">例については、&lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples / cluster / plot_affinity_propagation.pyを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="550540d863fd72ef27788284d554fe3cf91d4d31" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples/cluster/plot_dbscan.py&lt;/a&gt;.</source>
          <target state="translated">例は、&lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples / cluster / plot_dbscan.pyを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="5a609c98d64d09ee64c2c225998c2e63797d885b" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples/cluster/plot_mean_shift.py&lt;/a&gt;.</source>
          <target state="translated">例は、&lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples / cluster / plot_mean_shift.pyを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="0c7aeec7cbc3121a908ff21339ef38d8e3682ea4" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples/linear_model/plot_ard.py&lt;/a&gt;.</source>
          <target state="translated">例については、&lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples / linear_model / plot_ard.pyを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="8606b3a4ce46a8dc7d8f3fc386175108176c1a2f" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples/linear_model/plot_bayesian_ridge.py&lt;/a&gt;.</source>
          <target state="translated">例については、&lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples / linear_model / plot_bayesian_ridge.pyを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="1a253fcf6bd3baedce8e1812aafc9e481fd05853" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples/linear_model/plot_lasso_coordinate_descent_path.py&lt;/a&gt;.</source>
          <target state="translated">例については、&lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples / linear_model / plot_lasso_coordinate_descent_path.pyを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="dd893fae3c875581ed42afc5493c8a73ae57ea3a" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples/linear_model/plot_lasso_model_selection.py&lt;/a&gt;.</source>
          <target state="translated">例については、&lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples / linear_model / plot_lasso_model_selection.pyを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="7185c2666c1903f0809fab3c9082ec1324c6a9c7" translate="yes" xml:space="preserve">
          <source>For an introduction to Unicode and character encodings in general, see Joel Spolsky&amp;rsquo;s &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;Absolute Minimum Every Software Developer Must Know About Unicode&lt;/a&gt;.</source>
          <target state="translated">一般的なUnicodeと文字エンコーディングの&lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;概要について&lt;/a&gt;は、Joel Spolskyの絶対的な最小要件を参照してください。</target>
        </trans-unit>
        <trans-unit id="3a88e794655306a2809e5d03a41b7ab87506c6aa" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 (inlier) or -1 (outlier) is returned.</source>
          <target state="translated">1クラスモデルの場合は,+1(正規分布)または-1(外れ値)が返されます.</target>
        </trans-unit>
        <trans-unit id="6b041f627b95dafb713c53f369d3fb59c9505ee0" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 or -1 is returned.</source>
          <target state="translated">1クラスモデルの場合、+1または-1が返されます。</target>
        </trans-unit>
        <trans-unit id="90c9667034ee59a28024b8500c3a1c0772f75e0b" translate="yes" xml:space="preserve">
          <source>For an overview of available strategies in scikit-learn, see also the &lt;a href=&quot;computing#scaling-strategies&quot;&gt;out-of-core learning&lt;/a&gt; documentation.</source>
          <target state="translated">scikit-learnで利用可能な戦略の概要については、&lt;a href=&quot;computing#scaling-strategies&quot;&gt;コア外学習の&lt;/a&gt;ドキュメントもご覧ください。</target>
        </trans-unit>
        <trans-unit id="bd83f9999f935e2f6fce3641b48e5b3393b97a71" translate="yes" xml:space="preserve">
          <source>For binary classification with a true label \(y \in \{0,1\}\) and a probability estimate \(p = \operatorname{Pr}(y = 1)\), the log loss per sample is the negative log-likelihood of the classifier given the true label:</source>
          <target state="translated">真のラベル \(y \in \{0,1\})と確率推定値 \(p=\operatoratorname{Pr}(y=1)1)を持つ2値分類の場合,log loss per sampleは,真のラベルが与えられた分類器の負の対数尤度である.</target>
        </trans-unit>
        <trans-unit id="23a4f6b8b8e57d58b02ac23ef6f32b82b6049d45" translate="yes" xml:space="preserve">
          <source>For binary classification, \(f(x)\) passes through the logistic function \(g(z)=1/(1+e^{-z})\) to obtain output values between zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the rest to the negative class.</source>
          <target state="translated">2値分類では、0から1の間の出力値を得るために、ロジスティック関数\(g(z)=1/(1+e^{-z})を通過します。0.5に設定されたしきい値は、0.5より大きいか等しい出力のサンプルを正のクラスに割り当て、残りのサンプルを負のクラスに割り当てます。</target>
        </trans-unit>
        <trans-unit id="883efcc2dc17e184b74392564fb44d2a6f9c0bf6" translate="yes" xml:space="preserve">
          <source>For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:</source>
          <target state="translated">二進法の問題では、以下のようにして、真の陰性、偽の陽性、偽の陰性、真の陽性のカウントを得ることができます。</target>
        </trans-unit>
        <trans-unit id="7f954d6786e07c3ef37ff8e0245acb91efbb4b2a" translate="yes" xml:space="preserve">
          <source>For classification with &lt;code&gt;loss='deviance'&lt;/code&gt; the target response is logit(p).</source>
          <target state="translated">&lt;code&gt;loss='deviance'&lt;/code&gt; の分類の場合、ターゲット応答はlogit（p）です。</target>
        </trans-unit>
        <trans-unit id="4118e1de638d62fd337275c2f8d28f38f1e40db3" translate="yes" xml:space="preserve">
          <source>For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">ロジスティック損失のある分類の場合、平均戦略を使用するSGDの別のバリアントは、&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; の&lt;/a&gt;ソルバーとして利用可能な確率的平均勾配（SAG）アルゴリズムで利用できます。</target>
        </trans-unit>
        <trans-unit id="7a750c34f262db1f2c0c844eb9774104416e6f5c" translate="yes" xml:space="preserve">
          <source>For classification you can think of it as the regression score before the link function.</source>
          <target state="translated">分類の場合は、リンク機能の前の回帰スコアと考えることができます。</target>
        </trans-unit>
        <trans-unit id="6a3d9b2c887776af95639587c4c76f62b0d1c8f1" translate="yes" xml:space="preserve">
          <source>For classification, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt;&lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='hinge'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_hinge'&lt;/code&gt; (PA-II). For regression, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt;&lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; (PA-II).</source>
          <target state="translated">分類の場合、&lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt; &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; &lt;/a&gt;は &lt;code&gt;loss='hinge'&lt;/code&gt; （PA-I）または &lt;code&gt;loss='squared_hinge'&lt;/code&gt; （PA-II）で使用できます。回帰の場合、&lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt; &lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt; &lt;/a&gt;は &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; （PA-I）または &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; （PA-II）で使用できます。</target>
        </trans-unit>
        <trans-unit id="27898fb8346ff80dce087f616900965fd4196842" translate="yes" xml:space="preserve">
          <source>For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first &lt;code&gt;partial_fit&lt;/code&gt; call using the &lt;code&gt;classes=&lt;/code&gt; parameter.</source>
          <target state="translated">分類に関して注意すべき重要なことは、ステートレスな特徴抽出ルーチンは新しい/目に見えない属性に対処できるかもしれないが、インクリメンタルラーナー自体は新しい/目に見えないターゲットクラスに対処できないかもしれないということです。この場合、 &lt;code&gt;classes=&lt;/code&gt; パラメータを使用して、可能なすべてのクラスを最初の &lt;code&gt;partial_fit&lt;/code&gt; 呼び出しに渡す必要があります。</target>
        </trans-unit>
        <trans-unit id="fda4c776ec3f2170d3e99301b50afa3144298d48" translate="yes" xml:space="preserve">
          <source>For classification, as in the labeling &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;iris&lt;/a&gt; task, linear regression is not the right approach as it will give too much weight to data far from the decision frontier. A linear approach is to fit a sigmoid function or &lt;strong&gt;logistic&lt;/strong&gt; function:</source>
          <target state="translated">分類では、&lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;アイリス&lt;/a&gt;のラベリングタスクと同様に、線形回帰は、意思決定フロンティアから離れたデータに過度の重みを与えるため、適切なアプローチではありません。線形アプローチは、シグモイド関数または&lt;strong&gt;ロジスティック&lt;/strong&gt;関数を近似することです。</target>
        </trans-unit>
        <trans-unit id="049512f622ab4a1900a694aa9ec5fcf56244d47a" translate="yes" xml:space="preserve">
          <source>For classification: &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt;&lt;code&gt;f_classif&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">分類の場合：&lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt; &lt;code&gt;f_classif&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2b5c234d472222c920c562abfd3ef4ec80e6cfee" translate="yes" xml:space="preserve">
          <source>For comparison, a quantized image using a random codebook (colors picked up randomly) is also shown.</source>
          <target state="translated">比較のために、ランダムなコードブック(色をランダムに拾ったもの)を使って量子化した画像も示しています。</target>
        </trans-unit>
        <trans-unit id="3be5878f7d3ebd4495804d5a6a55058ed71eceb1" translate="yes" xml:space="preserve">
          <source>For comparison, the documents are also clustered using MiniBatchKMeans. The document clusters derived from the biclusters achieve a better V-measure than clusters found by MiniBatchKMeans.</source>
          <target state="translated">比較のために、文書もMiniBatchKMeansを用いてクラスタ化しています。バイクラスターから得られた文書クラスタは、MiniBatchKMeansで得られたクラスタよりも優れたV値を達成しています。</target>
        </trans-unit>
        <trans-unit id="6d421941474530194b10c6be8d7b89e2eb530191" translate="yes" xml:space="preserve">
          <source>For comparison, we also add the output from &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt;. It can force any arbitrary distribution into a gaussian, provided that there are enough training samples (thousands). Because it is a non-parametric method, it is harder to interpret than the parametric ones (Box-Cox and Yeo-Johnson).</source>
          <target state="translated">比較のために、 &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt; からの出力も追加します。十分なトレーニングサンプル（数千）がある場合、任意の分布を強制的にガウス分布にすることができます。これはノンパラメトリックな方法であるため、パラメトリックな方法（Box-CoxおよびYeo-Johnson）よりも解釈が困難です。</target>
        </trans-unit>
        <trans-unit id="3fedc899dde91ba75e541f7b8d87d7a3665ca193" translate="yes" xml:space="preserve">
          <source>For compatibility, user code relying on this method should wrap its calls in &lt;code&gt;np.asarray&lt;/code&gt; to avoid type issues.</source>
          <target state="translated">互換性のために、このメソッドに依存するユーザーコードは、型の問題を回避するために、その呼び出しを &lt;code&gt;np.asarray&lt;/code&gt; にラップする必要があります。</target>
        </trans-unit>
        <trans-unit id="6f31aee2196032d492cf3c67f7f43ab3f6981996" translate="yes" xml:space="preserve">
          <source>For continuous parameters, such as &lt;code&gt;C&lt;/code&gt; above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing &lt;code&gt;n_iter&lt;/code&gt; will always lead to a finer search.</source>
          <target state="translated">上記の &lt;code&gt;C&lt;/code&gt; などの連続パラメーターの場合、ランダム化を最大限に活用するには、連続分布を指定することが重要です。このように、 &lt;code&gt;n_iter&lt;/code&gt; を増やすと、常により詳細な検索が行われます。</target>
        </trans-unit>
        <trans-unit id="4288bdf523218f188616117af5e7ab510c1e81a7" translate="yes" xml:space="preserve">
          <source>For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms give roughly the same results. They differ with regards to their execution speed and sources of numerical errors.</source>
          <target state="translated">交差検証のために、Lassoパスを計算するために2つのアルゴリズムを20倍に使用します:LassoCVクラスで実装されている座標降下とLassoLarsCVクラスで実装されているLars(最小角度回帰)です。どちらのアルゴリズムもほぼ同じ結果が得られます。両者は,実行速度と数値誤差の発生源が異なります.</target>
        </trans-unit>
        <trans-unit id="c8cbf2457961ac615bed8ee5d0896fee418cd0e6" translate="yes" xml:space="preserve">
          <source>For custom messages if &amp;ldquo;%(name)s&amp;rdquo; is present in the message string, it is substituted for the estimator name.</source>
          <target state="translated">カスタムメッセージの場合、「％（name）s」がメッセージ文字列に存在する場合は、推定器名の代わりに使用されます。</target>
        </trans-unit>
        <trans-unit id="67e0a596cb4bf62edc844af1488251663768a571" translate="yes" xml:space="preserve">
          <source>For details on the precise mathematical formulation of the provided kernel functions and how &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;coef0&lt;/code&gt; and &lt;code&gt;degree&lt;/code&gt; affect each other, see the corresponding section in the narrative documentation: &lt;a href=&quot;../svm#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt;.</source>
          <target state="translated">提供されているカーネル関数の正確な数学的定式化の詳細と、 &lt;code&gt;gamma&lt;/code&gt; 、 &lt;code&gt;coef0&lt;/code&gt; 、および &lt;code&gt;degree&lt;/code&gt; が互いにどのように影響するかについては、ナラティブドキュメントの対応するセクション「&lt;a href=&quot;../svm#svm-kernels&quot;&gt;カーネル関数&lt;/a&gt;」を参照してください。</target>
        </trans-unit>
        <trans-unit id="e4ed0f1e2f8642affc7014ca7518ae7bfac1b31c" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_features, n_k], with &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; It is the rotation of the Gaussian distribution, i.e. its principal axis.</source>
          <target state="translated">各クラスkについて、形状の配列[n_features、n_k]、 &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; これは、ガウス分布、つまりその主軸の回転です。</target>
        </trans-unit>
        <trans-unit id="a5ae820e12dddee4457f060f6b705b304ca3a1e3" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_k]. It contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system.</source>
          <target state="translated">各クラス k に対して形状 [n_k]の配列。これは,その主軸に沿ったガウス分布のスケーリング,すなわち回転座標系における分散を含みます.</target>
        </trans-unit>
        <trans-unit id="bb1ef71f090300eb5b67a4f2bd338bada7005d30" translate="yes" xml:space="preserve">
          <source>For each class of models we make the model complexity vary through the choice of relevant model parameters and measure the influence on both computational performance (latency) and predictive power (MSE or Hamming Loss).</source>
          <target state="translated">モデルのクラスごとに、関連するモデルパラメータの選択によってモデルの複雑さを変化させ、計算性能(レイテンシー)と予測力(MSEまたはハミング損失)の両方への影響を測定します。</target>
        </trans-unit>
        <trans-unit id="51af5a1cbe5b213e1b6f97e9040e40d195d22222" translate="yes" xml:space="preserve">
          <source>For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</source>
          <target state="translated">各コンポーネントkについて、 &lt;code&gt;|u| = |v| = 1&lt;/code&gt; ように、最大​​corr（Xk u、Yk v）を最大化する重みu、vを見つけます。= | v | = 1</target>
        </trans-unit>
        <trans-unit id="0acc93a412fe0bc296f4de29ad2df21b9415e5fb" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimize:</source>
          <target state="translated">各成分 k について,最適化された重み u,v を求めます.</target>
        </trans-unit>
        <trans-unit id="34ffb7458447c8e84aeb0c0dcae78f73f1c9783e" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimizes: &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt;, such that &lt;code&gt;|u| = 1&lt;/code&gt;</source>
          <target state="translated">各コンポーネントkについて、最適化する重みu、vを見つけます &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt; 、 &lt;code&gt;|u| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f9db939b51ba3d78630796e1c0444915001bc251" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator.</source>
          <target state="translated">Xの各データポイントx、およびアンサンブル内の各木について、各推定器で終了する葉xのインデックスを返します。</target>
        </trans-unit>
        <trans-unit id="4571d700205de7840eb7f685393b9c35a449521a" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. In the case of binary classification n_classes is 1.</source>
          <target state="translated">Xの各データポイントxとアンサンブル内の各木について、各推定器で終わる葉xのインデックスを返します。2値分類の場合、n_classesは1である。</target>
        </trans-unit>
        <trans-unit id="44ac20da24a40ac490697d0897d69873261c6900" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.</source>
          <target state="translated">X の各データポイント x と森の各木について、x が終点とする葉のインデックスを返します。</target>
        </trans-unit>
        <trans-unit id="d82941d49be2d46b8acb0305feded896c81f7a70" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt;, possibly with gaps in the numbering.</source>
          <target state="translated">Xの各データポイントxについて、xが最終的に終わる葉のインデックスを返します。葉には &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt; 、おそらく番号付けにギャップがあります。</target>
        </trans-unit>
        <trans-unit id="7b85d3d77de0d2c34f470b25ce92cd73fbf8293c" translate="yes" xml:space="preserve">
          <source>For each dataset, 15% of samples are generated as random uniform noise. This proportion is the value given to the nu parameter of the OneClassSVM and the contamination parameter of the other outlier detection algorithms. Decision boundaries between inliers and outliers are displayed in black except for Local Outlier Factor (LOF) as it has no predict method to be applied on new data when it is used for outlier detection.</source>
          <target state="translated">各データセットでは,サンプルの15%がランダムな一様ノイズとして生成されます.この割合は、OneClassSVMのnuパラメータと他の外れ値検出アルゴリズムのコンタミネーションパラメータに与えられた値です。インライアとアウトライアの間の決定境界は、アウトライア検出のために使用されるときに新しいデータに適用される予測方法を持たないので、ローカル・アウトライア・ファクター(LOF)を除いて黒で表示されます。</target>
        </trans-unit>
        <trans-unit id="894d665cd020f2e7e68923ae49f3798173d1a959" translate="yes" xml:space="preserve">
          <source>For each document &lt;code&gt;#i&lt;/code&gt;, count the number of occurrences of each word &lt;code&gt;w&lt;/code&gt; and store it in &lt;code&gt;X[i, j]&lt;/code&gt; as the value of feature &lt;code&gt;#j&lt;/code&gt; where &lt;code&gt;j&lt;/code&gt; is the index of word &lt;code&gt;w&lt;/code&gt; in the dictionary.</source>
          <target state="translated">各ドキュメント &lt;code&gt;#i&lt;/code&gt; について、各単語 &lt;code&gt;w&lt;/code&gt; の出現回数をカウントし、 &lt;code&gt;X[i, j]&lt;/code&gt; に特徴 &lt;code&gt;#j&lt;/code&gt; の値として格納します。ここで、 &lt;code&gt;j&lt;/code&gt; は辞書内の単語 &lt;code&gt;w&lt;/code&gt; のインデックスです。</target>
        </trans-unit>
        <trans-unit id="f8df1081a030b15d2d8afea6cd05ff167080d1cd" translate="yes" xml:space="preserve">
          <source>For each document \(d\), draw \(\theta_d \sim \mathrm{Dirichlet}(\alpha), \: d=1...D\)</source>
          <target state="translated">For each document ¶(d),draw \(D\)theta_d \sim \mathrm{Dirichlet}(\alpha),I'm sorry:d=1...D\)</target>
        </trans-unit>
        <trans-unit id="d88ce97fd881b7b3225357f99ece9e603e7378b5" translate="yes" xml:space="preserve">
          <source>For each observation, tells whether or not (+1 or -1) it should be considered as an inlier according to the fitted model.</source>
          <target state="translated">各オブザベーションについて、フィットしたモデルに従って、それをインライアとみなすべきかどうか(+1か-1か)を指示する。</target>
        </trans-unit>
        <trans-unit id="ceeb3b0129a3e252c3947f10f0ffdea6343158bd" translate="yes" xml:space="preserve">
          <source>For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples.</source>
          <target state="translated">虹彩の特徴の各ペアについて、決定木は、訓練サンプルから推測される単純な閾値ルールの組み合わせからなる決定境界を学習する。</target>
        </trans-unit>
        <trans-unit id="ebc579ef4368e5d7216210ea27aaa16d7216a1cd" translate="yes" xml:space="preserve">
          <source>For each sample, the generative process is:</source>
          <target state="translated">各サンプルについて、生成プロセスは</target>
        </trans-unit>
        <trans-unit id="884540e9625ad90cb4316f6468437987a56adbd4" translate="yes" xml:space="preserve">
          <source>For each topic \(k\), draw \(\beta_k \sim \mathrm{Dirichlet}(\eta),\: k =1...K\)</source>
          <target state="translated">For each topic ¶(k),draw \(k \beta_k ━━━━━━━━!!!!</target>
        </trans-unit>
        <trans-unit id="ebc60a8584824b9b236f9d24a5aa9b01b10427f0" translate="yes" xml:space="preserve">
          <source>For each value of &lt;code&gt;n_components&lt;/code&gt;, we plot:</source>
          <target state="translated">&lt;code&gt;n_components&lt;/code&gt; の各値について、以下をプロットします。</target>
        </trans-unit>
        <trans-unit id="6ad9736839514923dd21aa4c5541f4da9ec0e497" translate="yes" xml:space="preserve">
          <source>For each value of the &amp;lsquo;target&amp;rsquo; features in the &lt;code&gt;grid&lt;/code&gt; the partial dependence function need to marginalize the predictions of a tree over all possible values of the &amp;lsquo;complement&amp;rsquo; features. In decision trees this function can be evaluated efficiently without reference to the training data. For each grid point a weighted tree traversal is performed: if a split node involves a &amp;lsquo;target&amp;rsquo; feature, the corresponding left or right branch is followed, otherwise both branches are followed, each branch is weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all visited leaves. For tree ensembles the results of each individual tree are again averaged.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 内の「ターゲット」フィーチャの各値について、部分依存関数は、「補数」フィーチャのすべての可能な値に対してツリーの予測を周辺化する必要があります。決定木では、この関数はトレーニングデータを参照せずに効率的に評価できます。各グリッドポイントに対して、重み付きツリートラバーサルが実行されます。分割ノードに「ターゲット」機能が含まれる場合、対応する左または右のブランチが追跡されます。それ以外の場合は、両方のブランチが追跡され、各ブランチは、入力されたトレーニングサンプルの割合によって重み付けされます。ブランチ。最後に、部分的な依存関係は、すべての訪問した葉の加重平均によって与えられます。ツリーアンサンブルの場合、個々のツリーの結果が再び平均化されます。</target>
        </trans-unit>
        <trans-unit id="719d4a30c8dd97a24789edd5bdd1f3a14cdc8a6c" translate="yes" xml:space="preserve">
          <source>For each word \(i\) in document \(d\):</source>
          <target state="translated">For each word \(i)in document ♦♦.</target>
        </trans-unit>
        <trans-unit id="d13eb916a8aa10457ca38f56195d8da451f22b82" translate="yes" xml:space="preserve">
          <source>For efficiency reasons, the euclidean distance between a pair of row vector x and y is computed as:</source>
          <target state="translated">効率的な理由から、行ベクトル x と y のペア間のユークリッド距離は次のように計算されます。</target>
        </trans-unit>
        <trans-unit id="9aa1f675d2607b44cb2ebebaba9400cb8fdf4c6d" translate="yes" xml:space="preserve">
          <source>For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values.</source>
          <target state="translated">複数のメトリクスを評価するためには、(一意の)文字列のリストを与えるか、名前をキーとし、呼び出し可能な値を持つ dict を与えます。</target>
        </trans-unit>
        <trans-unit id="091a4026feae279bd855844156c1035b747b54da" translate="yes" xml:space="preserve">
          <source>For example &lt;code&gt;average_precision&lt;/code&gt; or the area under the roc curve can not be computed using discrete predictions alone.</source>
          <target state="translated">たとえば、 &lt;code&gt;average_precision&lt;/code&gt; またはroc曲線の下の領域は、離散予測のみを使用して計算することはできません。</target>
        </trans-unit>
        <trans-unit id="d73a71b2256308d0d5e12341f8e7d64f3e849f98" translate="yes" xml:space="preserve">
          <source>For example try instead of the &lt;code&gt;SVC&lt;/code&gt;:</source>
          <target state="translated">たとえば、 &lt;code&gt;SVC&lt;/code&gt; の代わりに試してください。</target>
        </trans-unit>
        <trans-unit id="541edf61321b8728dd0c7cafa11d713cadb8fb1e" translate="yes" xml:space="preserve">
          <source>For example, a less computationally intensive alternative to &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; would be &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt;.</source>
          <target state="translated">たとえば、 &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; の代わりに計算量が少ない代替手段は &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="65cec63d764ed66c423ae4b0636bcfb02973f7ba" translate="yes" xml:space="preserve">
          <source>For example, a simple linear regression can be extended by constructing &lt;strong&gt;polynomial features&lt;/strong&gt; from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:</source>
          <target state="translated">たとえば、係数から&lt;strong&gt;多項式の特徴&lt;/strong&gt;を作成することにより、単純な線形回帰を拡張できます。標準の線形回帰の場合、2次元データに対して次のようなモデルがある可能性があります。</target>
        </trans-unit>
        <trans-unit id="f87725ef6020293ce39f30b54128c30f50570f0e" translate="yes" xml:space="preserve">
          <source>For example, if each point is just a single number (8 bytes), then an effective \(k\)-NN estimator in a paltry \(p \sim 20\) dimensions would require more training data than the current estimated size of the entire internet (&amp;plusmn;1000 Exabytes or so).</source>
          <target state="translated">たとえば、各ポイントが単一の数値（8バイト）のみの場合、わずかな\（p \ sim 20 \）次元の効果的な\（k \）-NN推定量は、現在の推定サイズよりも多くのトレーニングデータを必要とします。インターネット全体（&amp;plusmn;1000エクサバイト程度）。</target>
        </trans-unit>
        <trans-unit id="5e44134c443036a12804aff41c3842c8f74c39ce" translate="yes" xml:space="preserve">
          <source>For example, in random projection, this warning is raised when the number of components, which quantifies the dimensionality of the target projection space, is higher than the number of features, which quantifies the dimensionality of the original source space, to imply that the dimensionality of the problem will not be reduced.</source>
          <target state="translated">例えば、ランダム投影では、対象投影空間の次元性を定量化する成分の数が、元の元空間の次元性を定量化する特徴量の数よりも多い場合に、問題の次元性が低減されないことを暗示するために、この警告を発する。</target>
        </trans-unit>
        <trans-unit id="4bb7f297d1cf6895bcaff7553e1e2a2edd1164e9" translate="yes" xml:space="preserve">
          <source>For example, in the cases of multiple experiments, &lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt;&lt;code&gt;LeaveOneGroupOut&lt;/code&gt;&lt;/a&gt; can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one:</source>
          <target state="translated">たとえば、複数の実験の場合、&lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt; &lt;code&gt;LeaveOneGroupOut&lt;/code&gt; &lt;/a&gt;を使用して、さまざまな実験に基づく交差検証を作成できます。1つを除くすべての実験のサンプルを使用してトレーニングセットを作成します。</target>
        </trans-unit>
        <trans-unit id="9dc98c27045ddaa13bc1efc30f0c70951a11ebf1" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:</source>
          <target state="translated">たとえば、多項式の単純ベイズ分類器の結果を見てみましょう。これはトレーニングが高速で、適切なFスコアを実現します。</target>
        </trans-unit>
        <trans-unit id="85d6aa34dc31a086da5a0b5a46fa6367e968ccaf" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s say we&amp;rsquo;re dealing with a corpus of two documents: &lt;code&gt;['words', 'wprds']&lt;/code&gt;. The second document contains a misspelling of the word &amp;lsquo;words&amp;rsquo;. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:</source>
          <target state="translated">たとえば、2つのドキュメントのコーパス &lt;code&gt;['words', 'wprds']&lt;/code&gt; を扱っているとしましょう。2番目のドキュメントには、単語「words」のスペルミスが含まれています。単純な単語のバッグ表現では、これら2つは非常に異なるドキュメントと見なされ、2つの可能な機能の両方が異なります。ただし、文字の2グラム表現は、8つの機能のうち4つに一致するドキュメントを検出します。これにより、優先分類子がより適切に判断できるようになります。</target>
        </trans-unit>
        <trans-unit id="a34fc3c98b3c662bed7829df5d3e68b291f14f22" translate="yes" xml:space="preserve">
          <source>For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word &amp;lsquo;sat&amp;rsquo; in the sentence &amp;lsquo;The cat sat on the mat.&amp;rsquo;:</source>
          <target state="translated">たとえば、シーケンス分類子（チャンカーなど）をトレーニングするための補足タグとして使用したい品詞（PoS）タグを抽出する最初のアルゴリズムがあるとします。次の口述は、「猫がマットの上に座った」という文の「土」という単語の周りに抽出された特徴のウィンドウのようなものです。</target>
        </trans-unit>
        <trans-unit id="875d6b4f0ebd92b4525ff9ff007e35a4ef09b992" translate="yes" xml:space="preserve">
          <source>For example, the following snippet uses &lt;code&gt;chardet&lt;/code&gt; (not shipped with scikit-learn, must be installed separately) to figure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not shown here.</source>
          <target state="translated">たとえば、次のスニペットは、 &lt;code&gt;chardet&lt;/code&gt; （scikit-learnに同梱されていません。個別にインストールする必要があります）を使用して、3つのテキストのエンコーディングを把握します。次に、テキストをベクトル化し、学習した語彙を出力します。出力はここには表示されません。</target>
        </trans-unit>
        <trans-unit id="a5b55b26c17fcbb577abf03053fdea355f9d1a85" translate="yes" xml:space="preserve">
          <source>For example, this warning may occur when the user</source>
          <target state="translated">例えば、この警告は、ユーザーが</target>
        </trans-unit>
        <trans-unit id="fa61683485e98ae724ab66c0cc502f9a28b6f341" translate="yes" xml:space="preserve">
          <source>For example, to download a dataset of gene expressions in mice brains:</source>
          <target state="translated">例えば、マウス脳内の遺伝子発現のデータセットをダウンロードするには、以下のようにします。</target>
        </trans-unit>
        <trans-unit id="bea8752fb35944e93422e8c1c3a159c63e531901" translate="yes" xml:space="preserve">
          <source>For example, we can compute the tf-idf of the first term in the first document in the &lt;code&gt;counts&lt;/code&gt; array as follows:</source>
          <target state="translated">たとえば、次のように &lt;code&gt;counts&lt;/code&gt; 配列の最初のドキュメントの最初の項のtf-idfを計算できます。</target>
        </trans-unit>
        <trans-unit id="8e5e851ba9e40a81086c0f41a19109e3eeaaee54" translate="yes" xml:space="preserve">
          <source>For example, when dealing with boolean features, \(x_i^n = x_i\) for all \(n\) and is therefore useless; but \(x_i x_j\) represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier:</source>
          <target state="translated">例えば、ブーリアン特徴を扱う場合、\(x_i^n=x_i\)for all \(n\)and is therefore useless;but \(x_i x_j)は、2つのブーリアンの結合を表します。このようにして、線形分類器でXOR問題を解くことができます。</target>
        </trans-unit>
        <trans-unit id="dfe2d757676022996b9aa748350ec295d5357a7e" translate="yes" xml:space="preserve">
          <source>For example, when using a validation set, set the &lt;code&gt;test_fold&lt;/code&gt; to 0 for all samples that are part of the validation set, and to -1 for all other samples.</source>
          <target state="translated">たとえば、検証セットを使用する場合、検証セットの一部であるすべてのサンプルについては &lt;code&gt;test_fold&lt;/code&gt; を0に設定し、他のすべてのサンプルについては-1に設定します。</target>
        </trans-unit>
        <trans-unit id="98a47ab600b3d6adb5169d4d316e6fcca6b662b8" translate="yes" xml:space="preserve">
          <source>For examples on how it is to be used refer to the sections below.</source>
          <target state="translated">使用例については、以下のセクションを参照してください。</target>
        </trans-unit>
        <trans-unit id="1fd7dfc113fdc87e0b1f446fd7d77e9da35e9457" translate="yes" xml:space="preserve">
          <source>For further details on bias-variance decomposition, see section 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">バイアス分散分解の詳細については、&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]の&lt;/a&gt;セクション7.3を参照してください。</target>
        </trans-unit>
        <trans-unit id="2f1f137cd461ac2e31c6cc087610e0dfea901ed1" translate="yes" xml:space="preserve">
          <source>For further details, &amp;ldquo;How to Use t-SNE Effectively&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt; provides a good discussion of the effects of various parameters, as well as interactive plots to explore those effects.</source>
          <target state="translated">詳細については、「t-SNEを効果的に使用する方法」&lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt;で、さまざまなパラメーターの効果についての適切な説明と、それらの効果を調べるためのインタラクティブなプロットを提供しています。</target>
        </trans-unit>
        <trans-unit id="0f1bbe6e8000be72ab1e63dd45896ee0e4b6eb7a" translate="yes" xml:space="preserve">
          <source>For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (&lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;) can perform effective non-linear feature extraction.</source>
          <target state="translated">手書きの数字認識のように、ピクセル値を白い背景の黒さの度合いとして解釈できるグレースケール画像データの場合、ベルヌーイ制限付きボルツマンマシンモデル（&lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt;）は、効果的な非線形特徴抽出を実行できます。</target>
        </trans-unit>
        <trans-unit id="866314f9db098f25a642d6cb6f4a133adac68ce1" translate="yes" xml:space="preserve">
          <source>For high-dimensional datasets with many collinear regressors, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; is most often preferable. However, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt;&lt;code&gt;LassoLarsCV&lt;/code&gt;&lt;/a&gt; has the advantage of exploring more relevant values of &lt;code&gt;alpha&lt;/code&gt; parameter, and if the number of samples is very small compared to the number of features, it is often faster than &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">多くの共線回帰子を持つ高次元のデータセットの場合、&lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt;が最もよく使用されます。ただし、&lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt; &lt;code&gt;LassoLarsCV&lt;/code&gt; に&lt;/a&gt;は、 &lt;code&gt;alpha&lt;/code&gt; パラメーターのより関連性の高い値を探索できるという利点があり、サンプルの数がフィーチャの数と比較して非常に少ない場合は、多くの場合&lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt;よりも高速です。</target>
        </trans-unit>
        <trans-unit id="aff7e1aca1f3054316321a609c5b24bbfe0a02a6" translate="yes" xml:space="preserve">
          <source>For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.</source>
          <target state="translated">NIST の前処理ルーチンについては、M.D.Garris,J.L.Blue,G.T.Candela,D.L.Dimmick,J.Geist,P.J.Grother,S.A.Janet,and C.L.Wilson,NIST Form-Based Handprint Recognition System,NISTIR 5469,1994 を参照してください。</target>
        </trans-unit>
        <trans-unit id="ee57e485cfe4c61d12541e3ea6e169aab79014e8" translate="yes" xml:space="preserve">
          <source>For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.</source>
          <target state="translated">例えば、10,000個の短いテキスト文書(電子メールなど)のコレクションは、それぞれの文書が個別に100〜1000個のユニークな単語を使用している間に、合計で10万個のユニークな単語の順序でサイズの語彙を使用しています。</target>
        </trans-unit>
        <trans-unit id="4e9f2f3fee78ca296cddfe5ea5b4e060f79a0a4e" translate="yes" xml:space="preserve">
          <source>For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">例えば、学習アルゴリズムの目的関数で使用される多くの要素(サポートベクターマシンのRBFカーネルや線形モデルのL1およびL2正則化器など)は、すべての特徴が0を中心とし、同じ順序で分散を持つことを前提としています。ある特徴が他の特徴よりも桁違いに大きい分散を持っている場合、それが目的関数を支配し、推定器が他の特徴から期待通りに正しく学習できなくなる可能性があります。</target>
        </trans-unit>
        <trans-unit id="9c54ff6618aa4505fc044efee7a1b7aa2d457afd" translate="yes" xml:space="preserve">
          <source>For instance the below given table</source>
          <target state="translated">例えば、以下の表のようになります。</target>
        </trans-unit>
        <trans-unit id="61fc71afaf6946d5d1cad0702c1f4030326fcb83" translate="yes" xml:space="preserve">
          <source>For instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.</source>
          <target state="translated">例えば、グループはサンプルの収集年とすることができ、それによって時間ベースの分割に対する交差検証を可能にします。</target>
        </trans-unit>
        <trans-unit id="c007161505dacd37b43b63ce0c84bfbd3ce25160" translate="yes" xml:space="preserve">
          <source>For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below.</source>
          <target state="translated">例えば、インライアのデータがガウス分布であると仮定すると、インライアの位置と共分散をロバストな方法で(つまり、外れ値の影響を受けずに)推定します。この推定値から得られるマハラノビス距離は、アウトライネスの尺度を導出するために使用されます。この戦略は以下に図示されている。</target>
        </trans-unit>
        <trans-unit id="7519828cefe279ed0b1f593fd20db7243c914832" translate="yes" xml:space="preserve">
          <source>For instance, given a matrix of shape &lt;code&gt;(10, 10)&lt;/code&gt;, one possible bicluster with three rows and two columns induces a submatrix of shape &lt;code&gt;(3, 2)&lt;/code&gt;:</source>
          <target state="translated">たとえば、形状 &lt;code&gt;(10, 10)&lt;/code&gt; 行列が与えられた場合、3つの行と2つの列を持つ1つの可能なバイクラスターは、形状の部分行列 &lt;code&gt;(3, 2)&lt;/code&gt; 3、2 ）を誘導します。</target>
        </trans-unit>
        <trans-unit id="66b8e52235d720becae09b00e19696b279a13527" translate="yes" xml:space="preserve">
          <source>For instance, if \(p\) singular vectors were calculated, the \(q\) best are found as described, where \(q&amp;lt;p\). Let \(U\) be the matrix with columns the \(q\) best left singular vectors, and similarly \(V\) for the right. To partition the rows, the rows of \(A\) are projected to a \(q\) dimensional space: \(A * V\). Treating the \(m\) rows of this \(m \times q\) matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to \(A^{\top} * U\) and clustering this \(n \times q\) matrix yields the column labels.</source>
          <target state="translated">たとえば、\（p \）特異ベクトルが計算された場合、\（q \）は、説明どおりに見つかります。ここで、\（q &amp;lt;p \）です。\（U \）を列の行列、\（q \）を左の特異ベクトル、そして右を\（V \）とする。行を分割するには、\（A \）の行を\（q \）次元空間に投影します：\（A * V \）。この\（m \ times q \）行列の\（m \）行をサンプルとして扱い、k平均を使用してクラスタリングすると、行ラベルが生成されます。同様に、列を\（A ^ {\ top} * U \）に射影し、この\（n \ times q \）行列をクラスター化すると、列ラベルが生成されます。</target>
        </trans-unit>
        <trans-unit id="040e034a022032a75dc3b85f4ce9de7cff88a6fc" translate="yes" xml:space="preserve">
          <source>For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time.</source>
          <target state="translated">例えば,顔認識のために64x64ピクセルのグレーレベル画像を扱う場合,データの次元数は4096であり,このような広いデータでRBFサポートベクターマシンを訓練するのは遅い.さらに,人間の顔の写真はすべて似ているので,データの本質的な次元は4096よりもはるかに低いことがわかっています.サンプルは,はるかに低い次元の多様体上に存在します(例えば,200程度).PCAアルゴリズムは、データを線形変換するために使用できますが、同時に次元数を減らし、同時に説明された分散の大部分を保存します。</target>
        </trans-unit>
        <trans-unit id="f676ab37a8d5ec2f850de1fcd3ee779d6ce55a52" translate="yes" xml:space="preserve">
          <source>For instance, in the case of the digits dataset, &lt;code&gt;digits.data&lt;/code&gt; gives access to the features that can be used to classify the digits samples:</source>
          <target state="translated">たとえば、数字データセットの場合、 &lt;code&gt;digits.data&lt;/code&gt; は、数字サンプルの分類に使用できる機能へのアクセスを提供します。</target>
        </trans-unit>
        <trans-unit id="0b72faa109feccaf14265d5a54672e188bc4b73a" translate="yes" xml:space="preserve">
          <source>For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.</source>
          <target state="translated">例えば、以下の例では、決定木はデータから学習し、一連のif-then-else決定ルールでサインカーブを近似します。木が深ければ深いほど、決定ルールは複雑になり、モデルはよりフィットします。</target>
        </trans-unit>
        <trans-unit id="bb4b6181584be99d136bb8fd3d82dd09da37eec0" translate="yes" xml:space="preserve">
          <source>For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">例えば、学習アルゴリズムの目的関数で使用される多くの要素(サポートベクターマシンのRBFカーネルや線形モデルのl1およびl2正則化器など)は、すべての特徴がゼロを中心とし、同じ順序で分散を持つことを前提としています。ある特徴が他の特徴よりも桁違いに大きい分散を持っている場合、それが目的関数を支配し、推定器が他の特徴から期待通りに正しく学習できなくなる可能性があります。</target>
        </trans-unit>
        <trans-unit id="2500a85d8a54066d44afc291418c2bdbdb2d8331" translate="yes" xml:space="preserve">
          <source>For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size \(n_{samples} = 400\) and \(n_{features} = 64 \times 64 = 4096\), the computation time is less than 1s:</source>
          <target state="translated">例えば、以下はオリベッティデータセットの16個のサンプルポートレート(0.0を中心とした)を示しています。右側は,最初の16個の特異点ベクトルをポートレートにリシェイプしたものである.サイズが\(n_{samples}=400)と\(n_{features}=64 \times 64=4096)のデータセットの先頭16個の特異ベクトルだけを求めればよいので,計算時間は1秒以下である.</target>
        </trans-unit>
        <trans-unit id="8f189274df939cb66095eb188cc3a399c86cb294" translate="yes" xml:space="preserve">
          <source>For instance, we can perform a \(\chi^2\) test to the samples to retrieve only the two best features as follows:</source>
          <target state="translated">例えば、以下のように、2つの最良の特徴だけを取り出すために、サンプルに対して「\chi^2」テストを実行することができます。</target>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="translated">例えば</target>
        </trans-unit>
        <trans-unit id="21205df9d4ba13a75af14823666b84f64bd04084" translate="yes" xml:space="preserve">
          <source>For integer/None inputs &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">整数/なしの入力の場合、 &lt;code&gt;KFold&lt;/code&gt; が使用されます。</target>
        </trans-unit>
        <trans-unit id="f4cdf9352c6e062816193041b97f5514c42b421e" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">整数/なし入力の場合、 &lt;code&gt;KFold&lt;/code&gt; が使用されます。</target>
        </trans-unit>
        <trans-unit id="f206c091dc56a7e693c1c1efe6b0899c57cec04a" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used, else, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">整数/なしの入力の場合、 &lt;code&gt;y&lt;/code&gt; がバイナリまたはマルチクラスの場合、&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt;が使用され、それ以外の場合、&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt;が使用されます。</target>
        </trans-unit>
        <trans-unit id="84373ac49af10a751441a8470e060e3de62490b1" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">整数/なしの入力の場合、 &lt;code&gt;y&lt;/code&gt; がバイナリまたはマルチクラスの場合、&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt;が使用されます。 &lt;code&gt;y&lt;/code&gt; がバイナリでもマルチクラスでもない場合、&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt;が使用されます。</target>
        </trans-unit>
        <trans-unit id="8d4fea32021fed22e35126e0e01d0c620369dc78" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If the estimator is a classifier or if &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">整数/なしの入力の場合、 &lt;code&gt;y&lt;/code&gt; がバイナリまたはマルチクラスの場合、&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt;が使用されます。推定器が分類子である場合、または &lt;code&gt;y&lt;/code&gt; がバイナリでもマルチクラスでもない場合、&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt;が使用されます。</target>
        </trans-unit>
        <trans-unit id="68ad85210cd514ab63c161f2689020aa738ee186" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if classifier is True and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">整数/なしの入力の場合、分類子がTrueで &lt;code&gt;y&lt;/code&gt; がバイナリまたはマルチクラスの場合、&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt;が使用されます。他のすべての場合では、&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;が使用されます。</target>
        </trans-unit>
        <trans-unit id="821cadb32f750528bd31875526972b81201437b9" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">整数/なしの入力の場合、推定器が分類子であり、 &lt;code&gt;y&lt;/code&gt; がバイナリまたはマルチクラスの場合、&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt;が使用されます。他のすべての場合では、&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;が使用されます。</target>
        </trans-unit>
        <trans-unit id="cfc9bcb00c8530f8c99a68584e1330a4da8fc56a" translate="yes" xml:space="preserve">
          <source>For intermediate values, we can see on the second plot that good models can be found on a diagonal of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. Smooth models (lower &lt;code&gt;gamma&lt;/code&gt; values) can be made more complex by increasing the importance of classifying each point correctly (larger &lt;code&gt;C&lt;/code&gt; values) hence the diagonal of good performing models.</source>
          <target state="translated">中間値については、2番目のプロットで、 &lt;code&gt;C&lt;/code&gt; と &lt;code&gt;gamma&lt;/code&gt; の対角線上に適切なモデルがあることがわかります。スムーズなモデル（ &lt;code&gt;gamma&lt;/code&gt; 値が低い）は、各ポイントを正しく分類する（ &lt;code&gt;C&lt;/code&gt; 値が大きい）ことの重要性を高めることにより、より複雑にすることができます。</target>
        </trans-unit>
        <trans-unit id="eb96f16ecd15a6be088f1dc93fa28ca4ca7ecca5" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is (n_samples_test, n_samples_train).</source>
          <target state="translated">kernel =&amp;rdquo; precomputed&amp;rdquo;の場合、Xの予想される形状は（n_samples_test、n_samples_train）です。</target>
        </trans-unit>
        <trans-unit id="93c16e02e4641d6fe7bdb8a83439c237817b53cd" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is [n_samples_test, n_samples_train]</source>
          <target state="translated">kernel =&amp;rdquo; precomputed&amp;rdquo;の場合、Xの予想される形状は[n_samples_test、n_samples_train]です</target>
        </trans-unit>
        <trans-unit id="9cb299cfc771ccbc3a241c16ffeed37fabbcff7c" translate="yes" xml:space="preserve">
          <source>For large dataset, you may also consider using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with &amp;lsquo;log&amp;rsquo; loss.</source>
          <target state="translated">大規模なデータセットの場合、「ログ」の損失を&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;使用を検討することもできます。</target>
        </trans-unit>
        <trans-unit id="98187e1f181515ca77d41de7fa27ac44b69c7c11" translate="yes" xml:space="preserve">
          <source>For many estimators, including the SVMs, having datasets with unit standard deviation for each feature is important to get good prediction.</source>
          <target state="translated">SVMをはじめとする多くの推定器では、各特徴に対して単位標準偏差を持つデータセットを持つことが、良好な予測を得るために重要である。</target>
        </trans-unit>
        <trans-unit id="f1359c1e0656157adbc7e3ee11ae253cb961bb70" translate="yes" xml:space="preserve">
          <source>For mono-output tasks it is:</source>
          <target state="translated">単出力タスクの場合はそうです。</target>
        </trans-unit>
        <trans-unit id="c24592da8118b35d1dd067bf2a75576669aef344" translate="yes" xml:space="preserve">
          <source>For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression,&amp;rdquo; Annals of Statistics (with discussion), 407-499. (&lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;)</source>
          <target state="translated">詳細については、Bradley Efron、Trevor Hastie、Iain Johnstone、およびRobert Tibshirani（2004）&amp;ldquo; Least Angle Regression&amp;rdquo;、Annals of Statistics（discussion）、407-499を参照してください。（&lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="a4bc4c3f735998ec8c1614ec6127a37e3e7a02d8" translate="yes" xml:space="preserve">
          <source>For more information, see &lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;.</source>
          <target state="translated">詳細については、「&lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;階層的クラスタリング&lt;/a&gt;」を参照してください。</target>
        </trans-unit>
        <trans-unit id="b089e1ecd97ba592f22037a3c3fabf2924387c39" translate="yes" xml:space="preserve">
          <source>For more on usage see the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">使用方法の詳細については、&lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;ユーザーガイドを&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="2e682df49d58d058f1f4b4c26ca6fb15a2f979d8" translate="yes" xml:space="preserve">
          <source>For multi-class classification, &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt;&lt;code&gt;AdaBoostClassifier&lt;/code&gt;&lt;/a&gt; implements AdaBoost-SAMME and AdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt;.</source>
          <target state="translated">マルチクラス分類の場合、&lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt; &lt;code&gt;AdaBoostClassifier&lt;/code&gt; &lt;/a&gt;はAdaBoost-SAMMEおよびAdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]を実装します&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="416ab9ed1829c79f2f091ddf9b13cfb5bb7486ce" translate="yes" xml:space="preserve">
          <source>For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.</source>
          <target state="translated">マルチクラス分類では、n_class分類器を1対1のアプローチで学習します。具体的には、Ridgeの多変数応答サポートを利用して実装しています。</target>
        </trans-unit>
        <trans-unit id="65042013a5d26811a6a7088f4c47e70c6ddb0074" translate="yes" xml:space="preserve">
          <source>For multi-class models, you need to set the class label for which the PDPs should be created via the &lt;code&gt;label&lt;/code&gt; argument:</source>
          <target state="translated">マルチクラスモデルの場合、 &lt;code&gt;label&lt;/code&gt; 引数を使用して、PDPを作成するクラスラベルを設定する必要があります。</target>
        </trans-unit>
        <trans-unit id="ccc2264ef7a998ec0c6ed9c92245080e0f0807c7" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, the scores for all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at the keys ending with that scorer&amp;rsquo;s name (&lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt;) instead of &lt;code&gt;'_score'&lt;/code&gt; shown above. (&amp;lsquo;split0_test_precision&amp;rsquo;, &amp;lsquo;mean_train_precision&amp;rsquo; etc.)</source>
          <target state="translated">マルチメトリック評価の場合、すべてのスコアラーのスコアは、上記の &lt;code&gt;'_score'&lt;/code&gt; 代わりに、スコアラーの名前（ &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; ）で終わるキーの &lt;code&gt;cv_results_&lt;/code&gt; dictで利用できます。（ 'split0_test_precision'、 'mean_train_precision'など）</target>
        </trans-unit>
        <trans-unit id="6cd27769ef18013ec211b9a824f9bced7dd1ce74" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute holds the validated &lt;code&gt;scoring&lt;/code&gt; dict which maps the scorer key to the scorer callable.</source>
          <target state="translated">マルチメトリック評価の場合、この属性は、スコアラーキーをスコアラー呼び出し可能オブジェクトにマップする検証済みの &lt;code&gt;scoring&lt;/code&gt; ディクテーションを保持します。</target>
        </trans-unit>
        <trans-unit id="39c0f65b87a914c1f3244978c44bbc4d0d1190be" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">マルチメトリック評価の場合、この属性は &lt;code&gt;refit&lt;/code&gt; が指定されている場合にのみ存在します。</target>
        </trans-unit>
        <trans-unit id="dd788cb84c37fa5cbd110321907573fb764ddce9" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is not available if &lt;code&gt;refit&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;. See &lt;code&gt;refit&lt;/code&gt; parameter for more information.</source>
          <target state="translated">マルチメトリック評価では、 &lt;code&gt;refit&lt;/code&gt; が &lt;code&gt;False&lt;/code&gt; の場合、これは使用できません。詳細については、 &lt;code&gt;refit&lt;/code&gt; パラメータを参照してください。</target>
        </trans-unit>
        <trans-unit id="4367b150d838b05eaff7170a1426f1dc4e6edc76" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">マルチメトリック評価の場合、これは &lt;code&gt;refit&lt;/code&gt; が指定されている場合にのみ存在します。</target>
        </trans-unit>
        <trans-unit id="d328aa31182c6b57c5d921c1da1c7333c03486fe" translate="yes" xml:space="preserve">
          <source>For multi-output tasks it is:</source>
          <target state="translated">複数出力のタスクの場合はそれになります。</target>
        </trans-unit>
        <trans-unit id="22c12fa701004eab18f67dae2fb9f6cb3b68f428" translate="yes" xml:space="preserve">
          <source>For multi-output, the weights of each column of y will be multiplied.</source>
          <target state="translated">多出力の場合は、yの各列の重みが乗算されます。</target>
        </trans-unit>
        <trans-unit id="77f8b599f18368a52e2142c2cada7c61eef9fbca" translate="yes" xml:space="preserve">
          <source>For multiclass classification with a &amp;ldquo;negative class&amp;rdquo;, it is possible to exclude some labels:</source>
          <target state="translated">「ネガティブクラス」のマルチクラス分類の場合、一部のラベルを除外することができます。</target>
        </trans-unit>
        <trans-unit id="393c73f8bfb73747a6a85987ccf51d73c8d3636f" translate="yes" xml:space="preserve">
          <source>For multiclass problems, only &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; handle multinomial loss; &amp;lsquo;liblinear&amp;rsquo; is limited to one-versus-rest schemes.</source>
          <target state="translated">マルチクラス問題の場合、多項式損失を処理するのは 'newton-cg'、 'sag'、 'saga'および 'lbfgs'のみです。「liblinear」は、1対残りのスキームに限定されています。</target>
        </trans-unit>
        <trans-unit id="de227a68bb98af9d7f682ac4556427f028c04d66" translate="yes" xml:space="preserve">
          <source>For multiple labels per instance, use &lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">インスタンスごとに複数のラベルを使用するには、&lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; を&lt;/a&gt;使用します。</target>
        </trans-unit>
        <trans-unit id="088c3cd08ec3b30c1e8d705dc9f773b25266ffd1" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer is used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">多重メトリック評価の場合、これはスコアラーを示す文字列である必要があり、最後に推定器のリフィッティングに最適なパラメータを見つけるために使用されます。</target>
        </trans-unit>
        <trans-unit id="ab406c3bb6ddaeec6408e58ba4985d8a5097ee33" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">複数のメトリック評価の場合、これは最後に推定器のリフィッティングに最適なパラメータを見つけるために使用されるスコアラーを示す文字列である必要があります。</target>
        </trans-unit>
        <trans-unit id="f895ac59b8264ca94c275f903e2d6c6c438b4c9c" translate="yes" xml:space="preserve">
          <source>For multiplicative-update (&amp;lsquo;mu&amp;rsquo;) solver, the Frobenius norm (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss, by changing the beta_loss parameter.</source>
          <target state="translated">乗法更新（ 'mu'）ソルバーの場合、Frobeniusノルム（0.5 * || X-WH || _Fro ^ 2）は、beta_lossパラメーターを変更することにより、別のベータ発散損失に変更できます。</target>
        </trans-unit>
        <trans-unit id="4d0bed9bc5aa3b36bb0ba6ad6bc592a5bb3e78af" translate="yes" xml:space="preserve">
          <source>For n_components == &amp;lsquo;mle&amp;rsquo;, this class uses the method of &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt;</source>
          <target state="translated">n_components == 'mle'の場合、このクラスは &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt; メソッドを使用します。NIPS、pp。598-604</target>
        </trans-unit>
        <trans-unit id="9f27cc961ae174b8e96b15764c2907159174c2c2" translate="yes" xml:space="preserve">
          <source>For non-sparse models, i.e. when there are not many zeros in &lt;code&gt;coef_&lt;/code&gt;, this may actually &lt;em&gt;increase&lt;/em&gt; memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt;, must be more than 50% for this to provide significant benefits.</source>
          <target state="translated">非スパースモデルの場合、つまり &lt;code&gt;coef_&lt;/code&gt; にゼロがない場合、これは実際にメモリ使用量を&lt;em&gt;増やす&lt;/em&gt;可能性があるため、このメソッドは注意して使用してください。経験則では、 &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt; で計算できるゼロ要素の数は、これが大きな利点を提供するために50％を超える必要があります。</target>
        </trans-unit>
        <trans-unit id="e62cf2ed735d43186d3b55660d3dd2856258814f" translate="yes" xml:space="preserve">
          <source>For normalized mutual information and adjusted mutual information, the normalizing value is typically some &lt;em&gt;generalized&lt;/em&gt; mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides &amp;ldquo;qualitatively similar behaviours&amp;rdquo; [YAT2016]. In our implementation, this is controlled by the &lt;code&gt;average_method&lt;/code&gt; parameter.</source>
          <target state="translated">正規化された相互情報および調整された相互情報の場合、正規化値は通常、各クラスタリングのエントロピーのいくつかの&lt;em&gt;一般化された&lt;/em&gt;平均です。さまざまな一般化された手段が存在し、他の手段よりも1つを優先するための確固たるルールはありません。決定は主にフィールドごとの基準です。たとえば、コミュニティ検出では、算術平均が最も一般的です。各正規化方法は、「質的に類似した動作」[YAT2016]を提供します。私たちの実装では、これは &lt;code&gt;average_method&lt;/code&gt; パラメータによって制御されます。</target>
        </trans-unit>
        <trans-unit id="5573de0c3d8eae2871980794db1007675aa56eed" translate="yes" xml:space="preserve">
          <source>For now, we will consider the estimator as a black box:</source>
          <target state="translated">とりあえず、推定値をブラックボックスとして考えてみます。</target>
        </trans-unit>
        <trans-unit id="a7f8c21b68cc173c251c1992bff906fb9b13f276" translate="yes" xml:space="preserve">
          <source>For parameter estimation, the posterior distribution is:</source>
          <target state="translated">パラメータ推定の場合、事後分布は</target>
        </trans-unit>
        <trans-unit id="acaedbca04fb48c0d8436452cabe74f03629d275" translate="yes" xml:space="preserve">
          <source>For regression the default learning rate schedule is inverse scaling (&lt;code&gt;learning_rate='invscaling'&lt;/code&gt;), given by</source>
          <target state="translated">回帰の場合、デフォルトの学習率スケジュールは、次の式で与えられる逆スケーリング（ &lt;code&gt;learning_rate='invscaling'&lt;/code&gt; ）です。</target>
        </trans-unit>
        <trans-unit id="17d6bf85a6ee02e9c1e3f5f799f4a791f96ca437" translate="yes" xml:space="preserve">
          <source>For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">二乗損失とl2ペナルティのある回帰の場合、平均化戦略を備えたSGDの別のバリアントは、&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; の&lt;/a&gt;ソルバーとして利用可能な確率的平均勾配（SAG）アルゴリズムで利用できます。</target>
        </trans-unit>
        <trans-unit id="82e8631b28597b123d5803439222a08ee2e59047" translate="yes" xml:space="preserve">
          <source>For regression, &lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt;&lt;code&gt;AdaBoostRegressor&lt;/code&gt;&lt;/a&gt; implements AdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt;.</source>
          <target state="translated">回帰の場合、&lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt; &lt;code&gt;AdaBoostRegressor&lt;/code&gt; &lt;/a&gt;はAdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]を&lt;/a&gt;実装します。</target>
        </trans-unit>
        <trans-unit id="faab950ebeb88e87f11e22c6efcd943439e07aea" translate="yes" xml:space="preserve">
          <source>For regression, MLP uses the Square Error loss function; written as,</source>
          <target state="translated">回帰では、MLPは二乗誤差損失関数を使用します;次のように書かれています。</target>
        </trans-unit>
        <trans-unit id="a8c842b7da02e24dac30b073413aa7112e52aecd" translate="yes" xml:space="preserve">
          <source>For regression: &lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt;&lt;code&gt;f_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">回帰の場合：&lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt; &lt;code&gt;f_regression&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e8aa16ccbf6b94ae32b7c5b78e608f800f0eb6cd" translate="yes" xml:space="preserve">
          <source>For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance. Going forward, np.ndarray returns an np.ndarray, as expected.</source>
          <target state="translated">scikit-learnのバージョン0.14.1以前では、return_as=np.ndarrayは密なnp.matrixインスタンスを返すことで処理されていました。今後、np.ndarrayは期待通りnp.ndarrayを返すようになります。</target>
        </trans-unit>
        <trans-unit id="2410f1ccaa1a03065aaeec2b709967381feb9cea" translate="yes" xml:space="preserve">
          <source>For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping:</source>
          <target state="translated">単純な変換の場合は、Transformerオブジェクトの代わりに、変換とその逆マッピングを定義する関数のペアを渡すことができます。</target>
        </trans-unit>
        <trans-unit id="2f72f7e3c1f68f97fc714ad1c06f3f5738fb15a6" translate="yes" xml:space="preserve">
          <source>For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples.</source>
          <target state="translated">簡単にするために、上の式は単一の学習例について書かれています。重みに対する勾配は、上の式に対応する2つの項から成り立っています。これらは、それぞれの符号の関係から、通常、正の勾配と負の勾配として知られています。この実施形態では、勾配はサンプルのミニバッチにわたって推定される。</target>
        </trans-unit>
        <trans-unit id="27c46746207f2a31ae25da1632ef3ccf3ef87e4d" translate="yes" xml:space="preserve">
          <source>For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</source>
          <target state="translated">単一メトリック評価の場合、スコアリングパラメーターは文字列、呼び出し可能またはなしであり、キーは- &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a0a8bb77034843b440cd9ae1f7c55bd3aa47ece1" translate="yes" xml:space="preserve">
          <source>For small data sets (\(N\) less than 30 or so), \(\log(N)\) is comparable to \(N\), and brute force algorithms can be more efficient than a tree-based approach. Both &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; address this through providing a &lt;em&gt;leaf size&lt;/em&gt; parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small \(N\).</source>
          <target state="translated">小さなデータセット（\（N \）が30未満）の場合、\（\ log（N）\）は\（N \）に相当し、ブルートフォースアルゴリズムはツリーベースのアプローチよりも効率的です。&lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; は&lt;/a&gt;どちらも、&lt;em&gt;リーフサイズ&lt;/em&gt;パラメータを提供することでこれに対処しています。これは、クエリがブルートフォースに切り替わるサンプルの数を制御します。これにより、両方のアルゴリズムで、小さい\（N \）のブルートフォース計算の効率に近づくことができます。</target>
        </trans-unit>
        <trans-unit id="4638d963661692a289a12b8cac2d92a9d2c758fa" translate="yes" xml:space="preserve">
          <source>For small datasets, &amp;lsquo;liblinear&amp;rsquo; is a good choice, whereas &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; are faster for large ones.</source>
          <target state="translated">小さなデータセットの場合、「liblinear」が適切な選択ですが、「sag」と「saga」は大きなデータセットの方が高速です。</target>
        </trans-unit>
        <trans-unit id="dffce5e2239efe7c22f00e78e9cfce148c8ba698" translate="yes" xml:space="preserve">
          <source>For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.</source>
          <target state="translated">アプリケーションによっては、例題の量、特徴(またはその両方)、および/または処理速度が従来のアプローチでは困難な場合があります。このような場合、scikit-learnはシステムを拡張するためのオプションをいくつか用意しています。</target>
        </trans-unit>
        <trans-unit id="d0fa030cdd6e029de147eaddac9b22a69b1ced78" translate="yes" xml:space="preserve">
          <source>For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).</source>
          <target state="translated">いくつかのアプリケーションでは、推定者の性能(主に予測時のレイテンシとスループット)が非常に重要である。トレーニングのスループットを考慮することも興味があるかもしれませんが、これは本番環境(オフラインで行われることが多い)ではあまり重要ではないことが多いです。</target>
        </trans-unit>
        <trans-unit id="7400fa7073eb75f62370e5aadbb0f2aef8d5fc81" translate="yes" xml:space="preserve">
          <source>For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using &lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt;&lt;code&gt;PredefinedSplit&lt;/code&gt;&lt;/a&gt; it is possible to use these folds e.g. when searching for hyperparameters.</source>
          <target state="translated">一部のデータセットでは、トレーニングと検証の分割、またはいくつかの相互検証の分割へのデータの定義済み分割がすでに存在しています。使用&lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt; &lt;code&gt;PredefinedSplit&lt;/code&gt; を&lt;/a&gt;ハイパーを検索するときに、これらの折り目などを使用することが可能です。</target>
        </trans-unit>
        <trans-unit id="c50bf24a893de08f1d0809fe397202f1a031fb85" translate="yes" xml:space="preserve">
          <source>For some miscellaneous data such as images, videos, and audio, you may wish to refer to:</source>
          <target state="translated">画像や動画、音声などの雑多なデータについては、参考にしてみてはいかがでしょうか。</target>
        </trans-unit>
        <trans-unit id="303cfe0bd7811405e77868ed843c4904556ebde5" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">スパース入力の場合、データは効率的なCythonルーチンに送られる前&lt;strong&gt;に、圧縮スパース行表現&lt;/strong&gt;（ &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; を参照）に&lt;strong&gt;変換され&lt;/strong&gt;ます。不必要なメモリコピーを回避するために、上流のCSR表現を選択することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="44ae99861a7942ab4350b3f16d27ddb33207e51f" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">スパース入力の場合、データは&lt;strong&gt;圧縮スパース行表現に変換され&lt;/strong&gt;ます（ &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; を参照）。不必要なメモリコピーを回避するために、上流のCSR表現を選択することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="a6ddfb481ebd700db5464677bebe705030e704c9" translate="yes" xml:space="preserve">
          <source>For speed and space efficiency reasons &lt;code&gt;scikit-learn&lt;/code&gt; loads the target attribute as an array of integers that corresponds to the index of the category name in the &lt;code&gt;target_names&lt;/code&gt; list. The category integer id of each sample is stored in the &lt;code&gt;target&lt;/code&gt; attribute:</source>
          <target state="translated">速度とスペース効率の理由から、 &lt;code&gt;scikit-learn&lt;/code&gt; は &lt;code&gt;target_names&lt;/code&gt; リスト内のカテゴリ名のインデックスに対応する整数の配列としてターゲット属性をロードします。各サンプルのカテゴリー整数IDは、 &lt;code&gt;target&lt;/code&gt; 属性に保管されます。</target>
        </trans-unit>
        <trans-unit id="7ace947ef3298ab26b0edee5253deecf977a3b02" translate="yes" xml:space="preserve">
          <source>For speed, all real work is done at the C level in function copy_predict (libsvm_helper.c).</source>
          <target state="translated">スピードを上げるために、実際の作業はすべて関数 copy_predict (libsvm_helper.c)の C レベルで行われます。</target>
        </trans-unit>
        <trans-unit id="1c3a0f29bcc1c543ffc020478b77d5b706223ce4" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit domain-specific stratification of the dataset.</source>
          <target state="translated">データセットの明示的なドメイン固有の層別化に従ってデータを分割するためのもの。</target>
        </trans-unit>
        <trans-unit id="0adf7a63adc917db1adffd6d4cf61e05de34a6e7" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit, domain-specific stratification of the dataset.</source>
          <target state="translated">データセットの明示的なドメイン固有の層別化に従ってデータを分割するためのもの。</target>
        </trans-unit>
        <trans-unit id="ab4a742934d8510715858d09854f728742beaaec" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;arpack&amp;rsquo;, refer to &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt;.</source>
          <target state="translated">svd_solver == 'arpack'については、 &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt; を参照してください。</target>
        </trans-unit>
        <trans-unit id="25caaa7ea914cf58c60f262a55964ee17d21e090" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;randomized&amp;rsquo;, see: &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; and also &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</source>
          <target state="translated">svd_solver == 'randomized'については、Halko &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; また、 &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e7341be727234aad9fa4e331ba9d61b6fce122ff" translate="yes" xml:space="preserve">
          <source>For the &amp;lsquo;liblinear&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">'liblinear'、 'sag'、および 'lbfgs'ソルバーは、冗長性のためにverboseを任意の正の数に設定します。</target>
        </trans-unit>
        <trans-unit id="a081b6c50a07cf5457333031806f4c48e54ea42e" translate="yes" xml:space="preserve">
          <source>For the &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the &lt;code&gt;nu&lt;/code&gt; parameter of &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; was used to influence the number of support vectors.</source>
          <target state="translated">非線形カーネルを備えたアルゴリズムの&lt;a href=&quot;classes#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; &lt;/a&gt;ファミリーの場合、レイテンシはサポートベクターの数（少ないほど高速）に関連付けられます。レイテンシとスループットは、SVCまたはSVRモデルのサポートベクトルの数に比例して（漸近的に）増加するはずです。カーネルは、サポートベクトルごとに入力ベクトルの投影を計算するために使用されるため、レイテンシにも影響します。次のグラフでは、 &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; の &lt;code&gt;nu&lt;/code&gt; パラメーターを使用して、サポートベクターの数に影響を与えています。</target>
        </trans-unit>
        <trans-unit id="49dfac47eea992144c43ccc29f61713fabd0e5ae" translate="yes" xml:space="preserve">
          <source>For the &lt;code&gt;l2&lt;/code&gt; penalty case, the best result comes from the case where &lt;code&gt;C&lt;/code&gt; is not scaled.</source>
          <target state="translated">以下のために &lt;code&gt;l2&lt;/code&gt; ペナルティ場合、最良の結果がケースから来ている &lt;code&gt;C&lt;/code&gt; はスケーリングされません。</target>
        </trans-unit>
        <trans-unit id="d9f81a56586341e43516abb99b238b1b5d6587c8" translate="yes" xml:space="preserve">
          <source>For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data.</source>
          <target state="translated">Cs値のグリッド(デフォルトでは1e-4から1e-4の間の対数スケールで10の値に設定されています)では、クロスバリデータStratifiedKFoldによって最適なハイパーパラメータが選択されますが、cvパラメータを使用して変更することができます。newton-cgソルバーやlbfgsソルバーの場合、パスに沿ってウォームスタートします。</target>
        </trans-unit>
        <trans-unit id="93f0b6841feed67e5fc00af0443562656921cce7" translate="yes" xml:space="preserve">
          <source>For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">liblinearソルバーとlbfgsソルバーでは、冗長性のためにverboseを任意の正の数に設定します。</target>
        </trans-unit>
        <trans-unit id="4c4a7d0fb25ecd0231acfef000eb4ebb4024b077" translate="yes" xml:space="preserve">
          <source>For the most common use cases, you can designate a scorer object with the &lt;code&gt;scoring&lt;/code&gt; parameter; the table below shows all possible values. All scorer objects follow the convention that &lt;strong&gt;higher return values are better than lower return values&lt;/strong&gt;. Thus metrics which measure the distance between the model and the data, like &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;metrics.mean_squared_error&lt;/code&gt;&lt;/a&gt;, are available as neg_mean_squared_error which return the negated value of the metric.</source>
          <target state="translated">最も一般的な使用例では、 &lt;code&gt;scoring&lt;/code&gt; オブジェクトでスコアラーオブジェクトを指定できます。以下の表は、可能なすべての値を示しています。すべてのスコアラーオブジェクトは、&lt;strong&gt;高い戻り値が低い戻り値よりも優れ&lt;/strong&gt;ているという規則に従い&lt;strong&gt;ます&lt;/strong&gt;。したがって、&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;metrics.mean_squared_error&lt;/code&gt; の&lt;/a&gt;ように、モデルとデータの間の距離を測定するメトリックは、メトリックの否定された値を返すneg_mean_squared_errorとして使用できます。</target>
        </trans-unit>
        <trans-unit id="5c5d7d9872e083b1cf44dccc9ef51ebf6d1fc473" translate="yes" xml:space="preserve">
          <source>For the rationale behind the names &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;, i.e. naive Bayes as a linear classifier, see J. Rennie et al. (2003), Tackling the poor assumptions of naive Bayes text classifiers, ICML.</source>
          <target state="translated">名前 &lt;code&gt;coef_&lt;/code&gt; および &lt;code&gt;intercept_&lt;/code&gt; の根拠、つまり線形分類器としての単純ベイズについては、J。Rennie et al。を参照してください。（2003）、ナイーブベイズテキスト分類器の不十分な仮定に取り組む、ICML。</target>
        </trans-unit>
        <trans-unit id="08233f540a36ed45603c5cb01e2e4f593cd79c27" translate="yes" xml:space="preserve">
          <source>For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can be used:</source>
          <target state="translated">2つのデータセット間の最近傍を見つける単純なタスクでは、&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt;内の教師なしアルゴリズムを使用できます。</target>
        </trans-unit>
        <trans-unit id="daed0c58d42835f25cc91f4ef37c8c2918d442fd" translate="yes" xml:space="preserve">
          <source>For this data, we might want to encode the &lt;code&gt;'city'&lt;/code&gt; column as a categorical variable, but apply a &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;'title'&lt;/code&gt; column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say &lt;code&gt;'city_category'&lt;/code&gt; and &lt;code&gt;'title_bow'&lt;/code&gt;. By default, the remaining rating columns are ignored (&lt;code&gt;remainder='drop'&lt;/code&gt;):</source>
          <target state="translated">このデータの場合、 &lt;code&gt;'city'&lt;/code&gt; 列をカテゴリー変数としてエンコードしますが、&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt;を &lt;code&gt;'title'&lt;/code&gt; 列に適用します。同じ列で複数の特徴抽出メソッドを使用する可能性があるため、各トランスフォーマーに &lt;code&gt;'city_category'&lt;/code&gt; や &lt;code&gt;'title_bow'&lt;/code&gt; などの一意の名前を付けます。デフォルトでは、残りの評価列は無視されます（ &lt;code&gt;remainder='drop'&lt;/code&gt; ）：</target>
        </trans-unit>
        <trans-unit id="2e47bbc09921a29cf4a007e2d92242f5a8a9f3d8" translate="yes" xml:space="preserve">
          <source>For this example we will use the &lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;yeast&lt;/a&gt; dataset which contains 2417 datapoints each with 103 features and 14 possible labels. Each data point has at least one label. As a baseline we first train a logistic regression classifier for each of the 14 labels. To evaluate the performance of these classifiers we predict on a held-out test set and calculate the &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard similarity score&lt;/a&gt;.</source>
          <target state="translated">この例では、それぞれ103の特徴と14の可能なラベルを持つ2417データポイントを含む&lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;酵母&lt;/a&gt;データセットを使用します。各データポイントには少なくとも1つのラベルがあります。ベースラインとして、最初に14の各ラベルのロジスティック回帰分類子をトレーニングします。これらの分類子のパフォーマンスを評価するために、保留されたテストセットを予測し、&lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;ジャカード類似性スコア&lt;/a&gt;を計算します。</target>
        </trans-unit>
        <trans-unit id="6ab77ac3ad8536d0d4bc113a409f055607cd6e01" translate="yes" xml:space="preserve">
          <source>For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems. It is best to try several random seeds in order to check results.</source>
          <target state="translated">この方法では、M は密行列、疎行列、一般線形演算子のいずれかである。警告.ARPACKは、問題によっては不安定な場合があります。結果を確認するために、いくつかのランダムな種を試してみるのがよいでしょう。</target>
        </trans-unit>
        <trans-unit id="201d282b655d8d026b78eb9f9255553e50678277" translate="yes" xml:space="preserve">
          <source>For this purpose, the estimators use a &amp;lsquo;connectivity&amp;rsquo; matrix, giving which samples are connected.</source>
          <target state="translated">この目的のために、推定器は「接続性」行列を使用して、接続されているサンプルを示します。</target>
        </trans-unit>
        <trans-unit id="764ea2ccb7951b3db73f825ee916559c0e4bce1d" translate="yes" xml:space="preserve">
          <source>For this reason, the functions that load 20 Newsgroups data provide a parameter called &lt;strong&gt;remove&lt;/strong&gt;, telling it what kinds of information to strip out of each file. &lt;strong&gt;remove&lt;/strong&gt; should be a tuple containing any subset of &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt;, telling it to remove headers, signature blocks, and quotation blocks respectively.</source>
          <target state="translated">このため、20個のニュースグループデータをロードする関数は、&lt;strong&gt;remove&lt;/strong&gt;というパラメーターを提供し、各ファイルから削除する情報の種類を伝えます。&lt;strong&gt;remove&lt;/strong&gt;は、 &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt; サブセットを含むタプルで、ヘッダー、署名ブロック、引用ブロックをそれぞれ削除するように指示する必要があります。</target>
        </trans-unit>
        <trans-unit id="f2d2e6058597b408c702846b2d537e901630ce3a" translate="yes" xml:space="preserve">
          <source>For two clusters, it solves a convex relaxation of the &lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;normalised cuts&lt;/a&gt; problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images: graph vertices are pixels, and edges of the similarity graph are a function of the gradient of the image.</source>
          <target state="translated">2つのクラスターの場合、類似性グラフの&lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;正規化されたカット&lt;/a&gt;問題の凸緩和を解決します。グラフを2つにカットして、カットされたエッジの重みが各クラスター内のエッジの重みに比べて小さくなるようにします。この基準は、画像で作業する場合に特に興味深いものです。グラフの頂点はピクセルであり、類似度グラフのエッジは画像の勾配の関数です。</target>
        </trans-unit>
        <trans-unit id="4092abbbb6ead577ab2b40e6704455f3cb4d3df5" translate="yes" xml:space="preserve">
          <source>For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt; entry on imputation.</source>
          <target state="translated">さまざまな理由により、多くの現実世界のデータセットには欠損値が含まれ、多くの場合、空白、NaN、またはその他のプレースホルダーとしてエンコードされます。ただし、そのようなデータセットは、配列内のすべての値が数値であり、すべてが意味を持ち保持していると想定するscikit-learn推定器と互換性がありません。不完全なデータセットを使用する基本的な戦略は、欠損値を含む行や列全体を破棄することです。ただし、これはデータが失われるという代償を伴います（データは不完全ですが）。より良い戦略は、欠損値を補うことです。つまり、データの既知の部分からそれらを推測することです。帰属に関する&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;一般用語とAPI要素&lt;/a&gt;の用語集のエントリを参照してください。</target>
        </trans-unit>
        <trans-unit id="7f12e7919ffa1c3009c9eefff46504b7c0642e13" translate="yes" xml:space="preserve">
          <source>For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints.</source>
          <target state="translated">t-SNEの主な用途である可視化のためには、Barnes-Hut法を用いることを強く推奨します。厳密t-SNE法は、高次元空間での埋め込みの理論的性質を確認するのに便利ですが、計算上の制約から小さなデータセットに限られています。</target>
        </trans-unit>
        <trans-unit id="e7a5b4b1244321faa67509dff73df9a23d7da1b3" translate="yes" xml:space="preserve">
          <source>For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the bicluster contiguous.</source>
          <target state="translated">可視化の目的のために、2 つのクラスターが与えられた場合、データマトリクスの行と列を並べ替えて、2 つのクラスターが連続するようにすることができます。</target>
        </trans-unit>
        <trans-unit id="d62d3122e2e4eef979e7c46fd629936aec0233be" translate="yes" xml:space="preserve">
          <source>For visualization purposes, we need to lay out the different symbols on a 2D canvas. For this we use &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold learning&lt;/a&gt; techniques to retrieve 2D embedding.</source>
          <target state="translated">視覚化のために、2Dキャンバスにさまざまなシンボルをレイアウトする必要があります。このために、2D埋め込みを取得するために&lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;多様体学習&lt;/a&gt;手法を使用します。</target>
        </trans-unit>
        <trans-unit id="c502fd7960fae5affa9295a7a329adeddad6ab37" translate="yes" xml:space="preserve">
          <source>Force row-by-row generation by reducing &lt;code&gt;working_memory&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;working_memory&lt;/code&gt; を減らして、行ごとに強制的に生成します。</target>
        </trans-unit>
        <trans-unit id="4bb98e5d778957b0dd66fa6aed87be22d170768c" translate="yes" xml:space="preserve">
          <source>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</source>
          <target state="translated">Forina,M.et al,PARVUS-An Extendible Package for Data Exploration,Classification and Correlation.Institute of Pharmaceutical and Food Analysis and Technologies,Via Brigata Salerno,16147 Genoa,Italy.</target>
        </trans-unit>
        <trans-unit id="35705e005c1f18ed14dab92df9e1435742858283" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the average precision is defined as</source>
          <target state="translated">Formally,given a binary indicator matrix of the ground truth labels ♦(y \in ﾞleft ﾞleft\\{0,1\right}^{n_\text{samples}ﾞ\times n_\text{labels}})and the score associated with each label ♦(\hat{f}ﾞ\in ﾞmathbb{R}^{n_\text{samples}ﾞ\times n_\text{labels}})の2値指標行列が与えられると、平均精度は次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="4f395914b8fb9e643646835cc07cbc38c9742edc" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the coverage is defined as</source>
          <target state="translated">Formally,given a binary indicator matrix of the ground truth labels \(y \in ﾞleft ﾞleft\\{0,1\right}^{n_\text{samples}ﾞ\times n_\text{labels}})and the score associated with each label ﾞ\(\hat{f}ﾞ\in ﾞmathbb{R}^{n_\text{samples}ﾞ\times n_\text{labels}}),the coverage defined with the coverage as</target>
        </trans-unit>
        <trans-unit id="c175d46f254d733413b5b0ee831c9d600136a7b6" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the ranking loss is defined as</source>
          <target state="translated">Formally,given a binary indicator matrix of the ground truth labels \(y \in ﾞleft ﾞleft\\{0,1\right}^{n_\text{samples}ﾞ\times n_\text{labels}})and the score associated with each label ﾞ\(\hat{f}ﾞ\in ﾞmathbb{R}^{n_\text{samples}ﾞ\times n_\text{labels}}),ranking loss is defined as as</target>
        </trans-unit>
        <trans-unit id="47fb5045fef615598469a37da8a59110352753ff" translate="yes" xml:space="preserve">
          <source>Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.</source>
          <target state="translated">指定された関数で与えられた親和行列を形成し、対応するグラフラプラシアンにスペクトル分解を適用します。結果として得られる変換は,各データポイントの固有ベクトルの値によって与えられます.</target>
        </trans-unit>
        <trans-unit id="638babaaa209a18fe959f40f19725a01af068351" translate="yes" xml:space="preserve">
          <source>Fortunately, &lt;strong&gt;most values in X will be zeros&lt;/strong&gt; since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically &lt;strong&gt;high-dimensional sparse datasets&lt;/strong&gt;. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.</source>
          <target state="translated">幸い、&lt;strong&gt;Xのほとんどの値はゼロになります。&lt;/strong&gt;これは、特定のドキュメントでは数千の異なる単語しか使用されないためです。このため、通常、単語のバッグは&lt;strong&gt;高次元のスパースデータセットである&lt;/strong&gt;と言います。特徴ベクトルのゼロ以外の部分をメモリに格納するだけで、多くのメモリを節約できます。</target>
        </trans-unit>
        <trans-unit id="659b18cdaec75234c8e955e09af5dc004ab6498a" translate="yes" xml:space="preserve">
          <source>Frequently asked questions about the project and contributing.</source>
          <target state="translated">プロジェクトや貢献に関するよくある質問</target>
        </trans-unit>
        <trans-unit id="c378e5372fbcc6968de3f23916b1cc385b9617be" translate="yes" xml:space="preserve">
          <source>Friedman et al, &lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Sparse inverse covariance estimation with the graphical lasso&amp;rdquo;&lt;/a&gt;, Biostatistics 9, pp 432, 2008</source>
          <target state="translated">フリードマン他、&lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;「グラフィカルな投げ縄によるスパース逆共分散推定」&lt;/a&gt;、Biostatistics 9、pp 432、2008</target>
        </trans-unit>
        <trans-unit id="8438e27109208985a133518d65493568dedc6924" translate="yes" xml:space="preserve">
          <source>Friedman, &amp;ldquo;Stochastic Gradient Boosting&amp;rdquo;, 1999</source>
          <target state="translated">フリードマン、「確率的勾配ブースティング」、1999</target>
        </trans-unit>
        <trans-unit id="9fcad16d5a3614a8ac9a3dd3615a46004936d92d" translate="yes" xml:space="preserve">
          <source>Friedman, Stochastic Gradient Boosting, 1999</source>
          <target state="translated">フリードマン、確率的勾配ブースティング、1999年</target>
        </trans-unit>
        <trans-unit id="d93c5ad51427861e9927c0f93ba75f21fa7b3769" translate="yes" xml:space="preserve">
          <source>Frobenius norm of the matrix difference, or beta-divergence, between the training data &lt;code&gt;X&lt;/code&gt; and the reconstructed data &lt;code&gt;WH&lt;/code&gt; from the fitted model.</source>
          <target state="translated">トレーニングデータ &lt;code&gt;X&lt;/code&gt; と近似モデルから再構築されたデータ &lt;code&gt;WH&lt;/code&gt; との間の行列差、つまりベータダイバージェンスのフロベニウスノルム。</target>
        </trans-unit>
        <trans-unit id="2a2bd03e6f160e636919837a5a755bde731a1eeb" translate="yes" xml:space="preserve">
          <source>From images</source>
          <target state="translated">画像から</target>
        </trans-unit>
        <trans-unit id="5ff0ffd1e24dbd90ba4e307313dc3fed8b0cd6c4" translate="yes" xml:space="preserve">
          <source>From occurrences to frequencies</source>
          <target state="translated">発生から頻度まで</target>
        </trans-unit>
        <trans-unit id="4a6ea847ae49dd26abc66504268644d690f3206b" translate="yes" xml:space="preserve">
          <source>From scikit-learn: [&amp;lsquo;cityblock&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;, &amp;lsquo;euclidean&amp;rsquo;, &amp;lsquo;l1&amp;rsquo;, &amp;lsquo;l2&amp;rsquo;, &amp;lsquo;manhattan&amp;rsquo;]. These metrics support sparse matrix inputs.</source>
          <target state="translated">scikit-learnから：['cityblock'、 'cosine'、 'euclidean'、 'l1'、 'l2'、 'manhattan']。これらのメトリックは、スパースマトリックス入力をサポートします。</target>
        </trans-unit>
        <trans-unit id="6d8d962b98fbbe50de709ee2f1e71db53d579c2e" translate="yes" xml:space="preserve">
          <source>From scipy.spatial.distance: [&amp;lsquo;braycurtis&amp;rsquo;, &amp;lsquo;canberra&amp;rsquo;, &amp;lsquo;chebyshev&amp;rsquo;, &amp;lsquo;correlation&amp;rsquo;, &amp;lsquo;dice&amp;rsquo;, &amp;lsquo;hamming&amp;rsquo;, &amp;lsquo;jaccard&amp;rsquo;, &amp;lsquo;kulsinski&amp;rsquo;, &amp;lsquo;mahalanobis&amp;rsquo;, &amp;lsquo;minkowski&amp;rsquo;, &amp;lsquo;rogerstanimoto&amp;rsquo;, &amp;lsquo;russellrao&amp;rsquo;, &amp;lsquo;seuclidean&amp;rsquo;, &amp;lsquo;sokalmichener&amp;rsquo;, &amp;lsquo;sokalsneath&amp;rsquo;, &amp;lsquo;sqeuclidean&amp;rsquo;, &amp;lsquo;yule&amp;rsquo;] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.</source>
          <target state="translated">scipy.spatial.distanceから：['braycurtis'、 'canberra'、 'chebyshev'、 'correlation'、 'dice'、 'hamming'、 'jaccard'、 'kulsinski'、 'mahalanobis'、 'minkowski'、 'rogerstanimoto '、' russellrao '、' seuclidean '、' sokalmichener '、' sokalsneath '、' sqeuclidean '、' yule ']これらのメトリックの詳細については、scipy.spatial.distanceのドキュメントをご覧ください。これらのメトリックは、スパース行列入力をサポートしていません。</target>
        </trans-unit>
        <trans-unit id="d3990f36d057d6745fedc272447b2563e02193f7" translate="yes" xml:space="preserve">
          <source>From text</source>
          <target state="translated">本文より</target>
        </trans-unit>
        <trans-unit id="13bce2493b501286a428cebcb4e0bc57e6083c63" translate="yes" xml:space="preserve">
          <source>From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.</source>
          <target state="translated">実装の観点から見ると、これは単なるOrdinary Least Squares (scipy.linalg.lstsq)を予測オブジェクトとしてラップしただけのものです。</target>
        </trans-unit>
        <trans-unit id="ae8e3bdf9c1967ed71af43f356a6c2d5d1712708" translate="yes" xml:space="preserve">
          <source>From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly 50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the &lt;code&gt;partial_fit&lt;/code&gt; method of the online KMeans object, MiniBatchKMeans.</source>
          <target state="translated">プログラミングの観点からは、scikit-learnのオンラインAPIを使用して非常に大きなデータセットをチャンクで処理する方法を示しているので興味深いです。次に進む方法は、一度にイメージをロードし、このイメージからランダムに50個のパッチを抽出することです。これらのパッチを500個（10個の画像を使用して）累積したら、オンラインKMeansオブジェクトであるMiniBatchKMeansの &lt;code&gt;partial_fit&lt;/code&gt; メソッドを実行します。</target>
        </trans-unit>
        <trans-unit id="f1e410ad1472b42cb42cc98962428637290b6706" translate="yes" xml:space="preserve">
          <source>Function</source>
          <target state="translated">Function</target>
        </trans-unit>
        <trans-unit id="9f410a9e5384dfe1720c4cd228fe7bb63965656b" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">2つの配列Xおよびyを取り、配列のペア（スコア、p値）またはスコアのある単一の配列を返す関数。デフォルトはf_classifです（以下の「関連項目」も参照）。デフォルトの関数は分類タスクでのみ機能します。</target>
        </trans-unit>
        <trans-unit id="a8696032e0adf35ffec7c9da28cd036adeb91c99" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">2つの配列Xとyを取り、配列のペア（スコア、p値）を返す関数。デフォルトはf_classifです（以下の「関連項目」も参照）。デフォルトの関数は分類タスクでのみ機能します。</target>
        </trans-unit>
        <trans-unit id="acf1f055cd0885a9fc7d245efda7d1c727fca691" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes &amp;lsquo;percentile&amp;rsquo; or &amp;lsquo;kbest&amp;rsquo; it can return a single array scores.</source>
          <target state="translated">2つの配列Xとyを取り、配列のペア（スコア、p値）を返す関数。'percentile'または 'kbest'モードの場合、単一の配列スコアを返すことができます。</target>
        </trans-unit>
        <trans-unit id="0c64f21c81859fb42c302c0d2cd301e40332c2c7" translate="yes" xml:space="preserve">
          <source>Function to apply to &lt;code&gt;y&lt;/code&gt; before passing to &lt;code&gt;fit&lt;/code&gt;. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt;. The function needs to return a 2-dimensional array. If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, the function used will be the identity function.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; に渡す前に &lt;code&gt;y&lt;/code&gt; に適用する関数。 &lt;code&gt;transformer&lt;/code&gt; と同時に設定できません。関数は2次元配列を返す必要があります。場合 &lt;code&gt;func&lt;/code&gt; はありません &lt;code&gt;None&lt;/code&gt; 、使用する関数は恒等関数になります。</target>
        </trans-unit>
        <trans-unit id="f712e33ad68950dd5132b77ad3129994bf2cbbce" translate="yes" xml:space="preserve">
          <source>Function to apply to the prediction of the regressor. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt; as well. The function needs to return a 2-dimensional array. The inverse function is used to return predictions to the same space of the original training labels.</source>
          <target state="translated">リグレッサの予測に適用する関数。 &lt;code&gt;transformer&lt;/code&gt; と同時に設定することはできません。関数は2次元配列を返す必要があります。逆関数は、元のトレーニングラベルと同じ空間に予測を返すために使用されます。</target>
        </trans-unit>
        <trans-unit id="2b961dea1dc0c60ddf9a2c8e9d090f6f7d082483" translate="yes" xml:space="preserve">
          <source>Functions</source>
          <target state="translated">Functions</target>
        </trans-unit>
        <trans-unit id="c216053588b385d3de175b467017426b8b421912" translate="yes" xml:space="preserve">
          <source>Further discussion on the importance of centering and scaling data is available on this FAQ: &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;Should I normalize/standardize/rescale the data?&lt;/a&gt;</source>
          <target state="translated">データのセンタリングとスケーリングの重要性に関する詳細な説明は、このFAQで利用できます。データ&lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;を正規化/標準化/再スケーリングする必要がありますか？&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="606b0f774f1d5e4151969dbb768df62ebca8a20e" translate="yes" xml:space="preserve">
          <source>Further removes the linear correlation across features with &amp;lsquo;whiten=True&amp;rsquo;.</source>
          <target state="translated">さらに、「whiten = True」の機能間の線形相関を削除します。</target>
        </trans-unit>
        <trans-unit id="ec9ba56eabfa3f70786eb84612f0623df80dfc4d" translate="yes" xml:space="preserve">
          <source>Further, the model supports &lt;a href=&quot;multiclass#multiclass&quot;&gt;multi-label classification&lt;/a&gt; in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to &lt;code&gt;0.5&lt;/code&gt; are rounded to &lt;code&gt;1&lt;/code&gt;, otherwise to &lt;code&gt;0&lt;/code&gt;. For a predicted output of a sample, the indices where the value is &lt;code&gt;1&lt;/code&gt; represents the assigned classes of that sample:</source>
          <target state="translated">さらに、モデルは、サンプルが複数のクラスに属することができる&lt;a href=&quot;multiclass#multiclass&quot;&gt;マルチラベル分類を&lt;/a&gt;サポートしています。クラスごとに、生の出力はロジスティック関数を通過します。 &lt;code&gt;0.5&lt;/code&gt; 以上の値は &lt;code&gt;1&lt;/code&gt; に丸められ、それ以外の場合は &lt;code&gt;0&lt;/code&gt; に丸められます。サンプルの予測出力の場合、値が &lt;code&gt;1&lt;/code&gt; のインデックスは、そのサンプルに割り当てられたクラスを表します。</target>
        </trans-unit>
        <trans-unit id="cc118108875cca01a2724ce6e20debf4e124a846" translate="yes" xml:space="preserve">
          <source>Furthermore, &lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt;&lt;code&gt;adjusted_rand_score&lt;/code&gt;&lt;/a&gt; is &lt;strong&gt;symmetric&lt;/strong&gt;: swapping the argument does not change the score. It can thus be used as a &lt;strong&gt;consensus measure&lt;/strong&gt;:</source>
          <target state="translated">さらに、&lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt; &lt;code&gt;adjusted_rand_score&lt;/code&gt; &lt;/a&gt;は&lt;strong&gt;対称的&lt;/strong&gt;です。引数を入れ替えてもスコアは変わりません。したがって、&lt;strong&gt;コンセンサス指標&lt;/strong&gt;として使用できます。</target>
        </trans-unit>
        <trans-unit id="7218de362b7befd5a71b1a5a01365e3552aa1087" translate="yes" xml:space="preserve">
          <source>Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples.</source>
          <target state="translated">さらに、処理された例の数に応じて、異なるアルゴリズムの性能が変化することも示しています。</target>
        </trans-unit>
        <trans-unit id="17753e7322d4f150d032ddf1f2dbdf4fe6d38592" translate="yes" xml:space="preserve">
          <source>Furthermore, the default parameter &lt;code&gt;smooth_idf=True&lt;/code&gt; adds &amp;ldquo;1&amp;rdquo; to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:</source>
          <target state="translated">さらに、デフォルトのパラメーター &lt;code&gt;smooth_idf=True&lt;/code&gt; は、コレクション内のすべての用語を1回だけ含む追加のドキュメントが見られたかのように、分子と分母に「1」を追加します。</target>
        </trans-unit>
        <trans-unit id="2e93583dd7fd8dcf1f0371a9818f0db1fd3c80a7" translate="yes" xml:space="preserve">
          <source>Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR as follows:</source>
          <target state="translated">さらに、tf、idfの計算式は、以下のようにIRで使用されるSMART記法に対応するパラメータの設定に依存する。</target>
        </trans-unit>
        <trans-unit id="2b6ca190d547b1e777d8fa3e93274ce6ad7c42b4" translate="yes" xml:space="preserve">
          <source>G. Brier, &lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;Verification of forecasts expressed in terms of probability&lt;/a&gt;, Monthly weather review 78.1 (1950)</source>
          <target state="translated">G.ブライアー、&lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;確率&lt;/a&gt;で表された予報の検証、月間天気調査78.1（1950）</target>
        </trans-unit>
        <trans-unit id="8ccf25498da17f5ff69133909511a6d98d2976f3" translate="yes" xml:space="preserve">
          <source>G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert, &amp;ldquo;Regularization in regression: comparing Bayesian and frequentist methods in a poorly informative situation&amp;rdquo;, 2009.</source>
          <target state="translated">G.セルー、M。エルアンバリ、J.-M。マリン、CPロバート、「回帰の正則化：情報量の少ない状況でのベイズ法と頻度主義法の比較」、2009年。</target>
        </trans-unit>
        <trans-unit id="755f0c9208b383f3b380dd0d2b1a156d6d5865c4" translate="yes" xml:space="preserve">
          <source>G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, Springer 2013.</source>
          <target state="translated">G.ジェームズ、D。ウィッテン、T。ハスティ、Rティブシラニ、&lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;『統計学習入門』&lt;/a&gt;、Springer 2013。</target>
        </trans-unit>
        <trans-unit id="28ef1689ee2219c624cfde5d7c88afdcae0138ec" translate="yes" xml:space="preserve">
          <source>G. Louppe and P. Geurts, &amp;ldquo;Ensembles on Random Patches&amp;rdquo;, Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</source>
          <target state="translated">G. LouppeとP. Geurts、「ランダムパッチのアンサンブル」、機械学習とデータベースでの知識の発見、346-361、2012年。</target>
        </trans-unit>
        <trans-unit id="c722e87d5d9d7dbc54dd2b811a759cc621efb047" translate="yes" xml:space="preserve">
          <source>G. Louppe, &amp;ldquo;Understanding Random Forests: From Theory to Practice&amp;rdquo;, PhD Thesis, U. of Liege, 2014.</source>
          <target state="translated">G.ルーペ、「ランダムフォレストの理解：理論から実践へ」、博士論文、U。of Liege、2014年。</target>
        </trans-unit>
        <trans-unit id="a8ed6bad205ec1f52f0b48e7f8377435663ec074" translate="yes" xml:space="preserve">
          <source>G.E.P. Box and D.R. Cox, &amp;ldquo;An Analysis of Transformations&amp;rdquo;, Journal of the Royal Statistical Society B, 26, 211-252 (1964).</source>
          <target state="translated">GEP BoxおよびDR Cox、「An Analysis of Transformations」、Journal of the Royal Statistical Society B、26、211〜252（1964）。</target>
        </trans-unit>
        <trans-unit id="b8cb867b444fe174ab482df0a111ed147a9ceddf" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage &lt;code&gt;n_classes_&lt;/code&gt; regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</source>
          <target state="translated">GBは、段階的な方法で追加モデルを構築します。任意の微分可能な損失関数の最適化を可能にします。各段階で、 &lt;code&gt;n_classes_&lt;/code&gt; 回帰木は、二項または多項の逸脱損失関数の負の勾配に適合します。バイナリ分類は、単一の回帰ツリーのみが誘導される特殊なケースです。</target>
        </trans-unit>
        <trans-unit id="80f39c4fc4a6461ea00d5d7be636d9c6f77055de" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.</source>
          <target state="translated">GBは、段階的に加法モデルを構築し、任意の微分可能な損失関数の最適化を可能にします。各段階では、与えられた損失関数の負の勾配に回帰木が適合します。</target>
        </trans-unit>
        <trans-unit id="2c1af0078ebec6d87c6fe14b52a6ca7ecb93e0e6" translate="yes" xml:space="preserve">
          <source>GBRT considers additive models of the following form:</source>
          <target state="translated">GBRTでは、以下のような形式の加法モデルを考えています。</target>
        </trans-unit>
        <trans-unit id="348ddf733ebe39c89fe60cc4aea0def489f0df0c" translate="yes" xml:space="preserve">
          <source>GMM covariances</source>
          <target state="translated">GMM 共分散</target>
        </trans-unit>
        <trans-unit id="89a541e422be32f4e38c95b70a35778f6b3b29a5" translate="yes" xml:space="preserve">
          <source>G[i,j] gives the shortest distance from point i to point j along the graph.</source>
          <target state="translated">G[i,j]はグラフに沿って点iから点jまでの最短距離を与えます。</target>
        </trans-unit>
        <trans-unit id="dc7da4ca9757d9015c0ba1d2228560006792966e" translate="yes" xml:space="preserve">
          <source>Gallery generated by Sphinx-Gallery</source>
          <target state="translated">Sphinx-Galleryによって生成されたギャラリー</target>
        </trans-unit>
        <trans-unit id="24f0f86d8b8da4a3eb66c5315b49fb7db14a0fa6" translate="yes" xml:space="preserve">
          <source>Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.</source>
          <target state="translated">RBF,laplacian,polynomial,exponential chi2,sigmoid カーネルのガンマパラメータ.デフォルト値の解釈はカーネルに委ねられています;sklearn.metrics.pairwiseのドキュメントを参照してください。他のカーネルでは無視されます。</target>
        </trans-unit>
        <trans-unit id="8abb933fe9bd6d8a92eb104bdc2fd613c351d44f" translate="yes" xml:space="preserve">
          <source>Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other kernels. 0.1 by default.</source>
          <target state="translated">rbf,poly,sigmoid カーネルのガンマパラメータ.他のカーネルでは無視されます。デフォルトでは0.1。</target>
        </trans-unit>
        <trans-unit id="86050f4573c138fca290821e4b579d6d320e40d1" translate="yes" xml:space="preserve">
          <source>Gates, G.W. (1972) &amp;ldquo;The Reduced Nearest Neighbor Rule&amp;rdquo;. IEEE Transactions on Information Theory, May 1972, 431-433.</source>
          <target state="translated">ゲイツ、GW（1972）「削減された最近隣ルール」。情報理論に関するIEEEトランザクション、1972年5月、431-433。</target>
        </trans-unit>
        <trans-unit id="46a57bcdd34ea523f3417e94b431a41097b638e9" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Ellipsoids</source>
          <target state="translated">ガウス混合モデル楕円体</target>
        </trans-unit>
        <trans-unit id="2f22bd1dad8340bd3d8973db40a56083b791482c" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Selection</source>
          <target state="translated">ガウス混合モデルの選択</target>
        </trans-unit>
        <trans-unit id="7ada59d703243073c5122ce20c200108df4cf582" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Sine Curve</source>
          <target state="translated">ガウス混合モデル 正弦曲線</target>
        </trans-unit>
        <trans-unit id="662a25df4ddd527b4e6e6b4415fd19857fcb55fc" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture.</source>
          <target state="translated">ガウス混合物。</target>
        </trans-unit>
        <trans-unit id="52d32c3ce740bd6bf6fa9b8c9a00c471e2b8ab61" translate="yes" xml:space="preserve">
          <source>Gaussian Naive Bayes (GaussianNB)</source>
          <target state="translated">ガウシアンナイーブベイズ</target>
        </trans-unit>
        <trans-unit id="3e71cc209c706f89187660af28df9dbd656b7dfb" translate="yes" xml:space="preserve">
          <source>Gaussian Processes regression: basic introductory example</source>
          <target state="translated">ガウス過程回帰:基本的な入門例</target>
        </trans-unit>
        <trans-unit id="7c9060d2e2a8ab44211d4b8690374c1230f1b7f2" translate="yes" xml:space="preserve">
          <source>Gaussian kernel (&lt;code&gt;kernel = 'gaussian'&lt;/code&gt;)</source>
          <target state="translated">ガウスカーネル（ &lt;code&gt;kernel = 'gaussian'&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="16bd9bbb5a5342036acd14278f2e03ad41c57f6a" translate="yes" xml:space="preserve">
          <source>Gaussian mixture model fit with a variational inference.</source>
          <target state="translated">変分推論を用いたガウス混合モデルのフィット。</target>
        </trans-unit>
        <trans-unit id="c4278ff51902cddfc2c28028add69085822b616d" translate="yes" xml:space="preserve">
          <source>Gaussian mixture models, useful for clustering, are described in &lt;a href=&quot;mixture#mixture&quot;&gt;another chapter of the documentation&lt;/a&gt; dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component.</source>
          <target state="translated">クラスタリングに役立つガウス混合モデルは、混合モデル専用&lt;a href=&quot;mixture#mixture&quot;&gt;のドキュメントの別の章で&lt;/a&gt;説明されています。KMeansは、コンポーネントごとに等しい共分散をもつ混合ガウスモデルの特殊なケースと見なすことができます。</target>
        </trans-unit>
        <trans-unit id="52102b8851b98924c7d8b1f347902fc1a6a2f6c4" translate="yes" xml:space="preserve">
          <source>Gaussian mixtures</source>
          <target state="translated">ガウス混合</target>
        </trans-unit>
        <trans-unit id="fb2ed046d4b5b73ab490df316744dbd7803b27c6" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) based on Laplace approximation.</source>
          <target state="translated">ラプラス近似に基づくガウスプロセス分類(GPC)。</target>
        </trans-unit>
        <trans-unit id="6022eb0f0e245ca9c1dcd7d4b4311ff01e4db354" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) on iris dataset</source>
          <target state="translated">虹彩データセットにおけるガウスプロセス分類(GPC</target>
        </trans-unit>
        <trans-unit id="21a63bbdb2d774ad21ffa6c87b635dc00ddbdcbd" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) on Mauna Loa CO2 data.</source>
          <target state="translated">マウナロアCO2データのガウス過程回帰(GPR)。</target>
        </trans-unit>
        <trans-unit id="0c7b8e025d47923893c509b893c584646dec60f9" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) with noise-level estimation</source>
          <target state="translated">ノイズレベル推定を用いたガウス過程回帰(GPR)</target>
        </trans-unit>
        <trans-unit id="e020234a1ce464bccd79fd7ca6cd9571320c3263" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR).</source>
          <target state="translated">ガウス過程回帰(GPR)。</target>
        </trans-unit>
        <trans-unit id="3eef2758f8f04922436ba69e73f365c3b677d080" translate="yes" xml:space="preserve">
          <source>GaussianNaiveBayes tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features.</source>
          <target state="translated">GaussianNaiveBayesは,確率を0または1に押し上げる傾向がある(ヒストグラムのカウントに注意).これは主に,特徴がクラスを与えられた条件付きで独立しているという仮定をしているためですが,2つの冗長な特徴を含むこのデータセットではそうではありません.</target>
        </trans-unit>
        <trans-unit id="9ee50bfb8852bcfbfa07c7c7a246c842043563a2" translate="yes" xml:space="preserve">
          <source>General KDD structure :</source>
          <target state="translated">一般的なKDD構造 .</target>
        </trans-unit>
        <trans-unit id="340183f53d5a585fe2f90b1573169f80622dc9bd" translate="yes" xml:space="preserve">
          <source>General-purpose, even cluster size, flat geometry, not too many clusters</source>
          <target state="translated">汎用、均一なクラスタサイズ、フラットジオメトリ、クラスタ数が多すぎない</target>
        </trans-unit>
        <trans-unit id="a807e718c7c2444084ecd599b5293f02618f18b0" translate="yes" xml:space="preserve">
          <source>Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models.</source>
          <target state="translated">一般的に、モデルの複雑さが増すと、予測能力とレイテンシは増加すると考えられています。予測力の増加は通常興味深いものですが、多くのアプリケーションでは、予測待ち時間をあまり増加させない方がよいでしょう。ここでは、教師付きモデルのさまざまなファミリについて、この考え方をレビューします。</target>
        </trans-unit>
        <trans-unit id="af9887d0c879889fc0d4b97d28831fef1da0e335" translate="yes" xml:space="preserve">
          <source>Generate a distance matrix chunk by chunk with optional reduction</source>
          <target state="translated">オプションのリダクションを用いて,距離行列をチャンクごとに生成します.</target>
        </trans-unit>
        <trans-unit id="06c2a79c89c40ddc99e314455bfeabb348baaefc" translate="yes" xml:space="preserve">
          <source>Generate a mostly low rank matrix with bell-shaped singular values</source>
          <target state="translated">ベル型の特異値を持つほぼ低ランクの行列を生成する</target>
        </trans-unit>
        <trans-unit id="c1825817fcf44112a4d64fe6f2acf131fceae396" translate="yes" xml:space="preserve">
          <source>Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</source>
          <target state="translated">指定された次数以下の次数を持つ特徴のすべての多項式の組み合わせからなる新しい特徴行列を生成します.例えば,入力サンプルが2次元で[a,b]の形式の場合,次数2の多項式特徴量は[1,a,b,a^2,ab,b^2]となり,次数2の多項式特徴量は[1,a,b,a^2,ab,b^2]となります.</target>
        </trans-unit>
        <trans-unit id="138afdc51f7a90d9b74b5dc5c84735ab7ad5ab97" translate="yes" xml:space="preserve">
          <source>Generate a random multilabel classification problem.</source>
          <target state="translated">ランダムなマルチラベル分類問題を生成する。</target>
        </trans-unit>
        <trans-unit id="6e53d56707f7eb93fc64a285e9e5b0c1571546a7" translate="yes" xml:space="preserve">
          <source>Generate a random n-class classification problem.</source>
          <target state="translated">ランダムなnクラスの分類問題を生成する。</target>
        </trans-unit>
        <trans-unit id="45b70aa4bfe7b5254dd4845949fd163391dae828" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem with sparse uncorrelated design</source>
          <target state="translated">疎な無相関設計によるランダム回帰問題の生成</target>
        </trans-unit>
        <trans-unit id="097811da2f026de1c67525043ab17d6d057450a6" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem.</source>
          <target state="translated">ランダム回帰問題を生成します。</target>
        </trans-unit>
        <trans-unit id="90aba5bbbbad8863550c06ced91ee520b1c0caff" translate="yes" xml:space="preserve">
          <source>Generate a random symmetric, positive-definite matrix.</source>
          <target state="translated">ランダムな対称正定値行列を生成します。</target>
        </trans-unit>
        <trans-unit id="b303920886f4c442ac72ea67b8bd3cb1b7460430" translate="yes" xml:space="preserve">
          <source>Generate a signal as a sparse combination of dictionary elements.</source>
          <target state="translated">辞書要素の疎な組み合わせとして信号を生成します。</target>
        </trans-unit>
        <trans-unit id="035b22a208f9d34d7467f70a3f8e5a4c27edb9b2" translate="yes" xml:space="preserve">
          <source>Generate a sparse random projection matrix</source>
          <target state="translated">疎なランダム射影行列を生成する</target>
        </trans-unit>
        <trans-unit id="4dc557ac054fd2b6925cea078345560226a5469c" translate="yes" xml:space="preserve">
          <source>Generate a sparse symmetric definite positive matrix.</source>
          <target state="translated">疎な対称正定値行列を生成します。</target>
        </trans-unit>
        <trans-unit id="2b6ed08a20bd86f602cf70906530ae751a13aa6a" translate="yes" xml:space="preserve">
          <source>Generate a swiss roll dataset.</source>
          <target state="translated">スイスロールデータセットを生成します。</target>
        </trans-unit>
        <trans-unit id="2f7e815b3b193bc1cd3e7e4a28307316625909c7" translate="yes" xml:space="preserve">
          <source>Generate an S curve dataset.</source>
          <target state="translated">S曲線データセットを生成します。</target>
        </trans-unit>
        <trans-unit id="a97cf86ca659bda28267893fc11990f8622b62e7" translate="yes" xml:space="preserve">
          <source>Generate an array with block checkerboard structure for biclustering.</source>
          <target state="translated">バイクラスタリングのためのブロックチェッカーボード構造の配列を生成します。</target>
        </trans-unit>
        <trans-unit id="a9f13a8783d09446e6122b3e3234e1d6fcb95591" translate="yes" xml:space="preserve">
          <source>Generate an array with constant block diagonal structure for biclustering.</source>
          <target state="translated">バイクラスタリングのための一定のブロック対角構造を持つ配列を生成します.</target>
        </trans-unit>
        <trans-unit id="afeaee3f091598162e7eb33b08779a77e0e748f4" translate="yes" xml:space="preserve">
          <source>Generate cross-validated estimates for each input data point</source>
          <target state="translated">各入力データポイントについて交差検証された推定値を生成する</target>
        </trans-unit>
        <trans-unit id="99b9ba538a40d50737f63d924a3c7ce27d75993f" translate="yes" xml:space="preserve">
          <source>Generate datasets. We choose the size big enough to see the scalability of the algorithms, but not too big to avoid too long running times</source>
          <target state="translated">データセットを生成します。アルゴリズムのスケーラビリティを確認するのに十分な大きさを選びますが、実行時間が長くなりすぎないように大きすぎないようにします。</target>
        </trans-unit>
        <trans-unit id="c00dd920cc2725de42546dcb337634c4ac897029" translate="yes" xml:space="preserve">
          <source>Generate indices to split data into training and test set.</source>
          <target state="translated">データをトレーニングセットとテストセットに分割するためのインデックスを生成します。</target>
        </trans-unit>
        <trans-unit id="5107cc8a6ff57cac684ccce1f62420eaa4260507" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian and label samples by quantile</source>
          <target state="translated">等方性ガウシアンを生成し、分級値でサンプルをラベル付けする</target>
        </trans-unit>
        <trans-unit id="8e89de3bc63d92fa78eda36337c27db80aab71fe" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian blobs for clustering.</source>
          <target state="translated">クラスタリングのための等方性ガウスブロブを生成します。</target>
        </trans-unit>
        <trans-unit id="37d03dbfefb10390fe483e5ed2d7b03c5a459fa1" translate="yes" xml:space="preserve">
          <source>Generate missing values indicator for X.</source>
          <target state="translated">Xの欠損値指標を生成します。</target>
        </trans-unit>
        <trans-unit id="462cab2784077aa54955d18bb40a9de12e6edf3c" translate="yes" xml:space="preserve">
          <source>Generate polynomial and interaction features.</source>
          <target state="translated">多項式と相互作用の特徴を生成します。</target>
        </trans-unit>
        <trans-unit id="d1bba874447d3710a4261bda204e3775c6148149" translate="yes" xml:space="preserve">
          <source>Generate random samples from the fitted Gaussian distribution.</source>
          <target state="translated">フィットしたガウス分布からランダムサンプルを生成します。</target>
        </trans-unit>
        <trans-unit id="ce67c2d91c83a1d56ab9a9ee35d822063af6506a" translate="yes" xml:space="preserve">
          <source>Generate random samples from the model.</source>
          <target state="translated">モデルからランダムサンプルを生成します。</target>
        </trans-unit>
        <trans-unit id="f4defed702b6f02ff908f1cd9f8b411c35ee40dd" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #1&amp;rdquo; regression problem</source>
          <target state="translated">「フリードマン＃1」の回帰問題を生成する</target>
        </trans-unit>
        <trans-unit id="75088d435099809ee2a5f0ec830b6e2b26fb0500" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #2&amp;rdquo; regression problem</source>
          <target state="translated">「フリードマン＃2」回帰問題を生成する</target>
        </trans-unit>
        <trans-unit id="18ca02f4b303dec3c31289cd6db22246b19d8adb" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #3&amp;rdquo; regression problem</source>
          <target state="translated">「フリードマン＃3」回帰問題を生成する</target>
        </trans-unit>
        <trans-unit id="1526c84b2e9b495f9ed3216009ebf8b31d461518" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al.</source>
          <target state="translated">Hastie et al.で使用されているバイナリ分類用のデータを生成します。</target>
        </trans-unit>
        <trans-unit id="c2cb269fed6a06711794c0a014b9a89e92300ddb" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al. 2009, Example 10.2.</source>
          <target state="translated">Hastie et al.2009,例10.2で使用されている二値分類用のデータを生成します。</target>
        </trans-unit>
        <trans-unit id="fbfd61fc35f16aea2f376426724b313bf45b644a" translate="yes" xml:space="preserve">
          <source>Generates indices to split data into training and test set.</source>
          <target state="translated">データをトレーニングセットとテストセットに分割するためのインデックスを生成します。</target>
        </trans-unit>
        <trans-unit id="9a963ad633fdf36ff4f1d429308e1f3d90a2ceea" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on predefined splits.</source>
          <target state="translated">事前に定義されたスプリットに基づいて訓練/テスト指標を生成します。</target>
        </trans-unit>
        <trans-unit id="4678269441c5cad2dec162c29e80b19e70944794" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on random permutation.</source>
          <target state="translated">ランダムな並べ替えに基づいて訓練/テスト指標を生成します。</target>
        </trans-unit>
        <trans-unit id="025efadf6f18cb5d61732c8188dd311431f2fe8b" translate="yes" xml:space="preserve">
          <source>Generator on parameters sampled from given distributions.</source>
          <target state="translated">与えられた分布からサンプリングされたパラメータを生成します。</target>
        </trans-unit>
        <trans-unit id="9008c79b50b6e856f48dd8a1acb75bd481c83565" translate="yes" xml:space="preserve">
          <source>Generator to create n_packs slices going up to n.</source>
          <target state="translated">n までの n_packs スライスを作成するジェネレータ。</target>
        </trans-unit>
        <trans-unit id="6a34af9aa1c17133e53bdde13fa952c7bcbcf3f6" translate="yes" xml:space="preserve">
          <source>Geometry (metric used)</source>
          <target state="translated">幾何学(メートル法を使用</target>
        </trans-unit>
        <trans-unit id="e5f048789e3e59e8993091df470af502112331aa" translate="yes" xml:space="preserve">
          <source>George W Bush</source>
          <target state="translated">ジョージ・W・ブッシュ</target>
        </trans-unit>
        <trans-unit id="b583db923d23716d80d92ca8bb6a609aa1f738a2" translate="yes" xml:space="preserve">
          <source>Gerhard Schroeder</source>
          <target state="translated">ゲルハルト・シュローダー</target>
        </trans-unit>
        <trans-unit id="33868dad5f60b783d41cfb7c4e686fd5af82ea02" translate="yes" xml:space="preserve">
          <source>Get a list of all estimators from sklearn.</source>
          <target state="translated">sklearnのすべての見積もり担当者のリストを取得します。</target>
        </trans-unit>
        <trans-unit id="c89b4f911ae16fa0b7caa09ce0c140306df6a7bd" translate="yes" xml:space="preserve">
          <source>Get a mask, or integer index, of the features selected</source>
          <target state="translated">選択された特徴のマスク、または整数値のインデックスを取得します。</target>
        </trans-unit>
        <trans-unit id="72908cf84377de645c7534a22afeddeeaba91d9d" translate="yes" xml:space="preserve">
          <source>Get a scorer from string</source>
          <target state="translated">文字列からスコアラーを取得</target>
        </trans-unit>
        <trans-unit id="45a250b2600ca82b0e59f392e6c981ee3cc2728d" translate="yes" xml:space="preserve">
          <source>Get feature names from all transformers.</source>
          <target state="translated">すべてのトランスフォーマーから機能名を取得します。</target>
        </trans-unit>
        <trans-unit id="4be0c520942fc8926cfd53e42cd4ae1d1cc70df9" translate="yes" xml:space="preserve">
          <source>Get parameters for this estimator.</source>
          <target state="translated">この推定子のパラメータを取得します。</target>
        </trans-unit>
        <trans-unit id="fe15f50ace10fe1b8c70139542f4a1796682abb3" translate="yes" xml:space="preserve">
          <source>Get parameters of this kernel.</source>
          <target state="translated">このカーネルのパラメータを取得します。</target>
        </trans-unit>
        <trans-unit id="1314abe875bac1db97b1a7155d7b4a8c13c230ee" translate="yes" xml:space="preserve">
          <source>Get predictions from each split of cross-validation for diagnostic purposes.</source>
          <target state="translated">診断のためのクロスバリデーションの各スプリットから予測値を取得します。</target>
        </trans-unit>
        <trans-unit id="dd0a065fc935a1fd709e1a1d7d55ca6c3433dca5" translate="yes" xml:space="preserve">
          <source>Get the given distance metric from the string identifier.</source>
          <target state="translated">文字列識別子から与えられた距離メトリックを取得します。</target>
        </trans-unit>
        <trans-unit id="df2089c702273c8bc78b6842775813fe9702ad55" translate="yes" xml:space="preserve">
          <source>Get the parameters of the VotingClassifier</source>
          <target state="translated">VotingClassifier のパラメータを取得します。</target>
        </trans-unit>
        <trans-unit id="1f6030226293d5ed7b4d4b045e215d6de20db61c" translate="yes" xml:space="preserve">
          <source>Getter for the precision matrix.</source>
          <target state="translated">精密行列のゲッターです。</target>
        </trans-unit>
        <trans-unit id="53379a8bafa1cbd8bc5da14050f14d3817f01039" translate="yes" xml:space="preserve">
          <source>Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the &amp;lsquo;directions of covariance&amp;rsquo;, i.e. the components of each datasets that explain the most shared variance between both datasets. This is apparent on the &lt;strong&gt;scatterplot matrix&lt;/strong&gt; display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the first diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different components is weak: the point cloud is very spherical.</source>
          <target state="translated">2つの多変量共変2次元データセット、X、およびYが与えられると、PLSは「共分散の方向」、つまり両方のデータセット間で最も共有される分散を説明する各データセットのコンポーネントを抽出します。これは、&lt;strong&gt;散布図のマトリックス&lt;/strong&gt;表示で明らかです。データセットXとデータセットYのコンポーネント1は、最大の相関関係があります（ポイントは最初の対角線の周りにあります）。これは両方のデータセットのコンポーネント2にも当てはまりますが、異なるコンポーネントのデータセット間の相関は弱く、点群は非常に球形です。</target>
        </trans-unit>
        <trans-unit id="16179644ab5a4c2a1f730ff634ab3d4d3a869791" translate="yes" xml:space="preserve">
          <source>Given a candidate centroid \(x_i\) for iteration \(t\), the candidate is updated according to the following equation:</source>
          <target state="translated">候補のセントロイドがあると、次の式で更新されます。</target>
        </trans-unit>
        <trans-unit id="4368fa47ed8eb35b757e7b3d5aaf6d7ee1cd4ff6" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to a binary one-hot encoding.</source>
          <target state="translated">2つの特徴を持つデータセットが与えられた場合、エンコーダに特徴ごとにユニークな値を見つけさせ、データをバイナリのワンショットエンコーディングに変換させます。</target>
        </trans-unit>
        <trans-unit id="a65060cb3a96ad97e8800308b9076a9a49180060" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding.</source>
          <target state="translated">2つの特徴量を持つデータセットが与えられた場合,エンコーダーに特徴量ごとのユニークな値を見つけさせ,データを順序エンコーディングに変換させます.</target>
        </trans-unit>
        <trans-unit id="6cc0cdb4252ae3fe585bd759a612161dfe7c6d85" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^n\) and \(y_i \in \{0, 1\}\), a one hidden layer one hidden neuron MLP learns the function \(f(x) = W_2 g(W_1^T x + b_1) + b_2\) where \(W_1 \in \mathbf{R}^m\) and \(W_2, b_1, b_2 \in \mathbf{R}\) are model parameters. \(W_1, W_2\) represent the weights of the input layer and hidden layer, respectively; and \(b_1, b_2\) represent the bias added to the hidden layer and the output layer, respectively. \(g(\cdot) : R \rightarrow R\) is the activation function, set by default as the hyperbolic tan. It is given as,</source>
          <target state="translated">隠れ層1ニューロンMLPは,関数 \(f(x)=W_2 g(W_1^T x+b_1)+b_2\)を学習する.ここで,W\(W_1 in \mathbf{R}^m)と\(W_2,b_1,b_2 \mathbf{R})はモデルパラメータである.\W\(W_1,W_2)は,入力層と隠蔽層の重みを,\(b_1,b_2\)は,隠蔽層と出力層のバイアスをそれぞれ表している.\は、活性化関数であり、双曲線のtanがデフォルトで設定されている。次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="99b85508f1069fad6e9945b3624fea4140b5fbae" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \{-1,1\}\), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions, we simply look at the sign of \(f(x)\). A common choice to find the model parameters is by minimizing the regularized training error given by</source>
          <target state="translated">our goal is to learn a linear scoring function &quot;\(f(x)=w^T x+b\)&quot; with model parameters &quot;w (w \in \mathbf{R}^m)&quot; and intercept &quot;b (b \in \mathbf{R})&quot; with model parameters &quot;w \(w \in \mathbf{R}^m)予測をするためには、「\(f(x)x)\」の符号を見るだけでよい。モデル・パラメータを見つけるための一般的な選択は、次式で与えられる正則化された訓練誤差を最小化することである。</target>
        </trans-unit>
        <trans-unit id="f05ffd1dc56829aeb2ce3b1aa47183d5a5a71272" translate="yes" xml:space="preserve">
          <source>Given an exception, a callable to raise the exception, and a message string, tests that the correct exception is raised and that the message is a substring of the error thrown. Used to test that the specific message thrown during an exception is correct.</source>
          <target state="translated">例外と、例外を発生させるための callable、そしてメッセージ文字列が与えられると、 正しい例外が発生し、そのメッセージがスローされたエラーの部分文字列であるかどうかをテストします。例外が発生した際にスローされた特定のメッセージが正しいかどうかをテストするために使用されます。</target>
        </trans-unit>
        <trans-unit id="d5588778e54082615cf481fafbc7dcf0b337d76d" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (&lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt;&lt;code&gt;RFE&lt;/code&gt;&lt;/a&gt;) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">フィーチャ（たとえば、線形モデルの係数）に重みを割り当てる外部推定器がある場合、再帰的フィーチャ除去（&lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt; &lt;code&gt;RFE&lt;/code&gt; &lt;/a&gt;）は、フィーチャのより小さなセットを再帰的に考慮してフィーチャを選択することです。最初に、推定器は最初の特徴セットでトレーニングされ、各特徴の重要度は &lt;code&gt;coef_&lt;/code&gt; 属性または &lt;code&gt;feature_importances_&lt;/code&gt; 属性のいずれかによって取得されます。次に、最も重要度の低い機能が現在の機能セットからプルーニングされます。この手順は、選択した機能の数が最終的に到達するまで、プルーニングセットで再帰的に繰り返されます。</target>
        </trans-unit>
        <trans-unit id="0a3e62329db7e0582a525546102e4bc5a3e414ee" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">フィーチャ（たとえば、線形モデルの係数）に重みを割り当てる外部推定量が与えられた場合、再帰的フィーチャ除去（RFE）の目的は、フィーチャのより小さなセットを再帰的に考慮することによってフィーチャを選択することです。最初に、推定器は最初の特徴セットでトレーニングされ、各特徴の重要度は &lt;code&gt;coef_&lt;/code&gt; 属性または &lt;code&gt;feature_importances_&lt;/code&gt; 属性のいずれかによって取得されます。次に、最も重要でない機能が現在の機能セットから削除されます。その手順は、最終的に選択する機能の数が最終的に到達するまで、プルーニングセットで再帰的に繰り返されます。</target>
        </trans-unit>
        <trans-unit id="2e4a90e9413cabdb8d0d79c137af8efe3fbd16ef" translate="yes" xml:space="preserve">
          <source>Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the &lt;code&gt;init='k-means++'&lt;/code&gt; parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.</source>
          <target state="translated">十分な時間が与えられれば、K-meansは常に収束しますが、これは局所的な最小値になる場合があります。これは重心の初期化に大きく依存します。その結果、重心の異なる初期化を使用して、計算がしばしば数回行われます。この問題に対処するための1つの方法は、k-means ++初期化スキームです。これは、scikit-learnに実装されています（ &lt;code&gt;init='k-means++'&lt;/code&gt; パラメーターを使用）。これにより、図に示されているように、重心が互いに（一般的に）離れるように初期化され、ランダムな初期化よりも明らかに良い結果が得られます。</target>
        </trans-unit>
        <trans-unit id="74d4aecb20e2cdcd5c8865136aad914eecac7d61" translate="yes" xml:space="preserve">
          <source>Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a &lt;strong&gt;clustering task&lt;/strong&gt;: split the observations into well-separated group called &lt;em&gt;clusters&lt;/em&gt;.</source>
          <target state="translated">私たちはアイリスの3種類があったことを知っていたが、それらにラベルを付けるために分類学者へのアクセス権を持っていなかった場合は、アイリスデータセットを考える：私たちは試みることができる&lt;strong&gt;クラスタリングタスクを&lt;/strong&gt;：十分に分離グループと呼ばれるに観測を分割し&lt;em&gt;たクラスタ&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="7ffdaa4cdda4b54b62086a7f5ac68bd7ea3b5908" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;Mutual Information&lt;/strong&gt; is a function that measures the &lt;strong&gt;agreement&lt;/strong&gt; of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, &lt;strong&gt;Normalized Mutual Information (NMI)&lt;/strong&gt; and &lt;strong&gt;Adjusted Mutual Information (AMI)&lt;/strong&gt;. NMI is often used in the literature, while AMI was proposed more recently and is &lt;strong&gt;normalized against chance&lt;/strong&gt;:</source>
          <target state="translated">グラウンドトゥルースクラスの割り当て &lt;code&gt;labels_true&lt;/code&gt; と、同じサンプル &lt;code&gt;labels_pred&lt;/code&gt; のクラスタリングアルゴリズムの割り当てに関する知識がある場合、&lt;strong&gt;相互情報&lt;/strong&gt;は、順列を無視して、2つの割り当ての&lt;strong&gt;一致&lt;/strong&gt;を測定する関数です。このメジャーの2つの異なる正規化バージョン、&lt;strong&gt;Normalized Mutual Information（NMI）&lt;/strong&gt;と&lt;strong&gt;Adjusted Mutual Information（AMI）を使用でき&lt;/strong&gt;ます。 NMIは文献でよく使用されますが、AMIはより最近提案され&lt;strong&gt;、偶然に対して正規化&lt;/strong&gt;されています。</target>
        </trans-unit>
        <trans-unit id="943836cb04e0640667940c68f56d5deeb3e35898" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;adjusted Rand index&lt;/strong&gt; is a function that measures the &lt;strong&gt;similarity&lt;/strong&gt; of the two assignments, ignoring permutations and &lt;strong&gt;with chance normalization&lt;/strong&gt;:</source>
          <target state="translated">グラウンドトゥルースクラスの割り当て &lt;code&gt;labels_true&lt;/code&gt; と、同じサンプル &lt;code&gt;labels_pred&lt;/code&gt; のクラスタリングアルゴリズムの割り当てに関する知識がある場合、&lt;strong&gt;調整されたRandインデックス&lt;/strong&gt;は、順列を無視し&lt;strong&gt;、偶然に正規化&lt;/strong&gt;して、2つの割り当ての&lt;strong&gt;類似性&lt;/strong&gt;を測定する関数です。&lt;strong&gt;&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="3a989bbd6a98db5dab53799fee5637e2080ce141" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.</source>
          <target state="translated">サンプルの基底真理クラスの割り当ての知識があれば、条件付きエントロピー分析を用いて直感的なメトリックを定義することができます。</target>
        </trans-unit>
        <trans-unit id="4d7a7b1af5c7c7276434270fce7100038c705add" translate="yes" xml:space="preserve">
          <source>Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered.</source>
          <target state="translated">これらの特異なベクトルが与えられると、それらは、断片的に一定のベクトルで最もよく近似できるものに応じてランク付けされる。各ベクトルの近似は,1次元のk-meansを用いて発見され,ユークリッド距離を用いて採点される.最良の左右特異ベクトルのいくつかの部分集合が選択される。次に、データはこの最良の特異ベクトルの部分集合に投影され、クラスタ化される。</target>
        </trans-unit>
        <trans-unit id="21675a464e2ca3b8f99eef191d00e106aa21c0dd" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in R^n\), i=1,&amp;hellip;, l and a label vector \(y \in R^l\), a decision tree recursively partitions the space such that the samples with the same labels are grouped together.</source>
          <target state="translated">トレーニングベクトル\（x_i \ in R ^ n \）、i = 1、&amp;hellip;、lとラベルベクトル\（y \ in R ^ l \）が与えられると、決定木は空間が再帰的に分割され、同じサンプルがラベルは一緒にグループ化されます。</target>
        </trans-unit>
        <trans-unit id="02fd4db44c84fce9026584422f7727ba079bc40a" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, and a vector \(y \in \mathbb{R}^n\)\(\varepsilon\)-SVR solves the following primal problem:</source>
          <target state="translated">与えられた訓練ベクトル\（x_i \ in \ mathbb {R} ^ p \）、i = 1、&amp;hellip;、n、およびベクトル\（y \ in \ mathbb {R} ^ n \）\（\ varepsilon \）- SVRは次の主要な問題を解決します。</target>
        </trans-unit>
        <trans-unit id="70e397398a5003e0a6b00de067e9804bfe571e70" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, in two classes, and a vector \(y \in \{1, -1\}^n\), SVC solves the following primal problem:</source>
          <target state="translated">与えられたトレーニングベクトル\（x_i \ in \ mathbb {R} ^ p \）、i = 1、&amp;hellip;、n、2つのクラス、およびベクトル\（y \ in \ {1、-1 \} ^ n \） 、SVCは次の主要な問題を解決します。</target>
        </trans-unit>
        <trans-unit id="e44bf83eca8aa1cc0c5bdaa89da0afa702f51625" translate="yes" xml:space="preserve">
          <source>Gives the number of (complex) sampling points.</source>
          <target state="translated">(複素数の)サンプリングポイントの数を与えます。</target>
        </trans-unit>
        <trans-unit id="f36c7685daa8ebc7e1344aa0d6e3a7d679decebf" translate="yes" xml:space="preserve">
          <source>Global structure is not explicitly preserved. This is problem is mitigated by initializing points with PCA (using &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt;).</source>
          <target state="translated">グローバル構造は明示的に保存されません。これは、PCAでポイントを初期化することで問題が軽減されます（ &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="178c27bf7200da0534de904ea7e6ca7da842dbb5" translate="yes" xml:space="preserve">
          <source>Glorot, Xavier, and Yoshua Bengio. &amp;ldquo;Understanding the difficulty of</source>
          <target state="translated">Glorot、Xavier、Yoshua Bengio。「の難しさを理解する</target>
        </trans-unit>
        <trans-unit id="7427cf697be16a4ec1d916910128a59d920125e7" translate="yes" xml:space="preserve">
          <source>Glossary</source>
          <target state="translated">Glossary</target>
        </trans-unit>
        <trans-unit id="f7c22aaad44fb28f4ee8f06d6d4f4f14ac9ce899" translate="yes" xml:space="preserve">
          <source>Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,</source>
          <target state="translated">ゴルブとC.ヴァンローン。行列計算、第三版、第五章。</target>
        </trans-unit>
        <trans-unit id="1de5b736be2f9def46d07ed88549feeeea5a97b0" translate="yes" xml:space="preserve">
          <source>Gorodkin, (2004). Comparing two K-category assignments by a K-category correlation coefficient</source>
          <target state="translated">Gorodkin,(2004).Kカテゴリ相関係数による2つのKカテゴリ割り当ての比較</target>
        </trans-unit>
        <trans-unit id="46268d41f41f8e1954ca3d54fd29ddb1959ea6db" translate="yes" xml:space="preserve">
          <source>Gradient Boosting Out-of-Bag estimates</source>
          <target state="translated">勾配ブーストのアウトオブバグ推定</target>
        </trans-unit>
        <trans-unit id="3e3d95a92c5a33953c001956fd3fd6ac3b1082fa" translate="yes" xml:space="preserve">
          <source>Gradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent direction is the negative gradient of the loss function evaluated at the current model \(F_{m-1}\) which can be calculated for any differentiable loss function:</source>
          <target state="translated">勾配ブースト法は、この最小化問題を急勾配降下を用いて数値的に解決しようとするものである。最急降下の方向は、現在のモデルで評価された損失関数の負の勾配であり、微分可能な損失関数に対して計算することができます。</target>
        </trans-unit>
        <trans-unit id="be45c92854a0f55592d6c3c1c28201cf75d59d94" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for classification.</source>
          <target state="translated">分類のための勾配ブースト</target>
        </trans-unit>
        <trans-unit id="65fd480d2da13d80eb18643fd08c31b9e5239c9a" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for regression.</source>
          <target state="translated">回帰のための勾配ブースト</target>
        </trans-unit>
        <trans-unit id="23dcf8253cdacbdd915f0e5e69e684c3457ad1df" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regression</source>
          <target state="translated">勾配ブースト回帰</target>
        </trans-unit>
        <trans-unit id="33b1659de13c2a7e036f71b3c26eda1d552a4b1c" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regularization</source>
          <target state="translated">勾配ブースト正則化</target>
        </trans-unit>
        <trans-unit id="cf9557c4e6e59de44aebd2e8b07221ff19e46958" translate="yes" xml:space="preserve">
          <source>Gradient boosting is an ensembling technique where several weak learners (regression trees) are combined to yield a powerful single model, in an iterative fashion.</source>
          <target state="translated">勾配ブーストは、いくつかの弱い学習者(回帰木)を組み合わせて、強力な単一モデルを反復的に生成するアンサンブル技術である。</target>
        </trans-unit>
        <trans-unit id="c4611f197e5e7430aa271445ae503720ad1cf3d4" translate="yes" xml:space="preserve">
          <source>Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True.</source>
          <target state="translated">シータの位置におけるカーネルハイパーパラメータに対する対数倍尤度の勾配。eval_gradientがTrueの場合のみ返される。</target>
        </trans-unit>
        <trans-unit id="f77edae6db0cdcd4449adeeb038c653af7406ea3" translate="yes" xml:space="preserve">
          <source>Gram Orthogonal Matching Pursuit (OMP)</source>
          <target state="translated">グラム直交マッチングパシュート(OMP</target>
        </trans-unit>
        <trans-unit id="10ef9123115df39a65f62ffa3d9d0e10899ca7cd" translate="yes" xml:space="preserve">
          <source>Gram matrix of the input data: X.T * X</source>
          <target state="translated">入力データのグラム行列。X.T*X</target>
        </trans-unit>
        <trans-unit id="a83784084519ce853a92535121a74c85019c19b0" translate="yes" xml:space="preserve">
          <source>Graph distance (e.g. nearest-neighbor graph)</source>
          <target state="translated">グラフの距離(例:最も近い隣のグラフ)</target>
        </trans-unit>
        <trans-unit id="8d5c9a04db77341319c1b38643f0d38066fc8710" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel connections</source>
          <target state="translated">画素間接続のグラフ</target>
        </trans-unit>
        <trans-unit id="1b6f746d097f9fe3740f364d944363a7e3d991f9" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel gradient connections</source>
          <target state="translated">ピクセル間のグラデーション接続のグラフ</target>
        </trans-unit>
        <trans-unit id="933bf21afdd55a0d2283845fed0e7bbdd1f5db49" translate="yes" xml:space="preserve">
          <source>Green</source>
          <target state="translated">Green</target>
        </trans-unit>
        <trans-unit id="9786dcbe8afbab8ac93bdfcd6653b6cd7aa7993b" translate="yes" xml:space="preserve">
          <source>Grid of Cs used for cross-validation.</source>
          <target state="translated">クロスバリデーションに使用されるCsのグリッド。</target>
        </trans-unit>
        <trans-unit id="5bd85812ea7e2436359885d902fd71d10cd1c2d9" translate="yes" xml:space="preserve">
          <source>Grid of parameters with a discrete number of values for each.</source>
          <target state="translated">それぞれの値が離散的な数のパラメータのグリッド。</target>
        </trans-unit>
        <trans-unit id="4a6f9190abeab5c3ccde3d9c276bc4db019e7d38" translate="yes" xml:space="preserve">
          <source>Grid search can also be performed on the different preprocessing steps defined in the &lt;code&gt;ColumnTransformer&lt;/code&gt; object, together with the classifier&amp;rsquo;s hyperparameters as part of the &lt;code&gt;Pipeline&lt;/code&gt;. We will search for both the imputer strategy of the numeric preprocessing and the regularization parameter of the logistic regression using &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">グリッド検索は、 &lt;code&gt;Pipeline&lt;/code&gt; の一部としての分類子のハイパー &lt;code&gt;ColumnTransformer&lt;/code&gt; と共に、ColumnTransformerオブジェクトで定義されたさまざまな前処理ステップで実行することもできます。私たちは、数値の前処理のimputer戦略と使用してロジスティック回帰の正則化パラメータの両方を検索します&lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; を&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="71a1782f5aa6d2b7cc26a083f91eef66c1cf3aff" translate="yes" xml:space="preserve">
          <source>Grid-search</source>
          <target state="translated">Grid-search</target>
        </trans-unit>
        <trans-unit id="ed926e289de9aa5047e6b09f7b537df04bde4bbf" translate="yes" xml:space="preserve">
          <source>Grid-search and cross-validated estimators</source>
          <target state="translated">グリッド検索と交差検証された推定量</target>
        </trans-unit>
        <trans-unit id="64ba146c44fdd8e95f622a314398320f76845aed" translate="yes" xml:space="preserve">
          <source>GridSearchCV implements a &amp;ldquo;fit&amp;rdquo; and a &amp;ldquo;score&amp;rdquo; method. It also implements &amp;ldquo;predict&amp;rdquo;, &amp;ldquo;predict_proba&amp;rdquo;, &amp;ldquo;decision_function&amp;rdquo;, &amp;ldquo;transform&amp;rdquo; and &amp;ldquo;inverse_transform&amp;rdquo; if they are implemented in the estimator used.</source>
          <target state="translated">GridSearchCVは、「フィット」および「スコア」メソッドを実装します。また、使用する推定器に実装されている場合は、「predict」、「predict_proba」、「decision_function」、「transform」、「inverse_transform」も実装します。</target>
        </trans-unit>
        <trans-unit id="2e6f2bdd92d1c5e33352841cf6b10ed864b19fa7" translate="yes" xml:space="preserve">
          <source>Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification: An Overview. International Journal of Data Warehousing &amp;amp; Mining, 3(3), 1-13, July-September 2007.</source>
          <target state="translated">Grigorios Tsoumakas、Ioannis Katakis。マルチラベル分類：概要。International Journal of Data Warehousing＆Mining、3（3）、1-13、July-September 2007。</target>
        </trans-unit>
        <trans-unit id="796325c68f51f69a2afcc84a9fb61fa1d8420435" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels for n_samples samples.</source>
          <target state="translated">n_samplesサンプルの基底真理(正しい)ラベル。</target>
        </trans-unit>
        <trans-unit id="740dd68aa13d511b42941c79135a52aa5a0f5bc4" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels.</source>
          <target state="translated">根拠のある真実(正しい)のラベル。</target>
        </trans-unit>
        <trans-unit id="cf154969e860842a471602bf65b740057751e47b" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values.</source>
          <target state="translated">根拠のある真実(正しい)目標値。</target>
        </trans-unit>
        <trans-unit id="691f624e8ff75b4d50175631f407699ccfb7e35d" translate="yes" xml:space="preserve">
          <source>Ground truth class labels to be used as a reference</source>
          <target state="translated">参照として使用する基底真理クラスのラベル</target>
        </trans-unit>
        <trans-unit id="2859baca63ac3255284d20bc28f887a4c54fefb4" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set.</source>
          <target state="translated">データセットを訓練/テストセットに分割する際に使用するサンプルのグループラベル。</target>
        </trans-unit>
        <trans-unit id="f5ee660cf40b3d432d2833cbe2c4255cb71b873d" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set. This &amp;lsquo;groups&amp;rsquo; parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.</source>
          <target state="translated">データセットをトレーニング/テストセットに分割するときに使用されるサンプルのグループラベル。この「グループ」パラメーターは、分割数を計算するために常に指定する必要がありますが、他のパラメーターは省略できます。</target>
        </trans-unit>
        <trans-unit id="2fe58cc1aca321453c1632eb3218b2ee2034ed27" translate="yes" xml:space="preserve">
          <source>Grow a tree with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">&lt;code&gt;max_leaf_nodes&lt;/code&gt; を使用して、ツリーを最初の方法で成長させます。最良のノードは、不純物の相対的な減少として定義されます。Noneの場合、リーフノードの数に制限はありません。</target>
        </trans-unit>
        <trans-unit id="9f319cd9d13cdc03649579ff252c6b96c720508d" translate="yes" xml:space="preserve">
          <source>Grow trees with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">&lt;code&gt;max_leaf_nodes&lt;/code&gt; を使用してツリーを最高の方法で成長させます。最良のノードは、不純物の相対的な減少として定義されます。Noneの場合、リーフノードの数に制限はありません。</target>
        </trans-unit>
        <trans-unit id="bf073fae640ded81eeb7a4cee70faff4a623c16c" translate="yes" xml:space="preserve">
          <source>Guide</source>
          <target state="translated">ガイド</target>
        </trans-unit>
        <trans-unit id="1fd932db6b504d046b60a30c3273eb39ba2ac7a5" translate="yes" xml:space="preserve">
          <source>Guyon, I., Weston, J., Barnhill, S., &amp;amp; Vapnik, V., &amp;ldquo;Gene selection for cancer classification using support vector machines&amp;rdquo;, Mach. Learn., 46(1-3), 389&amp;ndash;422, 2002.</source>
          <target state="translated">Guyon、I.、Weston、J.、Barnhill、S。、およびVapnik、V。、「サポートベクターマシンを使用した癌分類のための遺伝子選択」、Mach。Learn。、46（1-3）、389&amp;ndash;422、2002。</target>
        </trans-unit>
        <trans-unit id="dd4d457c816b0cb358c91f5b8813986bac26cb3d" translate="yes" xml:space="preserve">
          <source>H. Zhang (2004). &lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;The optimality of Naive Bayes.&lt;/a&gt; Proc. FLAIRS.</source>
          <target state="translated">H. Zhang（2004）。&lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;ナイーブベイズの最適性。&lt;/a&gt;手続き FLAIRS。</target>
        </trans-unit>
        <trans-unit id="f5b6915b0e377ea69d7b62d27d3f027cc63657d7" translate="yes" xml:space="preserve">
          <source>Hagai Attias. (2000). &amp;ldquo;A Variational Bayesian Framework for Graphical Models&amp;rdquo;. In Advances in Neural Information Processing Systems 12.</source>
          <target state="translated">Hagai Attias。（2000）。「グラフィカルモデルの変分ベイズフレームワーク」。神経情報処理システムの進歩12。</target>
        </trans-unit>
        <trans-unit id="8446ed65374f4c03b547ffebe7ab69437207be78" translate="yes" xml:space="preserve">
          <source>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo; Journal of Intelligent Information Systems, 17(2-3), 107-145. &lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi:10.1023/A:1012801612483&lt;/a&gt;.</source>
          <target state="translated">ハルキディ、マリア; バチスタキス、ヤニス。Vazirgiannis、Michalis（2001）。「クラスタリング検証手法について」Journal of Intelligent Information Systems、17（2-3）、107-145。&lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi：10.1023 / A：1012801612483&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="57fe625410e680c160d128700bfb1af1b965809e" translate="yes" xml:space="preserve">
          <source>HammingDistance</source>
          <target state="translated">HammingDistance</target>
        </trans-unit>
        <trans-unit id="dee17735ec3038cb9f5dda5413031eefdf59071a" translate="yes" xml:space="preserve">
          <source>Handle or name of the output file. If &lt;code&gt;None&lt;/code&gt;, the result is returned as a string.</source>
          <target state="translated">出力ファイルのハンドルまたは名前。場合 &lt;code&gt;None&lt;/code&gt; 、結果は文字列として返されます。</target>
        </trans-unit>
        <trans-unit id="077bb86f8a4736a0992a0b108c1d4b8e9298e04f" translate="yes" xml:space="preserve">
          <source>Hard constraint to select the backend. If set to &amp;lsquo;sharedmem&amp;rsquo;, the selected backend will be single-host and thread-based even if the user asked for a non-thread based backend with parallel_backend.</source>
          <target state="translated">バックエンドを選択するためのハード制約。'sharedmem'に設定すると、ユーザーがparallel_backendを使用して非スレッドベースのバックエンドを要求した場合でも、選択されたバックエンドは単一ホストでスレッドベースになります。</target>
        </trans-unit>
        <trans-unit id="9b9156693e970a15a3c18a9425374c7bf2903574" translate="yes" xml:space="preserve">
          <source>Hard limit on iterations within solver, or -1 for no limit.</source>
          <target state="translated">ソルバー内での繰り返しのハードリミット、またはリミットがない場合は-1。</target>
        </trans-unit>
        <trans-unit id="73dd008516fbc283773051e5943e3b658488b1b1" translate="yes" xml:space="preserve">
          <source>Harrison, D. and Rubinfeld, D.L.</source>
          <target state="translated">ハリソン、D.とルビンフェルド、D.L.</target>
        </trans-unit>
        <trans-unit id="c23f4e8aad7e2235e0ebdbc3c9d2bf9b602d6e3d" translate="yes" xml:space="preserve">
          <source>Hash function g(p,x) for a tree is an array of 32 randomly generated float arrays with the same dimension as the data set. This array is stored in GaussianRandomProjectionHash object and can be obtained from &lt;code&gt;components_&lt;/code&gt; attribute.</source>
          <target state="translated">ツリーのハッシュ関数g（p、x）は、データセットと同じ次元を持つ32個のランダムに生成されたfloat配列の配列です。この配列はGaussianRandomProjectionHashオブジェクトに格納され、 &lt;code&gt;components_&lt;/code&gt; 属性から取得できます。</target>
        </trans-unit>
        <trans-unit id="8b5d87d4a16c0b826cb8988befd77a0e52c765c1" translate="yes" xml:space="preserve">
          <source>Hashing feature transformation using Totally Random Trees</source>
          <target state="translated">完全ランダム木を用いた特徴量のハッシュ化変換</target>
        </trans-unit>
        <trans-unit id="717a562588a8bf4bd25fb65069c4d3192c7a16dc" translate="yes" xml:space="preserve">
          <source>HashingVectorizer does not provide IDF weighting as this is a stateless model (the fit method does nothing). When IDF weighting is needed it can be added by pipelining its output to a TfidfTransformer instance.</source>
          <target state="translated">HashingVectorizerはステートレスモデルなので、IDF重み付けを提供しません(フィットメソッドは何もしません)。IDF重み付けが必要な場合は、その出力をTfidfTransformerインスタンスにパイプライニングすることで追加することができます。</target>
        </trans-unit>
        <trans-unit id="d06cc92706967f16b8b9c95848cf5aff7ec1c456" translate="yes" xml:space="preserve">
          <source>HashingVectorizer hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space.</source>
          <target state="translated">HashingVectorizerは,単語の出現を固定次元空間にハッシュします.これは,高次元空間でk-meansが動作するために重要であると考えられています.</target>
        </trans-unit>
        <trans-unit id="28041ffc119d6685560d28cedcd34e917cd495e5" translate="yes" xml:space="preserve">
          <source>Hastie, R. Tibshirani and J. Friedman, &amp;ldquo;Elements of Statistical Learning Ed. 2&amp;rdquo;, Springer, 2009.</source>
          <target state="translated">Hastie、R。TibshiraniおよびJ. Friedman、「Elements of Statistical Learning Ed。2インチ、Springer、2009年。</target>
        </trans-unit>
        <trans-unit id="b8dd0d155e19e8a71f19b1bbe40cdccabf151805" translate="yes" xml:space="preserve">
          <source>Have a look at the &lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;Hashing Vectorizer&lt;/a&gt; as a memory efficient alternative to &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">見てい&lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;ハッシングベクトラ&lt;/a&gt;のメモリ効率的な代替として&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; を&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a899619755f5d06da20b9b2964b88739a1ab106e" translate="yes" xml:space="preserve">
          <source>Have a look at using &lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core Classification&lt;/a&gt; to learn from data that would not fit into the computer main memory.</source>
          <target state="translated">&lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core Classification&lt;/a&gt;を使用して、コンピューターのメインメモリに収まらないデータから学習する方法をご覧ください。</target>
        </trans-unit>
        <trans-unit id="27a688f240efc4c1f8111e73298dc1d5dd7e9964" translate="yes" xml:space="preserve">
          <source>HaversineDistance</source>
          <target state="translated">HaversineDistance</target>
        </trans-unit>
        <trans-unit id="260c7f8bcac0cff0858b268328a3c57270e6d05b" translate="yes" xml:space="preserve">
          <source>He, Kaiming, et al. &amp;ldquo;Delving deep into rectifiers: Surpassing human-level</source>
          <target state="translated">彼、Kaiming、他。「整流器の詳細：人間レベルを超える</target>
        </trans-unit>
        <trans-unit id="2f8a00b4f7c2990e23253c9271642cb45a1f2224" translate="yes" xml:space="preserve">
          <source>Helper class for readable parallel mapping.</source>
          <target state="translated">読み取り可能な並列マッピングのためのヘルパークラスです。</target>
        </trans-unit>
        <trans-unit id="15e3ecfce92d858c5fac5d21e2153dba45c36e72" translate="yes" xml:space="preserve">
          <source>Helper function to test the message raised in an exception.</source>
          <target state="translated">例外で発生したメッセージをテストするためのヘルパー関数です。</target>
        </trans-unit>
        <trans-unit id="e22b8152bb5ec7ad5480951d5d1692b1809abba4" translate="yes" xml:space="preserve">
          <source>Hence using random projections on the digits dataset which only has 64 features in the input space does not make sense: it does not allow for dimensionality reduction in this case.</source>
          <target state="translated">したがって,入力空間に64個の特徴しかない桁のデータセットにランダム投影を使っても意味がありません.</target>
        </trans-unit>
        <trans-unit id="858c4ba42a503184b8af0061cb8145e1add6548c" translate="yes" xml:space="preserve">
          <source>Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:</source>
          <target state="translated">そのため、学習コーパスで見られなかった単語は、今後の変換メソッドの呼び出しでは完全に無視されます。</target>
        </trans-unit>
        <trans-unit id="3fea43b2d3bbf05cef0fdbdec4ca7b01a2de9eb5" translate="yes" xml:space="preserve">
          <source>Hence, the None case results in:</source>
          <target state="translated">したがって、Noneの場合は、結果として</target>
        </trans-unit>
        <trans-unit id="4fffc6a6ec537bad9c19e154df4fbf4c1dee1839" translate="yes" xml:space="preserve">
          <source>Here &lt;code&gt;func&lt;/code&gt; is a function which takes two one-dimensional numpy arrays, and returns a distance. Note that in order to be used within the BallTree, the distance must be a true metric: i.e. it must satisfy the following properties</source>
          <target state="translated">ここで &lt;code&gt;func&lt;/code&gt; は、2つの1次元のnumpy配列を取り、距離を返す関数です。BallTree内で使用するには、距離が真のメトリックでなければならないことに注意してください。つまり、次のプロパティを満たしている必要があります。</target>
        </trans-unit>
        <trans-unit id="8918252717f29fe05952e0490941948a7c1afcd2" translate="yes" xml:space="preserve">
          <source>Here a sine function is fit with a polynomial of order 3, for values close to zero.</source>
          <target state="translated">ここでは、ゼロに近い値に対して、次数3の多項式で正弦関数がフィットします。</target>
        </trans-unit>
        <trans-unit id="dd2684d229285b3b1a04454d9cd068cd1e054408" translate="yes" xml:space="preserve">
          <source>Here a small example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a binary class problem:</source>
          <target state="translated">以下は、バイナリクラスの問題でsvm分類&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;使用したHinge_loss関数の使用を示す小さな例です。</target>
        </trans-unit>
        <trans-unit id="5113795d86cf9b1916006aecdb0ceee73192da33" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the Gaussian random projection transformer:</source>
          <target state="translated">ここでは、ガウスランダム射影変換器の使い方を説明しています。</target>
        </trans-unit>
        <trans-unit id="d838251a264bfc0a86e50e7c2f5d9ef6f54d10aa" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the sparse random projection transformer:</source>
          <target state="translated">ここでは,疎なランダム射影変換器の使い方を説明しています.</target>
        </trans-unit>
        <trans-unit id="32f51b9dd909238771016da8eae995fa183bb752" translate="yes" xml:space="preserve">
          <source>Here are a few suggestions to help further your scikit-learn intuition upon the completion of this tutorial:</source>
          <target state="translated">このチュートリアルが終了したときに、あなたの scikit-learn の直観力をさらに高めるためのいくつかの提案がここにあります。</target>
        </trans-unit>
        <trans-unit id="8d2ed57227d29b030e11d07a6ad14619156d6baa" translate="yes" xml:space="preserve">
          <source>Here are some recommended ways to load standard columnar data into a format usable by scikit-learn:</source>
          <target state="translated">ここでは、標準的な柱状データを scikit-learn で使用可能な形式にロードするための推奨される方法をいくつか紹介します。</target>
        </trans-unit>
        <trans-unit id="6caa2e3f5fa319efda163f3ada59f70b9af4251d" translate="yes" xml:space="preserve">
          <source>Here are some small examples in binary classification:</source>
          <target state="translated">ここでは、二値分類の小さな例を紹介します。</target>
        </trans-unit>
        <trans-unit id="cad58f968788a0c8b830200526f46c2e8380af6d" translate="yes" xml:space="preserve">
          <source>Here is a list of incremental estimators for different tasks:</source>
          <target state="translated">ここでは、さまざまなタスクのインクリメンタル・エスティメー タのリストを示します。</target>
        </trans-unit>
        <trans-unit id="e5cc3ef05cd44a377ff0113c5a0144a6cd05b3f4" translate="yes" xml:space="preserve">
          <source>Here is a sample output of a run on a quad-core machine:</source>
          <target state="translated">クアッドコアマシンで実行した場合の出力サンプルです。</target>
        </trans-unit>
        <trans-unit id="00dac27806e77f637d738445566eb627365e0881" translate="yes" xml:space="preserve">
          <source>Here is a sketch of a system designed to achieve this goal:</source>
          <target state="translated">この目標を達成するために設計されたシステムのスケッチです。</target>
        </trans-unit>
        <trans-unit id="c595619fa24f58ee3930e8429960f874f9b329e7" translate="yes" xml:space="preserve">
          <source>Here is a small example illustrating the usage of the &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">以下は、&lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; &lt;/a&gt;関数の使用法を示す小さな例です。</target>
        </trans-unit>
        <trans-unit id="423aaa3f8753fc630af578bc1fbb46728b5a06e5" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">次に、&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt;関数の使用例をいくつか示します。</target>
        </trans-unit>
        <trans-unit id="cb41f576a02130e8636700bae5b58c941781076b" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">以下は、&lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt;関数の使用例です。</target>
        </trans-unit>
        <trans-unit id="7c0ba7d72bd4599fa8b6676ec86f846e3705f7da" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">以下は、&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt;関数の使用例です。</target>
        </trans-unit>
        <trans-unit id="0be0450f469be9534c036908ab2afdbd59b24548" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">以下は、&lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; &lt;/a&gt;関数の使用例です。</target>
        </trans-unit>
        <trans-unit id="f8da86e09b21d704ee9aa6f7fcb4b0cf6258a18d" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">次に、&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; &lt;/a&gt;関数の使用例をいくつか示します。</target>
        </trans-unit>
        <trans-unit id="e0060b4a19332fa9cdf176d47debc4e3de22af1f" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">次に、&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;関数の使用例をいくつか示します。</target>
        </trans-unit>
        <trans-unit id="2121874e07dc9ac1fb205417370f94e728d5e5e6" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function:</source>
          <target state="translated">この関数の使用例を少しだけご紹介します。</target>
        </trans-unit>
        <trans-unit id="f03ea6f9a5b7db0b84376e166dbfe9d87d690fa9" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function::</source>
          <target state="translated">この関数の使用例を以下に示します。</target>
        </trans-unit>
        <trans-unit id="da9291cb119f102218681b72119ede84a1e93115" translate="yes" xml:space="preserve">
          <source>Here is a usage example:</source>
          <target state="translated">使用例をご紹介します。</target>
        </trans-unit>
        <trans-unit id="ec46f6fe41e667dcb81fcf9a89e2aaf0a6763af5" translate="yes" xml:space="preserve">
          <source>Here is a visual representation of such a confusion matrix (this figure comes from the &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;Confusion matrix&lt;/a&gt; example):</source>
          <target state="translated">以下は、このような混同行列を視覚的に表したものです（この図は、&lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;混同行列の&lt;/a&gt;例からのものです）。</target>
        </trans-unit>
        <trans-unit id="876abdb2188ee5022ae84c77928e2082f05a478c" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior.</source>
          <target state="translated">クロスバリデーションの動作を可視化したものです。</target>
        </trans-unit>
        <trans-unit id="d5a8fd11bd11ae3f4eb764b39ba1acfee92579af" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">これは、相互検証動作の視覚化です。&lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;はクラスやグループの影響を受けないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="e10cd61d7e44ad9e6bb0d4cec30745248d4c4e93" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">これは、相互検証動作の視覚化です。&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; &lt;/a&gt;はクラスやグループの影響を受けないことに注意してください。</target>
        </trans-unit>
        <trans-unit id="0b166c480658b240c273df0a43ce9ffa8405561c" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a multiclass problem:</source>
          <target state="translated">以下は、マルチクラス問題でのsvm分類器での&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;関数の使用を示す例です。</target>
        </trans-unit>
        <trans-unit id="58bc8e3595ba3624485387b6def524d2d31bae65" translate="yes" xml:space="preserve">
          <source>Here is an example of &lt;code&gt;cross_validate&lt;/code&gt; using a single metric:</source>
          <target state="translated">以下は、単一のメトリックを使用した &lt;code&gt;cross_validate&lt;/code&gt; の例です。</target>
        </trans-unit>
        <trans-unit id="f7e50cdf4078c7823c206e72ce0bf5486f1e2a9f" translate="yes" xml:space="preserve">
          <source>Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:</source>
          <target state="translated">次数の異なる多項式特徴量を使用して、このアイデアを一次元データに適用した例を紹介します。</target>
        </trans-unit>
        <trans-unit id="8375acd14d3c16b75f14ad4cf9799bf09154cba1" translate="yes" xml:space="preserve">
          <source>Here is an example of building custom scorers, and of using the &lt;code&gt;greater_is_better&lt;/code&gt; parameter:</source>
          <target state="translated">カスタムスコアラーを作成し、 &lt;code&gt;greater_is_better&lt;/code&gt; パラメーターを使用する例を次に示します。</target>
        </trans-unit>
        <trans-unit id="120ffb4ca9b2da814644df8eb634b8cef584b2a0" translate="yes" xml:space="preserve">
          <source>Here is an example to scale a toy data matrix to the &lt;code&gt;[0, 1]&lt;/code&gt; range:</source>
          <target state="translated">おもちゃのデータマトリックスを &lt;code&gt;[0, 1]&lt;/code&gt; 範囲にスケーリングする例を次に示します。</target>
        </trans-unit>
        <trans-unit id="8f89ae42a83e786b17cd6f1b83024799754e5687" translate="yes" xml:space="preserve">
          <source>Here is an example using &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; with the &lt;code&gt;elasticnet&lt;/code&gt; penalty. The regularization strength is globally controlled by the &lt;code&gt;alpha&lt;/code&gt; parameter. With a sufficiently high &lt;code&gt;alpha&lt;/code&gt;, one can then increase the &lt;code&gt;l1_ratio&lt;/code&gt; parameter of &lt;code&gt;elasticnet&lt;/code&gt; to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.</source>
          <target state="translated">以下は、 &lt;code&gt;elasticnet&lt;/code&gt; ペナルティで &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; を使用する例です。正則化の強度は、 &lt;code&gt;alpha&lt;/code&gt; パラメーターによってグローバルに制御されます。 &lt;code&gt;alpha&lt;/code&gt; が十分に高い場合、 &lt;code&gt;l1_ratio&lt;/code&gt; パラメータを &lt;code&gt;elasticnet&lt;/code&gt; て、モデル係数にさまざまなレベルのスパース性を適用できます。ここでスパース性が高いと、モデルを完全に記述するために必要な係数が少なくなるため、モデルの複雑さが少なくなると解釈されます。もちろん、スパースドット積は非ゼロ係数の数にほぼ比例して時間がかかるため、スパース性は予測時間に影響します。</target>
        </trans-unit>
        <trans-unit id="540ee2aaf7182c6dfc449b18e5accb694e3b0894" translate="yes" xml:space="preserve">
          <source>Here is an example:</source>
          <target state="translated">ここでは一例を紹介します。</target>
        </trans-unit>
        <trans-unit id="a1ce1cc95adf7777aaf8483ebc72e46f7e0c5dd5" translate="yes" xml:space="preserve">
          <source>Here is how to use the toy data from the previous example with this scaler:</source>
          <target state="translated">先ほどのおもちゃのデータをこのスケーラーで使う方法をご紹介します。</target>
        </trans-unit>
        <trans-unit id="da00252cb105e8e07c4719d8131543c2597c6b64" translate="yes" xml:space="preserve">
          <source>Here is sample code that illustrates the use of the &lt;code&gt;sparsify()&lt;/code&gt; method:</source>
          <target state="translated">&lt;code&gt;sparsify()&lt;/code&gt; メソッドの使用法を示すサンプルコードを次に示します。</target>
        </trans-unit>
        <trans-unit id="b1b76d97b9ed98e3661e06b53d247e6f552362c3" translate="yes" xml:space="preserve">
          <source>Here is sample code to test the sparsity of your input:</source>
          <target state="translated">ここでは、入力の分散性をテストするためのサンプルコードを示します。</target>
        </trans-unit>
        <trans-unit id="7a4f1fdf399f62578619e41a5fba4a345597683a" translate="yes" xml:space="preserve">
          <source>Here is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:</source>
          <target state="translated">ここでは、自動モデル選択のための赤池情報基準(AIC)またはベイズ情報基準(BIC)の恩恵を受けるモデルの一覧です。</target>
        </trans-unit>
        <trans-unit id="24d46233c5b1cf5947d798926d1e317b272fc656" translate="yes" xml:space="preserve">
          <source>Here is the list of such models:</source>
          <target state="translated">そんな機種の一覧です。</target>
        </trans-unit>
        <trans-unit id="8029b08717fd12d596415c9951c99ad442611512" translate="yes" xml:space="preserve">
          <source>Here the computation is achieved thanks to Martinsson&amp;rsquo;s Randomized SVD algorithm implemented in scikit-learn.</source>
          <target state="translated">ここでは、scikit-learnに実装されたMartinssonのランダム化SVDアルゴリズムのおかげで計算が行われます。</target>
        </trans-unit>
        <trans-unit id="baa6fd34087f3f3b80a068e5198c152eb2224084" translate="yes" xml:space="preserve">
          <source>Here the results are not as good as they could be as our choice for the regularization parameter C was not the best. In real life applications this parameter is usually chosen using &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">ここでは、正則化パラメーターCの選択が最良ではなかったので、結果は良い結果にはなりません。実際のアプリケーションでは、このパラメーターは通常&lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;、推定器のハイパーパラメーターの調整&lt;/a&gt;を使用して選択されます。</target>
        </trans-unit>
        <trans-unit id="e33a1b9fd8981a72cf8c17c638c489933e2535f4" translate="yes" xml:space="preserve">
          <source>Here we choose the SAGA solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.</source>
          <target state="translated">ここでは、SAGAソルバーを選択しました。これは、ロジスティック回帰の損失に対して、非平滑でスパースシティを誘発するl1ペナルティで効率的に最適化できるからです。</target>
        </trans-unit>
        <trans-unit id="d4668460d1dfffc9a12dd3f0ca645c8016c22599" translate="yes" xml:space="preserve">
          <source>Here we compare 3 approaches:</source>
          <target state="translated">ここでは3つのアプローチを比較します。</target>
        </trans-unit>
        <trans-unit id="3aadfee7aa5bef8aeacf179790398b01b017dc93" translate="yes" xml:space="preserve">
          <source>Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on &lt;em&gt;clusterings with an infinite, unbounded, number of partitions&lt;/em&gt;. Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model.</source>
          <target state="translated">ここでは、ディリクレプロセス混合の変分推論アルゴリズムについて説明します。ディリクレプロセスは&lt;em&gt;、無限で無限の数のパーティションを持つクラスタリングの&lt;/em&gt;事前確率分布です。変分法を使用すると、有限ガウス混合モデルと比較して、推論時間のペナルティがほとんどなく、ガウス混合モデルにこの以前の構造を組み込むことができます。</target>
        </trans-unit>
        <trans-unit id="19eba1946aa4b2b6c504a0a7a0b9c334a19205ae" translate="yes" xml:space="preserve">
          <source>Here we fit a multinomial logistic regression with L1 penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the l1-penalty. Test accuracy reaches &amp;gt; 0.8, while weight vectors remains &lt;em&gt;sparse&lt;/em&gt; and therefore more easily &lt;em&gt;interpretable&lt;/em&gt;.</source>
          <target state="translated">ここでは、MNISTディジット分類タスクのサブセットにL1ペナルティを伴う多項ロジスティック回帰を当てはめます。この目的のためにSAGAアルゴリズムを使用します。これは、サンプルの数が特徴の数よりも大幅に多い場合に高速で、l1-penaltyの場合のように滑らかでない目的関数を細かく最適化できるソルバーです。テストの精度は0.8を超えますが、重みベクトルは&lt;em&gt;スパースな&lt;/em&gt;まま&lt;em&gt;な&lt;/em&gt;ので、より簡単に&lt;em&gt;解釈でき&lt;/em&gt;ます。</target>
        </trans-unit>
        <trans-unit id="bc0bdb5bd44175832d7fcee805ba26710508e043" translate="yes" xml:space="preserve">
          <source>Here we have used &lt;code&gt;kernel='gaussian'&lt;/code&gt;, as seen above. Mathematically, a kernel is a positive function \(K(x;h)\) which is controlled by the bandwidth parameter \(h\). Given this kernel form, the density estimate at a point \(y\) within a group of points \(x_i; i=1\cdots N\) is given by:</source>
          <target state="translated">ここでは、上記のように &lt;code&gt;kernel='gaussian'&lt;/code&gt; を使用しています。数学的には、カーネルは帯域幅パラメーター\（h \）によって制御される正の関数\（K（x; h）\）です。このカーネル形式が与えられると、点のグループ\（x_i; i = 1 \ cdots N \）内の点\（y \）での密度推定は、次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="9bc4f467db4b070f940a4ae98fc40a9d9951075c" translate="yes" xml:space="preserve">
          <source>Here we simulate independent sources using a highly non-Gaussian process, 2 student T with a low number of degrees of freedom (top left figure). We mix them to create observations (top right figure). In this raw observation space, directions identified by PCA are represented by orange vectors. We represent the signal in the PCA space, after whitening by the variance corresponding to the PCA vectors (lower left). Running ICA corresponds to finding a rotation in this space to identify the directions of largest non-Gaussianity (lower right).</source>
          <target state="translated">ここでは、高度に非ガウス過程を用いて独立したソースをシミュレーションしています。それらを混合してオブザベーションを作成します(右上図)。この生の観測空間では、PCAで同定された方向がオレンジ色のベクトルで表現されています。PCA空間では、PCAベクトルに対応する分散で白化した後の信号を表現します(左下)。ICAを実行することは、最大の非ガウス性の方向を特定するために、この空間で回転を見つけることに相当します(右下)。</target>
        </trans-unit>
        <trans-unit id="4efad2f65eb490631f07741acecbb3c6dbc45f0c" translate="yes" xml:space="preserve">
          <source>Here we use the l1 sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing l2 penalty instead.</source>
          <target state="translated">ここでは、情報的でない特徴の重みをゼロにするl1 sparsityを使用しています。これは,各クラスの識別力の強い語彙を抽出することが目的であれば,良い方法です.最高の予測精度を得ることが目的であれば,代わりに非スパース度を誘発するl2ペナルティを使う方が良いでしょう.</target>
        </trans-unit>
        <trans-unit id="0260adbe3be54cb933a36e08a92f87d76459f0fc" translate="yes" xml:space="preserve">
          <source>Here, \(\alpha \geq 0\) is a complexity parameter that controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="translated">ここでは、縮み量を制御する複雑度パラメータである\(\(Alpha)の値が大きいほど、縮み量が大きくなり、係数がより強固なコリニアリティになる。</target>
        </trans-unit>
        <trans-unit id="9412689bc5806775f9bf0419d0db25d5c39e9741" translate="yes" xml:space="preserve">
          <source>Here, the classifier is &lt;code&gt;fit()&lt;/code&gt; on a 2d binary label representation of &lt;code&gt;y&lt;/code&gt;, using the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt;&lt;code&gt;LabelBinarizer&lt;/code&gt;&lt;/a&gt;. In this case &lt;code&gt;predict()&lt;/code&gt; returns a 2d array representing the corresponding multilabel predictions.</source>
          <target state="translated">ここでは、分類子は&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt; &lt;code&gt;LabelBinarizer&lt;/code&gt; &lt;/a&gt;を使用して、 &lt;code&gt;y&lt;/code&gt; の 2Dバイナリラベル表現の &lt;code&gt;fit()&lt;/code&gt; です。この場合、 &lt;code&gt;predict()&lt;/code&gt; は、対応するマルチラベル予測を表す2d配列を返します。</target>
        </trans-unit>
        <trans-unit id="e42ec4b790491f01a91defa6334fdc833c4f6019" translate="yes" xml:space="preserve">
          <source>Here, the default kernel &lt;code&gt;rbf&lt;/code&gt; is first changed to &lt;code&gt;linear&lt;/code&gt; via &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt;&lt;code&gt;SVC.set_params()&lt;/code&gt;&lt;/a&gt; after the estimator has been constructed, and changed back to &lt;code&gt;rbf&lt;/code&gt; to refit the estimator and to make a second prediction.</source>
          <target state="translated">ここで、デフォルトのカーネル &lt;code&gt;rbf&lt;/code&gt; は、推定器が構築された後、&lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt; &lt;code&gt;SVC.set_params()&lt;/code&gt; &lt;/a&gt;を介して最初に &lt;code&gt;linear&lt;/code&gt; 変更され、推定器を再 &lt;code&gt;rbf&lt;/code&gt; して2番目の予測を行うためにrbfに戻されます。</target>
        </trans-unit>
        <trans-unit id="379b23b33ba6458bba2f568a4c2b136ad7c827a5" translate="yes" xml:space="preserve">
          <source>Here, the first &lt;code&gt;predict()&lt;/code&gt; returns an integer array, since &lt;code&gt;iris.target&lt;/code&gt; (an integer array) was used in &lt;code&gt;fit&lt;/code&gt;. The second &lt;code&gt;predict()&lt;/code&gt; returns a string array, since &lt;code&gt;iris.target_names&lt;/code&gt; was for fitting.</source>
          <target state="translated">ここで、最初の &lt;code&gt;iris.target&lt;/code&gt; &lt;code&gt;predict()&lt;/code&gt; は整数配列を返します。これは、iris.target（整数配列）が &lt;code&gt;fit&lt;/code&gt; で使用されたためです。2番目の &lt;code&gt;iris.target_names&lt;/code&gt; &lt;code&gt;predict()&lt;/code&gt; は、iris.target_namesがフィッティング用であったため、文字列配列を返します。</target>
        </trans-unit>
        <trans-unit id="21a97ae1e0557499a4ed4420c47199de8f6f0cde" translate="yes" xml:space="preserve">
          <source>Here, the number of samples is slightly larger than the number of dimensions, thus the empirical covariance is still invertible. However, as the observations are strongly correlated, the empirical covariance matrix is ill-conditioned and as a result its inverse &amp;ndash;the empirical precision matrix&amp;ndash; is very far from the ground truth.</source>
          <target state="translated">ここで、サンプルの数は次元の数よりもわずかに多いため、経験的共分散は依然として可逆です。ただし、観測値は強く相関しているため、経験的共分散行列は悪条件であり、その結果、その逆数-経験的精度行列-は、グラウンドトゥルースから非常に離れています。</target>
        </trans-unit>
        <trans-unit id="2c6a31e993187ebfe932ff15824a46e0c83fd078" translate="yes" xml:space="preserve">
          <source>Here, the predicted class label is 2, since it has the highest average probability.</source>
          <target state="translated">ここで、予測されたクラスラベルは、最も平均的な確率が高いので、2である。</target>
        </trans-unit>
        <trans-unit id="264995c0dc7ac7309d4709ed0ce1258e4439b015" translate="yes" xml:space="preserve">
          <source>Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, &lt;code&gt;sklearn&lt;/code&gt; implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, with the keyword &lt;code&gt;method = 'hessian'&lt;/code&gt;. It requires &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt;.</source>
          <target state="translated">Hessian Eigenmapping（別名Hessian-based LLE：HLLE）は、LLEの正則化問題を解決するもう1つの方法です。局所線形構造を回復するために使用される、各近傍でのヘッセ行列に基づく2次形式を中心に展開します。他の実装では、データサイズによるスケーリングが不十分であることに注意していますが、 &lt;code&gt;sklearn&lt;/code&gt; はアルゴリズムの改善を実装しているため、出力コストが小さい他のLLEバリアントに匹敵します。 HLLEは、ローカルで関数&lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt;または対応するオブジェクト指向の&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt;で、キーワード &lt;code&gt;method = 'hessian'&lt;/code&gt; ます。それは必要と &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="afac02e66e409c4004e2cc2adafb5b5e842109eb" translate="yes" xml:space="preserve">
          <source>Hierarchical agglomerative clustering: Ward</source>
          <target state="translated">階層的凝集クラスタリング.ウォード</target>
        </trans-unit>
        <trans-unit id="09f7d65b121068e93f6d1d655d20b242aded6b7b" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia page&lt;/a&gt; for more details.</source>
          <target state="translated">階層的クラスタリングは、それらを連続的にマージまたは分割することによりネストされたクラスタを構築するクラスタリングアルゴリズムの一般的なファミリです。このクラスターの階層は、ツリー（または樹状図）として表されます。ツリーのルートは、すべてのサンプルを収集する一意のクラスターであり、葉は1つのサンプルのみを持つクラスターです。詳細については、&lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipediaのページ&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="dbe40063cd6e20f1519715e45afa5ca7ed6442de" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering: structured vs unstructured ward</source>
          <target state="translated">階層的クラスタリング:構造化された区と構造化されていない区の比較</target>
        </trans-unit>
        <trans-unit id="645ba4388b8ba9172558b321f188082e4d2fd9ef" translate="yes" xml:space="preserve">
          <source>High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.</source>
          <target state="translated">高次元のデータセットを可視化するのは非常に困難です。2次元や3次元のデータは,データの固有の構造を示すためにプロットすることができるが,同等の高次元のプロットは直感的ではない.データセットの構造の可視化を助けるためには,次元を何らかの方法で小さくしなければならない.</target>
        </trans-unit>
        <trans-unit id="75c18021e736bcb99a099c597122a642054caa8c" translate="yes" xml:space="preserve">
          <source>Hinge: (soft-margin) Support Vector Machines.</source>
          <target state="translated">ヒンジ。(ソフトマージン)サポートベクターマシン。</target>
        </trans-unit>
        <trans-unit id="a319ae13863bb8d6da087a8b6e0305de9278e27f" translate="yes" xml:space="preserve">
          <source>Hinton, Geoffrey E.</source>
          <target state="translated">ヒントン、ジェフリーE.</target>
        </trans-unit>
        <trans-unit id="b8cd925555526484cde7ba65174f600e33250f6b" translate="yes" xml:space="preserve">
          <source>Hochreiter, Bodenhofer, et. al., 2010. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;FABIA: factor analysis for bicluster acquisition&lt;/a&gt;.</source>
          <target state="translated">Hochreiter、Bodenhoferなど al。、&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;2010。FABIA：バイクラスター獲得の因子分析&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7a5c3ac7ac58dcde82acc59c23c0dce0d31966d1" translate="yes" xml:space="preserve">
          <source>Holds the label for each class.</source>
          <target state="translated">各クラスのラベルを保持します。</target>
        </trans-unit>
        <trans-unit id="4b7dceb5fe5f7e92199815a2b66fef8fdf05dc27" translate="yes" xml:space="preserve">
          <source>Homogeneity and completeness scores are formally given by:</source>
          <target state="translated">均質性と完全性のスコアは正式には次のように与えられます。</target>
        </trans-unit>
        <trans-unit id="7c13e77bc830b382c041eec0e3fcc1723f375e05" translate="yes" xml:space="preserve">
          <source>Homogeneity metric of a cluster labeling given a ground truth.</source>
          <target state="translated">基底真理を与えられたクラスタラベリングの同質性メトリック。</target>
        </trans-unit>
        <trans-unit id="3afc9b67230d985e8782525c7b58b615e013e8f9" translate="yes" xml:space="preserve">
          <source>Homogeneity, completeness and V-measure can be computed at once using &lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt;&lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt;&lt;/a&gt; as follows:</source>
          <target state="translated">均質性、完全性、Vメジャーは、次のように&lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt; &lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt; &lt;/a&gt;を使用して一度に計算できます。</target>
        </trans-unit>
        <trans-unit id="0878824f511837fc1a1c8d27240af19053ebdbd4" translate="yes" xml:space="preserve">
          <source>HouseAge median house age in block</source>
          <target state="translated">HouseAge ブロック内の住宅築年数の中央値</target>
        </trans-unit>
        <trans-unit id="be45c283b4c54643c38f84bc65a4bfc525d6d30a" translate="yes" xml:space="preserve">
          <source>How often to evaluate perplexity. Only used in &lt;code&gt;fit&lt;/code&gt; method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.</source>
          <target state="translated">混乱を評価する頻度。 &lt;code&gt;fit&lt;/code&gt; 法でのみ使用されます。それを0または負の数に設定すると、トレーニングの混乱をまったく評価しません。混乱を評価することは、トレーニングプロセスの収束をチェックするのに役立ちますが、トレーニングの合計時間も増加します。すべての反復で混乱を評価すると、トレーニング時間が最大2倍になる可能性があります。</target>
        </trans-unit>
        <trans-unit id="060faa287065b4ad6ba6c00f598635b42adc21de" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">分母でノーマライザを計算する方法。可能なオプションは、「min」、「geometric」、「arithmetic」、および「max」です。「警告」の場合、「ジオメトリック」が使用されます。バージョン0.22では、デフォルトが「算術」に変更されます。</target>
        </trans-unit>
        <trans-unit id="b8efa217d0db9ce56ac60653645beebe151304f9" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;max&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">分母でノーマライザを計算する方法。可能なオプションは、「min」、「geometric」、「arithmetic」、および「max」です。「警告」の場合、「最大」が使用されます。バージョン0.22では、デフォルトが「算術」に変更されます。</target>
        </trans-unit>
        <trans-unit id="cf894bb3f8fceedab10cdd5da9a5edd37e00865d" translate="yes" xml:space="preserve">
          <source>How to construct the affinity matrix.</source>
          <target state="translated">アフィニティーマトリックスの構築方法</target>
        </trans-unit>
        <trans-unit id="d34268ba2716d71aaaeee2c35e527ca55a46dd2f" translate="yes" xml:space="preserve">
          <source>However ARI can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO).</source>
          <target state="translated">しかし、ARI は純粋に教師なしの設定では、クラスタリングモデルの選択に使用できるコンセンサス指数の構成要素としても有用である(TODO)。</target>
        </trans-unit>
        <trans-unit id="0121c2b22395d1a9db3fd77f24d0a9f0e41170e3" translate="yes" xml:space="preserve">
          <source>However MI-based measures can also be useful in purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection.</source>
          <target state="translated">しかし,MI に基づく測定値は,クラスタリングモデルの選択に用いることができるコンセンサス指数の構成要素として,純粋に教師なしの設定でも有用である.</target>
        </trans-unit>
        <trans-unit id="117b6230e0e8ab3fcdc1b277507becef8a05f759" translate="yes" xml:space="preserve">
          <source>However care must taken to always make the affinity matrix symmetric so that the eigenvector decomposition works as expected.</source>
          <target state="translated">しかし、固有ベクトル分解が期待通りに動作するように、親和行列を常に対称にするように注意しなければなりません。</target>
        </trans-unit>
        <trans-unit id="925f5b77eb2888a89c04118c35bff0f0ace7255e" translate="yes" xml:space="preserve">
          <source>However the RI score does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples).</source>
          <target state="translated">しかし、RIスコアは、ラベルのランダムな割り当てがゼロに近い値を得ることを保証するものではありません(特に、クラスターの数がサンプル数と同じ桁数である場合)。</target>
        </trans-unit>
        <trans-unit id="11d179b15971b8eaae6270be9fce57c0bd0d2416" translate="yes" xml:space="preserve">
          <source>However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.</source>
          <target state="translated">しかし、利用可能なデータを3つのセットに分割することで、モデルの学習に使用できるサンプルの数を大幅に減らし、結果は(訓練、検証)セットのペアのための特定のランダムな選択に依存することができます。</target>
        </trans-unit>
        <trans-unit id="56c680421b5fb07e56baa9a65f13a80fce385b54" translate="yes" xml:space="preserve">
          <source>However, coefficient estimates for Ordinary Least Squares rely on the independence of the model terms. When terms are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed response, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="translated">ただし、通常最小二乗法の係数推定は、モデル項の独立性に依存しています。項が相関していて、計画行列\（X \）の列に近似線形依存がある場合、計画行列は特異に近くなり、その結果、最小二乗推定は観測された応答のランダム誤差に非常に敏感になります。大きな分散を生成します。この&lt;em&gt;多重共線性の&lt;/em&gt;状況は、たとえば、実験計画なしでデータが収集された場合に発生する可能&lt;em&gt;性&lt;/em&gt;があります。</target>
        </trans-unit>
        <trans-unit id="bf734282463bfc3a9cb343729f546342ec401691" translate="yes" xml:space="preserve">
          <source>However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.</source>
          <target state="translated">しかし、問題の訓練サイズに対して学習曲線が急峻な場合、5倍または10倍のクロスバリデーションは一般化誤差を過大評価する可能性があります。</target>
        </trans-unit>
        <trans-unit id="fc162d85afa0b20b4064f40b16eb0e55ca89c629" translate="yes" xml:space="preserve">
          <source>However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.</source>
          <target state="translated">しかし、あるハイパーパラメータ値について、推定器がオーバーフィットしているか、あるいはアンダーフィットしているかを調べるために、1つのハイパーパラメータのトレーニングスコアとバリデーションスコアへの影響をプロットすると便利な場合があります。</target>
        </trans-unit>
        <trans-unit id="5c8b24673bb3f660f66f90035a856044be6d6f9e" translate="yes" xml:space="preserve">
          <source>However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. If categorical features are represented as numeric values such as int, the DictVectorizer can be followed by &lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; to complete binary one-hot encoding.</source>
          <target state="translated">ただし、このトランスフォーマーは、機能の値が文字列型である場合にのみ、バイナリワンホットエンコーディングを実行することに注意してください。カテゴリー特徴がintなどの数値として表される場合、DictVectorizerの後に&lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt; &lt;/a&gt;を続けて、バイナリワンホットエンコーディングを完了することができます。</target>
        </trans-unit>
        <trans-unit id="8b25d9ad009118aef0894664f601ac10786f8b49" translate="yes" xml:space="preserve">
          <source>However, this is not the most precise way of doing this computation, and the distance matrix returned by this function may not be exactly symmetric as required by, e.g., &lt;code&gt;scipy.spatial.distance&lt;/code&gt; functions.</source>
          <target state="translated">ただし、これはこの計算を行う最も正確な方法ではなく、この関数によって返される距離行列は、たとえば &lt;code&gt;scipy.spatial.distance&lt;/code&gt; 関数で必要とされるように正確に対称的ではない場合があります。</target>
        </trans-unit>
        <trans-unit id="379cfb166aa26713fe1131478e9b37a4224780ad" translate="yes" xml:space="preserve">
          <source>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</source>
          <target state="translated">Hsiang-Fu Yu,Fang-Lan Huang,Chih-Jen Lin (2011).二重座標降下</target>
        </trans-unit>
        <trans-unit id="15d1d4b26d2ba06629e368bf6c8a860d8762ef89" translate="yes" xml:space="preserve">
          <source>Huber (&lt;code&gt;'huber'&lt;/code&gt;): Another robust loss function that combines least squares and least absolute deviation; use &lt;code&gt;alpha&lt;/code&gt; to control the sensitivity with regards to outliers (see &lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt; for more details).</source>
          <target state="translated">Huber（ &lt;code&gt;'huber'&lt;/code&gt; ）：最小二乗と最小絶対偏差を組み合わせた別のロバスト損失関数。外れ値に関する感度を制御するには、 &lt;code&gt;alpha&lt;/code&gt; を使用します（詳細については、&lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt;を参照してください）。</target>
        </trans-unit>
        <trans-unit id="3d12d436101cbc3212af50bf81000f6d78d4cf01" translate="yes" xml:space="preserve">
          <source>HuberRegressor vs Ridge on dataset with strong outliers</source>
          <target state="translated">強い外れ値を持つデータセットにおけるHuberRegressorとRidgeの比較</target>
        </trans-unit>
        <trans-unit id="7e58a6e8d89e8504ad31e135de9b485ad40f05f6" translate="yes" xml:space="preserve">
          <source>Hue</source>
          <target state="translated">Hue</target>
        </trans-unit>
        <trans-unit id="c7a8b2b20a9c45f674f17cd8ef7ece305e1c36eb" translate="yes" xml:space="preserve">
          <source>Hue:</source>
          <target state="translated">Hue:</target>
        </trans-unit>
        <trans-unit id="4e99bcdee413a9c98d317e0e8e4199a2bf582f90" translate="yes" xml:space="preserve">
          <source>Hugo Chavez</source>
          <target state="translated">ヒューゴ・チャベス</target>
        </trans-unit>
        <trans-unit id="135e7e12c7ab5ea7496649e72ee134478ecf558e" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter:アルファパラメータよりも前のガンマ分布の逆スケールパラメータ(レートパラメータ).デフォルトは 1.e-6.</target>
        </trans-unit>
        <trans-unit id="761054fe5bafcad49a16f057764235860452b8da" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6</source>
          <target state="translated">Hyper-parameter:ラムダパラメータよりも前のガンマ分布の逆スケールパラメータ(レートパラメータ).デフォルトは 1.e-6</target>
        </trans-unit>
        <trans-unit id="6001ea6392d3003569381e7107254e88f75fd600" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter:ラムダパラメータよりも前のガンマ分布の逆スケールパラメータ(レートパラメータ).デフォルトは1.e-6です。</target>
        </trans-unit>
        <trans-unit id="81e171654bf22a490946ec147c219e96694497ff" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6</source>
          <target state="translated">Hyper-parameter:アルファパラメータに対するガンマ分布の事前分布の形状パラメータ.デフォルトは 1.e-6 です.</target>
        </trans-unit>
        <trans-unit id="b07af48fd68aeaacb4df041ef30bae006150c237" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter:アルファパラメータよりもガンマ分布の優先順位を表す形状パラメータ.デフォルトは 1.e-6.</target>
        </trans-unit>
        <trans-unit id="1398aea0b1e181e76b6d9d73db4040ccf06ee2f7" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter:ラムダパラメータに対するガンマ分布優先度の形状パラメータ.デフォルトは 1.e-6.</target>
        </trans-unit>
        <trans-unit id="7a5b8a439bb2492412d2944256add4dcdf337928" translate="yes" xml:space="preserve">
          <source>Hyper-parameter optimizers</source>
          <target state="translated">ハイパーパラメータオプティマイザ</target>
        </trans-unit>
        <trans-unit id="223bf115da53d3d9cdf837b624135b565596fd92" translate="yes" xml:space="preserve">
          <source>Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;kernel&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt; for Support Vector Classifier, &lt;code&gt;alpha&lt;/code&gt; for Lasso, etc.</source>
          <target state="translated">ハイパーパラメーターは、推定器内で直接学習されないパラメーターです。scikit-learnでは、それらは引数として推定クラスのコンストラクターに渡されます。典型的な例には、 &lt;code&gt;C&lt;/code&gt; 、 &lt;code&gt;kernel&lt;/code&gt; 、サポートベクトル分類子の &lt;code&gt;gamma&lt;/code&gt; 、投げ縄の &lt;code&gt;alpha&lt;/code&gt; などがあります。</target>
        </trans-unit>
        <trans-unit id="568b05951392672a52de0358537dd29fcafbe544" translate="yes" xml:space="preserve">
          <source>Hyper-parameters of an estimator can be updated after it has been constructed via the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params()&lt;/a&gt; method. Calling &lt;code&gt;fit()&lt;/code&gt; more than once will overwrite what was learned by any previous &lt;code&gt;fit()&lt;/code&gt;:</source>
          <target state="translated">推定器のハイパーパラメータは、&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params（）&lt;/a&gt;メソッドを介して構築された後に更新できます。 &lt;code&gt;fit()&lt;/code&gt; を複数回呼び出すと、以前の &lt;code&gt;fit()&lt;/code&gt; で学習した内容が上書きされます。</target>
        </trans-unit>
        <trans-unit id="1db8c072507305b4aa23189287be39423349b8f4" translate="yes" xml:space="preserve">
          <source>Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).</source>
          <target state="translated">逆変換を学習する尾根回帰のハイパーパラ メ タ (fit_inverse_transform=True の場合)。</target>
        </trans-unit>
        <trans-unit id="181eca8daf7aaeed93f61701c7eddb643dc6b36a" translate="yes" xml:space="preserve">
          <source>Hyperparameters:</source>
          <target state="translated">Hyperparameters:</target>
        </trans-unit>
        <trans-unit id="8bb86931be2a9d0449c3eec151da751cb88591f1" translate="yes" xml:space="preserve">
          <source>I. Guyon, &amp;ldquo;Design of experiments for the NIPS 2003 variable selection benchmark&amp;rdquo;, 2003.</source>
          <target state="translated">I. Guyon、「NIPS 2003変数選択ベンチマークの実験計画法」、2003年。</target>
        </trans-unit>
        <trans-unit id="a238a89365b9d0ce7f5fb26e189eb03cdc08fbe5" translate="yes" xml:space="preserve">
          <source>ICA can also be used as yet another non linear decomposition that finds components with some sparsity:</source>
          <target state="translated">ICAは、ある程度の疎さを持つ成分を見つける別の非線形分解としても使用することができます。</target>
        </trans-unit>
        <trans-unit id="fcc34dd193c826ae2f0c8b804c532252b4a25480" translate="yes" xml:space="preserve">
          <source>INDUS proportion of non-retail business acres per town</source>
          <target state="translated">INDUS 1町あたりの非小売業の面積の割合</target>
        </trans-unit>
        <trans-unit id="44a4d7b7db7815be999da6a406f4dadd2c4327c5" translate="yes" xml:space="preserve">
          <source>Identification number of each sample, as ordered in dataset.data.</source>
          <target state="translated">dataset.data.dataで順番に並べられた各サンプルの識別番号。</target>
        </trans-unit>
        <trans-unit id="02d51b4f13558cbcfc807b53522b1ffb156ad7e7" translate="yes" xml:space="preserve">
          <source>Identity: d(x, y) = 0 if and only if x == y</source>
          <target state="translated">同一性:d(x,y)=0 (ただし、x ==y の場合のみ)</target>
        </trans-unit>
        <trans-unit id="35bd2069c37f2c6a308bc5401948b247d5bcfc02" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;all&amp;rdquo;, the imputer mask will represent all features.</source>
          <target state="translated">「すべて」の場合、入力マスクはすべての機能を表します。</target>
        </trans-unit>
        <trans-unit id="84934b5d658c0a370c458ee55fa3255bb65884a6" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo; (default), the imputer mask will be of same type as input.</source>
          <target state="translated">「auto」（デフォルト）の場合、入力マスクは入力と同じタイプになります。</target>
        </trans-unit>
        <trans-unit id="7cdc1bc49e801caf1ff212e1000d88bb13d7e93e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">「auto」の場合、 &lt;code&gt;max_features=n_features&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="164a2722286c1b34bc2df80a90c75397afce3e6b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">「auto」の場合、 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="911a50d98b398312fa01572b5d7b864da542117b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt;.</source>
          <target state="translated">「auto」の場合、 &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="dca169b413a6ec050ae0928eb38f43b00b9c08e0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;constant&amp;rdquo;, then replace missing values with fill_value. Can be used with strings or numeric data.</source>
          <target state="translated">「定数」の場合、欠損値をfill_valueに置き換えます。文字列または数値データで使用できます。</target>
        </trans-unit>
        <trans-unit id="fed653e1ff76c14b62a8cb9c0f4474c620b2641e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;log2&amp;rdquo;, then &lt;code&gt;max_features=log2(n_features)&lt;/code&gt;.</source>
          <target state="translated">「log2」の場合、 &lt;code&gt;max_features=log2(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e799052bdd1932be1b28378fc91f87421f6d1065" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along each column. Can only be used with numeric data.</source>
          <target state="translated">「平均」の場合、各列の平均を使用して欠損値を置き換えます。数値データでのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="8c28cbae695709f5ae6daaba6d2035fa26d4e040" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along the axis.</source>
          <target state="translated">「平均」の場合、軸に沿った平均を使用して欠損値を置き換えます。</target>
        </trans-unit>
        <trans-unit id="d5353b7f39f25231d62cbbc36fcd604e05d2faa0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along each column. Can only be used with numeric data.</source>
          <target state="translated">「中央値」の場合、各列の中央値を使用して欠損値を置き換えます。数値データでのみ使用できます。</target>
        </trans-unit>
        <trans-unit id="2c7bf0a70af62c9d1ff80c38810d3732da415b46" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along the axis.</source>
          <target state="translated">「中央値」の場合、軸に沿った中央値を使用して欠損値を置き換えます。</target>
        </trans-unit>
        <trans-unit id="b9dfb246debbef95e2bc6e78da2b0aca54b8e768" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;missing-only&amp;rdquo; (default), the imputer mask will only represent features containing missing values during fit time.</source>
          <target state="translated">「missing-only」（デフォルト）の場合、インプターマスクは、適合時間中に欠損値を含む特徴のみを表します。</target>
        </trans-unit>
        <trans-unit id="fb9cd590a090a11e857ebbc7c5d49f19787d4a57" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along each column. Can be used with strings or numeric data.</source>
          <target state="translated">「most_frequency」の場合、各列に沿って最も頻度の高い値を使用して、missingを置き換えます。文字列または数値データで使用できます。</target>
        </trans-unit>
        <trans-unit id="a90ab2bff0e7c4a2db0c7d70bcb17fa44e1b8cb3" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along the axis.</source>
          <target state="translated">「most_frequency」の場合、軸に沿って最も頻度の高い値を使用して、missingを置き換えます。</target>
        </trans-unit>
        <trans-unit id="d25972b438acba3aa495003bff5b880c3dc78f95" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;prefit&amp;rdquo; is passed, it is assumed that base_estimator has been fitted already and all data is used for calibration.</source>
          <target state="translated">「prefit」が渡された場合、base_estimatorはすでに適合していると見なされ、すべてのデータがキャリブレーションに使用されます。</target>
        </trans-unit>
        <trans-unit id="ddc01f2adfc0b5da49bb0bea104c04055f37082b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; (same as &amp;ldquo;auto&amp;rdquo;).</source>
          <target state="translated">「sqrt」の場合、 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; （「auto」と同じ）。</target>
        </trans-unit>
        <trans-unit id="050de520f25e08567f8dcb7bcdaa6887bd8ca53c" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">「sqrt」の場合、 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="274dd12ab1d70a7a9d00df8bbe2aa7f35f2ca3c4" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;SAMME.R&amp;rsquo; then use the SAMME.R real boosting algorithm. &lt;code&gt;base_estimator&lt;/code&gt; must support calculation of class probabilities. If &amp;lsquo;SAMME&amp;rsquo; then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.</source>
          <target state="translated">「SAMME.R」の場合、SAMME.Rリアルブースティングアルゴリズムを使用します。 &lt;code&gt;base_estimator&lt;/code&gt; は、クラス確率の計算をサポートする必要があります。「SAMME」の場合、SAMME離散ブースティングアルゴリズムを使用します。SAMME.Rアルゴリズムは通常、SAMMEよりも速く収束し、ブースティングの反復回数を減らしてテストエラーを低減します。</target>
        </trans-unit>
        <trans-unit id="7bac8214432dad215f7331c0af16dfd3868b9e19" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;balanced&amp;rsquo;, class weights will be given by &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;. If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform.</source>
          <target state="translated">「バランス」の場合、クラスの重みは &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; で与えられます。辞書が指定されている場合、キーはクラスであり、値は対応するクラスの重みです。Noneを指定すると、クラスの重みは均一になります。</target>
        </trans-unit>
        <trans-unit id="456401c87cfc2a15cdd408f2dd8be315dc3e833e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;english&amp;rsquo;, a built-in stop word list for English is used. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">「英語」の場合、英語の組み込みストップワードリストが使用されます。'english'にはいくつかの既知の問題があり、別の方法を検討する必要があります（&lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;ストップワードの使用を&lt;/a&gt;参照）。</target>
        </trans-unit>
        <trans-unit id="f2019bdbdfa8956b7b54e8a5677954f4996f6374" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;file&amp;rsquo;, the sequence items must have a &amp;lsquo;read&amp;rsquo; method (file-like object) that is called to fetch the bytes in memory.</source>
          <target state="translated">'file'の場合、シーケンスアイテムには、メモリ内のバイトをフェッチするために呼び出される 'read'メソッド（ファイルのようなオブジェクト）が必要です。</target>
        </trans-unit>
        <trans-unit id="3dd1f3ca74314afe1ddf15184f6ab04977d1a20b" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;filename&amp;rsquo;, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.</source>
          <target state="translated">'filename'の場合、フィットする引数として渡されるシーケンスは、分析するために生のコンテンツをフェッチするために読み取る必要があるファイル名のリストであることが期待されます。</target>
        </trans-unit>
        <trans-unit id="6381402e5f026c95872c614d3f284a846d432d3e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;hard&amp;rsquo;, uses predicted class labels for majority rule voting. Else if &amp;lsquo;soft&amp;rsquo;, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.</source>
          <target state="translated">「ハード」の場合、多数決ルールの投票に予測クラスラベルを使用します。そうでない場合は、「ソフト」の場合、予測確率の合計のargmaxに基づいてクラスラベルを予測します。これは、十分に調整された分類器のアンサンブルに推奨されます。</target>
        </trans-unit>
        <trans-unit id="034a602ff18129b75a77961464f62c916fb178cb" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;precomputed&amp;rsquo;, the training input X is expected to be a distance matrix.</source>
          <target state="translated">「事前計算済み」の場合、トレーニング入力Xは距離行列であると予想されます。</target>
        </trans-unit>
        <trans-unit id="efe2693abb0ad6fb0bb32fc7410403c6f8a6131c" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. In that case, &amp;lsquo;n_init&amp;rsquo; is ignored and only a single initialization occurs upon the first call. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">「warm_start」がTrueの場合、最後のフィッティングの解が次のfit（）呼び出しの初期化として使用されます。これにより、類似の問題でfitが複数回呼び出されたときに、収束を高速化できます。その場合、「n_init」は無視され、最初の呼び出しで初期化が1回だけ発生します。&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="245f671779646599b33075b7dcf37bf735b34555" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">「warm_start」がTrueの場合、最後のフィッティングの解が次のfit（）呼び出しの初期化として使用されます。これにより、類似の問題でfitが複数回呼び出されたときに、収束を高速化できます。&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="e311856e2dbc16a358c30263eb21c62e3f976c09" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt;&lt;code&gt;MinMaxScaler&lt;/code&gt;&lt;/a&gt; is given an explicit &lt;code&gt;feature_range=(min, max)&lt;/code&gt; the full formula is:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt; &lt;code&gt;MinMaxScaler&lt;/code&gt; に&lt;/a&gt;明示的な &lt;code&gt;feature_range=(min, max)&lt;/code&gt; が指定されている場合、完全な式は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="6f352ac5787c6e0994736f1793f840aefef2eb74" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.</source>
          <target state="translated">もし &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; と &lt;code&gt;svd_solver == 'full'&lt;/code&gt; 、ニーズが説明されること分散量がn_componentsによって指定されたパーセンテージよりも大きくなるように構成要素の数を選択します。</target>
        </trans-unit>
        <trans-unit id="6154e481a1bfebf053da4021c41ed6b15075ac75" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;Gram&lt;/code&gt; is overwritten.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; の場合、 &lt;code&gt;Gram&lt;/code&gt; は上書きされます。</target>
        </trans-unit>
        <trans-unit id="c8ea59a59509714d84c6c3be2a8959e87ca2c339" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; is overwritten.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; の場合、 &lt;code&gt;X&lt;/code&gt; は上書きされます。</target>
        </trans-unit>
        <trans-unit id="ecd953eee019b7cf39fa95c1745e9486ce5fb903" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; の場合、正しく分類されたサンプルの数を返します。それ以外の場合は、正しく分類されたサンプルの割合を返します。</target>
        </trans-unit>
        <trans-unit id="8dd52da2b8b6d856acbbc6b969b86fd0a4246941" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of misclassifications. Otherwise, return the fraction of misclassifications.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; の場合、誤分類の数を返します。それ以外の場合は、誤分類の割合を返します。</target>
        </trans-unit>
        <trans-unit id="85fcfc531652a8814592a07e791b2030fbc9598e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the sum of the Jaccard similarity coefficient over the sample set. Otherwise, return the average of Jaccard similarity coefficient.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; の場合、サンプルセットのJaccard類似係数の合計を返します。それ以外の場合は、Jaccard類似度係数の平均を返します。</target>
        </trans-unit>
        <trans-unit id="c21045a17e85201e2f77134fc96d9edd698a8ef9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, the &lt;code&gt;cv_results_&lt;/code&gt; attribute will not include training scores.</source>
          <target state="translated">場合 &lt;code&gt;False&lt;/code&gt; 、 &lt;code&gt;cv_results_&lt;/code&gt; の属性は、トレーニングのスコアは含まれません。</target>
        </trans-unit>
        <trans-unit id="4f7a2b9af6d7b5ad533a302e51ca948f564886c6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt; the estimator&amp;rsquo;s default scorer is used.</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; の場合、推定器のデフォルトのスコアラーが使用されます。</target>
        </trans-unit>
        <trans-unit id="0206caad9c301767e28c1eb37b72a3a7f7598e94" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; の場合、各クラスのスコアが返されます。それ以外の場合、これはデータに対して実行される平均化のタイプを決定します。</target>
        </trans-unit>
        <trans-unit id="f1170dbf5a5618e80add067200212cb5804a58cd" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt; the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; の場合、フルパスは &lt;code&gt;coef_path_&lt;/code&gt; 属性に格納されます。大きな問題または多くのターゲットのソリューションを計算する場合、 &lt;code&gt;fit_path&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定すると、特に小さなアルファでスピードアップにつながります。</target>
        </trans-unit>
        <trans-unit id="e703de20e5680ee264e2b1b950a8b1ca587cd24f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, X will be copied; else, it may be overwritten.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; の場合、Xがコピーされます。それ以外の場合、上書きされる可能性があります。</target>
        </trans-unit>
        <trans-unit id="8e26b0bb501133486ba99496e311f44a49ce472b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, perform metric MDS; otherwise, perform nonmetric MDS.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; の場合、メトリックMDSを実行します。それ以外の場合は、ノンメトリックMDSを実行します。</target>
        </trans-unit>
        <trans-unit id="a2b129bca8e38a348fd53d1896c7796acded2f57" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return a sparse feature matrix</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; の場合、スパース機能行列を返します</target>
        </trans-unit>
        <trans-unit id="887d26ef7077238a636d223d3985225a211c8d82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return the prior class probability and conditional probabilities of features given classes, from which the data was drawn.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; の場合、データが取得された、指定されたクラスの特徴の以前のクラス確率と条件付き確率を返します。</target>
        </trans-unit>
        <trans-unit id="0b7bef40bad08d9d2c4e67da6c9b56ea79751005" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, some instances might not belong to any class.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; の場合、一部のインスタンスはどのクラスにも属していない可能性があります。</target>
        </trans-unit>
        <trans-unit id="e223ba26a8622e808f0df02e86007844177282d6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; or &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the penalty applied to the L1 norm. If &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the absolute value of the threshold below which coefficients will be squashed to zero. If &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides &lt;code&gt;n_nonzero_coefs&lt;/code&gt;.</source>
          <target state="translated">もし &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; または &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 、 &lt;code&gt;alpha&lt;/code&gt; ペナルティがL1ノルムに適用されます。もし &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt; 、 &lt;code&gt;alpha&lt;/code&gt; 係数がゼロに押しつぶさされるの下閾値の絶対値です。 &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt; 場合、 &lt;code&gt;alpha&lt;/code&gt; は許容誤差パラメーター、つまり対象となる再構成エラーの値です。この場合、 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; をオーバーライドします。</target>
        </trans-unit>
        <trans-unit id="a88caf8f6b6232395c9c1524315c6ed672bcf763" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt; and X is encoded as a CSR matrix;</source>
          <target state="translated">もし &lt;code&gt;axis=0&lt;/code&gt; かつXがCSRマトリックスとして符号化されます。</target>
        </trans-unit>
        <trans-unit id="b74f02ef0c3e3aceaf2040e39764c8bf5d153fee" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt;, then impute along columns.</source>
          <target state="translated">&lt;code&gt;axis=0&lt;/code&gt; の場合、列に沿って代入します。</target>
        </trans-unit>
        <trans-unit id="231cba4e2ed9eee1fca5c3a6b02c0c6f66c47550" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt; and X is encoded as a CSC matrix.</source>
          <target state="translated">もし &lt;code&gt;axis=1&lt;/code&gt; 及びXは、CSCマトリックスとして符号化されます。</target>
        </trans-unit>
        <trans-unit id="4405a4c1e894889993d89bb6694cef1a6a8f7db3" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt;, then impute along rows.</source>
          <target state="translated">&lt;code&gt;axis=1&lt;/code&gt; の場合、行に沿って代入します。</target>
        </trans-unit>
        <trans-unit id="bf71a4a70c1ff1b9076f02c43d89e78c4b0ffc27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;backend&lt;/code&gt; is a string it must match a previously registered implementation using the &lt;code&gt;register_parallel_backend&lt;/code&gt; function.</source>
          <target state="translated">&lt;code&gt;backend&lt;/code&gt; が文字列の場合は、 &lt;code&gt;register_parallel_backend&lt;/code&gt; 関数を使用して以前に登録された実装と一致する必要があります。</target>
        </trans-unit>
        <trans-unit id="6456a4494f2ba1f052aff4cf6d35ef66e787bc14" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;base_estimator&lt;/code&gt; is None, then &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; is used for target values of dtype float.</source>
          <target state="translated">場合 &lt;code&gt;base_estimator&lt;/code&gt; はその後なし、ではない &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; DTYPEフロートの目標値のために使用されます。</target>
        </trans-unit>
        <trans-unit id="898731158b64382ef9aad2307b39d22b5cd2a315" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dense&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the dense binary indicator format. If &lt;code&gt;'sparse'&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the sparse binary indicator format. &lt;code&gt;False&lt;/code&gt; returns a list of lists of labels.</source>
          <target state="translated">&lt;code&gt;dense&lt;/code&gt; 場合、高密度のバイナリインジケータ形式で &lt;code&gt;Y&lt;/code&gt; を返します。 &lt;code&gt;'sparse'&lt;/code&gt; 場合、スパースバイナリインジケーター形式で &lt;code&gt;Y&lt;/code&gt; を返します。 &lt;code&gt;False&lt;/code&gt; はラベルのリストのリストを返します。</target>
        </trans-unit>
        <trans-unit id="b1213caecc623f2a5139e82ba5db339edb5f6265" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape (1,) when the given problem is binary. In particular, when &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt;, &lt;code&gt;intercept_&lt;/code&gt; corresponds to outcome 1 (True) and &lt;code&gt;-intercept_&lt;/code&gt; corresponds to outcome 0 (False).</source>
          <target state="translated">&lt;code&gt;fit_intercept&lt;/code&gt; がFalseに設定されている場合、切片はゼロに設定されます。与えられた問題がバイナリの場合、 &lt;code&gt;intercept_&lt;/code&gt; の形状は（1）です。特に、 &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt; 場合、 &lt;code&gt;intercept_&lt;/code&gt; は結果1（True）に対応し、 &lt;code&gt;-intercept_&lt;/code&gt; は結果0（False）に対応します。</target>
        </trans-unit>
        <trans-unit id="4504cae87026fef1f6989cfa20e2e5bc171d37e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape(1,) when the problem is binary.</source>
          <target state="translated">&lt;code&gt;fit_intercept&lt;/code&gt; がFalseに設定されている場合、切片はゼロに設定されます。問題がバイナリの場合、 &lt;code&gt;intercept_&lt;/code&gt; はshape（1、）です。</target>
        </trans-unit>
        <trans-unit id="646836188c841e4fea39e4e4200d1f27e6191986" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;loss&lt;/code&gt; is a callable, then it should be a function that takes two arrays as inputs, the true and predicted value and returns a 1-D array with the i-th value of the array corresponding to the loss on &lt;code&gt;X[i]&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;loss&lt;/code&gt; が呼び出し可能である場合、それは2つの配列を入力として真と予測値を取り、 &lt;code&gt;X[i]&lt;/code&gt; 損失に対応する配列のi番目の値を含む1次元配列を返す関数である必要があります。</target>
        </trans-unit>
        <trans-unit id="c42d62680a26d66fc0f439c634f24ee01c670c77" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;memory&lt;/code&gt; is not joblib.Memory-like.</source>
          <target state="translated">&lt;code&gt;memory&lt;/code&gt; がjoblib.Memory-likeでない場合。</target>
        </trans-unit>
        <trans-unit id="e14dd7c153267d74f6b2214ce66bcf041482d792" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_bins&lt;/code&gt; is an array, and there is an ignored feature at index &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;n_bins[i]&lt;/code&gt; will be ignored.</source>
          <target state="translated">&lt;code&gt;n_bins&lt;/code&gt; が配列で、インデックス &lt;code&gt;i&lt;/code&gt; に無視される機能がある場合、 &lt;code&gt;n_bins[i]&lt;/code&gt; は無視されます。</target>
        </trans-unit>
        <trans-unit id="20ab457ec31da79f104a0f7a337ba6bfe94b5438" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the data is reduced from 100,000 samples to a set of 158 clusters. This can be viewed as a preprocessing step before the final (global) clustering step that further reduces these 158 clusters to 100 clusters.</source>
          <target state="translated">場合 &lt;code&gt;n_clusters&lt;/code&gt; が Noneに設定され、データ10万個のサンプルから158個のクラスタのセットに縮小されます。これは、これらの158クラスターをさらに100クラスターに削減する最終（グローバル）クラスター化ステップの前の前処理ステップと見なすことができます。</target>
        </trans-unit>
        <trans-unit id="1769c2fe615105013ff722090827f85ac960dff9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components == 'mle'&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, Minka&amp;rsquo;s MLE is used to guess the dimension. Use of &lt;code&gt;n_components == 'mle'&lt;/code&gt; will interpret &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; as &lt;code&gt;svd_solver == 'full'&lt;/code&gt;.</source>
          <target state="translated">場合 &lt;code&gt;n_components == 'mle'&lt;/code&gt; と &lt;code&gt;svd_solver == 'full'&lt;/code&gt; 、民家のMLEは、ディメンションを推測するために使用されます。 &lt;code&gt;n_components == 'mle'&lt;/code&gt; を使用すると、svd_solver == ' &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; は &lt;code&gt;svd_solver == 'full'&lt;/code&gt; と解釈されます。</target>
        </trans-unit>
        <trans-unit id="959758e689ea656dd0e72e3bda18b0a45bbef2e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components&lt;/code&gt; is not set then all components are stored and the sum of the ratios is equal to 1.0.</source>
          <target state="translated">&lt;code&gt;n_components&lt;/code&gt; が設定されていない場合、すべてのコンポーネントが格納され、比率の合計は1.0になります。</target>
        </trans-unit>
        <trans-unit id="4aaa2df3fa37c88b24d06638ef7188d7e8ebe112" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each parameter setting(and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">場合 &lt;code&gt;n_jobs&lt;/code&gt; のものよりも高い値に設定し、データは、各パラメータの設定のためにコピーされ（そしてれない &lt;code&gt;n_jobs&lt;/code&gt; 回）。これは、個々のジョブにほとんど時間がかからない場合の効率上の理由で行われますが、データセットが大きく、十分なメモリが利用できない場合は、エラーが発生する可能性があります。この場合の回避策は、 &lt;code&gt;pre_dispatch&lt;/code&gt; を設定することです。その後、メモリは &lt;code&gt;pre_dispatch&lt;/code&gt; のみ何度もコピーされます。 &lt;code&gt;pre_dispatch&lt;/code&gt; の適切な値は &lt;code&gt;2 * n_jobs&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="53682a81a25d0884d79ca09b064b0fc6e7cabd67" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each point in the grid (and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">場合 &lt;code&gt;n_jobs&lt;/code&gt; のものよりも高い値に設定し、データは、グリッドの各点のためにコピーされ（そしてれない &lt;code&gt;n_jobs&lt;/code&gt; 回）。これは、個々のジョブにほとんど時間がかからない場合の効率上の理由で行われますが、データセットが大きく、十分なメモリが利用できない場合は、エラーが発生する可能性があります。この場合の回避策は、 &lt;code&gt;pre_dispatch&lt;/code&gt; を設定することです。その後、メモリは &lt;code&gt;pre_dispatch&lt;/code&gt; のみ何度もコピーされます。 &lt;code&gt;pre_dispatch&lt;/code&gt; の適切な値は &lt;code&gt;2 * n_jobs&lt;/code&gt; です。</target>
        </trans-unit>
        <trans-unit id="c1cc035dd2ff12188f95601d6fe5c4679ad6bb78" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_samples == 10000&lt;/code&gt;, storing &lt;code&gt;X&lt;/code&gt; as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = &lt;strong&gt;4GB in RAM&lt;/strong&gt; which is barely manageable on today&amp;rsquo;s computers.</source>
          <target state="translated">&lt;code&gt;n_samples == 10000&lt;/code&gt; 場合、float32型のNumPy配列として &lt;code&gt;X&lt;/code&gt; を格納するには、10000 x 100000 x 4バイト= &lt;strong&gt;4GBのRAMが必要&lt;/strong&gt;であり、今日のコンピューターではほとんど管理できません。</target>
        </trans-unit>
        <trans-unit id="8e65ba558868cb56b0c8a76632ea67901b254afe" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the average Jaccard similarity coefficient, else it returns the sum of the Jaccard similarity coefficient over the sample set.</source>
          <target state="translated">&lt;code&gt;normalize == True&lt;/code&gt; 場合、平均Jaccard類似度係数を返します。それ以外の場合は、サンプルセットのJaccard類似度係数の合計を返します。</target>
        </trans-unit>
        <trans-unit id="c606413521700e073d3e669024415faa2300a113" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of correctly classified samples (float), else returns the number of correctly classified samples (int).</source>
          <target state="translated">&lt;code&gt;normalize == True&lt;/code&gt; 場合、正しく分類されたサンプルの割合（float）を返します。それ以外の場合は、正しく分類されたサンプルの数（int）を返します。</target>
        </trans-unit>
        <trans-unit id="cd9dc36a4d7167817c2823908b4ce633913b9765" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int).</source>
          <target state="translated">&lt;code&gt;normalize == True&lt;/code&gt; 場合、誤分類の割合（float）を返します。それ以外の場合は、誤分類の数（int）を返します。</target>
        </trans-unit>
        <trans-unit id="d0b860961bc5b4a4c45189c0fd4de5c2d61f1ab4" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;out=None&lt;/code&gt;, returns a new array containing the mean values, otherwise a reference to the output array is returned.</source>
          <target state="translated">&lt;code&gt;out=None&lt;/code&gt; の場合は、平均値を含む新しい配列を返します。それ以外の場合は、出力配列への参照を返します。</target>
        </trans-unit>
        <trans-unit id="a1e72f87e91fc5bb6f8882ece59a46bf9ee089e2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;pos_label is None&lt;/code&gt; and in binary classification, this function returns the average precision, recall and F-measure if &lt;code&gt;average&lt;/code&gt; is one of &lt;code&gt;'micro'&lt;/code&gt;, &lt;code&gt;'macro'&lt;/code&gt;, &lt;code&gt;'weighted'&lt;/code&gt; or &lt;code&gt;'samples'&lt;/code&gt;.</source>
          <target state="translated">場合 &lt;code&gt;pos_label is None&lt;/code&gt; バイナリ分類であれば、この関数は、平均精度、再現率とF値を返す &lt;code&gt;average&lt;/code&gt; の一つである &lt;code&gt;'micro'&lt;/code&gt; 、 &lt;code&gt;'macro'&lt;/code&gt; 、 &lt;code&gt;'weighted'&lt;/code&gt; または &lt;code&gt;'samples'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7bb4c6eca31ede3ca3e8fe5a9b41ecd9a55b266b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;return_path==True&lt;/code&gt; returns the entire path, else returns only the last point of the path.</source>
          <target state="translated">&lt;code&gt;return_path==True&lt;/code&gt; がパス全体を返す場合は、パスの最後のポイントのみを返します。</target>
        </trans-unit>
        <trans-unit id="9e477bb8072307702a812f869b8166fe31bf9ca0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;smooth_idf=True&lt;/code&gt; (the default), the constant &amp;ldquo;1&amp;rdquo; is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.</source>
          <target state="translated">場合 &lt;code&gt;smooth_idf=True&lt;/code&gt; の IDF（D、T：（デフォルト）、余分なドキュメントがゼロ分裂を防ぎ、正確に一度、コレクション内のすべての用語を含む見られたかのように、定数「1」IDFの分子と分母に加算されます）= log [（1 + n）/（1 + df（d、t））] + 1。</target>
        </trans-unit>
        <trans-unit id="3f772487671a5560a2a2bb3464b54cf0bad5b348" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;svd_solver == 'arpack'&lt;/code&gt;, the number of components must be strictly less than the minimum of n_features and n_samples.</source>
          <target state="translated">&lt;code&gt;svd_solver == 'arpack'&lt;/code&gt; 場合、コンポーネントの数はn_featuresとn_samplesの最小値より厳密に少なくなければなりません。</target>
        </trans-unit>
        <trans-unit id="abdb8ed2b871d03510343cf9ee77736674d298ab" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;validate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; will be checked.</source>
          <target state="translated">&lt;code&gt;validate&lt;/code&gt; が &lt;code&gt;True&lt;/code&gt; の場合、 &lt;code&gt;X&lt;/code&gt; がチェックされます。</target>
        </trans-unit>
        <trans-unit id="5cfb594032bd50fcef2738a25df520e95867f411" translate="yes" xml:space="preserve">
          <source>If C is a ground truth class assignment and K the clustering, let us define \(a\) and \(b\) as:</source>
          <target state="translated">C が基底真理クラスの代入であり、K がクラスタリングであるとすると、\(a\)と \(b\)を次のように定義する。</target>
        </trans-unit>
        <trans-unit id="40e72ab25b1921db07187a1c526cc9080a10eaea" translate="yes" xml:space="preserve">
          <source>If False, X will be overwritten. &lt;code&gt;copy=False&lt;/code&gt; can be used to save memory but is unsafe for general use.</source>
          <target state="translated">Falseの場合、Xは上書きされます。 &lt;code&gt;copy=False&lt;/code&gt; はメモリを節約するために使用できますが、一般的な使用には安全ではありません。</target>
        </trans-unit>
        <trans-unit id="b5379fd8e8700833a560e6ab84ea58c40e10b6a8" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.</source>
          <target state="translated">False にする と 、 はめ込みに渡されたデータは上書きされ、 fit(X).transform(X)を実行しても期待した結果が得られませんので、 代わりに fit_transform(X)を使います。</target>
        </trans-unit>
        <trans-unit id="3d545281a4ef31d03ffb244079b728a3c5cc8b18" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten. Defaults to True.</source>
          <target state="translated">False にする と 、 はめ込みに渡されたデータは上書きされます。デフォルトは True です。</target>
        </trans-unit>
        <trans-unit id="4bf616e8d9d604d2525c59d889782525e410270c" translate="yes" xml:space="preserve">
          <source>If False, distances will not be returned</source>
          <target state="translated">Falseの場合、距離は返されません。</target>
        </trans-unit>
        <trans-unit id="d8cdf8e9cb326e6212f67c80aca3a4f04326fc4c" translate="yes" xml:space="preserve">
          <source>If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">False の場合、ソースサイトからデータをダウンロードしようとするのではなく、データがローカルで利用できない場合に IOError を発生させます。</target>
        </trans-unit>
        <trans-unit id="707f36c34b2f81eacbaf143a8e62cb9371b1332e" translate="yes" xml:space="preserve">
          <source>If False, raise an IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">False の場合、ソースサイトからデータをダウンロードしようとするのではなく、データがローカルで利用できない場合に IOError を発生させます。</target>
        </trans-unit>
        <trans-unit id="d32001af2bcb0806daa431ab8cf432f700c0bb79" translate="yes" xml:space="preserve">
          <source>If False, the imputer mask will be a numpy array.</source>
          <target state="translated">False の場合、インピュターマスクは numpy 配列になります。</target>
        </trans-unit>
        <trans-unit id="2823ebb07c9bdb5cae0bfca227f5db48d585ba5a" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and dictionary will not be checked.</source>
          <target state="translated">Falseの場合、入力配列Xと辞書はチェックされません。</target>
        </trans-unit>
        <trans-unit id="bd8e933f9aa74b9b27da566d4ffb96f4e62218cf" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and y will not be checked.</source>
          <target state="translated">Falseの場合、入力配列Xとyはチェックされません。</target>
        </trans-unit>
        <trans-unit id="85aa52dd8c7d5d29b6bebfd616a7ca3fe91cde14" translate="yes" xml:space="preserve">
          <source>If False, the projected data uses a sparse representation if the input is sparse.</source>
          <target state="translated">Falseの場合、入力が疎な場合、投影データは疎な表現を使用します。</target>
        </trans-unit>
        <trans-unit id="7bfec8f3204bdf713e2d3557ec53ea6f3960ad24" translate="yes" xml:space="preserve">
          <source>If False, there is no input validation.</source>
          <target state="translated">Falseの場合、入力の検証は行われません。</target>
        </trans-unit>
        <trans-unit id="a67320198a0b746d35fcc941198f1221ee73c87b" translate="yes" xml:space="preserve">
          <source>If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.</source>
          <target state="translated">Falseの場合、コピーを回避し、代わりにインプレーススケーリングを行います。これは常にインプレースで動作することを保証するものではありません;例えば、データが NumPy 配列や scipy.sparse CSR 行列でない場合、コピーが返される可能性があります。</target>
        </trans-unit>
        <trans-unit id="fcd08eda0bca1685e28de89ae046095006b92653" translate="yes" xml:space="preserve">
          <source>If None (default), load all the categories. If not None, list of category names to load (other categories ignored).</source>
          <target state="translated">None(デフォルト)の場合、すべてのカテゴリを読み込みます。Noneでない場合は、読み込むカテゴリ名のリスト(他のカテゴリは無視されます)。</target>
        </trans-unit>
        <trans-unit id="a7cbd99fe721a3cd173045eddaf688b83e7620c1" translate="yes" xml:space="preserve">
          <source>If None the estimator&amp;rsquo;s default scorer, if available, is used.</source>
          <target state="translated">Noneの場合、推定器のデフォルトのスコアラー（使用可能な場合）が使用されます。</target>
        </trans-unit>
        <trans-unit id="96ad58db5ee1b903109c173fcab72b0955bb0408" translate="yes" xml:space="preserve">
          <source>If None, defaults to 1.0 / n_features</source>
          <target state="translated">None の場合、デフォルトは 1.0/n_features</target>
        </trans-unit>
        <trans-unit id="e5ce9a9046a52014390758ba790166ae01779c1f" translate="yes" xml:space="preserve">
          <source>If None, do not try to decode the content of the files (e.g. for images or other non-text content). If not None, encoding to use to decode text files to Unicode if load_content is True.</source>
          <target state="translated">None の場合は、ファイルの内容をデコードしようとしません (画像やその他のテキスト以外の内容の場合など)。None でない場合は、load_content が True の場合にテキストファイルを Unicode にデコードするために使用するエンコーディング。</target>
        </trans-unit>
        <trans-unit id="3e5e8d666168a7a15a80edb16364256ae0a379e4" translate="yes" xml:space="preserve">
          <source>If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.</source>
          <target state="translated">max_df は [0.7,1.0]の範囲の値を設定することで、コーパス内の文書の用語の頻度に基づいて停止語を自動的に検出し、フィルタリングすることができます。</target>
        </trans-unit>
        <trans-unit id="02542a43a2f09f5328657402d69f49ce442cb6c2" translate="yes" xml:space="preserve">
          <source>If None, pairwise_distances_chunked returns a generator of vertical chunks of the distance matrix.</source>
          <target state="translated">Noneの場合、pairwise_distances_chunkedは、距離行列の垂直チャンクの生成器を返します。</target>
        </trans-unit>
        <trans-unit id="eb5d73cb83520641b0e2c815c109159135d569cc" translate="yes" xml:space="preserve">
          <source>If None, the estimator&amp;rsquo;s default scorer (if available) is used.</source>
          <target state="translated">Noneの場合、推定器のデフォルトのスコアラー（使用可能な場合）が使用されます。</target>
        </trans-unit>
        <trans-unit id="651c3653c15c90a6722a00465ba57c19c30adb9b" translate="yes" xml:space="preserve">
          <source>If None, the threshold is assumed to be half way between neg_label and pos_label.</source>
          <target state="translated">Noneの場合、閾値はneg_labelとpos_labelの中間にあると仮定します。</target>
        </trans-unit>
        <trans-unit id="ac652d29bc285e4e46f5aaf2fe5415c63aee1f09" translate="yes" xml:space="preserve">
          <source>If None, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">Noneの場合、 &lt;code&gt;max_features=n_features&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2f9e5ee96434f57529c2481b71d631d9dd0cb5e7" translate="yes" xml:space="preserve">
          <source>If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.</source>
          <target state="translated">True(デフォルト)の場合,誤差2乗法線をn_featuresで分割します.Falseの場合,誤差2乗法線は再スケーリングされません.</target>
        </trans-unit>
        <trans-unit id="da82574bb396bf8045c493d20398be74e4e9ef51" translate="yes" xml:space="preserve">
          <source>If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).</source>
          <target state="translated">True(デフォルト)の場合、バイアス列を含みます。これは、すべての多項式の累乗が0である特徴(すなわち、1の列-線形モデルの切片項として機能します)を含みます。</target>
        </trans-unit>
        <trans-unit id="d150b2a4c21e929dfd726f6463d03ca9f005e91a" translate="yes" xml:space="preserve">
          <source>If True (default), transform will raise an error when there are features with missing values in transform that have no missing values in fit This is applicable only when &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt;.</source>
          <target state="translated">True（デフォルト）の場合、変換に欠損値があり、フィットに欠損値がない特徴がある場合、transformはエラーを発生させます。これは &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt; の場合にのみ適用されます。</target>
        </trans-unit>
        <trans-unit id="d91ad850130f94be79fb668041bf3eecd01a29b0" translate="yes" xml:space="preserve">
          <source>If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to &amp;lsquo;sag&amp;rsquo;. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.</source>
          <target state="translated">TrueでXがスパースの場合、メソッドは切片も返し、ソルバーは自動的に「サグ」に変更されます。これは、切片をスパースデータに合わせるための一時的な修正にすぎません。密なデータの場合、回帰の前にsklearn.linear_model._preprocess_dataを使用します。</target>
        </trans-unit>
        <trans-unit id="a68108e0ea5bf983075f127e077ee80517d6dfdd" translate="yes" xml:space="preserve">
          <source>If True the covariance matrices are computed and stored in the &lt;code&gt;self.covariance_&lt;/code&gt; attribute.</source>
          <target state="translated">Trueの場合、共分散行列が計算され、 &lt;code&gt;self.covariance_&lt;/code&gt; 属性に格納されます。</target>
        </trans-unit>
        <trans-unit id="5c5d5facc126a265032a533a4664c8926339ade0" translate="yes" xml:space="preserve">
          <source>If True the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">Trueの場合、フルパスは &lt;code&gt;coef_path_&lt;/code&gt; 属性に格納されます。大きな問題または多くのターゲットのソリューションを計算する場合、 &lt;code&gt;fit_path&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定すると、特に小さなアルファでスピードアップにつながります。</target>
        </trans-unit>
        <trans-unit id="53e960778922c6ba257a9c66f18d0e260655f043" translate="yes" xml:space="preserve">
          <source>If True the function returns the pairwise distance matrix else it returns the componentwise L1 pairwise-distances. Not supported for sparse matrix inputs.</source>
          <target state="translated">True の場合,この関数はペアワイズ距離行列を返し,そうでない場合はコンポーネントワイズ L1 のペアワイズ距離を返します.疎な行列の入力ではサポートされません。</target>
        </trans-unit>
        <trans-unit id="2546c89362b151bbba35dab463b810d1f7c0a359" translate="yes" xml:space="preserve">
          <source>If True the order of the dataset is shuffled to avoid having images of the same person grouped.</source>
          <target state="translated">Trueの場合、同じ人物の画像がグループ化されないように、データセットの順序がシャッフルされます。</target>
        </trans-unit>
        <trans-unit id="618a67ac95fc4ccc3385ae319143bf344e1ffb63" translate="yes" xml:space="preserve">
          <source>If True then raise a warning if conversion is required.</source>
          <target state="translated">Trueの場合は、変換が必要な場合に警告を表示します。</target>
        </trans-unit>
        <trans-unit id="19362eed638b2dc6d204e12092075aedd87e6e93" translate="yes" xml:space="preserve">
          <source>If True then raise an exception if array is not symmetric.</source>
          <target state="translated">真の場合、配列が対称でない場合は例外を発生させます。</target>
        </trans-unit>
        <trans-unit id="baf82faf959595f525e5d5a94c6b8526ad942779" translate="yes" xml:space="preserve">
          <source>If True, X will be copied; else, it may be overwritten.</source>
          <target state="translated">True の場合は X がコピーされ、そうでない場合は上書きされる可能性があります。</target>
        </trans-unit>
        <trans-unit id="aa91f074d713faca021e35ddd5331805b9848f9e" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, a copy may still be returned if X&amp;rsquo;s dtype is not a floating point type.</source>
          <target state="translated">Trueの場合、Xのコピーが作成されます。Falseの場合、Xのdtypeが浮動小数点型でない場合でもコピーが返される可能性があります。</target>
        </trans-unit>
        <trans-unit id="054b374d036034be5d7258a6a975038eb8fec935" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if &lt;code&gt;copy=False&lt;/code&gt;:</source>
          <target state="translated">Trueの場合、Xのコピーが作成されます。Falseの場合、代入は可能な限りインプレースで行われます。次の場合、 &lt;code&gt;copy=False&lt;/code&gt; であっても、常に新しいコピーが作成されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="f237ade75e04520befb53fc36267d58be41dc5ae" translate="yes" xml:space="preserve">
          <source>If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.</source>
          <target state="translated">Trueの場合、学習データの永続的なコピーがオブジェクトに格納されます。そうでない場合は、学習データへの参照のみが格納され、データが外部から変更された場合に予測値が変更される可能性があります。</target>
        </trans-unit>
        <trans-unit id="5b4ca3bdeb594ab34289df8cfc7fa70c9919ad95" translate="yes" xml:space="preserve">
          <source>If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.</source>
          <target state="translated">これは、整数カウントではなくバイナリイベントをモデル化する離散確率モデルに有用です。</target>
        </trans-unit>
        <trans-unit id="0beec2b2d6b910456e155063f83ec1e59c6c0df1" translate="yes" xml:space="preserve">
          <source>If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs.)</source>
          <target state="translated">これは出力が0/1の値しか持たないという意味ではなく、tf-idfのtf項がバイナリであることを意味します。(0/1の出力を得るためにidfと正規化をFalseに設定します)。</target>
        </trans-unit>
        <trans-unit id="b69242de00e60526a79082b37846a2a724d7b3dd" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling.</source>
          <target state="translated">True の場合、スケーリングの前にデータを中央に配置します。</target>
        </trans-unit>
        <trans-unit id="6c9a4d2da449884c97015be12ce056a53dc04757" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">True の場合、スケーリングの前にデータをセンタリングします。これは、疎な行列で試した場合には動作しません(例外が発生します)。なぜなら、中央揃えを行うと、一般的な使用例ではメモリに収まりきらないほど大きな行列を作成しなければならないからです。</target>
        </trans-unit>
        <trans-unit id="9d7135e468914be214009091b1fc11f6721afd0a" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This will cause &lt;code&gt;transform&lt;/code&gt; to raise an exception when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">Trueの場合、スケーリングの前にデータを中央揃えにします。これにより、スパースマトリックスでの試行時に &lt;code&gt;transform&lt;/code&gt; 例外が発生します。これは、スパースマトリックスを中央に配置すると、通常のユースケースではメモリに収まりきらない可能性が高い密なマトリックスを構築する必要があるためです。</target>
        </trans-unit>
        <trans-unit id="20839df17b3bca6b55533337f17b7c17f0881d1a" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False</source>
          <target state="translated">Trueの場合、モデルの各ステップで目的関数を計算します。デフォルトはFalseです。</target>
        </trans-unit>
        <trans-unit id="afb80999f635ad0619301e31bb4cd6b353188af0" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False.</source>
          <target state="translated">Trueの場合、モデルの各ステップで目的関数を計算します。デフォルトはFalseです。</target>
        </trans-unit>
        <trans-unit id="9df36ef1b24eb49a93a9d932c395b3324554689c" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">Trueの場合、データは計算前に中央揃えされません。平均値が有意にゼロに等しいが、正確にはゼロではないデータを扱うのに便利です。Falseの場合、データは計算前に中央揃えされます。</target>
        </trans-unit>
        <trans-unit id="cdd266c276f30f1ed8fec5e418bed1790d829f9f" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation.</source>
          <target state="translated">Trueの場合、データは計算前に中央揃えされません。平均値がほぼゼロではなく、正確にはゼロではないデータを扱う場合に便利です。False(デフォルト)の場合、データは計算の前に中央揃えされます。</target>
        </trans-unit>
        <trans-unit id="b4e1adfc1374b7a62eee31a2ac4be08eea958149" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">Trueの場合、データは計算前に中央揃えされません。平均値がほぼゼロではなく、正確にはゼロではないデータを扱う場合に便利です。Falseの場合、データは計算の前に中央揃えされます。</target>
        </trans-unit>
        <trans-unit id="a2902b07926590508dee697d1e97c541f35cd531" translate="yes" xml:space="preserve">
          <source>If True, ensure that the output of the random projection is a dense numpy array even if the input and random projection matrix are both sparse. In practice, if the number of components is small the number of zero components in the projected data will be very small and it will be more CPU and memory efficient to use a dense representation.</source>
          <target state="translated">Trueの場合、入力行列とランダム投影行列が共に疎な場合でも、ランダム投影の出力が密なnumpy配列になるようにします。実際には,成分の数が少ない場合,投影データ中のゼロ成分の数は非常に少なくなり,密な表現を用いた方が CPU やメモリの効率が良くなります.</target>
        </trans-unit>
        <trans-unit id="a3946c2202800dbed0f15da39a81b563f2054e41" translate="yes" xml:space="preserve">
          <source>If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed.</source>
          <target state="translated">True の場合、個々の木は、置換を行ってサンプリングされた訓練データのランダムな部分集合にフィットします。Falseの場合、置換なしのサンプリングが実行されます。</target>
        </trans-unit>
        <trans-unit id="47a8f3ebe1bf3e97122b0404e375e9c09f1cef3f" translate="yes" xml:space="preserve">
          <source>If True, input X is copied and stored by the model in the &lt;code&gt;X_fit_&lt;/code&gt; attribute. If no further changes will be done to X, setting &lt;code&gt;copy_X=False&lt;/code&gt; saves memory by storing a reference.</source>
          <target state="translated">Trueの場合、入力Xがコピーされ、モデルによって &lt;code&gt;X_fit_&lt;/code&gt; 属性に格納されます。Xにそれ以上の変更が行われない場合、 &lt;code&gt;copy_X=False&lt;/code&gt; を設定すると、参照が保存されてメモリが節約されます。</target>
        </trans-unit>
        <trans-unit id="10ab7d9c4ef9751f0861c3cfbcd363da08ee1196" translate="yes" xml:space="preserve">
          <source>If True, return a sparse CSR continency matrix. If &lt;code&gt;eps is not None&lt;/code&gt;, and &lt;code&gt;sparse is True&lt;/code&gt;, will throw ValueError.</source>
          <target state="translated">Trueの場合、まばらなCSR大陸性行列を返します。場合は &lt;code&gt;eps is not None&lt;/code&gt; 、と &lt;code&gt;sparse is True&lt;/code&gt; 、とValueErrorを送出します。</target>
        </trans-unit>
        <trans-unit id="bbadcb21277fb2fd7500e5e018984adc0515f847" translate="yes" xml:space="preserve">
          <source>If True, return output as dict</source>
          <target state="translated">True の場合、出力を dict として返します。</target>
        </trans-unit>
        <trans-unit id="4c0f661b8fac7f22363a5a1e55327c6967bb56d8" translate="yes" xml:space="preserve">
          <source>If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. If False, return the average score across folds. Default is True, but will change to False in version 0.21, to correspond to the standard definition of cross-validation.</source>
          <target state="translated">True の場合、各テスト・セットのサンプル数で重み付けされたヒダ全体の平均スコアを返します。この場合、データはひだ全体で同じように分布していると仮定され、最小化される損失はサンプルごとの総損失であり、ひだ全体の平均損失ではありません。Falseの場合、ひだ全体の平均スコアを返します。デフォルトはTrueですが、バージョン0.21ではFalseに変更され、クロスバリデーションの標準的な定義に対応します。</target>
        </trans-unit>
        <trans-unit id="cd7031ac688b02e25258c5831c7d3ea164486db6" translate="yes" xml:space="preserve">
          <source>If True, return the distance between the clusters.</source>
          <target state="translated">真の場合、クラスタ間の距離を返します。</target>
        </trans-unit>
        <trans-unit id="48eeb388f7c432fd1b00c136703cc691939b77aa" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; object.</source>
          <target state="translated">Trueの場合、Bunchオブジェクトの代わりに &lt;code&gt;(data, target)&lt;/code&gt; 返します。 &lt;code&gt;data&lt;/code&gt; と &lt;code&gt;target&lt;/code&gt; オブジェクトの詳細については、以下を参照してください。</target>
        </trans-unit>
        <trans-unit id="b8ef976b3bf0a8c5fe0d328bf69ca77595314915" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; objects.</source>
          <target state="translated">Trueの場合、Bunchオブジェクトの代わりに &lt;code&gt;(data, target)&lt;/code&gt; 返します。 &lt;code&gt;data&lt;/code&gt; と &lt;code&gt;target&lt;/code&gt; オブジェクトの詳細については、以下を参照してください。</target>
        </trans-unit>
        <trans-unit id="d467c22a2bd71ab649cbad26364f06a31ce58ac1" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data.data, data.target)&lt;/code&gt; instead of a Bunch object.</source>
          <target state="translated">Trueの場合、Bunchオブジェクトの代わりに &lt;code&gt;(data.data, data.target)&lt;/code&gt; 返します。</target>
        </trans-unit>
        <trans-unit id="248b6d0d481e47f352d5f3203242e6800072c658" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;dataset.data&lt;/code&gt; and &lt;code&gt;dataset.target&lt;/code&gt; object.</source>
          <target state="translated">Trueの場合、Bunchオブジェクトの代わりに &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; 返します。 &lt;code&gt;dataset.data&lt;/code&gt; および &lt;code&gt;dataset.target&lt;/code&gt; オブジェクトの詳細については、以下を参照してください。</target>
        </trans-unit>
        <trans-unit id="c220af25fde2fda1e9985d78b63166a331905d63" translate="yes" xml:space="preserve">
          <source>If True, scale the data to interquartile range.</source>
          <target state="translated">Trueの場合は、データを四分位間の範囲にスケールします。</target>
        </trans-unit>
        <trans-unit id="0f395f8257fe0bdfb8a823c88f3be9843583f8a6" translate="yes" xml:space="preserve">
          <source>If True, scale the data to unit variance (or equivalently, unit standard deviation).</source>
          <target state="translated">真の場合、データを単位分散(または同等に、単位標準偏差)にスケールします。</target>
        </trans-unit>
        <trans-unit id="a491ca8d8797fa01293c195b8787d725c30e073a" translate="yes" xml:space="preserve">
          <source>If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope.</source>
          <target state="translated">真の場合、クラスタは超立方体の頂点に置かれます。Falseの場合、クラスタはランダムな多面体の頂点に置かれます。</target>
        </trans-unit>
        <trans-unit id="dc426ca785aa82bc3726faf833e38673875ff1ab" translate="yes" xml:space="preserve">
          <source>If True, the coefficients of the underlying linear model are returned.</source>
          <target state="translated">Trueの場合、基礎となる線形モデルの係数が返されます。</target>
        </trans-unit>
        <trans-unit id="5724be8fac97575b0a1ba6783afc1c2fbe0fcac8" translate="yes" xml:space="preserve">
          <source>If True, the covariance of the joint predictive distribution at the query points is returned along with the mean</source>
          <target state="translated">Trueの場合,問い合わせ点における共同予測分布の共分散が,平均</target>
        </trans-unit>
        <trans-unit id="d28765388f526f443e8440713d9f120592d23759" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. If True, theta must not be None.</source>
          <target state="translated">真の場合、θの位置におけるカーネル・ハイパーパラメータに対する対数最尤度の勾配が追加で返されます。真の場合、θはNoneであってはなりません。</target>
        </trans-unit>
        <trans-unit id="2ecaf6f4ca3e741011335b25b38b91313c972ea3" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. Note that gradient computation is not supported for non-binary classification. If True, theta must not be None.</source>
          <target state="translated">真の場合、位置θにおけるカーネル・ハイパーパラメータに対する対数最尤度の勾配が追加で返されます。勾配の計算は、非二値分類ではサポートされていないことに注意してください。真の場合、θはNoneであってはなりません。</target>
        </trans-unit>
        <trans-unit id="65ddb79c8d6a76beebeeb976d10455d7eb1fb46d" translate="yes" xml:space="preserve">
          <source>If True, the imputer mask will be a sparse matrix.</source>
          <target state="translated">Trueの場合、インピュータマスクはスパース行列になります。</target>
        </trans-unit>
        <trans-unit id="a134a28236267fd097c12ab6f4f7b85d957aa9ca" translate="yes" xml:space="preserve">
          <source>If True, the method also returns &lt;code&gt;n_iter&lt;/code&gt;, the actual number of iteration performed by the solver.</source>
          <target state="translated">Trueの場合、メソッドは、ソルバーによって実行された実際の反復数である &lt;code&gt;n_iter&lt;/code&gt; も返します。</target>
        </trans-unit>
        <trans-unit id="af50e40087f45610f2fbcc323b88ccd93dcf9324" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learned more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">Trueの場合、リグレッサXは回帰前に正規化されます。このパラメーターは、 &lt;code&gt;fit_intercept&lt;/code&gt; がFalseに設定されている場合は無視されます。リグレッサが正規化されている場合、これによりハイパーパラメータがより堅牢になり、サンプル数にほとんど依存しないことに注意してください。同じプロパティは、標準化されたデータには無効です。ただし、標準化したい場合は、 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; を使用してから、 &lt;code&gt;normalize=False&lt;/code&gt; を使用して推定器で &lt;code&gt;fit&lt;/code&gt; を呼び出してください。</target>
        </trans-unit>
        <trans-unit id="016e8fefc2c3108a2b7a4351de86dada7ecbd68e" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">Trueの場合、リグレッサXは回帰前に正規化されます。このパラメーターは、 &lt;code&gt;fit_intercept&lt;/code&gt; がFalseに設定されている場合は無視されます。リグレッサが正規化されている場合、これによりハイパーパラメータがより堅牢になり、サンプル数にほとんど依存しないことに注意してください。同じプロパティは、標準化されたデータには無効です。ただし、標準化したい場合は、 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; を使用してから、 &lt;code&gt;normalize=False&lt;/code&gt; を使用して推定器で &lt;code&gt;fit&lt;/code&gt; を呼び出してください。</target>
        </trans-unit>
        <trans-unit id="c3fdb7e36f654834d666d698b8cb7219451a4481" translate="yes" xml:space="preserve">
          <source>If True, the return value will be an array of integers, rather than a boolean mask.</source>
          <target state="translated">True の場合、戻り値はブール値のマスクではなく、整数の配列になります。</target>
        </trans-unit>
        <trans-unit id="d17d4bbcdf2df4ef33c01a7114516c2e4e90d7d8" translate="yes" xml:space="preserve">
          <source>If True, the standard-deviation of the predictive distribution at the query points is returned along with the mean.</source>
          <target state="translated">Trueの場合,問い合わせ点における予測分布の標準偏差が平均とともに返されます.</target>
        </trans-unit>
        <trans-unit id="3f294130716f356d91654999eee9757bd2ba6c1c" translate="yes" xml:space="preserve">
          <source>If True, the support of robust location and covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">Trueの場合、ロバストな位置推定値と共分散推定値のサポートが計算され、データをセンタリングせずに共分散推定値が再計算されます。平均が有意にゼロに等しいが、正確にはゼロではないデータを扱うのに便利です。Falseの場合,ロバストな位置と共分散は,追加の処理をせずにFastMCDアルゴリズムを用いて直接計算されます.</target>
        </trans-unit>
        <trans-unit id="667a36b1a1b94aa0d760aa610f277fcd1918ae0a" translate="yes" xml:space="preserve">
          <source>If True, the support of the robust location and the covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">Trueの場合、ロバスト位置と共分散推定値のサポートが計算され、データをセンタリングせずに、そこから共分散推定値が再計算されます。平均が有意にゼロに等しいが、正確にはゼロではないデータを扱うのに便利です。Falseの場合,ロバストな位置と共分散は,追加の処理をせずにFastMCDアルゴリズムを用いて直接計算されます.</target>
        </trans-unit>
        <trans-unit id="28346b69fb6f1a64d688b2ef42aabb524b6563d8" translate="yes" xml:space="preserve">
          <source>If True, then X will be converted to a 2-dimensional NumPy array or sparse matrix. If the conversion is not possible an exception is raised.</source>
          <target state="translated">Trueの場合,Xは2次元のNumPy配列または疎な行列に変換されます.変換できない場合は例外が発生します。</target>
        </trans-unit>
        <trans-unit id="5c0fef9c1e6748fc4d3cb84e62459147a039a4fd" translate="yes" xml:space="preserve">
          <source>If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be &amp;lt; n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless.</source>
          <target state="translated">Trueの場合、固有値がゼロのすべてのコンポーネントが削除されるため、出力のコンポーネントの数は&amp;lt;n_components（数値の不安定性のためにゼロになる場合もあります）になる場合があります。n_componentsがNoneの場合、このパラメーターは無視され、固有値がゼロのコンポーネントは関係なく削除されます。</target>
        </trans-unit>
        <trans-unit id="42874c4e7adb064c98a0fb44835161462f985220" translate="yes" xml:space="preserve">
          <source>If True, then compute normalized Laplacian.</source>
          <target state="translated">真であれば、正規化されたラプラシアンを計算します。</target>
        </trans-unit>
        <trans-unit id="7818bd11ce52f2496690f2a19701e1fdc3bace9d" translate="yes" xml:space="preserve">
          <source>If True, transpose the downloaded data array.</source>
          <target state="translated">True の場合、ダウンロードしたデータ配列を転置します。</target>
        </trans-unit>
        <trans-unit id="09f8cdcb36703e0413a45867eb062d48bc42e4a4" translate="yes" xml:space="preserve">
          <source>If True, validation for finiteness will be skipped, saving time, but leading to potential crashes. If False, validation for finiteness will be performed, avoiding error. Global default: False.</source>
          <target state="translated">True の場合、有限性の検証はスキップされ、時間は節約されますが、クラッシュの可能性があります。False の場合、有限性の検証が実行され、エラーを回避します。グローバルなデフォルト。False。</target>
        </trans-unit>
        <trans-unit id="f770d204acb3934762188e63b6bd0977cfe619aa" translate="yes" xml:space="preserve">
          <source>If True, will return the parameters for this estimator and contained subobjects that are estimators.</source>
          <target state="translated">Trueの場合、この推定子のパラメータと、その推定子である含まれているサブオブジェクトを返します。</target>
        </trans-unit>
        <trans-unit id="edc518974aa9f8ea115d0f36469bc2b1e2cd15a2" translate="yes" xml:space="preserve">
          <source>If True, will return the query_id array for each file.</source>
          <target state="translated">True の場合、各ファイルの query_id 配列を返します。</target>
        </trans-unit>
        <trans-unit id="80fdba1026cc980884a84dfa72ad1747c034afc6" translate="yes" xml:space="preserve">
          <source>If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.</source>
          <target state="translated">X と y が np.float64 の C 次数で連続した配列ではなく,X が scipy.sparse.csr_matrix でない場合,X と y はコピーされます.</target>
        </trans-unit>
        <trans-unit id="a601440183ce5a872479c357e441563196aab652" translate="yes" xml:space="preserve">
          <source>If X is a dense array, then the other methods will not support sparse matrices as input.</source>
          <target state="translated">X が密な配列である場合,他のメソッドは入力として疎な行列をサポートしません.</target>
        </trans-unit>
        <trans-unit id="efab9063b47a2afb85758f067069188b21b34f3e" translate="yes" xml:space="preserve">
          <source>If X is encoded as a CSR matrix.</source>
          <target state="translated">XがCSR行列として符号化されている場合。</target>
        </trans-unit>
        <trans-unit id="da96e9fabf18fc39905756390120e4a7a1e09f97" translate="yes" xml:space="preserve">
          <source>If X is not a C-ordered contiguous array it is copied.</source>
          <target state="translated">X が C 次の連続配列でない場合はコピーされます.</target>
        </trans-unit>
        <trans-unit id="9cc8f34afbd30e04cc65d91f1b233abc1c382996" translate="yes" xml:space="preserve">
          <source>If X is not an array of floating values;</source>
          <target state="translated">Xが浮動小数点数の配列でない場合。</target>
        </trans-unit>
        <trans-unit id="e7ca7ce1d419c3d60265041304210343f2e8b91d" translate="yes" xml:space="preserve">
          <source>If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational basis: we want to learn loadings L and a set of components C such that &lt;em&gt;X = L C&lt;/em&gt;. Different criteria exist to choose the components</source>
          <target state="translated">Xが多変量データである場合、解決しようとしている問題は、別の観測に基づいてデータを書き換えることです。つまり、負荷Lと、&lt;em&gt;X = LCである&lt;/em&gt;ようなコンポーネントのセットCを学習したいとします。コンポーネントを選択するためのさまざまな基準が存在します</target>
        </trans-unit>
        <trans-unit id="4691b6eeb6f44a63af5f24ac30dc066ab023aa61" translate="yes" xml:space="preserve">
          <source>If X is sparse and &lt;code&gt;missing_values=0&lt;/code&gt;;</source>
          <target state="translated">Xがスパースで、 &lt;code&gt;missing_values=0&lt;/code&gt; の場合 ;</target>
        </trans-unit>
        <trans-unit id="5bf131136f9283115170d3f032466f07678834a5" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise distance between the arrays from both X and Y.</source>
          <target state="translated">Y が与えられた場合(デフォルトは None),返される行列は,X と Y の両方からの配列間のペアワイズ距離です.</target>
        </trans-unit>
        <trans-unit id="a6e7fe3e345be28d5e67984fa49ad01aeaa444dd" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise kernel between the arrays from both X and Y.</source>
          <target state="translated">Y が与えられた場合(デフォルトは None),返される行列は,X と Y の両方の配列間のペアワイズカーネルとなります.</target>
        </trans-unit>
        <trans-unit id="15e0fd61e3b85d8ebad2fc2135f6d5822723ce41" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}\) is the estimated target output, \(y\) the corresponding (correct) target output, and \(Var\) is &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt;, the square of the standard deviation, then the explained variance is estimated as follow:</source>
          <target state="translated">\（\ハット{Y}は\）推定ターゲット出力された場合、\（Y \）に対応する（正しい）目標出力、および\（VAR \）は&lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;分散&lt;/a&gt;、標準偏差の平方、次に説明される分散であります次のように推定されます：</target>
        </trans-unit>
        <trans-unit id="c8fb389b8a60a2b57fc22c9161412c484a73dd18" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the 0-1 loss \(L_{0-1}\) is defined as:</source>
          <target state="translated">\(i\)th サンプルの予測値であり、対応する真の値であるならば、0-1の損失は、0-1の損失と定義される。</target>
        </trans-unit>
        <trans-unit id="01fda7f04ce93ad5d6b5843c80c53ee91e04866d" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the fraction of correct predictions over \(n_\text{samples}\) is defined as</source>
          <target state="translated">\(i\)th サンプルの予測値であり、対応する真の値であるとすると、正しい予測値の割合は、次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="af46aeec0a0c654990b43c84ec26ca8a3817bbd3" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the median absolute error (MedAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\(i\)th サンプルの予測値であり、対応する真の値であるならば、\(y_i\)th サンプル上で推定された中央絶対誤差(MedAE)は、次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="27ab05c62dcfc0b1ca98228a2c106c2bd25d72f6" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the score R&amp;sup2; estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\（\ hat {y} _i \）が\（i \）番目のサンプルの予測値であり、\（y_i \）が対応する真の値である場合、\（n _ {\ text {サンプル}} \）は次のように定義されます</target>
        </trans-unit>
        <trans-unit id="e9be139031431f33624d9549cf24272bbec27cad" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean absolute error (MAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\(i\)th サンプルの予測値であり、対応する真の値であるとすると、平均絶対誤差(MAE)は、次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="b7836e2114e345225a74c22cbe0d3b5d52c8f253" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared error (MSE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\(i\)th サンプルの予測値であり、対応する真の値であるとすると、平均二乗誤差(MSE)は、次のように定義される。</target>
        </trans-unit>
        <trans-unit id="1cf47fc0a0aaaffd1c0a818b8704218b8ae722c1" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\(i\)th サンプルの予測値であり、対応する真の値であるとすると、平均二乗対数誤差(MSLE)は、次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="f295fa8851d145ac326bc63b92809e2fbf3d1f72" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_j\) is the predicted value for the \(j\)-th label of a given sample, \(y_j\) is the corresponding true value, and \(n_\text{labels}\) is the number of classes or labels, then the Hamming loss \(L_{Hamming}\) between two samples is defined as:</source>
          <target state="translated">\(\(j)番目のラベルの予測値、\(y_\_j)が対応する真の値、\(n_\text{labels})がクラス数である場合、2つのサンプル間のハミング損失は、次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="7fa4bf510f83c73a55e8dec038abaacf960351ca" translate="yes" xml:space="preserve">
          <source>If \(c_0 = 0\) the kernel is said to be homogeneous.</source>
          <target state="translated">If \(c_0=0)は、カーネルが均質であることを示している。</target>
        </trans-unit>
        <trans-unit id="c7d07701826b4f4c8efa455b46a49990993930f2" translate="yes" xml:space="preserve">
          <source>If \(h_i\) is given, the above equation automatically implies the following probabilistic interpretation:</source>
          <target state="translated">\(h_i)が与えられると、上の式は自動的に次のような確率的解釈をする。</target>
        </trans-unit>
        <trans-unit id="9d9af8bc9b90a6fbf0d539b126efbd694bdaddf6" translate="yes" xml:space="preserve">
          <source>If \(y_i\) is the true value of the \(i\)-th sample, and \(w_i\) is the corresponding sample weight, then we adjust the sample weight to:</source>
          <target state="translated">\(y_i\)th サンプルの真の値と、対応するサンプルの重さを 合わせて、重さを調整します。</target>
        </trans-unit>
        <trans-unit id="4c76ddad0ef7a743f202685a0861880cbc2062e2" translate="yes" xml:space="preserve">
          <source>If \(y_w\) is the predicted decision for true label and \(y_t\) is the maximum of the predicted decisions for all other labels, where predicted decisions are output by decision function, then multiclass hinge loss is defined by:</source>
          <target state="translated">\(y_w\)が真のラベルの予測決定であり、\(y_t\)が他のラベルの予測決定の最大値である場合、予測決定が決定関数で出力されるので、マルチクラスヒンジ損失は次のように定義される。</target>
        </trans-unit>
        <trans-unit id="b8371056060d53aef38082b274a12c6e92dd981b" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">CSR,CSC,COO,BSR の疎な行列が供給され、 accept_sparse によって受け入れられた場合、 accept_large_sparse は、そのインデックスが 32 ビットの dtype で格納されている場合にのみ、その行列を受け入れます。</target>
        </trans-unit>
        <trans-unit id="ca2f554a4272574081b19f205bd8db66223aa9f8" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse=False will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">CSR,CSC,COO,BSR の疎な行列が与えられて accept_sparse で受け入れられる場合、 accept_large_sparse=False を指定すると、そのインデックスが 32 ビットの dtype で格納されている場合にのみ受け入れられるようになります。</target>
        </trans-unit>
        <trans-unit id="b0090a224443cfea422c2167734e98ec705e71a5" translate="yes" xml:space="preserve">
          <source>If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.</source>
          <target state="translated">callableが渡されると,未処理の生の入力から特徴のシーケンスを抽出するために利用されます.</target>
        </trans-unit>
        <trans-unit id="dcd8eacb988e0fa27afa1a7692943530b07747f6" translate="yes" xml:space="preserve">
          <source>If a callable is passed, it should take arguments X, k and and a random state and return an initialization.</source>
          <target state="translated">callable が渡された場合、引数 X,k とランダムな状態を取り、初期化を返します。</target>
        </trans-unit>
        <trans-unit id="15b5ddf5d35e7b6d7436896e31247316b726ba30" translate="yes" xml:space="preserve">
          <source>If a float, that value is added to all values in the contingency matrix. This helps to stop NaN propagation. If &lt;code&gt;None&lt;/code&gt;, nothing is adjusted.</source>
          <target state="translated">浮動小数点数の場合、その値は偶発行列のすべての値に追加されます。これは、NaNの伝播を停止するのに役立ちます。 &lt;code&gt;None&lt;/code&gt; の場合、何も調整されません。</target>
        </trans-unit>
        <trans-unit id="aec58020de2b924f9656034ee47c95a7ace302db" translate="yes" xml:space="preserve">
          <source>If a list is passed it&amp;rsquo;s expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">リストが渡される場合、リストはn_targetsのような配列の1つであると予想されます。パスに沿った係数の変動値。 &lt;code&gt;fit_path&lt;/code&gt; パラメータが &lt;code&gt;False&lt;/code&gt; の場合は存在しません。</target>
        </trans-unit>
        <trans-unit id="8e3b6cd9422926a607fefd39c3e9bd3020c06d14" translate="yes" xml:space="preserve">
          <source>If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if &lt;code&gt;analyzer == 'word'&lt;/code&gt;.</source>
          <target state="translated">リストの場合、そのリストにはストップワードが含まれていると見なされ、そのすべてが結果のトークンから削除されます。 &lt;code&gt;analyzer == 'word'&lt;/code&gt; 場合にのみ適用されます。</target>
        </trans-unit>
        <trans-unit id="58d1b02436a7aa9160e580f582400827e1ad046d" translate="yes" xml:space="preserve">
          <source>If a string, it is passed to _check_stop_list and the appropriate stop list is returned. &amp;lsquo;english&amp;rsquo; is currently the only supported string value. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">文字列の場合、_check_stop_listに渡され、適切なストップリストが返されます。「english」は現在サポートされている唯一の文字列値です。'english'にはいくつかの既知の問題があり、別の方法を検討する必要があります（&lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;ストップワードの使用を&lt;/a&gt;参照）。</target>
        </trans-unit>
        <trans-unit id="564b43bc82acf22a1de3b85bb28ac591ff97b4bf" translate="yes" xml:space="preserve">
          <source>If a string, this may be one of &amp;lsquo;nearest_neighbors&amp;rsquo;, &amp;lsquo;precomputed&amp;rsquo;, &amp;lsquo;rbf&amp;rsquo; or one of the kernels supported by &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt;.</source>
          <target state="translated">文字列の場合、これは「nearest_neighbors」、「precomputed」、「rbf」のいずれか、または &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt; でサポートされているカーネルのいずれかです。</target>
        </trans-unit>
        <trans-unit id="675cbfc234c52633edd36cba3388cd72c1b8e2d3" translate="yes" xml:space="preserve">
          <source>If a target is a classification outcome taking on values 0,1,&amp;hellip;,K-1, for node \(m\), representing a region \(R_m\) with \(N_m\) observations, let</source>
          <target state="translated">ターゲットが、ノード\（m \）の値0、1、&amp;hellip;、K-1をとる分類結果であり、\（N_m \）の観測値を持つ領域\（R_m \）を表す場合、</target>
        </trans-unit>
        <trans-unit id="373500a68bbbd934744d157d24ba37240f790a20" translate="yes" xml:space="preserve">
          <source>If affinity is &amp;ldquo;precomputed&amp;rdquo; X : array-like, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples.</source>
          <target state="translated">アフィニティが「事前計算された」Xである場合：配列のような形状（n_samples、n_samples）、Xをサンプルから計算された事前計算された隣接グラフとして解釈します。</target>
        </trans-unit>
        <trans-unit id="c306493486d7abaed871db69a1b0f3a0d3e230f4" translate="yes" xml:space="preserve">
          <source>If affinity is the adjacency matrix of a graph, this method can be used to find normalized graph cuts.</source>
          <target state="translated">親和性がグラフの隣接行列である場合、この方法は正規化されたグラフの切り口を見つけるために使用することができます。</target>
        </trans-unit>
        <trans-unit id="fef3ba1186af33eef8e244a6a4bb530d43ffdf84" translate="yes" xml:space="preserve">
          <source>If all examples are from the same class, it uses a one-class SVM.</source>
          <target state="translated">すべての例が同じクラスのものであれば、1クラスのSVMを使用します。</target>
        </trans-unit>
        <trans-unit id="b53805960d76925767243aca9c19243fc1b08d06" translate="yes" xml:space="preserve">
          <source>If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.</source>
          <target state="translated">すべてのパラメータがリストとして提示された場合、置換なしのサンプリングが実行されます。少なくとも1つのパラメータが分布として与えられている場合,置換によるサンプリングが使用される.連続パラメータには連続分布を使用することが強く推奨されます。</target>
        </trans-unit>
        <trans-unit id="4f219d1953670922fbbfe88159427df7222c9397" translate="yes" xml:space="preserve">
          <source>If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points \(x_i\), one may use the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm to the mapped data points \(\phi(x_i)\). The advantage of using \(k\) is that the mapping \(\phi\) never has to be calculated explicitly, allowing for arbitrary large features (even infinite).</source>
          <target state="translated">線形支持ベクトルマシンやPCAなどのアルゴリズムが、データ点のスカラー積にしか依存しない場合は、\(k(x_i,x_j)の値を使うことができます。この方法の利点は、マッピングを明示的に計算する必要がなく、任意の大きな特徴量(無限大)にも対応できることです。</target>
        </trans-unit>
        <trans-unit id="9fea95f95d0577b3d2b8dde1c99a4f5f11240b1d" translate="yes" xml:space="preserve">
          <source>If an exception is triggered, use &lt;code&gt;%debug&lt;/code&gt; to fire-up a post mortem ipdb session.</source>
          <target state="translated">例外がトリガーされた場合は、 &lt;code&gt;%debug&lt;/code&gt; を使用して事後分析のipdbセッションを起動します。</target>
        </trans-unit>
        <trans-unit id="5c4a826b768bf0ea44b0de0cf9a279c59410da1d" translate="yes" xml:space="preserve">
          <source>If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details.</source>
          <target state="translated">整数が与えられた場合、使用する alpha のグリッド上の点の数を固定します。リストが与えられた場合は、使用するグリッドを指定します。詳細はクラスdocstringのノートを参照してください。</target>
        </trans-unit>
        <trans-unit id="3f54c86c80b29ba93fdb4405121f2a742f42412e" translate="yes" xml:space="preserve">
          <source>If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.</source>
          <target state="translated">ndarrayが渡された場合、それは形状(n_clusters,n_features)であり、初期中心を与えなければなりません。</target>
        </trans-unit>
        <trans-unit id="7bf20b6ab9e24e0313be1f29e5bd87350c62fc72" translate="yes" xml:space="preserve">
          <source>If bandwidth is not given, it is determined using a heuristic based on the median of all pairwise distances. This will take quadratic time in the number of samples. The sklearn.cluster.estimate_bandwidth function can be used to do this more efficiently.</source>
          <target state="translated">帯域幅が与えられていない場合は、すべてのペアワイズ距離の中央値に基づくヒューリスティックを使用して決定されます。これにはサンプル数の2次的な時間がかかります.sklearn.cluster.estimate_bandwidth関数を使うとより効率的に行うことができます。</target>
        </trans-unit>
        <trans-unit id="307d133eac653119e68c3f800803dcc4776573b9" translate="yes" xml:space="preserve">
          <source>If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If &amp;lsquo;auto&amp;rsquo;, it is assigned to False for dense &lt;code&gt;X&lt;/code&gt; and to True for sparse &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">boolの場合は、すべてのフィーチャを離散と見なすか、連続と見なすかを決定します。配列の場合は、形状（n_features）のブールマスクか、離散フィーチャのインデックスを持つ配列のいずれかである必要があります。'auto'の場合、密 &lt;code&gt;X&lt;/code&gt; の場合はFalseに、疎 &lt;code&gt;X&lt;/code&gt; の場合は Trueに割り当てられます。</target>
        </trans-unit>
        <trans-unit id="14d26c2cb6ccf4f84a440b5eee94360749d30490" translate="yes" xml:space="preserve">
          <source>If boolean, whether or not to fit the isotonic regression with y increasing or decreasing.</source>
          <target state="translated">ブール値の場合,y を増加させるか減少させるかで等張回帰を適合させるかどうかを指定します.</target>
        </trans-unit>
        <trans-unit id="c7380002883f78b7443a4cf144369e2cdf9c7dd5" translate="yes" xml:space="preserve">
          <source>If bytes or files are given to analyze, this encoding is used to decode.</source>
          <target state="translated">解析にバイトやファイルが与えられた場合、このエンコーディングがデコードに使用されます。</target>
        </trans-unit>
        <trans-unit id="b1cd46fc8b5b18d3d8ec258c92a5832fe49ecb69" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the AMI is null:</source>
          <target state="translated">クラスのメンバが異なるクラスタに完全に分割されている場合、割り当ては完全に完全ではないので AMI は NULL になります。</target>
        </trans-unit>
        <trans-unit id="e6f2dbc2c288fdff952bc5d6d0612fad044f50c2" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the NMI is null:</source>
          <target state="translated">クラスのメンバーが異なるクラスタに完全に分割されている場合、割り当ては完全に完全ではないので、NMIはnullです。</target>
        </trans-unit>
        <trans-unit id="60b93fba5d2befe30dad173ef2989a32caf2707d" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the ARI is very low:</source>
          <target state="translated">クラスのメンバーが異なるクラスタに完全に分割されている場合、割り当てが完全に不完全であるため、ARIは非常に低くなります。</target>
        </trans-unit>
        <trans-unit id="e02bb35b2a969fdf2ff25b865a0b3ba938fb92a9" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null:</source>
          <target state="translated">クラス メンバが異なるクラスタ間で完全に分割されている場合、割り当ては完全に不完全であるため、V-Measure は NULL になります。</target>
        </trans-unit>
        <trans-unit id="d0974a75f074fb11fd0a08f495fdb803227dd0c6" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally random, hence the FMI is null:</source>
          <target state="translated">クラスのメンバーが異なるクラスタに完全に分割されている場合、割り当ては完全にランダムであり、したがって FMI はヌルです。</target>
        </trans-unit>
        <trans-unit id="3a4c44f6cadbe5141305e79bc3f8068062652c4d" translate="yes" xml:space="preserve">
          <source>If classes members are split across different clusters, the assignment cannot be complete:</source>
          <target state="translated">クラスのメンバーが異なるクラスタに分割されている場合、割り当てを完了することはできません。</target>
        </trans-unit>
        <trans-unit id="baf96abe9df5cd386826eafcd47454c9dcc36819" translate="yes" xml:space="preserve">
          <source>If copy is False, the affinity matrix is modified inplace by the algorithm, for memory efficiency</source>
          <target state="translated">copy が False の場合,メモリ効率を考慮して,アフィニティ行列はアルゴリズムによって代用されます.</target>
        </trans-unit>
        <trans-unit id="671e4e16873255449d2ba54f06975c272f73c34d" translate="yes" xml:space="preserve">
          <source>If density = &amp;lsquo;auto&amp;rsquo;, the value is set to the minimum density as recommended by Ping Li et al.: 1 / sqrt(n_features).</source>
          <target state="translated">Density = 'auto'の場合、値はPing Li et al。が推奨する最小密度に設定されます：1 / sqrt（n_features）。</target>
        </trans-unit>
        <trans-unit id="391517cb3cfce3ac9c6c32b7ceac049807282afc" translate="yes" xml:space="preserve">
          <source>If documents are pre-tokenized by an external package, then store them in files (or strings) with the tokens separated by whitespace and pass &lt;code&gt;analyzer=str.split&lt;/code&gt;</source>
          <target state="translated">ドキュメントが外部パッケージによって事前トークン化されている場合は、トークンを空白で区切ってファイル（または文字列）に保存し、 &lt;code&gt;analyzer=str.split&lt;/code&gt; を渡します。</target>
        </trans-unit>
        <trans-unit id="80416b24b5f24b22b80d50d90aa942e77a2dadfc" translate="yes" xml:space="preserve">
          <source>If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix reveals the biclusters on the diagonal. Here is an example of this structure where biclusters have higher average values than the other rows and columns:</source>
          <target state="translated">各行と各列が正確に 1 つのビクラスタに属している場合、データ行列の行と列を並べ替えると、対角線上のビクラスタがわかります。ビックラスターの平均値が他の行と列よりも高い場合のこの構造の例を以下に示します。</target>
        </trans-unit>
        <trans-unit id="1db9c10662b6d49b6b84ca8b7b7ce2a9580afa10" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default (the parameter is unspecified), the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">浮動小数点数の場合、0.0と1.0の間で、テスト分割に含めるデータセットの比率を表す必要があります。 intの場合、テストサンプルの絶対数を表します。 Noneの場合、値はトレインサイズの補数に設定されます。デフォルトでは（パラメーターは指定されていません）、値は0.1に設定されています。デフォルトはバージョン0.21で変更されます。 &lt;code&gt;train_size&lt;/code&gt; が指定されていない場合のみ0.1のまま &lt;code&gt;train_size&lt;/code&gt; 。それ以外の場合は、指定されたtrain_sizeを補完します。</target>
        </trans-unit>
        <trans-unit id="73ce93bb920d84bef49715afdf02f771938f8206" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">浮動小数点数の場合、0.0と1.0の間で、テスト分割に含めるデータセットの比率を表す必要があります。 intの場合、テストサンプルの絶対数を表します。 Noneの場合、値はトレインサイズの補数に設定されます。デフォルトでは、値は0.1に設定されています。デフォルトはバージョン0.21で変更されます。 &lt;code&gt;train_size&lt;/code&gt; が指定されていない場合のみ0.1のまま &lt;code&gt;train_size&lt;/code&gt; 。それ以外の場合は、指定されたtrain_sizeを補完します。</target>
        </trans-unit>
        <trans-unit id="aa74ab3ce21513c4e7c83e9b91dad63582c835b8" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.2. The default will change in version 0.21. It will remain 0.2 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">浮動小数点数の場合、0.0と1.0の間で、テスト分割に含めるデータセットの比率を表す必要があります。 intの場合、テストサンプルの絶対数を表します。 Noneの場合、値はトレインサイズの補数に設定されます。デフォルトでは、値は0.2に設定されています。デフォルトはバージョン0.21で変更されます。 &lt;code&gt;train_size&lt;/code&gt; が指定されていない場合のみ0.2のまま &lt;code&gt;train_size&lt;/code&gt; 。それ以外の場合は、指定されたtrain_sizeを補完します。</target>
        </trans-unit>
        <trans-unit id="dfe0bc4ba0825c6b9e54b3177711e714d6e28a2b" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">浮動小数点数の場合、0.0と1.0の間で、テスト分割に含めるデータセットの比率を表す必要があります。intの場合、テストサンプルの絶対数を表します。Noneの場合、値はトレインサイズの補数に設定されます。デフォルトでは、値は0.25に設定されています。デフォルトはバージョン0.21で変更されます。 &lt;code&gt;train_size&lt;/code&gt; が指定されていない場合のみ、0.25のまま &lt;code&gt;train_size&lt;/code&gt; 。それ以外の場合は、指定されたtrain_sizeを補完します。</target>
        </trans-unit>
        <trans-unit id="a3b7da8f21403a8e0fce55a369b8d82b4da5bfd1" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">float の場合は,0.0 から 1.0 の間の値で,訓練分割に含めるデータセットの割合を表します.int の場合は,訓練サンプルの絶対数を表します.None の場合,値は自動的にテストサイズの補数に設定されます.</target>
        </trans-unit>
        <trans-unit id="62b47f7a89d7c976d2813299381cdc4f4f5b3cad" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the groups to include in the train split. If int, represents the absolute number of train groups. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">float の場合は 0.0 から 1.0 の間で、列車分割に含めるグループの割合を表します。int の場合、列車分割に含めるグループの絶対数を表します。Noneの場合は,自動的にテストサイズの補数に設定されます.</target>
        </trans-unit>
        <trans-unit id="b0ffbda1c59db43809cf9245f33efa45a65a5c05" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;max_features&lt;/code&gt; is a fraction and &lt;code&gt;int(max_features * n_features)&lt;/code&gt; features are considered at each split.</source>
          <target state="translated">floatの場合、 &lt;code&gt;max_features&lt;/code&gt; は分数であり、 &lt;code&gt;int(max_features * n_features)&lt;/code&gt; 機能は各分割で考慮されます。</target>
        </trans-unit>
        <trans-unit id="47d0c1ebc9dc44a7018a0ce88d0d45201068f385" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; are the minimum number of samples for each node.</source>
          <target state="translated">floatの場合、 &lt;code&gt;min_samples_leaf&lt;/code&gt; は分数で、 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; は各ノードの最小サンプル数です。</target>
        </trans-unit>
        <trans-unit id="c217707834c84f95b745c6fd735e46ef1d5cb29d" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; is the minimum number of samples for each node.</source>
          <target state="translated">floatの場合、 &lt;code&gt;min_samples_leaf&lt;/code&gt; は分数で、 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; は各ノードのサンプルの最小数です。</target>
        </trans-unit>
        <trans-unit id="f5813e9c1656f619ed6234eb86815e1c76ec9071" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; are the minimum number of samples for each split.</source>
          <target state="translated">floatの場合、 &lt;code&gt;min_samples_split&lt;/code&gt; は分数で、 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; は各分割の最小サンプル数です。</target>
        </trans-unit>
        <trans-unit id="c32957ea85344bbb6b41aa874b4a5e7763ff6b76" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; is the minimum number of samples for each split.</source>
          <target state="translated">floatの場合、 &lt;code&gt;min_samples_split&lt;/code&gt; は分数で、 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; は各分割のサンプルの最小数です。</target>
        </trans-unit>
        <trans-unit id="81fe24c94c96599e85080c0cc195542bdb1ce722" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; features.</source>
          <target state="translated">floatの場合、 &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; フィーチャーを描画します。</target>
        </trans-unit>
        <trans-unit id="a2b1676fcae8577852e20614ce418906e2f79102" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; samples.</source>
          <target state="translated">floatの場合、 &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; サンプルを描画します。</target>
        </trans-unit>
        <trans-unit id="f234188a9694365b66e83538b40de1fa4059a074" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration.</source>
          <target state="translated">1以上の場合、 &lt;code&gt;step&lt;/code&gt; は各反復で削除する特徴の（整数の）数に対応します。（0.0、1.0）以内の場合、 &lt;code&gt;step&lt;/code&gt; は各反復で削除するフィーチャのパーセンテージ（切り捨て）に対応します。</target>
        </trans-unit>
        <trans-unit id="d3aeb6a39c457b69bdde12a943780461d08c388d" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration. Note that the last iteration may remove fewer than &lt;code&gt;step&lt;/code&gt; features in order to reach &lt;code&gt;min_features_to_select&lt;/code&gt;.</source>
          <target state="translated">1以上の場合、 &lt;code&gt;step&lt;/code&gt; は各反復で削除する特徴の（整数の）数に対応します。（0.0、1.0）以内の場合、 &lt;code&gt;step&lt;/code&gt; は各反復で削除するフィーチャのパーセンテージ（切り捨て）に対応します。最後の反復では、 &lt;code&gt;min_features_to_select&lt;/code&gt; に到達するために、 &lt;code&gt;step&lt;/code&gt; フィーチャよりも少ない数のフィーチャが削除される場合があることに注意してください。</target>
        </trans-unit>
        <trans-unit id="1351e34960b08f78869e356e9d9129a529a17168" translate="yes" xml:space="preserve">
          <source>If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">QDAモデルで、共分散行列が対角であると想定する場合、入力は各クラスで条件付きで独立していると想定され、結果の分類器はガウスナイーブベイズ分類器&lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt; &lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt; と同等&lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="47884bf7f490577d7025ceb970813cb997acfc82" translate="yes" xml:space="preserve">
          <source>If init=&amp;rsquo;custom&amp;rsquo;, it is used as initial guess for the solution.</source>
          <target state="translated">init = 'custom'の場合、ソリューションの初期推定として使用されます。</target>
        </trans-unit>
        <trans-unit id="5b9c788e52ff7adb618d2b8cf3dd396e088800c8" translate="yes" xml:space="preserve">
          <source>If int, it is the total number of points equally divided among clusters. If array-like, each element of the sequence indicates the number of samples per cluster.</source>
          <target state="translated">int の場合は,クラスタ間で等分されたポイントの総数を表します.配列のような場合、シーケンスの各要素はクラスタごとのサンプル数を示します。</target>
        </trans-unit>
        <trans-unit id="2403d0bd3d2ac8c8a9756c4cde9cd9202cbe9926" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。</target>
        </trans-unit>
        <trans-unit id="676c2454734bf9216c5797d643595f0b850b4e7a" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Note that different initializations might result in different local minima of the cost function.</source>
          <target state="translated">intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。初期化が異なると、コスト関数の極小値が異なる可能性があることに注意してください。</target>
        </trans-unit>
        <trans-unit id="a1933900181bd8e24f0237ebecc7a06b2ef8b486" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Only used when &lt;code&gt;svd_method&lt;/code&gt; equals &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;svd_method&lt;/code&gt; が 'randomized'に等しい場合にのみ使用されます。</target>
        </trans-unit>
        <trans-unit id="4914440c8828885c0b1efea7679daefd5f1e4eaf" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;eigen_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;eigen_solver&lt;/code&gt; == 'arpack'の場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="90657492e3c46d11fb3a1c799a4569e4854211ed" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; == True.</source>
          <target state="translated">intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;shuffle&lt;/code&gt; == Trueの場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="7e39c8ed74a38762200c178da772671eaa6b3f52" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; is True.</source>
          <target state="translated">intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;shuffle&lt;/code&gt; がTrueの場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="636d95a8f3edcd08bb7a122de05f8944c1a330ee" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;solver&lt;/code&gt; == 'arpack'の場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="d8bb54edf7a318cb4f3734b3f7721b6c840db8b1" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;svd_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo; or &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">intの場合、random_stateは乱数ジェネレータによって使用されるシードです。RandomStateインスタンスの場合、random_stateは乱数ジェネレーターです。Noneの場合、乱数ジェネレータはnp.randomによって使用される &lt;code&gt;np.random&lt;/code&gt; インスタンスです。 &lt;code&gt;svd_solver&lt;/code&gt; == 'arpack'または 'randomized'の場合に使用されます。</target>
        </trans-unit>
        <trans-unit id="49fc99a0f8ec79ab5cf6633c8b91ad397447b0ae" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Note that this is used by subsampling and smoothing noise.</source>
          <target state="translated">intの場合、random_stateは乱数生成器が使用するシードです。 RandomStateインスタンスの場合、random_stateは乱数生成器です。これは、サブサンプリングやノイズの平滑化などで使用されることに注意してください。</target>
        </trans-unit>
        <trans-unit id="c8faba5e55a8f0e899120109354364cac8c2354b" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;max_features&lt;/code&gt; features at each split.</source>
          <target state="translated">intの場合、分割ごとに &lt;code&gt;max_features&lt;/code&gt; 機能を検討します。</target>
        </trans-unit>
        <trans-unit id="79438cfe8b8de1684467307814da6af61cdfe6ba" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_leaf&lt;/code&gt; as the minimum number.</source>
          <target state="translated">intの場合、 &lt;code&gt;min_samples_leaf&lt;/code&gt; を最小数と見なします。</target>
        </trans-unit>
        <trans-unit id="69e04ca78560d3ef445be4d724f5c0cc8198187a" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_split&lt;/code&gt; as the minimum number.</source>
          <target state="translated">intの場合、 &lt;code&gt;min_samples_split&lt;/code&gt; を最小数と見なします。</target>
        </trans-unit>
        <trans-unit id="a8d276c242fbe315ce14903af35e7ebf9a0c3619" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_features&lt;/code&gt; features.</source>
          <target state="translated">intの場合、 &lt;code&gt;max_features&lt;/code&gt; 機能を描画します。</target>
        </trans-unit>
        <trans-unit id="0771ca4ef29dd427aac0ffda56943aa541e3af54" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_samples&lt;/code&gt; samples.</source>
          <target state="translated">intの場合、 &lt;code&gt;max_samples&lt;/code&gt; サンプルを描画します。</target>
        </trans-unit>
        <trans-unit id="4430c154e22158c0d6435f75a2d640312ee73ab2" translate="yes" xml:space="preserve">
          <source>If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, \(u_1\) and \(v_1\). are discarded. From now on, the &amp;ldquo;first&amp;rdquo; singular vectors refers to \(u_2 \dots u_{p+1}\) and \(v_2 \dots v_{p+1}\) except in the case of log normalization.</source>
          <target state="translated">対数正規化が使用された場合、すべての特異ベクトルは意味があります。ただし、独立した正規化または双安定化が使用された場合、最初の特異ベクトル、\（u_1 \）および\（v_1 \）。破棄されます。これ以降、「最初の」特異ベクトルは、ログ正規化の場合を除いて、\（u_2 \ dots u_ {p + 1} \）および\（v_2 \ dots v_ {p + 1} \）を参照します。</target>
        </trans-unit>
        <trans-unit id="f38434d38fce86523bd80aac7625c65019a5f868" translate="yes" xml:space="preserve">
          <source>If max_samples is larger than the number of samples provided, all samples will be used for all trees (no sampling).</source>
          <target state="translated">max_samples が提供されたサンプル数よりも大きい場合、すべてのツリーにすべてのサンプルが使用されます(サンプリングなし)。</target>
        </trans-unit>
        <trans-unit id="0512919782ceb898f5e82137605d37f1918f7fe8" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;auto&amp;rdquo;, the ratio of n_samples / n_population is used to determine which algorithm to use: If ratio is between 0 and 0.01, tracking selection is used. If ratio is between 0.01 and 0.99, numpy.random.permutation is used. If ratio is greater than 0.99, reservoir sampling is used. The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">method ==&amp;ldquo; auto&amp;rdquo;の場合、n_samples / n_populationの比率を使用して、使用するアルゴリズムを決定します。比率が0〜0.01の場合、追跡選択が使用されます。比率が0.01から0.99の場合、numpy.random.permutationが使用されます。比率が0.99より大きい場合、貯水池サンプリングが使用されます。選択した整数の順序は定義されていません。ランダムな順序が必要な場合は、選択したサブセットをシャッフルする必要があります。</target>
        </trans-unit>
        <trans-unit id="aa3ae5990e2e00fef99c00cc07049cdbc9d8e931" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;pool&amp;rdquo;, a pool based algorithm is particularly fast, even faster than the tracking selection method. Hovewer, a vector containing the entire population has to be initialized. If n_samples ~ n_population, the reservoir sampling method is faster.</source>
          <target state="translated">method ==&amp;ldquo; pool&amp;rdquo;の場合、プールベースのアルゴリズムは特に高速で、追跡選択メソッドよりもさらに高速です。Hovewer、母集団全体を含むベクトルを初期化する必要があります。n_samples〜n_populationの場合、リザーバーサンプリング方式の方が高速です。</target>
        </trans-unit>
        <trans-unit id="6916dcd6d6c8f00e1ac0ef865ab4c75ff38ebedf" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;reservoir_sampling&amp;rdquo;, a reservoir sampling algorithm is used which is suitable for high memory constraint or when O(&lt;code&gt;n_samples&lt;/code&gt;) ~ O(&lt;code&gt;n_population&lt;/code&gt;). The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">もし方法==「reservoir_sampling」Aリザーバサンプリングアルゴリズム高いメモリ制約場合やO（適しているが使用される &lt;code&gt;n_samples&lt;/code&gt; ）〜O（ &lt;code&gt;n_population&lt;/code&gt; ）。選択した整数の順序は定義されていません。ランダムな順序が必要な場合は、選択したサブセットをシャッフルする必要があります。</target>
        </trans-unit>
        <trans-unit id="0dea8a6c91cef90e0430014d895bb3954c8fb19c" translate="yes" xml:space="preserve">
          <source>If method ==&amp;rdquo;tracking_selection&amp;rdquo;, a set based implementation is used which is suitable for &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt;.</source>
          <target state="translated">method ==&amp;rdquo; tracking_selection&amp;rdquo;の場合、 &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt; に適したセットベースの実装が使用されます。</target>
        </trans-unit>
        <trans-unit id="79ae0bba548597b9902c41c70fbd6bf9602e8534" translate="yes" xml:space="preserve">
          <source>If metric is &amp;lsquo;precomputed&amp;rsquo;, Y is ignored and X is returned.</source>
          <target state="translated">メトリックが「事前計算」されている場合、Yは無視され、Xが返されます。</target>
        </trans-unit>
        <trans-unit id="ca8cb47e72e166fd730a116349e050254e6876d5" translate="yes" xml:space="preserve">
          <source>If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy&amp;rsquo;s metrics, but is less efficient than passing the metric name as a string.</source>
          <target state="translated">メトリックが呼び出し可能な関数の場合、インスタンス（行）の各ペアで呼び出され、結果の値が記録されます。呼び出し可能オブジェクトは、2つの配列を入力として取り、それらの間の距離を示す1つの値を返す必要があります。これはScipyのメトリックスでは機能しますが、メトリックス名を文字列として渡すよりも効率的ではありません。</target>
        </trans-unit>
        <trans-unit id="27305db22802f1bb3d3e2d60db076f6f0d275369" translate="yes" xml:space="preserve">
          <source>If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen.</source>
          <target state="translated">ミニバッチ k-means が使用される場合、最良の初期化が選択され、アルゴリズムは一度だけ実行される。そうでない場合は、アルゴリズムは各初期化ごとに実行され、最良の解が選択されます。</target>
        </trans-unit>
        <trans-unit id="16e6684b95c26e373af21b6c0d2ea50b705c0505" translate="yes" xml:space="preserve">
          <source>If multioutput is &amp;lsquo;raw_values&amp;rsquo;, then mean absolute error is returned for each output separately. If multioutput is &amp;lsquo;uniform_average&amp;rsquo; or an ndarray of weights, then the weighted average of all output errors is returned.</source>
          <target state="translated">multioutputが 'raw_values'の場合​​、平均絶対エラーが各出力に対して個別に返されます。multioutputが 'uniform_average'または重みのndarrayの場合、すべての出力エラーの加重平均が返されます。</target>
        </trans-unit>
        <trans-unit id="caf3d42023b133b9efdfbb493fc66df503092e71" translate="yes" xml:space="preserve">
          <source>If no scoring is specified and the estimator has no score function, we can either return None or raise an exception.</source>
          <target state="translated">スコアリングが指定されておらず、推定子にスコア関数がない場合は、Noneを返すか、例外を発生させることができます。</target>
        </trans-unit>
        <trans-unit id="5519c1c6825bf59bfd06c08f68bb64b64ee1a0ae" translate="yes" xml:space="preserve">
          <source>If no valid consensus set could be found. This occurs if &lt;code&gt;is_data_valid&lt;/code&gt; and &lt;code&gt;is_model_valid&lt;/code&gt; return False for all &lt;code&gt;max_trials&lt;/code&gt; randomly chosen sub-samples.</source>
          <target state="translated">有効なコンセンサスセットが見つからなかった場合。これは、ランダムに選択されたすべての &lt;code&gt;max_trials&lt;/code&gt; サブサンプルに対して &lt;code&gt;is_data_valid&lt;/code&gt; および &lt;code&gt;is_model_valid&lt;/code&gt; がFalseを返した場合に発生します。</target>
        </trans-unit>
        <trans-unit id="e48a960f323664c14eed43108cfafa1159796090" translate="yes" xml:space="preserve">
          <source>If normalize is &lt;code&gt;True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.</source>
          <target state="translated">normalizeが &lt;code&gt;True&lt;/code&gt; の場合、誤分類の割合（float）を返します。それ以外の場合は、誤分類の数（int）を返します。最高のパフォーマンスは0です。</target>
        </trans-unit>
        <trans-unit id="d93355ca0c397a92c0eb63483bbae0b531d00cf1" translate="yes" xml:space="preserve">
          <source>If not &lt;code&gt;None&lt;/code&gt;, the standardized partial AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt; over the range [0, max_fpr] is returned.</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; でない場合、[0、max_fpr]の範囲の標準化された部分AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3&lt;/a&gt; ]が返されます。</target>
        </trans-unit>
        <trans-unit id="954d968337062d6fae676f5915fb0dc48db9ccef" translate="yes" xml:space="preserve">
          <source>If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</source>
          <target state="translated">Noneでない場合は、コーパス全体で用語の頻度が高い順に並べられた上位のmax_featuresのみを考慮した語彙を構築します。</target>
        </trans-unit>
        <trans-unit id="6b6dff5f6d294c2bdbfe5ee6b0ee56319193880c" translate="yes" xml:space="preserve">
          <source>If not None, data is split in a stratified fashion, using this as the class labels.</source>
          <target state="translated">None でない場合は、これをクラス・ラベルとして使用して、データを階層化して分割します。</target>
        </trans-unit>
        <trans-unit id="d9fe4271c08ca870db7143f08e0938aa49f2d1d0" translate="yes" xml:space="preserve">
          <source>If not None, set the highest value of the fit to y_max.</source>
          <target state="translated">None でない と きは、 はめ込みの最高値を y_max に設定します。</target>
        </trans-unit>
        <trans-unit id="3c138b5d1ed12eddb3226ed7535814059b7a615c" translate="yes" xml:space="preserve">
          <source>If not None, set the lowest value of the fit to y_min.</source>
          <target state="translated">None でない と きは、 はめ込みの最小値を y_min に設定します。</target>
        </trans-unit>
        <trans-unit id="ebcf44116da09ed76a723aed5cadbe6d4ed2530d" translate="yes" xml:space="preserve">
          <source>If not None, this argument is passed as &lt;code&gt;sample_weight&lt;/code&gt; keyword argument to the &lt;code&gt;score&lt;/code&gt; method of the final estimator.</source>
          <target state="translated">Noneでない場合、この引数は、 &lt;code&gt;sample_weight&lt;/code&gt; キーワード引数として最終推定器の &lt;code&gt;score&lt;/code&gt; メソッドに渡されます。</target>
        </trans-unit>
        <trans-unit id="f0f7d0b7263b16cf926e8314af8096b9ae6c9066" translate="yes" xml:space="preserve">
          <source>If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below).</source>
          <target state="translated">もし指定されていなければ、帯域幅はsklearn.cluster.estimate_bandwidthを使って推定されます;スケーラビリティのヒントについては、その関数のドキュメントを参照してください (下記の注意事項も参照してください)。</target>
        </trans-unit>
        <trans-unit id="3fc57ade66d3b29b2e5dacfcee394aac7f4ec951" translate="yes" xml:space="preserve">
          <source>If not provided, labels will be inferred from y_true. If &lt;code&gt;labels&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; has shape (n_samples,) the labels are assumed to be binary and are inferred from &lt;code&gt;y_true&lt;/code&gt;. .. versionadded:: 0.18</source>
          <target state="translated">指定しない場合、ラベルはy_trueから推定されます。場合 &lt;code&gt;labels&lt;/code&gt; ありません &lt;code&gt;None&lt;/code&gt; と &lt;code&gt;y_pred&lt;/code&gt; 形状（n_samplesは、）を有するラベルがバイナリであると想定されるから推測される &lt;code&gt;y_true&lt;/code&gt; 。.. versionadded :: 0.18</target>
        </trans-unit>
        <trans-unit id="d3a1f4e96f04c6f8dfd4835d50de53587903b2e7" translate="yes" xml:space="preserve">
          <source>If one-of-K coding is applied to categorical features, this will include the constructed feature names but not the original ones.</source>
          <target state="translated">カテゴリ特徴に1-of-K符号化が適用されている場合、これは構築された特徴名を含みますが、元の特徴名は含みません。</target>
        </trans-unit>
        <trans-unit id="c3b8faf61102e14148418b48bf3dbb3389d54ef3" translate="yes" xml:space="preserve">
          <source>If only the diagonal of the auto-covariance is being used, the method &lt;code&gt;diag()&lt;/code&gt; of a kernel can be called, which is more computationally efficient than the equivalent call to &lt;code&gt;__call__&lt;/code&gt;: &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt;</source>
          <target state="translated">自動共分散の対角線のみが使用されている場合、カーネルのメソッド &lt;code&gt;diag()&lt;/code&gt; を呼び出すことができます。これは、 &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt; への同等の呼び出しよりも計算効率が高くなり &lt;code&gt;__call__&lt;/code&gt; ：np.diag（k（X、X））== k.diag（X）</target>
        </trans-unit>
        <trans-unit id="db7f35e5fc1dd73c10c86bdccb4a2449d5a89ec7" translate="yes" xml:space="preserve">
          <source>If order is &amp;lsquo;random&amp;rsquo; a random ordering will be used.</source>
          <target state="translated">順序が「ランダム」の場合、ランダムな順序が使用されます。</target>
        </trans-unit>
        <trans-unit id="f42275492b00fc14b5861ea85e0f4944992d0324" translate="yes" xml:space="preserve">
          <source>If passed, include the name of the estimator in warning messages.</source>
          <target state="translated">渡された場合は、警告メッセージにエスティメー タの名前を含める。</target>
        </trans-unit>
        <trans-unit id="908cd551a5eab6201799122746b2ad3d99f4a3d2" translate="yes" xml:space="preserve">
          <source>If positive, restrict regression coefficients to be positive</source>
          <target state="translated">正の場合、回帰係数を正に制限する</target>
        </trans-unit>
        <trans-unit id="c5484f943f94af044829e2453a87ea1beff675d6" translate="yes" xml:space="preserve">
          <source>If return_costs is True, the objective function and dual gap at each iteration are returned.</source>
          <target state="translated">return_costsがTrueの場合、各反復時の対物関数とデュアルギャップが返されます。</target>
        </trans-unit>
        <trans-unit id="3a07f641c209d2556442bcd652915ffb4ab857db" translate="yes" xml:space="preserve">
          <source>If safe is false, clone will fall back to a deep copy on objects that are not estimators.</source>
          <target state="translated">safeがfalseの場合、cloneは推定値ではないオブジェクトのディープコピーにフォールバックします。</target>
        </trans-unit>
        <trans-unit id="179d83839b7c246b21dd4fad6260ec3c338cc783" translate="yes" xml:space="preserve">
          <source>If seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError.</source>
          <target state="translated">seedがNoneの場合、np.randomによって使用されるRandomStateのシングルトンを返します。seed が int の場合は、seed でシードされた新しい RandomState インスタンスを返します。seed が既に RandomState インスタンスである場合は、それを返します。それ以外の場合は ValueError を発生させます。</target>
        </trans-unit>
        <trans-unit id="7108bbb3c9ecad70c2ad038e49ece7ce906a1c8f" translate="yes" xml:space="preserve">
          <source>If seq[i] is an int or a tuple with one int value, a one-way PDP is created; if seq[i] is a tuple of two ints, a two-way PDP is created. If feature_names is specified and seq[i] is an int, seq[i] must be &amp;lt; len(feature_names). If seq[i] is a string, feature_names must be specified, and seq[i] must be in feature_names.</source>
          <target state="translated">seq [i]がintまたは1つのint値を持つタプルである場合、一方向PDPが作成されます。seq [i]が2つの整数のタプルである場合、双方向PDPが作成されます。feature_namesが指定され、seq [i]がintの場合、seq [i]は&amp;lt;len（feature_names）でなければなりません。seq [i]が文字列の場合、feature_namesを指定し、seq [i]をfeature_namesに含める必要があります。</target>
        </trans-unit>
        <trans-unit id="b01ea458e6ed79ec076f21dd79564eecc3f7a882" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4</source>
          <target state="translated">「ランダム」に設定されている場合、ランダム係数は、デフォルトでフィーチャを順次ループするのではなく、反復ごとに更新されます。これ（「ランダム」に設定）は、特にtolが1e-4より大きい場合に、収束が大幅に速くなることがよくあります。</target>
        </trans-unit>
        <trans-unit id="7a4374896942a67a58d05d59133607fc7483d7a2" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4.</source>
          <target state="translated">「ランダム」に設定されている場合、ランダム係数は、デフォルトでフィーチャを順次ループするのではなく、反復ごとに更新されます。これ（「ランダム」に設定）は、特にtolが1e-4より大きい場合に、収束が大幅に速くなることがよくあります。</target>
        </trans-unit>
        <trans-unit id="d15f7a009cd3b6e81a757534913f0edc7a2b7947" translate="yes" xml:space="preserve">
          <source>If set to True, forces coefficients to be positive. (Only allowed when &lt;code&gt;y.ndim == 1&lt;/code&gt;).</source>
          <target state="translated">Trueに設定すると、係数が強制的に正になります。（ &lt;code&gt;y.ndim == 1&lt;/code&gt; 場合にのみ許可されます）。</target>
        </trans-unit>
        <trans-unit id="6bd31584b0a279bb6a357ab195ba9534cb5cad4e" translate="yes" xml:space="preserve">
          <source>If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged.</source>
          <target state="translated">Trueに設定されている場合、スコアはすべてのフォールドで平均化され、最良のスコアに対応するcoefsとCが取られ、これらのパラメータを使用して最終的な再フィットが行われます。そうでない場合は、ひだ全体で最高のスコアに対応するcoefs、切片、Cが平均化されます。</target>
        </trans-unit>
        <trans-unit id="f3d43f9f7c9e3af1ca6eddc0268b8ee91bb07ee3" translate="yes" xml:space="preserve">
          <source>If set, scikit-learn will attempt to limit the size of temporary arrays to this number of MiB (per job when parallelised), often saving both computation time and memory on expensive operations that can be performed in chunks. Global default: 1024.</source>
          <target state="translated">設定されている場合、scikit-learnは一時的な配列のサイズを(並列化されている場合はジョブごとに)この数のMiBに制限しようとします。グローバルデフォルトは1024です。</target>
        </trans-unit>
        <trans-unit id="cd9d66e1ab8be1fe689482ddb0cbca43b44a3950" translate="yes" xml:space="preserve">
          <source>If strictly positive, stop reading any new line of data once the position in the file has reached the (offset + length) bytes threshold.</source>
          <target state="translated">厳密に正の場合、ファイル内の位置が(オフセット+長さ)バイトのしきい値に達した時点で、データの新しい行の読み込みを停止します。</target>
        </trans-unit>
        <trans-unit id="af99c20b0f1015ebcecc8bfb6a50ca848ab08d15" translate="yes" xml:space="preserve">
          <source>If string, specifies the path that will contain the data. If file-like, data will be written to f. f should be opened in binary mode.</source>
          <target state="translated">文字列の場合は、データを格納するパスを指定します。ファイルライクな場合、データはfに書き込まれます。</target>
        </trans-unit>
        <trans-unit id="d25cbbfb18995acebbdc8e788e3994e16b21b8ae" translate="yes" xml:space="preserve">
          <source>If sum_over_features is False shape is (n_samples_X * n_samples_Y, n_features) and D contains the componentwise L1 pairwise-distances (ie. absolute difference), else shape is (n_samples_X, n_samples_Y) and D contains the pairwise L1 distances.</source>
          <target state="translated">sum_over_featuresがFalseの場合、形状は(n_samples_X*n_samples_Y,n_features)であり、Dは成分単位のL1のペアウィズ距離(すなわち絶対差)を含み、そうでない場合、形状は(n_samples_X,n_samples_Y)であり、DはペアウィズL1の距離を含む。</target>
        </trans-unit>
        <trans-unit id="1979731cc29c616c5ac5593ab888192599b2d46b" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;loss&lt;/code&gt; does not support probabilities.</source>
          <target state="translated">&lt;code&gt;loss&lt;/code&gt; が確率をサポートしていない場合。</target>
        </trans-unit>
        <trans-unit id="41f96f9118cd39448d94e68aca8aa6d327b368ef" translate="yes" xml:space="preserve">
          <source>If the algorithm is &amp;ldquo;deflation&amp;rdquo;, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge.</source>
          <target state="translated">アルゴリズムが「デフレ」の場合、n_iterはすべてのコンポーネントで実行される反復の最大数です。そうでなければ、それらは収束するために取られた反復の数です。</target>
        </trans-unit>
        <trans-unit id="b72ab6a8a780a6da86f798b7381c54dc2236b80c" translate="yes" xml:space="preserve">
          <source>If the algorithm stops before fully converging (because of &lt;code&gt;tol&lt;/code&gt; of &lt;code&gt;max_iter&lt;/code&gt;), &lt;code&gt;labels_&lt;/code&gt; and &lt;code&gt;means_&lt;/code&gt; will not be consistent, i.e. the &lt;code&gt;means_&lt;/code&gt; will not be the means of the points in each cluster. Also, the estimator will reassign &lt;code&gt;labels_&lt;/code&gt; after the last iteration to make &lt;code&gt;labels_&lt;/code&gt; consistent with &lt;code&gt;predict&lt;/code&gt; on the training set.</source>
          <target state="translated">アルゴリズムが完全に収束する前に停止した場合（ &lt;code&gt;max_iter&lt;/code&gt; の &lt;code&gt;tol&lt;/code&gt; のため）、 &lt;code&gt;labels_&lt;/code&gt; と &lt;code&gt;means_&lt;/code&gt; は一貫しません。つまり、 &lt;code&gt;means_&lt;/code&gt; は各クラスターのポイントの平均ではありません。また、推定器は、再割り当てされます &lt;code&gt;labels_&lt;/code&gt; を作るために、最後の反復後 &lt;code&gt;labels_&lt;/code&gt; はと一致 &lt;code&gt;predict&lt;/code&gt; トレーニングセットに。</target>
        </trans-unit>
        <trans-unit id="4043c787702cc081c41f25b031d69ea98cf35c42" translate="yes" xml:space="preserve">
          <source>If the array is not symmetric, then a symmetrized version is returned. Optionally, a warning or exception is raised if the matrix is not symmetric.</source>
          <target state="translated">配列が対称でない場合は,対称化されたものが返されます.オプションとして,行列が対称でない場合には警告や例外が発生します.</target>
        </trans-unit>
        <trans-unit id="d4238935d45ce51eea0be6c47146a609e05c26ed" translate="yes" xml:space="preserve">
          <source>If the attributes are not found.</source>
          <target state="translated">属性が見つからない場合</target>
        </trans-unit>
        <trans-unit id="10f86bc0a8ef8d94dd88200305e21d6ac290743f" translate="yes" xml:space="preserve">
          <source>If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions).</source>
          <target state="translated">分類器がどちらのクラスでも同じように良好な性能を発揮する場合、この項は従来の精度(すなわち、正しい予測値の数を予測値の総数で割った数)を低下させる。</target>
        </trans-unit>
        <trans-unit id="9d0651dbf433477af9dfe8c9482b03c0b28a7aea" translate="yes" xml:space="preserve">
          <source>If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross- validation result. However, the opposite may be true if the samples are not independently and identically distributed. For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples.</source>
          <target state="translated">データの順序が任意でない場合(例えば、同じクラスラベルを持つサンプルが連続しているなど)、意味のある交差検証結果を得るためには、最初にデータをシャッフルすることが必要不可欠かもしれません。しかし、サンプルが独立して同じように分布していない場合は、逆のこともあります。例えば、サンプルがニュース記事に対応しており、掲載時期順に並べられている場合、データをシャッフルすると、モデルがオーバーフィットし、検証スコアが高くなる可能性があります:学習サンプルと人為的に似ている(時間的に近い)サンプルでテストされます。</target>
        </trans-unit>
        <trans-unit id="4fdc5debb409dcec7a673c288152f0ef6e4738ef" translate="yes" xml:space="preserve">
          <source>If the default value is passed, then &lt;code&gt;keepdims&lt;/code&gt; will not be passed through to the &lt;code&gt;mean&lt;/code&gt; method of sub-classes of &lt;code&gt;ndarray&lt;/code&gt;, however any non-default value will be. If the sub-class&amp;rsquo; method does not implement &lt;code&gt;keepdims&lt;/code&gt; any exceptions will be raised.</source>
          <target state="translated">デフォルト値が渡された場合、その後、 &lt;code&gt;keepdims&lt;/code&gt; はに渡されることはありません &lt;code&gt;mean&lt;/code&gt; のサブクラスの方法 &lt;code&gt;ndarray&lt;/code&gt; 、しかし、任意のデフォルト以外の値になります。サブクラスのメソッドが &lt;code&gt;keepdims&lt;/code&gt; を実装しない場合、例外が発生します。</target>
        </trans-unit>
        <trans-unit id="ca5777d1057fb92ff835301c12f93dc71bd51069" translate="yes" xml:space="preserve">
          <source>If the difference between the current prediction and the correct label is below this threshold, the model is not updated.</source>
          <target state="translated">現在の予測値と正しいラベルの差がこの閾値を下回る場合、モデルは更新されません。</target>
        </trans-unit>
        <trans-unit id="54f187a0c12dbeb2b22455f8308653334a568512" translate="yes" xml:space="preserve">
          <source>If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes.</source>
          <target state="translated">エスティメー タが増分学習をサポートしている場合は、これを使用して、異なる訓練セットサイズに対するフィッティングを高速化します。</target>
        </trans-unit>
        <trans-unit id="28446974a089033b0f005a14dd2e5e7cde0d019d" translate="yes" xml:space="preserve">
          <source>If the file does not exist yet, it is downloaded from mldata.org .</source>
          <target state="translated">ファイルがまだ存在しない場合は、mldata.orgからダウンロードします。</target>
        </trans-unit>
        <trans-unit id="3377386ec971b5f97505ad0b0efacd641307a6b2" translate="yes" xml:space="preserve">
          <source>If the folder does not already exist, it is automatically created.</source>
          <target state="translated">フォルダがまだ存在しない場合は、自動的に作成されます。</target>
        </trans-unit>
        <trans-unit id="bcf86cd76452a354a39a384d6cc008f0521fadc0" translate="yes" xml:space="preserve">
          <source>If the gradient norm is below this threshold, the optimization will be stopped.</source>
          <target state="translated">勾配ノルムがこのしきい値以下の場合、最適化を停止します。</target>
        </trans-unit>
        <trans-unit id="aaea0ac91de1101ebb5583d72a39edadc546a9ed" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (&lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt;&lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt;&lt;/a&gt;) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:</source>
          <target state="translated">グラウンドトゥルースラベルが不明な場合は、モデル自体を使用して評価を実行する必要があります。シルエット係数（&lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt; &lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt; &lt;/a&gt;）はそのような評価の例であり、高いシルエット係数スコアは、クラスターがより明確に定義されたモデルに関連しています。シルエット係数はサンプルごとに定義され、2つのスコアで構成されます。</target>
        </trans-unit>
        <trans-unit id="bb3f5944370bdcf362ff4bffb46e8bf1ded41ef1" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Calinski-Harabaz index (&lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt;&lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt;&lt;/a&gt;) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabaz score relates to a model with better defined clusters.</source>
          <target state="translated">グラウンドトゥルースラベルが不明な場合、Calinski-Harabazインデックス（&lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt; &lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt; &lt;/a&gt;）（別名分散比基準）を使用してモデルを評価できます。この場合、Calinski-Harabazスコアが高いモデルは、より適切に定義されたクラスター。</target>
        </trans-unit>
        <trans-unit id="a4a519d35f95c18e319df7e8878b98f013f9bd44" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Davies-Bouldin index (&lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt;&lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt;&lt;/a&gt;) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters.</source>
          <target state="translated">グラウンドトゥルースラベルが不明な場合、Davies-Bouldinインデックス（&lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt; &lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt; &lt;/a&gt;）を使用してモデルを評価できます。Davies -Bouldinインデックスが低いほど、クラスター間の分離が良好なモデルに関連付けられます。</target>
        </trans-unit>
        <trans-unit id="7ce6861d9ec6948a6bc8aef858e97abae7ed0654" translate="yes" xml:space="preserve">
          <source>If the input is a sparse matrix, only the non-zero values are subject to update by the Binarizer class.</source>
          <target state="translated">入力が疎な行列の場合,Binarizer クラスによって更新されるのは,0 ではない値のみです.</target>
        </trans-unit>
        <trans-unit id="c10a8d9d8c40f9f50dafe36b727f5b47e7075f19" translate="yes" xml:space="preserve">
          <source>If the input matrix X is very sparse, it is recommended to convert to sparse &lt;code&gt;csc_matrix&lt;/code&gt; before calling fit and sparse &lt;code&gt;csr_matrix&lt;/code&gt; before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.</source>
          <target state="translated">入力行列Xは非常に希薄である場合には、スパースに変換することが推奨され &lt;code&gt;csc_matrix&lt;/code&gt; フィット感とスパース呼び出す前に &lt;code&gt;csr_matrix&lt;/code&gt; を予測呼び出す前に。ほとんどのサンプルでフィーチャの値がゼロの場合、スパース行列入力の方が密行列に比べてトレーニング時間が数桁速くなります。</target>
        </trans-unit>
        <trans-unit id="661cb29a3f5fe68fda8ea5f8c53273efb7753ce1" translate="yes" xml:space="preserve">
          <source>If the labels are encoded with +1 and -1, \(y\): is the true value, and \(w\) is the predicted decisions as output by &lt;code&gt;decision_function&lt;/code&gt;, then the hinge loss is defined as:</source>
          <target state="translated">ラベルが+1および-1でエンコードされている場合、\（y \）：は真の値であり、\（w \）はDecision_functionの出力として予測された &lt;code&gt;decision_function&lt;/code&gt; である場合、ヒンジ損失は次のように定義されます。</target>
        </trans-unit>
        <trans-unit id="30e7605353fedb22ff0c25c7418abbab28137e70" translate="yes" xml:space="preserve">
          <source>If the loss on a sample is greater than the &lt;code&gt;residual_threshold&lt;/code&gt;, then this sample is classified as an outlier.</source>
          <target state="translated">サンプルの損失が &lt;code&gt;residual_threshold&lt;/code&gt; より大きい場合、このサンプルは外れ値として分類されます。</target>
        </trans-unit>
        <trans-unit id="fd47c1f065810b1b45f0d8d994aa0a4ff29505d4" translate="yes" xml:space="preserve">
          <source>If the metric constructor parameter is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be the distance matrix between the data to be predicted and &lt;code&gt;self.centroids_&lt;/code&gt;.</source>
          <target state="translated">メトリックコンストラクターパラメーターが「事前計算」されている場合、Xは予測されるデータと &lt;code&gt;self.centroids_&lt;/code&gt; の間の距離行列であると見なされます。</target>
        </trans-unit>
        <trans-unit id="c8476d320358236ab03a9b2e5775d6207658d4f7" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row.</source>
          <target state="translated">メトリックが「事前計算」されている場合、Xは平方距離行列でなければなりません。それ以外の場合は、行ごとのサンプルが含まれます。</target>
        </trans-unit>
        <trans-unit id="ed2846275337b6c05f70ce353bc177c3fd2fc1b0" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row. If the method is &amp;lsquo;exact&amp;rsquo;, X may be a sparse matrix of type &amp;lsquo;csr&amp;rsquo;, &amp;lsquo;csc&amp;rsquo; or &amp;lsquo;coo&amp;rsquo;.</source>
          <target state="translated">メトリックが「事前計算」されている場合、Xは平方距離行列でなければなりません。それ以外の場合は、行ごとのサンプルが含まれます。メソッドが 'exact'の場合、Xは 'csr'、 'csc'、または 'coo'タイプのスパース行列になります。</target>
        </trans-unit>
        <trans-unit id="be2b4ccc2ee21bcc622b72ad8a09e29905f86d22" translate="yes" xml:space="preserve">
          <source>If the number of features is \(p\), you now require \(n \sim 1/d^p\) points. Let&amp;rsquo;s say that we require 10 points in one dimension: now \(10^p\) points are required in \(p\) dimensions to pave the \([0, 1]\) space. As \(p\) becomes large, the number of training points required for a good estimator grows exponentially.</source>
          <target state="translated">特徴の数が\（p \）の場合、\（n \ sim 1 / d ^ p \）ポイントが必要になります。1次元で10ポイントが必要であるとしましょう。\（[0、1] \）スペースを舗装するには、\（p \）次元で\（10 ^ p \）ポイントが必要です。\（p \）が大きくなると、優れた推定量に必要なトレーニングポイントの数は指数関数的に増加します。</target>
        </trans-unit>
        <trans-unit id="31f6fadae8fdb5e25318685f0dd36c90a3234b90" translate="yes" xml:space="preserve">
          <source>If the number of features is much greater than the number of samples, avoid over-fitting in choosing &lt;a href=&quot;#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt; and regularization term is crucial.</source>
          <target state="translated">特徴の数がサンプルの数よりもはるかに多い場合は、&lt;a href=&quot;#svm-kernels&quot;&gt;カーネル関数&lt;/a&gt;を選択する際の過剰適合を避けてください。正則化の用語は重要です。</target>
        </trans-unit>
        <trans-unit id="421f2017551080d266c05ad587f0ba1ecff6bff8" translate="yes" xml:space="preserve">
          <source>If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans.</source>
          <target state="translated">データのインスタンス数を減らす必要がある場合や、前処理のステップとして、あるいはそうでない場合に、多数のサブクラスタを必要とする場合には、MiniBatchKMeansよりもBirchの方が便利です。</target>
        </trans-unit>
        <trans-unit id="0169ea68b458a3b315ac7acca32e943f8bdb1bed" translate="yes" xml:space="preserve">
          <source>If the option chosen is &amp;lsquo;ovr&amp;rsquo;, then a binary problem is fit for each label. For &amp;lsquo;multinomial&amp;rsquo; the loss minimised is the multinomial loss fit across the entire probability distribution, &lt;em&gt;even when the data is binary&lt;/em&gt;. &amp;lsquo;multinomial&amp;rsquo; is unavailable when solver=&amp;rsquo;liblinear&amp;rsquo;. &amp;lsquo;auto&amp;rsquo; selects &amp;lsquo;ovr&amp;rsquo; if the data is binary, or if solver=&amp;rsquo;liblinear&amp;rsquo;, and otherwise selects &amp;lsquo;multinomial&amp;rsquo;.</source>
          <target state="translated">選択したオプションが 'ovr'の場合、バイナリ問題は各ラベルに適合します。「多項式」の場合、最小化される損失は&lt;em&gt;、データがバイナリの場合でも、&lt;/em&gt;確率分布全体に当てはまる多項式損失です。'multinomial'は、solver = 'liblinear'の場合は使用できません。'auto'は、データがバイナリの場合、またはsolver = 'liblinear'の場合は 'ovr'を選択し、それ以外の場合は 'multinomial'を選択します。</target>
        </trans-unit>
        <trans-unit id="fb7bde2a134f67333f646adb3cb422fdb3b0a619" translate="yes" xml:space="preserve">
          <source>If the prediction task is to classify the observations in a set of finite labels, in other words to &amp;ldquo;name&amp;rdquo; the objects observed, the task is said to be a &lt;strong&gt;classification&lt;/strong&gt; task. On the other hand, if the goal is to predict a continuous target variable, it is said to be a &lt;strong&gt;regression&lt;/strong&gt; task.</source>
          <target state="translated">予測タスクが一連の有限ラベルで観測を分類すること、つまり、観測されたオブジェクトに「名前を付ける」ことである場合、タスクは&lt;strong&gt;分類&lt;/strong&gt;タスクと呼ばれます。一方、目標が連続的なターゲット変数を予測することである場合、それは&lt;strong&gt;回帰&lt;/strong&gt;タスクと呼ばれます。</target>
        </trans-unit>
        <trans-unit id="c305135e1b17987e86652a7910deafacf0f5dc9d" translate="yes" xml:space="preserve">
          <source>If the pyamg package is installed, it is used: this greatly speeds up computation.</source>
          <target state="translated">pyamgパッケージがインストールされている場合は、それを使用します。</target>
        </trans-unit>
        <trans-unit id="04f07265ff7d7b70145be5aec52784e1b24d6f09" translate="yes" xml:space="preserve">
          <source>If the radius of the subcluster obtained by merging the new sample and the nearest subcluster is greater than the square of the threshold and if the number of subclusters is greater than the branching factor, then a space is temporarily allocated to this new sample. The two farthest subclusters are taken and the subclusters are divided into two groups on the basis of the distance between these subclusters.</source>
          <target state="translated">新しいサンプルと最も近いサブクラスターをマージして得られたサブクラスターの半径が閾値の二乗よりも大きく、サブクラスターの数が分岐係数よりも大きい場合には、この新しいサンプルに一時的にスペースが割り当てられる。最も遠い2つのサブクラスターを取り、これらのサブクラスター間の距離に基づいてサブクラスターを2つのグループに分割する。</target>
        </trans-unit>
        <trans-unit id="41eef1b8a501219131b2619b097a82294e78c28a" translate="yes" xml:space="preserve">
          <source>If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt;, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.</source>
          <target state="translated">サンプルが重み付けされている場合、 &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt; などの重みベースの事前剪定基準を使用してツリー構造を最適化する方が簡単です。これにより、リーフノードに少なくともサンプルの重みの合計の一部が含まれるようになります。</target>
        </trans-unit>
        <trans-unit id="0c1baaebbfab363e25ace0c6b6ee91539fab116b" translate="yes" xml:space="preserve">
          <source>If the selected solver is &amp;lsquo;L-BFGS&amp;rsquo;, training does not support online nor mini-batch learning.</source>
          <target state="translated">選択したソルバーが「L-BFGS」の場合、トレーニングはオンライン学習もミニバッチ学習もサポートしていません。</target>
        </trans-unit>
        <trans-unit id="6b79b273c5ccf0e85d810ff5a4ad56e9102a13be" translate="yes" xml:space="preserve">
          <source>If the target is a continuous value, then for node \(m\), representing a region \(R_m\) with \(N_m\) observations, common criteria to minimise as for determining locations for future splits are Mean Squared Error, which minimizes the L2 error using mean values at terminal nodes, and Mean Absolute Error, which minimizes the L1 error using median values at terminal nodes.</source>
          <target state="translated">目標が連続値であれば、将来の分割位置を決定するための共通の基準は、端末ノードの平均値を用いてL2の誤差を最小化するMean Squared Errorと、端末ノードの中央値を用いてL1の誤差を最小化するMean Absolute Errorである。</target>
        </trans-unit>
        <trans-unit id="bd360b0d17d9758c91116a373249c96c3c493365" translate="yes" xml:space="preserve">
          <source>If the text is in a mish-mash of encodings that is simply too hard to sort out (which is the case for the 20 Newsgroups dataset), you can fall back on a simple single-byte encoding such as &lt;code&gt;latin-1&lt;/code&gt;. Some text may display incorrectly, but at least the same sequence of bytes will always represent the same feature.</source>
          <target state="translated">テキストが単純に整理するのが難しすぎるエンコーディングの寄せ集めに含まれている場合（20ニュースグループデータセットの場合）、 &lt;code&gt;latin-1&lt;/code&gt; などの単純なシングルバイトエンコーディングに頼ることができます。一部のテキストは正しく表示されない場合がありますが、少なくとも同じバイトシーケンスは常に同じ機能を表します。</target>
        </trans-unit>
        <trans-unit id="21286b430a50cd4c4f50df67c996d4b5b475416f" translate="yes" xml:space="preserve">
          <source>If the text you are loading is not actually encoded with UTF-8, however, you will get a &lt;code&gt;UnicodeDecodeError&lt;/code&gt;. The vectorizers can be told to be silent about decoding errors by setting the &lt;code&gt;decode_error&lt;/code&gt; parameter to either &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; or &lt;code&gt;&quot;replace&quot;&lt;/code&gt;. See the documentation for the Python function &lt;code&gt;bytes.decode&lt;/code&gt; for more details (type &lt;code&gt;help(bytes.decode)&lt;/code&gt; at the Python prompt).</source>
          <target state="translated">ロードするテキストが実際にUTF-8でエンコードされていない場合は、 &lt;code&gt;UnicodeDecodeError&lt;/code&gt; が発生します。 &lt;code&gt;decode_error&lt;/code&gt; は、decode_errorパラメーターを &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; または &lt;code&gt;&quot;replace&quot;&lt;/code&gt; のいずれかに設定することにより、デコードエラーについて何も通知しないように指示できます。詳細については、Python関数 &lt;code&gt;bytes.decode&lt;/code&gt; のドキュメントを参照してください（Pythonプロンプトで &lt;code&gt;help(bytes.decode)&lt;/code&gt; と入力します）。</target>
        </trans-unit>
        <trans-unit id="13a9f813159d9e6258a164870cb1dc6a301dddd5" translate="yes" xml:space="preserve">
          <source>If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary the parameter \(\gamma\) of an SVM on the digits dataset.</source>
          <target state="translated">トレーニングスコアとバリデーションスコアの両方が低い場合、推定器はアンダーフィッティングになります。トレーニングスコアが高く、バリデーションスコアが低い場合、推定器はオーバーフィットしており、それ以外の場合は非常にうまく機能しています。トレーニングスコアが低く、バリデーションスコアが高い場合は、通常は不可能です。この3つのケースはすべて、数字データセット上でSVMのパラメータ\(Gamma)を変化させたプロットにあります。</target>
        </trans-unit>
        <trans-unit id="80e789647361ff21671194a00300fe312dc530d2" translate="yes" xml:space="preserve">
          <source>If the transformed output consists of a mix of sparse and dense data, it will be stacked as a sparse matrix if the density is lower than this value. Use &lt;code&gt;sparse_threshold=0&lt;/code&gt; to always return dense. When the transformed output consists of all sparse or all dense data, the stacked result will be sparse or dense, respectively, and this keyword will be ignored.</source>
          <target state="translated">変換された出力が疎データと密データの混合で構成されている場合、密度がこの値よりも低いと、疎行列として積み重ねられます。常に密に戻すには、 &lt;code&gt;sparse_threshold=0&lt;/code&gt; を使用します。変換された出力がすべてスパースデータまたはすべてデンスデータで構成される場合、スタックされた結果はそれぞれスパースまたはデンスとなり、このキーワードは無視されます。</target>
        </trans-unit>
        <trans-unit id="efca83041c066057e65d83989c4190b74b69dba6" translate="yes" xml:space="preserve">
          <source>If the underlying graph has nodes with much more connections than the average node, the algorithm will miss some of these connections.</source>
          <target state="translated">基礎となるグラフに平均ノードよりもはるかに多くの接続を持つノードがある場合、アルゴリズムはこれらの接続の一部を見逃してしまいます。</target>
        </trans-unit>
        <trans-unit id="27f470c8c1d74aa01f92e63860f4d2b9149dd0bb" translate="yes" xml:space="preserve">
          <source>If there are few data points per dimension, noise in the observations induces high variance:</source>
          <target state="translated">次元あたりのデータ点が少ない場合,オブザベーションのノイズが高い分散を引き起こす.</target>
        </trans-unit>
        <trans-unit id="fbaec65eed1139944f6f906201326eaef7bc4d08" translate="yes" xml:space="preserve">
          <source>If there are more than two classes, \(f(x)\) itself would be a vector of size (n_classes,). Instead of passing through logistic function, it passes through the softmax function, which is written as,</source>
          <target state="translated">クラスが2つ以上ある場合は、\(f(x)mm)自体がサイズ(n_classes,)のベクトルになります。ロジスティック関数を通さずに、ソフトマックス関数を通します。</target>
        </trans-unit>
        <trans-unit id="e2c9b002eac60ce02e4a3cafb47196266855c43e" translate="yes" xml:space="preserve">
          <source>If there are more than two labels, &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; uses a multiclass variant due to Crammer &amp;amp; Singer. &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;Here&lt;/a&gt; is the paper describing it.</source>
          <target state="translated">ラベルが3つ以上ある場合、&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;＆SingerによるHinge_lossはマルチクラスバリアントを使用します。&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;ここでは&lt;/a&gt;それについての論文があります。</target>
        </trans-unit>
        <trans-unit id="362b6e0ad023f937918b9ce5c294c8243f2c8ea1" translate="yes" xml:space="preserve">
          <source>If there is a possibility that the training data might have missing categorical features, it can often be better to specify &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; instead of setting the &lt;code&gt;categories&lt;/code&gt; manually as above. When &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is specified and unknown categories are encountered during transform, no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros (&lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is only supported for one-hot encoding):</source>
          <target state="translated">トレーニングデータにカテゴリ特徴が欠落している可能性がある場合は、上記のように手動で &lt;code&gt;categories&lt;/code&gt; を設定する代わりに、 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; を指定する方が良い場合があります。とき &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; に指定され、未知のカテゴリは変換中に発生し、エラーは発生しませんが、この機能の結果としてワンホットエンコードされた列は、（すべてゼロになります &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 唯一のワンホットエンコーディングのためにサポートされています）：</target>
        </trans-unit>
        <trans-unit id="76c3aee0f8dcd7757eeddd2a91b271bc2108fb17" translate="yes" xml:space="preserve">
          <source>If there is more than one such value, only the first is returned. The bin-count for the modal bins is also returned.</source>
          <target state="translated">このような値が複数ある場合は、最初の値だけが返されます。モーダルビンのビンカウントも返されます。</target>
        </trans-unit>
        <trans-unit id="e4599c6e53b844db2376ed9e56ed7b49e63c6ec3" translate="yes" xml:space="preserve">
          <source>If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.</source>
          <target state="translated">これがintのタプルである場合、以前のように1つの軸またはすべての軸ではなく、複数の軸に渡って平均が実行されます。</target>
        </trans-unit>
        <trans-unit id="015e500928e7c3f86f2c9b5121c746246fcd7a9f" translate="yes" xml:space="preserve">
          <source>If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.</source>
          <target state="translated">これが True に設定されている場合、縮小された軸はサイズ 1 の寸法として結果に残されます。このオプションを指定すると、結果は入力配列に対して正しくブロードキャストされます。</target>
        </trans-unit>
        <trans-unit id="1d47783a4de427039b4db643f305b3dd195e7ba2" translate="yes" xml:space="preserve">
          <source>If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two. If there is no room, then this node is again split into two and the process is continued recursively, till it reaches the root.</source>
          <target state="translated">この分割ノードに親サブクラスターがあり、新しいサブクラスターを作成する余地がある場合、親サブクラスターを2つに分割します。もし空きがなければ、このノードは再び2つに分割され、処理はルートに到達するまで再帰的に続けられます。</target>
        </trans-unit>
        <trans-unit id="012f5a7e85e6e4a424dae20b5f0c103c4fa516ee" translate="yes" xml:space="preserve">
          <source>If true (default), use a breadth-first approach to the problem. Otherwise use a depth-first approach.</source>
          <target state="translated">true (デフォルト)の場合、問題に対して幅優先のアプローチを使用します。そうでない場合は、深さ優先のアプローチを使用します。</target>
        </trans-unit>
        <trans-unit id="8002efc50268110232cc0ffbdad97c50440caadd" translate="yes" xml:space="preserve">
          <source>If true, X and y will be centered.</source>
          <target state="translated">trueの場合、Xとyは中央に配置されます。</target>
        </trans-unit>
        <trans-unit id="de73b79cb6d725781d21d3fce59be150e3f40a4c" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. Ignored if seeds argument is not None.</source>
          <target state="translated">真の場合、初期カーネルの位置はすべての点の位置ではなく、むしろ離散化された点の位置となります。このオプションをTrueに設定すると、初期化されるシード数が少なくなるため、アルゴリズムが高速化されます。seeds 引数が None でない場合は無視されます。</target>
        </trans-unit>
        <trans-unit id="382a329c7d03e3dc0838d8f846116bb309725e41" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False Ignored if seeds argument is not None.</source>
          <target state="translated">真の場合、初期カーネルの位置はすべての点の位置ではなく、むしろ離散化された点の位置となります。このオプションをTrueに設定すると、初期化されるシード数が少なくなるため、アルゴリズムが高速化されます。False seeds 引数が None でない場合は無視されます。</target>
        </trans-unit>
        <trans-unit id="5f8f1503f4ad4447aabcfddb9ab2a5dcd7bfde79" translate="yes" xml:space="preserve">
          <source>If true, only interaction features are produced: features that are products of at most &lt;code&gt;degree&lt;/code&gt;&lt;em&gt;distinct&lt;/em&gt; input features (so not &lt;code&gt;x[1] ** 2&lt;/code&gt;, &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt;, etc.).</source>
          <target state="translated">trueの場合、唯一の相互作用の特徴が生成される：最大での生成物である特徴 &lt;code&gt;degree&lt;/code&gt; &lt;em&gt;の異なる&lt;/em&gt;入力フィーチャ（そうではない &lt;code&gt;x[1] ** 2&lt;/code&gt; 、 &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt; 、など）。</target>
        </trans-unit>
        <trans-unit id="2d6ce18ffe19be253728c95b7f8882b85215b418" translate="yes" xml:space="preserve">
          <source>If true, randomize the order of coordinates in the CD solver.</source>
          <target state="translated">true の場合、CD ソルバーの座標の順序をランダムにします。</target>
        </trans-unit>
        <trans-unit id="c9d7c7ecbdd08be425848806cc6f9d68c29d7323" translate="yes" xml:space="preserve">
          <source>If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses.</source>
          <target state="translated">真の場合、サンプルごとの平均損失を返します。そうでなければ、サンプルごとの損失の合計を返します。</target>
        </trans-unit>
        <trans-unit id="6d32e9cabd3e10e0279ad9dd2b7c71514057bf52" translate="yes" xml:space="preserve">
          <source>If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1.</source>
          <target state="translated">もし真であれば、どのカーネル内にもないオーファンであっても、すべての点がクラスタ化されます。孤児は最も近いカーネルに割り当てられます。falseの場合、オーファンにはクラスタラベル-1が与えられます。</target>
        </trans-unit>
        <trans-unit id="87535a59e28d16a49b32c83a6882f2aefd67503d" translate="yes" xml:space="preserve">
          <source>If true, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree algorithms can have better scaling for large N.</source>
          <target state="translated">真の場合は、二重木アルゴリズムを使用します。そうでなければ、シングルツリーアルゴリズムを使用します。二重木アルゴリズムは、大きなNに対してより良いスケーリングを持つことができます。</target>
        </trans-unit>
        <trans-unit id="a62f22814f97612f53d5c62d8ea50a484acea3fc" translate="yes" xml:space="preserve">
          <source>If two variables are almost equally correlated with the response, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.</source>
          <target state="translated">2つの変数が応答とほぼ等しく相関している場合、それらの係数はほぼ同じ速度で増加するはずです。このアルゴリズムは、このようにして直感が期待するように動作し、より安定しています。</target>
        </trans-unit>
        <trans-unit id="c6d813716240a58cb1e341ee096ed78ff4d17824" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are plotted at each iteration.</source>
          <target state="translated">verboseがTrueの場合、目的関数とデュアルギャップが各反復でプロットされます。</target>
        </trans-unit>
        <trans-unit id="c845cf733c967b921d034159fbd6e6d551e8f5a1" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are printed at each iteration.</source>
          <target state="translated">verboseがTrueの場合、目的関数とデュアルギャップが各イテレーションで印刷されます。</target>
        </trans-unit>
        <trans-unit id="d04bb65f95446e17ac813ad52d517cc52bee8bef" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and duality gap are printed at each iteration.</source>
          <target state="translated">verboseがTrueの場合、目的関数と双対性ギャップが各イテレーションで印刷されます。</target>
        </trans-unit>
        <trans-unit id="c297e2c2dea63aea70529d05594e08d33ba95930" translate="yes" xml:space="preserve">
          <source>If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">ウォームスタートが有効になっている場合、事後モードのラプラス近似の最後のニュートン反復の解が、_posterior_mode（）の次の呼び出しの初期化として使用されます。これにより、ハイパーパラメーター最適化と同様の問題で_posterior_modeが複数回呼び出されたときに、収束を高速化できます。&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;用語集を&lt;/a&gt;参照してください。</target>
        </trans-unit>
        <trans-unit id="0beda1307d8fba8bf455ad043421d0e1b5743334" translate="yes" xml:space="preserve">
          <source>If we consider the loss function to be the individual error per sample, then the data-fit term, or the sum of the error for each sample, will increase as we add more samples. The penalization term, however, will not increase.</source>
          <target state="translated">損失関数を標本ごとの個々の誤差と考えると、データフィット項(各標本の誤差の合計)は、標本を増やすにつれて増加します。しかし、ペナルティ項は増加しません。</target>
        </trans-unit>
        <trans-unit id="232b0b83965c86185ba5cb4047fca7ca1eb2e5e8" translate="yes" xml:space="preserve">
          <source>If we define &lt;code&gt;s = 1 / density&lt;/code&gt;, the elements of the random matrix are drawn from</source>
          <target state="translated">&lt;code&gt;s = 1 / density&lt;/code&gt; を定義すると、ランダム行列の要素は、</target>
        </trans-unit>
        <trans-unit id="1427e0ae27c6ada10257117ddf3e602836e812e6" translate="yes" xml:space="preserve">
          <source>If we note &lt;code&gt;s = 1 / density&lt;/code&gt; the components of the random matrix are drawn from:</source>
          <target state="translated">&lt;code&gt;s = 1 / density&lt;/code&gt; 注意すると、ランダム行列の成分は以下から引き出されます。</target>
        </trans-unit>
        <trans-unit id="f9cbc4d2964857d1865d4f91b696f12d3cce3cff" translate="yes" xml:space="preserve">
          <source>If we note \(n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})\) and \(n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})\), the time complexity of the randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is \(O(n_{\max}^2 \cdot n_{\mathrm{components}})\) instead of \(O(n_{\max}^2 \cdot n_{\min})\) for the exact method implemented in &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">\（n _ {\ max} = \ max（n _ {\ mathrm {samples}}、n _ {\ mathrm {features}}）\）と\（n _ {\ min} = \ min（n _ {\ mathrm {samples}}、n _ {\ mathrm {features}}）\）、ランダム化された&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; の&lt;/a&gt;時間の複雑さは\（O（n _ {\ max} ^ 2 \ cdot n _ {\ mathrm {components}}）\）です実装正確な方法\（O（N _ {\最大} ^ 2 \ CDOT N _ {\分}）\）の&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="483854d9b52bcc6a7814b6c99e197398a7767c0e" translate="yes" xml:space="preserve">
          <source>If we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number of samples is small, we need to shrink a lot. As a result, the Ledoit-Wolf precision is fairly close to the ground truth precision, that is not far from being diagonal, but the off-diagonal structure is lost.</source>
          <target state="translated">Ledoit-Wolf 推定器のように l2 縮小を使うと、サンプル数が少ないので、かなりの縮小が必要になります。その結果、Ledoit-Wolfの精度は、対角線には遠く及ばないが、対角線から外れた構造が失われているという、かなり地真の精度に近いものになります。</target>
        </trans-unit>
        <trans-unit id="c048d2d806d4fc664d0d49e2240da1f9038d7165" translate="yes" xml:space="preserve">
          <source>If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:</source>
          <target state="translated">平面ではなく放物線をデータにフィットさせたい場合は、2次多項式の特徴を組み合わせることで、このようなモデルになります。</target>
        </trans-unit>
        <trans-unit id="9cd2bdd13889db844fd297c5019d226d5f2214ec" translate="yes" xml:space="preserve">
          <source>If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries are the same) we would obtain &lt;code&gt;PPCA&lt;/code&gt;.</source>
          <target state="translated">モデルをさらに制限する場合は、ガウスノイズが等方性である（すべての対角要素が同じである）と &lt;code&gt;PPCA&lt;/code&gt; することにより、PPCAを取得します。</target>
        </trans-unit>
        <trans-unit id="722d8ecf13f293f3aeb1f206f30063d8afe3b1aa" translate="yes" xml:space="preserve">
          <source>If whiten is false, the data is already considered to be whitened, and no whitening is performed.</source>
          <target state="translated">美白が偽物であれば、すでに美白が行われていると考えられているデータであり、美白は行われていません。</target>
        </trans-unit>
        <trans-unit id="7d7146f3cf5f6ee3a12daad9561636b3070c19dc" translate="yes" xml:space="preserve">
          <source>If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.</source>
          <target state="translated">ホワイトニングが有効な場合、inverse_transformは、ホワイトニングの反転を含む正確な反転演算を計算します。</target>
        </trans-unit>
        <trans-unit id="d6701cdf426d6a22381b3dd206332eab0defeb4b" translate="yes" xml:space="preserve">
          <source>If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant &lt;code&gt;c&lt;/code&gt; such that the average L2 norm of the training data equals one.</source>
          <target state="translated">PCAを使用して抽出された特徴にSGDを適用した場合、学習データの平均L2ノルムが1となるように、特徴値を定数 &lt;code&gt;c&lt;/code&gt; でスケーリングすることがしばしば賢明であることがわかりました。</target>
        </trans-unit>
        <trans-unit id="88e7b5f8ea1b769958767cd4c4986cb0d84b69d4" translate="yes" xml:space="preserve">
          <source>If you are having trouble decoding text, here are some things to try:</source>
          <target state="translated">テキストの解読にお困りの方は、以下のことを試してみてはいかがでしょうか。</target>
        </trans-unit>
        <trans-unit id="79c8dbf5a9dd8e0bcb21dbc0b3d21d4a002af1f5" translate="yes" xml:space="preserve">
          <source>If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to:</source>
          <target state="translated">L1とL2のペナルティを別々にコントロールしたい場合は、それに相当することを覚えておきましょう。</target>
        </trans-unit>
        <trans-unit id="fda5569b1e927ca58d1243554ef8331577e43162" translate="yes" xml:space="preserve">
          <source>If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.</source>
          <target state="translated">a-preriori辞書を提供せず、ある種の特徴選択を行う分析器を使用しない場合、特徴の数はデータを分析して見つかった語彙数と同じになります。</target>
        </trans-unit>
        <trans-unit id="44906a85511569286aafb87826155b3c2905ddf9" translate="yes" xml:space="preserve">
          <source>If you don&amp;rsquo;t have labels, try using &lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;Clustering&lt;/a&gt; on your problem.</source>
          <target state="translated">ラベルがない場合は、問題に対して&lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;クラスタリング&lt;/a&gt;を使用してみてください。</target>
        </trans-unit>
        <trans-unit id="1e3c9a76f4703e92f09f761bb01632ba0994c7fc" translate="yes" xml:space="preserve">
          <source>If you experience hanging subprocesses with &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; or &lt;code&gt;n_jobs=-1&lt;/code&gt;, make sure you have a single-threaded BLAS library, or set &lt;code&gt;n_jobs=1&lt;/code&gt;, or upgrade to Python 3.4 which has a new version of &lt;code&gt;multiprocessing&lt;/code&gt; that should be immune to this problem.</source>
          <target state="translated">&lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; または &lt;code&gt;n_jobs=-1&lt;/code&gt; でサブプロセスがハングする場合は、シングルスレッドのBLASライブラリがあることを確認するか、 &lt;code&gt;n_jobs=1&lt;/code&gt; を設定するか、これに影響されない新しいバージョンの &lt;code&gt;multiprocessing&lt;/code&gt; を持つPython 3.4にアップグレードしてください問題。</target>
        </trans-unit>
        <trans-unit id="b4d3f940390acd5b7c567c108aba27222ddceb7d" translate="yes" xml:space="preserve">
          <source>If you have a kernel matrix of a kernel \(K\) that computes a dot product in a feature space defined by function \(phi\), a &lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt;&lt;code&gt;KernelCenterer&lt;/code&gt;&lt;/a&gt; can transform the kernel matrix so that it contains inner products in the feature space defined by \(phi\) followed by removal of the mean in that space.</source>
          <target state="translated">関数\（phi \）で定義された特徴空間でドット積を計算するカーネル\（K \）のカーネル行列がある場合、&lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt; &lt;code&gt;KernelCenterer&lt;/code&gt; &lt;/a&gt;はカーネル行列を変換して、定義された特徴空間の内積を含めることができます。 \（phi \）に続けて、そのスペースの平均を削除します。</target>
        </trans-unit>
        <trans-unit id="2615ef2dcc8005e7f8f1fe1a8b864f8c5c8b40ba" translate="yes" xml:space="preserve">
          <source>If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel:</source>
          <target state="translated">0 は同一の要素を意味し、高い値は非常に非類似な要素を意味する距離行列のような親和性行列を持っている場合は、ガウシアン(RBF,heat)カーネルを適用することで、アルゴリズムに適した類似性行列に変換することができます。</target>
        </trans-unit>
        <trans-unit id="fe35ae96a69c356c4c790a684b706493b567f769" translate="yes" xml:space="preserve">
          <source>If you have multiple labels per document, e.g categories, have a look at the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;Multiclass and multilabel section&lt;/a&gt;.</source>
          <target state="translated">ドキュメントごとに複数のラベル（カテゴリなど）がある場合は、&lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;マルチクラスとマルチラベルのセクションをご覧ください&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4c5f38a361bcbafd12cc29508483a444dfcf9728" translate="yes" xml:space="preserve">
          <source>If you have several classes to predict, an option often used is to fit one-versus-all classifiers and then use a voting heuristic for the final decision.</source>
          <target state="translated">予測するクラスが複数ある場合,よく使われるオプションは,1対すべての分類器を適合させ,最終的な決定に投票ヒューリスティックを使用することです.</target>
        </trans-unit>
        <trans-unit id="f97615967054f5695c197161c38be5160cb380e9" translate="yes" xml:space="preserve">
          <source>If you need the raw values of the partial dependence function rather than the plots you can use the &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt;&lt;code&gt;partial_dependence&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">プロットではなく部分依存関数の生の値が必要な場合は、&lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt; &lt;code&gt;partial_dependence&lt;/code&gt; &lt;/a&gt;関数を使用できます。</target>
        </trans-unit>
        <trans-unit id="57ce2ca8cd47ab25ffa05da2880829913ab0ea8d" translate="yes" xml:space="preserve">
          <source>If you really want to use &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you can instantiate the estimator with the &lt;code&gt;novelty&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt; before fitting the estimator. In this case, &lt;code&gt;fit_predict&lt;/code&gt; is not available.</source>
          <target state="translated">あなたが本当に使用したい場合は&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; を&lt;/a&gt;ノベルティ検出のために、すなわち、ラベルを予測したり、新たな目に見えないデータの異常のスコアを計算し、あなたが推定をインスタンス化することができ &lt;code&gt;novelty&lt;/code&gt; にパラメータセット &lt;code&gt;True&lt;/code&gt; 推定器を取り付ける前に。この場合、 &lt;code&gt;fit_predict&lt;/code&gt; は使用できません。</target>
        </trans-unit>
        <trans-unit id="9f2c392b35ce2be9956cf3b6740e21a9cb0e7eb8" translate="yes" xml:space="preserve">
          <source>If you set load_content=True, you should also specify the encoding of the text using the &amp;lsquo;encoding&amp;rsquo; parameter. For many modern text files, &amp;lsquo;utf-8&amp;rsquo; will be the correct encoding. If you leave encoding equal to None, then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;.</source>
          <target state="translated">load_content = Trueを設定する場合は、 'encoding'パラメーターを使用してテキストのエンコーディングも指定する必要があります。最近の多くのテキストファイルでは、「utf-8」が正しいエンコーディングになります。エンコードをNoneのままにすると、コンテンツはUnicodeではなくバイトで作成され、 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; のほとんどの関数を使用できなくなります。</target>
        </trans-unit>
        <trans-unit id="bc2de8e7176de3a7ec5f2ba66eebe7612bd92fb1" translate="yes" xml:space="preserve">
          <source>If you specify &lt;code&gt;max_depth=h&lt;/code&gt; then complete binary trees of depth &lt;code&gt;h&lt;/code&gt; will be grown. Such trees will have (at most) &lt;code&gt;2**h&lt;/code&gt; leaf nodes and &lt;code&gt;2**h - 1&lt;/code&gt; split nodes.</source>
          <target state="translated">&lt;code&gt;max_depth=h&lt;/code&gt; を指定すると、深さ &lt;code&gt;h&lt;/code&gt; の完全なバイナリツリーが成長します。このようなツリーには、（最大で） &lt;code&gt;2**h&lt;/code&gt; リーフノードと &lt;code&gt;2**h - 1&lt;/code&gt; スプリットノードがあります。</target>
        </trans-unit>
        <trans-unit id="b5e591b33a0bd24a7e9f3db1117e493d465fb600" translate="yes" xml:space="preserve">
          <source>If you use sparse data (i.e. data represented as sparse matrices), &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt; will deal with the data without making it dense.</source>
          <target state="translated">スパースデータ（つまり、スパース行列として表されるデータ）を使用する場合、&lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; &lt;/a&gt;はデータを密にすることなく処理します。</target>
        </trans-unit>
        <trans-unit id="a1ac2d367786359c2f555cec450b280865094e6b" translate="yes" xml:space="preserve">
          <source>If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using &lt;code&gt;warm_start=True&lt;/code&gt; and &lt;code&gt;max_iter=1&lt;/code&gt; and iterating yourself can be helpful:</source>
          <target state="translated">SGDの停止基準または学習率をより詳細に制御したい場合、または追加の監視を実行したい場合は、 &lt;code&gt;warm_start=True&lt;/code&gt; および &lt;code&gt;max_iter=1&lt;/code&gt; を使用して、自分自身を繰り返すと便利です。</target>
        </trans-unit>
        <trans-unit id="5572f4380789e92c52811040d71f90082bdb6315" translate="yes" xml:space="preserve">
          <source>If you want to know more about these issues and explore other possible serialization methods, please refer to this &lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;talk by Alex Gaynor&lt;/a&gt;.</source>
          <target state="translated">これらの問題の詳細を知り、他の可能なシリアライゼーション方法を探索したい場合は&lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;、Alex Gaynorによる&lt;/a&gt;この講演を参照してください。</target>
        </trans-unit>
        <trans-unit id="c9d28c176517636a408517ab464ebf5a8ed78a10" translate="yes" xml:space="preserve">
          <source>If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed.</source>
          <target state="translated">属性に固有のスケールがある場合(単語の頻度や指標特徴など)は、スケーリングは必要ありません。</target>
        </trans-unit>
        <trans-unit id="1c54fdc8274e03b04e776296dd28d5ff9fd81085" translate="yes" xml:space="preserve">
          <source>If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt;&lt;code&gt;robust_scale&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt;&lt;code&gt;RobustScaler&lt;/code&gt;&lt;/a&gt; as drop-in replacements instead. They use more robust estimates for the center and range of your data.</source>
          <target state="translated">データに多くの外れ値が含まれている場合、データの平均と分散を使用したスケーリングはあまりうまく機能しない可能性があります。これらの場合、代わりに&lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt; &lt;code&gt;robust_scale&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt; &lt;code&gt;RobustScaler&lt;/code&gt; &lt;/a&gt;をドロップイン置換として使用できます。彼らはあなたのデータの中心と範囲のためのより堅牢な推定値を使用します。</target>
        </trans-unit>
        <trans-unit id="a10bfba29a759d89e81956a541262290109cf294" translate="yes" xml:space="preserve">
          <source>If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the &lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;Unsupervised learning&lt;/a&gt; methods implement a &lt;code&gt;transform&lt;/code&gt; method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.</source>
          <target state="translated">機能の数が多い場合は、監視ありステップの前に監視なしステップを使用してそれを減らすと便利な場合があります。&lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;教師なし学習&lt;/a&gt;メソッドの多くは、次元数を減らすために使用できる &lt;code&gt;transform&lt;/code&gt; メソッドを実装しています。以下では、頻繁に使用されるこのパターンの2つの特定の例について説明します。</target>
        </trans-unit>
        <trans-unit id="933c257247173f2464c3cf8d4de4af2d678e5a7c" translate="yes" xml:space="preserve">
          <source>If your number of observations is not large compared to the number of edges in your underlying graph, you will not recover it.</source>
          <target state="translated">もしオブザベーションの数が、基礎となるグラフのエッジの数に比べて大きくない場合は、それを回復することはできません。</target>
        </trans-unit>
        <trans-unit id="4f34c772816074a373c0d5e918e2e9ac136448b7" translate="yes" xml:space="preserve">
          <source>Ignore the offset first bytes by seeking forward, then discarding the following bytes up until the next new line character.</source>
          <target state="translated">オフセットの最初のバイトを無視してフォワードを求め、次の改行文字までの次のバイトを破棄します。</target>
        </trans-unit>
        <trans-unit id="78fee1435d74666b84850cd5e82c18229351da5d" translate="yes" xml:space="preserve">
          <source>Ignored</source>
          <target state="translated">Ignored</target>
        </trans-unit>
        <trans-unit id="1e65bb4eca2d3c71529c96890a4b735eb7dafeac" translate="yes" xml:space="preserve">
          <source>Ignored.</source>
          <target state="translated">Ignored.</target>
        </trans-unit>
        <trans-unit id="1e417badfc4d52f79664b451110854e41b4a0daf" translate="yes" xml:space="preserve">
          <source>Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline.</source>
          <target state="translated">無視されます。このパラメータはsklearn.pipeline.Pipeline.Pipelineとの互換性のためだけに存在します。</target>
        </trans-unit>
        <trans-unit id="2d34b7c897f7b41a0f0625575a2c9cc21b1078a7" translate="yes" xml:space="preserve">
          <source>Illustration of &lt;code&gt;Pipeline&lt;/code&gt; and &lt;code&gt;GridSearchCV&lt;/code&gt;</source>
          <target state="translated">イラスト &lt;code&gt;Pipeline&lt;/code&gt; と &lt;code&gt;GridSearchCV&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="643998f34944846c305de7d49de1c3e80f814d2d" translate="yes" xml:space="preserve">
          <source>Illustration of Gaussian process classification (GPC) on the XOR dataset</source>
          <target state="translated">XORデータセット上でのガウス過程分類(GPC)の図解</target>
        </trans-unit>
        <trans-unit id="c2cd661f8089fd4df71dfb566ea137083aa22024" translate="yes" xml:space="preserve">
          <source>Illustration of how the performance of an estimator on unseen data (test data) is not the same as the performance on training data. As the regularization increases the performance on train decreases while the performance on test is optimal within a range of values of the regularization parameter. The example with an Elastic-Net regression model and the performance is measured using the explained variance a.k.a. R^2.</source>
          <target state="translated">見えないデータ(テストデータ)に対する推定器の性能が、訓練データに対する性能と同じではないことを示す図。正則化が増加すると訓練時の性能は低下しますが、テスト時の性能は正則化パラメータの値の範囲内で最適です。Elastic-Net回帰モデルを用いた例で、性能は説明済み分散 a.k.a.R^2.を用いて測定されます。</target>
        </trans-unit>
        <trans-unit id="5790a5aaa3a6c4543a820b9b12ce6d261eeb0581" translate="yes" xml:space="preserve">
          <source>Illustration of prior and posterior Gaussian process for different kernels</source>
          <target state="translated">異なるカーネルに対する事前・事後ガウス過程の説明</target>
        </trans-unit>
        <trans-unit id="27c062ea4e410688effe23ac51313a8ab9a70f1c" translate="yes" xml:space="preserve">
          <source>Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">勾配ブースティングのさまざまな正則化戦略の効果の図。この例は、Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;からの引用です。</target>
        </trans-unit>
        <trans-unit id="5beb1d257bbb65ddb7ec568445a1c3acdcb42d37" translate="yes" xml:space="preserve">
          <source>Image denoising using dictionary learning</source>
          <target state="translated">辞書学習を用いた画像デノイジング</target>
        </trans-unit>
        <trans-unit id="5ab7decf36c80b04aff06a11c0e8ef068c85a1b9" translate="yes" xml:space="preserve">
          <source>Image histogram</source>
          <target state="translated">画像ヒストグラム</target>
        </trans-unit>
        <trans-unit id="5c328038b14054033bab1147ef5d1ad234b3373d" translate="yes" xml:space="preserve">
          <source>Imagine you have three subjects, each with an associated number from 1 to 3:</source>
          <target state="translated">3つの被験者がいて、それぞれに1から3までの番号が関連付けられていると想像してください。</target>
        </trans-unit>
        <trans-unit id="8781d615fd77be9578225c40ac67b9471394cced" translate="yes" xml:space="preserve">
          <source>Implementation</source>
          <target state="translated">Implementation</target>
        </trans-unit>
        <trans-unit id="8d522809f4125f5930c1f4f77ec91f8735a003d8" translate="yes" xml:space="preserve">
          <source>Implementation based on &lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt; 基づく実装、独立成分分析：アルゴリズムとアプリケーション、ニューラルネットワーク、13（4-5）、2000、pp.411-430</target>
        </trans-unit>
        <trans-unit id="b6ac8df85fe47d2d00b4a78e1facdef4fbcae73b" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. Furthermore SVC multi-class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest. It is possible to implement one vs the rest with SVC by using the &lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; wrapper. Finally SVC can fit dense data without memory copy if the input is C-contiguous. Sparse data will still incur memory copy though.</source>
          <target state="translated">libsvmを使用したサポートベクターマシン分類器の実装：カーネルは非線形にすることができますが、そのSMOアルゴリズムは、LinearSVCのように多数のサンプルにスケーリングしません。さらに、SVCマルチクラスモードは1対1のスキームを使用して実装され、LinearSVCは残りの1対を使用します。&lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt;ラッパーを使用することで、SVCを使用して残りの1つを実装することが可能です。最後に、入力がC隣接である場合、SVCはメモリコピーなしで密データに適合できます。スパースデータでもメモリコピーは発生します。</target>
        </trans-unit>
        <trans-unit id="78b58091d5da65fb36aa747d9975841d97302dec" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using the same library as this class (liblinear).</source>
          <target state="translated">本クラスと同じライブラリ(liblinear)を用いたサポートベクターマシン分類器の実装。</target>
        </trans-unit>
        <trans-unit id="0faf8832b17d93a1b230f7a6ca4feb36d15cc4ac" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does.</source>
          <target state="translated">libsvmを用いたサポートベクターマシン回帰の実装:カーネルは非線形にすることができますが、そのSMOアルゴリズムはLinearSVCのように大量のサンプル数にはスケールしません。</target>
        </trans-unit>
        <trans-unit id="adae10003f16f5885f71700e866f2cc76e2c6af9" translate="yes" xml:space="preserve">
          <source>Implements feature hashing, aka the hashing trick.</source>
          <target state="translated">機能ハッシュ、別名ハッシュトリックを実装します。</target>
        </trans-unit>
        <trans-unit id="d98e09b894119d4a2d55bf3e2f04052ec103359a" translate="yes" xml:space="preserve">
          <source>Implements resampling with replacement. If False, this will implement (sliced) random permutations.</source>
          <target state="translated">置換による再サンプリングを実装します。Falseの場合、これは(スライスされた)ランダムな並べ替えを実装します。</target>
        </trans-unit>
        <trans-unit id="9f9d0b6a3b9dbc770ff8e17c3a6979d6ebb5425d" translate="yes" xml:space="preserve">
          <source>Implements the Birch clustering algorithm.</source>
          <target state="translated">Birchクラスタリングアルゴリズムを実装しています.</target>
        </trans-unit>
        <trans-unit id="82028db75262dc1a82a0dc4cf2e6f254032ff9f7" translate="yes" xml:space="preserve">
          <source>Implements the incremental PCA model from: &lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; See &lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; からのインクリメンタルPCAモデルを実装します。 2008年5月&lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;。http：&lt;/a&gt;//www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdfを参照</target>
        </trans-unit>
        <trans-unit id="06be3bf25c44efb35fdd12e8816c051b94a6e5d6" translate="yes" xml:space="preserve">
          <source>Implements the probabilistic PCA model from: &lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt;Tipping, M. E., and Bishop, C. M. (1999). &amp;ldquo;Probabilistic principal component analysis&amp;rdquo;. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. via the score and score_samples methods. See &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt; Tipping、ME、and Bishop、CM（1999）の確率的PCAモデルを実装します。「確率論的主成分分析」。王立統計学会誌：シリーズB（統計手法）、61（3）、611-622。scoreメソッドとscore_samplesメソッドを使用します。&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdfを&lt;/a&gt;参照してください</target>
        </trans-unit>
        <trans-unit id="496d8573358dc0bbf8fad0d466b95c37d153e5fd" translate="yes" xml:space="preserve">
          <source>Importance of Feature Scaling</source>
          <target state="translated">フィーチャースケーリングの重要性</target>
        </trans-unit>
        <trans-unit id="dee0fbd7a096536203f3e083c7a95f20ef772057" translate="yes" xml:space="preserve">
          <source>Important members are fit, predict.</source>
          <target state="translated">重要なメンバーはフィット、予測。</target>
        </trans-unit>
        <trans-unit id="0004bf233145469d6159f141af0ae0b05f3c5e9a" translate="yes" xml:space="preserve">
          <source>Imputation transformer for completing missing values.</source>
          <target state="translated">欠落している値を補完するためのインピュテーション変換器。</target>
        </trans-unit>
        <trans-unit id="8154b566118976ff2097cfffb2c92470797b0a69" translate="yes" xml:space="preserve">
          <source>Impute all missing values in X.</source>
          <target state="translated">Xの欠落している値をすべてインプリートします。</target>
        </trans-unit>
        <trans-unit id="510c592fb9a4fd828788fc0bdd902c165ca78889" translate="yes" xml:space="preserve">
          <source>Imputing missing values before building an estimator</source>
          <target state="translated">推定器を構築する前に欠落値を注入する</target>
        </trans-unit>
        <trans-unit id="e5d148df74ab3f703a9d283fda0c99f4936ff674" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt;, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in &lt;code&gt;ElasticNet&lt;/code&gt;, we control the combination of L1 and L2 with the &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) parameter, and the intensity of the regularization with the &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) parameter. Then the priors terms are:</source>
          <target state="translated">で&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;、L1およびL2事前確率モデルを正則化するために、損失関数に追加することができます。L2の事前分布はフロベニウスノルムを使用し、L1の事前分布は要素ごとのL1ノルムを使用します。 &lt;code&gt;ElasticNet&lt;/code&gt; と同様に、L1とL2の組み合わせを &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ rho \））パラメーターで制御し、正則化の強度を &lt;code&gt;alpha&lt;/code&gt; （\（\ alpha \））パラメーターで制御します。次に、事前条件は次のとおりです。</target>
        </trans-unit>
        <trans-unit id="eee03375c59654f18a55a7961e4f26a36fbc2cee" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt;&lt;code&gt;OutputCodeClassifier&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;code_size&lt;/code&gt; attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt; &lt;code&gt;OutputCodeClassifier&lt;/code&gt; &lt;/a&gt;、 &lt;code&gt;code_size&lt;/code&gt; 属性は、ユーザが使用する分類子の数を制御することを可能にします。クラスの総数に対する割合です。</target>
        </trans-unit>
        <trans-unit id="92869ea1268ab85728d32cc145b2fc2a3cf98201" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, if data for classification are unbalanced (e.g. many positive and few negative), set &lt;code&gt;class_weight='balanced'&lt;/code&gt; and/or try different penalty parameters &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;、場合分類のためのデータがアンバランスである（例えば、多くの肯定的及び少数陰性）、セット &lt;code&gt;class_weight='balanced'&lt;/code&gt; 及び/又は異なるペナルティパラメータ試みる &lt;code&gt;C&lt;/code&gt; を。</target>
        </trans-unit>
        <trans-unit id="d4a2dd93e9c8bc18123ea577d336109c88e8c2c3" translate="yes" xml:space="preserve">
          <source>In &lt;strong&gt;averaging methods&lt;/strong&gt;, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</source>
          <target state="translated">で&lt;strong&gt;メソッドを平均化&lt;/strong&gt;、駆動原理は平均的な彼らの予測に独立して、いくつかの推定を構築することです。平均して、結合された推定量は、その分散が減少するため、通常、単一ベースの推定量よりも優れています。</target>
        </trans-unit>
        <trans-unit id="baeac0931c0e4b4385579000935f2bb52ceb9f07" translate="yes" xml:space="preserve">
          <source>In a binary classification task, the terms &amp;lsquo;&amp;rsquo;positive&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;negative&amp;rsquo;&amp;rsquo; refer to the classifier&amp;rsquo;s prediction, and the terms &amp;lsquo;&amp;rsquo;true&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;false&amp;rsquo;&amp;rsquo; refer to whether that prediction corresponds to the external judgment (sometimes known as the &amp;lsquo;&amp;rsquo;observation&amp;rsquo;&amp;lsquo;). Given these definitions, we can formulate the following table:</source>
          <target state="translated">バイナリ分類タスクでは、用語「ポジティブ」および「ネガティブ」は分類子の予測を指し、用語「true」および「false」はその予測が外部判断に対応するかどうかを示します（ 「オブザベーション」としても知られています）。これらの定義を前提として、次の表を作成できます。</target>
        </trans-unit>
        <trans-unit id="995a1ae8b5be72e5d8cbdf391052b665e77e9964" translate="yes" xml:space="preserve">
          <source>In a first step, the hierarchical clustering is performed without connectivity constraints on the structure and is solely based on distance, whereas in a second step the clustering is restricted to the k-Nearest Neighbors graph: it&amp;rsquo;s a hierarchical clustering with structure prior.</source>
          <target state="translated">最初のステップでは、階層的クラスタリングは構造の接続制約なしで実行され、距離のみに基づいています。一方、2番目のステップでは、クラスタリングはk-Nearest Neighborsグラフに制限されます。これは、事前構造を持つ階層的クラスタリングです。</target>
        </trans-unit>
        <trans-unit id="0336cb4d8e7c9adb72abdea9417802db49cccd1f" translate="yes" xml:space="preserve">
          <source>In a large text corpus, some words will be very present (e.g. &amp;ldquo;the&amp;rdquo;, &amp;ldquo;a&amp;rdquo;, &amp;ldquo;is&amp;rdquo; in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.</source>
          <target state="translated">大きなテキストコーパスでは、いくつかの単語は非常に存在します（たとえば、英語では「the」、「a」、「is」）。したがって、ドキュメントの実際の内容に関する意味のある情報はほとんどありません。直接カウントデータを分類子に直接フィードする場合、これらの非常に頻繁な用語は、より珍しいがより興味深い用語の頻度を覆い隠します。</target>
        </trans-unit>
        <trans-unit id="cd0ed349168abd35a1fce87e6ae9e8ccc5ff58f4" translate="yes" xml:space="preserve">
          <source>In a nutshell, the following table summarizes the solvers characteristics:</source>
          <target state="translated">一言で言えば、ソルバーの特徴をまとめると以下のようになります。</target>
        </trans-unit>
        <trans-unit id="b4ec4bcaff3e86d4d99c938f5623ab4b737e65c7" translate="yes" xml:space="preserve">
          <source>In a real world setting, the &lt;code&gt;n_features&lt;/code&gt; parameter can be left to its default value of &lt;code&gt;2 ** 20&lt;/code&gt; (roughly one million possible features). If memory or downstream models size is an issue selecting a lower value such as &lt;code&gt;2 **
18&lt;/code&gt; might help without introducing too many additional collisions on typical text classification tasks.</source>
          <target state="translated">実際の設定では、 &lt;code&gt;n_features&lt;/code&gt; パラメーターをデフォルト値の &lt;code&gt;2 ** 20&lt;/code&gt; （約100万個の可能な機能）のままにすることができます。メモリまたはダウンストリームモデルのサイズが問題である場合、 &lt;code&gt;2 ** 18&lt;/code&gt; などの低い値を選択すると、通常のテキスト分類タスクで追加の衝突が多くなりすぎずに役立つ場合があります。</target>
        </trans-unit>
        <trans-unit id="c75e295f24e05d06cacc1a2f9bb570a61bdd802e" translate="yes" xml:space="preserve">
          <source>In a similar manner, the boston housing data set is used to show the impact of transforming the targets before learning a model. In this example, the targets to be predicted corresponds to the weighted distances to the five Boston employment centers.</source>
          <target state="translated">同様に、モデルを学習する前にターゲットを変換した場合の影響を示すために、ボストンの住宅データセットを使用しています。この例では、予測されるターゲットは、ボストンの5つの雇用センターまでの加重距離に対応しています。</target>
        </trans-unit>
        <trans-unit id="c210295417ef2f29cc593be46bbc5efed5892a5f" translate="yes" xml:space="preserve">
          <source>In addition of using an imputing method, we can also keep an indication of the missing information using &lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt;&lt;/a&gt; which might carry some information.</source>
          <target state="translated">補完メソッドを使用するだけでなく、&lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt; &lt;/a&gt;を使用して欠落している情報を示すこともできます。</target>
        </trans-unit>
        <trans-unit id="45e3263783ee36dc90c5821ae7ffc8d210750151" translate="yes" xml:space="preserve">
          <source>In addition to its current contents, this module will eventually be home to refurbished versions of Pipeline and FeatureUnion.</source>
          <target state="translated">現在の内容に加えて、このモジュールは最終的にPipelineとFeatureUnionの改修版のホームになる予定です。</target>
        </trans-unit>
        <trans-unit id="3dda0db479e61a3dc2539c918fe85f072a3cc4a4" translate="yes" xml:space="preserve">
          <source>In addition to standard scikit-learn estimator API, GaussianProcessRegressor:</source>
          <target state="translated">標準的なscikit-learnの推定APIに加えて、GaussianProcessRegressorがあります。</target>
        </trans-unit>
        <trans-unit id="6c259b1081efe473add72a6c01ee26dbc96ee486" translate="yes" xml:space="preserve">
          <source>In addition to the mean of the predictive distribution, also its standard deviation can be returned.</source>
          <target state="translated">予測分布の平均に加えて,その標準偏差も返すことができます.</target>
        </trans-unit>
        <trans-unit id="f5f01da0b407208bd57121f71ed7408cbbce3fe6" translate="yes" xml:space="preserve">
          <source>In addition, as there is no useful information in the intensity of the image, or its gradient, we choose to perform the spectral clustering on a graph that is only weakly informed by the gradient. This is close to performing a Voronoi partition of the graph.</source>
          <target state="translated">さらに、画像の強度やグラデーションには有用な情報がないので、グラデーションの情報が弱いグラフ上でスペクトルクラスタリングを実行することを選択します。これはグラフのボロノイ分割の実行に近いものです。</target>
        </trans-unit>
        <trans-unit id="8be2789c3e2f5a1d410c34b62e20c28c8adc9fef" translate="yes" xml:space="preserve">
          <source>In addition, if the &lt;code&gt;dask&lt;/code&gt; and &lt;code&gt;distributed&lt;/code&gt; Python packages are installed, it is possible to use the &amp;lsquo;dask&amp;rsquo; backend for better scheduling of nested parallel calls without over-subscription and potentially distribute parallel calls over a networked cluster of several hosts.</source>
          <target state="translated">さらに、 &lt;code&gt;dask&lt;/code&gt; および &lt;code&gt;distributed&lt;/code&gt; Pythonパッケージがインストールされている場合は、「dask」バックエンドを使用して、オーバーサブスクリプションなしでネストされた並列呼び出しのスケジューリングを改善し、複数のホストのネットワーク化されたクラスターに並列呼び出しを分散することができます。</target>
        </trans-unit>
        <trans-unit id="1068dbee0dd3c16e2fc93e44b0ce455f5b052f8b" translate="yes" xml:space="preserve">
          <source>In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.</source>
          <target state="translated">さらに、scikit-learnには、サイズと複雑さが制御された人工的なデータセットを構築するために使用できる様々なランダムサンプルジェネレータが含まれています。</target>
        </trans-unit>
        <trans-unit id="67ed28f1d0cd9fef0560dd0bbfe2b568680ea5a6" translate="yes" xml:space="preserve">
          <source>In addition, there are also miscellanous tools to load datasets of other formats or from other locations, described in the &lt;a href=&quot;#loading-other-datasets&quot;&gt;Loading other datasets&lt;/a&gt; section.</source>
          <target state="translated">さらに、&lt;a href=&quot;#loading-other-datasets&quot;&gt;他のデータセット&lt;/a&gt;のロードセクションで説明されている、他の形式のデータセットまたは他の場所からロードするためのその他のツールもあります。</target>
        </trans-unit>
        <trans-unit id="846c6c3d11b49bd5243a8b72c066c7aae06dcfe3" translate="yes" xml:space="preserve">
          <source>In addition, we use the mask of the objects to restrict the graph to the outline of the objects. In this example, we are interested in separating the objects one from the other, and not from the background.</source>
          <target state="translated">さらに、オブジェクトのマスクを使用して、グラフをオブジェクトの輪郭に限定しています。この例では、オブジェクトを背景からではなく、1つのオブジェクトから他のオブジェクトに分離したいと考えています。</target>
        </trans-unit>
        <trans-unit id="b490744f01019d4237b3a8568465e031a5ae6e1f" translate="yes" xml:space="preserve">
          <source>In all these strategies, the &lt;code&gt;predict&lt;/code&gt; method completely ignores the input data.</source>
          <target state="translated">これらすべての戦略で、 &lt;code&gt;predict&lt;/code&gt; メソッドは入力データを完全に無視します。</target>
        </trans-unit>
        <trans-unit id="450c8a41f7c5da3bb2b7da9a15306b194b36c681" translate="yes" xml:space="preserve">
          <source>In an &lt;strong&gt;unsupervised setting&lt;/strong&gt; it can be used to group similar documents together by applying clustering algorithms such as &lt;a href=&quot;clustering#k-means&quot;&gt;K-means&lt;/a&gt;:</source>
          <target state="translated">で&lt;strong&gt;教師なしの設定&lt;/strong&gt;、それは、次のようなクラスタリングアルゴリズムを適用することによって、一緒にグループに同様のドキュメントを使用することができる&lt;a href=&quot;clustering#k-means&quot;&gt;K-手段&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e5af8b183011d6bf7238d58f71b94384a5a96f84" translate="yes" xml:space="preserve">
          <source>In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process.</source>
          <target state="translated">いずれにしても、上記のようにモデルの複雑さを減らすと精度が低下する可能性があることに注意してください。例えば、非線形に分離可能な問題は、高速な線形モデルで処理することができますが、その過程で予測力が低下する可能性が高いです。</target>
        </trans-unit>
        <trans-unit id="8c17ce8abfedb506f7ed46ebde27121f581c8bb7" translate="yes" xml:space="preserve">
          <source>In applications where a high false positive rate is not tolerable the parameter &lt;code&gt;max_fpr&lt;/code&gt; of &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; can be used to summarize the ROC curve up to the given limit.</source>
          <target state="translated">高い偽陽性率は、パラメータの許容されていないアプリケーションでは &lt;code&gt;max_fpr&lt;/code&gt; の&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; を&lt;/a&gt;与えられた限界までROC曲線を要約するために使用することができます。</target>
        </trans-unit>
        <trans-unit id="bbcc07c440f8193b3e7ecf64eaaca0e386adcbad" translate="yes" xml:space="preserve">
          <source>In bin edges for feature &lt;code&gt;i&lt;/code&gt;, the first and last values are used only for &lt;code&gt;inverse_transform&lt;/code&gt;. During transform, bin edges are extended to:</source>
          <target state="translated">特徴 &lt;code&gt;i&lt;/code&gt; の &lt;code&gt;inverse_transform&lt;/code&gt; は、最初と最後の値は、inverse_transformに対してのみ使用されます。変換中、ビンのエッジは次のように拡張されます。</target>
        </trans-unit>
        <trans-unit id="9d083c0b55e1a00133d4b27dd85f5ea26aca3922" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, the Jaccard similarity coefficient score is equal to the classification accuracy.</source>
          <target state="translated">二元分類や多元分類では、ジャカード類似度係数スコアは分類精度に等しい。</target>
        </trans-unit>
        <trans-unit id="b0b30958f6975dadd3d7eaf24f5447254d56c629" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equal to the &lt;code&gt;jaccard_similarity_score&lt;/code&gt; function.</source>
          <target state="translated">バイナリおよびマルチクラス分類では、この関数は &lt;code&gt;jaccard_similarity_score&lt;/code&gt; 関数と同じです。</target>
        </trans-unit>
        <trans-unit id="e731c9654f26a8d7255165d2c0d5f78e47c3bd9a" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equivalent to the &lt;code&gt;accuracy_score&lt;/code&gt;. It differs in the multilabel classification problem.</source>
          <target state="translated">バイナリおよびマルチクラス分類では、この関数は &lt;code&gt;accuracy_score&lt;/code&gt; スコアと同等です。マルチラベル分類の問題が異なります。</target>
        </trans-unit>
        <trans-unit id="c4cb57bb3e2c0bb1485829473f6ca6362cee5b90" translate="yes" xml:space="preserve">
          <source>In binary class case, assuming labels in y_true are encoded with +1 and -1, when a prediction mistake is made, &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; is always negative (since the signs disagree), implying &lt;code&gt;1 - margin&lt;/code&gt; is always greater than 1. The cumulated hinge loss is therefore an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">バイナリクラスの場合、y_trueのラベルが+1と-1でエンコードされていると仮定すると、予測ミスが発生した場合、 &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; は常に負であり（符号が一致しないため）、 &lt;code&gt;1 - margin&lt;/code&gt; を意味します-marginは常に1より大きいです。したがって、累積されたヒンジ損失は、分類子によって行われた誤りの数の上限です。</target>
        </trans-unit>
        <trans-unit id="f2c8a5d61695d64c32adbdec8051b4ecc7f404cb" translate="yes" xml:space="preserve">
          <source>In binary classification settings</source>
          <target state="translated">二値分類の設定では</target>
        </trans-unit>
        <trans-unit id="0e8524872beef3003e748e1d9b4f90c7ce280313" translate="yes" xml:space="preserve">
          <source>In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve &lt;code&gt;n_iter_no_change&lt;/code&gt; times in a row. The improvement is evaluated with a tolerance &lt;code&gt;tol&lt;/code&gt;, and the algorithm stops in any case after a maximum number of iteration &lt;code&gt;max_iter&lt;/code&gt;.</source>
          <target state="translated">どちらの場合も、基準はエポックによって1回評価され、基準が連続して &lt;code&gt;n_iter_no_change&lt;/code&gt; 時間を改善しない場合、アルゴリズムは停止します。改善は許容誤差 &lt;code&gt;tol&lt;/code&gt; で評価され、アルゴリズムは最大反復回数 &lt;code&gt;max_iter&lt;/code&gt; の後でいずれにしても停止します。</target>
        </trans-unit>
        <trans-unit id="f03bffae8069d1108a85252dc34a4485434865b8" translate="yes" xml:space="preserve">
          <source>In both cases, the kernel&amp;rsquo;s parameters are estimated using the maximum likelihood principle.</source>
          <target state="translated">どちらの場合も、カーネルのパラメーターは最尤法を使用して推定されます。</target>
        </trans-unit>
        <trans-unit id="dc8004c8d5b437fc6c479e2e5882eb635fa97592" translate="yes" xml:space="preserve">
          <source>In both examples below, the main result is that the empirical covariance estimate, as a non-robust one, is highly influenced by the heterogeneous structure of the observations. Although the robust covariance estimate is able to focus on the main mode of the data distribution, it sticks to the assumption that the data should be Gaussian distributed, yielding some biased estimation of the data structure, but yet accurate to some extent. The One-Class SVM does not assume any parametric form of the data distribution and can therefore model the complex shape of the data much better.</source>
          <target state="translated">以下の両方の事例において、主な結果は、経験的な共分散推定が、非ロバストなものとして、オブザベーションの不均一構造に大きく影響されることです。ロバスト共分散推定は、データ分布の主なモードに焦点を当てることができますが、データがガウス分布であるべきだという仮定に固執し、データ構造の推定に若干の偏りが生じますが、ある程度正確です。ワンクラスSVMはデータ分布のパラメトリックな形態を仮定しないので、データの複雑な形状をより良くモデル化することができます。</target>
        </trans-unit>
        <trans-unit id="7f2b051010be4e679b8556fa182a1724fa1e49b9" translate="yes" xml:space="preserve">
          <source>In case the file contains a pairwise preference constraint (known as &amp;ldquo;qid&amp;rdquo; in the svmlight format) these are ignored unless the query_id parameter is set to True. These pairwise preference constraints can be used to constraint the combination of samples when using pairwise loss functions (as is the case in some learning to rank problems) so that only pairs with the same query_id value are considered.</source>
          <target state="translated">ファイルにペアワイズ設定制約（svmlight形式では「qid」と呼ばれる）が含まれている場合、query_idパラメーターがTrueに設定されていない限り、これらは無視されます。これらのペアワイズ優先制約は、ペアワイズ損失関数を使用するときにサンプルの組み合わせを制約するために使用でき（問題をランク付けする学習の場合のように）、同じquery_id値を持つペアのみが考慮されます。</target>
        </trans-unit>
        <trans-unit id="b2d3bbc7ab1d1edcd850a10f6fb01a251c14a28b" translate="yes" xml:space="preserve">
          <source>In case unknown categories are encountered (all zero&amp;rsquo;s in the one-hot encoding), &lt;code&gt;None&lt;/code&gt; is used to represent this category.</source>
          <target state="translated">不明なカテゴリが検出された場合（ワンホットエンコーディングではすべてゼロ）、 &lt;code&gt;None&lt;/code&gt; を使用してこのカテゴリを表します。</target>
        </trans-unit>
        <trans-unit id="ca499264726caa99e06bab43fc3d4644ea84df01" translate="yes" xml:space="preserve">
          <source>In cases where not all of a pairwise distance matrix needs to be stored at once, this is used to calculate pairwise distances in &lt;code&gt;working_memory&lt;/code&gt;-sized chunks. If &lt;code&gt;reduce_func&lt;/code&gt; is given, it is run on each chunk and its return values are concatenated into lists, arrays or sparse matrices.</source>
          <target state="translated">すべてのペアワイズ距離マトリックスを一度に格納する必要がない場合は、これを使用して &lt;code&gt;working_memory&lt;/code&gt; サイズのチャンクのペアワイズ距離を計算します。 &lt;code&gt;reduce_func&lt;/code&gt; が指定されている場合、各チャンクで実行され、その戻り値はリスト、配列、または疎行列に連結されます。</target>
        </trans-unit>
        <trans-unit id="261de18f8066fcaced5cb3f145cb26c170301e09" translate="yes" xml:space="preserve">
          <source>In cases where the data is not uniformly sampled, radius-based neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt;&lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt;&lt;/a&gt; can be a better choice. The user specifies a fixed radius \(r\), such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;.</source>
          <target state="translated">データが均一にサンプリングされていない場合は、&lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt; &lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt; &lt;/a&gt;での半径ベースの近傍分類の方が適しています。ユーザーは固定半径\（r \）を指定します。これにより、まばらな近傍のポイントは、分類に使用する最近傍を少なくします。高次元のパラメーター空間の場合、この方法は、いわゆる「次元の呪い」のために効果が低くなります。</target>
        </trans-unit>
        <trans-unit id="46149a533d1136e96a72fc2595f06ccb02814862" translate="yes" xml:space="preserve">
          <source>In certain cases Theil-Sen performs better than &lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt; which is also a robust method. This is illustrated in the second example below where outliers with respect to the x-axis perturb RANSAC. Tuning the &lt;code&gt;residual_threshold&lt;/code&gt; parameter of RANSAC remedies this but in general a priori knowledge about the data and the nature of the outliers is needed. Due to the computational complexity of Theil-Sen it is recommended to use it only for small problems in terms of number of samples and features. For larger problems the &lt;code&gt;max_subpopulation&lt;/code&gt; parameter restricts the magnitude of all possible combinations of p subsample points to a randomly chosen subset and therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger problems with the drawback of losing some of its mathematical properties since it then works on a random subset.</source>
          <target state="translated">場合によっては、Theil-Senは、堅牢な方法である&lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;よりもパフォーマンスが優れています。これは、x軸に関する外れ値がRANSACを混乱させる以下の2番目の例に示されています。チューニング &lt;code&gt;residual_threshold&lt;/code&gt; の RANSAC救済のパラメータにこれを一般的には、データや外れ値の性質についての事前知識が必要とされています。 Theil-Senは計算が複雑であるため、サンプルと機能の数の点で小さな問題に対してのみ使用することをお勧めします。より大きな問題の場合、 &lt;code&gt;max_subpopulation&lt;/code&gt; パラメータは、p個のサブサンプルポイントのすべての可能な組み合わせの大きさをランダムに選択されたサブセットに制限し、実行時間も制限します。したがって、Theil-Senはより大きな問題に適用でき、ランダムなサブセットで機能するため、その数学的特性の一部が失われるという欠点があります。</target>
        </trans-unit>
        <trans-unit id="08403787ed9849b402f6d04f68a0bae46063dfaf" translate="yes" xml:space="preserve">
          <source>In contrast to &lt;a href=&quot;#id13&quot;&gt;Bayesian Ridge Regression&lt;/a&gt;, each coordinate of \(w_{i}\) has its own standard deviation \(\lambda_i\). The prior over all \(\lambda_i\) is chosen to be the same gamma distribution given by hyperparameters \(\lambda_1\) and \(\lambda_2\).</source>
          <target state="translated">&lt;a href=&quot;#id13&quot;&gt;ベイジアンリッジ回帰&lt;/a&gt;とは対照的に、\（w_ {i} \）の各座標には独自の標準偏差\（\ lambda_i \）があります。すべての前の\（\ lambda_i \）は、ハイパーパラメーター\（\ lambda_1 \）および\（\ lambda_2 \）によって与えられる同じガンマ分布になるように選択されます。</target>
        </trans-unit>
        <trans-unit id="741dc2ca1b0b96b753a4293cdc66da483cb961b9" translate="yes" xml:space="preserve">
          <source>In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.</source>
          <target state="translated">GridSearchCVとは異なり,すべてのパラメータ値を試すのではなく,指定された分布の中から一定数のパラメータ設定をサンプリングします.試されるパラメータ設定数は n_iter で与えられます.</target>
        </trans-unit>
        <trans-unit id="f7007cbebb915951a7329a621ec59e7bfd3c1528" translate="yes" xml:space="preserve">
          <source>In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.</source>
          <target state="translated">多数決(ハード投票)とは対照的に、ソフト投票は、予測された確率の総和のargmaxとしてクラスラベルを返します。</target>
        </trans-unit>
        <trans-unit id="a5222e41535c7e60d0bed8020d5a39a4cdb9c58d" translate="yes" xml:space="preserve">
          <source>In contrast to the original publication &lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]&lt;/a&gt;, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.</source>
          <target state="translated">元の出版物&lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]&lt;/a&gt;とは対照的に、scikit-learnの実装では、各分類子に単一のクラスに投票させるのではなく、確率予測を平均化することで分類子を組み合わせます。</target>
        </trans-unit>
        <trans-unit id="092465bd0b61837459fb29bf14c2dda6ed20e949" translate="yes" xml:space="preserve">
          <source>In contrast to the regression setting, the posterior of the latent function \(f\) is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">回帰設定とは対照的に、潜在クラス\（f \）の事後は、ガウス分布の尤度は離散クラスラベルには不適切であるため、GP以前の場合でもガウス分布ではありません。むしろ、ロジスティックリンク関数（ロジット）に対応する非ガウス尤度が使用されます。 GaussianProcessClassifierは、ラプラス近似に基づいて、ガウス以外の事後をガウスで近似します。詳細については、&lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]の&lt;/a&gt;第3章を参照してください。</target>
        </trans-unit>
        <trans-unit id="2d7f12a42ea8277b24625f1aff8d53cb363a14e0" translate="yes" xml:space="preserve">
          <source>In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to \(\frac{1}{\text{n\_classes}}\).</source>
          <target state="translated">これに対して,従来の精度が偶然性を超えているのは,分類器が不均衡なテストセットを利用しているからに過ぎないとすると,均衡した精度は,適宜,\(\frac{1}{\text{n\_classes}})まで落ちてしまうことになる.</target>
        </trans-unit>
        <trans-unit id="89611c1358b346353d5469c5670ea64fb02fcdd7" translate="yes" xml:space="preserve">
          <source>In descending order of quality, when trained (outside of this example) on all 4 features using 30 estimators and scored using 10 fold cross validation, we see:</source>
          <target state="translated">品質の高い順に、30 個の推定子を使用して 4 つの特徴すべてについて(この例以外で)学習し、10 倍のクロスバリデーションを使用してスコアリングすると、次のようになります。</target>
        </trans-unit>
        <trans-unit id="0732cca6c2251b860da4c331fa5748d479b14945" translate="yes" xml:space="preserve">
          <source>In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).</source>
          <target state="translated">アンサンブルアルゴリズムでは、バギング法は、元の訓練セットのランダムな部分集合上にブラックボックス推定器の複数のインスタンスを構築し、それらの個々の予測を集約して最終予測を形成するアルゴリズムのクラスを形成する。これらの手法は、基本推定量(例えば、決定木)の構築手順にランダム化を導入し、その中からアンサンブルを作成することで、基本推定量(例えば、決定木)の分散を減らす方法として使用される。多くの場合、バギング手法は、基礎となる基本アルゴリズムを適応させる必要がなく、単一モデルに関して改善する非常にシンプルな方法を構成している。オーバーフィットを減らす方法を提供するので、バギング手法は、強力で複雑なモデル(例えば、完全に開発された決定木)に最適に働き、通常は弱いモデル(例えば、浅い決定木)に最適に働くブースティング手法とは対照的です。</target>
        </trans-unit>
        <trans-unit id="5305d1e9b70806a8391e61e804a0df6abd8f6cc5" translate="yes" xml:space="preserve">
          <source>In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the &lt;code&gt;average&lt;/code&gt; parameter.</source>
          <target state="translated">バイナリメトリックをマルチクラスまたはマルチラベルの問題に拡張する場合、データは、クラスごとに1つずつ、バイナリ問題のコレクションとして扱われます。次に、クラスのセット全体でバイナリメトリック計算を平均化する方法がいくつかあります。それぞれの方法は、いくつかのシナリオで役立ちます。可能な場合は、 &lt;code&gt;average&lt;/code&gt; パラメーターを使用してこれらの中から選択する必要があります。</target>
        </trans-unit>
        <trans-unit id="e87cfc9dff0fe670bd40ebf7e26edaa15ca842ad" translate="yes" xml:space="preserve">
          <source>In extremely randomized trees (see &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt;&lt;code&gt;ExtraTreesRegressor&lt;/code&gt;&lt;/a&gt; classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias:</source>
          <target state="translated">極端にランダム化されたツリー（&lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt;および&lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt; &lt;code&gt;ExtraTreesRegressor&lt;/code&gt; &lt;/a&gt;クラスを参照）では、ランダム性は、スプリットの計算方法をさらに一歩進めます。ランダムフォレストの場合と同様に、候補フィーチャのランダムサブセットが使用されますが、最も特徴的なしきい値を探す代わりに、各候補フィーチャに対してランダムにしきい値が描画され、ランダムに生成されたこれらのしきい値のうち最良のものが分割ルールとして選択されます。これにより通常、モデルの分散をもう少し減らすことができますが、バイアスが少し大きくなります。</target>
        </trans-unit>
        <trans-unit id="5c1305e3ce4cbb99adc8d313e42a43efab81ea5c" translate="yes" xml:space="preserve">
          <source>In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:</source>
          <target state="translated">実際,このデータセットには1つのバージョンしかない.一方,虹彩データセットには複数のバージョンがある.</target>
        </trans-unit>
        <trans-unit id="63493dde535d33b43819cf48666bb2a9620c2476" translate="yes" xml:space="preserve">
          <source>In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.</source>
          <target state="translated">フランス語ですが、まだ参照してください。Tenenhaus,M.(1998).回帰PLS:理論と実践.パリ。エディション・テクニック。</target>
        </trans-unit>
        <trans-unit id="6e95c3ada3b2525ed5f608da19594b4a42ad3dc4" translate="yes" xml:space="preserve">
          <source>In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:</source>
          <target state="translated">一般的に、多くの理由(分岐予測可能性、CPUキャッシュ、線形代数ライブラリの最適化など)から、バルク(多数のインスタンスを同時に)で予測を行う方が効率的である。ここでは、いくつかの特徴を持つ設定では、推定器の選択に関係なくバルクモードの方が常に高速であり、1~2桁の差があることを示している。</target>
        </trans-unit>
        <trans-unit id="d5f14cdf8cb9c0df1b6ffce69bd866cdeffd9355" translate="yes" xml:space="preserve">
          <source>In general, a learning problem considers a set of n &lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;samples&lt;/a&gt; of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;multivariate&lt;/a&gt; data), it is said to have several attributes or &lt;strong&gt;features&lt;/strong&gt;.</source>
          <target state="translated">一般に、学習問題はデータのn個の&lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;サンプル&lt;/a&gt;のセットを考慮し、不明なデータのプロパティを予測しようとします。各サンプルが単一の数値よりも多く、たとえば多次元エントリ（別名&lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;多変量&lt;/a&gt;データ）である場合、複数の属性または&lt;strong&gt;特徴&lt;/strong&gt;があると言われ&lt;strong&gt;ます&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="9cf7334c38597a2189c7af702ab9abdbe9f10093" translate="yes" xml:space="preserve">
          <source>In general, is a technique used for analyzing similarity or dissimilarity data. &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.</source>
          <target state="translated">一般的に、は類似性または非類似性データの分析に使用される手法です。&lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt;は、類似性または非類似性データを幾何学的空間の距離としてモデル化しようとします。データは、オブジェクト間の類似性の評価、分子の相互作用頻度、または国間の貿易指数です。</target>
        </trans-unit>
        <trans-unit id="71aab6786f00490669e72ac36911ce2d2486dab4" translate="yes" xml:space="preserve">
          <source>In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding \(p\)-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.</source>
          <target state="translated">一般的には、初期オブザベーション分布の輪郭を区切る大まかで近いフロンティアを学習しようとしていますが、これは、埋め込み(p\)次元空間にプロットされます。そして、さらなるオブザベーションがフロンティアで区切られた部分空間内にある場合、それらは初期オブザベーションと同じ母集団から来ていると考えられます。そうでなければ、それらがフロンティアの外にある場合、我々の評価で与えられた信頼度でそれらが異常であると言うことができる。</target>
        </trans-unit>
        <trans-unit id="c9bca25ec918e4e036ec8a37ec502896ec56d542" translate="yes" xml:space="preserve">
          <source>In general, learning algorithms benefit from standardization of the data set. If some outliers are present in the set, robust scalers or transformers are more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in &lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;Compare the effect of different scalers on data with outliers&lt;/a&gt;.</source>
          <target state="translated">一般に、学習アルゴリズムはデータセットの標準化から利益を得ます。セットに外れ値が存在する場合は、堅牢なスケーラーまたはトランスフォーマーがより適切です。限界外れ値を含むデータセット上のさまざまなスケーラー、トランスフォーマー、ノーマライザの動作は、「外れ値を持つデータ&lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;に対するさまざまなスケーラーの影響の比較」&lt;/a&gt;で強調表示されています。</target>
        </trans-unit>
        <trans-unit id="baeb2b7a43c2bc0dd04675c021d6ed663a58bf2d" translate="yes" xml:space="preserve">
          <source>In general, the run time cost to construct a balanced binary tree is \(O(n_{samples}n_{features}\log(n_{samples}))\) and query time \(O(\log(n_{samples}))\). Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through \(O(n_{features})\) to find the feature that offers the largest reduction in entropy. This has a cost of \(O(n_{features}n_{samples}\log(n_{samples}))\) at each node, leading to a total cost over the entire trees (by summing the cost at each node) of \(O(n_{features}n_{samples}^{2}\log(n_{samples}))\).</source>
          <target state="translated">一般的に、バランスのとれたバイナリツリーを構築するための実行時間コストは、\(O(n_{samples}n_{features})log(n_{samples}))であり、問い合わせ時間は、\(O(\log(n_{samples})))である。木の構築アルゴリズムは、バランスのとれた木を生成しようとしますが、必ずしもバランスがとれているとは限りません。部分木がほぼ均衡していると仮定すると、各ノードでのコストは、エントロピーの最大の削減をもたらす特徴を見つけるために、\(O(n_{features}))を検索することになる。これにより、各ノードでのコストは\(O(n_{features}n_{samples}\log(n_{samples})))となり、木全体でのコストは(各ノードでのコストを合計して)\(O(n_{features}n_{samples}^{2}\log(n_{samples}))となる。</target>
        </trans-unit>
        <trans-unit id="635895acc09f2d99381585bc2d144c9a66a85f3a" translate="yes" xml:space="preserve">
          <source>In gradient descent, the gradient \(\nabla Loss_{W}\) of the loss with respect to the weights is computed and deducted from \(W\). More formally, this is expressed as,</source>
          <target state="translated">勾配降下法では、重みに対する損失の勾配を計算し、\(W\(W)から差し引く。より正式には、これは次のように表されます。</target>
        </trans-unit>
        <trans-unit id="2c51a2af5a19ac0ce7e4fb04fd6d887c03b6fecb" translate="yes" xml:space="preserve">
          <source>In high-dimensional spaces, linear classifiers often achieve excellent accuracy. For sparse binary data, BernoulliNB is particularly well-suited. The bottom row compares the decision boundary obtained by BernoulliNB in the transformed space with an ExtraTreesClassifier forests learned on the original data.</source>
          <target state="translated">高次元空間では、線形分類器はしばしば優れた精度を達成する。疎なバイナリデータでは、BernoulliNBが特に適しています。下の行は、変換された空間でBernoulliNBによって得られた決定境界を、元のデータで学習したExtraTreesClassifierの森と比較したものです。</target>
        </trans-unit>
        <trans-unit id="7b577c96674cf299faa19ce0d11e2224d3c2c813" translate="yes" xml:space="preserve">
          <source>In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.</source>
          <target state="translated">多数決では,特定の標本の予測されたクラス・ラベルは,個々の分類器によって予測されたクラス・ラベルの多数派(モード)を表すクラス・ラベルである.</target>
        </trans-unit>
        <trans-unit id="589394183aec0e7af2afe4b456559f6baedc9992" translate="yes" xml:space="preserve">
          <source>In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application.</source>
          <target state="translated">多くの場合、アプリケーションの全体的なレイテンシが遅すぎる場合に最適化を開始するのに適した場所である可能性があるため、特徴抽出コードの時間を慎重に設定してプロファイルを作成することをお勧めします。</target>
        </trans-unit>
        <trans-unit id="aeae04273a5ed1fc88f796de718e3c2190c04f0d" translate="yes" xml:space="preserve">
          <source>In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness.</source>
          <target state="translated">多くのモデリングシナリオでは、データセットの特徴の正規性が望まれます。動力変換は,分散を安定させ,歪度を最小化するために,任意の分布から可能な限りガウス分布に近い分布にデータをマッピングすることを目的としたパラメトリックな単調変換の一群である.</target>
        </trans-unit>
        <trans-unit id="c82f65d47c3f4e11ad468a4165bdc787c51720a5" translate="yes" xml:space="preserve">
          <source>In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use &lt;code&gt;FeatureUnion&lt;/code&gt; to combine features obtained by PCA and univariate selection.</source>
          <target state="translated">多くの実際の例では、データセットから特徴を抽出する多くの方法があります。多くの場合、良いパフォーマンスを得るためにいくつかの方法を組み合わせることが有益です。この例は、 &lt;code&gt;FeatureUnion&lt;/code&gt; を使用して、PCAによって取得された特徴と単変量選択を組み合わせる方法を示しています。</target>
        </trans-unit>
        <trans-unit id="9c0b7f3861d3fe001968b978c49f3447d1233fa3" translate="yes" xml:space="preserve">
          <source>In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.</source>
          <target state="translated">数学では、ジョンソン-リンデンストラウス・リーマは、高次元空間から低次元ユークリッド空間への点の低歪み埋め込みに関する結果である。このリーマは、高次元空間の小さな点の集合は、点間の距離がほぼ保存されるような方法で、はるかに低い次元の空間に埋め込むことができることを述べています。埋め込みに用いられる写像は、少なくともリップシッツであり、直交射影であるとみなすこともできます。</target>
        </trans-unit>
        <trans-unit id="35a3805825da50966c5f8cb649b1d2ea852b8f59" translate="yes" xml:space="preserve">
          <source>In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of \(v\) and \(h\) given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes.</source>
          <target state="translated">対数尤度を最大化する場合、正の勾配は、観測された訓練データと互換性のある隠れた状態をモデルが好むようにする。RBMは二部構造であるため、効率的に計算できる。しかし、負の勾配は難解である。負の勾配の目的は、モデルが好む結合状態のエネルギーを下げることで、データに忠実な状態を維持することです。負の勾配は、ブロックギブスサンプリングを用いたマルコフ連鎖モンテカルロ法で近似することができます。このようにして生成されたサンプルを空想粒子と呼ぶことがある。これは非効率的で、マルコフ連鎖が混ざるかどうかを判断するのが難しい。</target>
        </trans-unit>
        <trans-unit id="54db7da5f1b2e2f16e8f4dc3a375dac661b78213" translate="yes" xml:space="preserve">
          <source>In multi-label classification, the &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function is extended by averaging over the labels as &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;.</source>
          <target state="translated">マルチラベル分類では、&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt;関数は&lt;a href=&quot;#average&quot;&gt;上記のように&lt;/a&gt;ラベルを平均することによって拡張されます。</target>
        </trans-unit>
        <trans-unit id="d9be5dcb267dcb84c278d12d7b1a881ada760886" translate="yes" xml:space="preserve">
          <source>In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.</source>
          <target state="translated">マルチラベル分類では、これはサブセット精度であり、各サンプルに対して各ラベルセットが正しく予測されることが要求されるため、厳しい指標となります。</target>
        </trans-unit>
        <trans-unit id="9ff5420b9cd3095ee44bf9941c38c72dce6d517a" translate="yes" xml:space="preserve">
          <source>In multi-label settings</source>
          <target state="translated">マルチラベル設定では</target>
        </trans-unit>
        <trans-unit id="cf7a69d811fd496380ea6a3966d13bf17ca83f43" translate="yes" xml:space="preserve">
          <source>In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the &lt;code&gt;average&lt;/code&gt; argument to the &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; (multilabel only), &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt;&lt;code&gt;f1_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt;&lt;code&gt;precision_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt;&lt;code&gt;recall_score&lt;/code&gt;&lt;/a&gt; functions, as described &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;. Note that if all labels are included, &amp;ldquo;micro&amp;rdquo;-averaging in a multiclass setting will produce precision, recall and \(F\) that are all identical to accuracy. Also note that &amp;ldquo;weighted&amp;rdquo; averaging may produce an F-score that is not between precision and recall.</source>
          <target state="translated">マルチクラスおよびマルチラベル分類タスクでは、精度、再現率、およびFメジャーの概念を各ラベルに個別に適用できます。指定されたラベルの両端の結果を結合するいくつかの方法がある &lt;code&gt;average&lt;/code&gt; の引数&lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt;（マルチラベルのみ）、&lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt; &lt;code&gt;f1_score&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt;、&lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt; &lt;code&gt;precision_score&lt;/code&gt; &lt;/a&gt;と&lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt; &lt;code&gt;recall_score&lt;/code&gt; &lt;/a&gt;記載されているような機能は、&lt;a href=&quot;#average&quot;&gt;上に&lt;/a&gt;。すべてのラベルが含まれている場合、マルチクラス設定での「マイクロ」平均は、精度と同じである精度、再現率、および\（F \）を生成することに注意してください。また、「加重」平均では、精度と再現率の間にないFスコアが生成される可能性があることに注意してください。</target>
        </trans-unit>
        <trans-unit id="afc91520f5287da47360dcd6fd00b4fb446bcf96" translate="yes" xml:space="preserve">
          <source>In multiclass case, the function expects that either all the labels are included in y_true or an optional labels argument is provided which contains all the labels. The multilabel margin is calculated according to Crammer-Singer&amp;rsquo;s method. As in the binary case, the cumulated hinge loss is an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">マルチクラスの場合、関数はすべてのラベルがy_trueに含まれるか、すべてのラベルを含むオプションのラベル引数が提供されることを期待します。マルチラベルマージンは、Crammer-Singerの方法に従って計算されます。バイナリの場合と同様に、累積されたヒンジ損失は、分類器によって行われた誤りの数の上限です。</target>
        </trans-unit>
        <trans-unit id="a7ec36140af641cfb5e4e5e11dec536798cfb2f8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss correspond to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is equivalent to the subset &lt;code&gt;zero_one_loss&lt;/code&gt; function.</source>
          <target state="translated">マルチクラス分類では、ハミング損失は、サブセット &lt;code&gt;zero_one_loss&lt;/code&gt; 関数と同等の &lt;code&gt;y_true&lt;/code&gt; と &lt;code&gt;y_pred&lt;/code&gt; の間のハミング距離に対応します。</target>
        </trans-unit>
        <trans-unit id="ff1916ae5265c4d87d1472e5cc3e0c2594a22de8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss corresponds to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is similar to the &lt;a href=&quot;#zero-one-loss&quot;&gt;Zero one loss&lt;/a&gt; function. However, while zero-one loss penalizes prediction sets that do not strictly match true sets, the Hamming loss penalizes individual labels. Thus the Hamming loss, upper bounded by the zero-one loss, is always between zero and one, inclusive; and predicting a proper subset or superset of the true labels will give a Hamming loss between zero and one, exclusive.</source>
          <target state="translated">マルチクラス分類では、ハミング損失は &lt;code&gt;y_true&lt;/code&gt; と &lt;code&gt;y_pred&lt;/code&gt; の間のハミング距離に対応します。これは、&lt;a href=&quot;#zero-one-loss&quot;&gt;ゼロ1損失&lt;/a&gt;関数に似ています。ただし、ゼロ1損失は真のセットと厳密には一致しない予測セットにペナルティを課しますが、ハミング損失は個々のラベルにペナルティを課します。したがって、ハミング損失は、ゼロ1損失によって上限が定められ、常に0と1の間にあります。真のラベルの適切なサブセットまたはスーパーセットを予測すると、ゼロと1の間のハミング損失が発生します。</target>
        </trans-unit>
        <trans-unit id="cf7ce831a18d046dad4e38dc2cae92648b792778" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt;&lt;code&gt;zero_one_loss&lt;/code&gt;&lt;/a&gt; scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets. To get the count of such subsets instead, set &lt;code&gt;normalize&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">マルチラベル分類では、&lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt; &lt;code&gt;zero_one_loss&lt;/code&gt; は&lt;/a&gt;、ラベルが予測と完全に一致する場合、サブセットを1としてスコア付けし、エラーがある場合、0としてスコア付けします。デフォルトでは、この関数は不完全に予測されたサブセットの割合を返します。代わりにそのようなサブセットの数を取得するには、 &lt;code&gt;normalize&lt;/code&gt; を &lt;code&gt;False&lt;/code&gt; に設定します</target>
        </trans-unit>
        <trans-unit id="2cdc777c3fd9aacea19e984339f1423c55608098" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes the individual labels.</source>
          <target state="translated">マルチラベル分類では,ハミング損失はサブセット・ゼロワン損失とは異なります.ゼロワン損失は,与えられたサンプルのラベルの集合全体が真のラベルの集合と完全に一致していない場合,ラベルの集合全体が不正確であるとみなします.ハミング損失は,個々のラベルにペナルティを与えるという点で,より寛容です.</target>
        </trans-unit>
        <trans-unit id="00e9bece59054d08c4ac787e06eeb4fc8070bdab" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.</source>
          <target state="translated">マルチラベル分類では,この関数は部分集合の精度を返します.サンプルの予測ラベルのセット全体が,真のラベルのセットと厳密に一致する場合,サブセット精度は1.0となり,そうでない場合は0.0となります.</target>
        </trans-unit>
        <trans-unit id="7cd1b88a6c55666089bdc7543f7e259d70d5898d" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the zero_one_loss function corresponds to the subset zero-one loss: for each sample, the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one.</source>
          <target state="translated">マルチラベル分類では,zero_one_loss関数は,サブセットの0-1損失に対応します:各サンプルについて,ラベルのセット全体が正しく予測されなければならず,そうでなければ,そのサンプルの損失は1に等しくなります.</target>
        </trans-unit>
        <trans-unit id="c56a96e702a01557c0cb1c7c6c5d254cdaebcc8b" translate="yes" xml:space="preserve">
          <source>In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must &lt;em&gt;exactly&lt;/em&gt; match the corresponding set of labels in y_true.</source>
          <target state="translated">マルチラベル分類では、この関数はサブセットの精度を計算します。サンプルに対して予測されたラベルのセットは、y_trueの対応するラベルのセットと&lt;em&gt;正確に&lt;/em&gt;一致する必要があります。</target>
        </trans-unit>
        <trans-unit id="3fad4287dcc0210ad8169708b233947ca706f077" translate="yes" xml:space="preserve">
          <source>In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels.</source>
          <target state="translated">マルチラベル学習では、各サンプルはそれに関連付けられた任意の数の基底真理ラベルを持つことができます。目標は,基底真理ラベルに高得点とより良いランクを与えることである.</target>
        </trans-unit>
        <trans-unit id="9d6449537c42279d12e406059563c338784d06f3" translate="yes" xml:space="preserve">
          <source>In multilabel learning, the joint set of binary classification tasks is expressed with label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values: the one, i.e. the non zero elements, corresponds to the subset of labels. An array such as &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample.</source>
          <target state="translated">マルチラベル学習では、バイナリ分類タスクのジョイントセットはラベルバイナリインジケーター配列で表されます。各サンプルは、バイナリ値を持つ形状の2次元配列（n_samples、n_classes）の1行です。1つ、つまり非ゼロ要素は、ラベルのサブセット。 &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; などの配列は、最初のサンプルのラベル0、2番目のサンプルのラベル1および2を表します、3番目のサンプルにはラベルがありません。</target>
        </trans-unit>
        <trans-unit id="6c2c0f769c8a98dc6df3f2e7afe566ac80c0f339" translate="yes" xml:space="preserve">
          <source>In normal usage, the Calinski-Harabaz index is applied to the results of a cluster analysis.</source>
          <target state="translated">通常の使用法では、Calinski-Harabaz指数は、クラスタ分析の結果に適用されます。</target>
        </trans-unit>
        <trans-unit id="5f0c7d20ec265094d1673fd625fd38165b384452" translate="yes" xml:space="preserve">
          <source>In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows:</source>
          <target state="translated">通常の使用法では、Davies-Bouldin指数は、以下のようにクラスタ分析の結果に適用されます。</target>
        </trans-unit>
        <trans-unit id="0488e7351783ef8ef785f4bdea49af8c75724adf" translate="yes" xml:space="preserve">
          <source>In normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis.</source>
          <target state="translated">通常の使用法では、シルエット係数はクラスタ分析の結果に適用されます。</target>
        </trans-unit>
        <trans-unit id="af7916eabb756a4304309b1e18ceea097a7a5071" translate="yes" xml:space="preserve">
          <source>In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs should thus be taken into account. Many such models will thus be casted as &amp;ldquo;Structured output&amp;rdquo; problems which are currently outside of the scope of scikit-learn.</source>
          <target state="translated">自然言語理解のより広い課題に取り組むために、文と段落のローカル構造を考慮に入れるべきです。したがって、このようなモデルの多くは、現在scikit-learnの範囲外である「構造化出力」の問題としてキャストされます。</target>
        </trans-unit>
        <trans-unit id="819693d214fc959100941f9c2bf3cb570fc069ec" translate="yes" xml:space="preserve">
          <source>In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:</source>
          <target state="translated">これに対応するために、scikit-learnはテキストコンテンツから数値特徴量を抽出する最も一般的な方法である、以下のようなユーティリティを提供しています。</target>
        </trans-unit>
        <trans-unit id="5bdd52099ccc039c40b609f18b326c63aea62fae" translate="yes" xml:space="preserve">
          <source>In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the &lt;code&gt;scipy.sparse&lt;/code&gt; package.</source>
          <target state="translated">そのような行列をメモリに格納できるようにするだけでなく、代数演算行列/ベクトルを高速化するために、実装は通常、 &lt;code&gt;scipy.sparse&lt;/code&gt; パッケージで利用可能な実装などの疎な表現を使用します。</target>
        </trans-unit>
        <trans-unit id="b0bf98f40bc311f4824763dea8c552bc0812d861" translate="yes" xml:space="preserve">
          <source>In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; as demonstrated in the following example that extract &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt; vectors of unigram tokens from a subset of 20news:</source>
          <target state="translated">予測モデルまたはクラスタリングモデルにテキストデータをフィードするには、まずテキストを統計分析に適した数値のベクトルに変換する必要があります。これは、 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; のサブセットからユニグラムトークンの&lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt;ベクトルを抽出する次の例に示すように、sklearn.feature_extraction.textのユーティリティを使用して実現できます。</target>
        </trans-unit>
        <trans-unit id="a439a73e36b65ee0a94b3f1d9d89e3ac154697cf" translate="yes" xml:space="preserve">
          <source>In order to get faster execution times for this first example we will work on a partial dataset with only 4 categories out of the 20 available in the dataset:</source>
          <target state="translated">この最初の例の実行時間を速くするために、データセット内の20のカテゴリのうち4つだけのカテゴリを持つ部分的なデータセットで作業を行います。</target>
        </trans-unit>
        <trans-unit id="da7edac191ef2f2a6bab6d167570c5dc3d626b83" translate="yes" xml:space="preserve">
          <source>In order to learn good latent representations from a small dataset, we artificially generate more labeled data by perturbing the training data with linear shifts of 1 pixel in each direction.</source>
          <target state="translated">少ないデータセットから良い潜在表現を学習するために、訓練データを各方向に1ピクセルの線形シフトで摂動することで、より多くのラベル付きデータを人工的に生成します。</target>
        </trans-unit>
        <trans-unit id="6983d2c6ff1cbf277ea5d9522b128070bfd0a615" translate="yes" xml:space="preserve">
          <source>In order to make the vectorizer =&amp;gt; transformer =&amp;gt; classifier easier to work with, &lt;code&gt;scikit-learn&lt;/code&gt; provides a &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; class that behaves like a compound classifier:</source>
          <target state="translated">ベクトライザー=&amp;gt;トランスフォーマー=&amp;gt;分類子を扱いやすくするために、 &lt;code&gt;scikit-learn&lt;/code&gt; は複合分類子のように動作する&lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;クラスを提供します。</target>
        </trans-unit>
        <trans-unit id="5257e11193f291f6f81d5d2347e3cbb71ec9f310" translate="yes" xml:space="preserve">
          <source>In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.</source>
          <target state="translated">テキスト文書に対して機械学習を行うためには、まず、テキストの内容を数値特徴ベクトルに変換する必要があります。</target>
        </trans-unit>
        <trans-unit id="7b973d24b18f4331d1cc68b945953f9c40c766fe" translate="yes" xml:space="preserve">
          <source>In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support &lt;code&gt;predict_proba&lt;/code&gt; method):</source>
          <target state="translated">予測されたクラス確率に基づいてクラスラベルを予測するには（VottingClassifierのscikit-learn推定器が &lt;code&gt;predict_proba&lt;/code&gt; メソッドをサポートしている必要があります）：</target>
        </trans-unit>
        <trans-unit id="a7ffbb7849ad7a74935991324e062c6b6722378d" translate="yes" xml:space="preserve">
          <source>In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf&amp;ndash;idf transform.</source>
          <target state="translated">カウント機能を分類器による使用に適した浮動小数点値に再重み付けするために、tf-idf変換を使用することは非常に一般的です。</target>
        </trans-unit>
        <trans-unit id="4707665df8a323c1a68b209bc6166b3798e4ea75" translate="yes" xml:space="preserve">
          <source>In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:</source>
          <target state="translated">将来のバージョンのscikit-learnで同様のモデルを再構築するためには、追加のメタデータをピクルスモデルに沿って保存しなければなりません。</target>
        </trans-unit>
        <trans-unit id="168239ecf279021917cbfef805f1d7d711ae1c44" translate="yes" xml:space="preserve">
          <source>In order to test if a classification score is significative a technique in repeating the classification procedure after randomizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is greater than the classification score obtained in the first place.</source>
          <target state="translated">分類スコアが有意であるかどうかをテストするために、ラベルをランダム化し、入れ替えた後に、分類手順を繰り返す手法です。p値は、得られたスコアが最初に得られた分類スコアよりも大きいランのパーセンテージで与えられます。</target>
        </trans-unit>
        <trans-unit id="fdc8e1656ba1332f0933f9f656403151b15252d2" translate="yes" xml:space="preserve">
          <source>In other words, return an input X_original whose transform would be X.</source>
          <target state="translated">言い換えれば、変換がXになる入力X_originalを返します。</target>
        </trans-unit>
        <trans-unit id="f84fbaf022a2c87e2f72b92c7b8059751d7f8963" translate="yes" xml:space="preserve">
          <source>In other words, we &lt;em&gt;decomposed&lt;/em&gt; matrix \(\mathbf{X}\).</source>
          <target state="translated">つまり、マトリックス\（\ mathbf {X} \）を&lt;em&gt;分解しました&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="573ad5780d66d8749d635925a4f90732aa002652" translate="yes" xml:space="preserve">
          <source>In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:</source>
          <target state="translated">特に、Rosenberg and Hirschberg (2007)は、どのようなクラスター割り当てにおいても、以下の2つの望ましい目的を定義しています。</target>
        </trans-unit>
        <trans-unit id="dafd8fff090495231531a6dce6a0d9bf23cd3c87" translate="yes" xml:space="preserve">
          <source>In particular in a &lt;strong&gt;supervised setting&lt;/strong&gt; it can be successfully combined with fast and scalable linear models to train &lt;strong&gt;document classifiers&lt;/strong&gt;, for instance:</source>
          <target state="translated">特に、&lt;strong&gt;監視付きの設定&lt;/strong&gt;では、高速かつスケーラブルな線形モデルとうまく組み合わせて、次のような&lt;strong&gt;ドキュメント分類子&lt;/strong&gt;をトレーニングできます。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
